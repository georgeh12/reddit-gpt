{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit API data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T05:54:40.843063Z",
     "iopub.status.busy": "2024-02-25T05:54:40.842063Z",
     "iopub.status.idle": "2024-02-25T05:54:41.349233Z",
     "shell.execute_reply": "2024-02-25T05:54:41.348185Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://medium.com/bitgrit-data-science-publication/sentiment-analysis-on-reddit-tech-news-with-python-cbaddb8e9bb6\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# misc\n",
    "import datetime as dt\n",
    "from pprint import pprint\n",
    "from itertools import chain\n",
    "import json\n",
    "\n",
    "# reddit crawler\n",
    "import praw\n",
    "\n",
    "# converting created dates from reddit API into human readable format\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# make directories for data collection\n",
    "import os\n",
    "\n",
    "# copy data structure\n",
    "import copy\n",
    "\n",
    "# regular expression search PRAW results\n",
    "import re\n",
    "\n",
    "# wait time for api limits and api retry\n",
    "import time\n",
    "#import asyncio # Not implemented\n",
    "\n",
    "# debugging tools\n",
    "import traceback\n",
    "import logging\n",
    "# Change logging level to print to the debug.log file\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# Create a file handler to log debug messages to a file\n",
    "debug_handler = logging.FileHandler('debug.log')\n",
    "debug_handler.setLevel(logging.DEBUG)\n",
    "# Create a formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "debug_handler.setFormatter(formatter)\n",
    "# Add the debug handler to the root logger\n",
    "logging.getLogger('').addHandler(debug_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config with run iteration and Reddit user logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T05:54:41.352391Z",
     "iopub.status.busy": "2024-02-25T05:54:41.351858Z",
     "iopub.status.idle": "2024-02-25T05:54:41.364085Z",
     "shell.execute_reply": "2024-02-25T05:54:41.363502Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_config(users):\n",
    "    # Check if config file exists\n",
    "    config_file = 'config.json'\n",
    "    if not os.path.exists(config_file):\n",
    "        # Create config file with default content\n",
    "        with open(config_file, 'w') as f:\n",
    "            json.dump({\"run\": 0, 'reddit_user': users[0]}, f)\n",
    "\n",
    "    # Load run information and increment run\n",
    "    with open(config_file) as f:\n",
    "        config = json.load(f)\n",
    "        config['reddit_user'] = users[config['run']%len(users)]\n",
    "        config['run'] = config['run'] + 1\n",
    "\n",
    "    # Write new run to config file\n",
    "    with open(config_file, 'w') as f:\n",
    "        json.dump(config, f)\n",
    "        \n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Reddit user login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T05:54:41.367205Z",
     "iopub.status.busy": "2024-02-25T05:54:41.366689Z",
     "iopub.status.idle": "2024-02-25T05:54:41.869911Z",
     "shell.execute_reply": "2024-02-25T05:54:41.869404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit user: Zealousideal-Land259\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON credentials file\n",
    "with open('reddit.json') as f:\n",
    "    reddit_users = json.load(f)\n",
    "\n",
    "# Get user and run_name\n",
    "config = get_config(list(reddit_users.keys()))\n",
    "\n",
    "# Uncomment to change user manually\n",
    "#config['reddit_user'] = 'reddit_user4'\n",
    "#config['reddit_user'] = 'Zealousideal-Land259'\n",
    "\n",
    "# Get credentials for user\n",
    "credentials = reddit_users[config['reddit_user']]\n",
    "\n",
    "# Instantiate praw.Reddit object\n",
    "reddit = praw.Reddit(\n",
    "    client_id=credentials['client_id'],\n",
    "    client_secret=credentials['client_secret'],\n",
    "    user_agent=credentials['user_agent'],\n",
    "    redirect_uri=credentials['redirect_uri'],\n",
    "    refresh_token=credentials['refresh_token']\n",
    ")\n",
    "\n",
    "# test connection\n",
    "run_name = '{:03d}'.format(config['run'])\n",
    "print(f\"Run name: {run_name}\")\n",
    "reddit_user = str(reddit.user.me())\n",
    "print(f\"Reddit user: {reddit_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API scraper to pause between API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T05:54:41.871917Z",
     "iopub.status.busy": "2024-02-25T05:54:41.871917Z",
     "iopub.status.idle": "2024-02-25T05:54:41.885002Z",
     "shell.execute_reply": "2024-02-25T05:54:41.885002Z"
    }
   },
   "outputs": [],
   "source": [
    "# After successful result \n",
    "# Retry every 10 seconds 12 times for a total of 2 minutes\n",
    "def retry_function(func, *args, max_attempts=12, delay=10, **kwargs):\n",
    "    attempts = 0\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            start_time = time.time() # record start time of api call\n",
    "            result = func(*args, **kwargs)  # Call the function\n",
    "            end_time = time.time() # record start time of api call\n",
    "            # Reddit API restricts to 100 queries per minute\n",
    "            reddit_api_restriction = 100/60\n",
    "            # wait for the difference between the api restriction and the total api call time\n",
    "            api_wait_time = reddit_api_restriction - (end_time-start_time)\n",
    "            if api_wait_time > 0: time.sleep(api_wait_time)\n",
    "            return result  # Return the result if successful\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(f\"Function: {func}\")\n",
    "            print(f\"Args: {args}\")\n",
    "            traceback.print_exc()\n",
    "            attempts += 1\n",
    "            if attempts < max_attempts:\n",
    "                print(f\"Retrying attempt #{attempts} in {delay} seconds...\")\n",
    "                for _ in range(delay): time.sleep(1)\n",
    "    print(\"Max attempts reached. Continuing loop.\")\n",
    "    return None  # Or you can raise an exception here if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape posts from Reddit with target date range\n",
    "Filter results from API mentioning GPT-3 and GPT-4 from November 1 2022 to January 31 2023 and February 15, 2023 to May 15, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T05:54:41.888003Z",
     "iopub.status.busy": "2024-02-25T05:54:41.888003Z",
     "iopub.status.idle": "2024-02-25T05:54:41.900176Z",
     "shell.execute_reply": "2024-02-25T05:54:41.900176Z"
    }
   },
   "outputs": [],
   "source": [
    "#November 1 2022 to January 31 2023\n",
    "# Define the date range for GPT-3 hype analysis\n",
    "gpt3_start = datetime(2022, 11, 1)\n",
    "gpt3_end = datetime(2023, 1, 31)\n",
    "\n",
    "#February 15 to May 15 that includes the launch GPT-4 on March 14, 2023\n",
    "# Define the date range for GPT-4 hype analysis\n",
    "gpt4_start = datetime(2023, 2, 15)\n",
    "gpt4_end = datetime(2023, 5, 15)\n",
    "\n",
    "# API structure\n",
    "posts_dict_template = {\"id\":[],\n",
    "                        \"subreddit\":[],\n",
    "                        \"query\":[],\n",
    "                        \"sort\":[],\n",
    "                        \"date\":[],\n",
    "                        \"title\":[],\n",
    "                        \"author\":[],\n",
    "                        \"stickied\":[],\n",
    "                        \"upvote_ratio\":[],\n",
    "                        \"score\":[],\n",
    "                        \"url\":[],\n",
    "                        \"num_comments\": [],\n",
    "                        \"created\": [],\n",
    "                        \"body\":[]}\n",
    "\n",
    "def scrape_submission(posts_dict, submission, other):\n",
    "    # Scrape only dates within the timeframes\n",
    "    date = datetime.utcfromtimestamp(submission.created)\n",
    "    if (gpt3_start <= date < gpt3_end + timedelta(days=1)) | (gpt4_start <= date < gpt4_end + timedelta(days=1)):\n",
    "        # build the dictionary\n",
    "        posts_dict[\"date\"].append(date)\n",
    "        posts_dict[\"title\"].append(submission.title)\n",
    "        posts_dict[\"author\"].append(str(submission.author))\n",
    "        posts_dict[\"stickied\"].append(submission.stickied)\n",
    "        posts_dict[\"upvote_ratio\"].append(submission.upvote_ratio)\n",
    "        posts_dict[\"score\"].append(submission.score)\n",
    "        posts_dict[\"id\"].append(submission.id)\n",
    "        posts_dict[\"url\"].append(submission.url)\n",
    "        posts_dict[\"num_comments\"].append(submission.num_comments)\n",
    "        posts_dict[\"created\"].append(submission.created)\n",
    "        posts_dict[\"body\"].append(submission.selftext)\n",
    "        # add subreddit, query, sort\n",
    "        for entry in other:\n",
    "            posts_dict[entry].append(other[entry])\n",
    "    return posts_dict\n",
    "\n",
    "# API structure\n",
    "comments_dict_template = {\"id\":[],\n",
    "                        \"comment_id\":[],\n",
    "                        \"comment_date\":[],\n",
    "                        \"comment_score\":[],\n",
    "                        \"comment_num_replies\":[],\n",
    "                        \"comment_body\":[]}\n",
    "\n",
    "def scrape_comment(comments_dict, submission_dict, comment):\n",
    "    # Scrape only dates within the timeframes\n",
    "    date = datetime.utcfromtimestamp(comment.created_utc)\n",
    "    if (gpt3_start <= date < gpt3_end + timedelta(days=1)) | (gpt4_start <= date < gpt4_end + timedelta(days=1)):\n",
    "        # copy id from submission\n",
    "        comments_dict[\"id\"].append(submission_dict[\"id\"])\n",
    "        comments_dict[\"comment_id\"].append(comment.id)\n",
    "        comments_dict[\"comment_date\"].append(date)\n",
    "        comments_dict[\"comment_score\"].append(comment.score)\n",
    "        comments_dict[\"comment_num_replies\"].append(len(comment.replies))\n",
    "        comments_dict[\"comment_body\"].append(comment.body)\n",
    "    return comments_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit forum targeting\n",
    "Using forums like r/artificial, r/machinelearning, r/bigscience. Using queries like 'ChatGPT', 'OpenAI'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T05:54:41.903176Z",
     "iopub.status.busy": "2024-02-25T05:54:41.902178Z",
     "iopub.status.idle": "2024-02-25T05:54:41.915713Z",
     "shell.execute_reply": "2024-02-25T05:54:41.915197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the subreddits to search\n",
    "subreddits = sorted(['artificial', 'datascience', 'datasets', 'deeplearning', 'LanguageTechnology', 'MachineLearning', 'learnmachinelearning',\n",
    "                     'chatgpt', 'ChatGPTPromptGenius', 'ChatGPTCoding', 'GPT3', 'OpenAI'], key=lambda x: x.lower())\n",
    "\n",
    "# Define queries to search\n",
    "queries = sorted(['ChatGPT', 'GPT-4', 'GPT-3', 'GPT', 'OpenAI', 'Open-AI', 'LLM'], key=lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit metadata for number of subscribers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T05:54:41.918722Z",
     "iopub.status.busy": "2024-02-25T05:54:41.917725Z",
     "iopub.status.idle": "2024-02-25T05:54:43.457115Z",
     "shell.execute_reply": "2024-02-25T05:54:43.456115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>subscribers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>artificial</td>\n",
       "      <td>720349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chatgpt</td>\n",
       "      <td>4507776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ChatGPTCoding</td>\n",
       "      <td>104377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ChatGPTPromptGenius</td>\n",
       "      <td>193302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>datascience</td>\n",
       "      <td>1366943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>datasets</td>\n",
       "      <td>187715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deeplearning</td>\n",
       "      <td>149118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GPT3</td>\n",
       "      <td>722368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LanguageTechnology</td>\n",
       "      <td>46152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>learnmachinelearning</td>\n",
       "      <td>384569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>2870543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OpenAI</td>\n",
       "      <td>1125484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  subscribers\n",
       "0             artificial       720349\n",
       "1                chatgpt      4507776\n",
       "2          ChatGPTCoding       104377\n",
       "3    ChatGPTPromptGenius       193302\n",
       "4            datascience      1366943\n",
       "5               datasets       187715\n",
       "6           deeplearning       149118\n",
       "7                   GPT3       722368\n",
       "8     LanguageTechnology        46152\n",
       "9   learnmachinelearning       384569\n",
       "10       MachineLearning      2870543\n",
       "11                OpenAI      1125484"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe structure\n",
    "subreddit_dict = {  \"name\":[],\n",
    "                    \"subscribers\":[] }\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    sub = reddit.subreddit(subreddit)\n",
    "    sub_dir = '/'.join(['data', reddit_user, run_name])\n",
    "    if not os.path.exists(sub_dir):\n",
    "        os.makedirs(sub_dir)\n",
    "    subreddit_dict[\"name\"].append(sub.display_name)\n",
    "    subreddit_dict[\"subscribers\"].append(sub.subscribers)\n",
    "subreddit_data = pd.DataFrame(subreddit_dict)\n",
    "subreddit_data.to_csv('/'.join(['data', reddit_user, run_name, 'subreddits' + '.meta']))\n",
    "subreddit_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through Subreddit API calls\n",
    "Loop through queries, controversial, and matched submissions within each Subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T05:54:43.460454Z",
     "iopub.status.busy": "2024-02-25T05:54:43.459924Z",
     "iopub.status.idle": "2024-02-25T07:15:33.537986Z",
     "shell.execute_reply": "2024-02-25T07:15:33.537986Z"
    }
   },
   "outputs": [],
   "source": [
    "# total data retrieved\n",
    "running_count_searches = 0\n",
    "# number of queries\n",
    "running_count_queries = 0\n",
    "# number of unqiue submissions\n",
    "running_count_submissions = 0\n",
    "# number of comments\n",
    "running_count_comments = 0\n",
    "\n",
    "# Query for a subreddit by name\n",
    "for subreddit in subreddits:\n",
    "    subreddit_posts = pd.DataFrame()\n",
    "    subreddit_comments = pd.DataFrame()\n",
    "    sub = reddit.subreddit(subreddit)\n",
    "    sub_dir = '/'.join(['data', reddit_user, run_name, subreddit])\n",
    "    if not os.path.exists(sub_dir):\n",
    "        os.makedirs(sub_dir)\n",
    "\n",
    "    def sub_search(query):\n",
    "        posts_dict = copy.deepcopy(posts_dict_template)\n",
    "        sort_options = ['top','comments','relevance']\n",
    "        for sort in sort_options:\n",
    "            # search posts by keyword\n",
    "            search_results = sub.search(query=query, sort=sort, syntax='plain', time_filter='all')\n",
    "            for submission in search_results:\n",
    "                # Append the post list after checking the dates\n",
    "                posts_dict = scrape_submission(posts_dict, submission, {'subreddit': subreddit, 'query': query, 'sort': sort})\n",
    "        return pd.DataFrame(posts_dict)\n",
    "\n",
    "    # PRAW search function\n",
    "    for query in queries:\n",
    "        # Call the retry_function with your function\n",
    "        posts_data = retry_function(sub_search, query)\n",
    "        running_count_queries+=1\n",
    "        if((posts_data is not None) & (not posts_data.empty)):\n",
    "            print(posts_data)\n",
    "            subreddit_posts = pd.concat([subreddit_posts, posts_data], ignore_index=True)\n",
    "            running_count_searches += len(posts_data)\n",
    "\n",
    "    def sub_controversial():\n",
    "        posts_dict = copy.deepcopy(posts_dict_template)\n",
    "        # search by all controversial posts within the subreddit\n",
    "        search_results = sub.controversial(time_filter='all')\n",
    "        for submission in search_results:\n",
    "            # search for query text in controversial posts\n",
    "            for query in queries:\n",
    "                pattern = re.compile(r'\\b' + re.escape(query) + r'\\b', re.IGNORECASE)  # Compile regex pattern\n",
    "                if pattern.search(submission.title) or pattern.search(submission.selftext):\n",
    "                    # Append the post list after checking the dates\n",
    "                    posts_dict = scrape_submission(posts_dict, submission, {'subreddit': subreddit, 'query': query, 'sort': 'controversial'})\n",
    "                    break\n",
    "        return pd.DataFrame(posts_dict)\n",
    "\n",
    "    # PRAW controversial function\n",
    "    posts_data = retry_function(sub_controversial)\n",
    "    running_count_queries+=1\n",
    "    if((posts_data is not None) & (not posts_data.empty)):\n",
    "        print(posts_data)\n",
    "        subreddit_posts = pd.concat([subreddit_posts, posts_data], ignore_index=True)\n",
    "        running_count_searches += len(posts_data)\n",
    "\n",
    "    def comment_search(submission_dict):\n",
    "        comments_dict = copy.deepcopy(comments_dict_template)\n",
    "        # search each submission for comments\n",
    "        submission = reddit.submission(id=submission_dict['id'])\n",
    "        for comment in submission.comments:\n",
    "            # More comments object shows up at the end of long lists of comments\n",
    "            if type(comment).__name__ == 'Comment':\n",
    "                # Append the comment list after checking the dates\n",
    "                comments_dict = scrape_comment(comments_dict, submission_dict, comment)\n",
    "        return pd.DataFrame(comments_dict)\n",
    "\n",
    "    # Write CSV with unique subreddit posts\n",
    "    subreddit_posts.drop_duplicates(subset='id', inplace=True)  # Drop duplicates based on the 'id' column\n",
    "    subreddit_posts.to_csv('/'.join([sub_dir, 'posts.csv']))\n",
    "    \n",
    "    # Scrape comments in each subreddit post\n",
    "    for index, row in subreddit_posts.iterrows():\n",
    "        submission_dict = row.to_dict()\n",
    "        comments_data = retry_function(comment_search, submission_dict)\n",
    "        running_count_submissions+=1\n",
    "        if((comments_data is not None) & (not comments_data.empty)):\n",
    "            subreddit_comments = pd.concat([subreddit_comments, comments_data], ignore_index=True)\n",
    "            running_count_comments += len(comments_data)\n",
    "    # Write CSV with all subreddit comments, unique to each post\n",
    "    subreddit_comments.to_csv('/'.join([sub_dir, 'comments.csv']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries sent: 96\n",
      "Total submissions from searches: 7459\n",
      "Total unique submissions: 2604\n",
      "Total submission comments: 53396\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(f\"Total queries sent: {running_count_queries}\")\n",
    "print(f\"Total submissions from searches: {running_count_searches}\")\n",
    "print(f\"Total unique submissions: {running_count_submissions}\")\n",
    "print(f\"Total submission comments: {running_count_comments}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
