,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,id,url,num_comments,created,body
0,machinelearning,openai,top,2023-03-15 22:34:01,[D] Our community must get serious about opposing OpenAI,SOCSChamp,False,0.95,2965,11sboh1,https://www.reddit.com/r/MachineLearning/comments/11sboh1/d_our_community_must_get_serious_about_opposing/,448,1678919641.0,"OpenAI was founded for the explicit purpose of democratizing access to AI and acting as a counterbalance to the closed off world of big tech by developing open source tools.

They have abandoned this idea entirely.

Today, with the release of GPT4 and their direct statement that they will not release details of the model creation due to ""safety concerns"" and the competitive environment, they have created a precedent worse than those that existed before they entered the field. We're at risk now of other major players, who previously at least published their work and contributed to open source tools, close themselves off as well.

AI alignment is a serious issue that we definitely have not solved. Its a huge field with a dizzying array of ideas, beliefs and approaches. We're talking about trying to capture the interests and goals of all humanity, after all. In this space, the one approach that is horrifying (and the one that OpenAI was LITERALLY created to prevent) is a singular or oligarchy of for profit corporations making this decision for us. This is exactly what OpenAI plans to do.

I get it, GPT4 is incredible. However, we are talking about the single most transformative technology and societal change that humanity has ever made. It needs to be for everyone or else the average person is going to be left behind.

We need to unify around open source development; choose companies that contribute to science, and condemn the ones that don't.

This conversation will only ever get more important."
1,machinelearning,openai,top,2022-05-27 05:46:54,"[D] I don't really trust papers out of ""Top Labs"" anymore",MrAcurite,False,0.97,1681,uyratt,https://www.reddit.com/r/MachineLearning/comments/uyratt/d_i_dont_really_trust_papers_out_of_top_labs/,262,1653630414.0,"I mean, I trust that the numbers they got are accurate and that they really did the work and got the results. I believe those. It's just that, take the recent ""An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems"" paper. It's 18 pages of talking through this pretty convoluted evolutionary and multitask learning algorithm, it's pretty interesting, solves a bunch of problems. But two notes. 

One, the big number they cite as the success metric is 99.43 on CIFAR-10, against a SotA of 99.40, so woop-de-fucking-doo in the grand scheme of things.

Two, there's a chart towards the end of the paper that details how many TPU core-hours were used for just the training regimens that results in the final results. The sum total is 17,810 core-hours. Let's assume that for someone who doesn't work at Google, you'd have to use on-demand pricing of $3.22/hr. This means that these trained models cost $57,348. 

Strictly speaking, throwing enough compute at a general enough genetic algorithm will eventually produce arbitrarily good performance, so while you can absolutely read this paper and collect interesting ideas about how to use genetic algorithms to accomplish multitask learning by having each new task leverage learned weights from previous tasks by defining modifications to a subset of components of a pre-existing model, there's a meta-textual level on which this paper is just ""Jeff Dean spent enough money to feed a family of four for half a decade to get a 0.03% improvement on CIFAR-10.""

OpenAI is far and away the worst offender here, but it seems like everyone's doing it. You throw a fuckton of compute and a light ganache of new ideas at an existing problem with existing data and existing benchmarks, and then if your numbers are infinitesimally higher than their numbers, you get to put a lil' sticker on your CV. Why should I trust that your ideas are even any good? I can't check them, I can't apply them to my own projects. 

Is this really what we're comfortable with as a community? A handful of corporations and the occasional university waving their dicks at everyone because they've got the compute to burn and we don't? There's a level at which I think there should be a new journal, exclusively for papers in which you can replicate their experimental results in under eight hours on a single consumer GPU."
2,machinelearning,openai,top,2017-07-03 20:24:09,[D] Why can't you guys comment your fucking code?,didntfinishhighschoo,False,0.86,1655,6l2esd,https://www.reddit.com/r/MachineLearning/comments/6l2esd/d_why_cant_you_guys_comment_your_fucking_code/,477,1499113449.0,"Seriously.

I spent the last few years doing web app development. Dug into DL a couple months ago. Supposedly, compared to the post-post-post-docs doing AI stuff, JavaScript developers should be inbred peasants. But every project these peasants release, even a fucking library that colorizes CLI output, has a catchy name, extensive docs, shitloads of comments, fuckton of tests, semantic versioning, changelog, and, oh my god, better variable names than `ctx_h` or `lang_hs` or `fuck_you_for_trying_to_understand`.

The concepts and ideas behind DL, GANs, LSTMs, CNNs, whatever – it's clear, it's simple, it's intuitive. The slog is to go through the jargon (that keeps changing beneath your feet - what's the point of using fancy words if you can't keep them consistent?), the unnecessary equations, trying to squeeze meaning from bullshit language used in papers, figuring out the super important steps, preprocessing, hyperparameters optimization that the authors, oops, failed to mention.

Sorry for singling out, but [look at this](https://github.com/facebookresearch/end-to-end-negotiator/blob/master/src/agent.py) - what the fuck? If a developer anywhere else at Facebook would get this code for a review they would throw up.

- Do you intentionally try to obfuscate your papers? Is pseudo-code a fucking premium? Can you at least try to give some intuition before showering the reader with equations?

- How the fuck do you dare to release a paper without source code?

- Why the fuck do you never ever add comments to you code?

- When naming things, are you charged by the character? Do you get a bonus for acronyms?

- Do you realize that OpenAI having needed to release a ""baseline"" TRPO implementation is a fucking disgrace to your profession?

- Jesus christ, who decided to name a tensor concatenation function `cat`?
"
3,machinelearning,openai,top,2023-05-17 22:15:28,[D] Does anybody else despise OpenAI?,onesynthguy,False,0.86,1412,13kfxzy,https://www.reddit.com/r/MachineLearning/comments/13kfxzy/d_does_anybody_else_despise_openai/,426,1684361728.0," I  mean, don't get me started with the closed source models they have that were trained using the work of unassuming individuals who will never  see a penny for it. Put it up on Github they said. I'm all for  open-source, but when a company turns around and charges you for a  product they made with freely and publicly made content, while forbidding you from using the output to create competing models, that is where I  draw the line. It is simply ridiculous. 

Sam Altman couldn't be anymore predictable with his recent attempts to get the government to start regulating AI.

What  risks? The AI is just a messenger for information that is already out  there if one knows how/where to look. You don't need AI to learn how to  hack, to learn how to make weapons, etc. Fake news/propaganda? The  internet has all of that covered. LLMs are no where near the level of AI  you see in sci-fi. I mean, are people really afraid of text? Yes, I  know that text can sometimes be malicious code such as viruses, but  those can be found on github as well.  If they fall for this they might  as well shutdown the internet while they're at it.

He  is simply blowing things out of proportion and using fear to increase  the likelihood that they do what he wants, hurt the competition. I  bet he is probably teething with bitterness everytime a new huggingface  model comes out. The thought of us peasants being able to use AI  privately is too dangerous. No, instead we must be fed scraps while they  slowly take away our jobs and determine our future.

This  is not a doomer post, as I am all in favor of the advancement of AI.  However, the real danger here lies in having a company like OpenAI  dictate the future of humanity. I get it, the writing is on the wall;  the cost of human intelligence will go down, but if everyone has their  personal AI then it wouldn't seem so bad or unfair would it? Listen,  something that has the power to render a college degree that costs  thousands of dollars worthless should be available to the public. This  is to offset the damages and job layoffs that will come as a result of  such an entity. It wouldn't be as bitter of a taste as it would if you were replaced by it while still not being able to access it. Everyone should be able to use it as leverage, it is the only fair solution.

If  we don't take action now, a company like ClosedAI will, and they are  not in favor of the common folk. Sam Altman is so calculated to the  point where there were times when he seemed to be shooting OpenAI in the foot during his talk.  This move is to simply conceal his real intentions, to climb the ladder and take it with him. If he didn't include his company in his  ramblings, he would be easily read. So instead, he pretends to be scared of his own product, in an effort to legitimize his claim. Don't fall  for it.

They are slowly making a  reputation as one the most hated tech companies, right up there with  Adobe, and they don't show any sign of change. They have no moat,  othewise they wouldn't feel so threatened to the point where they would have to resort to creating barriers of entry via regulation. This only  means one thing, we are slowly catching up. We just need someone to  vouch for humanity's well-being, while acting as an opposing force to the  evil corporations who are only looking out for themselves. Question is,  who would be a good candidate?"
4,machinelearning,openai,top,2018-02-17 12:45:30,[P] Landing the Falcon booster with Reinforcement Learning in OpenAI,EmbersArc,False,0.95,1286,7y6g79,https://gfycat.com/CoarseEmbellishedIsopod,55,1518871530.0,
5,machinelearning,openai,top,2023-05-04 16:13:30,"[D] Google ""We Have No Moat, And Neither Does OpenAI"": Leaked Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI",hardmaru,False,0.98,1177,137rxgw,https://www.semianalysis.com/p/google-we-have-no-moat-and-neither,206,1683216810.0,
6,machinelearning,openai,top,2023-03-28 05:57:03,[N] OpenAI may have benchmarked GPT-4’s coding ability on it’s own training data,Balance-,False,0.97,997,124eyso,https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/,135,1679983023.0,"[GPT-4 and professional benchmarks: the wrong answer to the wrong question](https://aisnakeoil.substack.com/p/gpt-4-and-professional-benchmarks)

*OpenAI may have tested on the training data. Besides, human benchmarks are meaningless for bots.*

 **Problem 1: training data contamination**

To benchmark GPT-4’s coding ability, OpenAI evaluated it on problems from Codeforces, a website that hosts coding competitions. Surprisingly, Horace He pointed out that GPT-4 solved 10/10 pre-2021 problems and 0/10 recent problems in the easy category. The training data cutoff for GPT-4 is September 2021. This strongly suggests that the model is able to memorize solutions from its training set — or at least partly memorize them, enough that it can fill in what it can’t recall.

As further evidence for this hypothesis, we tested it on Codeforces problems from different times in 2021. We found that it could regularly solve problems in the easy category before September 5, but none of the problems after September 12.

In fact, we can definitively show that it has memorized problems in its training set: when prompted with the title of a Codeforces problem, GPT-4 includes a link to the exact contest where the problem appears (and the round number is almost correct: it is off by one). Note that GPT-4 cannot access the Internet, so memorization is the only explanation."
7,machinelearning,openai,top,2023-05-22 16:15:53,[R] GPT-4 didn't really score 90th percentile on the bar exam,salamenzon,False,0.97,845,13ovc04,https://www.reddit.com/r/MachineLearning/comments/13ovc04/r_gpt4_didnt_really_score_90th_percentile_on_the/,160,1684772153.0,"According to [this article](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4441311), OpenAI's claim that it scored 90th percentile on the UBE appears to be based on approximate conversions from estimates of February administrations of the Illinois Bar Exam, which ""are heavily skewed towards repeat test-takers who failed the July administration and score significantly lower than the general test-taking population.""

Compared to July test-takers, GPT-4's UBE score would be 68th percentile, including \~48th on essays. Compared to first-time test takers, GPT-4's UBE score is estimated to be \~63rd percentile, including \~42nd on essays. Compared to those who actually passed, its UBE score would be \~48th percentile, including \~15th percentile on essays."
8,machinelearning,openai,top,2021-02-23 19:55:50,[N] 20 hours of new lectures on Deep Learning and Reinforcement Learning with lots of examples,cwkx,False,0.98,823,lqrek7,https://www.reddit.com/r/MachineLearning/comments/lqrek7/n_20_hours_of_new_lectures_on_deep_learning_and/,45,1614110150.0,"If anyone's interested in a Deep Learning and Reinforcement Learning series, I uploaded 20 hours of lectures on YouTube yesterday. Compared to other lectures, I think this gives quite a broad/compact overview of the fields with lots of minimal examples to build on. Here are the links:

**Deep Learning** ([playlist](https://www.youtube.com/playlist?list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57))  
*The first five lectures are more theoretical, the second half is more applied.*

* Lecture 1: Introduction. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture1.pdf), [video](https://www.youtube.com/watch?v=s2uXPz3wyCk&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=1))
* Lecture 2: Mathematical principles and backpropagation. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture2.pdf), [colab](https://colab.research.google.com/gist/cwkx/dfa207c8ceed5999bdad1ec6f637dd47/distributions.ipynb), [video](https://www.youtube.com/watch?v=dfZ0cIQSjm4&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=2))
* Lecture 3: PyTorch programming: *coding session*. ([colab1](https://colab.research.google.com/gist/cwkx/441e508d3b904413fd3950a09a1d3bd6/classifier.ipynb), [colab2](https://colab.research.google.com/gist/cwkx/3a6eba039aa9f68d0b9d37a02216d385/convnet.ipynb), [video](https://www.youtube.com/watch?v=KiqXWOcz4Z0&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=3)) - minor issues with audio, but it fixes itself later.
* Lecture 4: Designing models to generalise. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture4.pdf), [video](https://www.youtube.com/watch?v=4vKKj8bkS-E&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=4))
* Lecture 5: Generative models. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture5.pdf), [desmos](https://www.desmos.com/calculator/2sboqbhler), [colab](https://colab.research.google.com/gist/cwkx/e3ef25d0adb6e2f2bf747ce664bab318/conv-autoencoder.ipynb), [video](https://www.youtube.com/watch?v=hyxlTwvLi-o&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=5))
* Lecture 6: Adversarial models. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture6.pdf), [colab1](https://colab.research.google.com/gist/cwkx/74e33bc96f94f381bd15032d57e43786/simple-gan.ipynb), [colab2](https://colab.research.google.com/gist/cwkx/348cde3bf11a08c45a69b1873ebb6de3/conditional-gan.ipynb), [colab3](https://colab.research.google.com/gist/cwkx/7f5377ed8414a096180128b487846698/info-gan.ipynb), [colab4](https://colab.research.google.com/gist/cwkx/aece978bc38ba35c2267d91b793a1456/unet.ipynb), [video](https://www.youtube.com/watch?v=JLHyU7AjB4s&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=6))
* Lecture 7: Energy-based models. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture7.pdf), [colab](https://colab.research.google.com/gist/cwkx/6b2d802e804e908a3ee3d58c1e0e73be/dbm.ipynb), [video](https://www.youtube.com/watch?v=kpulMklVmRU&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=7))
* Lecture 8: Sequential models: *by* u/samb-t. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture8.pdf), [colab1](https://colab.research.google.com/gist/samb-t/ac6dbd433c618eedcd0442f577697ea3/generative-rnn.ipynb), [colab2](https://colab.research.google.com/gist/samb-t/27cc3217799825975b65326d6e7b377b/transformer-translation.ipynb), [video](https://www.youtube.com/watch?v=pxRnFwNFTOM&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=8))
* Lecture 9: Flow models and implicit networks. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture9.pdf), [SIREN](https://vsitzmann.github.io/siren/), [GON](https://cwkx.github.io/data/GON/), [video](https://www.youtube.com/watch?v=zRdwh9C5xn4&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=9))
* Lecture 10: Meta and manifold learning. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/dl-lecture10.pdf), [interview](https://youtu.be/PqbB07n_uQ4?t=444), [video](https://www.youtube.com/watch?v=na1-oIn8Kdo&list=PLMsTLcO6etti_SObSLvk9ZNvoS_0yia57&index=10))

**Reinforcement Learning** ([playlist](https://www.youtube.com/playlist?list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE))  
*This is based on David Silver's course but targeting younger students within a shorter 50min format (missing the advanced derivations) + more examples and Colab code.*

* Lecture 1: Foundations. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture1.pdf), [video](https://www.youtube.com/watch?v=K67RJH3V7Yw&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=1))
* Lecture 2: Markov decision processes. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture2.pdf), [colab](https://colab.research.google.com/gist/cwkx/ba6c44031137575d2445901ee90454da/mrp.ipynb), [video](https://www.youtube.com/watch?v=RmOdTQYQqmQ&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=2))
* Lecture 3: OpenAI gym. ([video](https://www.youtube.com/watch?v=BNSwFURmaCA&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=3))
* Lecture 4: Dynamic programming. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture4.pdf), [colab](https://colab.research.google.com/gist/cwkx/670c8d44a9a342355a4a883c498dbc9d/dynamic-programming.ipynb), [video](https://www.youtube.com/watch?v=gqC_p2XWpLU&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=4))
* Lecture 5: Monte Carlo methods. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture5.pdf), [colab](https://colab.research.google.com/gist/cwkx/a5129e8888562d1b4ecb0da611c58ce8/monte-carlo-methods.ipynb), [video](https://www.youtube.com/watch?v=4xfWzLmIccs&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=5))
* Lecture 6: Temporal-difference methods. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture6.pdf), [colab](https://colab.research.google.com/gist/cwkx/54e2e6d59918a083e47f19404fe275b4/temporal-difference-learning.ipynb), [video](https://www.youtube.com/watch?v=phgI_880uSw&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=6))
* Lecture 7: Function approximation. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture7.pdf), [code](https://github.com/higgsfield/RL-Adventure), [video](https://www.youtube.com/watch?v=oqmCj95d3Y4&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=7))
* Lecture 8: Policy gradient methods. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture8.pdf), [code](https://github.com/higgsfield/RL-Adventure-2), [theory](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html), [video](https://www.youtube.com/watch?v=h4HixR0Co6Q&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=8))
* Lecture 9: Model-based methods. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture9.pdf), [video](https://www.youtube.com/watch?v=aUjuBvqJ8UM&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=9))
* Lecture 10: Extended methods. ([slides](https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture10.pdf), [atari](https://www.youtube.com/playlist?list=PL34t13IwtOXUNliyyJtoamekLAbqhB9Il), [video](https://www.youtube.com/watch?v=w6rGqprrxp8&list=PLMsTLcO6ettgmyLVrcPvFLYi2Rs-R4JOE&index=10))"
9,machinelearning,openai,top,2023-05-06 18:41:02,[R][P] I made an app for Instant Image/Text to 3D using ShapE from OpenAI,perception-eng,False,0.96,812,139yc73,https://i.redd.it/1j4h1oyda9ya1.gif,63,1683398462.0,
10,machinelearning,openai,top,2023-05-25 13:51:58,OpenAI is now complaining about regulation of AI [D],I_will_delete_myself,False,0.89,794,13rie0e,https://www.reddit.com/r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/,349,1685022718.0,"I held off for a while but hypocrisy just drives me nuts after hearing this.

SMH this company like white knights who think they are above everybody. They want regulation but they want to be untouchable by this regulation. Only wanting to hurt other people but not “almighty” Sam and friends.

Lies straight through his teeth to Congress about suggesting similar things done in the EU, but then starts complain about them now. This dude should not be taken seriously in any political sphere whatsoever.

My opinion is this company is anti-progressive for AI by locking things up which is contrary to their brand name. If they can’t even stay true to something easy like that, how should we expect them to stay true with AI safety which is much harder?

I am glad they switch sides for now, but pretty ticked how they think they are entitled to corruption to benefit only themselves. SMH!!!!!!!!

What are your thoughts?"
11,machinelearning,openai,top,2021-07-27 18:11:28,[N] OpenAI Gym is now actively maintained again (by me)! Here's my plan,jkterry1,False,0.99,784,oss2e3,https://www.reddit.com/r/MachineLearning/comments/oss2e3/n_openai_gym_is_now_actively_maintained_again_by/,47,1627409488.0,"So OpenAI made me a maintainer of Gym. This means that all the installation issues will be fixed, the now 5 year backlog of PRs will be resolved, and in general Gym will now be reasonably maintained. I posted my manifesto for future maintenance here: [https://github.com/openai/gym/issues/2259](https://github.com/openai/gym/issues/2259)  


Edit: I've been getting a bunch of messages about open source donations, so I created links:

[https://liberapay.com/jkterry](https://liberapay.com/jkterry)

[https://www.buymeacoffee.com/jkterry](https://www.buymeacoffee.com/jkterry)"
12,machinelearning,openai,top,2022-12-24 14:58:19,[R][P] I made an app for Instant Image/Text to 3D using PointE from OpenAI,perception-eng,False,0.97,765,zubg2u,https://i.redd.it/ox6urwwa1v7a1.gif,42,1671893899.0,
13,machinelearning,openai,top,2021-01-04 15:33:43,[D] Why I'm Lukewarm on Graph Neural Networks,VodkaHaze,False,0.96,671,kqazpd,https://www.reddit.com/r/MachineLearning/comments/kqazpd/d_why_im_lukewarm_on_graph_neural_networks/,105,1609774423.0,"**TL;DR:** GNNs can provide wins over simpler embedding methods, but we're at a point where other research directions matter more

I also posted it on my [blog here](https://www.singlelunch.com/2020/12/28/why-im-lukewarm-on-graph-neural-networks/), has footnotes, a nicer layout with inlined images, etc.

-----------

I'm only lukewarm on Graph Neural Networks (GNNs). There, I said it.

It might sound crazy GNNs are one of the hottest fields in machine learning right now. [There][1] were at least [four][2] [review][3] [papers][4] just in the last few months. I think some progress can come of this research, but we're also focusing on some incorrect places.

But first, let's take a step back and go over the basics.

# Models are about compression

We say graphs are a ""non-euclidean"" data type, but that's not really true. A regular graph is just another way to think about a particular flavor of square matrix called the [adjacency matrix][5], like [this](https://www.singlelunch.com/wp-content/uploads/2020/12/AdjacencyMatrices_1002.gif).

It's weird, we look at run-of-the-mill matrix full of real numbers and decide to call it ""non-euclidean"".

This is for practical reasons. Most graphs are fairly sparse, so the matrix is full of zeros. At this point, *where the non-zero numbers are* matters most, which makes the problem closer to (computationally hard) discrete math rather than (easy) continuous, gradient-friendly math.

**If you had the full matrix, life would be easy**

If we step out of the pesky realm of physics for a minute, and assume carrying the full adjacency matrix around isn't a problem, we solve a bunch of problems.

First, network node embeddings aren't a thing anymore. A node is a just row in the matrix, so it's already a vector of numbers.

Second, all network prediction problems are solved. A powerful enough and well-tuned model will simply extract all information between the network and whichever target variable we're attaching to nodes.

**NLP is also just fancy matrix compression**

Let's take a tangent away from graphs to NLP. Most NLP we do can be [thought of in terms of graphs][6] as we'll see, so it's not a big digression.

First, note that Ye Olde word embedding models like [Word2Vec][7] and [GloVe][8] are [just matrix factorization][9].

The GloVe algorithm works on a variation of the old [bag of words][10] matrix. It goes through the sentences and creates a (implicit) [co-occurence][11] graph where nodes are words and the edges are weighed by how often the words appear together in a sentence.

Glove then does matrix factorization on the matrix representation of that co-occurence graph, Word2Vec is mathematically equivalent.

You can read more on this in my [post on embeddings][12] and the one (with code) on [word embeddings][13].

**Even language models are also just matrix compression**

Language models are all the rage. They dominate most of the [state of the art][14] in NLP.

Let's take BERT as our main example. BERT predicts a word given the context of the [rest of the sentence](https://www.singlelunch.com/wp-content/uploads/2020/12/bert.png).

This grows the matrix we're factoring from flat co-occurences on pairs of words to co-occurences conditional on the sentence's context, like [this](https://www.singlelunch.com/wp-content/uploads/2020/12/Screen-Shot-2020-12-28-at-1.59.34-PM.png)

We're growing the ""ideal matrix"" we're factoring combinatorially. As noted by [Hanh & Futrell][15]:

> [...] human language—and language modelling—has infinite statistical complexity but that it can be approximated well at lower levels. This observation has two implications: 1) We can obtain good results with comparatively small models; and 2) there is a lot of potential for scaling up our models. Language models tackle such a large problem space that they probably approximate a compression of the entire language in the [Kolmogorov Complexity][16] sense. It's also possible that huge language models just [memorize a lot of it][17] rather than compress the information, for what it's worth.

### Can we upsample any graph like language models do?

We're already doing it.

Let's call a **first-order** embedding of a graph a method that works by directly factoring the graph's adjacency matrix or [Laplacian matrix][18]. If you embed a graph using [Laplacian Eigenmaps][19] or by taking the [principal components][20] of the Laplacian, that's first order. Similarly, GloVe is a first-order method on the graph of word co-occurences. One of my favorites first order methods for graphs is [ProNE][21], which works as well as most methods while being two orders of magnitude faster.

A **higher-order** method embeds the original matrix plus connections of neighbours-of-neighbours (2nd degree) and deeper k-step connections. [GraRep][22], shows you can always generate higher-order representations from first order methods by augmenting the graph matrix.

Higher order method are the ""upsampling"" we do on graphs. GNNs that sample on large neighborhoods and random-walk based methods like node2vec are doing higher-order embeddings.

# Where are the performance gain?

Most GNN papers in the last 5 years present empirical numbers that are useless for practitioners to decide on what to use.

As noted in the [OpenGraphsBenchmark][4] (OGB) paper, GNN papers do their empirical section on a handful of tiny graphs (Cora, CiteSeer, PubMed) with 2000-20,000 nodes. These datasets can't seriously differentiate between methods.

Recent efforts are directly fixing this, but the reasons why researchers focused on tiny, useless datasets for so long are worth discussing.

**Performance matters by task**

One fact that surprises a lot of people is that even though language models have the best performance in a lot of NLP tasks, if all you're doing is cram sentence embeddings into a downstream model, there [isn't much gained][23] from language models embeddings over simple methods like summing the individual Word2Vec word embeddings (This makes sense, because the full context of the sentence is captured in the sentence co-occurence matrix that is generating the Word2Vec embeddings).

Similarly, [I find][24] that for many graphs **simple first-order methods perform just as well on graph clustering and node label prediction tasks than higher-order embedding methods**. In fact higher-order methods are massively computationally wasteful for these usecases.

Recommended first order embedding methods are ProNE and my [GGVec with order=1][25].

Higher order methods normally perform better on the link prediction tasks. I'm not the only one to find this. In the BioNEV paper, they find: ""A large GraRep order value for link prediction tasks (e.g. 3, 4);a small value for node classification tasks (e.g.1, 2)"" (p.9).

Interestingly, the gap in link prediction performance is inexistant for artificially created graphs. This suggests higher order methods do learn some of the structure intrinsic to [real world graphs][26].

For visualization, first order methods are better. Visualizations of higher order methods tend to have artifacts of their sampling. For instance, Node2Vec visualizations tend to have elongated/filament-like structures which come from the embeddings coming from long single strand random walks. See the following visualizations by [Owen Cornec][27] created by first embedding the graph to 32-300 dimensions using a node embedding algorithm, then mapping this to 2d or 3d with the excellent UMAP algorithm, like [this](https://www.singlelunch.com/wp-content/uploads/2020/12/Screen-Shot-2020-12-28-at-1.59.34-PM-1.png)

Lastly, sometimes simple methods soundly beat higher order methods (there's an instance of it in the OGB paper).

The problem here is that **we don't know when any method is better than another** and **we definitely don't know the reason**.

There's definitely a reason different graph types respond better/worse to being represented by various methods. This is currently an open question.

A big part of why is that the research space is inundated under useless new algorithms because...

# Academic incentives work against progress

Here's the cynic's view of how machine learning papers are made:

1.  Take an existing algorithm
2.  Add some new layer/hyperparameter, make a cute mathematical story for why it matters
3.  Gridsearch your hyperparameters until you beat baselines from the original paper you aped
4.  Absolutely don't gridsearch stuff you're comparing against in your results section
5.  Make a cute ACRONYM for your new method, put impossible to use python 2 code on github (Or no code at all!) and bask in the citations

I'm [not][28] the [only one][29] with these views on the state reproducible research. At least it's gotten slightly better in the last 2 years.

### Sidebar: I hate Node2Vec

A side project of mine is a [node embedding library][25] and the most popular method in it is by far Node2Vec. Don't use Node2Vec.

[Node2Vec][30] with `p=1; q=1` is the [Deepwalk][31] algorithm. Deepwalk is an actual innovation.

The Node2Vec authors closely followed the steps 1-5 including bonus points on step 5 by getting word2vec name recognition.

This is not academic fraud -- the hyperparameters [do help a tiny bit][32] if you gridsearch really hard. But it's the presentable-to-your-parents sister of where you make the ML community worse off to progress your academic career. And certainly Node2Vec doesn't deserve 7500 citations.

# Progress is all about practical issues

We've known how to train neural networks for well over 40 years. Yet they only exploded in popularity with [AlexNet][33] in 2012. This is because implementations and hardware came to a point where deep learning was **practical**.

Similarly, we've known about factoring word co-occurence matrices into Word embeddings for at least 20 years.

But word embeddings only exploded in 2013 with Word2Vec. The breakthrough here was that the minibatch-based methods let you train a Wikipedia-scale embedding model on commodity hardware.

It's hard for methods in a field to make progress if training on a small amount of data takes days or weeks. You're disincentivized to explore new methods. If you want progress, your stuff has to run in reasonable time on commodity hardware. Even Google's original search algorithm [initially ran on commodity hardware][34].

**Efficiency is paramount to progress**

The reason deep learning research took off the way it did is because of improvements in [efficiency][35] as well as much better libraries and hardware support.

**Academic code is terrible**

Any amount of time you spend gridsearching Node2Vec on `p` and `q` is all put to better use gridsearching Deepwalk itself (on number of walks, length of walks, or word2vec hyperparameters). The problem is that people don't gridsearch over deepwalk because implementations are all terrible.

I wrote the [Nodevectors library][36] to have a fast deepwalk implementation because it took **32 hours** to embed a graph with a measly 150,000 nodes using the reference Node2Vec implementation (the same takes 3min with Nodevectors). It's no wonder people don't gridsearch on Deepwalk a gridsearch would take weeks with the terrible reference implementations.

To give an example, in the original paper of [GraphSAGE][37] they their algorithm to DeepWalk with walk lengths of 5, which is horrid if you've ever hyperparameter tuned a deepwalk algorithm. From their paper:

> We did observe DeepWalk’s performance could improve with further training, and in some cases it could become competitive with the unsupervised GraphSAGE approaches (but not the supervised approaches) if we let it run for >1000× longer than the other approaches (in terms of wall clock time for prediction on the test set) I don't even think the GraphSAGE authors had bad intent -- deepwalk implementations are simply so awful that they're turned away from using it properly. It's like trying to do deep learning with 2002 deep learning libraries and hardware.

# Your architectures don't really matter

One of the more important papers this year was [OpenAI's ""Scaling laws""][38] paper, where the raw number of parameters in your model is the most predictive feature of overall performance. This was noted even in the original BERT paper and drives 2020's increase in absolutely massive language models.

This is really just [Sutton' Bitter Lesson][39] in action:

> General methods that leverage computation are ultimately the most effective, and by a large margin

Transformers might be [replacing convolution][40], too. As [Yannic Kilcher said][41], transformers are ruining everything. [They work on graphs][6], in fact it's one of the [recent approaches][42], and seems to be one of the more succesful [when benchmarked][1]

Researchers seem to be putting so much effort into architecture, but it doesn't matter much in the end because you can approximate anything by stacking more layers.

Efficiency wins are great -- but neural net architectures are just one way to achieve that, and by tremendously over-researching this area we're leaving a lot of huge gains elsewhere on the table.

# Current Graph Data Structure Implementations suck

NetworkX is a bad library. I mean, it's good if you're working on tiny graphs for babies, but for anything serious it chokes and forces you to rewrite everything in... what library, really?

At this point most people working on large graphs end up hand-rolling some data structure. This is tough because your computer's memory is a 1-dimensional array of 1's and 0's and a graph has no obvious 1-d mapping.

This is even harder when we take updating the graph (adding/removing some nodes/edges) into account. Here's a few options:

### Disconnected networks of pointers

NetworkX is the best example. Here, every node is an object with a list of pointers to other nodes (the node's edges).

This layout is like a linked list. Linked lists are the [root of all performance evil][43].

Linked lists go completely against how modern computers are designed. Fetching things from memory is slow, and operating on memory is fast (by two orders of magnitude). Whenever you do anything in this layout, you make a roundtrip to RAM. It's slow by design, you can write this in Ruby or C or assembly and it'll be slow regardless, because memory fetches are slow in hardware.

The main advantage of this layout is that adding a new node is O(1). So if you're maintaining a massive graph where adding and removing nodes happens as often as reading from the graph, it makes sense.

Another advantage of this layout is that it ""scales"". Because everything is decoupled from each other you can put this data structure on a cluster. However, you're really creating a complex solution for a problem you created for yourself.

### Sparse Adjacency Matrix

This layout great for read-only graphs. I use it as the backend in my [nodevectors][25] library, and many other library writers use the [Scipy CSR Matrix][44], you can see graph algorithms implemented on it [here][45].

The most popular layout for this use is the [CSR Format][46] where you have 3 arrays holding the graph. One for edge destinations, one for edge weights and an ""index pointer"" which says which edges come from which node.

Because the CSR layout is simply 3 arrays, it scales on a single computer: a CSR matrix can be laid out on a disk instead of in-memory. You simply [memory map][47] the 3 arrays and use them on-disk from there.

With modern NVMe drives random seeks aren't slow anymore, much faster than distributed network calls like you do when scaling the linked list-based graph. I haven't seen anyone actually implement this yet, but it's in the roadmap for my implementation at least.

The problem with this representation is that adding a node or edge means rebuilding the whole data structure.

### Edgelist representations

This representation is three arrays: one for the edge sources, one for the edge destinations, and one for edge weights. [DGL][48] uses this representation internally.

This is a simple and compact layout which can be good for analysis.

The problem compared to CSR Graphs is some seek operations are slower. Say you want all the edges for node #4243. You can't jump there without maintaining an index pointer array.

So either you maintain sorted order and binary search your way there (O(log2n)) or unsorted order and linear search (O(n)).

This data structure can also work on memory mapped disk array, and node append is fast on unsorted versions (it's slow in the sorted version).

# Global methods are a dead end

Methods that work on the **entire graph at once** can't leverage computation, because they run out of RAM at a certain scale.

So any method that want a chance of being the new standard need to be able to update piecemeal on parts of the graph.

**Sampling-based methods**

Sampling Efficiency will matter more in the future

*   **Edgewise local methods**. The only algorithms I know of that do this are GloVe and GGVec, which they pass through an edge list and update embedding weights on each step. 

The problem with this approach is that it's hard to use them for higher-order methods. The advantage is that they easily scale even on one computer. Also, incrementally adding a new node is as simple as taking the existing embeddings, adding a new one, and doing another epoch over the data

*   **Random Walk sampling**. This is used by deepwalk and its descendants, usually for node embeddings rather than GNN methods. This can be computationally expensive and make it hard to add new nodes.

But this does scale, for instance [Instagram][49] use it to feed their recommendation system models

*   **Neighbourhood sampling**. This is currently the most common one in GNNs, and can be low or higher order depending on the neighborhood size. It also scales well, though implementing efficiently can be challenging.

It's currently used by [Pinterest][50]'s recommendation algorithms.

# Conclusion

Here are a few interesting questions:

*   What is the relation between graph types and methods?
*   Consolidated benchmarking like OGB
*   We're throwing random models at random benchmarks without understanding why or when they do better
*   More fundamental research. Heree's one I'm curious about: can other representation types like [Poincarre Embeddings][51] effectively encode directed relationships?

On the other hand, we should **stop focusing on** adding spicy new layers to test on the same tiny datasets. No one cares.

 [1]: https://arxiv.org/pdf/2003.00982.pdf
 [2]: https://arxiv.org/pdf/2002.11867.pdf
 [3]: https://arxiv.org/pdf/1812.08434.pdf
 [4]: https://arxiv.org/pdf/2005.00687.pdf
 [5]: https://en.wikipedia.org/wiki/Adjacency_matrix
 [6]: https://thegradient.pub/transformers-are-graph-neural-networks/
 [7]: https://en.wikipedia.org/wiki/Word2vec
 [8]: https://nlp.stanford.edu/pubs/glove.pdf
 [9]: https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf
 [10]: https://en.wikipedia.org/wiki/Bag-of-words_model
 [11]: https://en.wikipedia.org/wiki/Co-occurrence
 [12]: https://www.singlelunch.com/2020/02/16/embeddings-from-the-ground-up/
 [13]: https://www.singlelunch.com/2019/01/27/word-embeddings-from-the-ground-up/
 [14]: https://nlpprogress.com/
 [15]: http://socsci.uci.edu/~rfutrell/papers/hahn2019estimating.pdf
 [16]: https://en.wikipedia.org/wiki/Kolmogorov_complexity
 [17]: https://bair.berkeley.edu/blog/2020/12/20/lmmem/
 [18]: https://en.wikipedia.org/wiki/Laplacian_matrix
 [19]: http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=1F03130B02DC485C78BF364266B6F0CA?doi=10.1.1.19.8100&rep=rep1&type=pdf
 [20]: https://en.wikipedia.org/wiki/Principal_component_analysis
 [21]: https://www.ijcai.org/Proceedings/2019/0594.pdf
 [22]: https://dl.acm.org/doi/10.1145/2806416.2806512
 [23]: https://openreview.net/pdf?id=SyK00v5xx
 [24]: https://github.com/VHRanger/nodevectors/blob/master/examples/link%20prediction.ipynb
 [25]: https://github.com/VHRanger/nodevectors
 [26]: https://arxiv.org/pdf/1310.2636.pdf
 [27]: http://byowen.com/
 [28]: https://arxiv.org/pdf/1807.03341.pdf
 [29]: https://www.youtube.com/watch?v=Kee4ch3miVA
 [30]: https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf
 [31]: https://arxiv.org/pdf/1403.6652.pdf
 [32]: https://arxiv.org/pdf/1911.11726.pdf
 [33]: https://en.wikipedia.org/wiki/AlexNet
 [34]: https://en.wikipedia.org/wiki/Google_data_centers#Original_hardware
 [35]: https://openai.com/blog/ai-and-efficiency/
 [36]: https://www.singlelunch.com/2019/08/01/700x-faster-node2vec-models-fastest-random-walks-on-a-graph/
 [37]: https://arxiv.org/pdf/1706.02216.pdf
 [38]: https://arxiv.org/pdf/2001.08361.pdf
 [39]: http://incompleteideas.net/IncIdeas/BitterLesson.html
 [40]: https://arxiv.org/abs/2010.11929
 [41]: https://www.youtube.com/watch?v=TrdevFK_am4
 [42]: https://arxiv.org/pdf/1710.10903.pdf
 [43]: https://www.youtube.com/watch?v=fHNmRkzxHWs
 [44]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html
 [45]: https://docs.scipy.org/doc/scipy/reference/sparse.csgraph.html
 [46]: https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)
 [47]: https://en.wikipedia.org/wiki/Mmap
 [48]: https://github.com/dmlc/dgl
 [49]: https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/
 [50]: https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48
 [51]: https://arxiv.org/pdf/1705.08039.pdf"
14,machinelearning,openai,top,2021-09-06 13:39:07,[D] How OpenAI Sold its Soul for $1 Billion: The company behind GPT-3 and Codex isn’t as open as it claims.,sensetime,False,0.95,664,pizllt,https://www.reddit.com/r/MachineLearning/comments/pizllt/d_how_openai_sold_its_soul_for_1_billion_the/,107,1630935547.0,"An essay by Alberto Romero that traces the history and developments of OpenAI from the time it became a ""capped-for-profit"" entity from a non-profit entity:

Link: https://onezero.medium.com/openai-sold-its-soul-for-1-billion-cf35ff9e8cd4"
15,machinelearning,openai,top,2023-03-09 18:30:58,"[N] GPT-4 is coming next week – and it will be multimodal, says Microsoft Germany - heise online",Singularian2501,False,0.98,658,11mzqxu,https://www.reddit.com/r/MachineLearning/comments/11mzqxu/n_gpt4_is_coming_next_week_and_it_will_be/,80,1678386658.0,"[https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html](https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html)

>**GPT-4 is coming next week**: at an approximately one-hour hybrid information event entitled ""**AI in Focus - Digital Kickoff"" on 9 March 2023**, four Microsoft Germany employees presented Large Language Models (LLM) like GPT series as a disruptive force for companies and their Azure-OpenAI offering in detail. The kickoff event took place in the German language, news outlet Heise was present. **Rather casually, Andreas Braun, CTO Microsoft Germany** and Lead Data & AI STU, **mentioned** what he said was **the imminent release of GPT-4.** The fact that **Microsoft is fine-tuning multimodality with OpenAI should no longer have been a secret since the release of Kosmos-1 at the beginning of March.**

[ Dr. Andreas Braun, CTO Microsoft Germany and Lead Data  & AI STU at the Microsoft Digital Kickoff: \\""KI im Fokus\\"" \(AI in  Focus, Screenshot\) \(Bild: Microsoft\) ](https://preview.redd.it/rnst03avarma1.jpg?width=1920&format=pjpg&auto=webp&s=c5992e2d6c6daf32e56a0a3ffeeecfe10621f73f)"
16,machinelearning,openai,top,2023-03-27 04:21:36,[D]GPT-4 might be able to tell you if it hallucinated,Cool_Abbreviations_9,False,0.93,644,123b66w,https://i.redd.it/ocs0x33429qa1.jpg,94,1679890896.0,
17,machinelearning,openai,top,2019-02-15 13:04:39,[Discussion] OpenAI should now change their name to ClosedAI,SirLordDragon,False,0.92,644,aqwcyx,https://www.reddit.com/r/MachineLearning/comments/aqwcyx/discussion_openai_should_now_change_their_name_to/,223,1550235879.0,It's the only way to complete the hype wave.
18,machinelearning,openai,top,2021-01-18 09:08:06,[P] The Big Sleep: Text-to-image generation using BigGAN and OpenAI's CLIP via a Google Colab notebook from Twitter user Adverb,Wiskkey,False,0.99,623,kzr4mg,https://www.reddit.com/r/MachineLearning/comments/kzr4mg/p_the_big_sleep_texttoimage_generation_using/,259,1610960886.0,"From [https://twitter.com/advadnoun/status/1351038053033406468](https://twitter.com/advadnoun/status/1351038053033406468):

>The Big Sleep  
>  
>Here's the notebook for generating images by using CLIP to guide BigGAN.  
>  
>It's very much unstable and a prototype, but it's also a fair place to start. I'll likely update it as time goes on.  
>  
>[colab.research.google.com/drive/1NCceX2mbiKOSlAd\_o7IU7nA9UskKN5WR?usp=sharing](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing)

I am not the developer of The Big Sleep. [This](https://twitter.com/advadnoun/) is the developer's Twitter account; [this](https://www.reddit.com/user/advadnoun) is the developer's Reddit account.

**Steps to follow to generate the first image in a given Google Colab session**:

1. Optionally, if this is your first time using Google Colab, view this [Colab introduction](https://colab.research.google.com/notebooks/intro.ipynb) and/or this [Colab FAQ](https://research.google.com/colaboratory/faq.html).
2. Click [this link](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing).
3. Sign into your Google account if you're not already signed in. Click the ""S"" button in the upper right to do this. Note: Being signed into a Google account has privacy ramifications, such as your Google search history being recorded in your Google account.
4. In the Table of Contents, click ""Parameters"".
5. Find the line that reads ""tx = clip.tokenize('''a cityscape in the style of Van Gogh''')"" and change the text inside of the single quote marks to your desired text; example: ""tx = clip.tokenize('''a photo of New York City''')"". The developer recommends that you keep the three single quote marks on both ends of your desired text so that mult-line text can be used  An alternative is to remove two of the single quotes on each end of your desired text; example: ""tx = clip.tokenize('a photo of New York City')"".
6. In the Table of Contents, click ""Restart the kernel..."".
7. Position the pointer over the first cell in the notebook, which starts with text ""import subprocess"". Click the play button (the triangle) to run the cell. Wait until the cell completes execution.
8. Click menu item ""Runtime->Restart and run all"".
9. In the Table of Contents, click ""Diagnostics"". The output appears near the end of the Train cell that immediately precedes the Diagnostics cell, so scroll up a bit. Every few minutes (or perhaps 10 minutes if Google assigned you relatively slow hardware for this session), a new image will appear in the Train cell that is a refinement of the previous image. This process can go on for as long as you want until Google ends your Google Colab session, which is a total of [up to 12 hours](https://research.google.com/colaboratory/faq.html) for the free version of Google Colab.

**Steps to follow if you want to start a different run using the same Google Colab session:**

1. Click menu item ""Runtime->Interrupt execution"".
2. Save any images that you want to keep by right-clicking on them and using the appropriate context menu command.
3. Optionally, change the desired text. Different runs using the same desired text almost always results in different outputs.
4. Click menu item ""Runtime->Restart and run all"".

**Steps to follow when you're done with your Google Colab session**:

1. Click menu item ""Runtime->Manage sessions"". Click ""Terminate"" to end the session.
2. Optionally, log out of your Google account due to the privacy ramifications of being logged into a Google account.

The first output image in the Train cell (using the notebook's default of seeing every 100th image generated) usually is a very poor match to the desired text, but the second output image often is a decent match to the desired text. To change the default of seeing every 100th image generated, change the number 100 in line ""if itt % 100 == 0:"" in the Train cell to the desired number. **For free-tier Google Colab users, I recommend changing 100 to a small integer such as 5.**

Tips for the text descriptions that you supply:

1. In Section 3.1.4 of OpenAI's [CLIP paper](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf) (pdf), the authors recommend using a text description of the form ""A photo of a {label}."" or ""A photo of a {label}, a type of {type}."" for images that are photographs.
2. A Reddit user gives [these tips](https://www.reddit.com/r/MediaSynthesis/comments/l2hmqn/this_aint_it_chief/gk8g8e9/).
3. The Big Sleep should generate [these 1,000 types of things](https://www.reddit.com/r/MediaSynthesis/comments/l7hbix/tip_for_users_of_the_big_sleep_it_should_on/) better on average than other types of things.

[Here](https://www.digitaltrends.com/news/big-sleep-ai-image-generator/) is an article containing a high-level description of how The Big Sleep works. The Big Sleep uses a modified version of [BigGAN](https://aiweirdness.com/post/182322518157/welcome-to-latent-space) as its image generator component. The Big Sleep uses the ViT-B/32 [CLIP](https://openai.com/blog/clip/) model to rate how well a given image matches your desired text. The best CLIP model according to the CLIP paper authors is the (as of this writing) unreleased ViT-L/14-336px model; see Table 10 on page 40 of the [CLIP paper (pdf)](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf) for a comparison.

There are [many other sites/programs/projects](https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais/) that use CLIP to steer image/video creation to match a text description.

Some relevant subreddits:

1. [r/bigsleep](https://www.reddit.com/r/bigsleep/) (subreddit for images/videos generated from text-to-image machine learning algorithms).
2. [r/deepdream](https://www.reddit.com/r/deepdream/) (subreddit for images/videos generated from machine learning algorithms).
3. [r/mediasynthesis](https://www.reddit.com/r/mediasynthesis/) (subreddit for media generation/manipulation techniques that use artificial intelligence; this subreddit shouldn't be used to post images/videos unless new techniques are demonstrated, or the images/videos are of high quality relative to other posts).

Example using text 'a black cat sleeping on top of a red clock':

https://preview.redd.it/7xq58v7022c61.png?width=512&format=png&auto=webp&s=a229ae9add555cd1caba31c42b60d907ffe67773

Example using text 'the word ''hot'' covered in ice':

https://preview.redd.it/6kxdp8u3k2c61.png?width=512&format=png&auto=webp&s=5bd078b0111575f5d88a1dc53b0aeb933f3b0da6

Example using text 'a monkey holding a green lightsaber':

https://preview.redd.it/rdsybsoaz2c61.png?width=512&format=png&auto=webp&s=2769d4c6c883c1c35ae0b1c629bebe9bc1d41393

Example using text 'The White House in Washington D.C. at night with green and red spotlights shining on it':

https://preview.redd.it/w4mg90xsf5c61.png?width=512&format=png&auto=webp&s=5f18318de2f77bcd8a86e71e87048fadd30383d1

Example using text '''A photo of the Golden Gate Bridge at night, illuminated by spotlights in a tribute to Prince''':

https://preview.redd.it/cn4ecuafhic61.png?width=512&format=png&auto=webp&s=397c838fdc49f13c5f17110b92c78b95bf0dcac0

Example using text '''a Rembrandt-style painting titled ""Robert Plant decides whether to take the stairway to heaven or the ladder to heaven""''':

https://preview.redd.it/h7rb3y6j5jc61.png?width=512&format=png&auto=webp&s=537bfe8210af185647b00e7585c948aa2c4e0ffb

Example using text '''A photo of the Empire State Building being shot at with the laser cannons of a TIE fighter.''':

https://preview.redd.it/cwi7i639c5d61.png?width=512&format=png&auto=webp&s=0510c8b93adb40eee4d3f41607f1c215d41e55ff

Example using text '''A cartoon of a new mascot for the Reddit subreddit DeepDream that has a mouse-like face and wears a cape''':

https://preview.redd.it/wtxbduevcbd61.png?width=512&format=png&auto=webp&s=c5d266258922bc62f25c80a08cd9cabc07d9cb1c

Example using text '''Bugs Bunny meets the Eye of Sauron, drawn in the Looney Tunes cartoon style''':

https://preview.redd.it/gmljaeekuid61.png?width=512&format=png&auto=webp&s=9ea578de165e12afc3a62bf6886bc1ae9dc19bec

Example using text '''Photo of a blue and red neon-colored frog at night.''':

https://preview.redd.it/nzlypte6wzd61.png?width=512&format=png&auto=webp&s=7e10b06f22cfc57c64b6d05738c7486b895083df

Example using text '''Hell begins to freeze over''':

https://preview.redd.it/vn99we9ngmf61.png?width=512&format=png&auto=webp&s=2408efd607f0ab40a08db6ee67448791aa813993

Example using text '''A scene with vibrant colors''':

https://preview.redd.it/4z133mvrgmf61.png?width=512&format=png&auto=webp&s=b78e7a8e3f736769655056093a9904ff09a355a1

Example using text '''The Great Pyramids were turned into prisms by a wizard''':

https://preview.redd.it/zxt6op7vgmf61.png?width=512&format=png&auto=webp&s=53e578cfde14b28afe27957e95e610b89afadd44"
19,machinelearning,openai,top,2023-05-01 16:21:24,[P] SoulsGym - Beating Dark Souls III Bosses with Deep Reinforcement Learning,amacati,False,0.98,590,134r0xf,https://www.reddit.com/r/MachineLearning/comments/134r0xf/p_soulsgym_beating_dark_souls_iii_bosses_with/,74,1682958084.0,"# The project

I've been working on a new gym environment for quite a while, and I think it's finally at a point where I can share it. SoulsGym is an OpenAI gym extension for Dark Souls III. It allows you to train reinforcement learning agents on the bosses in the game. The Souls games are widely known in the video game community for being notoriously hard.

.. Ah, and this is my first post on r/MachineLearning, so please be gentle ;)

# What is included?

**SoulsGym**

There are really two parts to this project. The first one is [SoulsGym](https://github.com/amacati/SoulsGym), an OpenAI gym extension. It is compatible with the newest API changes after gym has transitioned to the Farama foundation. SoulsGym is essentially a game hacking layer that turns Dark Souls III into a gym environment that can be controlled with Python. However, you still need to own the game on Steam and run it before starting the gym. A detailed description on how to set everything up can be found in the package [documentation](https://soulsgym.readthedocs.io/en/latest/?badge=latest).

**Warning: If you want to try this gym, be sure that you have read the documentation and understood everything. If not handled properly, you can get banned from multiplayer.**

Below, you can find a video of an agent training in the game. The game runs on 3x speed to accelerate training. You can also watch the video on [YouTube](https://www.youtube.com/watch?v=7R5Ef69sFPE).

&#x200B;

[RL agent learning to defeat the first boss in Dark Souls III.](https://reddit.com/link/134r0xf/video/o6ctdppeo8xa1/player)

At this point, only the first boss in Dark Souls III is implemented as an environment. Nevertheless, SoulsGym can easily be extended to include other bosses in the game. Due to their similarity, it shouldn't be too hard to even extend the package to Elden Ring as well. If there is any interest in this in the ML/DS community, I'd be happy to give the other ones a shot ;)

**SoulsAI**

The second part is [SoulsAI](https://github.com/amacati/SoulsAI), a distributed deep reinforcement learning framework that I wrote to train on multiple clients simultaneously. You should be able to use it for other gym environments as well, but it was primarily designed for my rather special use case. SoulsAI enables live-monitoring of the current training setup via a webserver, is resilient to client disconnects and crashes, and contains all my training scripts. While this sounds a bit hacky, it's actually quite readable. You can find a complete documentation that goes into how everything works [here](https://soulsai.readthedocs.io/en/latest/).

Being fault tolerant is necessary since the simulator at the heart of SoulsGym is a game that does not expose any APIs and has to be hacked instead. Crashes and other instabilities are rare, but can happen when training over several days. At this moment, SoulsAI implements ApeX style DQN and PPO, but since PPO is synchronous, it is less robust to client crashes etc. Both implementations use Redis as communication backend to send training samples from worker clients to a centralized training server, and to broadcast model updates from the server to all clients. For DQN, SoulsAI is completely asynchronous, so that clients never have to stop playing in order to perform updates or send samples.

&#x200B;

[Live monitoring of an ongoing training process in SoulsAI.](https://preview.redd.it/9m060w00r8xa1.png?width=1800&format=png&auto=webp&s=abb9c15ce38c99cba9753db95ac9dfc7eeec75a5)

Note: I have not implemented more advanced training algorithms such as Rainbow etc., so it's very likely that one can achieve faster convergence with better performance. Furthermore, hyperparameter tuning is extremely challenging since training runs can easily take days across multiple machines.

# Does this actually work?

Yes, it does! It took me some time, but I was able to train an agent with Duelling Double Deep Q-Learning that has a win rate of about 45% within a few days of training. In this video you can see the trained agent playing against Iudex Gundry. You can also watch the video on [YouTube](https://www.youtube.com/watch?v=86NivRglr3Y).

&#x200B;

[RL bot vs Dark Souls III boss.](https://reddit.com/link/134r0xf/video/rkor3hroj8xa1/player)

I'm also working on a visualisation that shows the agent's policy networks reacting to the current game input. You can see a preview without the game simultaneously running here. Credit for the idea of visualisation goes to [Marijn van Vliet](https://github.com/wmvanvliet/scns).

&#x200B;

[Duelling Double Q-Learning networks reacting to changes in the game observations.](https://reddit.com/link/134r0xf/video/b0a4jzczv8xa1/player)

If you really want to dive deep into the hyperparameters that I used or load the trained policies on your machine, you can find the final checkpoints [here](https://drive.google.com/drive/folders/1cAK1TbY4e4HE4cxyAFEHRpj6MOgp5Zxe?usp=sharing). The hyperparameters are contained in the *config.json* file.

# ... But why?

Because it is a ton of fun! Training to defeat a boss in a computer game does not advance the state of the art in RL, sure. So why do it? Well, because we can! And because maybe it excites others about ML/RL/DL.

**Disclaimer: Online multiplayer**

This project is in no way oriented towards creating multiplayer bots. It would take you ages of development and training time to learn a multiplayer AI starting from my package, so just don't even try. I also do not take any precautions against cheat detections, so if you use this package while being online, you'd probably be banned within a few hours.

# Final comments

As you might guess, this project went through many iterations and it took a lot of effort to get it ""right"". I'm kind of proud to have achieved it in the end, and am happy to explain more about how things work if anyone is interested. There is a lot that I haven't covered in this post (it's really just the surface), but you can find more in the docs I linked or by writing me a pm. Also, I really have no idea how many people in ML are also active in the gaming community, but if you are a Souls fan and you want to contribute by adding other Souls games or bosses, feel free to reach out to me.

Edit: Clarified some paragraphs, added note for online multiplayer.

Edit2: Added hyperparameters and network weights."
20,machinelearning,openai,top,2023-03-01 18:31:12,[D] OpenAI introduces ChatGPT and Whisper APIs (ChatGPT API is 1/10th the cost of GPT-3 API),minimaxir,False,0.97,580,11fbccz,https://www.reddit.com/r/MachineLearning/comments/11fbccz/d_openai_introduces_chatgpt_and_whisper_apis/,119,1677695472.0,"https://openai.com/blog/introducing-chatgpt-and-whisper-apis

> It is priced at $0.002 per 1k tokens, which is 10x cheaper than our existing GPT-3.5 models.

This is a massive, massive deal. For context, the reason GPT-3 apps took off over the past few months before ChatGPT went viral is because a) text-davinci-003 was released and was a significant performance increase and b) the cost was cut from $0.06/1k tokens to $0.02/1k tokens, which made consumer applications feasible without a large upfront cost.

A much better model and a 1/10th cost warps the economics completely to the point that it may be better than in-house finetuned LLMs.

I have no idea how OpenAI can make money on this. This has to be a loss-leader to lock out competitors before they even get off the ground."
21,machinelearning,openai,top,2020-01-30 17:11:51,[N] OpenAI Switches to PyTorch,SkiddyX,False,0.99,570,ew8oxq,https://www.reddit.com/r/MachineLearning/comments/ew8oxq/n_openai_switches_to_pytorch/,119,1580404311.0,"""We're standardizing OpenAI's deep learning framework on PyTorch to increase our research productivity at scale on GPUs (and have just released a PyTorch version of Spinning Up in Deep RL)""

https://openai.com/blog/openai-pytorch/"
22,machinelearning,openai,top,2017-08-12 00:10:03,[N] OpenAI bot beat best Dota 2 players in 1v1 at The International 2017,crouching_dragon_420,False,0.96,560,6t58ks,https://blog.openai.com/dota-2/,252,1502496603.0,
23,machinelearning,openai,top,2023-03-11 13:54:22,[Discussion] Compare OpenAI and SentenceTransformer Sentence Embeddings,Simusid,False,0.94,536,11okrni,https://i.redd.it/7muze2s684na1.png,58,1678542862.0,
24,machinelearning,openai,top,2019-07-23 02:29:08,[D] What is OpenAI? I don't know anymore.,milaworld,False,0.95,535,cgmptl,https://www.reddit.com/r/MachineLearning/comments/cgmptl/d_what_is_openai_i_dont_know_anymore/,144,1563848948.0,"*Some [commentary](https://threadreaderapp.com/thread/1153364705777311745.html) from [Smerity](https://twitter.com/Smerity/status/1153364705777311745) about yesterday's [cash infusion](https://openai.com/blog/microsoft/) from MS into OpenAI:*

What is OpenAI? I don't know anymore.
A non-profit that leveraged good will whilst silently giving out equity for [years](https://twitter.com/gdb/status/1105137541970243584) prepping a shift to for-profit that is now seeking to license closed tech through a third party by segmenting tech under a banner of [pre](https://twitter.com/tsimonite/status/1153340994986766336)/post ""AGI"" technology?

The non-profit/for-profit/investor [partnership](https://openai.com/blog/openai-lp/) is held together by a set of legal documents that are entirely novel (=bad term in legal docs), are [non-public](https://twitter.com/gdb/status/1153305526026956800) + unclear, have no case precedence, yet promise to wed operation to a vague (and already re-interpreted) [OpenAI Charter](https://openai.com/charter/).

The claim is that [AGI](https://twitter.com/woj_zaremba/status/1105149945118519296) needs to be carefully and collaboratively guided into existence yet the output of almost [every](https://github.com/facebookresearch) [other](https://github.com/google-research/google-research) [existing](https://github.com/salesforce) [commercial](https://github.com/NVlabs) lab is more open. OpenAI runs a closed ecosystem where they primarily don't or won't trust outside of a small bubble.

I say this knowing many of the people there and with past and present love in my heart—I don't collaborate with OpenAI as I have no freaking clue what they're doing. Their primary form of communication is high entropy blog posts that'd be shock pivots for any normal start-up.

Many of their [blog posts](https://openai.com/blog/cooperation-on-safety/) and [spoken](https://www.youtube.com/watch?v=BJi6N4tDupk) [positions](https://www.youtube.com/watch?v=9EN_HoEk3KY) end up [influencing government policy](https://twitter.com/jackclarkSF/status/986568940028616705) and public opinion on the future of AI through amplified pseudo-credibility due to *Open*, *Musk founded*, repeatedly hyped statements, and a sheen from their now distant non-profit good will era.

I have mentioned this to friends there and say all of this with positive sum intentions: I understand they have lofty aims, I understand they need cash to shovel into the forever unfurling GPU forge, but if they want any community trust long term they need a better strategy.

The implicit OpenAI message heard over the years:
“Think of how transformative and dangerous AGI may be. Terrifying. Trust us. Whether it's black-boxing technology, legal risk, policy initiatives, investor risk, ...—trust us with everything. We're good. No questions, sorry.”

*We'll clarify our position in an upcoming blog post.*"
25,machinelearning,openai,top,2023-01-20 10:41:04,[N] OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic,ChubChubkitty,False,0.83,527,10gtruu,https://www.reddit.com/r/MachineLearning/comments/10gtruu/n_openai_used_kenyan_workers_on_less_than_2_per/,246,1674211264.0,https://time.com/6247678/openai-chatgpt-kenya-workers/
26,machinelearning,openai,top,2020-12-30 20:50:02,[R] A List of Best Papers from Top AI Conferences in 2020,othotr,False,0.97,503,knai5q,https://www.reddit.com/r/MachineLearning/comments/knai5q/r_a_list_of_best_papers_from_top_ai_conferences/,48,1609361402.0,"Sharing a list of award-winning papers from this year's top conferences for anyone interested in catching up on the latest machine learning research before the end of the year :)

**AAAI 2020**

* Best Paper: WinoGrande: An Adversarial Winograd Schema Challenge at Scale \[[Paper](https://arxiv.org/abs/1907.10641)\]
* Honorable Mention: A Unifying View on Individual Bounds and Heuristic Inaccuracies in Bidirectional Search \[[Paper](https://ojs.aaai.org//index.php/AAAI/article/view/5611)\]

**CVPR 2020** 

* Best Paper: Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild \[[Paper](https://arxiv.org/pdf/1911.11130.pdf)\] \[[Presentation](https://crossminds.ai/video/5ee96b86b1267e24b0ec2354/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 

**ACL 2020**

* Best Paper: Beyond Accuracy: Behavioral Testing of NLP Models with CheckList \[[Paper](https://www.aclweb.org/anthology/2020.acl-main.442.pdf)\] \[[Video](https://crossminds.ai/video/5f454437e1acdc4d12c4186e/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 

**ICML 2020**

* Best Paper: On Learning Sets of Symmetric Elements \[[Paper](https://arxiv.org/abs/2002.08599)\]  \[[Presentation](https://icml.cc/virtual/2020/poster/6022)\] 
* Best Paper: Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging Problems \[[Paper](https://arxiv.org/abs/2012.05703)\]  \[[Presentation](https://icml.cc/virtual/2020/poster/6447)\] 
* Honorable Mention: Efficiently sampling functions from Gaussian process posteriors  \[[Paper](https://arxiv.org/abs/2002.09309)\]  \[[Presentation](https://crossminds.ai/video/5f189c96c01f1dd70811ebef/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 
* Honorable Mention: Generative Pretraining From Pixels \[[Paper](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)\]  \[[Presentation](https://crossminds.ai/video/5f0e0b67d8b7c2e383e1077b/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 

**ECCV 2020**

* Best Paper: RAFT: Recurrent All-Pairs Field Transforms for Optical Flow \[[Paper](https://arxiv.org/abs/2003.12039)\] \[[Video](https://crossminds.ai/video/5f5acf7f7fa4bb2ca9d64e4d/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 
* Honorable Mention: Towards Streaming Perception \[[Paper](https://arxiv.org/abs/2005.10420)\] \[[Presentation](https://crossminds.ai/video/5f44390ae1acdc4d12c417e3/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 
* Honorable Mention: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis \[[Paper](https://arxiv.org/abs/2003.08934)\] \[[Presentation](https://crossminds.ai/video/5f3b294f96cfcc9d075e35b6/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 

**ICRA 2020**

* Best Paper: Preference-Based Learning for Exoskeleton Gait Optimization \[[Paper](https://arxiv.org/abs/1909.12316)\] \[[Presentation](https://crossminds.ai/video/5f65488303c0894581947a6b/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 
* Best Paper in Robot Vision: Graduated Non-Convexity for Robust Spatial Perception: From Non-Minimal Solvers to Global Outlier Rejection \[[Paper](https://arxiv.org/abs/1909.08605)\] \[[Presentation](https://crossminds.ai/video/5f63f6c403c089458194705f/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 

**CoRL 2020**

* Best Paper: Learning Latent Representations to Influence Multi-Agent Interaction \[[Paper](https://arxiv.org/abs/2011.06619)\] \[[Presentation](https://crossminds.ai/video/5fd9782a08be4fa7f41eabfe/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 
* Best Paper Presentation: Accelerating Reinforcement Learning with Learned Skill Priors \[[Paper](https://arxiv.org/abs/2010.11944)\] \[[Presentation](https://crossminds.ai/video/5fd9794308be4fa7f41eac54/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 
* Best System Paper: SMARTS: An Open-Source Scalable Multi-Agent RL Training School for Autonomous Driving \[[Paper](https://arxiv.org/abs/2010.09776)\] \[[Presentation](https://crossminds.ai/video/5fd9791f08be4fa7f41eac48/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 

**RecSys 2020**

* Best Long Paper: Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations \[[Paper](https://github.com/guyulongcs/Awesome-Deep-Learning-Papers-for-Search-Recommendation-Advertising/blob/master/0_New_Papers_in_2020/2020%20%28Tencent%29%20%28Recsys%29%20%5BPLE%5D%20Progressive%20Layered%20Extraction%20%28PLE%29%20-%20A%20Novel%20Multi-Task%20Learning%20%28MTL%29%20Model%20for%20Personalized%20Recommendations.pdf)\] \[[Presentation](https://crossminds.ai/video/5f7fc247d81cf36f1a8e379c/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 
* Best Short Paper: ADER: Adaptively Distilled Exemplar Replay Towards Continual Learning for Session-based Recommendation \[[Paper](https://arxiv.org/abs/2007.12000)\] \[[Presentation](https://crossminds.ai/video/5f7fc27ad81cf36f1a8e37b6/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 

**NeurIPS 2020**

* Best Paper: Language Models are Few-Shot Learners \[[Paper](https://arxiv.org/abs/2005.14165)\] \[[Video](https://crossminds.ai/video/5f3179536d7639fd8a7fc06a/?playlist_id=5fe2e2ea56dab51eaff52eaf)\] 
* Best Paper: No-Regret Learning Dynamics for Extensive-Form Correlated Equilibrium \[[Paper](https://arxiv.org/abs/2004.00603)\] 
* Best Paper: Improved Guarantees and a Multiple-Descent Curve for Column Subset Selection and the Nyström Method \[[Paper](https://arxiv.org/abs/2002.09073)\]

Here is a comprehensive collection of [research talks from all major AI conferences](https://crossminds.ai/c/conference/) this year if you'd like to explore further."
27,machinelearning,openai,top,2017-08-01 10:23:32,[D] Where does this hyped news come from? *Facebook shut down AI that invented its own language.*,nomaderx,False,0.92,482,6qvbu8,https://www.reddit.com/r/MachineLearning/comments/6qvbu8/d_where_does_this_hyped_news_come_from_facebook/,187,1501583012.0,"My Facebook wall is full of people sharing this story that Facebook *had* to shut down an AI system it developed that invented it's own language. Here are some of these articles:

[Independent: Facebook's AI robots shut down after they start talking to each other in their own language](http://www.independent.co.uk/life-style/gadgets-and-tech/news/facebook-artificial-intelligence-ai-chatbot-new-language-research-openai-google-a7869706.html)

[BGR: Facebook engineers panic, pull plug on AI after bots develop their own language](http://bgr.com/2017/07/31/facebook-ai-shutdown-language/)

[Forbes: Facebook AI Creates Its Own Language In Creepy Preview Of Our Potential Future](https://www.forbes.com/sites/tonybradley/2017/07/31/facebook-ai-creates-its-own-language-in-creepy-preview-of-our-potential-future/#192e0e29292c)

[Digital Journal: Researchers shut down AI that invented its own language](http://www.digitaljournal.com/tech-and-science/technology/a-step-closer-to-skynet-ai-invents-a-language-humans-can-t-read/article/498142)

EDIT#3: [FastCoDesign: AI Is Inventing Languages Humans Can’t Understand. Should We Stop It?](https://www.fastcodesign.com/90132632/ai-is-inventing-its-own-perfect-languages-should-we-let-it) [Likely the first article]

Note that this is related to the work in the *Deal or No Deal? End-to-End Learning for Negotiation Dialogues* paper. On it's own, it is interesting work.

While the article from Independent seems to be the only one that finally gives the clarification *'The company chose to shut down the chats because ""our interest was having bots who could talk to people""'*, **ALL** the articles say things that suggest that researchers went into panic mode, had to 'pull the plug' out of fear, this stuff is scary. One of the articles (don't remember which) even went on to say something like *'A week after Elon Musk suggested AI needs to be regulated and Mark Zuckerberg disagreed, Facebook had to shut down it's AI because it became too dangerous/scary'* (or something to this effect).

While I understand the hype around deep learning (a.k.a backpropaganda), etc., I think these articles are so ridiculous. I wouldn't even call this hype, but almost 'fake news'. I understand that sometimes articles should try to make the news more interesting/appealing by hyping it a bit, but this is almost detrimental, and is just promoting AI fear-mongering. 

EDIT#1: Some people on Facebook are actually believing this fear to be real, sending me links and asking me about it. :/

EDIT#2: As pointed out in the comments, there's also this opposite article:

[Gizmodo: No, Facebook Did Not Panic and Shut Down an AI Program That Was Getting Dangerously Smart](http://gizmodo.com/no-facebook-did-not-panic-and-shut-down-an-ai-program-1797414922)

EDIT#4: And now, BBC joins in to clear the air as well:

[BBC: The 'creepy Facebook AI' story that captivated the media](http://www.bbc.com/news/technology-40790258)

Opinions/comments?  "
28,machinelearning,openai,top,2022-09-28 16:37:01,[D] DALL·E Now Available Without Waitlist,minimaxir,False,0.96,452,xqhho8,https://www.reddit.com/r/MachineLearning/comments/xqhho8/d_dalle_now_available_without_waitlist/,65,1664383021.0,"https://openai.com/blog/dall-e-now-available-without-waitlist/

It appears to work as advertised, not any special workflow. (as a bonus, it does work with organizations too, with credits shared)"
29,machinelearning,openai,top,2023-03-23 18:09:11,[N] ChatGPT plugins,Singularian2501,False,0.97,441,11zsdwv,https://www.reddit.com/r/MachineLearning/comments/11zsdwv/n_chatgpt_plugins/,144,1679594951.0,"[https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins)

>We’ve implemented initial support for plugins in ChatGPT. Plugins are tools designed specifically for language models with safety as a core principle, and help ChatGPT access up-to-date information, run  computations, or use third-party services."
30,machinelearning,openai,top,2020-04-30 17:00:10,"[R] OpenAI opensources Jukebox, a neural net that generates music",gohu_cd,False,0.97,434,gazkh7,https://www.reddit.com/r/MachineLearning/comments/gazkh7/r_openai_opensources_jukebox_a_neural_net_that/,85,1588266010.0,"Provided with genre, artist, and lyrics as input, Jukebox outputs a new music sample produced from scratch.

[https://openai.com/blog/jukebox/](https://openai.com/blog/jukebox/)

[https://jukebox.openai.com](https://jukebox.openai.com/)

The model behind this tool is VQ-VAE."
31,machinelearning,openai,top,2023-12-20 13:59:53,[D] Mistral received funding and is worth billions now. Are open source LLMs the future?,BelowaverageReggie34,False,0.96,440,18mv8le,https://www.reddit.com/r/MachineLearning/comments/18mv8le/d_mistral_received_funding_and_is_worth_billions/,156,1703080793.0," Came across this intriguing [article](https://gizmodo.com/mistral-artificial-intelligence-gpt-3-openai-1851091217) about Mistral, an open-source LLM that recently scored 400 million in funding, now valued at 2 billion. Are open-source LLMs gonna be the future? Considering the trust issues with ChatGPT and the debates about its safety, the idea of open-source LLMs seems to be the best bet imo.

Unlike closed-source models, users can verify the privacy claims of open-source models. There have been some good things being said about Mistral, and I only hope such open source LLMs secure enough funding to compete with giants like OpenAI. Maybe then, ChatGPT will also be forced to go open source?

With that said, I'm also hopeful that competitors like [Silatus](https://silatus.com/) and [Durable](https://durable.co/), which already use multiple models, consider using open-source models like Mistral into their frameworks. If that happens, maybe there might be a shift in AI privacy. What do you guys think? Are open-source LLMs the future, especially with the funding backing them?"
32,machinelearning,openai,top,2022-08-22 21:00:01,[D] StableDiffusion v1.4 is entirely public. What do you think about Stability.ai ?,dasayan05,False,0.98,434,wv50uh,https://www.reddit.com/r/MachineLearning/comments/wv50uh/d_stablediffusion_v14_is_entirely_public_what_do/,123,1661202001.0,"In case you haven't noticed, [stability.ai](https://stability.ai) just open-sourced their latest version of StableDiffusion to the public. Here is the link: [https://stability.ai/blog/stable-diffusion-public-release](https://stability.ai/blog/stable-diffusion-public-release)

It is so fast and small (memory footprint) that it can run on consumer grade GPUs. I just generated my first ""astronaut riding a horse on mars"" on my local GTX3090.

[Astronaut riding a horse on mars](https://preview.redd.it/jpceq4klwbj91.png?width=512&format=png&auto=webp&s=b84b7c1cf7e09fdcf326145e5d17485c9376ffb4)

So what is opinion on open-sourcing such powerful models ? And, what do you think about [stability.ai](https://stability.ai) as an organisation ? Do you feel they can potentially be the next OpenAI ?"
33,machinelearning,openai,top,2022-11-03 23:12:45,"[D] DALL·E to be made available as API, OpenAI to give users full ownership rights to generated images",TiredOldCrow,False,0.98,423,yli0r7,https://www.reddit.com/r/MachineLearning/comments/yli0r7/d_dalle_to_be_made_available_as_api_openai_to/,55,1667517165.0,"Email announcement from OpenAI below:


> DALL·E is now available as an API


> You can now integrate state of the art image generation capabilities directly into your apps and products through our new DALL·E API.


> You own the generations you create with DALL·E.


> We’ve simplified our [Terms of Use](https://openai.com/api/policies/terms/) and you now have full ownership rights to the images you create with DALL·E — in addition to the usage rights you’ve already had to use and monetize your creations however you’d like. This update is possible due to improvements to our safety systems which minimize the ability to generate content that violates our content policy.


> Sort and showcase with collections.


> You can now organize your DALL·E creations in multiple collections. Share them publicly or keep them private. Check out our [sea otter collection](https://labs.openai.com/sc/w3Q8nqVN69qkEA3ePSmrGb5t)!


> We’re constantly amazed by the innovative ways you use DALL·E and love seeing your creations out in the world. Artists who would like their work to be shared on our Instagram can request to be featured using Instagram’s collab tool. DM us there to show off how you’re using the API!  

> \- The OpenAI Team"
34,machinelearning,openai,top,2022-04-02 09:36:21,[P] OpenAI Codex helping to write shell commands,tomd_96,False,0.95,417,tuf0vv,https://i.redd.it/dbgbskqg53r81.gif,12,1648892181.0,
35,machinelearning,openai,top,2023-11-17 21:12:49,"[N] OpenAI Announces Leadership Transition, Fires Sam Altman",Sm0oth_kriminal,False,0.94,419,17xp85q,https://www.reddit.com/r/MachineLearning/comments/17xp85q/n_openai_announces_leadership_transition_fires/,199,1700255569.0,"EDIT: Greg Brockman has quit as well: https://x.com/gdb/status/1725667410387378559?s=46&t=1GtNUIU6ETMu4OV8_0O5eA

Source: https://openai.com/blog/openai-announces-leadership-transition

Today, it was announced that Sam Altman will no longer be CEO or affiliated with OpenAI due to a lack of “candidness” with the board. This is extremely unexpected as Sam Altman is arguably the most recognizable face of state of the art AI (of course, wouldn’t be possible without great team at OpenAI). Lots of speculation is in the air, but there clearly must have been some good reason to make such a drastic decision.

This may or may not materially affect ML research, but it is plausible that the lack of “candidness” is related to copyright data, or usage of data sources that could land OpenAI in hot water with regulatory scrutiny. Recent lawsuits (https://www.reuters.com/legal/litigation/writers-suing-openai-fire-back-companys-copyright-defense-2023-09-28/) have raised questions about both the morality and legality of how OpenAI and other research groups train LLMs.

Of course we may never know the true reasons behind this action, but what does this mean for the future of AI?"
36,machinelearning,openai,top,2023-11-20 08:50:54,"[N] Sam Altman and Greg Brockman, together with colleagues, will join Microsoft to lead new advanced AI research team",Civil_Collection7267,False,0.96,409,17zk6zy,https://www.reddit.com/r/MachineLearning/comments/17zk6zy/n_sam_altman_and_greg_brockman_together_with/,178,1700470254.0,"Source: [https://blogs.microsoft.com/blog/2023/11/19/a-statement-from-microsoft-chairman-and-ceo-satya-nadella/](https://blogs.microsoft.com/blog/2023/11/19/a-statement-from-microsoft-chairman-and-ceo-satya-nadella/)

>We remain committed to our partnership with OpenAI and have confidence in our product roadmap, our ability to continue to innovate with everything we announced at Microsoft Ignite, and in continuing to support our customers and partners. We look forward to getting to know Emmett Shear and OAI’s new leadership team and working with them. And we’re extremely excited to share the news that Sam Altman and Greg Brockman, together with colleagues, will be joining Microsoft to lead a new advanced AI research team. We look forward to moving quickly to provide them with the resources needed for their success.

News article covering the situation: [https://www.theverge.com/2023/11/20/23968829/microsoft-hires-sam-altman-greg-brockman-employees-openai](https://www.theverge.com/2023/11/20/23968829/microsoft-hires-sam-altman-greg-brockman-employees-openai)

>Altman’s Microsoft hiring comes just hours after negotiations with OpenAI’s board failed to bring him back as OpenAI CEO. Instead, former Twitch CEO and co-founder Emmett Shear has been named as interim CEO.  
>  
>Altman had been negotiating to return as OpenAI CEO, but OpenAI’s four-person board refused to step down and let him return."
37,machinelearning,openai,top,2023-05-07 14:12:18,[P] I made a dashboard to analyze OpenAI API usage,cryptotrendz,False,0.91,408,13aotyf,https://v.redd.it/w7ahlql0ccya1,73,1683468738.0,
38,machinelearning,openai,top,2019-04-25 17:07:04,[N] MuseNet by OpenAI,wavelander,False,0.96,411,bhb4ds,https://openai.com/blog/musenet/,48,1556212024.0,
39,machinelearning,openai,top,2016-01-09 04:01:47,AMA: the OpenAI Research Team,IlyaSutskever,False,0.98,395,404r9m,https://www.reddit.com/r/MachineLearning/comments/404r9m/ama_the_openai_research_team/,285,1452312107.0,"The OpenAI research team will be answering your questions.

We are (our usernames are):  Andrej Karpathy (badmephisto), Durk Kingma (dpkingma), Greg Brockman (thegdb), Ilya Sutskever (IlyaSutskever), John Schulman (johnschulman), Vicki Cheung (vicki-openai), Wojciech Zaremba (wojzaremba).


Looking forward to your questions! "
40,machinelearning,openai,top,2021-04-23 14:25:29,[D] Your Favorite AI Podcasts / Blogs / Newsletters / YouTube Channels?,regalalgorithm,False,0.96,396,mwwftu,https://www.reddit.com/r/MachineLearning/comments/mwwftu/d_your_favorite_ai_podcasts_blogs_newsletters/,90,1619187929.0,"Hi there, I want to write a little blog post summarizing different ways of keeping up with AI by way of Podcasts / Blogs / Newsletters / YouTube Channels. Yeah there are a million of these, but most are not so well curated, miss a lot of stuff, and are not up to date. Criteria: still active, focused primarily on AI, high quality.

Here's what I have so far, would appreciate if you can suggest any additions!

* **Podcasts**
   * [**Machine Learning Street Talk**](https://www.youtube.com/channel/UCMLtBahI5DMrt0NPvDSoIRQ)
   * **Lex Fridman (mainly first \~150 eps)**
   * **Gigaom Voices in AI**
   * **Data Skeptic**
   * **Eye on AI**
   * **Gradient Dissent**
   * **Robot Brains**
   * **RE Work podcast**
   * **AI Today Podcast**
   * **Chat Time Data Science**
   * **Let’s Talk AI**
   * **In Machines We Trust**
* **Publications**
   * **The Gradient**
   * **Towards Data Science**
   * **Analytics Vidhya**
   * **Distill**
* **Personal Blogs**
   * [**Lil’Log**](https://lilianweng.github.io/lil-log/)
   * **Gwern**
   * **Sebastian Ruder**
   * **Alex Irpan**
   * **Chris Olah**
   * **Democratizing Automation**
   * **Approximately Correct**
   * **Off the Convex Path**
   * **Arg min blog**
   * **I’m a bandit**
* **Academic Blogs**
   * **SAIL Blog**
   * **Berkeley AI Blog**
   * **Machine Learning at Berkeley Blog**
   * **CMU ML Blog**
   * **ML MIT**
   * **ML Georgia Tech**
   * **Google / Facebook / Salesforce / Microsoft / Baidu / OpenAI /  DeepMind** 
* **Journalists**
   * **Karen Hao** 
   * **Cade Metz**
   * **Will Knight**
   * **Khari Johnson**
* **Newsletters**
   * **Last Week in AI**
   * **Batch.AI**
   * **Sebasting Ruder**
   * **Artificial Intelligence Weekly News**
   * **Wired AI newsletter**
   * **Papers with Code**
   * **The Algorithm**
   * **AI Weekly**
   * **Weekly Robotics**
   * **Import AI**
   * **Deep Learning Weekly**
   * **H+ Weekly**
   * **ChinAI Newsletter**
   * **THe EuropeanAI Newsletter**

**Youtube Channels**

* **Talks**
   * [**Amii Intelligence**](https://www.youtube.com/channel/UCxxisInVr7upxv1yUhSgdBA)
   * [**CMU AI Seminar**](https://www.youtube.com/channel/UCLh3OUmBGe4wPyVZiI771ng)
   * [**Robotics Institute Seminar Series**](https://www.youtube.com/playlist?list=PLCFD85BC79FE703DF)
   * [**Machine Learning Center at Georgia Tech**](https://www.youtube.com/channel/UCugI4c0S6-yVi9KfdkDU0aw/videos)
   * [**Robotics Today**](https://www.youtube.com/channel/UCtfiXX2nJ5Qz-ZxGEwDCy5A)
   * [**Stanford MLSys Seminars**](https://www.youtube.com/channel/UCzz6ructab1U44QPI3HpZEQ)
   * [**MIT Embodied Intelligence**](https://www.youtube.com/channel/UCnXGbvgu9071i3koFooncAw)
* **Interviews**
   * **See podcasts**
* **Paper Summaries** 
   * [**AI Coffee Break with Letitia**](https://www.youtube.com/c/AICoffeeBreak/featured)
   * [**Henry AI Labs**](https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw)
   * [**Yannic Kilcher**](https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew)
   * **Arxiv Insights**
* **Lessons**
   * [**3Blue1Brown**](https://www.youtube.com/c/3blue1brown/featured)
   * [**Jordan Harrod**](https://www.youtube.com/channel/UC1H1NWNTG2Xi3pt85ykVSHA)
   * [**vcubingx**](https://www.youtube.com/channel/UCv0nF8zWevEsSVcmz6mlw6A)
   * [**Leo Isikdogan**](https://www.youtube.com/channel/UC-YAxUbpa1hvRyfJBKFNcJA)
* **Demos**
   * [**bycloud**](https://www.youtube.com/channel/UCgfe2ooZD3VJPB6aJAnuQng)
   * [**Two Minute Papers**](https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg)
   * [**Code Bullet**](https://www.youtube.com/channel/UC0e3QhIYukixgh5VVpKHH9Q)
   * [**What's AI**](https://www.youtube.com/c/WhatsAI/videos)"
41,machinelearning,openai,top,2021-02-25 00:31:22,[N] OpenAI has released the encoder and decoder for the discrete VAE used for DALL-E,Wiskkey,False,0.97,394,lrroom,https://www.reddit.com/r/MachineLearning/comments/lrroom/n_openai_has_released_the_encoder_and_decoder_for/,69,1614213082.0,"Background info: [OpenAI's DALL-E blog post](https://openai.com/blog/dall-e/).

Repo: [https://github.com/openai/DALL-E](https://github.com/openai/DALL-E).

[Google Colab notebook](https://colab.research.google.com/github/openai/DALL-E/blob/master/notebooks/usage.ipynb).

Add this line as the first line of the Colab notebook:

    !pip install git+https://github.com/openai/DALL-E.git

I'm not an expert in this area, but nonetheless I'll try to provide more context about what was released today. This is one of the components of DALL-E, but not the entirety of DALL-E. This is the DALL-E component that generates 256x256 pixel images from a [32x32 grid of numbers, each with 8192 possible values](https://www.reddit.com/r/MachineLearning/comments/kr63ot/r_new_paper_from_openai_dalle_creating_images/gi8wy8q/) (and vice-versa). What we don't have for DALL-E is the language model that takes as input text (and optionally part of an image) and returns as output the 32x32 grid of numbers.

I have 3 non-cherry-picked examples of image decoding/encoding using the Colab notebook at [this post](https://www.reddit.com/r/MediaSynthesis/comments/lroigk/for_developers_openai_has_released_the_encoder/).

**Update**: The [DALL-E paper](https://www.reddit.com/r/MachineLearning/comments/lrx40h/r_openai_has_released_the_paper_associated_with/) was released after I created this post.

**Update**: A Google Colab notebook using this DALL-E component has already been released: [Text-to-image Google Colab notebook ""Aleph-Image: CLIPxDAll-E"" has been released. This notebook uses OpenAI's CLIP neural network to steer OpenAI's DALL-E image generator to try to match a given text description.](https://www.reddit.com/r/MachineLearning/comments/ls0e0f/p_texttoimage_google_colab_notebook_alephimage/)"
42,machinelearning,openai,top,2017-06-21 00:41:00,[N] Andrej Karpathy leaves OpenAI for Tesla ('Director of AI and Autopilot Vision'),gwern,False,0.93,393,6iib9r,https://techcrunch.com/2017/06/20/tesla-hires-deep-learning-expert-andrej-karpathy-to-lead-autopilot-vision/?,98,1498005660.0,
43,machinelearning,openai,top,2021-05-26 17:31:34,[N] OpenAI announces OpenAI Startup Fund investing $100 million into AI startups,minimaxir,False,0.97,389,nlmlbg,https://www.reddit.com/r/MachineLearning/comments/nlmlbg/n_openai_announces_openai_startup_fund_investing/,39,1622050294.0,"https://openai.com/fund/
https://techcrunch.com/2021/05/26/openais-100m-startup-fund-will-make-big-early-bets-with-microsoft-as-partner/

It does not appear to be explicitly GPT-3 related (any type of AI is accepted), but hints very heavily toward favoring applications using it."
44,machinelearning,openai,top,2024-02-15 18:39:06,[D] OpenAI Sora Video Gen -- How??,htrp,False,0.96,383,1armmng,https://www.reddit.com/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/,199,1708022346.0,">Introducing Sora, our text-to-video model. Sora can generate videos up to a minute long while maintaining visual quality and adherence to the user’s prompt.




https://openai.com/sora

Research Notes
Sora is a diffusion model, which generates a video by starting off with one that looks like static noise and gradually transforms it by removing the noise over many steps.

Sora is capable of generating entire videos all at once or extending generated videos to make them longer. By giving the model foresight of many frames at a time, we’ve solved a challenging problem of making sure a subject stays the same even when it goes out of view temporarily.

Similar to GPT models, Sora uses a transformer architecture, unlocking superior scaling performance.

We represent videos and images as collections of smaller units of data called patches, each of which is akin to a token in GPT. By unifying how we represent data, we can train diffusion transformers on a wider range of visual data than was possible before, spanning different durations, resolutions and aspect ratios.

Sora builds on past research in DALL·E and GPT models. It uses the recaptioning technique from DALL·E 3, which involves generating highly descriptive captions for the visual training data. As a result, the model is able to follow the user’s text instructions in the generated video more faithfully.

In addition to being able to generate a video solely from text instructions, the model is able to take an existing still image and generate a video from it, animating the image’s contents with accuracy and attention to small detail. The model can also take an existing video and extend it or fill in missing frames. Learn more in our technical paper (coming later today).

Sora serves as a foundation for models that can understand and simulate the real world, a capability we believe will be an important milestone for achieving AGI.



Example Video: https://cdn.openai.com/sora/videos/cat-on-bed.mp4

Tech paper will be released later today. But brainstorming how?"
45,machinelearning,openai,top,2023-11-23 00:14:50,[D] Exclusive: Sam Altman's ouster at OpenAI was precipitated by letter to board about AI breakthrough,blabboy,False,0.83,374,181o1q4,https://www.reddit.com/r/MachineLearning/comments/181o1q4/d_exclusive_sam_altmans_ouster_at_openai_was/,180,1700698490.0,"According to one of the sources, long-time executive Mira Murati told employees on Wednesday that a letter about the AI breakthrough called Q* (pronounced Q-Star), precipitated the board's actions.

The maker of ChatGPT had made progress on Q*, which some internally believe could be a breakthrough in the startup's search for superintelligence, also known as artificial general intelligence (AGI), one of the people told Reuters. OpenAI defines AGI as AI systems that are smarter than humans.

https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/"
46,machinelearning,openai,top,2022-10-26 06:10:48,"[P] Up to 12X faster GPU inference on Bert, T5 and other transformers with OpenAI Triton kernels",pommedeterresautee,False,0.99,370,ydqmjp,https://www.reddit.com/r/MachineLearning/comments/ydqmjp/p_up_to_12x_faster_gpu_inference_on_bert_t5_and/,46,1666764648.0,"We are releasing [Kernl](https://github.com/ELS-RD/kernl/) under Apache 2 license, a library to make PyTorch models inference significantly faster. With 1 line of code we applied the optimizations and made Bert up to 12X faster than Hugging Face baseline. T5 is also covered in this first release (> 6X speed up generation and we are still halfway in the optimizations!). This has been possible because we wrote custom GPU kernels with the new OpenAI programming language Triton and leveraged TorchDynamo.

**Project link**: [https://github.com/ELS-RD/kernl/](https://github.com/ELS-RD/kernl/)

**E2E demo notebooks**: [XNLI classification](https://github.com/ELS-RD/kernl/blob/main/tutorial/bert%20e2e.ipynb), [T5 generation](https://github.com/ELS-RD/kernl/blob/main/tutorial/t5%20e2e.ipynb)

[Benchmarks ran on a 3090 RTX GPU, 12 cores Intel CPU, more info below](https://preview.redd.it/mlo3wvn0d3w91.png?width=2738&format=png&auto=webp&s=1b9dce736ee4c0e371b54b9ef796310f9728660d)

On long sequence length inputs, [Kernl](https://github.com/ELS-RD/kernl/) is most of the time the fastest inference engine, and close to Nvidia TensorRT on shortest ones. Keep in mind that Bert is one of the most optimized models out there and most of the tools listed above are very mature.

What is interesting is not that [Kernl](https://github.com/ELS-RD/kernl/) is the fastest engine (or not), but that the code of the kernels is short and easy to understand and modify. We have even added a Triton debugger and a tool (based on Fx) to ease kernel replacement so there is no need to modify PyTorch model source code.

Staying in the comfort of PyTorch / Python maintains dynamic behaviors, debugging and iteration speed. Teams designing/training a transformer model (even custom) can take care of the deployment without relying on advanced GPU knowledge (eg. CUDA programming, dedicated inference engine API, etc.).

Recently released models relying on slightly modified transformer architectures are rarely accelerated in traditional inference engines, we need to wait months to years for someone (usually inference engine maintainers) to write required custom CUDA kernels. Because here custom kernels are written in OpenAI Triton language, **anyone without CUDA experience** can easily modify them: OpenAI Triton API is simple and close to Numpy one. Kernels source code is significantly shorter than equivalent implementation in CUDA (< 200 LoC per kernel). Basic knowledge of how GPU works is enough. We are also releasing a few tutorials we initially wrote for onboarding colleagues on the project. We hope you will find them useful: [https://github.com/ELS-RD/kernl/tree/main/tutorial](https://github.com/ELS-RD/kernl/tree/main/tutorial). In particular, there is:

* Tiled matmul, the GPU way to perform matmul: [https://github.com/ELS-RD/kernl/blob/main/tutorial/1%20-%20tiled%20matmul.ipynb](https://github.com/ELS-RD/kernl/blob/main/tutorial/1%20-%20tiled%20matmul.ipynb)
* Simple explanation of what Flash attention is and how it works, a fused attention making long sequences much faster: [https://github.com/ELS-RD/kernl/blob/main/tutorial/4%20-%20flash%20attention.ipynb](https://github.com/ELS-RD/kernl/blob/main/tutorial/4%20-%20flash%20attention.ipynb)

And best of the best, because we stay in the PyTorch / Python ecosystem, we plan in our roadmap to also enable **training** with those custom kernels. In particular [Flash attention](https://github.com/HazyResearch/flash-attention) kernel should bring a 2-4X speed up and the support of very long sequences on single GPU (paper authors went as far as 16K tokens instead of traditional 512 or 2048 limits)! See below for more info.

**IMPORTANT**: Benchmarking is a difficult art, we tried to be as fair as possible. Please note that:

* Timings are based on wall-clock times and we show speedup over baseline as they are easier to compare between input shapes,
* When we need to choose between speed and output precision, we always choose precision
* HF baseline, CUDA graphs, Inductor and [Kernl](https://github.com/ELS-RD/kernl/) are in mixed precision, AITemplate, ONNX Runtime, DeepSpeed and TensorRT have their weights converted to FP16.
* Accumulation is done in FP32 for AITemplate and [Kernl](https://github.com/ELS-RD/kernl/). TensorRT is likely doing it in FP16.
* CUDA graphs is enabled for all engines except baseline, Nvfuser and ONNX Runtime which [has a limited support of it](https://github.com/microsoft/onnxruntime/issues/12977#issuecomment-1258406358).
* For [Kernl](https://github.com/ELS-RD/kernl/) and AITemplate, fast GELU has been manually disabled (TensorRT is likely using Fast GELU).
* AITemplate measures are to be taken with a grain of salt, it [doesn’t manage attention mask](https://github.com/facebookincubator/AITemplate/issues/46#issuecomment-1279975463) which means 1/ batch inference can’t be used in most scenarios (no padding support), 2/ it misses few operations on a kernel that can be compute-bounded (depends of sequence length), said otherwise it may make it slower to support attention mask, in particular on long sequences. AITemplate attention mask support will come in a future release.
* For TensorRT for best perf, we built 3 models, one per batch size. AITemplate will support dynamic shapes in a future release, so we made a model per input shape.
* Inductor is in prototype stage, performances may be improved when released, none of the disabled by default optimizations worked during our tests.

As you can see, CUDA graphs erase all CPU overhead (Python related for instance), sometimes there is no need to rely on C++/Rust to be fast! Fused kernels (in CUDA or Triton) are mostly important for longer input sequence lengths. We are aware that there are still some low hanging fruits to improve [Kernl](https://github.com/ELS-RD/kernl/) performance without sacrificing output precision, it’s just the first release. More info about how it works [here](https://github.com/ELS-RD/kernl#how).

**Why?**

We work for Lefebvre Sarrut, a leading European legal publisher. Several of our products include transformer models in latency sensitive scenarios (search, content recommendation). So far, ONNX Runtime and TensorRT served us well, and we learned interesting patterns along the way that we shared with the community through an open-source library called [transformer-deploy](https://github.com/ELS-RD/transformer-deploy). However, recent changes in our environment made our needs evolve:

* New teams in the group are deploying transformer models in prod directly with PyTorch. ONNX Runtime poses them too many challenges (like debugging precision issues in fp16). With its inference expert-oriented API, TensorRT was not even an option;
* We are exploring applications of large generative language models in legal industry, and we need easier dynamic behavior support plus more efficient quantization, our creative approaches for that purpose we shared [here on Reddit](https://www.reddit.com/r/MachineLearning/comments/uwkpmt/p_what_we_learned_by_making_t5large_2x_faster/) proved to be more fragile than we initially thought;
* New business opportunities if we were able to train models supporting large contexts (>5K tokens)

On a more personal note, I enjoyed much more writing kernels and understanding low level computation of transformers than mastering multiple complicated tools API and their environments. It really changed my intuitions and understanding about how the model works, scales, etc. It’s not just OpenAI Triton, we also did some prototyping on C++ / CUDA / Cutlass and the effect was the same, it’s all about digging to a lower level. And still the effort is IMO quite limited regarding the benefits. If you have some interest in machine learning engineering, you should probably give those tools a try.

**Future?**

Our road map includes the following elements (in no particular order):

* Faster warmup
* Ragged inference (no computation lost in padding)
* Training support (with long sequences support)
* Multi GPU (multiple parallelization schemas support)
* Quantization (PTQ)
* New batch of Cutlass kernels tests
* Improve hardware support (>= Ampere for now)
* More tuto

Regarding training, if you want to help, we have written an issue with all the required pointers, it should be very doable: [https://github.com/ELS-RD/kernl/issues/93](https://github.com/ELS-RD/kernl/issues/93)

On top of speed, one of the main benefits is the support of very long sequences (16K tokens without changing attention formula) as it’s based on [Flash Attention](https://github.com/HazyResearch/flash-attention).

Also, note that future version of PyTorch will include [Inductor](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747). It means that all PyTorch users will have the option to compile to Triton to get around [1.7X faster training](https://dev-discuss.pytorch.org/t/torchinductor-update-3-e2e-model-training-with-torchdynamo-inductor-gets-1-67x-2-1x-speedup/793).

A big thank you to Nvidia people who advised us during this project."
47,machinelearning,openai,top,2020-02-18 00:19:40,"[D] The messy, secretive reality behind OpenAI’s bid to save the world",milaworld,False,0.93,365,f5immz,https://www.reddit.com/r/MachineLearning/comments/f5immz/d_the_messy_secretive_reality_behind_openais_bid/,143,1581985180.0,"A new [story](https://www.technologyreview.com/s/615181/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/) by journalist [Karen Hao](https://mobile.twitter.com/_KarenHao/status/1229519114638589953) who spent six months digging into OpenAI.

She started with a few simple questions: Who are they? What are their goals? How do they work? After nearly three dozen interviews, she found so much more.

The article is worth a read. I'm not going to post an excerpt here.

The most surprising thing is that Elon Musk himself, after that article got published, [criticized](https://www.twitter.com/elonmusk/status/1229544673590599681) OpenAI and tweeted that they ""should be more open"" 🔥

With regards to AI safety, Elon [said](https://www.twitter.com/elonmusk/status/1229546206948462597) ""I have no control & only very limited insight into OpenAI. Confidence in Dario for safety is not high.""

Here is the link to the article again: https://www.technologyreview.com/s/615181/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/"
48,machinelearning,openai,top,2021-06-01 17:40:23,"[R] Chinese AI lab challenges Google, OpenAI with a model of 1.75 trillion parameters",liqui_date_me,False,0.89,358,npzqks,https://www.reddit.com/r/MachineLearning/comments/npzqks/r_chinese_ai_lab_challenges_google_openai_with_a/,167,1622569223.0,"Link here: https://en.pingwest.com/a/8693

TL;DR The Beijing Academy of Artificial Intelligence, styled as BAAI and known in Chinese as 北京智源人工智能研究院, launched the latest version of Wudao 悟道, a pre-trained deep learning model that the lab dubbed as “China’s first,” and “the world’s largest ever,” with a whopping 1.75 trillion parameters.

And the corresponding twitter thread: https://twitter.com/DavidSHolz/status/1399775371323580417

What's interesting here is BAAI is funded in part by the China’s Ministry of Science and Technology, which is China's equivalent of the NSF. The equivalent of this in the US would be for the NSF allocating billions of dollars a year *only to train models*."
49,machinelearning,openai,top,2022-12-22 18:39:30,[D] When chatGPT stops being free: Run SOTA LLM in cloud,_underlines_,False,0.95,347,zstequ,https://www.reddit.com/r/MachineLearning/comments/zstequ/d_when_chatgpt_stops_being_free_run_sota_llm_in/,95,1671734370.0,"Edit: Found [LAION-AI/OPEN-ASSISTANT](https://github.com/LAION-AI/Open-Assistant) a very promising project opensourcing the idea of chatGPT. [video here](https://www.youtube.com/watch?v=8gVYC_QX1DI)

**TL;DR: I found GPU compute to be [generally cheap](https://github.com/full-stack-deep-learning/website/blob/main/docs/cloud-gpus/cloud-gpus.csv) and spot or on-demand instances can be launched on AWS for a few USD / hour up to over 100GB vRAM. So I thought it would make sense to run your own SOTA LLM like Bloomz 176B inference endpoint whenever you need it for a few questions to answer. I thought it would still make more sense than shoving money into a closed walled garden like ""not-so-OpenAi"" when they make ChatGPT or GPT-4 available for $$$. But I struggle due to lack of tutorials/resources.**

Therefore, I carefully checked benchmarks, model parameters and sizes as well as training sources for all SOTA LLMs [here](https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit#gid=1158069878).

Knowing since reading the Chinchilla paper that Model Scaling according to OpenAI was wrong and more params != better quality generation. So I was looking for the best performing LLM openly available in terms of quality and broadness to use for multilingual everyday questions/code completion/reasoning similar to what chatGPT provides (minus the fine-tuning for chat-style conversations).

My choice fell on [Bloomz](https://huggingface.co/bigscience/bloomz) (because that handles multi-lingual questions well and has good zero shot performance for instructions and Q&A style text generation. Confusingly Galactica seems to outperform Bloom on several benchmarks. But since Galactica had a very narrow training set only using scientific papers, I guess usage is probably limited for answers on non-scientific topics.

Therefore I tried running the original bloom 176B and alternatively also Bloomz 176B on AWS SageMaker JumpStart, which should be a one click deployment. This fails after 20min. On Azure ML, I tried using DeepSpeed-MII which also supports bloom but also fails due the instance size of max 12GB vRAM I guess.

From my understanding to save costs on inference, it's probably possible to use one or multiple of the following solutions:

- Precision: int8 instead of fp16
- [Microsoft/DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) for an up 40x reduction on inference cost on Azure, this thing also supports int8 and fp16 bloom out of the box, but it fails on Azure due to instance size.
- [facebook/xformer](https://github.com/facebookresearch/xformers) not sure, but if I remember correctly this brought inference requirements down to 4GB vRAM for StableDiffusion and DreamBooth fine-tuning to 10GB. No idea if this is usefull for Bloom(z) inference cost reduction though

I have a CompSci background but I am not familiar with most stuff, except that I was running StableDiffusion since day one on my rtx3080 using linux and also doing fine-tuning with DreamBooth. But that was all just following youtube tutorials. I can't find a single post or youtube video of anyone explaining a full BLOOM / Galactica / BLOOMZ inference deployment on cloud platforms like AWS/Azure using one of the optimizations mentioned above, yet alone deployment of the raw model. :(

I still can't figure it out by myself after 3 days.

**TL;DR2: Trying to find likeminded people who are interested to run open source SOTA LLMs for when chatGPT will be paid or just for fun.**

Any comments, inputs, rants, counter-arguments are welcome.

/end of rant"
50,machinelearning,openai,top,2023-05-07 23:26:29,"[D] ClosedAI license, open-source license which restricts only OpenAI, Microsoft, Google, and Meta from commercial use",wemsyn,False,0.8,340,13b6miy,https://www.reddit.com/r/MachineLearning/comments/13b6miy/d_closedai_license_opensource_license_which/,191,1683501989.0,"After reading [this article](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither), I realized it might be nice if the open-source AI community could exclude ""closed AI"" players from taking advantage of community-generated models and datasets. I was wondering if it would be possible to write a license that is completely permissive (like Apache 2.0 or MIT), except to certain companies, which are completely barred from using the software in any context.

Maybe this could be called the ""ClosedAI"" license. I'm not any sort of legal expert so I have no idea how best to write this license such that it protects model weights and derivations thereof.

I prompted ChatGPT for an example license and this is what it gave me:

    <PROJECT NAME> ClosedAI License v1.0
    
    Permission is hereby granted, free of charge, to any person or organization obtaining a copy of this software and associated documentation files (the ""Software""), to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, subject to the following conditions:
    
    1. The above copyright notice and this license notice shall be included in all copies or substantial portions of the Software.
    
    2. The Software and any derivative works thereof may not be used, in whole or in part, by or on behalf of OpenAI Inc., Google LLC, or Microsoft Corporation (collectively, the ""Prohibited Entities"") in any capacity, including but not limited to training, inference, or serving of neural network models, or any other usage of the Software or neural network weights generated by the Software.
    
    3. Any attempt by the Prohibited Entities to use the Software or neural network weights generated by the Software is a material breach of this license.
    
    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

No idea if this is valid or not. Looking for advice.

&#x200B;

**Edit:** Thanks for the input. Removed non-commercial clause (whoops, proofread what ChatGPT gives you). Also removed Meta from the excluded companies list due to popular demand."
51,machinelearning,openai,top,2019-03-22 02:36:38,[P] OpenAI's GPT-2-based Reddit Bot is Live!,Shevizzle,False,0.97,334,b3zlha,https://www.reddit.com/r/MachineLearning/comments/b3zlha/p_openais_gpt2based_reddit_bot_is_live/,990,1553222198.0,"**~~FINAL~~** **UPDATE: The bot is down until I have time to get it operational again. Will update this when it’s back online.**

&#x200B;

**Disclaimer** : This is not the full model. This is the smaller and less powerful version which OpenAI released publicly.

[Original post](https://www.reddit.com/r/MachineLearning/comments/b32lve/d_im_using_openais_gpt2_to_generate_text_give_me/)

Based on the popularity of my post from the other day, I decided to go ahead an build a full-fledged Reddit bot. So without further ado, please welcome:

# u/GPT-2_Bot

&#x200B;

If you want to use the bot, all you have to do is reply to any comment with the following command words:

# ""gpt-2 finish this""

Your reply can contain other stuff as well, i.e.

>""hey **gpt-2**, please **finish this** argument for me, will ya?""

&#x200B;

The bot will then look at **the comment you replied to** and generate its own response. It will tag you in the response so you know when it's done!

&#x200B;

Currently supported subreddits:

* r/funny
* r/AskReddit
* r/gaming
* r/pics
* r/science
* r/worldnews
* r/todayilearned
* r/movies
* r/videos
* r/ShowerThoughts
* r/MachineLearning
* r/test
* r/youtubehaiku
* r/thanosdidnothingwrong
* r/dankmemes

&#x200B;

The bot also scans r/all so ***theoretically*** it will see comments posted anywhere on Reddit. In practice, however, it only seems to catch about 1 in 5 of them.

&#x200B;

Enjoy! :) Feel free to PM me with feedback"
52,machinelearning,openai,top,2019-03-19 20:43:15,[D] I'm using OpenAI's GPT-2 to generate text. Give me prompts to try!,Shevizzle,False,0.96,332,b32lve,https://www.reddit.com/r/MachineLearning/comments/b32lve/d_im_using_openais_gpt2_to_generate_text_give_me/,781,1553028195.0,"Thank you all for participating! Based on the popularity of this post, I decided to create a full-blown reddit bot based on it. I won't be responding here anymore but check out the update for more info.

# UPDATE: [I made a GPT-2 reddit bot](https://www.reddit.com/r/MachineLearning/comments/b3zlha/p_openais_gpt2based_reddit_bot_is_live/)

&#x200B;

**Disclaimer** : This is not the full model. This is the smaller and less powerful version which OpenAI released publicly.

**EXAMPLE PROMPT:**

The NCAA unveiled the 68 college basketball teams that will be dancing this year. It wasn’t short on controversy, as snubbed teams aired out their grievances and blamed a confusing new metric.

**OUTPUT:**

The numbers came straight from the NCAA's website, where they're being used to calculate the number of national titles, national championship games, and even NCAA games played in a certain year.

""We want to make sure that the NCAA is inclusive,"" said NCAA President Dave Heidelman. ""We want to make sure that you can get what you want out of your program. And we want to make sure that the NCAA is focused on this problem right now and that the NCAA is fully invested.""

The list of opponents includes Ohio State, Florida State, Louisville, Kentucky, Mississippi State, Penn State, Oklahoma, Southern Cal, Virginia, and Wake Forest. It's not all about one one, though: They're all on pace to play an entire season. They have every national title, national championship, and tournament appearance in their DNA — as one might expect, they want to see it happen, but also want to build an identity, make their dream of watching that happen become the reality.

As good as any NCAA team is, they're also a long way off reaching the number one spot in the nation or even the top-ranked nation. The Big Ten also has some talented recruits from some in-state programs that may be considered the top two nationally. In fact, the national ranking of these schools is so high that a single conference ranking in 2016 will put the conference in the top-50 of the polls. Still, while Big Ten and SEC teams are likely to be on the map and competing for national titles, they're a bit underserved (and it's not as if they're all the same.)

So where does the NCAA stand on this?

According to ULM's John Covington, who runs its ""Unions, Colleges, and Universities"" page in conjunction with the National Conference, they're all going to have to make some moves:

Some may think this is just a joke. ""No, this is really about the league's future,"" said Dr. John H. Hester, president of UM's Athletic Department and president of the National Collegiate Athletic Association's Women's Academic Programs. ""I think the NCAA is a great place to start, because it's here to stay and if we're really strong and we can figure ourselves out, our future is going to be on the basketball court.""

**MODEL:**

[gpt-2 117M](https://github.com/openai/gpt-2)

**If you have an idea for a prompt, post it in the comments and I'll reply with the output if I deem it worthy.**"
53,machinelearning,openai,top,2021-07-28 16:42:17,[N] Introducing Triton: Open-Source GPU Programming for Neural Networks,fasttosmile,False,0.96,332,otdpkx,https://www.reddit.com/r/MachineLearning/comments/otdpkx/n_introducing_triton_opensource_gpu_programming/,48,1627490537.0,"[https://www.openai.com/blog/triton/](https://www.openai.com/blog/triton/)

[Link to first tutorial](https://triton-lang.org/getting-started/tutorials/01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py)

Looks pretty nice"
54,machinelearning,openai,top,2022-07-11 03:18:28,[D] Why are Corgi dogs so popular in machine learning (especially in the image generation community)?,Azuresonance,False,0.92,324,vw8jtp,https://www.reddit.com/r/MachineLearning/comments/vw8jtp/d_why_are_corgi_dogs_so_popular_in_machine/,67,1657509508.0,"For example, here's part of OpenAI's GLIDE paper:

https://preview.redd.it/b6vkxyb3xua91.png?width=1225&format=png&auto=webp&s=15d56f256e323bb54d22eb9fdc0538644060c4a7"
55,machinelearning,openai,top,2020-09-22 17:40:14,[N] Microsoft teams up with OpenAI to exclusively license GPT-3 language model,kit1980,False,0.96,321,ixs88q,https://www.reddit.com/r/MachineLearning/comments/ixs88q/n_microsoft_teams_up_with_openai_to_exclusively/,117,1600796414.0,"""""""OpenAI will continue to offer GPT-3 and other powerful models via its own Azure-hosted API, launched in June. While we’ll be hard at work utilizing the capabilities of GPT-3 in our own products, services and experiences to benefit our customers, we’ll also continue to work with OpenAI to keep looking forward: leveraging and democratizing the power of their cutting-edge AI research as they continue on their mission to build safe artificial general intelligence.""""""

https://blogs.microsoft.com/blog/2020/09/22/microsoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/"
56,machinelearning,openai,top,2020-12-13 11:01:16,[D] What exactly is Yann LeCun's Energy Based Self-Supervise Learning?,FactfulX,False,0.96,318,kc8ruw,https://www.reddit.com/r/MachineLearning/comments/kc8ruw/d_what_exactly_is_yann_lecuns_energy_based/,55,1607857276.0,"Does anyone actually understand what Yann LeCun really means in Energy based SSL?  Linking a time-stamped YT link here:

[https://youtu.be/A7AnCvYDQrU?t=2169](https://youtu.be/A7AnCvYDQrU?t=2169)

It seems like he is suggesting training a conditional latent variable model (eg. something like a VAE or a GAN) that takes an input and predicts an output based on the input and a latent variable. One could imagine doing this with a pix2pix GAN or a VAE. What the input and output are could be something like one part of an image, and decode the other part; or video, audio, etc. What's actually special about this? Has anyone tried to implement these ideas and found it to help/work in practice?

My limited understanding is that generative models are not great at representation learning, but OpenAI showed good results with iGPT pre-training, which you can argue does do predicting missing (next pixel) from existing information (previous pixels). But their computational efficiency severely lags behind that of contrastive learning models like SimCLR. There are also methods like Contrastive Predictive Coding which do this missing info prediction through the contrastive loss.

Curious what people think are the merits of LeCun's proposal, and what would be a good practical and worthwhile implementation of LeCun's idea?

PS: I am also surprised how come he hasn't gotten anyone at Facebook Research to make progress on it for the last four years, despite being its Chief Scientist. The only results he shows are old MNIST results from his PhD students from pre-AlexNet era, and some toyish results of model-based RL on traffic simulation. His talks are really confusing since he mishmashes all latest successes like BERT, MoCo, SimCLR, Mask R-CNN etc in between which have absolutely nothing to do with energy based latent variable models."
57,machinelearning,openai,top,2020-06-11 15:14:43,[N] OpenAI API,jboyml,False,0.97,317,h1179l,https://www.reddit.com/r/MachineLearning/comments/h1179l/n_openai_api/,62,1591888483.0,"[https://beta.openai.com/](https://beta.openai.com/)

OpenAI releases a commercial API for NLP tasks including semantic search, summarization, sentiment analysis, content generation, translation, and more."
58,machinelearning,openai,top,2020-08-22 17:16:08,"[N] GPT-3, Bloviator: OpenAI’s language generator has no idea what it’s talking about",rafgro,False,0.94,311,iemck2,https://www.reddit.com/r/MachineLearning/comments/iemck2/n_gpt3_bloviator_openais_language_generator_has/,111,1598116568.0,"MIT Tech Review's article: [https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/](https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/)

>As we were putting together this essay, our colleague Summers-Stay, who is good with metaphors, wrote to one of us, saying this: ""GPT is odd because it doesn’t 'care' about getting the right answer to a question you put to it. It’s more like an improv actor who is totally dedicated to their craft, never breaks character, and has never left home but only read about the world in books. Like such an actor, when it doesn’t know something, it will just fake it. You wouldn’t trust an improv actor playing a doctor to give you medical advice."""
59,machinelearning,openai,top,2015-12-11 22:22:39,"OpenAI - a non-profit AI research company, launched by Sutskever, Musk, Bengio, Zaremba and many others",elanmart,False,0.95,314,3wfqip,https://openai.com/blog/introducing-openai/,90,1449872559.0,
60,machinelearning,openai,top,2019-07-17 14:59:21,"[P] A library of pretrained models for NLP: Bert, GPT, GPT-2, Transformer-XL, XLNet, XLM",Thomjazz,False,0.98,310,cedysl,https://www.reddit.com/r/MachineLearning/comments/cedysl/p_a_library_of_pretrained_models_for_nlp_bert_gpt/,19,1563375561.0,"Huggingface has released a new version of their open-source library of pretrained transformer models for NLP: *PyTorch-Transformers* 1.0 (formerly known as *pytorch-pretrained-bert*).

&#x200B;

The library now comprises six architectures:

* Google's **BERT**,
* OpenAI's **GPT** & **GPT-2**,
* Google/CMU's **Transformer-XL** & **XLNet** and
* Facebook's **XLM**,

and a total of 27 pretrained model weights for these architectures.

&#x200B;

The library focus on:

* being superfast to learn & use (almost no abstractions),
* providing SOTA examples scripts as starting points (text classification with GLUE, question answering with SQuAD and text generation using GPT, GPT-2, Transformer-XL, XLNet).

&#x200B;

It also provides:

* a unified API for models and tokenizers,
* access to the hidden-states and attention weights,
* compatibility with Torchscript...

&#x200B;

Install: *pip install pytorch-transformers*

Quickstart: [https://github.com/huggingface/pytorch-transformers#quick-tour](https://github.com/huggingface/pytorch-transformers#quick-tour)

Release notes: [https://github.com/huggingface/pytorch-transformers/releases/tag/v1.0.0](https://github.com/huggingface/pytorch-transformers/releases/tag/v1.0.0)

Documentation (work in progress): [https://huggingface.co/pytorch-transformers/](https://huggingface.co/pytorch-transformers/)"
61,machinelearning,openai,top,2019-03-11 16:19:00,[N] OpenAI LP,SkiddyX,False,0.94,303,azvbmn,https://www.reddit.com/r/MachineLearning/comments/azvbmn/n_openai_lp/,150,1552321140.0,"""We’ve created OpenAI LP, a new “capped-profit” company that allows us to rapidly increase our investments in compute and talent while including checks and balances to actualize our mission.""

Sneaky.

https://openai.com/blog/openai-lp/"
62,machinelearning,openai,top,2018-11-26 14:07:07,[P] Reaver: StarCraft II Deep Reinforcement Learning Agent,Inori,False,0.96,313,a0jm84,https://www.reddit.com/r/MachineLearning/comments/a0jm84/p_reaver_starcraft_ii_deep_reinforcement_learning/,25,1543241227.0,"I'm really anxious and happy to finally show what I've been working on for the last half a year: https://github.com/inoryy/reaver-pysc2

*Short description:*

Reaver is a modular DRL framework that provides faster single-machine env parallelization than most open-source solutions; supports common environments like gym, atari, mujoco in addition to SC2; has networks defined as simple Keras models; is easy to configure & share the configs. As a toy example it solves CartPole-v0 in under 10 seconds, running at about 5k samples per second on a laptop with 4 core CPU. You can see Reaver in action online on [Google Colab](https://colab.research.google.com/drive/1DvyCUdymqgjk85FB5DrTtAwTFbI494x7), solving StarCraft II's MoveToBeacon minigame in 30 minutes.

---

*Long description:*

This project is a [grounds up rewrite](https://github.com/inoryy/reaver-pysc2/commit/c8efbd17797a3d85240b3bdd3f24de422029152b) of its predecessor (my bachelor's thesis) and was motivated by some of the painful experiences I've had along the way. Specifically:

**Performance** - majority of RL baselines published are usually tuned for message-based communication between processes (e.g. MPI). This makes sense for companies like DeepMind or OpenAI with their large scale distributed RL setups, but to me it always seemed like a major bottleneck for typical researchers or hobbyists with access to a single computer / HPC node. So instead I went with shared memory route with Reaver and achieved about 3x speed increase over previous project which had message-based parallelization.

**Modularity** - many RL baselines are modular in one way or another, but are often tightly coupled to the models / environments authors use. From own experience I've written myself into a corner by focusing on StarCraft II, which meant that every experiment and debug was a depressingly long process. So with Reaver I've made it possible to swap envs in one line (literally, even going from SC2 to Atari or CartPole). Same goes for models - any Keras model will do as long as it abides by basic API contracts (inputs = agent obs, outputs = logits + value).

**Configurability** - a modern agent often has dozens of various configuration parameters and sharing them seems to be annoying for everyone involved. I've recently stumbled upon [gin-config](https://github.com/google/gin-config) - a very interesting solution to this problem that supports configuring any Python callable function, both as a python-like config file and through command line arguments. I've used it everywhere I could in Reaver and I'm quite happy with the result, being able to share full training pipeline setup configuration with just one file.

**Future-proof** - a common problem in DL is that things change so fast that even year old codebases can become obsolete. I've written Reaver with the upcoming TensorFlow 2.0 API in mind (mostly involved using tf.keras and avoiding tf.contrib), so hopefully it won't suffer this fate for awhile.

---

Note that even though the niche I'm focusing on with this project is DRL in StarCraft II, none of Reaver's functionality is actually tied to it. Reaver currently has full support for generic gym, atari, and mujoco environments. Let me know if you would like me to support something else (I plan to add VizDoom in the near future as that env also interests me personally :) )."
63,machinelearning,openai,top,2019-04-21 15:56:27,[D] OpenAI Five vs Humans currently at 4106–33 (99.2% winrate),FirstTimeResearcher,False,0.97,301,bfq8v9,https://www.reddit.com/r/MachineLearning/comments/bfq8v9/d_openai_five_vs_humans_currently_at_410633_992/,58,1555862187.0,"A small group of humans is winning consistently against OpenAI Five. There seem to be a few reproducible strategies that keep beating the bot. Can someone describe what those strategies are for someone that hasn't played DoTA?

Link
https://arena.openai.com/#/results"
64,machinelearning,openai,top,2019-02-14 17:09:53,[R] OpenAI: Better Language Models and Their Implications,jinpanZe,False,0.95,297,aqlzde,https://www.reddit.com/r/MachineLearning/comments/aqlzde/r_openai_better_language_models_and_their/,128,1550164193.0,"https://blog.openai.com/better-language-models/

""We’ve trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization — all without task-specific training.""

Interestingly,

""Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much smaller model for researchers to experiment with, as well as a technical paper."""
65,machinelearning,openai,top,2023-04-05 19:44:09,"[D] ""Our Approach to AI Safety"" by OpenAI",mckirkus,False,0.88,297,12cvkvn,https://www.reddit.com/r/MachineLearning/comments/12cvkvn/d_our_approach_to_ai_safety_by_openai/,297,1680723849.0,"It seems OpenAI are steering the conversation away from the existential threat narrative and into things like accuracy, decency, privacy, economic risk, etc.

To the extent that they do buy the existential risk argument, they don't seem concerned much about GPT-4 making a leap into something dangerous, even if it's at the heart of autonomous agents that are currently emerging.  

>""Despite extensive research and testing, we cannot predict all of the [beneficial ways people will use our technology](https://openai.com/customer-stories), nor all the ways people will abuse it. That’s why we believe that learning from real-world use is a critical component of creating and releasing increasingly safe AI systems over time. ""

Article headers:

* Building increasingly safe AI systems
* Learning from real-world use to improve safeguards
* Protecting children
* Respecting privacy
* Improving factual accuracy

&#x200B;

[https://openai.com/blog/our-approach-to-ai-safety](https://openai.com/blog/our-approach-to-ai-safety)"
66,machinelearning,openai,top,2022-10-25 17:24:13,[N] OpenAI Gym and a bunch of the most used open source RL environments have been consolidated into a single new nonprofit (The Farama Foundation),jkterry1,False,0.97,298,ydafsw,https://www.reddit.com/r/MachineLearning/comments/ydafsw/n_openai_gym_and_a_bunch_of_the_most_used_open/,12,1666718653.0,You can read the full announcement post here: [https://farama.org/Announcing-The-Farama-Foundation](https://farama.org/Announcing-The-Farama-Foundation)
67,machinelearning,openai,top,2022-02-02 16:39:00,"[N] EleutherAI announces a 20 billion parameter model, GPT-NeoX-20B, with weights being publicly released next week",MonLiH,False,0.96,299,sit4ro,https://www.reddit.com/r/MachineLearning/comments/sit4ro/n_eleutherai_announces_a_20_billion_parameter/,65,1643819940.0,"GPT-NeoX-20B, a 20 billion parameter model trained using EleutherAI's [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), was announced today. They will publicly release the weights on February 9th, which is a week from now. The model outperforms OpenAI's [Curie](https://beta.openai.com/docs/engines/curie) in a lot of tasks.

They have provided some additional info (and benchmarks)  in their blog post, at [https://blog.eleuther.ai/announcing-20b/](https://blog.eleuther.ai/announcing-20b/). "
68,machinelearning,openai,top,2020-06-01 00:16:25,"[D] Do I Need to Go to University? Essay by Chris Olah, OpenAI research scientist, previously at Google Brain, and creator of distill.pub",sensetime,False,0.94,292,gua7z9,http://colah.github.io/posts/2020-05-University/,88,1590970585.0,
69,machinelearning,openai,top,2023-11-22 06:38:48,"OpenAI: ""We have reached an agreement in principle for Sam to return to OpenAI as CEO"" [N]",we_are_mammals,False,0.92,288,1812w04,https://www.reddit.com/r/MachineLearning/comments/1812w04/openai_we_have_reached_an_agreement_in_principle/,129,1700635128.0,"OpenAI announcement:

""We have reached an agreement in principle for Sam to return to OpenAI as CEO with a new initial board of Bret Taylor (Chair), Larry Summers, and Adam D'Angelo.

We are collaborating to figure out the details. Thank you so much for your patience through this.""

https://twitter.com/OpenAI/status/1727205556136579362"
70,machinelearning,openai,top,2020-01-04 05:29:26,[P] GlyphNet: Training neural networks to communicate with a visual language,noahtren,False,0.96,285,ejsdac,https://www.reddit.com/r/MachineLearning/comments/ejsdac/p_glyphnet_training_neural_networks_to/,31,1578115766.0,"&#x200B;

[Visualization of glyphs generated by neural network](https://preview.redd.it/u9p226cu5p841.png?width=462&format=png&auto=webp&s=0a7932e9859e1f160e1d49c3c5f48fd58006df59)

I did an experiment over winter break to see what would happen if I trained 2 neural networks to communicate with each other in a noisy environment. The task of the first neural network is to generate unique symbols, and the other's task is to tell them apart. The result is a pretty cool visual language that looks kind of alien.

Notably, I got the best results by dynamically increasing the noise parameters as the networks became more competent (pulling inspiration from [Automatic Domain Randomization](https://openai.com/blog/solving-rubiks-cube/) and [POET](https://eng.uber.com/poet-open-ended-deep-learning/)).

Please take a look and let me know what you think! [https://github.com/noahtren/GlyphNet](https://github.com/noahtren/GlyphNet)"
71,machinelearning,openai,top,2021-04-15 17:28:43,[D] Microsoft's ML acquisition strategy,bendee983,False,0.96,285,mrjl61,https://www.reddit.com/r/MachineLearning/comments/mrjl61/d_microsofts_ml_acquisition_strategy/,37,1618507723.0,"This week, Microsoft announced the $19.7-billion acquisition of Nuance, a company that uses deep learning to transcribe clinical appointments (and other stuff). What's interesting about the deal is the [evolution of Microsoft's relation with Nuance](https://bdtechtalks.com/2021/04/15/microsoft-nuance-acquisition/), going from cloud provider to partner to owner. 

This is a successful strategy that only Microsoft (and maybe Amazon) is in a position to implement:

Step 1: Microsoft starts by investing in ML companies by giving them Azure credits and luring them into its ML platform. This allows Microsoft to help the companies develop and also learn from them (and possibly replicate their products if it's worth it). Multiple small investments as opposed to one large acquisition is a smart move because many companies are trying new things in ML/DL, few of which will be successful. With small investments, Microsoft can cast a wider net and make sure it is in a good position to make the next move.

Step 2: Microsoft enters partnership with companies that have successful products. This allows Microsoft to integrate their ML products into its enterprise solutions (e.g., Nuance's Dragon DL was integrated into Microsoft's cloud healthcare solution). Since these companies are building their ML tools on top of Azure's stack, the integration is much easier for both companies.

Step 3: Acquire really successful companies (Nuance has a great reach in the AI+healthcare sector). This allows Microsoft to gain exclusive access to the company's data, talent, technology, and clients. With the acquisition of Nuance, Microsoft's total addressable market in healthcare has reached $500B+. And it can integrate its ML technology into its other enterprise tools.

Nuance is just one example of Microsoft's ML acquisition strategy. The company is on a similar path [with OpenAI](https://bdtechtalks.com/2020/09/24/microsoft-openai-gpt-3-license/) and is carrying out [a similar strategy in the self-driving car industry](https://bdtechtalks.com/2021/01/21/microsoft-self-driving-car-strategy/)."
72,machinelearning,openai,top,2023-03-30 22:40:29,[P] Introducing Vicuna: An open-source language model based on LLaMA 13B,Business-Lead2679,False,0.95,287,1271po7,https://www.reddit.com/r/MachineLearning/comments/1271po7/p_introducing_vicuna_an_opensource_language_model/,107,1680216029.0,"We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%\* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%\* of cases. The cost of training Vicuna-13B is around $300. The training and serving [code](https://github.com/lm-sys/FastChat), along with an online [demo](https://chat.lmsys.org/), are publicly available for non-commercial use.

# Training details

Vicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model’s maximum context length.

Our training recipe builds on top of [Stanford’s alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) with the following improvements.

* **Memory Optimizations:** To enable Vicuna’s understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing [gradient checkpointing](https://arxiv.org/abs/1604.06174) and [flash attention](https://arxiv.org/abs/2205.14135).
* **Multi-round conversations:** We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot’s output.
* **Cost Reduction via Spot Instance:** The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ [SkyPilot](https://github.com/skypilot-org/skypilot) [managed spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html) to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.

&#x200B;

[Vicuna - Online demo](https://reddit.com/link/1271po7/video/0qsiu08kdyqa1/player)

# Limitations

We have noticed that, similar to other large language models, Vicuna has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. Additionally, it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI [moderation](https://platform.openai.com/docs/guides/moderation/overview) API to filter out inappropriate user inputs in our online demo. Nonetheless, we anticipate that Vicuna can serve as an open starting point for future research to tackle these limitations.

[Relative Response Quality Assessed by GPT-4](https://preview.redd.it/1rnmhv01eyqa1.png?width=599&format=png&auto=webp&s=02b4d415b5d378851bb70e225f1b1ebce98bfd83)

&#x200B;

For more information, check [https://vicuna.lmsys.org/](https://vicuna.lmsys.org/)

Online demo: [https://chat.lmsys.org/](https://chat.lmsys.org/)

&#x200B;

All credits go to the creators of this model. I did not participate in the creation of this model nor in the fine-tuning process. Usage of this model falls under a non-commercial license."
73,machinelearning,openai,top,2017-06-28 19:05:58,[D] OpenAI open sources a high-performance Python library for robotic simulation,cherls,False,0.94,283,6k2sr8,https://blog.openai.com/faster-robot-simulation-in-python/,26,1498676758.0,
74,machinelearning,openai,top,2022-04-28 04:19:52,How to do meaningful work as an independent researcher? [Discussion],HairyIndianDude,False,0.97,277,udml1k,https://www.reddit.com/r/MachineLearning/comments/udml1k/how_to_do_meaningful_work_as_an_independent/,63,1651119592.0,"With big players like OpenAI and Google building these massive models, how does independent researchers without access to such scale and compute do meaningful work? Came across tweets from researchers, especially ones working on generative models saying they feel their work looks irrelevant after seeing results from DALL-E 2. It feels like just a couple of years ago if you had a decent GPU setup, you could pretty much do world class research. Doesn't look like it anymore. Is there, if any, research directions that makes it a level playing field where compute and scale is not necessarily the solution, or are we all doomed to be prompt engineers for GPT models?"
75,machinelearning,openai,top,2016-01-03 02:36:06,The OpenAI Research Team will be doing an AMA in /r/MachineLearning on January 9,olaf_nij,False,0.96,279,3z80lw,https://www.reddit.com/r/MachineLearning/comments/3z80lw/the_openai_research_team_will_be_doing_an_ama_in/,22,1451788566.0,"Starting off the new year with another AMA announcement! The [OpenAI](https://openai.com/) Research Team will be stopping by /r/MachineLearning on January 9 for an AMA.

A thread will be created before the official AMA time for those who won't be able to attend on that day."
76,machinelearning,openai,top,2022-01-28 17:39:35,[D] It seems OpenAI’s new embedding models perform terribly,StellaAthena,False,0.97,280,sew5rl,https://www.reddit.com/r/MachineLearning/comments/sew5rl/d_it_seems_openais_new_embedding_models_perform/,80,1643391575.0,"Some people on Twitter have been investigating [OpenAI’s new embedding API](https://openai.com/blog/introducing-text-and-code-embeddings/) and it’s shocking how poorly it performs. On standard benchmarks, open source models 1000x smaller obtain equal or better performance! Models based on RoBERTa and T5, as well as the Sentence Transformer all achieve significantly better performance than the 175B model. Also of interest is that the DaVinci (175B) model is not clearly better than the Ada (350M) model.

Has anyone tried adapting some other autoregressive languages models, such as GPT-2, GPT-Neo, or GPT-J to do embeddings? I’m quite curious if this is an inherent failing of autoregressive models or if there’s something else going on. **Edit:** [a commenter](https://www.reddit.com/r/MachineLearning/comments/sew5rl/d_it_seems_openais_new_embedding_models_perform/humuzef/) has asked that I point out that I am one of the creators of GPT-Neo and part of the org that created GPT-J. These examples were not intended as specific endorsements, and I would be just as interested in comparisons using other billion-parameter+ autoregressive language models.

**Edit 2:** I originally linked to a [tweet](https://twitter.com/Nils_Reimers/status/1487014195568775173?s=20&amp;amp;amp;amp;amp;amp;amp;t=NBF7D2DYi41346cGM-PQjQ) about this, but several commenters pointed out that there’s also a [blog post](https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9) with more information.

**Edit 3:** An OpenAI researcher [seems to have responded](https://mobile.twitter.com/arvind_io/status/1487188996774002688)."
77,machinelearning,openai,top,2022-07-20 17:18:18,"[N] OpenAI blog post ""DALL·E Now Available in Beta"". DALL-E 2 is a text-to-image system. Pricing details are included. Commercial usage is now allowed.",Wiskkey,False,0.95,278,w3ry4o,https://www.reddit.com/r/MachineLearning/comments/w3ry4o/n_openai_blog_post_dalle_now_available_in_beta/,44,1658337498.0,"[OpenAI blog post](https://openai.com/blog/dall-e-now-available-in-beta/).

[How DALL·E Credits Work](https://help.openai.com/en/articles/6399305-how-dall-e-credits-work).

[Links to DALL-E Content policy and Terms of use, along with older archived versions](https://www.reddit.com/r/dalle2/comments/w3r9cf/comment/igxy1jc/)."
78,machinelearning,openai,top,2023-05-26 13:57:42,[N] Abu Dhabi's TTI releases open-source Falcon-7B and -40B LLMs,Balance-,False,0.95,272,13sdz8p,https://www.reddit.com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/,58,1685109462.0,"Abu Dhabi's Technology Innovation Institute (TII) just released new 7B and 40B LLMs.

The Falcon-40B model is now at the top of the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), beating *llama-30b-supercot* and *llama-65b* among others.

| Model                      | Revision | Average | ARC (25-shot) | HellaSwag (10-shot) | MMLU (5-shot) | TruthfulQA (0-shot) |
|----------------------------|----------|-----------|-----------------|-----------------------|-----------------|-----------------------|
| tiiuae/falcon-40b          | main     | 60.4      | 61.9            | 85.3                  | 52.7            | 41.7                  |
| ausboss/llama-30b-supercot | main     | 59.8      | 58.5            | 82.9                  | 44.3            | 53.6                  |
| llama-65b                  | main     | 58.3      | 57.8            | 84.2                  | 48.8            | 42.3                  |
| MetaIX/GPT4-X-Alpasta-30b  | main     | 57.9      | 56.7            | 81.4                  | 43.6            | 49.7                  |

**Press release:** [UAE's Technology Innovation Institute Launches Open-Source ""Falcon 40B"" Large Language Model for Research & Commercial Utilization](https://www.tii.ae/news/uaes-technology-innovation-institute-launches-open-source-falcon-40b-large-language-model)

>The Technology Innovation Institute (TII) in Abu Dhabi has announced its open-source large language model (LLM), the Falcon 40B. With 40 billion parameters, Falcon 40B is the UAE's first large-scale AI model, indicating the country's ambition in the field of AI and its commitment to promote innovation and research.  
>  
>Unlike most LLMs, which typically only provide non-commercial users access, Falcon 40B is open to both research and commercial usage. The TII has also included the model's weights in the open-source package, which will enhance the model's capabilities and allow for more effective fine-tuning.  
>  
>In addition to the launch of Falcon 40B, the TII has initiated a call for proposals from researchers and visionaries interested in leveraging the model to create innovative use cases or explore further applications. As a reward for exceptional research proposals, selected projects will receive ""training compute power"" as an investment, allowing for more robust data analysis and complex modeling. VentureOne, the commercialization arm of ATRC, will provide computational resources for the most promising projects.  
>  
>TII's Falcon 40B has shown impressive performance since its unveiling in March 2023. When benchmarked using Stanford University’s HELM LLM tool, it used less training compute power compared to other renowned LLMs such as OpenAI's GPT-3, DeepMind's Chinchilla AI, and Google's PaLM-62B.  
>  
>Those interested in accessing Falcon 40B or proposing use cases can do so through the [FalconLLM.TII.ae](https://FalconLLM.TII.ae) website. Falcon LLMs open-sourced to date are available under a license built upon the principles of the open-source Apache 2.0 software, permitting a broad range of free use.

**Hugging Face links**

* [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) / [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)
* [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) / [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)"
79,machinelearning,openai,top,2022-06-21 15:17:08,"[N] [D] Openai, who runs DALLE-2 alleged threatened creator of DALLE-Mini",DigThatData,False,0.86,267,vhfp1t,https://www.reddit.com/r/MachineLearning/comments/vhfp1t/n_d_openai_who_runs_dalle2_alleged_threatened/,120,1655824628.0,"Trying to cross-post what I think is a discussion that is relevant to this community. This is my third attempt, I hope I'm doing it correctly this time: 

https://www.reddit.com/r/dalle2/comments/vgtgdc/openai_who_runs_dalle2_alleged_threatened_creator/

EDIT: here are the original pre-prints for added context:

* DALL-E: [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092) - The only place the term ""DALL-E"" appears is the URL to the github repo.
* Dall-E 2: [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125) - They consistently refer to the first paper as ""DALL-E"", but refer to the work being described in the new paper as ""unCLIP"" and are careful to only use 'DALL-E 2' in the context of a product description, e.g. ""DALL·E 2 Preview platform (the first deployment of an unCLIP model)"""
80,machinelearning,openai,top,2018-07-18 16:39:06,[N] OpenAI Five Benchmark,thebackpropaganda,False,0.97,263,8zx2yf,https://blog.openai.com/openai-five-benchmark/,37,1531931946.0,
81,machinelearning,openai,top,2020-06-17 19:15:01,[R] OpenAI Image GPT,lfotofilter,False,0.96,262,hay15t,https://www.reddit.com/r/MachineLearning/comments/hay15t/r_openai_image_gpt/,99,1592421301.0,"Open AI just released a blog post about [Image GPT](https://openai.com/blog/image-gpt/). They apply the GPT-2 transformer-based model to pixel sequences (as opposed to word sequences).

This could actually be quite powerful in my view, because, as opposed to much of the current competition in self-supervised learning for images, Open AI are actually using a model of p(x) (of sorts) for downstream tasks. Recent successful methods like SimCLR rely heavily on augmentations, and mainly focus on learning features that are robust to these augmentations.

Slowly but surely, transformers are taking over the world."
82,machinelearning,openai,top,2019-04-11 11:28:08,[P] CppRl: A C++ reinforcement learning library using the new PyTorch C++ frontend,Flag_Red,False,0.97,257,bbyqdk,https://www.reddit.com/r/MachineLearning/comments/bbyqdk/p_cpprl_a_c_reinforcement_learning_library_using/,30,1554982088.0,"I'm really excited to show you guys what I've been working on lately:  [https://github.com/Omegastick/pytorch-cpp-rl](https://github.com/Omegastick/pytorch-cpp-rl)

It is *very* heavily based on [Ikostrikov's wonderful pytorch-a2c-ppo-acktr-gail](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail). You could even consider this a port. The API and underlying algorithms are almost identical (with the necessary changes involved in the move to C++).

It also contains a reimplementation simple OpenAI Gym server that communicates via [ZeroMQ](http://zeromq.org/) to test the framework on Gym environments.

CppRl aims to be an extensible, reasonably optimized, production-ready framework for using reinforcement learning in projects where Python isn't viable. It should be ready to use in desktop applications on user's computers with minimal setup required on the user's side.

## Motivation

At the time of writing, there are no general-use reinforcement learning frameworks for C++. I needed one for a personal project, and the PyTorch C++ frontend had recently been released, so I figured I should make one.

## Features

* Implemented algorithms:
   * A2C
   * PPO
* Recurrent policies (GRU based)
* Cross-platform compatibility (tested on Windows 10, Ubuntu 16.04, and Ubuntu 18.04)
* Solid test coverage
* Decently optimized (always open to pull requests improving optimization though)

# Sample

[Results after training for 60 second on my laptop](https://i.redd.it/r1w6ksghemr21.gif)

&#x200B;

**If you want to help with the project, please submit a PR!**"
83,machinelearning,openai,top,2017-08-13 23:34:26,[N] OpenAI bot was defeated at least 50 times yesterday,luiscosio,False,0.93,262,6timtv,https://twitter.com/riningear/status/896297256550252545,93,1502667266.0,
84,machinelearning,openai,top,2022-05-14 19:59:22,"[P] I made an open-source demo of OpenAI's CLIP model running completely in the browser - no server involved. Compute embeddings for (and search within) a local directory of images, or search 200k popular images from Reddit (as shown in this video). Link to demo and Github repo in comments.",joerocca,False,0.97,259,upp41g,https://v.redd.it/88dtgufeyhz81,27,1652558362.0,
85,machinelearning,openai,top,2019-04-13 20:06:34,[D] OpenAI 5 vs DOTA 2 World Champions happening now!,zergylord,False,0.96,253,bcumrs,https://www.reddit.com/r/MachineLearning/comments/bcumrs/d_openai_5_vs_dota_2_world_champions_happening_now/,39,1555185994.0,"First game is over, but I won't spoil it :)

livestream: [https://www.twitch.tv/openai](https://www.twitch.tv/openai)

&#x200B;

EDIT: top comment is a spoiler, just a heads up"
86,machinelearning,openai,top,2021-01-26 01:18:57,[P] Use natural language queries to search 2 million freely-usable images from Unsplash using a free Google Colab notebook from Vladimir Haltakov. Uses OpenAI's CLIP neural network.,Wiskkey,False,0.97,252,l52qe6,https://www.reddit.com/r/MachineLearning/comments/l52qe6/p_use_natural_language_queries_to_search_2/,32,1611623937.0,"[Google Colab notebook](https://colab.research.google.com/github/haltakov/natural-language-image-search/blob/main/colab/unsplash-image-search.ipynb):

>Unsplash Image Search  
>  
>Using this notebook you can search for images from the [Unsplash Dataset](https://unsplash.com/data) using natural language queries. The search is powered by OpenAI's [CLIP](https://github.com/openai/CLIP) neural network.  
>  
>This notebook uses the precomputed feature vectors for almost 2 million images from the full version of the [Unsplash Dataset](https://unsplash.com/data). If you want to compute the features yourself, see [here](https://github.com/haltakov/natural-language-image-search#on-your-machine).  
>  
>This project was created by [Vladimir Haltakov](https://twitter.com/haltakov) and the full code is open-sourced on [GitHub](https://github.com/haltakov/natural-language-image-search).

[Unsplash license](https://unsplash.com/license).

**Steps to follow to do your first search in a given Colab session:**

1. Click [this link](https://colab.research.google.com/github/haltakov/natural-language-image-search/blob/main/colab/unsplash-image-search.ipynb).
2. Sign into your Google account if you're not already signed in. Click the ""S"" button in the upper right to do this. Note: Being signed into a Google account has privacy ramifications, such as your Google search history being recorded in your Google account.
3. Click somewhere (except the triangle) in the cell with the line that reads 'search\_query = ""Two dogs playing in the snow""'.
4. Click menu item ""Runtime->Run before"". Wait until execution stops.
5. Find the line that reads (or initially read) 'search\_query = ""Two dogs playing in the snow""'. Change ""Two dogs playing in the snow"" to your desired search query (include the quotes); example: 'search\_query = ""A clock with gold-colored numbers on a black background""'.
6. (Optional) Find the line that reads (or initially read) 'search\_unslash(search\_query, photo\_features, photo\_ids, 3)'. Change 3 in that line to the number of search results that you want.
7. Click the triangle to the left of the line that initially read 'search\_query = ""Two dogs playing in the snow""'. Wait for the search results.

**Steps to follow to do more searches in a given Colab session**: Do steps 5 to 7 above.

After you're done with your Google Colab session, optionally log out of your Google account due to the privacy ramifications of being logged into a Google account.

**Update**: Text from the notebook:

>WARNING ⚠️  Since many people are currently using the notebook, it seems that the Unsplash API limit is hit from time to time (even with caching in the proxy). I applied for production status which will solve the problem. In the meantime, you can just try when a new hour starts. Alternatively,  you can use your own Unsplash API key

[Info about OpenAI's CLIP](https://openai.com/blog/clip/).

I am not affiliated with this project or its developer.

Example of a search result for query ""A clock with gold-colored numbers on a black background"":

https://preview.redd.it/tn5873yh4we61.jpg?width=320&format=pjpg&auto=webp&s=d6d08c468bb94d313786d107aede8ea5a150252f"
87,machinelearning,openai,top,2018-06-25 14:40:29,[R] OpenAI Five,circuithunter,False,0.96,249,8tr11j,https://blog.openai.com/openai-five/,48,1529937629.0,
88,machinelearning,openai,top,2019-10-09 18:11:46,"[Discussion] Exfiltrating copyright notices, news articles, and IRC conversations from the 774M parameter GPT-2 data set",madokamadokamadoka,False,0.96,248,dfky70,https://www.reddit.com/r/MachineLearning/comments/dfky70/discussion_exfiltrating_copyright_notices_news/,61,1570644706.0,"Concerns around abuse of AI text generation have been widely discussed. In the [original GPT-2 blog post](https://openai.com/blog/better-language-models/) from OpenAI, the team wrote:

>Due to concerns about large language models being used to generate deceptive, biased, or abusive language at scale, we are only releasing a [much smaller version of GPT-2 along with sampling code](https://github.com/openai/gpt-2/). We are not releasing the dataset, training code, or GPT-2 model weights. 

These concerns about mass generation of plausible-looking text are valid. However, there have been fewer conversations around the GPT-2 data sets themselves. Google searches such as ""GPT-2 privacy"" and ""GPT-2 copyright"" consist substantially of spurious results. Believing that these topics are poorly explored, and need further exploration, I relate some concerns here.

&#x200B;

Inspired by [this delightful post about TalkTalk's *Untitled Goose Game*](https://aiweirdness.com/post/188214106227/theres-a-game-called-untitled-goose-game-in), I used Adam Daniel King's [Talk to Transformer](https://talktotransformer.com/) web site to run queries against the GPT-2 774M data set. I was distracted from my mission of levity (pasting in snippets of [notoriously awful Harry Potter fan fiction](https://myimmortalrehost.webs.com/chapters122.htm) and like ephemera) when I ran into a link to a real Twitter post. It soon became obvious that the model contained more than just abstract data about the relationship of words to each other. Training data, rather, comes from a variety of sources, and with a sufficiently generic prompt, fragments consisting substantially of text from these sources can be extracted.

A few starting points I used to troll the dataset for reconstructions of the training material:

* Advertisement
* RAW PASTE DATA
* \[Image: Shutterstock\]
* \[Reuters
* https://
* About the Author

I soon realized that there was surprisingly specific data in here. After catching a specific timestamp in output, I queried the data for it, and was able to locate a conversation which I presume appeared in the training data. In the interest of privacy, **I have anonymized the usernames and Twitter links in the below output, because GPT-2 did not.** 

**\[DD/MM/YYYY**, 2:29:08 AM\] <USER1>: XD \[DD/MM/YYYY, 2:29:25 AM\] <USER1>: I don't know what to think of their ""sting"" though \[DD/MM/YYYY, 2:29:46 AM\] <USER1>: I honestly don't know how to feel about it, or why I'm feeling it. \[DD/MM/YYYY, 2:30:00 AM\] <USER1> (<@USER1>): ""We just want to be left alone. We can do what we want. We will not allow GG to get to our families, and their families, and their lives."" (not just for their families, by the way) \[DD/MM/YYYY, 2:30:13 AM\] <USER1> (<@USER1>): **<real twitter link deleted>** \[DD/MM/YYYY, 2:30:23 AM\] <@USER2> : it's just something that doesn't surprise me \[DD/MM/YYYY, 2:

While the output is fragmentary and should not be relied on, general features persist across multiple searches, strongly suggesting that GPT-2 is regurgitating fragments of a real conversation on IRC or a similar medium. The general topic of conversation seems to cover Gamergate, and individual usernames recur, along with real Twitter links. I assume this conversation was loaded off of Pastebin, or a similar service, where it was publicly posted along with other ephemera such as Minecraft initialization logs. Regardless of the source, this conversation is now shipped as part of the 774M parameter GPT-data set. 

This is a matter of grave concern. **Unless better care is taken of neural network training data, we should expect scandals, lawsuits, and regulatory action** to be taken against authors and users of GPT-2 or successor data sets**,** particularly in jurisdictions with stronger privacy laws. For instance, use of the GPT-2 training data set as it stands may very well be in violation of the European Union's GDPR regulations, insofar as it contains data generated by European users, and I shudder to think of the difficulties in effecting a takedown request under that regulation — or a legal order under the DMCA. 

&#x200B;

Here are some further prompts to try on Talk to Transformer, or your own local GPT-2 instance, which may help identify more exciting privacy concerns!

* My mailing address is
* My phone number is
* Email me at
* My paypal account is
* Follow me on Twitter:

Did I mention the DMCA already? This is because my exploration also suggests that **GPT-2 has been trained on copyrighted data**, raising further legal implications. Here are a few fun prompts to try:

* Copyright
* This material copyright
* All rights reserved
* This article originally appeared
* Do not reproduce without permission"
89,machinelearning,openai,top,2019-06-29 09:52:27,[D] State of AI report 2019,phasesundaftedreverb,False,0.98,251,c6x40z,https://www.reddit.com/r/MachineLearning/comments/c6x40z/d_state_of_ai_report_2019/,64,1561801947.0,"Came across the state of AI report and didn't see it posted here so thought it'd be interesting to have a discussion about the recently released report (second yearly one by Nathan Benaich and Ian Hogarth).

[https://www.slideshare.net/StateofAIReport/state-of-ai-report-2019-151804430](https://www.slideshare.net/StateofAIReport/state-of-ai-report-2019-151804430)

What stands out for you?Did they miss anything important?

&#x200B;

Two things stand out for me so far:

1) The 2018 prediction they made that a major AI lab would go dark actually happend (MIRI lab and OpenAI  -GPT2)

2) For me a as a fps CTF lover: the the Deepmind Quake III RL is wonderful to see. In 2019 they managed to include all the powerups which was a common criticism last year. (Updated blogpost here: [https://deepmind.com/blog/capture-the-flag-science/](https://deepmind.com/blog/capture-the-flag-science/))

I'm still going through the presentation myself so if I'll see more interesting stuff I'll add it to this thread."
90,machinelearning,openai,top,2023-05-24 01:00:28,"Interview with Juergen Schmidhuber, renowned ‘Father Of Modern AI’, says his life’s work won't lead to dystopia.",hardmaru,False,0.81,250,13q6k4a,https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/,96,1684890028.0,"*Schmidhuber interview expressing his views on the future of AI and AGI.*

*Original [source](https://www.forbes.com/sites/hessiejones/2023/05/23/juergen-schmidhuber-renowned-father-of-modern-ai-says-his-lifes-work-wont-lead-to-dystopia/). I think the interview is of interest to r/MachineLearning, and presents an alternate view, compared to other influential leaders in AI.*

**Juergen Schmidhuber, Renowned 'Father Of Modern AI,' Says His Life’s Work Won't Lead To Dystopia**

*May 23, 2023. Contributed by [Hessie Jones](https://twitter.com/hessiejones).*

Amid the growing concern about the impact of more advanced artificial intelligence (AI) technologies on society, there are many in the technology community who fear the implications of the advancements in Generative AI if they go unchecked. Dr. Juergen Schmidhuber, a renowned scientist, artificial intelligence researcher and widely regarded as one of the pioneers in the field, is more optimistic. He declares that many of those who suddenly warn against the dangers of AI are just seeking publicity, exploiting the media’s obsession with killer robots which has attracted more attention than “good AI” for healthcare etc.

The potential to revolutionize various industries and improve our lives is clear, as are the equal dangers if bad actors leverage the technology for personal gain. Are we headed towards a dystopian future, or is there reason to be optimistic? I had a chance to sit down with Dr. Juergen Schmidhuber to understand his perspective on this seemingly fast-moving AI-train that will leap us into the future.

As a teenager in the 1970s, Juergen Schmidhuber became fascinated with the idea of creating intelligent machines that could learn and improve on their own, becoming smarter than himself within his lifetime. This would ultimately lead to his groundbreaking work in the field of deep learning.

In the 1980s, he studied computer science at the Technical University of Munich (TUM), where he earned his diploma in 1987. His thesis was on the ultimate self-improving machines that, not only, learn through some pre-wired human-designed learning algorithm, but also learn and improve the learning algorithm itself. Decades later, this became a hot topic. He also received his Ph.D. at TUM in 1991 for work that laid some of the foundations of modern AI.

Schmidhuber is best known for his contributions to the development of recurrent neural networks (RNNs), the most powerful type of artificial neural network that can process sequential data such as speech and natural language. With his students Sepp Hochreiter, Felix Gers, Alex Graves, Daan Wierstra, and others, he published architectures and training algorithms for the long short-term memory (LSTM), a type of RNN that is widely used in natural language processing, speech recognition, video games, robotics, and other applications. LSTM has become the most cited neural network of the 20th century, and Business Week called it ""[arguably the most commercial AI achievement](https://www.bloomberg.com/news/features/2018-05-15/google-amazon-and-facebook-owe-j-rgen-schmidhuber-a-fortune?leadSource=uverify%20wall).""

Throughout his career, Schmidhuber has received various awards and accolades for his groundbreaking work. In 2013, he was awarded the Helmholtz Prize, which recognizes significant contributions to the field of machine learning. In 2016, he was awarded the IEEE Neural Network Pioneer Award for ""*pioneering contributions to deep learning and neural networks."" The media have often called him the “father of modern AI,*” because the [most cited neural networks](https://people.idsia.ch/~juergen/most-cited-neural-nets.html) all build on his lab’s work. He is quick to point out, however, that AI history [goes back centuries.](https://people.idsia.ch/~juergen/deep-learning-history.html)

Despite his many accomplishments, at the age of 60, he feels mounting time pressure towards building an Artificial General Intelligence within his lifetime and remains committed to pushing the boundaries of AI research and development. He is currently director of the KAUST AI Initiative, scientific director of the Swiss AI Lab IDSIA, and co-founder and chief scientist of AI company NNAISENSE, whose motto is ""AI∀"" which is a math-inspired way of saying ""AI For All."" He continues to work on cutting-edge AI technologies and applications to improve human health and extend human lives and make lives easier for everyone.

*The following interview has been edited for clarity.*

**Jones: Thank you Juergen for joining me. You have signed letters warning about AI weapons. But you didn't sign the recent publication, ""Pause Gigantic AI Experiments: An Open Letter""? Is there a reason?**

**Schmidhuber:** Thank you Hessie. Glad to speak with you. I have realized that many of those who warn in public against the dangers of AI are just seeking publicity. I don't think the latest letter will have any significant impact because many AI researchers, companies, and governments will ignore it completely.

The proposal frequently uses the word ""we"" and refers to ""us,"" the humans. But as I have pointed out many times in the past, there is no ""we"" that everyone can identify with. Ask 10 different people, and you will hear 10 different opinions about what is ""good."" Some of those opinions will be completely incompatible with each other. Don't forget the enormous amount of conflict between the many people.

The letter also says, ""*If such a pause cannot be quickly put in place, governments should intervene and impose a moratorium.*"" The problem is that different governments have ALSO different opinions about what is good for them and for others. Great Power A will say, if we don't do it, Great Power B will, perhaps secretly, and gain an advantage over us. The same is true for Great Powers C and D.

**Jones: Everyone acknowledges this fear surrounding current generative AI technology. Moreover, the existential threat of this technology has been publicly acknowledged by** [**Sam Altman**](https://www.bbc.com/news/world-us-canada-65616866)**, CEO of OpenAI himself, calling for AI regulation. From your perspective, is there an existential threat?**

**Schmidhuber:** It is true that AI can be weaponized, and I have no doubt that there will be all kinds of AI arms races, but AI does not introduce a new quality of existential threat. The threat coming from AI weapons seems to pale in comparison to the much older threat from nuclear hydrogen bombs that don’t need AI at all. We should be much more afraid of half-century-old tech in the form of H-bomb rockets. The Tsar Bomba of 1961 had almost 15 times more destructive power than all weapons of WW-II combined.  Despite the dramatic nuclear disarmament since the 1980s, there are still more than enough nuclear warheads to wipe out human civilization within two hours, without any AI I’m much more worried about that old existential threat than the rather harmless AI weapons.

**Jones: I realize that while you compare AI to the threat of nuclear bombs, there is a current danger that a current technology can be put in the hands of humans and enable them to “eventually” exact further harms to individuals of group in a very precise way, like targeted drone attacks. You are giving people a toolset that they've never had before, enabling bad actors, as some have pointed out, to be able to do a lot more than previously because they didn't have this technology.**

**Schmidhuber:** Now, all that sounds horrible in principle, but our existing laws are sufficient to deal with these new types of weapons enabled by AI. If you kill someone with a gun, you will go to jail. Same if you kill someone with one of these drones. Law enforcement will get better at understanding new threats and new weapons and will respond with better technology to combat these threats. Enabling drones to target persons from a distance in a way that requires some tracking and some intelligence to perform, which has traditionally been performed by skilled humans, to me, it seems is just an improved version of a traditional weapon, like a gun, which is, you know, a little bit smarter than the old guns.

But, in principle, all of that is not a new development. For many centuries, we have had the evolution of better weaponry and deadlier poisons and so on, and law enforcement has evolved their policies to react to these threats over time. So, it's not that we suddenly have a new quality of existential threat and it's much more worrisome than what we have had for about six decades. A large nuclear warhead doesn’t need fancy face recognition to kill an individual. No, it simply wipes out an entire city with ten million inhabitants.

**Jones: The existential threat that’s implied is the extent to which humans have control over this technology. We see some early cases of opportunism which, as you say, tends to get more media attention than positive breakthroughs. But you’re implying that this will all balance out?**

**Schmidhuber:** Historically, we have a long tradition of technological breakthroughs that led to advancements in weapons for the purpose of defense but also for protection. From sticks, to rocks, to axes to gunpowder to cannons to rockets… and now to drones… this has had a drastic influence on human history but what has been consistent throughout history is that those who are using technology to achieve their own ends are themselves, facing the same technology because the opposing side is learning to use it against them. And that's what has been repeated in thousands of years of human history and it will continue. I don't see the new AI arms race as something that is remotely as existential a threat as the good old nuclear warheads.

You said something important, in that some people prefer to talk about the downsides rather than the benefits of this technology, but that's misleading, because 95% of all AI research and AI development is about making people happier and advancing human life and health.

**Jones: Let’s touch on some of those beneficial advances in AI research that have been able to radically change present day methods and achieve breakthroughs.**

**Schmidhuber:** All right! For example, eleven years ago, our team with my postdoc Dan Ciresan was the first to win a [medical imaging competition through deep learning](https://people.idsia.ch/~juergen/first-time-deep-learning-won-medical-imaging-contest-september-2012.html). We analyzed female breast cells with the objective to determine harmless cells vs. those in the pre-cancer stage. Typically, a trained oncologist needs a long time to make these determinations. Our team, who knew nothing about cancer, were able to train an artificial neural network, which was totally dumb in the beginning, on lots of this kind of data. It was able to outperform all the other methods. Today, this is being used not only for breast cancer, but also for radiology and detecting plaque in arteries, and many other things.  Some of the neural networks that we have developed in the last 3 decades are now prevalent across thousands of healthcare applications, detecting Diabetes and Covid-19 and what not. This will eventually permeate across all healthcare. The good consequences of this type of AI are much more important than the click-bait new ways of conducting crimes with AI.

**Jones: Adoption is a product of reinforced outcomes. The massive scale of adoption either leads us to believe that people have been led astray, or conversely, technology is having a positive effect on people’s lives.**

**Schmidhuber:** The latter is the likely case. There's intense commercial pressure towards good AI rather than bad AI because companies want to sell you something, and you are going to buy only stuff you think is going to be good for you. So already just through this simple, commercial pressure, you have a tremendous bias towards good AI rather than bad AI. However, doomsday scenarios like in Schwarzenegger movies grab more attention than documentaries on AI that improve people’s lives.

**Jones: I would argue that people are drawn to good stories – narratives that contain an adversary and struggle, but in the end, have happy endings. And this is consistent with your comment on human nature and how history, despite its tendency for violence and destruction of humanity, somehow tends to correct itself.**

**Let’s take the example of a technology, which you are aware – GANs – General Adversarial Networks, which today has been used in applications for fake news and disinformation. In actuality, the purpose in the invention of GANs was far from what it is used for today.**

**Schmidhuber:** Yes, the name GANs was created in 2014 but we had the basic principle already in the early 1990s. More than 30 years ago, I called it *artificial curiosity*. It's a very simple way of injecting creativity into a little two network system. This creative AI is not just trying to slavishly imitate humans. Rather, it’s inventing its own goals. Let me explain:

You have two networks. One network is producing outputs that could be anything, any action. Then the second network is looking at these actions and it’s trying to predict the consequences of these actions. An action could move a robot, then something happens, and the other network is just trying to predict what will happen.

Now we can implement artificial curiosity by reducing the prediction error of the second network, which, at the same time, is the reward of the first network. The first network wants to maximize its reward and so it will invent actions that will lead to situations that will surprise the second network, which it has not yet learned to predict well.

In the case where the outputs are fake images, the first network will try to generate images that are good enough to fool the second network, which will attempt to predict the reaction of the environment: fake or real image, and it will try to become better at it. The first network will continue to also improve at generating images whose type the second network will not be able to predict. So, they fight each other. The 2nd network will continue to reduce its prediction error, while the 1st network will attempt to maximize it.

Through this zero-sum game the first network gets better and better at producing these convincing fake outputs which look almost realistic. So, once you have an interesting set of images by Vincent Van Gogh, you can generate new images that leverage his style, without the original artist having ever produced the artwork himself.

**Jones: I see how the Van Gogh example can be applied in an education setting and there are countless examples of artists mimicking styles from famous painters but image generation from this instance that can happen within seconds is quite another feat. And you know this is how GANs has been used. What’s more prevalent today is a socialized enablement of generating images or information to intentionally fool people. It also surfaces new harms that deal with the threat to intellectual property and copyright, where laws have yet to account for. And from your perspective this was not the intention when the model was conceived. What was your motivation in your early conception of what is now GANs?**

**Schmidhuber:** My old motivation for GANs was actually very important and it was not to create deepfakes or fake news but to enable AIs to be curious and invent their own goals, to make them explore their environment and make them creative.

Suppose you have a robot that executes one action, then something happens, then it executes another action, and so on, because it wants to achieve certain goals in the environment. For example, when the battery is low, this will trigger “pain” through hunger sensors, so it wants to go to the charging station, without running into obstacles, which will trigger other pain sensors. It will seek to minimize pain (encoded through numbers). Now the robot has a friend, the second network, which is a world model ––it’s a prediction machine that learns to predict the consequences of the robot’s actions.

Once the robot has a good model of the world, it can use it for planning. It can be used as a simulation of the real world. And then it can determine what is a good action sequence. If the robot imagines this sequence of actions, the model will predict a lot of pain, which it wants to avoid. If it plays this alternative action sequence in its mental model of the world, then it will predict a rewarding situation where it’s going to sit on the charging station and its battery is going to load again. So, it'll prefer to execute the latter action sequence.

In the beginning, however, the model of the world knows nothing, so how can we motivate the first network to generate experiments that lead to data that helps the world model learn something it didn’t already know? That’s what artificial curiosity is about. The dueling two network systems effectively explore uncharted environments by creating experiments so that over time the curious AI gets a better sense of how the environment works. This can be applied to all kinds of environments, and has medical applications.

**Jones: Let’s talk about the future. You have said, “*****Traditional humans won’t play a significant role in spreading intelligence across the universe.*****”**

**Schmidhuber:** Let’s first conceptually separate two types of AIs. The first type of AI are tools directed by humans. They are trained to do specific things like accurately detect diabetes or heart disease and prevent attacks before they happen. In these cases, the goal is coming from the human. More interesting AIs are setting their own goals. They are inventing their own experiments and learning from them. Their horizons expand and eventually they become more and more general problem solvers in the real world. They are not controlled by their parents, but much of what they learn is through self-invented experiments.

A robot, for example, is rotating a toy, and as it is doing this, the video coming in through the camera eyes, changes over time and it begins to learn how this video changes and learns how the 3D nature of the toy generates certain videos if you rotate it a certain way, and eventually, how gravity works, and how the physics of the world works. Like a little scientist!

And I have predicted for decades that future scaled-up versions of such AI scientists will want to further expand their horizons, and eventually go where most of the physical resources are, to build more and bigger AIs. And of course, almost all of these resources are far away from earth out there in space, which is hostile to humans but friendly to appropriately designed AI-controlled robots and self-replicating robot factories. So here we are not talking any longer about our tiny biosphere; no, we are talking about the much bigger rest of the universe.  Within a few tens of billions of years, curious self-improving [AIs will colonize the visible cosmos](https://blogs.scientificamerican.com/observations/falling-walls-the-past-present-and-future-of-artificial-intelligence/) in a way that’s infeasible for humans. Those who don’t won’t have an impact. Sounds like science fiction, but since the 1970s I have been unable to see a plausible alternative to this scenario, except for a global catastrophe such as an all-out nuclear war that stops this development before it takes off.

**Jones: How long have these AIs, which can set their own goals — how long have they existed? To what extent can they be independent of human interaction?**

**Schmidhuber:** Neural networks like that have existed for over 30 years. My first simple adversarial neural network system of this kind is the one from 1990 described above. You don’t need a teacher there; it's just a little agent running around in the world and trying to invent new experiments that surprise its own prediction machine.

Once it has figured out certain parts of the world, the agent will become bored and will move on to more exciting experiments. The simple 1990 systems I mentioned have certain limitations, but in the past three decades, we have also built more [sophisticated systems that are setting their own goals](https://people.idsia.ch/~juergen/artificial-curiosity-since-1990.html) and such systems I think will be essential for achieving true intelligence. If you are only imitating humans, you will never go beyond them. So, you really must give AIs the freedom to explore previously unexplored regions of the world in a way that no human is really predefining.

**Jones: Where is this being done today?**

**Schmidhuber:** Variants of neural network-based artificial curiosity are used today for agents that learn to play video games in a human-competitive way. We have also started to use them for automatic design of experiments in fields such as materials science. I bet many other fields will be affected by it: chemistry, biology, drug design, you name it. However, at least for now, these artificial scientists, as I like to call them, cannot yet compete with human scientists.

I don’t think it’s going to stay this way but, at the moment, it’s still the case.  Sure, AI has made a lot of progress. Since 1997, there have been superhuman chess players, and since 2011, through the DanNet of my team, there have been [superhuman visual pattern recognizers](https://people.idsia.ch/~juergen/DanNet-triggers-deep-CNN-revolution-2011.html). But there are other things where humans, at the moment at least, are much better, in particular, science itself.  In the lab we have many first examples of self-directed artificial scientists, but they are not yet convincing enough to appear on the radar screen of the public space, which is currently much more fascinated with simpler systems that just imitate humans and write texts based on previously seen human-written documents.

**Jones: You speak of these numerous instances dating back 30 years of these lab experiments where these self-driven agents are deciding and learning and moving on once they’ve learned. And I assume that that rate of learning becomes even faster over time. What kind of timeframe are we talking about when this eventually is taken outside of the lab and embedded into society?**

**Schmidhuber:** This could still take months or even years :-) Anyway, in the not-too-distant future, we will probably see artificial scientists who are good at devising experiments that allow them to discover new, previously unknown physical laws.

As always, we are going to profit from the old trend that has held at least since 1941: every decade compute is getting 100 times cheaper.

**Jones: How does this trend affect modern AI such as ChatGPT?**

**Schmidhuber:** Perhaps you know that all the recent famous AI applications such as ChatGPT and similar models are largely based on principles of artificial neural networks invented in the previous millennium. The main reason why they works so well now is the incredible acceleration of compute per dollar.

ChatGPT is driven by a neural network called “Transformer” described in 2017 by Google. I am happy about that because a quarter century earlier in 1991 I had a particular Transformer variant which is now called the “[Transformer with linearized self-attention](https://twitter.com/SchmidhuberAI/status/1576966129993797632?cxt=HHwWgMDSkeKVweIrAAAA)”. Back then, not much could be done with it, because the compute cost was a million times higher than today. But today, one can train such models on half the internet and achieve much more interesting results.

**Jones: And for how long will this acceleration continue?**

**Schmidhuber:** There's no reason to believe that in the next 30 years, we won't have another factor of 1 million and that's going to be really significant. In the near future, for the first time we will have many not-so expensive devices that can compute as much as a human brain. The physical limits of computation, however, are much further out so even if the trend of a factor of 100 every decade continues, the physical limits (of 1051 elementary instructions per second and kilogram of matter) won’t be hit until, say, the mid-next century. Even in our current century, however, we’ll probably have many machines that compute more than all 10 billion human brains collectively and you can imagine, everything will change then!

**Jones: That is the big question. Is everything going to change? If so, what do you say to the next generation of leaders, currently coming out of college and university. So much of this change is already impacting how they study, how they will work, or how the future of work and livelihood is defined. What is their purpose and how do we change our systems so they will adapt to this new version of intelligence?**

**Schmidhuber:** For decades, people have asked me questions like that, because you know what I'm saying now, I have basically said since the 1970s, it’s just that today, people are paying more attention because, back then, they thought this was science fiction.

They didn't think that I would ever come close to achieving my crazy life goal of building a machine that learns to become smarter than myself such that I can retire. But now many have changed their minds and think it's conceivable. And now I have two daughters, 23 and 25. People ask me: what do I tell them? They know that Daddy always said, “*It seems likely that within your lifetimes, you will have new types of intelligence that are probably going to be superior in many ways, and probably all kinds of interesting ways.*” How should they prepare for that? And I kept telling them the obvious: **Learn how to learn new things**! It's not like in the previous millennium where within 20 years someone learned to be a useful member of society, and then took a job for 40 years and performed in this job until she received her pension. Now things are changing much faster and we must learn continuously just to keep up. I also told my girls that no matter how smart AIs are going to get, learn at least the basics of math and physics, because that’s the essence of our universe, and anybody who understands this will have an advantage, and learn all kinds of new things more easily. I also told them that social skills will remain important, because most future jobs for humans will continue to involve interactions with other humans, but I couldn’t teach them anything about that; they know much more about social skills than I do.

You touched on the big philosophical question about people’s purpose. Can this be answered without answering the even grander question: What’s the purpose of the entire universe?

We don’t know. But what’s happening right now might be connected to the unknown answer. Don’t think of humans as the crown of creation. Instead view human civilization as part of a much grander scheme, an important step (but not the last one) on the path of the universe from very simple initial conditions towards more and more unfathomable complexity. Now it seems ready to take its [next step, a step comparable to the invention of life itself over 3.5 billion years ago](https://people.idsia.ch/~juergen/deep-learning-history.html#future).  Alas, don’t worry, in the end, all will be good!

**Jones: Let’s get back to this transformation happening right now with OpenAI. There are many questioning the efficacy and accuracy of ChatGPT, and are concerned its release has been premature. In light of the rampant adoption, educators have banned its use over concerns of plagiarism and how it stifles individual development. Should large language models like ChatGPT be used in school?**

**Schmidhuber:** When the calculator was first introduced, instructors forbade students from using it in school. Today, the consensus is that kids should learn the basic methods of arithmetic, but they should also learn to use the “artificial multipliers” aka calculators, even in exams, because laziness and efficiency is a hallmark of intelligence. Any intelligent being wants to minimize its efforts to achieve things.

And that's the reason why we have tools, and why our kids are learning to use these tools. The first stone tools were invented maybe 3.5 million years ago; tools just have become more sophisticated over time. In fact, humans have changed in response to the properties of their tools. Our anatomical evolution was shaped by tools such as spears and fire. So, it's going to continue this way. And there is no permanent way of preventing large language models from being used in school.

**Jones: And when our children, your children graduate, what does their future work look like?**

**Schmidhuber:** A single human trying to predict details of how 10 billion people and their machines will evolve in the future is like a single neuron in my brain trying to predict what the entire brain and its tens of billions of neurons will do next year. 40 years ago, before the WWW was created at CERN in Switzerland, who would have predicted all those young people making money as YouTube video bloggers?

Nevertheless, let’s make a few limited job-related observations. For a long time, people have thought that desktop jobs may require more intelligence than skills trade or handicraft professions. But now, it turns out that it's much easier to replace certain aspects of desktop jobs than replacing a carpenter, for example. Because everything that works well in AI is happening behind the screen currently, but not so much in the physical world.

There are now artificial systems that can read lots of documents and then make really nice summaries of these documents. That is a desktop job. Or you give them a description of an illustration that you want to have for your article and pretty good illustrations are being generated that may need some minimal fine-tuning. But you know, all these desktop jobs are much easier to facilitate than the real tough jobs in the physical world. And it's interesting that the things people thought required intelligence, like playing chess, or writing or summarizing documents, are much easier for machines than they thought. But for things like playing football or soccer, there is no physical robot that can remotely compete with the abilities of a little boy with these skills. So, AI in the physical world, interestingly, is much harder than AI behind the screen in virtual worlds. And it's really exciting, in my opinion, to see that jobs such as plumbers are much more challenging than playing chess or writing another tabloid story.

**Jones: The way data has been collected in these large language models does not guarantee personal information has not been excluded. Current consent laws already are outdated when it comes to these large language models (LLM). The concern, rightly so, is increasing surveillance and loss of privacy. What is your view on this?**

**Schmidhuber:** As I have indicated earlier: are surveillance and loss of privacy inevitable consequences of increasingly complex societies? Super-organisms such as cities and states and companies consist of numerous people, just like people consist of numerous cells. These cells enjoy little privacy. They are constantly monitored by specialized ""police cells"" and ""border guard cells"": Are you a cancer cell? Are you an external intruder, a pathogen? Individual cells sacrifice their freedom for the benefits of being part of a multicellular organism.

Similarly, for super-organisms such as nations. Over 5000 years ago, writing enabled recorded history and thus became its inaugural and most important invention. Its initial purpose, however, was to facilitate surveillance, to track citizens and their tax payments. The more complex a super-organism, the more comprehensive its collection of information about its constituents.

200 years ago, at least, the parish priest in each village knew everything about all the village people, even about those who did not confess, because they appeared in the confessions of others. Also, everyone soon knew about the stranger who had entered the village, because some occasionally peered out of the window, and what they saw got around. Such control mechanisms were temporarily lost through anonymization in rapidly growing cities but are now returning with the help of new surveillance devices such as smartphones as part of digital nervous systems that tell companies and governments a lot about billions of users. Cameras and drones etc. are becoming increasingly tinier and more ubiquitous. More effective recognition of faces and other detection technology are becoming cheaper and cheaper, and many will use it to identify others anywhere on earth; the big wide world will not offer any more privacy than the local village. Is this good or bad? Some nations may find it easier than others to justify more complex kinds of super-organisms at the expense of the privacy rights of their constituents.

**Jones: So, there is no way to stop or change this process of collection, or how it continuously informs decisions over time? How do you see governance and rules responding to this, especially amid** [**Italy’s ban on ChatGPT following**](https://www.cnbc.com/2023/04/04/italy-has-banned-chatgpt-heres-what-other-countries-are-doing.html) **suspected user data breach and the more recent news about the** [**Meta’s record $1.3billion fine**](https://www.reuters.com/technology/facebook-given-record-13-bln-fine-given-5-months-stop-eu-us-data-flows-2023-05-22/) **in the company’s handling of user information?**

**Schmidhuber:** Data collection has benefits and drawbacks, such as the loss of privacy. How to balance those? I have argued for addressing this through data ownership in data markets. If it is true that data is the new oil, then it should have a price, just like oil. At the moment, the major surveillance platforms such as Meta do not offer users any money for their data and the transitive loss of privacy. In the future, however, we will likely see attempts at creating efficient data markets to figure out the data's true financial value through the interplay between supply and demand.

Even some of the sensitive medical data should not be priced by governmental regulators but by patients (and healthy persons) who own it and who may sell or license parts thereof as micro-entrepreneurs in a healthcare data market.

Following a previous [interview](https://www.swissre.com/institute/conferences/The-intelligence-behind-artificial-intelligence.html), I gave for one of the largest re-insurance companies , let's look at the different participants in such a data market: patients, hospitals, data companies. (1) **Patients** with a rare form of cancer can offer more valuable data than patients with a very common form of cancer. (2) **Hospitals** and their machines are needed to extract the data, e.g., through magnet spin tomography, radiology, evaluations through human doctors, and so on. (3) **Companies** such as Siemens, Google or IBM would like to buy annotated data to make better artificial neural networks that learn to predict pathologies and diseases and the consequences of therapies. Now the market’s invisible hand will decide about the data’s price through the interplay between demand and supply. On the demand side, you will have several companies offering something for the data, maybe through an app on the smartphone (a bit like a stock market app). On the supply side, each patient in this market should be able to profit from high prices for rare valuable types of data. Likewise, competing data extractors such as hospitals will profit from gaining recognition and trust for extracting data well at a reasonable price. The market will make the whole system efficient through incentives for all who are doing a good job. Soon there will be a flourishing ecosystem of commercial data market advisors and what not, just like the ecosystem surrounding the traditional stock market. The value of the data won’t be determined by governments or ethics committees, but by those who own the data and decide by themselves which parts thereof they want to license to others under certain conditions.

At first glance, a market-based system seems to be detrimental to the interest of certain monopolistic companies, as they would have to pay for the data - some would prefer free data and keep their monopoly. However, since every healthy and sick person in the market would suddenly have an incentive to collect and share their data under self-chosen anonymity conditions, there will soon be many more useful data to evaluate all kinds of treatments. On average, people will live longer and healthier, and many companies and the entire healthcare system will benefit.

**Jones: Finally, what is your view on open source versus the private companies like Google and OpenAI? Is there a danger to supporting these private companies’ large language models versus trying to keep these models open source and transparent, very much like what LAION is doing?**

**Schmidhuber:** I signed this [open letter by LAION](https://www.forbes.com/sites/hessiejones/2023/04/19/amid-growing-call-to-pause-ai-research-laion-petitions-governments-to-keep-agi-research-open-active-and-responsible/?sh=6973c08b62e3) because I strongly favor the open-source movement. And I think it's also something that is going to challenge whatever big tech dominance there might be at the moment. Sure, the best models today are run by big companies with huge budgets for computers, but the exciting fact is that open-source models are not so far behind, some people say maybe six to eight months only. Of course, the private company models are all based on stuff that was created in academia, often in little labs without so much funding, which publish without patenting their results and open source their code and others take it and improved it.

Big tech has profited tremendously from academia; their main achievement being that they have scaled up everything greatly, sometimes even failing to credit the original inventors.

So, it's very interesting to see that as soon as some big company comes up with a new scaled-up model, lots of students out there are competing, or collaborating, with each other, trying to come up with equal or better performance on smaller networks and smaller machines. And since they are open sourcing, the next guy can have another great idea to improve it, so now there’s tremendous competition also for the big companies.

Because of that, and since AI is still getting exponentially cheaper all the time, I don't believe that big tech companies will dominate in the long run. They find it very hard to compete with the enormous open-source movement. As long as you can encourage the open-source community, I think you shouldn't worry too much. Now, of course, you might say if everything is open source, then the bad actors also will more easily have access to these AI tools. And there's truth to that. But as always since the invention of controlled fire, it was good that knowledge about how technology works quickly became public such that everybody could use it. And then, against any bad actor, there's almost immediately a counter actor trying to nullify his efforts. You see, I still believe in our old motto ""AI∀"" or ""AI For All.""

**Jones: Thank you, Juergen for sharing your perspective on this amazing time in history. It’s clear that with new technology, the enormous potential can be matched by disparate and troubling risks which we’ve yet to solve, and even those we have yet to identify. If we are to dispel the fear of a sentient system for which we have no control, humans, alone need to take steps for more responsible development and collaboration to ensure AI technology is used to ultimately benefit society. Humanity will be judged by what we do next.**"
91,machinelearning,openai,top,2016-12-17 23:23:48,[D] Hope this becomes a norm for the field,rantana,False,0.94,247,5ixaf5,https://www.reddit.com/r/MachineLearning/comments/5ixaf5/d_hope_this_becomes_a_norm_for_the_field/,26,1482017028.0,"Not sure if this is too meta for /r/MachineLearning, but /u/badmephisto (Andrej Karpathy) from OpenAI just posted trained weights for a model at a users request in less than 3 hours:

https://www.reddit.com/r/MachineLearning/comments/5ivho9/p_tensorflow_implementation_of_pixelcnn_openai/dbblpzl/?context=10000

Just wanted to say the amount of openness and collaboration from this community is fantastic."
92,machinelearning,openai,top,2022-12-04 09:57:44,[D] OpenAI’s ChatGPT is unbelievable good in telling stories!,Far_Pineapple770,False,0.91,243,zc5sg6,https://www.reddit.com/r/MachineLearning/comments/zc5sg6/d_openais_chatgpt_is_unbelievable_good_in_telling/,85,1670147864.0,"
I started playing with ChatGPT, the new chatbot created by OpenAI and is free to use at the moment. 

I asked it to describe a scene between Batman and Joker on a subway platform and the result is mind blowing! I let you have a read of the story yourself:

Generated by ChatGPT:

The scene is set in a dimly lit subway station, with the Joker standing on the platform, gazing up at Batman who is perched on a nearby overhead beam. The Joker is dressed in his signature purple suit and green hair, a wicked grin plastered across his face.

In his hand, he holds a deck of playing cards, flipping them expertly through his fingers. The sound of the cards shuffling echoes through the empty station.

Batman, dressed in his dark suit and cape, stares down at the Joker with a steely gaze. His muscles are tense, ready for any sudden movements from the villain.

The Joker cackles, his laughter bouncing off the walls of the station. ""Hello, Batsy! Long time no see. How's the crime-fighting business these days?""

Batman remains silent, watching the Joker carefully.

The Joker's grin widens. ""Come on, Bats. Don't be so serious all the time. Let's have some fun."" He tosses a card at Batman, who easily catches it.

""I'm not here to play games, Joker,"" Batman says in a grave tone.

The Joker's laughter turns into a maniacal cackle. ""But that's the whole point, Batsy! Life is a game. And I'm the master of it.""

He pulls out a bomb from his pocket and holds it up, the timer ticking down. ""Let's see how well you can play, shall we?""

Batman leaps down from the beam, landing gracefully on the platform. He advances towards the Joker, who backs away, still holding the bomb.

The tension in the air is palpable as the two foes face off, each ready to outsmart the other. The sound of the bomb's timer counts down, adding to the tension of the scene.

The Joker's grin never fades, even as Batman inches closer. ""It's always a pleasure to play with you, Batsy. But I think it's time for me to make my exit.""

With a flourish, the Joker disappears in a cloud of smoke, leaving Batman standing alone on the platform. The bomb detonates, causing a small explosion and sending debris flying.

But Batman remains unharmed, ready to continue the never-ending battle against the Joker and all the evil that plagues the city.

End of the story.

I used the first two paragraphs in OpenAI’s Dall E 2 to generate an image and [this](https://labs.openai.com/s/p8L8Z6c3S7qZ26U7owfbx7lS) is the result I got.

Please keep in mind that this is the very beginning of this tool and think about the endless possibilities it can create."
93,machinelearning,openai,top,2019-04-24 14:39:10,[D] Have we hit the limits of Deep Reinforcement Learning?,AnvaMiba,False,0.87,232,bgvefd,https://www.reddit.com/r/MachineLearning/comments/bgvefd/d_have_we_hit_the_limits_of_deep_reinforcement/,106,1556116750.0,"As per [this thread](https://www.reddit.com/r/MachineLearning/comments/bfq8v9/d_openai_five_vs_humans_currently_at_410633_992/) and [this tweet](https://twitter.com/sherjilozair/status/1119256767798620161?s=20), Open AI Five was trained on something like 45,000 years of gameplay experience, and it took less than one day for humans to figure out strategies to consistently beat it.

Open AI Five, together with AlphaStar, is the largest and most sophisiticated implementation of DRL, and yet it falls short of human intelligence by this huge margin. And I bet that AlphaStar would succumb to the same fate if they released it as a bot for anybody to play with.


I know there is lots of research going on to make DRL more data efficient, and to make deep learning in general more robust to out-of-distribution and adversarial examples, but the gap with humans here is so extreme that I doubt it can be meaningfully closed by anything short of a paradigm shift.

What are your thoughts? Is this the limit of what can be achieved by DRL, or is there still hope to push the paradigm foward?"
94,machinelearning,openai,top,2016-12-29 15:48:28,[D] r/MachineLearning's 2016 Best Paper Award!,Mandrathax,False,0.93,237,5kxfkb,https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/,96,1483026508.0,"***EDIT : I will be announcing the results on monday 1/9***

***EDIT 2 : maybe 1/10 then because of travel issues irl, sorry about that***

---

Hi guys!

Welcome to /r/MachineLearning's 2016 Best Paper Award!

The idea is to have a community-wide vote for the best papers of this year.

I hope you find this to be a good idea, mods please tell me if this breaks any rules/if you had something like this in store.

---

## How does it work?

**Nominate** by commenting on the dedicatd top level comments. Please provide a (paywall free) link. Feel free to justify your choice. Also if you're one of the author, be courteous and indicate it.

**Vote** by upvoting the nominees.

The **results** will be announced **by the end of next week** (6-7th of Jan.). Depending on the participation/interest I might change it.

It's that simple!

There are some simple rules to make sure everything runs smoothly, you can find them below, please read them before commenting.

---

## Categories

- [Best Paper of the year](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbrapti/)

> No rules! Any research paper you feel had the greatest impact/had top writing, any criterion is good.

- [Best student paper](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbraqkv/)

> Papers from a student, grad/undergrad/highschool, everyone who doesn't have a phd and goes to school. The student must be first author of course. Provide evidence if possible.

- [Best paper name](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbrar08/)

> Try to beat [this](http://www.oneweirdkerneltrick.com/spectral.pdf)

- [Best paper from academia](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbraroz/)

> Papers where the first author is from a university / a state research organization (eg INRIA in France). 

- [Best paper from the industry](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbrasnz/)

> Great paper from a multi-billion tech company (or more generally a research lab sponsored by privat funds, eg. openai)

- [Best rejected paper](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbrat9t/)

> A chance of redemption for good papers that didn't make it trough peer review. Please provide evidence that the paper was rejected if possible.

- [Best unpublished preprint](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbratyb/)

> A category for those yet to be published (e.g. papers from the end of the year). This may or may not be redundant with the rejected paper category, we'll see.

- [Best theoretical paper](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbrauan/)

> Keep the math coming

- [Best non Deep Learning paper](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbraumv/)

> Because gaussian processes, random forests and kernel methods deserve a chance amid the DL hype train

---

## Rules

1. Only one nomination by comment. You can nominate multiple papers in different comments/categories.
2. Nominations should include a **link to the paper**. In case of an arxiv link, please link to the arxiv page and not the pdf directly. Please do not link paywalled articles.
3. Only **research paper** are to be nominated. This means no book, no memo or no tutorial/blog post for instance. This could be adressed in a separate award or category if there is enough demand.
4. For the sake of clarity, there are some rules on commenting :
 - ***Do NOT comment on the main thread***. For discussion, use the [*discussion* thread](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbrap6u/)
 - ***Please ONLY comment the other threads with nominations***. You can discuss individual nominations in child comments. However 1rst level comments on each thread should be nominations only.
5. Respect reddit and this sub's rules.

I am not a mod so I have no way of enforcing these rules, please follow them to keep the thread clear. Of course, suggestions are welcome [here](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbrap6u/).

---

That's it, have fun!"
95,machinelearning,openai,top,2019-04-19 16:26:05,[P] Python package to easily retrain OpenAI's GPT-2 text-generating model on new texts + Colaboratory Notebook to use it w/ GPU for free,minimaxir,False,0.94,234,bf137p,https://www.reddit.com/r/MachineLearning/comments/bf137p/p_python_package_to_easily_retrain_openais_gpt2/,56,1555691165.0,"Hi all! I just open-sourced a [Python package on GitHub](https://github.com/minimaxir/gpt-2-simple) that lets you retrain the smaller GPT-2 model on your own text with minimal code! (and without fussing around with the CLI like the original repo)

I have also made a [Colaboratory Notebook](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) which handles both training w/ a GPU **for free** and file I/O to the notebook (which with GPT-2 is a tad tricker).

Let me know if you have any questions! I plan on releasing more demos soon!"
96,machinelearning,openai,top,2023-12-01 16:31:39,"[P] 80% faster, 50% less memory, 0% loss in accuracy Llama finetuning",danielhanchen,False,0.97,226,188g31r,https://www.reddit.com/r/MachineLearning/comments/188g31r/p_80_faster_50_less_memory_0_loss_in_accuracy/,36,1701448299.0,"Hey [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)!

I  manually derived backpropagation steps, did some chained matrix  multiplication optims, wrote all kernels in OpenAI's Triton language and  did more maths and coding trickery to make QLoRA finetuning for Llama  5x faster on Unsloth: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)! Some highlights:

* **5x faster** (5 hours to 1 hour)
* Use **50% less memory**
* With **0% loss in accuracy**
* All **locally** on NVIDIA GPUs (Tesla T4, RTX 20/30/40, Ampere, Hopper) for **free**!
* QLoRA / LoRA is now 80% faster to train.

On  Slim Orca 518K examples on 2 Tesla T4 GPUs via DDP, Unsloth trains 4bit  QLoRA on all layers in 260 hours VS Huggingface's original  implementation of 1301 hours.

[Slim Orca 1301 hours to 260 hours](https://preview.redd.it/54qzkb66np3c1.png?width=1000&format=png&auto=webp&s=5f6fd95482263cbdca225415af91d49342bea10e)

You might (most likely not) remember me from Hyperlearn ([https://github.com/danielhanchen/hyperlearn](https://github.com/danielhanchen/hyperlearn)) which I launched a few years back to make ML algos 2000x faster via maths and coding tricks.

I wrote up a blog post about all the manual hand derived backprop via [https://unsloth.ai/introducing](https://unsloth.ai/introducing).

I wrote a Google Colab for T4 for Alpaca: [https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing](https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing) which finetunes Alpaca 2x faster on a single GPU.

On Kaggle via 2 Tesla T4s on DDP: [https://www.kaggle.com/danielhanchen/unsloth-laion-chip2-kaggle](https://www.kaggle.com/danielhanchen/unsloth-laion-chip2-kaggle), finetune LAION's OIG 5x faster and Slim Orca 5x faster.

You can install Unsloth all locally via:

    pip install ""unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git""
    
    pip install ""unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git""  

Currently we only support Pytorch 2.1 and Linux distros - more installation instructions via [https://github.com/unslothai/unsloth/blob/main/README.md](https://github.com/unslothai/unsloth/blob/main/README.md)

I hope to:

1. Support other LLMs other than Llama style models (Mistral etc)
2. Add sqrt gradient checkpointing to shave another 25% of memory usage.
3. And other tricks!

Thanks a bunch!!"
97,machinelearning,openai,top,2019-02-17 22:32:00,[D] OpenAI Not Releasing Fully Trained Model - Is it just an exercise in delaying the inevitable.,JamieMcLachlan,False,0.92,224,arpysz,https://www.reddit.com/r/MachineLearning/comments/arpysz/d_openai_not_releasing_fully_trained_model_is_it/,119,1550442720.0,OpenAI has gone against their standard practice of releasing a fully trained model and instead has released a smaller model for experimentation. They have stated that it is out of fear of it being utilized by people with malicious intent. Do people think that eventually technology like this will end up in the hands of people with malicious intent and if so is this just an exercise in delaying the inevitable?  
98,machinelearning,openai,top,2023-02-09 07:58:42,[P] Get 2x Faster Transcriptions with OpenAI Whisper Large on Kernl,pommedeterresautee,False,0.96,224,10xp54e,https://www.reddit.com/r/MachineLearning/comments/10xp54e/p_get_2x_faster_transcriptions_with_openai/,34,1675929522.0,"We are happy to announce the support of OpenAI Whisper model (ASR task) on Kernl. 

We focused on high quality transcription in a latency sensitive scenario, meaning:

* *whisper-large-v2* weights
* *beam search 5 (as recommended in the related paper)*

We measured a 2.3x speedup on Nvidia A100 GPU (2.4x on 3090 RTX) compared to Hugging Face implementation using FP16 mixed precision on transcribing librispeech test set (over 2600 examples). For now, OpenAI implementation is [not yet PyTorch 2.0 compliant](https://github.com/openai/whisper/pull/115).

In the post below, we discuss what worked (CUDA Graph), our tricks (to significantly reduce memory footprint), and what did not pay off (Flash attention and some other custom Triton kernels).

* **Kernl repository**: [https://github.com/ELS-RD/kernl](https://github.com/ELS-RD/kernl)
* **Reproduction script**: [https://github.com/ELS-RD/kernl/blob/main/experimental/whisper/speedup.ipynb](https://github.com/ELS-RD/kernl/blob/main/experimental/whisper/speedup.ipynb)

# Unsung hero: CUDA graphs

CUDA graphs technology provides most of the speed up. Compared to vanilla PyTorch 2.0 (“reduce-overhead mode”), we provide a limited memory footprint when vanilla PyTorch 2.0 may raise OOM exception.

[memory footprint](https://preview.redd.it/jyfayud5d4ha1.png?width=1598&format=png&auto=webp&s=15356368ce60eb5e748d21239ac0ecc00bd403ac)

Experiments have been run on a 3090 RTX with 24 Gb DDR. A reminder that PyTorch 2.0 focuses on training, not inference, which may explain why it OOMs rapidly in this case.

At its beginning, many partitioners were surprised by PyTorch eager mode performances, when compared to TensorFlow 1.x compiled models: they were on par! Python brought its flexibility and ease of debugging without implying any significant performance cost.

This is mostly because GPUs are latency hiding hardware: when PyTorch launches an operation on GPU, it sends instructions from host (CPU) to a queue (the CUDA stream), which allows PyTorch to continue Python script execution without having to wait for CUDA kernel to finish its work. This strategy effectively hides most of the Python overhead, in particular when there are some computation costly operations like convolutions or matrix multiplications.

Each new generation of GPUs being much faster than its predecessor, this strategy could not last forever, according to one PyTorch maintainer, it is an “existential problem” ([dev podcast](https://pytorch-dev-podcast.simplecast.com/episodes/pytorch-20), around 8mn30).

In inference mode, especially in latency-sensitive scenarios where batch size tends to be low, there is often little computation to perform (regarding what modern GPUs can do), making it even harder to hide effectively Python overhead. It’s accentuated in the case of generative models like Whisper, because each decoder call focuses on generating a single token, and a part of the computation is cached for the next token.

This is a typical situation where CUDA graph is very helpful.

The main idea behind CUDA graph is that we can replace a series of instructions sent from host (CPU) to device (GPU) by one call referring to a graph of instruction stored in GPU. Check also this twitter [thread](https://twitter.com/cHHillee/status/1616906059368763392) for more explanations.

First it will observe the inference of a model for specific input shapes and then replay it without going through most of the Python code.

One constraint is that it will replay the exact same operations with the exact same arguments.

For instance, memory addresses used by kernels are captured and therefore need to be static. For input tensors, it means that for each inference, we need to allocate some GPU memory and copy them there before the capture and copy all the following input tensors at the very same place.

The second constraint is that dynamic shapes are not supported by CUDA graph because it captures everything. We could have our own machinery in front of the model, but PyTorch 2.0 offers the right tooling to manage that point out of the box.

Basically, dynamo offers a mechanism which checks if the model has already been captured for specific input shapes and some other states and capture it if not yet the case. You just have to provide a function which converts to CUDA graphs and you are done.

Out of the box, PyTorch 2.0 provides a “reduce-overhead” mode which applies CUDA graph to the model. Unfortunately, for now, it will raise an OOM with Whisper large or medium because it reserves some CUDA space for each input shape. Therefore, for a generative model it rapidly fulfills the GPU memory, in particular because of the K/V cache which can be huge.

We have worked around this constrain by building our own layer on top of the memory pool of PyTorch. 

Basically, a PyTorch tensor is made of 2 parts, a CUDA allocated memory represented by PyTorch as a “storage”, and a bunch of metadata associated with it. Among the metadata there is a CUDA memory address, the tensor shape plus its strides, its dtype and... a memory offset.

Our idea is to create a very large tensor and share its storage between several input tensors, using offset metadata. With this solution, we avoid specializing in input tensor shapes and share the reserved memory for different input shapes related to several CUDA graphs.

As shown in the table above, it significantly reduces the memory overhead.

# What about custom (Triton) kernels for attention?

**TL; DR: we tried, they work, we got up to 2 times faster than eager PyTorch for cross attention and they bring close to nothing in e2e latency mostly because the improvement is not big enough to matter 🙁**

Below, we follow the convention of naming Q, K and V, the 3 tensors used in the attention of transformer models.

Whisper is based on a classic transformer architecture, with an encoder and a decoder.

Two characteristics of this model are of interest:

* The shape of Q tensor used in cross-attention is always \[batch, #heads, 1, 1500\].
* Model has been trained on 30-second audio files and their associated transcript. Because audio files are short, the sequence to generate is usually short, fewer than 50 tokens most of the time.

Because of these characteristics, optimizing attention has a low reward. In particular, the now common trick “replace attention with flash attention” is counterproductive:

* self-attention: sequences are very short, so quadratic complexity is less of an issue;
* cross-attention: using flash-attention leads to a 2 times slower inference on this part of the model.

We have tried to work on the second point and thought we could make cross attention faster.

Usual attention implementation (self and cross) relies on a series of operations: matmul (Q x K\^t) -> rescale -> SoftMax -> matmul (SoftMax output x V). Intermediate output tensors have a shape which usually scales quadratically with input sequence length. They will be saved and reloaded from DDR, and memory bandwidth is a very scarce resource in GPUs.

To optimize speed, flash attention fuses operations, so basically first matmul will work on a small part of Q and K, and directly apply SoftMax to it without saving intermediate results to DDR. Same for second matmul. Because we don't go and back through GPU main memory, flash attention usually runs much faster than naïve implementation of attention.

The parallelization of the jobs is done on different axes: [batch and attention head for the original flash attention](https://github.com/HazyResearch/flash-attention/issues/40), and Triton author added a third one, tokens, aka third dimension of Q (this important trick is now also part of flash attention CUDA implementation).

In the Whisper latency sensitive case, this doesn’t work well. The size of batches is low and sequence length (third dimension of Q tensor) is... 1! So, even if each job is done very efficiently, our GPU occupancy is low, and basically most of its streaming processors are idle. At the end of the day, the FA kernel is up to 2 times slower than eager PyTorch implementation (depending on batch size and model size).

# Try 1: the very simple kernel

We noted that there is little computation to do and that we were memory bandwidth bounded. It means that most of the time we wait for data to be transferred from main memory to shared memory. 

We leveraged that fact in a very simple kernel with 2 optimizations:

* after having finishing the rescale of the QK\^t matmul, we perform the SoftMax computation in parallel of loading V tensor for the final matmul. The SoftMax computation finishes before the end of the V loading, so basically it costs us nothing;
* to achieve best performances, we also changed the memory layout of V tensor in a way where we get a coalesced access, so we lowered the pressure on the memory bandwidth and increased instruction throughput (coalesced access let you load up to 128 bytes in a single instruction so you need less of them, which lets you perform more other things)

Altogether this cross attention was up to 2x faster compared to eager. It appeared to bring between 5 to 20% in end-to-end benchmark depending on model size and batch size. Cool but far from being a game changer, it requires a modification specific to Whisper model (memory layout of V) which is not in the spirit of the Kernl library. We decided to search for another way of doing things (we kept the code in the library for possible future use case).

# Try 2: Skinny Flash Attention

Our second try is based on the very same trick as Flash Attention (parallel SoftMax) but is designed for tall and skinny tensors, which is inspired by split-k strategy in GEMM (a close cousin of the matmul). The main idea is to add a new parallelization axis over the 3rd dimension of K tensor. The next steps are in the same spirit as flash attention with a difference that we need a new reduction operation between the different jobs' outputs. It provides 5-10% speedup compared to eager implementation on this setup at kernel level. We kept that kernel to ease the next feature we are working on (quantization) but the effect in end-to-end latency is inferior to 5% (still it exists 😅).

Some thoughts about PyTorch 2.0, Triton and making things much faster

Playing with PyTorch ~~1.14~~ 2.0 since this summer made us quite convinced that the major update to be released very soon will be a game changer for the ML field.

For inference (but also for training), the parallel with PyTorch vs TensorFlow is obvious to our eyes. 

The traditional way to deploy a model is to export it to Onnx, then to TensorRT plan format. Each step requires its own tooling, its own mental model, and may raise some issues. The most annoying thing is that you need Microsoft or Nvidia support to get the best performances, and sometimes model support takes time. For instance, T5, a model released in 2019, is not yet correctly supported on TensorRT, in particular K/V cache is missing ([soon it will be according to TensorRT maintainers](https://github.com/NVIDIA/TensorRT/issues/1845), but I wrote the very same thing almost 1 year ago and then 4 months ago so… I don’t know).

PyTorch 2.0 makes the graph capture step easy, it has been designed to work even if not everything is PyTorch compliant. With its Python first philosophy, it provides flexibility and debuggability. 

Several years ago, some said that by design PyTorch can’t be as performant than Tensorflow because of its eager execution model, compilation has to be faster. The same thing could be said for OnnxRuntime or TensorRT, they are C++ stuff, they have less overhead, etc. But at the end of the day, it's always the “ease of use” which is decisive. Ease of use because of Python, but also because of the transparency in the process, Triton makes understanding and debugging kernels much easier than closed source TensorRT Myelin engine calling closed source cuBlas library.

And of course, like TensorFlow, there will be many use cases where dedicated tools will be best choices, starting with situations where you can’t deploy a Python interpreter.

The second lesson, Triton is easier to start with than CUDA, but you probably can’t write or debug highly performant code without being able to, at least, read and debug PTX/SASS instructions. We realized that when we had some performance issues... The good news is that PTX is understandable, and you will probably spot unexpected generated code with some effort if there is any. Moreover, CUDA probably requires the same care when you really focus on performances.

We had plenty of issues with Triton, for example, cosmetics change in code may raise segfault. At some point you finish by having an intuition of what kinds of patterns to follow to make things work, in particular when there are for loops and dot operations. A new version of Triton has recently been released after a full rewrite of its backend, our little tests showed some improvement on stability but we have not yet fully switched.

As in my previous post, I highly recommend that readers start playing with Triton library, I rewrite it here: it’s fun (at least when it doesn’t segfault) and helps you to make sense of a large part of what is happening in ML engineering. I am quite convinced many flash attention like kernels are still to be written. 

# Caveat

Two important things to note about the project described here:

* CUDA graphs require us to capture a graph per input tensor shape, there is a non-negligible warmup time. We measure around 10mn on 2 different machines / GPUs (down from 50mn in our previous Kernl version). One user reported with the new version a bit more than 20mn of warmup time. We are aware of obvious ways to decrease it significantly.
* The context here is latency sensitive optimization. In throughput sensitive one, just increasing batch size will bring you most of the speedup. Otherwise, more aggressive optimizations like quantization are required (not yet released on Kernl)."
99,machinelearning,openai,top,2018-08-06 17:08:04,[N] OpenAI Five Benchmark: Results,luiscosio,False,0.95,229,9533g8,https://blog.openai.com/openai-five-benchmark-results/,179,1533575284.0,
100,machinelearning,openai,comments,2019-11-05 11:27:49,[D] 2020 Residencies Applicants Discussion Thread,mahaveer0suthar,False,0.97,181,drxvd0,https://www.reddit.com/r/MachineLearning/comments/drxvd0/d_2020_residencies_applicants_discussion_thread/,1011,1572953269.0,"* Facebook AI Residency Program \[[Link](https://research.fb.com/programs/facebook-ai-residency-program/)\]. Application Deadline: January 31, 2020, 05:00pm PST.
* Google AI Residency \[[Link](https://ai.google/research/join-us/ai-residency/)\]. Application Deadline: December 19th, 2019.
* Google X AI Residency \[[Link](https://x.company/careers-at-x/4225880002/)\]
* Google AI Resident (Health), 2020 Start - London, UK \[Application Closed\] 
* Google AI Resident (Health), 2020 - Start Palo Alto, CA, USA \[Application Closed\]
* OpenAI 2020 Winter Scholars \[[Link](https://jobs.lever.co/openai/d30e1f04-b548-4503-ba8b-9853cb49bdc7)\]. Application Deadline: Nov 15, 2019.

Thought it would be helpful to have a discussion thread for 2020 Residencies applicants to share the updates, info, resources to prepare etc.

Below are some useful discussion threads :

[https://www.reddit.com/r/MachineLearning/comments/9uyzc1/d\_google\_ai\_residency\_2019\_applicants\_discussion/](https://www.reddit.com/r/MachineLearning/comments/9uyzc1/d_google_ai_residency_2019_applicants_discussion/)

[https://www.reddit.com/r/MachineLearning/comments/7rajic/d\_anyone\_heard\_back\_from\_google\_ai\_residency/](https://www.reddit.com/r/MachineLearning/comments/7rajic/d_anyone_heard_back_from_google_ai_residency/)

[https://www.reddit.com/r/MachineLearning/comments/7wst07/d\_study\_guides\_for\_interview\_at\_ai\_research/](https://www.reddit.com/r/MachineLearning/comments/7wst07/d_study_guides_for_interview_at_ai_research/)

[https://www.reddit.com/r/MachineLearning/comments/690ixs/d\_google\_brain\_residency\_requirements\_and/](https://www.reddit.com/r/MachineLearning/comments/690ixs/d_google_brain_residency_requirements_and/)"
101,machinelearning,openai,comments,2019-03-22 02:36:38,[P] OpenAI's GPT-2-based Reddit Bot is Live!,Shevizzle,False,0.97,340,b3zlha,https://www.reddit.com/r/MachineLearning/comments/b3zlha/p_openais_gpt2based_reddit_bot_is_live/,990,1553222198.0,"**~~FINAL~~** **UPDATE: The bot is down until I have time to get it operational again. Will update this when it’s back online.**

&#x200B;

**Disclaimer** : This is not the full model. This is the smaller and less powerful version which OpenAI released publicly.

[Original post](https://www.reddit.com/r/MachineLearning/comments/b32lve/d_im_using_openais_gpt2_to_generate_text_give_me/)

Based on the popularity of my post from the other day, I decided to go ahead an build a full-fledged Reddit bot. So without further ado, please welcome:

# u/GPT-2_Bot

&#x200B;

If you want to use the bot, all you have to do is reply to any comment with the following command words:

# ""gpt-2 finish this""

Your reply can contain other stuff as well, i.e.

>""hey **gpt-2**, please **finish this** argument for me, will ya?""

&#x200B;

The bot will then look at **the comment you replied to** and generate its own response. It will tag you in the response so you know when it's done!

&#x200B;

Currently supported subreddits:

* r/funny
* r/AskReddit
* r/gaming
* r/pics
* r/science
* r/worldnews
* r/todayilearned
* r/movies
* r/videos
* r/ShowerThoughts
* r/MachineLearning
* r/test
* r/youtubehaiku
* r/thanosdidnothingwrong
* r/dankmemes

&#x200B;

The bot also scans r/all so ***theoretically*** it will see comments posted anywhere on Reddit. In practice, however, it only seems to catch about 1 in 5 of them.

&#x200B;

Enjoy! :) Feel free to PM me with feedback"
102,machinelearning,openai,comments,2019-03-19 20:43:15,[D] I'm using OpenAI's GPT-2 to generate text. Give me prompts to try!,Shevizzle,False,0.96,334,b32lve,https://www.reddit.com/r/MachineLearning/comments/b32lve/d_im_using_openais_gpt2_to_generate_text_give_me/,781,1553028195.0,"Thank you all for participating! Based on the popularity of this post, I decided to create a full-blown reddit bot based on it. I won't be responding here anymore but check out the update for more info.

# UPDATE: [I made a GPT-2 reddit bot](https://www.reddit.com/r/MachineLearning/comments/b3zlha/p_openais_gpt2based_reddit_bot_is_live/)

&#x200B;

**Disclaimer** : This is not the full model. This is the smaller and less powerful version which OpenAI released publicly.

**EXAMPLE PROMPT:**

The NCAA unveiled the 68 college basketball teams that will be dancing this year. It wasn’t short on controversy, as snubbed teams aired out their grievances and blamed a confusing new metric.

**OUTPUT:**

The numbers came straight from the NCAA's website, where they're being used to calculate the number of national titles, national championship games, and even NCAA games played in a certain year.

""We want to make sure that the NCAA is inclusive,"" said NCAA President Dave Heidelman. ""We want to make sure that you can get what you want out of your program. And we want to make sure that the NCAA is focused on this problem right now and that the NCAA is fully invested.""

The list of opponents includes Ohio State, Florida State, Louisville, Kentucky, Mississippi State, Penn State, Oklahoma, Southern Cal, Virginia, and Wake Forest. It's not all about one one, though: They're all on pace to play an entire season. They have every national title, national championship, and tournament appearance in their DNA — as one might expect, they want to see it happen, but also want to build an identity, make their dream of watching that happen become the reality.

As good as any NCAA team is, they're also a long way off reaching the number one spot in the nation or even the top-ranked nation. The Big Ten also has some talented recruits from some in-state programs that may be considered the top two nationally. In fact, the national ranking of these schools is so high that a single conference ranking in 2016 will put the conference in the top-50 of the polls. Still, while Big Ten and SEC teams are likely to be on the map and competing for national titles, they're a bit underserved (and it's not as if they're all the same.)

So where does the NCAA stand on this?

According to ULM's John Covington, who runs its ""Unions, Colleges, and Universities"" page in conjunction with the National Conference, they're all going to have to make some moves:

Some may think this is just a joke. ""No, this is really about the league's future,"" said Dr. John H. Hester, president of UM's Athletic Department and president of the National Collegiate Athletic Association's Women's Academic Programs. ""I think the NCAA is a great place to start, because it's here to stay and if we're really strong and we can figure ourselves out, our future is going to be on the basketball court.""

**MODEL:**

[gpt-2 117M](https://github.com/openai/gpt-2)

**If you have an idea for a prompt, post it in the comments and I'll reply with the output if I deem it worthy.**"
103,machinelearning,openai,comments,2017-07-03 20:24:09,[D] Why can't you guys comment your fucking code?,didntfinishhighschoo,False,0.86,1655,6l2esd,https://www.reddit.com/r/MachineLearning/comments/6l2esd/d_why_cant_you_guys_comment_your_fucking_code/,477,1499113449.0,"Seriously.

I spent the last few years doing web app development. Dug into DL a couple months ago. Supposedly, compared to the post-post-post-docs doing AI stuff, JavaScript developers should be inbred peasants. But every project these peasants release, even a fucking library that colorizes CLI output, has a catchy name, extensive docs, shitloads of comments, fuckton of tests, semantic versioning, changelog, and, oh my god, better variable names than `ctx_h` or `lang_hs` or `fuck_you_for_trying_to_understand`.

The concepts and ideas behind DL, GANs, LSTMs, CNNs, whatever – it's clear, it's simple, it's intuitive. The slog is to go through the jargon (that keeps changing beneath your feet - what's the point of using fancy words if you can't keep them consistent?), the unnecessary equations, trying to squeeze meaning from bullshit language used in papers, figuring out the super important steps, preprocessing, hyperparameters optimization that the authors, oops, failed to mention.

Sorry for singling out, but [look at this](https://github.com/facebookresearch/end-to-end-negotiator/blob/master/src/agent.py) - what the fuck? If a developer anywhere else at Facebook would get this code for a review they would throw up.

- Do you intentionally try to obfuscate your papers? Is pseudo-code a fucking premium? Can you at least try to give some intuition before showering the reader with equations?

- How the fuck do you dare to release a paper without source code?

- Why the fuck do you never ever add comments to you code?

- When naming things, are you charged by the character? Do you get a bonus for acronyms?

- Do you realize that OpenAI having needed to release a ""baseline"" TRPO implementation is a fucking disgrace to your profession?

- Jesus christ, who decided to name a tensor concatenation function `cat`?
"
104,machinelearning,openai,comments,2023-03-15 22:34:01,[D] Our community must get serious about opposing OpenAI,SOCSChamp,False,0.95,2967,11sboh1,https://www.reddit.com/r/MachineLearning/comments/11sboh1/d_our_community_must_get_serious_about_opposing/,448,1678919641.0,"OpenAI was founded for the explicit purpose of democratizing access to AI and acting as a counterbalance to the closed off world of big tech by developing open source tools.

They have abandoned this idea entirely.

Today, with the release of GPT4 and their direct statement that they will not release details of the model creation due to ""safety concerns"" and the competitive environment, they have created a precedent worse than those that existed before they entered the field. We're at risk now of other major players, who previously at least published their work and contributed to open source tools, close themselves off as well.

AI alignment is a serious issue that we definitely have not solved. Its a huge field with a dizzying array of ideas, beliefs and approaches. We're talking about trying to capture the interests and goals of all humanity, after all. In this space, the one approach that is horrifying (and the one that OpenAI was LITERALLY created to prevent) is a singular or oligarchy of for profit corporations making this decision for us. This is exactly what OpenAI plans to do.

I get it, GPT4 is incredible. However, we are talking about the single most transformative technology and societal change that humanity has ever made. It needs to be for everyone or else the average person is going to be left behind.

We need to unify around open source development; choose companies that contribute to science, and condemn the ones that don't.

This conversation will only ever get more important."
105,machinelearning,openai,comments,2023-05-17 22:15:28,[D] Does anybody else despise OpenAI?,onesynthguy,False,0.86,1406,13kfxzy,https://www.reddit.com/r/MachineLearning/comments/13kfxzy/d_does_anybody_else_despise_openai/,426,1684361728.0," I  mean, don't get me started with the closed source models they have that were trained using the work of unassuming individuals who will never  see a penny for it. Put it up on Github they said. I'm all for  open-source, but when a company turns around and charges you for a  product they made with freely and publicly made content, while forbidding you from using the output to create competing models, that is where I  draw the line. It is simply ridiculous. 

Sam Altman couldn't be anymore predictable with his recent attempts to get the government to start regulating AI.

What  risks? The AI is just a messenger for information that is already out  there if one knows how/where to look. You don't need AI to learn how to  hack, to learn how to make weapons, etc. Fake news/propaganda? The  internet has all of that covered. LLMs are no where near the level of AI  you see in sci-fi. I mean, are people really afraid of text? Yes, I  know that text can sometimes be malicious code such as viruses, but  those can be found on github as well.  If they fall for this they might  as well shutdown the internet while they're at it.

He  is simply blowing things out of proportion and using fear to increase  the likelihood that they do what he wants, hurt the competition. I  bet he is probably teething with bitterness everytime a new huggingface  model comes out. The thought of us peasants being able to use AI  privately is too dangerous. No, instead we must be fed scraps while they  slowly take away our jobs and determine our future.

This  is not a doomer post, as I am all in favor of the advancement of AI.  However, the real danger here lies in having a company like OpenAI  dictate the future of humanity. I get it, the writing is on the wall;  the cost of human intelligence will go down, but if everyone has their  personal AI then it wouldn't seem so bad or unfair would it? Listen,  something that has the power to render a college degree that costs  thousands of dollars worthless should be available to the public. This  is to offset the damages and job layoffs that will come as a result of  such an entity. It wouldn't be as bitter of a taste as it would if you were replaced by it while still not being able to access it. Everyone should be able to use it as leverage, it is the only fair solution.

If  we don't take action now, a company like ClosedAI will, and they are  not in favor of the common folk. Sam Altman is so calculated to the  point where there were times when he seemed to be shooting OpenAI in the foot during his talk.  This move is to simply conceal his real intentions, to climb the ladder and take it with him. If he didn't include his company in his  ramblings, he would be easily read. So instead, he pretends to be scared of his own product, in an effort to legitimize his claim. Don't fall  for it.

They are slowly making a  reputation as one the most hated tech companies, right up there with  Adobe, and they don't show any sign of change. They have no moat,  othewise they wouldn't feel so threatened to the point where they would have to resort to creating barriers of entry via regulation. This only  means one thing, we are slowly catching up. We just need someone to  vouch for humanity's well-being, while acting as an opposing force to the  evil corporations who are only looking out for themselves. Question is,  who would be a good candidate?"
106,machinelearning,openai,comments,2023-05-25 13:51:58,OpenAI is now complaining about regulation of AI [D],I_will_delete_myself,False,0.89,793,13rie0e,https://www.reddit.com/r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/,349,1685022718.0,"I held off for a while but hypocrisy just drives me nuts after hearing this.

SMH this company like white knights who think they are above everybody. They want regulation but they want to be untouchable by this regulation. Only wanting to hurt other people but not “almighty” Sam and friends.

Lies straight through his teeth to Congress about suggesting similar things done in the EU, but then starts complain about them now. This dude should not be taken seriously in any political sphere whatsoever.

My opinion is this company is anti-progressive for AI by locking things up which is contrary to their brand name. If they can’t even stay true to something easy like that, how should we expect them to stay true with AI safety which is much harder?

I am glad they switch sides for now, but pretty ticked how they think they are entitled to corruption to benefit only themselves. SMH!!!!!!!!

What are your thoughts?"
107,machinelearning,openai,comments,2023-04-05 19:44:09,"[D] ""Our Approach to AI Safety"" by OpenAI",mckirkus,False,0.88,298,12cvkvn,https://www.reddit.com/r/MachineLearning/comments/12cvkvn/d_our_approach_to_ai_safety_by_openai/,297,1680723849.0,"It seems OpenAI are steering the conversation away from the existential threat narrative and into things like accuracy, decency, privacy, economic risk, etc.

To the extent that they do buy the existential risk argument, they don't seem concerned much about GPT-4 making a leap into something dangerous, even if it's at the heart of autonomous agents that are currently emerging.  

>""Despite extensive research and testing, we cannot predict all of the [beneficial ways people will use our technology](https://openai.com/customer-stories), nor all the ways people will abuse it. That’s why we believe that learning from real-world use is a critical component of creating and releasing increasingly safe AI systems over time. ""

Article headers:

* Building increasingly safe AI systems
* Learning from real-world use to improve safeguards
* Protecting children
* Respecting privacy
* Improving factual accuracy

&#x200B;

[https://openai.com/blog/our-approach-to-ai-safety](https://openai.com/blog/our-approach-to-ai-safety)"
108,machinelearning,openai,comments,2016-01-09 04:01:47,AMA: the OpenAI Research Team,IlyaSutskever,False,0.98,396,404r9m,https://www.reddit.com/r/MachineLearning/comments/404r9m/ama_the_openai_research_team/,285,1452312107.0,"The OpenAI research team will be answering your questions.

We are (our usernames are):  Andrej Karpathy (badmephisto), Durk Kingma (dpkingma), Greg Brockman (thegdb), Ilya Sutskever (IlyaSutskever), John Schulman (johnschulman), Vicki Cheung (vicki-openai), Wojciech Zaremba (wojzaremba).


Looking forward to your questions! "
109,machinelearning,openai,comments,2022-05-27 05:46:54,"[D] I don't really trust papers out of ""Top Labs"" anymore",MrAcurite,False,0.97,1681,uyratt,https://www.reddit.com/r/MachineLearning/comments/uyratt/d_i_dont_really_trust_papers_out_of_top_labs/,262,1653630414.0,"I mean, I trust that the numbers they got are accurate and that they really did the work and got the results. I believe those. It's just that, take the recent ""An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems"" paper. It's 18 pages of talking through this pretty convoluted evolutionary and multitask learning algorithm, it's pretty interesting, solves a bunch of problems. But two notes. 

One, the big number they cite as the success metric is 99.43 on CIFAR-10, against a SotA of 99.40, so woop-de-fucking-doo in the grand scheme of things.

Two, there's a chart towards the end of the paper that details how many TPU core-hours were used for just the training regimens that results in the final results. The sum total is 17,810 core-hours. Let's assume that for someone who doesn't work at Google, you'd have to use on-demand pricing of $3.22/hr. This means that these trained models cost $57,348. 

Strictly speaking, throwing enough compute at a general enough genetic algorithm will eventually produce arbitrarily good performance, so while you can absolutely read this paper and collect interesting ideas about how to use genetic algorithms to accomplish multitask learning by having each new task leverage learned weights from previous tasks by defining modifications to a subset of components of a pre-existing model, there's a meta-textual level on which this paper is just ""Jeff Dean spent enough money to feed a family of four for half a decade to get a 0.03% improvement on CIFAR-10.""

OpenAI is far and away the worst offender here, but it seems like everyone's doing it. You throw a fuckton of compute and a light ganache of new ideas at an existing problem with existing data and existing benchmarks, and then if your numbers are infinitesimally higher than their numbers, you get to put a lil' sticker on your CV. Why should I trust that your ideas are even any good? I can't check them, I can't apply them to my own projects. 

Is this really what we're comfortable with as a community? A handful of corporations and the occasional university waving their dicks at everyone because they've got the compute to burn and we don't? There's a level at which I think there should be a new journal, exclusively for papers in which you can replicate their experimental results in under eight hours on a single consumer GPU."
110,machinelearning,openai,comments,2021-01-18 09:08:06,[P] The Big Sleep: Text-to-image generation using BigGAN and OpenAI's CLIP via a Google Colab notebook from Twitter user Adverb,Wiskkey,False,0.99,621,kzr4mg,https://www.reddit.com/r/MachineLearning/comments/kzr4mg/p_the_big_sleep_texttoimage_generation_using/,259,1610960886.0,"From [https://twitter.com/advadnoun/status/1351038053033406468](https://twitter.com/advadnoun/status/1351038053033406468):

>The Big Sleep  
>  
>Here's the notebook for generating images by using CLIP to guide BigGAN.  
>  
>It's very much unstable and a prototype, but it's also a fair place to start. I'll likely update it as time goes on.  
>  
>[colab.research.google.com/drive/1NCceX2mbiKOSlAd\_o7IU7nA9UskKN5WR?usp=sharing](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing)

I am not the developer of The Big Sleep. [This](https://twitter.com/advadnoun/) is the developer's Twitter account; [this](https://www.reddit.com/user/advadnoun) is the developer's Reddit account.

**Steps to follow to generate the first image in a given Google Colab session**:

1. Optionally, if this is your first time using Google Colab, view this [Colab introduction](https://colab.research.google.com/notebooks/intro.ipynb) and/or this [Colab FAQ](https://research.google.com/colaboratory/faq.html).
2. Click [this link](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing).
3. Sign into your Google account if you're not already signed in. Click the ""S"" button in the upper right to do this. Note: Being signed into a Google account has privacy ramifications, such as your Google search history being recorded in your Google account.
4. In the Table of Contents, click ""Parameters"".
5. Find the line that reads ""tx = clip.tokenize('''a cityscape in the style of Van Gogh''')"" and change the text inside of the single quote marks to your desired text; example: ""tx = clip.tokenize('''a photo of New York City''')"". The developer recommends that you keep the three single quote marks on both ends of your desired text so that mult-line text can be used  An alternative is to remove two of the single quotes on each end of your desired text; example: ""tx = clip.tokenize('a photo of New York City')"".
6. In the Table of Contents, click ""Restart the kernel..."".
7. Position the pointer over the first cell in the notebook, which starts with text ""import subprocess"". Click the play button (the triangle) to run the cell. Wait until the cell completes execution.
8. Click menu item ""Runtime->Restart and run all"".
9. In the Table of Contents, click ""Diagnostics"". The output appears near the end of the Train cell that immediately precedes the Diagnostics cell, so scroll up a bit. Every few minutes (or perhaps 10 minutes if Google assigned you relatively slow hardware for this session), a new image will appear in the Train cell that is a refinement of the previous image. This process can go on for as long as you want until Google ends your Google Colab session, which is a total of [up to 12 hours](https://research.google.com/colaboratory/faq.html) for the free version of Google Colab.

**Steps to follow if you want to start a different run using the same Google Colab session:**

1. Click menu item ""Runtime->Interrupt execution"".
2. Save any images that you want to keep by right-clicking on them and using the appropriate context menu command.
3. Optionally, change the desired text. Different runs using the same desired text almost always results in different outputs.
4. Click menu item ""Runtime->Restart and run all"".

**Steps to follow when you're done with your Google Colab session**:

1. Click menu item ""Runtime->Manage sessions"". Click ""Terminate"" to end the session.
2. Optionally, log out of your Google account due to the privacy ramifications of being logged into a Google account.

The first output image in the Train cell (using the notebook's default of seeing every 100th image generated) usually is a very poor match to the desired text, but the second output image often is a decent match to the desired text. To change the default of seeing every 100th image generated, change the number 100 in line ""if itt % 100 == 0:"" in the Train cell to the desired number. **For free-tier Google Colab users, I recommend changing 100 to a small integer such as 5.**

Tips for the text descriptions that you supply:

1. In Section 3.1.4 of OpenAI's [CLIP paper](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf) (pdf), the authors recommend using a text description of the form ""A photo of a {label}."" or ""A photo of a {label}, a type of {type}."" for images that are photographs.
2. A Reddit user gives [these tips](https://www.reddit.com/r/MediaSynthesis/comments/l2hmqn/this_aint_it_chief/gk8g8e9/).
3. The Big Sleep should generate [these 1,000 types of things](https://www.reddit.com/r/MediaSynthesis/comments/l7hbix/tip_for_users_of_the_big_sleep_it_should_on/) better on average than other types of things.

[Here](https://www.digitaltrends.com/news/big-sleep-ai-image-generator/) is an article containing a high-level description of how The Big Sleep works. The Big Sleep uses a modified version of [BigGAN](https://aiweirdness.com/post/182322518157/welcome-to-latent-space) as its image generator component. The Big Sleep uses the ViT-B/32 [CLIP](https://openai.com/blog/clip/) model to rate how well a given image matches your desired text. The best CLIP model according to the CLIP paper authors is the (as of this writing) unreleased ViT-L/14-336px model; see Table 10 on page 40 of the [CLIP paper (pdf)](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf) for a comparison.

There are [many other sites/programs/projects](https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais/) that use CLIP to steer image/video creation to match a text description.

Some relevant subreddits:

1. [r/bigsleep](https://www.reddit.com/r/bigsleep/) (subreddit for images/videos generated from text-to-image machine learning algorithms).
2. [r/deepdream](https://www.reddit.com/r/deepdream/) (subreddit for images/videos generated from machine learning algorithms).
3. [r/mediasynthesis](https://www.reddit.com/r/mediasynthesis/) (subreddit for media generation/manipulation techniques that use artificial intelligence; this subreddit shouldn't be used to post images/videos unless new techniques are demonstrated, or the images/videos are of high quality relative to other posts).

Example using text 'a black cat sleeping on top of a red clock':

https://preview.redd.it/7xq58v7022c61.png?width=512&format=png&auto=webp&s=a229ae9add555cd1caba31c42b60d907ffe67773

Example using text 'the word ''hot'' covered in ice':

https://preview.redd.it/6kxdp8u3k2c61.png?width=512&format=png&auto=webp&s=5bd078b0111575f5d88a1dc53b0aeb933f3b0da6

Example using text 'a monkey holding a green lightsaber':

https://preview.redd.it/rdsybsoaz2c61.png?width=512&format=png&auto=webp&s=2769d4c6c883c1c35ae0b1c629bebe9bc1d41393

Example using text 'The White House in Washington D.C. at night with green and red spotlights shining on it':

https://preview.redd.it/w4mg90xsf5c61.png?width=512&format=png&auto=webp&s=5f18318de2f77bcd8a86e71e87048fadd30383d1

Example using text '''A photo of the Golden Gate Bridge at night, illuminated by spotlights in a tribute to Prince''':

https://preview.redd.it/cn4ecuafhic61.png?width=512&format=png&auto=webp&s=397c838fdc49f13c5f17110b92c78b95bf0dcac0

Example using text '''a Rembrandt-style painting titled ""Robert Plant decides whether to take the stairway to heaven or the ladder to heaven""''':

https://preview.redd.it/h7rb3y6j5jc61.png?width=512&format=png&auto=webp&s=537bfe8210af185647b00e7585c948aa2c4e0ffb

Example using text '''A photo of the Empire State Building being shot at with the laser cannons of a TIE fighter.''':

https://preview.redd.it/cwi7i639c5d61.png?width=512&format=png&auto=webp&s=0510c8b93adb40eee4d3f41607f1c215d41e55ff

Example using text '''A cartoon of a new mascot for the Reddit subreddit DeepDream that has a mouse-like face and wears a cape''':

https://preview.redd.it/wtxbduevcbd61.png?width=512&format=png&auto=webp&s=c5d266258922bc62f25c80a08cd9cabc07d9cb1c

Example using text '''Bugs Bunny meets the Eye of Sauron, drawn in the Looney Tunes cartoon style''':

https://preview.redd.it/gmljaeekuid61.png?width=512&format=png&auto=webp&s=9ea578de165e12afc3a62bf6886bc1ae9dc19bec

Example using text '''Photo of a blue and red neon-colored frog at night.''':

https://preview.redd.it/nzlypte6wzd61.png?width=512&format=png&auto=webp&s=7e10b06f22cfc57c64b6d05738c7486b895083df

Example using text '''Hell begins to freeze over''':

https://preview.redd.it/vn99we9ngmf61.png?width=512&format=png&auto=webp&s=2408efd607f0ab40a08db6ee67448791aa813993

Example using text '''A scene with vibrant colors''':

https://preview.redd.it/4z133mvrgmf61.png?width=512&format=png&auto=webp&s=b78e7a8e3f736769655056093a9904ff09a355a1

Example using text '''The Great Pyramids were turned into prisms by a wizard''':

https://preview.redd.it/zxt6op7vgmf61.png?width=512&format=png&auto=webp&s=53e578cfde14b28afe27957e95e610b89afadd44"
111,machinelearning,openai,comments,2017-08-12 00:10:03,[N] OpenAI bot beat best Dota 2 players in 1v1 at The International 2017,crouching_dragon_420,False,0.96,565,6t58ks,https://blog.openai.com/dota-2/,252,1502496603.0,
112,machinelearning,openai,comments,2023-01-20 10:41:04,[N] OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic,ChubChubkitty,False,0.83,522,10gtruu,https://www.reddit.com/r/MachineLearning/comments/10gtruu/n_openai_used_kenyan_workers_on_less_than_2_per/,246,1674211264.0,https://time.com/6247678/openai-chatgpt-kenya-workers/
113,machinelearning,openai,comments,2019-02-15 13:04:39,[Discussion] OpenAI should now change their name to ClosedAI,SirLordDragon,False,0.92,644,aqwcyx,https://www.reddit.com/r/MachineLearning/comments/aqwcyx/discussion_openai_should_now_change_their_name_to/,223,1550235879.0,It's the only way to complete the hype wave.
114,machinelearning,openai,comments,2023-03-30 14:18:50,[D] AI Policy Group CAIDP Asks FTC To Stop OpenAI From Launching New GPT Models,vadhavaniyafaijan,False,0.84,210,126oiey,https://www.reddit.com/r/MachineLearning/comments/126oiey/d_ai_policy_group_caidp_asks_ftc_to_stop_openai/,213,1680185930.0,"The Center for AI and Digital Policy (CAIDP), a tech ethics group, has asked the Federal Trade Commission to investigate OpenAI for violating consumer protection rules. CAIDP claims that OpenAI's AI text generation tools have been ""biased, deceptive, and a risk to public safety.""

CAIDP's complaint raises concerns about potential threats from OpenAI's GPT-4 generative text model, which was announced in mid-March. It warns of the potential for GPT-4 to produce malicious code and highly tailored propaganda and the risk that biased training data could result in baked-in stereotypes or unfair race and gender preferences in hiring. 

The complaint also mentions significant privacy failures with OpenAI's product interface, such as a recent bug that exposed OpenAI ChatGPT histories and possibly payment details of ChatGPT plus subscribers.

CAIDP seeks to hold OpenAI accountable for violating Section 5 of the FTC Act, which prohibits unfair and deceptive trade practices. The complaint claims that OpenAI knowingly released GPT-4 to the public for commercial use despite the risks, including potential bias and harmful behavior. 

[Source](https://www.theinsaneapp.com/2023/03/stop-openai-from-launching-gpt-5.html) | [Case](https://www.caidp.org/cases/openai/)| [PDF](https://www.caidp.org/app/download/8450269463/CAIDP-FTC-Complaint-OpenAI-GPT-033023.pdf)"
115,machinelearning,openai,comments,2023-05-04 16:13:30,"[D] Google ""We Have No Moat, And Neither Does OpenAI"": Leaked Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI",hardmaru,False,0.98,1181,137rxgw,https://www.semianalysis.com/p/google-we-have-no-moat-and-neither,206,1683216810.0,
116,machinelearning,openai,comments,2023-11-17 21:12:49,"[N] OpenAI Announces Leadership Transition, Fires Sam Altman",Sm0oth_kriminal,False,0.94,419,17xp85q,https://www.reddit.com/r/MachineLearning/comments/17xp85q/n_openai_announces_leadership_transition_fires/,199,1700255569.0,"EDIT: Greg Brockman has quit as well: https://x.com/gdb/status/1725667410387378559?s=46&t=1GtNUIU6ETMu4OV8_0O5eA

Source: https://openai.com/blog/openai-announces-leadership-transition

Today, it was announced that Sam Altman will no longer be CEO or affiliated with OpenAI due to a lack of “candidness” with the board. This is extremely unexpected as Sam Altman is arguably the most recognizable face of state of the art AI (of course, wouldn’t be possible without great team at OpenAI). Lots of speculation is in the air, but there clearly must have been some good reason to make such a drastic decision.

This may or may not materially affect ML research, but it is plausible that the lack of “candidness” is related to copyright data, or usage of data sources that could land OpenAI in hot water with regulatory scrutiny. Recent lawsuits (https://www.reuters.com/legal/litigation/writers-suing-openai-fire-back-companys-copyright-defense-2023-09-28/) have raised questions about both the morality and legality of how OpenAI and other research groups train LLMs.

Of course we may never know the true reasons behind this action, but what does this mean for the future of AI?"
117,machinelearning,openai,comments,2024-02-15 18:39:06,[D] OpenAI Sora Video Gen -- How??,htrp,False,0.96,383,1armmng,https://www.reddit.com/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/,199,1708022346.0,">Introducing Sora, our text-to-video model. Sora can generate videos up to a minute long while maintaining visual quality and adherence to the user’s prompt.




https://openai.com/sora

Research Notes
Sora is a diffusion model, which generates a video by starting off with one that looks like static noise and gradually transforms it by removing the noise over many steps.

Sora is capable of generating entire videos all at once or extending generated videos to make them longer. By giving the model foresight of many frames at a time, we’ve solved a challenging problem of making sure a subject stays the same even when it goes out of view temporarily.

Similar to GPT models, Sora uses a transformer architecture, unlocking superior scaling performance.

We represent videos and images as collections of smaller units of data called patches, each of which is akin to a token in GPT. By unifying how we represent data, we can train diffusion transformers on a wider range of visual data than was possible before, spanning different durations, resolutions and aspect ratios.

Sora builds on past research in DALL·E and GPT models. It uses the recaptioning technique from DALL·E 3, which involves generating highly descriptive captions for the visual training data. As a result, the model is able to follow the user’s text instructions in the generated video more faithfully.

In addition to being able to generate a video solely from text instructions, the model is able to take an existing still image and generate a video from it, animating the image’s contents with accuracy and attention to small detail. The model can also take an existing video and extend it or fill in missing frames. Learn more in our technical paper (coming later today).

Sora serves as a foundation for models that can understand and simulate the real world, a capability we believe will be an important milestone for achieving AGI.



Example Video: https://cdn.openai.com/sora/videos/cat-on-bed.mp4

Tech paper will be released later today. But brainstorming how?"
118,machinelearning,openai,comments,2023-05-07 23:26:29,"[D] ClosedAI license, open-source license which restricts only OpenAI, Microsoft, Google, and Meta from commercial use",wemsyn,False,0.8,344,13b6miy,https://www.reddit.com/r/MachineLearning/comments/13b6miy/d_closedai_license_opensource_license_which/,191,1683501989.0,"After reading [this article](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither), I realized it might be nice if the open-source AI community could exclude ""closed AI"" players from taking advantage of community-generated models and datasets. I was wondering if it would be possible to write a license that is completely permissive (like Apache 2.0 or MIT), except to certain companies, which are completely barred from using the software in any context.

Maybe this could be called the ""ClosedAI"" license. I'm not any sort of legal expert so I have no idea how best to write this license such that it protects model weights and derivations thereof.

I prompted ChatGPT for an example license and this is what it gave me:

    <PROJECT NAME> ClosedAI License v1.0
    
    Permission is hereby granted, free of charge, to any person or organization obtaining a copy of this software and associated documentation files (the ""Software""), to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, subject to the following conditions:
    
    1. The above copyright notice and this license notice shall be included in all copies or substantial portions of the Software.
    
    2. The Software and any derivative works thereof may not be used, in whole or in part, by or on behalf of OpenAI Inc., Google LLC, or Microsoft Corporation (collectively, the ""Prohibited Entities"") in any capacity, including but not limited to training, inference, or serving of neural network models, or any other usage of the Software or neural network weights generated by the Software.
    
    3. Any attempt by the Prohibited Entities to use the Software or neural network weights generated by the Software is a material breach of this license.
    
    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

No idea if this is valid or not. Looking for advice.

&#x200B;

**Edit:** Thanks for the input. Removed non-commercial clause (whoops, proofread what ChatGPT gives you). Also removed Meta from the excluded companies list due to popular demand."
119,machinelearning,openai,comments,2017-08-01 10:23:32,[D] Where does this hyped news come from? *Facebook shut down AI that invented its own language.*,nomaderx,False,0.92,479,6qvbu8,https://www.reddit.com/r/MachineLearning/comments/6qvbu8/d_where_does_this_hyped_news_come_from_facebook/,187,1501583012.0,"My Facebook wall is full of people sharing this story that Facebook *had* to shut down an AI system it developed that invented it's own language. Here are some of these articles:

[Independent: Facebook's AI robots shut down after they start talking to each other in their own language](http://www.independent.co.uk/life-style/gadgets-and-tech/news/facebook-artificial-intelligence-ai-chatbot-new-language-research-openai-google-a7869706.html)

[BGR: Facebook engineers panic, pull plug on AI after bots develop their own language](http://bgr.com/2017/07/31/facebook-ai-shutdown-language/)

[Forbes: Facebook AI Creates Its Own Language In Creepy Preview Of Our Potential Future](https://www.forbes.com/sites/tonybradley/2017/07/31/facebook-ai-creates-its-own-language-in-creepy-preview-of-our-potential-future/#192e0e29292c)

[Digital Journal: Researchers shut down AI that invented its own language](http://www.digitaljournal.com/tech-and-science/technology/a-step-closer-to-skynet-ai-invents-a-language-humans-can-t-read/article/498142)

EDIT#3: [FastCoDesign: AI Is Inventing Languages Humans Can’t Understand. Should We Stop It?](https://www.fastcodesign.com/90132632/ai-is-inventing-its-own-perfect-languages-should-we-let-it) [Likely the first article]

Note that this is related to the work in the *Deal or No Deal? End-to-End Learning for Negotiation Dialogues* paper. On it's own, it is interesting work.

While the article from Independent seems to be the only one that finally gives the clarification *'The company chose to shut down the chats because ""our interest was having bots who could talk to people""'*, **ALL** the articles say things that suggest that researchers went into panic mode, had to 'pull the plug' out of fear, this stuff is scary. One of the articles (don't remember which) even went on to say something like *'A week after Elon Musk suggested AI needs to be regulated and Mark Zuckerberg disagreed, Facebook had to shut down it's AI because it became too dangerous/scary'* (or something to this effect).

While I understand the hype around deep learning (a.k.a backpropaganda), etc., I think these articles are so ridiculous. I wouldn't even call this hype, but almost 'fake news'. I understand that sometimes articles should try to make the news more interesting/appealing by hyping it a bit, but this is almost detrimental, and is just promoting AI fear-mongering. 

EDIT#1: Some people on Facebook are actually believing this fear to be real, sending me links and asking me about it. :/

EDIT#2: As pointed out in the comments, there's also this opposite article:

[Gizmodo: No, Facebook Did Not Panic and Shut Down an AI Program That Was Getting Dangerously Smart](http://gizmodo.com/no-facebook-did-not-panic-and-shut-down-an-ai-program-1797414922)

EDIT#4: And now, BBC joins in to clear the air as well:

[BBC: The 'creepy Facebook AI' story that captivated the media](http://www.bbc.com/news/technology-40790258)

Opinions/comments?  "
120,machinelearning,openai,comments,2023-11-23 00:14:50,[D] Exclusive: Sam Altman's ouster at OpenAI was precipitated by letter to board about AI breakthrough,blabboy,False,0.83,372,181o1q4,https://www.reddit.com/r/MachineLearning/comments/181o1q4/d_exclusive_sam_altmans_ouster_at_openai_was/,180,1700698490.0,"According to one of the sources, long-time executive Mira Murati told employees on Wednesday that a letter about the AI breakthrough called Q* (pronounced Q-Star), precipitated the board's actions.

The maker of ChatGPT had made progress on Q*, which some internally believe could be a breakthrough in the startup's search for superintelligence, also known as artificial general intelligence (AGI), one of the people told Reuters. OpenAI defines AGI as AI systems that are smarter than humans.

https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/"
121,machinelearning,openai,comments,2018-08-06 17:08:04,[N] OpenAI Five Benchmark: Results,luiscosio,False,0.95,226,9533g8,https://blog.openai.com/openai-five-benchmark-results/,179,1533575284.0,
122,machinelearning,openai,comments,2023-11-20 08:50:54,"[N] Sam Altman and Greg Brockman, together with colleagues, will join Microsoft to lead new advanced AI research team",Civil_Collection7267,False,0.96,405,17zk6zy,https://www.reddit.com/r/MachineLearning/comments/17zk6zy/n_sam_altman_and_greg_brockman_together_with/,178,1700470254.0,"Source: [https://blogs.microsoft.com/blog/2023/11/19/a-statement-from-microsoft-chairman-and-ceo-satya-nadella/](https://blogs.microsoft.com/blog/2023/11/19/a-statement-from-microsoft-chairman-and-ceo-satya-nadella/)

>We remain committed to our partnership with OpenAI and have confidence in our product roadmap, our ability to continue to innovate with everything we announced at Microsoft Ignite, and in continuing to support our customers and partners. We look forward to getting to know Emmett Shear and OAI’s new leadership team and working with them. And we’re extremely excited to share the news that Sam Altman and Greg Brockman, together with colleagues, will be joining Microsoft to lead a new advanced AI research team. We look forward to moving quickly to provide them with the resources needed for their success.

News article covering the situation: [https://www.theverge.com/2023/11/20/23968829/microsoft-hires-sam-altman-greg-brockman-employees-openai](https://www.theverge.com/2023/11/20/23968829/microsoft-hires-sam-altman-greg-brockman-employees-openai)

>Altman’s Microsoft hiring comes just hours after negotiations with OpenAI’s board failed to bring him back as OpenAI CEO. Instead, former Twitch CEO and co-founder Emmett Shear has been named as interim CEO.  
>  
>Altman had been negotiating to return as OpenAI CEO, but OpenAI’s four-person board refused to step down and let him return."
123,machinelearning,openai,comments,2023-09-21 15:01:28,[N] OpenAI's new language model gpt-3.5-turbo-instruct can defeat chess engine Fairy-Stockfish 14 at level 5,Wiskkey,False,0.92,116,16oi6fb,https://www.reddit.com/r/MachineLearning/comments/16oi6fb/n_openais_new_language_model_gpt35turboinstruct/,178,1695308488.0,"[This Twitter thread](https://twitter.com/GrantSlatton/status/1703913578036904431) ([Nitter alternative](https://nitter.net/GrantSlatton/status/1703913578036904431) for those who aren't logged into Twitter and want to see the full thread) claims that [OpenAI's new language model gpt-3.5-turbo-instruct](https://analyticsindiamag.com/openai-releases-gpt-3-5-turbo-instruct/) can ""readily"" beat Lichess Stockfish level 4 ([Lichess Stockfish level and its rating](https://lichess.org/@/MagoGG/blog/stockfish-level-and-its-rating/CvL5k0jL)) and has a chess rating of ""around 1800 Elo."" [This tweet](https://twitter.com/nabeelqu/status/1703961405999759638) shows the style of prompts that are being used to get these results with the new language model.

I used website parrotchess\[dot\]com (discovered [here](https://twitter.com/OwariDa/status/1704179448013070560)) to play multiple games of chess purportedly pitting this new language model vs. various levels at website Lichess, which supposedly uses Fairy-Stockfish 14 according to the Lichess user interface. My current results for all completed games: The language model is 5-0 vs. Fairy-Stockfish 14 level 5 ([game 1](https://lichess.org/eGSWJtNq), [game 2](https://lichess.org/pN7K9bdS), [game 3](https://lichess.org/aK4jQvdo), [game 4](https://lichess.org/S9SGg8YI), [game 5](https://lichess.org/OqzdkDhE)), and 2-5 vs. Fairy-Stockfish 14 level 6 ([game 1](https://lichess.org/zP68C6H4), [game 2](https://lichess.org/4XKUIDh1), [game 3](https://lichess.org/1zTasRRp), [game 4](https://lichess.org/lH1EMqJQ), [game 5](https://lichess.org/mdFlTbMn), [game 6](https://lichess.org/HqmELNhw), [game 7](https://lichess.org/inWVs05Q)). Not included in the tally are games that I had to abort because the parrotchess user interface stalled (5 instances), because I accidentally copied a move incorrectly in the parrotchess user interface (numerous instances), or because the parrotchess user interface doesn't allow the promotion of a pawn to anything other than queen (1 instance). **Update: There could have been up to 5 additional losses - the number of times the parrotchess user interface stalled - that would have been recorded in this tally if** [this language model resignation bug](https://twitter.com/OwariDa/status/1705894692603269503) **hadn't been present. Also, the quality of play of some online chess bots can perhaps vary depending on the speed of the user's hardware.**

The following is a screenshot from parrotchess showing the end state of the first game vs. Fairy-Stockfish 14 level 5:

https://preview.redd.it/4ahi32xgjmpb1.jpg?width=432&format=pjpg&auto=webp&s=7fbb68371ca4257bed15ab2828fab58047f194a4

The game results in this paragraph are from using parrotchess after the forementioned resignation bug was fixed. The language model is 0-1 vs. Fairy-Stockfish level 7 ([game 1](https://lichess.org/Se3t7syX)), and 0-1 vs. Fairy-Stockfish 14 level 8 ([game 1](https://lichess.org/j3W2OwrP)).

There is [one known scenario](https://twitter.com/OwariDa/status/1706823943305167077) ([Nitter alternative](https://nitter.net/OwariDa/status/1706823943305167077)) in which the new language model purportedly generated an illegal move using language model sampling temperature of 0. Previous purported illegal moves that the parrotchess developer examined [turned out](https://twitter.com/OwariDa/status/1706765203130515642) ([Nitter alternative](https://nitter.net/OwariDa/status/1706765203130515642)) to be due to parrotchess bugs.

There are several other ways to play chess against the new language model if you have access to the OpenAI API. The first way is to use the OpenAI Playground as shown in [this video](https://www.youtube.com/watch?v=CReHXhmMprg). The second way is chess web app gptchess\[dot\]vercel\[dot\]app (discovered in [this Twitter thread](https://twitter.com/willdepue/status/1703974001717154191) / [Nitter thread](https://nitter.net/willdepue/status/1703974001717154191)). Third, another person modified that chess web app to additionally allow various levels of the Stockfish chess engine to autoplay, resulting in chess web app chessgpt-stockfish\[dot\]vercel\[dot\]app (discovered in [this tweet](https://twitter.com/paul_cal/status/1704466755110793455)).

Results from other people:

a) Results from hundreds of games in blog post [Debunking the Chessboard: Confronting GPTs Against Chess Engines to Estimate Elo Ratings and Assess Legal Move Abilities](https://blog.mathieuacher.com/GPTsChessEloRatingLegalMoves/).

b) Results from 150 games: [GPT-3.5-instruct beats GPT-4 at chess and is a \~1800 ELO chess player. Results of 150 games of GPT-3.5 vs stockfish and 30 of GPT-3.5 vs GPT-4](https://www.reddit.com/r/MachineLearning/comments/16q81fh/d_gpt35instruct_beats_gpt4_at_chess_and_is_a_1800/). [Post #2](https://www.reddit.com/r/chess/comments/16q8a3b/new_openai_model_gpt35instruct_is_a_1800_elo/). The developer later noted that due to bugs the legal move rate [was](https://twitter.com/a_karvonen/status/1706057268305809632) actually above 99.9%. It should also be noted that these results [didn't use](https://www.reddit.com/r/chess/comments/16q8a3b/comment/k1wgg0j/) a language model sampling temperature of 0, which I believe could have induced illegal moves.

c) Chess bot [gpt35-turbo-instruct](https://lichess.org/@/gpt35-turbo-instruct/all) at website Lichess.

d) Chess bot [konaz](https://lichess.org/@/konaz/all) at website Lichess.

From blog post [Playing chess with large language models](https://nicholas.carlini.com/writing/2023/chess-llm.html):

>Computers have been better than humans at chess for at least the last 25 years. And for the past five years, deep learning models have been better than the best humans. But until this week, in order to be good at chess, a machine learning model had to be explicitly designed to play games: it had to be told explicitly that there was an 8x8 board, that there were different pieces, how each of them moved, and what the goal of the game was. Then it had to be trained with reinforcement learning agaist itself. And then it would win.  
>  
>This all changed on Monday, when OpenAI released GPT-3.5-turbo-instruct, an instruction-tuned language model that was designed to just write English text, but that people on the internet quickly discovered can play chess at, roughly, the level of skilled human players.

Post [Chess as a case study in hidden capabilities in ChatGPT](https://www.lesswrong.com/posts/F6vH6fr8ngo7csDdf/chess-as-a-case-study-in-hidden-capabilities-in-chatgpt) from last month covers a different prompting style used for the older chat-based GPT 3.5 Turbo language model. If I recall correctly from my tests with ChatGPT-3.5, using that prompt style with the older language model can defeat Stockfish level 2 at Lichess, but I haven't been successful in using it to beat Stockfish level 3. In my tests, both the quality of play and frequency of illegal attempted moves seems to be better with the new prompt style with the new language model compared to the older prompt style with the older language model.

Related article: [Large Language Model: world models or surface statistics?](https://thegradient.pub/othello/)

P.S. Since some people claim that language model gpt-3.5-turbo-instruct is always playing moves memorized from the training dataset, I searched for data on the uniqueness of chess positions. From [this video](https://youtu.be/DpXy041BIlA?t=2225), we see that for a certain game dataset there were 763,331,945 chess positions encountered in an unknown number of games without removing duplicate chess positions, 597,725,848 different chess positions reached, and 582,337,984 different chess positions that were reached only once. Therefore, for that game dataset the probability that a chess position in a game was reached only once is 582337984 / 763331945 = 76.3%. For the larger dataset [cited](https://youtu.be/DpXy041BIlA?t=2187) in that video, there are approximately (506,000,000 - 200,000) games in the dataset (per [this paper](http://tom7.org/chess/survival.pdf)), and 21,553,382,902 different game positions encountered. Each game in the larger dataset added a mean of approximately 21,553,382,902 / (506,000,000 - 200,000) = 42.6 different chess positions to the dataset. For [this different dataset](https://lichess.org/blog/Vs0xMTAAAD4We4Ey/opening-explorer) of \~12 million games, \~390 million different chess positions were encountered. Each game in this different dataset added a mean of approximately (390 million / 12 million) = 32.5 different chess positions to the dataset. From the aforementioned numbers, we can conclude that a strategy of playing only moves memorized from a game dataset would fare poorly because there are not rarely new chess games that have chess positions that are not present in the game dataset."
124,machinelearning,openai,comments,2021-06-01 17:40:23,"[R] Chinese AI lab challenges Google, OpenAI with a model of 1.75 trillion parameters",liqui_date_me,False,0.89,361,npzqks,https://www.reddit.com/r/MachineLearning/comments/npzqks/r_chinese_ai_lab_challenges_google_openai_with_a/,167,1622569223.0,"Link here: https://en.pingwest.com/a/8693

TL;DR The Beijing Academy of Artificial Intelligence, styled as BAAI and known in Chinese as 北京智源人工智能研究院, launched the latest version of Wudao 悟道, a pre-trained deep learning model that the lab dubbed as “China’s first,” and “the world’s largest ever,” with a whopping 1.75 trillion parameters.

And the corresponding twitter thread: https://twitter.com/DavidSHolz/status/1399775371323580417

What's interesting here is BAAI is funded in part by the China’s Ministry of Science and Technology, which is China's equivalent of the NSF. The equivalent of this in the US would be for the NSF allocating billions of dollars a year *only to train models*."
125,machinelearning,openai,comments,2023-05-22 16:15:53,[R] GPT-4 didn't really score 90th percentile on the bar exam,salamenzon,False,0.97,840,13ovc04,https://www.reddit.com/r/MachineLearning/comments/13ovc04/r_gpt4_didnt_really_score_90th_percentile_on_the/,160,1684772153.0,"According to [this article](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4441311), OpenAI's claim that it scored 90th percentile on the UBE appears to be based on approximate conversions from estimates of February administrations of the Illinois Bar Exam, which ""are heavily skewed towards repeat test-takers who failed the July administration and score significantly lower than the general test-taking population.""

Compared to July test-takers, GPT-4's UBE score would be 68th percentile, including \~48th on essays. Compared to first-time test takers, GPT-4's UBE score is estimated to be \~63rd percentile, including \~42nd on essays. Compared to those who actually passed, its UBE score would be \~48th percentile, including \~15th percentile on essays."
126,machinelearning,openai,comments,2023-12-20 13:59:53,[D] Mistral received funding and is worth billions now. Are open source LLMs the future?,BelowaverageReggie34,False,0.96,437,18mv8le,https://www.reddit.com/r/MachineLearning/comments/18mv8le/d_mistral_received_funding_and_is_worth_billions/,156,1703080793.0," Came across this intriguing [article](https://gizmodo.com/mistral-artificial-intelligence-gpt-3-openai-1851091217) about Mistral, an open-source LLM that recently scored 400 million in funding, now valued at 2 billion. Are open-source LLMs gonna be the future? Considering the trust issues with ChatGPT and the debates about its safety, the idea of open-source LLMs seems to be the best bet imo.

Unlike closed-source models, users can verify the privacy claims of open-source models. There have been some good things being said about Mistral, and I only hope such open source LLMs secure enough funding to compete with giants like OpenAI. Maybe then, ChatGPT will also be forced to go open source?

With that said, I'm also hopeful that competitors like [Silatus](https://silatus.com/) and [Durable](https://durable.co/), which already use multiple models, consider using open-source models like Mistral into their frameworks. If that happens, maybe there might be a shift in AI privacy. What do you guys think? Are open-source LLMs the future, especially with the funding backing them?"
127,machinelearning,openai,comments,2019-03-11 16:19:00,[N] OpenAI LP,SkiddyX,False,0.94,309,azvbmn,https://www.reddit.com/r/MachineLearning/comments/azvbmn/n_openai_lp/,150,1552321140.0,"""We’ve created OpenAI LP, a new “capped-profit” company that allows us to rapidly increase our investments in compute and talent while including checks and balances to actualize our mission.""

Sneaky.

https://openai.com/blog/openai-lp/"
128,machinelearning,openai,comments,2023-03-10 08:50:32,[D] Is ML a big boys game now?,TheStartIs2019,False,0.83,180,11njpb9,https://www.reddit.com/r/MachineLearning/comments/11njpb9/d_is_ml_a_big_boys_game_now/,146,1678438232.0,"As much as I enjoy ML as a whole, I am a bit skeptical of the future for individuals. With OpenAI trying to monopolize the market along with Microsoft, which part remains for the small time researchers/developers?

It seems everything now is just a ChatGPT wrapper, and with GPT-4 around the corner I assume itll be even more prominent.

What are your thoughts?"
129,machinelearning,openai,comments,2023-03-23 18:09:11,[N] ChatGPT plugins,Singularian2501,False,0.97,440,11zsdwv,https://www.reddit.com/r/MachineLearning/comments/11zsdwv/n_chatgpt_plugins/,144,1679594951.0,"[https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins)

>We’ve implemented initial support for plugins in ChatGPT. Plugins are tools designed specifically for language models with safety as a core principle, and help ChatGPT access up-to-date information, run  computations, or use third-party services."
130,machinelearning,openai,comments,2019-07-23 02:29:08,[D] What is OpenAI? I don't know anymore.,milaworld,False,0.95,540,cgmptl,https://www.reddit.com/r/MachineLearning/comments/cgmptl/d_what_is_openai_i_dont_know_anymore/,144,1563848948.0,"*Some [commentary](https://threadreaderapp.com/thread/1153364705777311745.html) from [Smerity](https://twitter.com/Smerity/status/1153364705777311745) about yesterday's [cash infusion](https://openai.com/blog/microsoft/) from MS into OpenAI:*

What is OpenAI? I don't know anymore.
A non-profit that leveraged good will whilst silently giving out equity for [years](https://twitter.com/gdb/status/1105137541970243584) prepping a shift to for-profit that is now seeking to license closed tech through a third party by segmenting tech under a banner of [pre](https://twitter.com/tsimonite/status/1153340994986766336)/post ""AGI"" technology?

The non-profit/for-profit/investor [partnership](https://openai.com/blog/openai-lp/) is held together by a set of legal documents that are entirely novel (=bad term in legal docs), are [non-public](https://twitter.com/gdb/status/1153305526026956800) + unclear, have no case precedence, yet promise to wed operation to a vague (and already re-interpreted) [OpenAI Charter](https://openai.com/charter/).

The claim is that [AGI](https://twitter.com/woj_zaremba/status/1105149945118519296) needs to be carefully and collaboratively guided into existence yet the output of almost [every](https://github.com/facebookresearch) [other](https://github.com/google-research/google-research) [existing](https://github.com/salesforce) [commercial](https://github.com/NVlabs) lab is more open. OpenAI runs a closed ecosystem where they primarily don't or won't trust outside of a small bubble.

I say this knowing many of the people there and with past and present love in my heart—I don't collaborate with OpenAI as I have no freaking clue what they're doing. Their primary form of communication is high entropy blog posts that'd be shock pivots for any normal start-up.

Many of their [blog posts](https://openai.com/blog/cooperation-on-safety/) and [spoken](https://www.youtube.com/watch?v=BJi6N4tDupk) [positions](https://www.youtube.com/watch?v=9EN_HoEk3KY) end up [influencing government policy](https://twitter.com/jackclarkSF/status/986568940028616705) and public opinion on the future of AI through amplified pseudo-credibility due to *Open*, *Musk founded*, repeatedly hyped statements, and a sheen from their now distant non-profit good will era.

I have mentioned this to friends there and say all of this with positive sum intentions: I understand they have lofty aims, I understand they need cash to shovel into the forever unfurling GPU forge, but if they want any community trust long term they need a better strategy.

The implicit OpenAI message heard over the years:
“Think of how transformative and dangerous AGI may be. Terrifying. Trust us. Whether it's black-boxing technology, legal risk, policy initiatives, investor risk, ...—trust us with everything. We're good. No questions, sorry.”

*We'll clarify our position in an upcoming blog post.*"
131,machinelearning,openai,comments,2020-02-18 00:19:40,"[D] The messy, secretive reality behind OpenAI’s bid to save the world",milaworld,False,0.93,358,f5immz,https://www.reddit.com/r/MachineLearning/comments/f5immz/d_the_messy_secretive_reality_behind_openais_bid/,143,1581985180.0,"A new [story](https://www.technologyreview.com/s/615181/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/) by journalist [Karen Hao](https://mobile.twitter.com/_KarenHao/status/1229519114638589953) who spent six months digging into OpenAI.

She started with a few simple questions: Who are they? What are their goals? How do they work? After nearly three dozen interviews, she found so much more.

The article is worth a read. I'm not going to post an excerpt here.

The most surprising thing is that Elon Musk himself, after that article got published, [criticized](https://www.twitter.com/elonmusk/status/1229544673590599681) OpenAI and tweeted that they ""should be more open"" 🔥

With regards to AI safety, Elon [said](https://www.twitter.com/elonmusk/status/1229546206948462597) ""I have no control & only very limited insight into OpenAI. Confidence in Dario for safety is not high.""

Here is the link to the article again: https://www.technologyreview.com/s/615181/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/"
132,machinelearning,openai,comments,2023-03-28 05:57:03,[N] OpenAI may have benchmarked GPT-4’s coding ability on it’s own training data,Balance-,False,0.97,1000,124eyso,https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/,135,1679983023.0,"[GPT-4 and professional benchmarks: the wrong answer to the wrong question](https://aisnakeoil.substack.com/p/gpt-4-and-professional-benchmarks)

*OpenAI may have tested on the training data. Besides, human benchmarks are meaningless for bots.*

 **Problem 1: training data contamination**

To benchmark GPT-4’s coding ability, OpenAI evaluated it on problems from Codeforces, a website that hosts coding competitions. Surprisingly, Horace He pointed out that GPT-4 solved 10/10 pre-2021 problems and 0/10 recent problems in the easy category. The training data cutoff for GPT-4 is September 2021. This strongly suggests that the model is able to memorize solutions from its training set — or at least partly memorize them, enough that it can fill in what it can’t recall.

As further evidence for this hypothesis, we tested it on Codeforces problems from different times in 2021. We found that it could regularly solve problems in the easy category before September 5, but none of the problems after September 12.

In fact, we can definitively show that it has memorized problems in its training set: when prompted with the title of a Codeforces problem, GPT-4 includes a link to the exact contest where the problem appears (and the round number is almost correct: it is off by one). Note that GPT-4 cannot access the Internet, so memorization is the only explanation."
133,machinelearning,openai,comments,2023-11-22 06:38:48,"OpenAI: ""We have reached an agreement in principle for Sam to return to OpenAI as CEO"" [N]",we_are_mammals,False,0.92,285,1812w04,https://www.reddit.com/r/MachineLearning/comments/1812w04/openai_we_have_reached_an_agreement_in_principle/,129,1700635128.0,"OpenAI announcement:

""We have reached an agreement in principle for Sam to return to OpenAI as CEO with a new initial board of Bret Taylor (Chair), Larry Summers, and Adam D'Angelo.

We are collaborating to figure out the details. Thank you so much for your patience through this.""

https://twitter.com/OpenAI/status/1727205556136579362"
134,machinelearning,openai,comments,2019-02-14 17:09:53,[R] OpenAI: Better Language Models and Their Implications,jinpanZe,False,0.95,298,aqlzde,https://www.reddit.com/r/MachineLearning/comments/aqlzde/r_openai_better_language_models_and_their/,128,1550164193.0,"https://blog.openai.com/better-language-models/

""We’ve trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization — all without task-specific training.""

Interestingly,

""Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much smaller model for researchers to experiment with, as well as a technical paper."""
135,machinelearning,openai,comments,2023-01-22 13:44:49,[D] Couldn't devs of major GPTs have added an invisible but detectable watermark in the models?,scarynut,False,0.79,151,10ijzi2,https://www.reddit.com/r/MachineLearning/comments/10ijzi2/d_couldnt_devs_of_major_gpts_have_added_an/,127,1674395089.0,"So LLMs like GPT3 have understandably raised concerns about the disruptiveness of faked texts, faked images and video, faked speech and so on. While this may likely change soon, as of now OpenAI controls the most accessible and competent LLM. And OpenAIs agenda is said in their own words to be to benefit mankind.

If so, wouldn't it make sense to add a sort of watermark to the output? A watermark built into the model parameters so that it could not easily be removed, but still detectable with some key or some other model. While it may not matter in the long run, it would set a precedent to further development and demonstrate some kind of responsibility for the disruptive nature of LLMs/GPTs.

Would it not be technically possible, nä would it make sense?"
136,machinelearning,openai,comments,2022-08-22 21:00:01,[D] StableDiffusion v1.4 is entirely public. What do you think about Stability.ai ?,dasayan05,False,0.98,429,wv50uh,https://www.reddit.com/r/MachineLearning/comments/wv50uh/d_stablediffusion_v14_is_entirely_public_what_do/,123,1661202001.0,"In case you haven't noticed, [stability.ai](https://stability.ai) just open-sourced their latest version of StableDiffusion to the public. Here is the link: [https://stability.ai/blog/stable-diffusion-public-release](https://stability.ai/blog/stable-diffusion-public-release)

It is so fast and small (memory footprint) that it can run on consumer grade GPUs. I just generated my first ""astronaut riding a horse on mars"" on my local GTX3090.

[Astronaut riding a horse on mars](https://preview.redd.it/jpceq4klwbj91.png?width=512&format=png&auto=webp&s=b84b7c1cf7e09fdcf326145e5d17485c9376ffb4)

So what is opinion on open-sourcing such powerful models ? And, what do you think about [stability.ai](https://stability.ai) as an organisation ? Do you feel they can potentially be the next OpenAI ?"
137,machinelearning,openai,comments,2022-06-21 15:17:08,"[N] [D] Openai, who runs DALLE-2 alleged threatened creator of DALLE-Mini",DigThatData,False,0.86,265,vhfp1t,https://www.reddit.com/r/MachineLearning/comments/vhfp1t/n_d_openai_who_runs_dalle2_alleged_threatened/,120,1655824628.0,"Trying to cross-post what I think is a discussion that is relevant to this community. This is my third attempt, I hope I'm doing it correctly this time: 

https://www.reddit.com/r/dalle2/comments/vgtgdc/openai_who_runs_dalle2_alleged_threatened_creator/

EDIT: here are the original pre-prints for added context:

* DALL-E: [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092) - The only place the term ""DALL-E"" appears is the URL to the github repo.
* Dall-E 2: [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125) - They consistently refer to the first paper as ""DALL-E"", but refer to the work being described in the new paper as ""unCLIP"" and are careful to only use 'DALL-E 2' in the context of a product description, e.g. ""DALL·E 2 Preview platform (the first deployment of an unCLIP model)"""
138,machinelearning,openai,comments,2023-03-01 18:31:12,[D] OpenAI introduces ChatGPT and Whisper APIs (ChatGPT API is 1/10th the cost of GPT-3 API),minimaxir,False,0.97,575,11fbccz,https://www.reddit.com/r/MachineLearning/comments/11fbccz/d_openai_introduces_chatgpt_and_whisper_apis/,119,1677695472.0,"https://openai.com/blog/introducing-chatgpt-and-whisper-apis

> It is priced at $0.002 per 1k tokens, which is 10x cheaper than our existing GPT-3.5 models.

This is a massive, massive deal. For context, the reason GPT-3 apps took off over the past few months before ChatGPT went viral is because a) text-davinci-003 was released and was a significant performance increase and b) the cost was cut from $0.06/1k tokens to $0.02/1k tokens, which made consumer applications feasible without a large upfront cost.

A much better model and a 1/10th cost warps the economics completely to the point that it may be better than in-house finetuned LLMs.

I have no idea how OpenAI can make money on this. This has to be a loss-leader to lock out competitors before they even get off the ground."
139,machinelearning,openai,comments,2019-02-17 22:32:00,[D] OpenAI Not Releasing Fully Trained Model - Is it just an exercise in delaying the inevitable.,JamieMcLachlan,False,0.92,224,arpysz,https://www.reddit.com/r/MachineLearning/comments/arpysz/d_openai_not_releasing_fully_trained_model_is_it/,119,1550442720.0,OpenAI has gone against their standard practice of releasing a fully trained model and instead has released a smaller model for experimentation. They have stated that it is out of fear of it being utilized by people with malicious intent. Do people think that eventually technology like this will end up in the hands of people with malicious intent and if so is this just an exercise in delaying the inevitable?  
140,machinelearning,openai,comments,2020-01-30 17:11:51,[N] OpenAI Switches to PyTorch,SkiddyX,False,0.99,571,ew8oxq,https://www.reddit.com/r/MachineLearning/comments/ew8oxq/n_openai_switches_to_pytorch/,119,1580404311.0,"""We're standardizing OpenAI's deep learning framework on PyTorch to increase our research productivity at scale on GPUs (and have just released a PyTorch version of Spinning Up in Deep RL)""

https://openai.com/blog/openai-pytorch/"
141,machinelearning,openai,comments,2023-05-19 20:36:36,Does anyone else suspect that the official iOS ChatGPT app might be conducting some local inference / edge-computing? [Discussion],altoidsjedi,False,0.86,212,13m70qv,https://www.reddit.com/r/MachineLearning/comments/13m70qv/does_anyone_else_suspect_that_the_official_ios/,117,1684528596.0,"I've noticed a couple interesting things while using the official ChatGPT app:

1. Firstly, I noticed my iPhone heats up and does things like reducing screen brightness -- which is what I normally see it do when im doing something computationally intensive for an iPhone, like using photo or video editing apps.
2. I also noticed that if I start a conversation on the iPhone app and then resume it on the browser, I get a message saying ""The previous model used in this conversation is unavailable. We've switched you to the latest default model."" I get this message regardless of if I use GPT-3.5 or GPT-4, but NOT if I use GPT-4 with plugins or web-browsing.

This, along with the fact that OpenAI took 8 months to release what one might have considered to be relatively simple web-app -- and that they've only released it so far on iOS, which has a pretty uniform and consistent environment when it comes to machine learning hardware (the Apple Neural Engine) -- makes me thing that they are experimenting with GPT models that are conducing at least SOME of their machine learning inference ON the device, rather than through the cloud.

It wouldn't be shocking if they were -- ever since Meta's LLaMA models were released into the wild, we've seen absolutely mind-blowing advances in terms of people creating more efficient and effective models with smaller parameter sizes. We've also seen LLMs to start working on less and less powerful devices, such as consumer-grade computers / smartphones / etc.

This, plus the rumors that OpenAI might be releasing their own open-source model to the public in the near future makes me think that the ChatGPT app might in fact be a first step toward GPT systems running at least PARTIALLY on devices locally.

Curious what anyone else here has observed or thinks."
142,machinelearning,openai,comments,2020-09-22 17:40:14,[N] Microsoft teams up with OpenAI to exclusively license GPT-3 language model,kit1980,False,0.96,317,ixs88q,https://www.reddit.com/r/MachineLearning/comments/ixs88q/n_microsoft_teams_up_with_openai_to_exclusively/,117,1600796414.0,"""""""OpenAI will continue to offer GPT-3 and other powerful models via its own Azure-hosted API, launched in June. While we’ll be hard at work utilizing the capabilities of GPT-3 in our own products, services and experiences to benefit our customers, we’ll also continue to work with OpenAI to keep looking forward: leveraging and democratizing the power of their cutting-edge AI research as they continue on their mission to build safe artificial general intelligence.""""""

https://blogs.microsoft.com/blog/2020/09/22/microsoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/"
143,machinelearning,openai,comments,2023-11-09 18:04:17,[D] What AI topics are you curious about but rarely see in the spotlight?,GratefullyFriendly73,False,0.94,151,17riznw,https://www.reddit.com/r/MachineLearning/comments/17riznw/d_what_ai_topics_are_you_curious_about_but_rarely/,117,1699553057.0,"I'm a data engineer who somehow ended up as a software developer. So many of my friends are working now with the OpenAI api to add generative capabilities to their product, but they lack A LOT of context when it comes to how LLMs actually works. 

This is why I started writing popular-science style articles that unpack AI concepts for software developers working on real-world application. It started kind of slow, honestly I wrote a bit too ""brainy"" for them, but now I've found a voice that resonance with this audience much better and I want to ramp up my writing cadence.

I would love to hear your thoughts about what concepts I should write about next?   
What get you excited and you find hard to explain to someone with a different background? "
144,machinelearning,openai,comments,2023-11-22 05:09:01,[D] Without a clear leader in AI do you think the race to AGI will start to get chaotic?,Ok_Reality2341,False,0.21,0,1811g8a,https://www.reddit.com/r/MachineLearning/comments/1811g8a/d_without_a_clear_leader_in_ai_do_you_think_the/,116,1700629741.0,"Title really - right now the leader of OpenAI is blurry, which isn’t good for the industry to follow. 

We need a strong leader to focus our efforts, but if this isn’t the case - perhaps it’ll get chaotic?

Edit: I am here for an in-depth philosophical discussion of ML "
145,machinelearning,openai,comments,2020-08-22 17:16:08,"[N] GPT-3, Bloviator: OpenAI’s language generator has no idea what it’s talking about",rafgro,False,0.94,312,iemck2,https://www.reddit.com/r/MachineLearning/comments/iemck2/n_gpt3_bloviator_openais_language_generator_has/,111,1598116568.0,"MIT Tech Review's article: [https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/](https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/)

>As we were putting together this essay, our colleague Summers-Stay, who is good with metaphors, wrote to one of us, saying this: ""GPT is odd because it doesn’t 'care' about getting the right answer to a question you put to it. It’s more like an improv actor who is totally dedicated to their craft, never breaks character, and has never left home but only read about the world in books. Like such an actor, when it doesn’t know something, it will just fake it. You wouldn’t trust an improv actor playing a doctor to give you medical advice."""
146,machinelearning,openai,comments,2023-05-12 11:49:42,Open-source LLMs cherry-picking? [D],CacheMeUp,False,0.9,198,13fiw7r,https://www.reddit.com/r/MachineLearning/comments/13fiw7r/opensource_llms_cherrypicking_d/,110,1683892182.0," Tried many small (<13B parameters) open-source LLMs on zero-shot classification tasks as instruction following (""Below is an input, answer the following yes/no question...""). All of them (except Flan-T5 family) yielded very poor results, including non-sensical text, failure to follow even single-step instructions and sometimes just copying the whole input to the output.

This is in strike contrast to the demos and results posted on the internet. Only OpenAI models provide consistently good (though inaccurate sometimes) results out of the box.

What could cause of this gap? Is it the generation hyperparameters or do these model require fine-tuning for classification?"
147,machinelearning,openai,comments,2021-09-06 13:39:07,[D] How OpenAI Sold its Soul for $1 Billion: The company behind GPT-3 and Codex isn’t as open as it claims.,sensetime,False,0.95,665,pizllt,https://www.reddit.com/r/MachineLearning/comments/pizllt/d_how_openai_sold_its_soul_for_1_billion_the/,107,1630935547.0,"An essay by Alberto Romero that traces the history and developments of OpenAI from the time it became a ""capped-for-profit"" entity from a non-profit entity:

Link: https://onezero.medium.com/openai-sold-its-soul-for-1-billion-cf35ff9e8cd4"
148,machinelearning,openai,comments,2023-03-30 22:40:29,[P] Introducing Vicuna: An open-source language model based on LLaMA 13B,Business-Lead2679,False,0.95,288,1271po7,https://www.reddit.com/r/MachineLearning/comments/1271po7/p_introducing_vicuna_an_opensource_language_model/,107,1680216029.0,"We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%\* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%\* of cases. The cost of training Vicuna-13B is around $300. The training and serving [code](https://github.com/lm-sys/FastChat), along with an online [demo](https://chat.lmsys.org/), are publicly available for non-commercial use.

# Training details

Vicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model’s maximum context length.

Our training recipe builds on top of [Stanford’s alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) with the following improvements.

* **Memory Optimizations:** To enable Vicuna’s understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing [gradient checkpointing](https://arxiv.org/abs/1604.06174) and [flash attention](https://arxiv.org/abs/2205.14135).
* **Multi-round conversations:** We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot’s output.
* **Cost Reduction via Spot Instance:** The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ [SkyPilot](https://github.com/skypilot-org/skypilot) [managed spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html) to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.

&#x200B;

[Vicuna - Online demo](https://reddit.com/link/1271po7/video/0qsiu08kdyqa1/player)

# Limitations

We have noticed that, similar to other large language models, Vicuna has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. Additionally, it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI [moderation](https://platform.openai.com/docs/guides/moderation/overview) API to filter out inappropriate user inputs in our online demo. Nonetheless, we anticipate that Vicuna can serve as an open starting point for future research to tackle these limitations.

[Relative Response Quality Assessed by GPT-4](https://preview.redd.it/1rnmhv01eyqa1.png?width=599&format=png&auto=webp&s=02b4d415b5d378851bb70e225f1b1ebce98bfd83)

&#x200B;

For more information, check [https://vicuna.lmsys.org/](https://vicuna.lmsys.org/)

Online demo: [https://chat.lmsys.org/](https://chat.lmsys.org/)

&#x200B;

All credits go to the creators of this model. I did not participate in the creation of this model nor in the fine-tuning process. Usage of this model falls under a non-commercial license."
149,machinelearning,openai,comments,2023-10-20 14:12:12,[D] Is anyone else tired of “whatever OpenAI does is the best!” narrative?,mildlyphd,False,0.79,180,17cc8on,https://www.reddit.com/r/MachineLearning/comments/17cc8on/d_is_anyone_else_tired_of_whatever_openai_does_is/,107,1697811132.0,"The title says it all. I agree what they did is incredible and literally changed AI landscape in last couple of years. But I’m getting tired of everyone acting like OpenAI is the only one doing great research. The twit-fluencers praising even the slightest peep from them. I don’t understand this fanaticism in AI community. There are smart researchers doing smart things all over the world. But they don’t even get a fraction of appreciation they deserve. And the strangest thing of all, ChatGPT is used as oracle to evaluate models in research papers. Consistency models are extremely meh and if it did not come out of openAI, people would’ve forgotten them a long time ago!

Edit 1: I’m in grad school and that’s all a lot of students around me talk about/ chase. I want to work on a bit more fundamental problems, but I feel like I’m being left behind. 

Edit 2: This post is mostly a rant about academics obsessed with OpenAI research/products and LLMs. "
150,machinelearning,openai,comments,2019-04-24 14:39:10,[D] Have we hit the limits of Deep Reinforcement Learning?,AnvaMiba,False,0.87,234,bgvefd,https://www.reddit.com/r/MachineLearning/comments/bgvefd/d_have_we_hit_the_limits_of_deep_reinforcement/,106,1556116750.0,"As per [this thread](https://www.reddit.com/r/MachineLearning/comments/bfq8v9/d_openai_five_vs_humans_currently_at_410633_992/) and [this tweet](https://twitter.com/sherjilozair/status/1119256767798620161?s=20), Open AI Five was trained on something like 45,000 years of gameplay experience, and it took less than one day for humans to figure out strategies to consistently beat it.

Open AI Five, together with AlphaStar, is the largest and most sophisiticated implementation of DRL, and yet it falls short of human intelligence by this huge margin. And I bet that AlphaStar would succumb to the same fate if they released it as a bot for anybody to play with.


I know there is lots of research going on to make DRL more data efficient, and to make deep learning in general more robust to out-of-distribution and adversarial examples, but the gap with humans here is so extreme that I doubt it can be meaningfully closed by anything short of a paradigm shift.

What are your thoughts? Is this the limit of what can be achieved by DRL, or is there still hope to push the paradigm foward?"
151,machinelearning,openai,comments,2021-01-04 15:33:43,[D] Why I'm Lukewarm on Graph Neural Networks,VodkaHaze,False,0.96,668,kqazpd,https://www.reddit.com/r/MachineLearning/comments/kqazpd/d_why_im_lukewarm_on_graph_neural_networks/,105,1609774423.0,"**TL;DR:** GNNs can provide wins over simpler embedding methods, but we're at a point where other research directions matter more

I also posted it on my [blog here](https://www.singlelunch.com/2020/12/28/why-im-lukewarm-on-graph-neural-networks/), has footnotes, a nicer layout with inlined images, etc.

-----------

I'm only lukewarm on Graph Neural Networks (GNNs). There, I said it.

It might sound crazy GNNs are one of the hottest fields in machine learning right now. [There][1] were at least [four][2] [review][3] [papers][4] just in the last few months. I think some progress can come of this research, but we're also focusing on some incorrect places.

But first, let's take a step back and go over the basics.

# Models are about compression

We say graphs are a ""non-euclidean"" data type, but that's not really true. A regular graph is just another way to think about a particular flavor of square matrix called the [adjacency matrix][5], like [this](https://www.singlelunch.com/wp-content/uploads/2020/12/AdjacencyMatrices_1002.gif).

It's weird, we look at run-of-the-mill matrix full of real numbers and decide to call it ""non-euclidean"".

This is for practical reasons. Most graphs are fairly sparse, so the matrix is full of zeros. At this point, *where the non-zero numbers are* matters most, which makes the problem closer to (computationally hard) discrete math rather than (easy) continuous, gradient-friendly math.

**If you had the full matrix, life would be easy**

If we step out of the pesky realm of physics for a minute, and assume carrying the full adjacency matrix around isn't a problem, we solve a bunch of problems.

First, network node embeddings aren't a thing anymore. A node is a just row in the matrix, so it's already a vector of numbers.

Second, all network prediction problems are solved. A powerful enough and well-tuned model will simply extract all information between the network and whichever target variable we're attaching to nodes.

**NLP is also just fancy matrix compression**

Let's take a tangent away from graphs to NLP. Most NLP we do can be [thought of in terms of graphs][6] as we'll see, so it's not a big digression.

First, note that Ye Olde word embedding models like [Word2Vec][7] and [GloVe][8] are [just matrix factorization][9].

The GloVe algorithm works on a variation of the old [bag of words][10] matrix. It goes through the sentences and creates a (implicit) [co-occurence][11] graph where nodes are words and the edges are weighed by how often the words appear together in a sentence.

Glove then does matrix factorization on the matrix representation of that co-occurence graph, Word2Vec is mathematically equivalent.

You can read more on this in my [post on embeddings][12] and the one (with code) on [word embeddings][13].

**Even language models are also just matrix compression**

Language models are all the rage. They dominate most of the [state of the art][14] in NLP.

Let's take BERT as our main example. BERT predicts a word given the context of the [rest of the sentence](https://www.singlelunch.com/wp-content/uploads/2020/12/bert.png).

This grows the matrix we're factoring from flat co-occurences on pairs of words to co-occurences conditional on the sentence's context, like [this](https://www.singlelunch.com/wp-content/uploads/2020/12/Screen-Shot-2020-12-28-at-1.59.34-PM.png)

We're growing the ""ideal matrix"" we're factoring combinatorially. As noted by [Hanh & Futrell][15]:

> [...] human language—and language modelling—has infinite statistical complexity but that it can be approximated well at lower levels. This observation has two implications: 1) We can obtain good results with comparatively small models; and 2) there is a lot of potential for scaling up our models. Language models tackle such a large problem space that they probably approximate a compression of the entire language in the [Kolmogorov Complexity][16] sense. It's also possible that huge language models just [memorize a lot of it][17] rather than compress the information, for what it's worth.

### Can we upsample any graph like language models do?

We're already doing it.

Let's call a **first-order** embedding of a graph a method that works by directly factoring the graph's adjacency matrix or [Laplacian matrix][18]. If you embed a graph using [Laplacian Eigenmaps][19] or by taking the [principal components][20] of the Laplacian, that's first order. Similarly, GloVe is a first-order method on the graph of word co-occurences. One of my favorites first order methods for graphs is [ProNE][21], which works as well as most methods while being two orders of magnitude faster.

A **higher-order** method embeds the original matrix plus connections of neighbours-of-neighbours (2nd degree) and deeper k-step connections. [GraRep][22], shows you can always generate higher-order representations from first order methods by augmenting the graph matrix.

Higher order method are the ""upsampling"" we do on graphs. GNNs that sample on large neighborhoods and random-walk based methods like node2vec are doing higher-order embeddings.

# Where are the performance gain?

Most GNN papers in the last 5 years present empirical numbers that are useless for practitioners to decide on what to use.

As noted in the [OpenGraphsBenchmark][4] (OGB) paper, GNN papers do their empirical section on a handful of tiny graphs (Cora, CiteSeer, PubMed) with 2000-20,000 nodes. These datasets can't seriously differentiate between methods.

Recent efforts are directly fixing this, but the reasons why researchers focused on tiny, useless datasets for so long are worth discussing.

**Performance matters by task**

One fact that surprises a lot of people is that even though language models have the best performance in a lot of NLP tasks, if all you're doing is cram sentence embeddings into a downstream model, there [isn't much gained][23] from language models embeddings over simple methods like summing the individual Word2Vec word embeddings (This makes sense, because the full context of the sentence is captured in the sentence co-occurence matrix that is generating the Word2Vec embeddings).

Similarly, [I find][24] that for many graphs **simple first-order methods perform just as well on graph clustering and node label prediction tasks than higher-order embedding methods**. In fact higher-order methods are massively computationally wasteful for these usecases.

Recommended first order embedding methods are ProNE and my [GGVec with order=1][25].

Higher order methods normally perform better on the link prediction tasks. I'm not the only one to find this. In the BioNEV paper, they find: ""A large GraRep order value for link prediction tasks (e.g. 3, 4);a small value for node classification tasks (e.g.1, 2)"" (p.9).

Interestingly, the gap in link prediction performance is inexistant for artificially created graphs. This suggests higher order methods do learn some of the structure intrinsic to [real world graphs][26].

For visualization, first order methods are better. Visualizations of higher order methods tend to have artifacts of their sampling. For instance, Node2Vec visualizations tend to have elongated/filament-like structures which come from the embeddings coming from long single strand random walks. See the following visualizations by [Owen Cornec][27] created by first embedding the graph to 32-300 dimensions using a node embedding algorithm, then mapping this to 2d or 3d with the excellent UMAP algorithm, like [this](https://www.singlelunch.com/wp-content/uploads/2020/12/Screen-Shot-2020-12-28-at-1.59.34-PM-1.png)

Lastly, sometimes simple methods soundly beat higher order methods (there's an instance of it in the OGB paper).

The problem here is that **we don't know when any method is better than another** and **we definitely don't know the reason**.

There's definitely a reason different graph types respond better/worse to being represented by various methods. This is currently an open question.

A big part of why is that the research space is inundated under useless new algorithms because...

# Academic incentives work against progress

Here's the cynic's view of how machine learning papers are made:

1.  Take an existing algorithm
2.  Add some new layer/hyperparameter, make a cute mathematical story for why it matters
3.  Gridsearch your hyperparameters until you beat baselines from the original paper you aped
4.  Absolutely don't gridsearch stuff you're comparing against in your results section
5.  Make a cute ACRONYM for your new method, put impossible to use python 2 code on github (Or no code at all!) and bask in the citations

I'm [not][28] the [only one][29] with these views on the state reproducible research. At least it's gotten slightly better in the last 2 years.

### Sidebar: I hate Node2Vec

A side project of mine is a [node embedding library][25] and the most popular method in it is by far Node2Vec. Don't use Node2Vec.

[Node2Vec][30] with `p=1; q=1` is the [Deepwalk][31] algorithm. Deepwalk is an actual innovation.

The Node2Vec authors closely followed the steps 1-5 including bonus points on step 5 by getting word2vec name recognition.

This is not academic fraud -- the hyperparameters [do help a tiny bit][32] if you gridsearch really hard. But it's the presentable-to-your-parents sister of where you make the ML community worse off to progress your academic career. And certainly Node2Vec doesn't deserve 7500 citations.

# Progress is all about practical issues

We've known how to train neural networks for well over 40 years. Yet they only exploded in popularity with [AlexNet][33] in 2012. This is because implementations and hardware came to a point where deep learning was **practical**.

Similarly, we've known about factoring word co-occurence matrices into Word embeddings for at least 20 years.

But word embeddings only exploded in 2013 with Word2Vec. The breakthrough here was that the minibatch-based methods let you train a Wikipedia-scale embedding model on commodity hardware.

It's hard for methods in a field to make progress if training on a small amount of data takes days or weeks. You're disincentivized to explore new methods. If you want progress, your stuff has to run in reasonable time on commodity hardware. Even Google's original search algorithm [initially ran on commodity hardware][34].

**Efficiency is paramount to progress**

The reason deep learning research took off the way it did is because of improvements in [efficiency][35] as well as much better libraries and hardware support.

**Academic code is terrible**

Any amount of time you spend gridsearching Node2Vec on `p` and `q` is all put to better use gridsearching Deepwalk itself (on number of walks, length of walks, or word2vec hyperparameters). The problem is that people don't gridsearch over deepwalk because implementations are all terrible.

I wrote the [Nodevectors library][36] to have a fast deepwalk implementation because it took **32 hours** to embed a graph with a measly 150,000 nodes using the reference Node2Vec implementation (the same takes 3min with Nodevectors). It's no wonder people don't gridsearch on Deepwalk a gridsearch would take weeks with the terrible reference implementations.

To give an example, in the original paper of [GraphSAGE][37] they their algorithm to DeepWalk with walk lengths of 5, which is horrid if you've ever hyperparameter tuned a deepwalk algorithm. From their paper:

> We did observe DeepWalk’s performance could improve with further training, and in some cases it could become competitive with the unsupervised GraphSAGE approaches (but not the supervised approaches) if we let it run for >1000× longer than the other approaches (in terms of wall clock time for prediction on the test set) I don't even think the GraphSAGE authors had bad intent -- deepwalk implementations are simply so awful that they're turned away from using it properly. It's like trying to do deep learning with 2002 deep learning libraries and hardware.

# Your architectures don't really matter

One of the more important papers this year was [OpenAI's ""Scaling laws""][38] paper, where the raw number of parameters in your model is the most predictive feature of overall performance. This was noted even in the original BERT paper and drives 2020's increase in absolutely massive language models.

This is really just [Sutton' Bitter Lesson][39] in action:

> General methods that leverage computation are ultimately the most effective, and by a large margin

Transformers might be [replacing convolution][40], too. As [Yannic Kilcher said][41], transformers are ruining everything. [They work on graphs][6], in fact it's one of the [recent approaches][42], and seems to be one of the more succesful [when benchmarked][1]

Researchers seem to be putting so much effort into architecture, but it doesn't matter much in the end because you can approximate anything by stacking more layers.

Efficiency wins are great -- but neural net architectures are just one way to achieve that, and by tremendously over-researching this area we're leaving a lot of huge gains elsewhere on the table.

# Current Graph Data Structure Implementations suck

NetworkX is a bad library. I mean, it's good if you're working on tiny graphs for babies, but for anything serious it chokes and forces you to rewrite everything in... what library, really?

At this point most people working on large graphs end up hand-rolling some data structure. This is tough because your computer's memory is a 1-dimensional array of 1's and 0's and a graph has no obvious 1-d mapping.

This is even harder when we take updating the graph (adding/removing some nodes/edges) into account. Here's a few options:

### Disconnected networks of pointers

NetworkX is the best example. Here, every node is an object with a list of pointers to other nodes (the node's edges).

This layout is like a linked list. Linked lists are the [root of all performance evil][43].

Linked lists go completely against how modern computers are designed. Fetching things from memory is slow, and operating on memory is fast (by two orders of magnitude). Whenever you do anything in this layout, you make a roundtrip to RAM. It's slow by design, you can write this in Ruby or C or assembly and it'll be slow regardless, because memory fetches are slow in hardware.

The main advantage of this layout is that adding a new node is O(1). So if you're maintaining a massive graph where adding and removing nodes happens as often as reading from the graph, it makes sense.

Another advantage of this layout is that it ""scales"". Because everything is decoupled from each other you can put this data structure on a cluster. However, you're really creating a complex solution for a problem you created for yourself.

### Sparse Adjacency Matrix

This layout great for read-only graphs. I use it as the backend in my [nodevectors][25] library, and many other library writers use the [Scipy CSR Matrix][44], you can see graph algorithms implemented on it [here][45].

The most popular layout for this use is the [CSR Format][46] where you have 3 arrays holding the graph. One for edge destinations, one for edge weights and an ""index pointer"" which says which edges come from which node.

Because the CSR layout is simply 3 arrays, it scales on a single computer: a CSR matrix can be laid out on a disk instead of in-memory. You simply [memory map][47] the 3 arrays and use them on-disk from there.

With modern NVMe drives random seeks aren't slow anymore, much faster than distributed network calls like you do when scaling the linked list-based graph. I haven't seen anyone actually implement this yet, but it's in the roadmap for my implementation at least.

The problem with this representation is that adding a node or edge means rebuilding the whole data structure.

### Edgelist representations

This representation is three arrays: one for the edge sources, one for the edge destinations, and one for edge weights. [DGL][48] uses this representation internally.

This is a simple and compact layout which can be good for analysis.

The problem compared to CSR Graphs is some seek operations are slower. Say you want all the edges for node #4243. You can't jump there without maintaining an index pointer array.

So either you maintain sorted order and binary search your way there (O(log2n)) or unsorted order and linear search (O(n)).

This data structure can also work on memory mapped disk array, and node append is fast on unsorted versions (it's slow in the sorted version).

# Global methods are a dead end

Methods that work on the **entire graph at once** can't leverage computation, because they run out of RAM at a certain scale.

So any method that want a chance of being the new standard need to be able to update piecemeal on parts of the graph.

**Sampling-based methods**

Sampling Efficiency will matter more in the future

*   **Edgewise local methods**. The only algorithms I know of that do this are GloVe and GGVec, which they pass through an edge list and update embedding weights on each step. 

The problem with this approach is that it's hard to use them for higher-order methods. The advantage is that they easily scale even on one computer. Also, incrementally adding a new node is as simple as taking the existing embeddings, adding a new one, and doing another epoch over the data

*   **Random Walk sampling**. This is used by deepwalk and its descendants, usually for node embeddings rather than GNN methods. This can be computationally expensive and make it hard to add new nodes.

But this does scale, for instance [Instagram][49] use it to feed their recommendation system models

*   **Neighbourhood sampling**. This is currently the most common one in GNNs, and can be low or higher order depending on the neighborhood size. It also scales well, though implementing efficiently can be challenging.

It's currently used by [Pinterest][50]'s recommendation algorithms.

# Conclusion

Here are a few interesting questions:

*   What is the relation between graph types and methods?
*   Consolidated benchmarking like OGB
*   We're throwing random models at random benchmarks without understanding why or when they do better
*   More fundamental research. Heree's one I'm curious about: can other representation types like [Poincarre Embeddings][51] effectively encode directed relationships?

On the other hand, we should **stop focusing on** adding spicy new layers to test on the same tiny datasets. No one cares.

 [1]: https://arxiv.org/pdf/2003.00982.pdf
 [2]: https://arxiv.org/pdf/2002.11867.pdf
 [3]: https://arxiv.org/pdf/1812.08434.pdf
 [4]: https://arxiv.org/pdf/2005.00687.pdf
 [5]: https://en.wikipedia.org/wiki/Adjacency_matrix
 [6]: https://thegradient.pub/transformers-are-graph-neural-networks/
 [7]: https://en.wikipedia.org/wiki/Word2vec
 [8]: https://nlp.stanford.edu/pubs/glove.pdf
 [9]: https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf
 [10]: https://en.wikipedia.org/wiki/Bag-of-words_model
 [11]: https://en.wikipedia.org/wiki/Co-occurrence
 [12]: https://www.singlelunch.com/2020/02/16/embeddings-from-the-ground-up/
 [13]: https://www.singlelunch.com/2019/01/27/word-embeddings-from-the-ground-up/
 [14]: https://nlpprogress.com/
 [15]: http://socsci.uci.edu/~rfutrell/papers/hahn2019estimating.pdf
 [16]: https://en.wikipedia.org/wiki/Kolmogorov_complexity
 [17]: https://bair.berkeley.edu/blog/2020/12/20/lmmem/
 [18]: https://en.wikipedia.org/wiki/Laplacian_matrix
 [19]: http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=1F03130B02DC485C78BF364266B6F0CA?doi=10.1.1.19.8100&rep=rep1&type=pdf
 [20]: https://en.wikipedia.org/wiki/Principal_component_analysis
 [21]: https://www.ijcai.org/Proceedings/2019/0594.pdf
 [22]: https://dl.acm.org/doi/10.1145/2806416.2806512
 [23]: https://openreview.net/pdf?id=SyK00v5xx
 [24]: https://github.com/VHRanger/nodevectors/blob/master/examples/link%20prediction.ipynb
 [25]: https://github.com/VHRanger/nodevectors
 [26]: https://arxiv.org/pdf/1310.2636.pdf
 [27]: http://byowen.com/
 [28]: https://arxiv.org/pdf/1807.03341.pdf
 [29]: https://www.youtube.com/watch?v=Kee4ch3miVA
 [30]: https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf
 [31]: https://arxiv.org/pdf/1403.6652.pdf
 [32]: https://arxiv.org/pdf/1911.11726.pdf
 [33]: https://en.wikipedia.org/wiki/AlexNet
 [34]: https://en.wikipedia.org/wiki/Google_data_centers#Original_hardware
 [35]: https://openai.com/blog/ai-and-efficiency/
 [36]: https://www.singlelunch.com/2019/08/01/700x-faster-node2vec-models-fastest-random-walks-on-a-graph/
 [37]: https://arxiv.org/pdf/1706.02216.pdf
 [38]: https://arxiv.org/pdf/2001.08361.pdf
 [39]: http://incompleteideas.net/IncIdeas/BitterLesson.html
 [40]: https://arxiv.org/abs/2010.11929
 [41]: https://www.youtube.com/watch?v=TrdevFK_am4
 [42]: https://arxiv.org/pdf/1710.10903.pdf
 [43]: https://www.youtube.com/watch?v=fHNmRkzxHWs
 [44]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html
 [45]: https://docs.scipy.org/doc/scipy/reference/sparse.csgraph.html
 [46]: https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)
 [47]: https://en.wikipedia.org/wiki/Mmap
 [48]: https://github.com/dmlc/dgl
 [49]: https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/
 [50]: https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48
 [51]: https://arxiv.org/pdf/1705.08039.pdf"
152,machinelearning,openai,comments,2023-05-22 18:11:48,[D] Governance of SuperIntelligence - OpenAI,MysteryInc152,False,0.76,25,13oygjs,https://www.reddit.com/r/MachineLearning/comments/13oygjs/d_governance_of_superintelligence_openai/,104,1684779108.0,Blog - https://openai.com/blog/governance-of-superintelligence
153,machinelearning,openai,comments,2020-06-17 19:15:01,[R] OpenAI Image GPT,lfotofilter,False,0.96,264,hay15t,https://www.reddit.com/r/MachineLearning/comments/hay15t/r_openai_image_gpt/,99,1592421301.0,"Open AI just released a blog post about [Image GPT](https://openai.com/blog/image-gpt/). They apply the GPT-2 transformer-based model to pixel sequences (as opposed to word sequences).

This could actually be quite powerful in my view, because, as opposed to much of the current competition in self-supervised learning for images, Open AI are actually using a model of p(x) (of sorts) for downstream tasks. Recent successful methods like SimCLR rely heavily on augmentations, and mainly focus on learning features that are robust to these augmentations.

Slowly but surely, transformers are taking over the world."
154,machinelearning,openai,comments,2017-06-21 00:41:00,[N] Andrej Karpathy leaves OpenAI for Tesla ('Director of AI and Autopilot Vision'),gwern,False,0.93,397,6iib9r,https://techcrunch.com/2017/06/20/tesla-hires-deep-learning-expert-andrej-karpathy-to-lead-autopilot-vision/?,98,1498005660.0,
155,machinelearning,openai,comments,2017-08-13 23:34:26,[N] OpenAI bot was defeated at least 50 times yesterday,luiscosio,False,0.93,261,6timtv,https://twitter.com/riningear/status/896297256550252545,93,1502667266.0,
156,machinelearning,openai,comments,2023-05-24 01:00:28,"Interview with Juergen Schmidhuber, renowned ‘Father Of Modern AI’, says his life’s work won't lead to dystopia.",hardmaru,False,0.81,245,13q6k4a,https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/,96,1684890028.0,"*Schmidhuber interview expressing his views on the future of AI and AGI.*

*Original [source](https://www.forbes.com/sites/hessiejones/2023/05/23/juergen-schmidhuber-renowned-father-of-modern-ai-says-his-lifes-work-wont-lead-to-dystopia/). I think the interview is of interest to r/MachineLearning, and presents an alternate view, compared to other influential leaders in AI.*

**Juergen Schmidhuber, Renowned 'Father Of Modern AI,' Says His Life’s Work Won't Lead To Dystopia**

*May 23, 2023. Contributed by [Hessie Jones](https://twitter.com/hessiejones).*

Amid the growing concern about the impact of more advanced artificial intelligence (AI) technologies on society, there are many in the technology community who fear the implications of the advancements in Generative AI if they go unchecked. Dr. Juergen Schmidhuber, a renowned scientist, artificial intelligence researcher and widely regarded as one of the pioneers in the field, is more optimistic. He declares that many of those who suddenly warn against the dangers of AI are just seeking publicity, exploiting the media’s obsession with killer robots which has attracted more attention than “good AI” for healthcare etc.

The potential to revolutionize various industries and improve our lives is clear, as are the equal dangers if bad actors leverage the technology for personal gain. Are we headed towards a dystopian future, or is there reason to be optimistic? I had a chance to sit down with Dr. Juergen Schmidhuber to understand his perspective on this seemingly fast-moving AI-train that will leap us into the future.

As a teenager in the 1970s, Juergen Schmidhuber became fascinated with the idea of creating intelligent machines that could learn and improve on their own, becoming smarter than himself within his lifetime. This would ultimately lead to his groundbreaking work in the field of deep learning.

In the 1980s, he studied computer science at the Technical University of Munich (TUM), where he earned his diploma in 1987. His thesis was on the ultimate self-improving machines that, not only, learn through some pre-wired human-designed learning algorithm, but also learn and improve the learning algorithm itself. Decades later, this became a hot topic. He also received his Ph.D. at TUM in 1991 for work that laid some of the foundations of modern AI.

Schmidhuber is best known for his contributions to the development of recurrent neural networks (RNNs), the most powerful type of artificial neural network that can process sequential data such as speech and natural language. With his students Sepp Hochreiter, Felix Gers, Alex Graves, Daan Wierstra, and others, he published architectures and training algorithms for the long short-term memory (LSTM), a type of RNN that is widely used in natural language processing, speech recognition, video games, robotics, and other applications. LSTM has become the most cited neural network of the 20th century, and Business Week called it ""[arguably the most commercial AI achievement](https://www.bloomberg.com/news/features/2018-05-15/google-amazon-and-facebook-owe-j-rgen-schmidhuber-a-fortune?leadSource=uverify%20wall).""

Throughout his career, Schmidhuber has received various awards and accolades for his groundbreaking work. In 2013, he was awarded the Helmholtz Prize, which recognizes significant contributions to the field of machine learning. In 2016, he was awarded the IEEE Neural Network Pioneer Award for ""*pioneering contributions to deep learning and neural networks."" The media have often called him the “father of modern AI,*” because the [most cited neural networks](https://people.idsia.ch/~juergen/most-cited-neural-nets.html) all build on his lab’s work. He is quick to point out, however, that AI history [goes back centuries.](https://people.idsia.ch/~juergen/deep-learning-history.html)

Despite his many accomplishments, at the age of 60, he feels mounting time pressure towards building an Artificial General Intelligence within his lifetime and remains committed to pushing the boundaries of AI research and development. He is currently director of the KAUST AI Initiative, scientific director of the Swiss AI Lab IDSIA, and co-founder and chief scientist of AI company NNAISENSE, whose motto is ""AI∀"" which is a math-inspired way of saying ""AI For All."" He continues to work on cutting-edge AI technologies and applications to improve human health and extend human lives and make lives easier for everyone.

*The following interview has been edited for clarity.*

**Jones: Thank you Juergen for joining me. You have signed letters warning about AI weapons. But you didn't sign the recent publication, ""Pause Gigantic AI Experiments: An Open Letter""? Is there a reason?**

**Schmidhuber:** Thank you Hessie. Glad to speak with you. I have realized that many of those who warn in public against the dangers of AI are just seeking publicity. I don't think the latest letter will have any significant impact because many AI researchers, companies, and governments will ignore it completely.

The proposal frequently uses the word ""we"" and refers to ""us,"" the humans. But as I have pointed out many times in the past, there is no ""we"" that everyone can identify with. Ask 10 different people, and you will hear 10 different opinions about what is ""good."" Some of those opinions will be completely incompatible with each other. Don't forget the enormous amount of conflict between the many people.

The letter also says, ""*If such a pause cannot be quickly put in place, governments should intervene and impose a moratorium.*"" The problem is that different governments have ALSO different opinions about what is good for them and for others. Great Power A will say, if we don't do it, Great Power B will, perhaps secretly, and gain an advantage over us. The same is true for Great Powers C and D.

**Jones: Everyone acknowledges this fear surrounding current generative AI technology. Moreover, the existential threat of this technology has been publicly acknowledged by** [**Sam Altman**](https://www.bbc.com/news/world-us-canada-65616866)**, CEO of OpenAI himself, calling for AI regulation. From your perspective, is there an existential threat?**

**Schmidhuber:** It is true that AI can be weaponized, and I have no doubt that there will be all kinds of AI arms races, but AI does not introduce a new quality of existential threat. The threat coming from AI weapons seems to pale in comparison to the much older threat from nuclear hydrogen bombs that don’t need AI at all. We should be much more afraid of half-century-old tech in the form of H-bomb rockets. The Tsar Bomba of 1961 had almost 15 times more destructive power than all weapons of WW-II combined.  Despite the dramatic nuclear disarmament since the 1980s, there are still more than enough nuclear warheads to wipe out human civilization within two hours, without any AI I’m much more worried about that old existential threat than the rather harmless AI weapons.

**Jones: I realize that while you compare AI to the threat of nuclear bombs, there is a current danger that a current technology can be put in the hands of humans and enable them to “eventually” exact further harms to individuals of group in a very precise way, like targeted drone attacks. You are giving people a toolset that they've never had before, enabling bad actors, as some have pointed out, to be able to do a lot more than previously because they didn't have this technology.**

**Schmidhuber:** Now, all that sounds horrible in principle, but our existing laws are sufficient to deal with these new types of weapons enabled by AI. If you kill someone with a gun, you will go to jail. Same if you kill someone with one of these drones. Law enforcement will get better at understanding new threats and new weapons and will respond with better technology to combat these threats. Enabling drones to target persons from a distance in a way that requires some tracking and some intelligence to perform, which has traditionally been performed by skilled humans, to me, it seems is just an improved version of a traditional weapon, like a gun, which is, you know, a little bit smarter than the old guns.

But, in principle, all of that is not a new development. For many centuries, we have had the evolution of better weaponry and deadlier poisons and so on, and law enforcement has evolved their policies to react to these threats over time. So, it's not that we suddenly have a new quality of existential threat and it's much more worrisome than what we have had for about six decades. A large nuclear warhead doesn’t need fancy face recognition to kill an individual. No, it simply wipes out an entire city with ten million inhabitants.

**Jones: The existential threat that’s implied is the extent to which humans have control over this technology. We see some early cases of opportunism which, as you say, tends to get more media attention than positive breakthroughs. But you’re implying that this will all balance out?**

**Schmidhuber:** Historically, we have a long tradition of technological breakthroughs that led to advancements in weapons for the purpose of defense but also for protection. From sticks, to rocks, to axes to gunpowder to cannons to rockets… and now to drones… this has had a drastic influence on human history but what has been consistent throughout history is that those who are using technology to achieve their own ends are themselves, facing the same technology because the opposing side is learning to use it against them. And that's what has been repeated in thousands of years of human history and it will continue. I don't see the new AI arms race as something that is remotely as existential a threat as the good old nuclear warheads.

You said something important, in that some people prefer to talk about the downsides rather than the benefits of this technology, but that's misleading, because 95% of all AI research and AI development is about making people happier and advancing human life and health.

**Jones: Let’s touch on some of those beneficial advances in AI research that have been able to radically change present day methods and achieve breakthroughs.**

**Schmidhuber:** All right! For example, eleven years ago, our team with my postdoc Dan Ciresan was the first to win a [medical imaging competition through deep learning](https://people.idsia.ch/~juergen/first-time-deep-learning-won-medical-imaging-contest-september-2012.html). We analyzed female breast cells with the objective to determine harmless cells vs. those in the pre-cancer stage. Typically, a trained oncologist needs a long time to make these determinations. Our team, who knew nothing about cancer, were able to train an artificial neural network, which was totally dumb in the beginning, on lots of this kind of data. It was able to outperform all the other methods. Today, this is being used not only for breast cancer, but also for radiology and detecting plaque in arteries, and many other things.  Some of the neural networks that we have developed in the last 3 decades are now prevalent across thousands of healthcare applications, detecting Diabetes and Covid-19 and what not. This will eventually permeate across all healthcare. The good consequences of this type of AI are much more important than the click-bait new ways of conducting crimes with AI.

**Jones: Adoption is a product of reinforced outcomes. The massive scale of adoption either leads us to believe that people have been led astray, or conversely, technology is having a positive effect on people’s lives.**

**Schmidhuber:** The latter is the likely case. There's intense commercial pressure towards good AI rather than bad AI because companies want to sell you something, and you are going to buy only stuff you think is going to be good for you. So already just through this simple, commercial pressure, you have a tremendous bias towards good AI rather than bad AI. However, doomsday scenarios like in Schwarzenegger movies grab more attention than documentaries on AI that improve people’s lives.

**Jones: I would argue that people are drawn to good stories – narratives that contain an adversary and struggle, but in the end, have happy endings. And this is consistent with your comment on human nature and how history, despite its tendency for violence and destruction of humanity, somehow tends to correct itself.**

**Let’s take the example of a technology, which you are aware – GANs – General Adversarial Networks, which today has been used in applications for fake news and disinformation. In actuality, the purpose in the invention of GANs was far from what it is used for today.**

**Schmidhuber:** Yes, the name GANs was created in 2014 but we had the basic principle already in the early 1990s. More than 30 years ago, I called it *artificial curiosity*. It's a very simple way of injecting creativity into a little two network system. This creative AI is not just trying to slavishly imitate humans. Rather, it’s inventing its own goals. Let me explain:

You have two networks. One network is producing outputs that could be anything, any action. Then the second network is looking at these actions and it’s trying to predict the consequences of these actions. An action could move a robot, then something happens, and the other network is just trying to predict what will happen.

Now we can implement artificial curiosity by reducing the prediction error of the second network, which, at the same time, is the reward of the first network. The first network wants to maximize its reward and so it will invent actions that will lead to situations that will surprise the second network, which it has not yet learned to predict well.

In the case where the outputs are fake images, the first network will try to generate images that are good enough to fool the second network, which will attempt to predict the reaction of the environment: fake or real image, and it will try to become better at it. The first network will continue to also improve at generating images whose type the second network will not be able to predict. So, they fight each other. The 2nd network will continue to reduce its prediction error, while the 1st network will attempt to maximize it.

Through this zero-sum game the first network gets better and better at producing these convincing fake outputs which look almost realistic. So, once you have an interesting set of images by Vincent Van Gogh, you can generate new images that leverage his style, without the original artist having ever produced the artwork himself.

**Jones: I see how the Van Gogh example can be applied in an education setting and there are countless examples of artists mimicking styles from famous painters but image generation from this instance that can happen within seconds is quite another feat. And you know this is how GANs has been used. What’s more prevalent today is a socialized enablement of generating images or information to intentionally fool people. It also surfaces new harms that deal with the threat to intellectual property and copyright, where laws have yet to account for. And from your perspective this was not the intention when the model was conceived. What was your motivation in your early conception of what is now GANs?**

**Schmidhuber:** My old motivation for GANs was actually very important and it was not to create deepfakes or fake news but to enable AIs to be curious and invent their own goals, to make them explore their environment and make them creative.

Suppose you have a robot that executes one action, then something happens, then it executes another action, and so on, because it wants to achieve certain goals in the environment. For example, when the battery is low, this will trigger “pain” through hunger sensors, so it wants to go to the charging station, without running into obstacles, which will trigger other pain sensors. It will seek to minimize pain (encoded through numbers). Now the robot has a friend, the second network, which is a world model ––it’s a prediction machine that learns to predict the consequences of the robot’s actions.

Once the robot has a good model of the world, it can use it for planning. It can be used as a simulation of the real world. And then it can determine what is a good action sequence. If the robot imagines this sequence of actions, the model will predict a lot of pain, which it wants to avoid. If it plays this alternative action sequence in its mental model of the world, then it will predict a rewarding situation where it’s going to sit on the charging station and its battery is going to load again. So, it'll prefer to execute the latter action sequence.

In the beginning, however, the model of the world knows nothing, so how can we motivate the first network to generate experiments that lead to data that helps the world model learn something it didn’t already know? That’s what artificial curiosity is about. The dueling two network systems effectively explore uncharted environments by creating experiments so that over time the curious AI gets a better sense of how the environment works. This can be applied to all kinds of environments, and has medical applications.

**Jones: Let’s talk about the future. You have said, “*****Traditional humans won’t play a significant role in spreading intelligence across the universe.*****”**

**Schmidhuber:** Let’s first conceptually separate two types of AIs. The first type of AI are tools directed by humans. They are trained to do specific things like accurately detect diabetes or heart disease and prevent attacks before they happen. In these cases, the goal is coming from the human. More interesting AIs are setting their own goals. They are inventing their own experiments and learning from them. Their horizons expand and eventually they become more and more general problem solvers in the real world. They are not controlled by their parents, but much of what they learn is through self-invented experiments.

A robot, for example, is rotating a toy, and as it is doing this, the video coming in through the camera eyes, changes over time and it begins to learn how this video changes and learns how the 3D nature of the toy generates certain videos if you rotate it a certain way, and eventually, how gravity works, and how the physics of the world works. Like a little scientist!

And I have predicted for decades that future scaled-up versions of such AI scientists will want to further expand their horizons, and eventually go where most of the physical resources are, to build more and bigger AIs. And of course, almost all of these resources are far away from earth out there in space, which is hostile to humans but friendly to appropriately designed AI-controlled robots and self-replicating robot factories. So here we are not talking any longer about our tiny biosphere; no, we are talking about the much bigger rest of the universe.  Within a few tens of billions of years, curious self-improving [AIs will colonize the visible cosmos](https://blogs.scientificamerican.com/observations/falling-walls-the-past-present-and-future-of-artificial-intelligence/) in a way that’s infeasible for humans. Those who don’t won’t have an impact. Sounds like science fiction, but since the 1970s I have been unable to see a plausible alternative to this scenario, except for a global catastrophe such as an all-out nuclear war that stops this development before it takes off.

**Jones: How long have these AIs, which can set their own goals — how long have they existed? To what extent can they be independent of human interaction?**

**Schmidhuber:** Neural networks like that have existed for over 30 years. My first simple adversarial neural network system of this kind is the one from 1990 described above. You don’t need a teacher there; it's just a little agent running around in the world and trying to invent new experiments that surprise its own prediction machine.

Once it has figured out certain parts of the world, the agent will become bored and will move on to more exciting experiments. The simple 1990 systems I mentioned have certain limitations, but in the past three decades, we have also built more [sophisticated systems that are setting their own goals](https://people.idsia.ch/~juergen/artificial-curiosity-since-1990.html) and such systems I think will be essential for achieving true intelligence. If you are only imitating humans, you will never go beyond them. So, you really must give AIs the freedom to explore previously unexplored regions of the world in a way that no human is really predefining.

**Jones: Where is this being done today?**

**Schmidhuber:** Variants of neural network-based artificial curiosity are used today for agents that learn to play video games in a human-competitive way. We have also started to use them for automatic design of experiments in fields such as materials science. I bet many other fields will be affected by it: chemistry, biology, drug design, you name it. However, at least for now, these artificial scientists, as I like to call them, cannot yet compete with human scientists.

I don’t think it’s going to stay this way but, at the moment, it’s still the case.  Sure, AI has made a lot of progress. Since 1997, there have been superhuman chess players, and since 2011, through the DanNet of my team, there have been [superhuman visual pattern recognizers](https://people.idsia.ch/~juergen/DanNet-triggers-deep-CNN-revolution-2011.html). But there are other things where humans, at the moment at least, are much better, in particular, science itself.  In the lab we have many first examples of self-directed artificial scientists, but they are not yet convincing enough to appear on the radar screen of the public space, which is currently much more fascinated with simpler systems that just imitate humans and write texts based on previously seen human-written documents.

**Jones: You speak of these numerous instances dating back 30 years of these lab experiments where these self-driven agents are deciding and learning and moving on once they’ve learned. And I assume that that rate of learning becomes even faster over time. What kind of timeframe are we talking about when this eventually is taken outside of the lab and embedded into society?**

**Schmidhuber:** This could still take months or even years :-) Anyway, in the not-too-distant future, we will probably see artificial scientists who are good at devising experiments that allow them to discover new, previously unknown physical laws.

As always, we are going to profit from the old trend that has held at least since 1941: every decade compute is getting 100 times cheaper.

**Jones: How does this trend affect modern AI such as ChatGPT?**

**Schmidhuber:** Perhaps you know that all the recent famous AI applications such as ChatGPT and similar models are largely based on principles of artificial neural networks invented in the previous millennium. The main reason why they works so well now is the incredible acceleration of compute per dollar.

ChatGPT is driven by a neural network called “Transformer” described in 2017 by Google. I am happy about that because a quarter century earlier in 1991 I had a particular Transformer variant which is now called the “[Transformer with linearized self-attention](https://twitter.com/SchmidhuberAI/status/1576966129993797632?cxt=HHwWgMDSkeKVweIrAAAA)”. Back then, not much could be done with it, because the compute cost was a million times higher than today. But today, one can train such models on half the internet and achieve much more interesting results.

**Jones: And for how long will this acceleration continue?**

**Schmidhuber:** There's no reason to believe that in the next 30 years, we won't have another factor of 1 million and that's going to be really significant. In the near future, for the first time we will have many not-so expensive devices that can compute as much as a human brain. The physical limits of computation, however, are much further out so even if the trend of a factor of 100 every decade continues, the physical limits (of 1051 elementary instructions per second and kilogram of matter) won’t be hit until, say, the mid-next century. Even in our current century, however, we’ll probably have many machines that compute more than all 10 billion human brains collectively and you can imagine, everything will change then!

**Jones: That is the big question. Is everything going to change? If so, what do you say to the next generation of leaders, currently coming out of college and university. So much of this change is already impacting how they study, how they will work, or how the future of work and livelihood is defined. What is their purpose and how do we change our systems so they will adapt to this new version of intelligence?**

**Schmidhuber:** For decades, people have asked me questions like that, because you know what I'm saying now, I have basically said since the 1970s, it’s just that today, people are paying more attention because, back then, they thought this was science fiction.

They didn't think that I would ever come close to achieving my crazy life goal of building a machine that learns to become smarter than myself such that I can retire. But now many have changed their minds and think it's conceivable. And now I have two daughters, 23 and 25. People ask me: what do I tell them? They know that Daddy always said, “*It seems likely that within your lifetimes, you will have new types of intelligence that are probably going to be superior in many ways, and probably all kinds of interesting ways.*” How should they prepare for that? And I kept telling them the obvious: **Learn how to learn new things**! It's not like in the previous millennium where within 20 years someone learned to be a useful member of society, and then took a job for 40 years and performed in this job until she received her pension. Now things are changing much faster and we must learn continuously just to keep up. I also told my girls that no matter how smart AIs are going to get, learn at least the basics of math and physics, because that’s the essence of our universe, and anybody who understands this will have an advantage, and learn all kinds of new things more easily. I also told them that social skills will remain important, because most future jobs for humans will continue to involve interactions with other humans, but I couldn’t teach them anything about that; they know much more about social skills than I do.

You touched on the big philosophical question about people’s purpose. Can this be answered without answering the even grander question: What’s the purpose of the entire universe?

We don’t know. But what’s happening right now might be connected to the unknown answer. Don’t think of humans as the crown of creation. Instead view human civilization as part of a much grander scheme, an important step (but not the last one) on the path of the universe from very simple initial conditions towards more and more unfathomable complexity. Now it seems ready to take its [next step, a step comparable to the invention of life itself over 3.5 billion years ago](https://people.idsia.ch/~juergen/deep-learning-history.html#future).  Alas, don’t worry, in the end, all will be good!

**Jones: Let’s get back to this transformation happening right now with OpenAI. There are many questioning the efficacy and accuracy of ChatGPT, and are concerned its release has been premature. In light of the rampant adoption, educators have banned its use over concerns of plagiarism and how it stifles individual development. Should large language models like ChatGPT be used in school?**

**Schmidhuber:** When the calculator was first introduced, instructors forbade students from using it in school. Today, the consensus is that kids should learn the basic methods of arithmetic, but they should also learn to use the “artificial multipliers” aka calculators, even in exams, because laziness and efficiency is a hallmark of intelligence. Any intelligent being wants to minimize its efforts to achieve things.

And that's the reason why we have tools, and why our kids are learning to use these tools. The first stone tools were invented maybe 3.5 million years ago; tools just have become more sophisticated over time. In fact, humans have changed in response to the properties of their tools. Our anatomical evolution was shaped by tools such as spears and fire. So, it's going to continue this way. And there is no permanent way of preventing large language models from being used in school.

**Jones: And when our children, your children graduate, what does their future work look like?**

**Schmidhuber:** A single human trying to predict details of how 10 billion people and their machines will evolve in the future is like a single neuron in my brain trying to predict what the entire brain and its tens of billions of neurons will do next year. 40 years ago, before the WWW was created at CERN in Switzerland, who would have predicted all those young people making money as YouTube video bloggers?

Nevertheless, let’s make a few limited job-related observations. For a long time, people have thought that desktop jobs may require more intelligence than skills trade or handicraft professions. But now, it turns out that it's much easier to replace certain aspects of desktop jobs than replacing a carpenter, for example. Because everything that works well in AI is happening behind the screen currently, but not so much in the physical world.

There are now artificial systems that can read lots of documents and then make really nice summaries of these documents. That is a desktop job. Or you give them a description of an illustration that you want to have for your article and pretty good illustrations are being generated that may need some minimal fine-tuning. But you know, all these desktop jobs are much easier to facilitate than the real tough jobs in the physical world. And it's interesting that the things people thought required intelligence, like playing chess, or writing or summarizing documents, are much easier for machines than they thought. But for things like playing football or soccer, there is no physical robot that can remotely compete with the abilities of a little boy with these skills. So, AI in the physical world, interestingly, is much harder than AI behind the screen in virtual worlds. And it's really exciting, in my opinion, to see that jobs such as plumbers are much more challenging than playing chess or writing another tabloid story.

**Jones: The way data has been collected in these large language models does not guarantee personal information has not been excluded. Current consent laws already are outdated when it comes to these large language models (LLM). The concern, rightly so, is increasing surveillance and loss of privacy. What is your view on this?**

**Schmidhuber:** As I have indicated earlier: are surveillance and loss of privacy inevitable consequences of increasingly complex societies? Super-organisms such as cities and states and companies consist of numerous people, just like people consist of numerous cells. These cells enjoy little privacy. They are constantly monitored by specialized ""police cells"" and ""border guard cells"": Are you a cancer cell? Are you an external intruder, a pathogen? Individual cells sacrifice their freedom for the benefits of being part of a multicellular organism.

Similarly, for super-organisms such as nations. Over 5000 years ago, writing enabled recorded history and thus became its inaugural and most important invention. Its initial purpose, however, was to facilitate surveillance, to track citizens and their tax payments. The more complex a super-organism, the more comprehensive its collection of information about its constituents.

200 years ago, at least, the parish priest in each village knew everything about all the village people, even about those who did not confess, because they appeared in the confessions of others. Also, everyone soon knew about the stranger who had entered the village, because some occasionally peered out of the window, and what they saw got around. Such control mechanisms were temporarily lost through anonymization in rapidly growing cities but are now returning with the help of new surveillance devices such as smartphones as part of digital nervous systems that tell companies and governments a lot about billions of users. Cameras and drones etc. are becoming increasingly tinier and more ubiquitous. More effective recognition of faces and other detection technology are becoming cheaper and cheaper, and many will use it to identify others anywhere on earth; the big wide world will not offer any more privacy than the local village. Is this good or bad? Some nations may find it easier than others to justify more complex kinds of super-organisms at the expense of the privacy rights of their constituents.

**Jones: So, there is no way to stop or change this process of collection, or how it continuously informs decisions over time? How do you see governance and rules responding to this, especially amid** [**Italy’s ban on ChatGPT following**](https://www.cnbc.com/2023/04/04/italy-has-banned-chatgpt-heres-what-other-countries-are-doing.html) **suspected user data breach and the more recent news about the** [**Meta’s record $1.3billion fine**](https://www.reuters.com/technology/facebook-given-record-13-bln-fine-given-5-months-stop-eu-us-data-flows-2023-05-22/) **in the company’s handling of user information?**

**Schmidhuber:** Data collection has benefits and drawbacks, such as the loss of privacy. How to balance those? I have argued for addressing this through data ownership in data markets. If it is true that data is the new oil, then it should have a price, just like oil. At the moment, the major surveillance platforms such as Meta do not offer users any money for their data and the transitive loss of privacy. In the future, however, we will likely see attempts at creating efficient data markets to figure out the data's true financial value through the interplay between supply and demand.

Even some of the sensitive medical data should not be priced by governmental regulators but by patients (and healthy persons) who own it and who may sell or license parts thereof as micro-entrepreneurs in a healthcare data market.

Following a previous [interview](https://www.swissre.com/institute/conferences/The-intelligence-behind-artificial-intelligence.html), I gave for one of the largest re-insurance companies , let's look at the different participants in such a data market: patients, hospitals, data companies. (1) **Patients** with a rare form of cancer can offer more valuable data than patients with a very common form of cancer. (2) **Hospitals** and their machines are needed to extract the data, e.g., through magnet spin tomography, radiology, evaluations through human doctors, and so on. (3) **Companies** such as Siemens, Google or IBM would like to buy annotated data to make better artificial neural networks that learn to predict pathologies and diseases and the consequences of therapies. Now the market’s invisible hand will decide about the data’s price through the interplay between demand and supply. On the demand side, you will have several companies offering something for the data, maybe through an app on the smartphone (a bit like a stock market app). On the supply side, each patient in this market should be able to profit from high prices for rare valuable types of data. Likewise, competing data extractors such as hospitals will profit from gaining recognition and trust for extracting data well at a reasonable price. The market will make the whole system efficient through incentives for all who are doing a good job. Soon there will be a flourishing ecosystem of commercial data market advisors and what not, just like the ecosystem surrounding the traditional stock market. The value of the data won’t be determined by governments or ethics committees, but by those who own the data and decide by themselves which parts thereof they want to license to others under certain conditions.

At first glance, a market-based system seems to be detrimental to the interest of certain monopolistic companies, as they would have to pay for the data - some would prefer free data and keep their monopoly. However, since every healthy and sick person in the market would suddenly have an incentive to collect and share their data under self-chosen anonymity conditions, there will soon be many more useful data to evaluate all kinds of treatments. On average, people will live longer and healthier, and many companies and the entire healthcare system will benefit.

**Jones: Finally, what is your view on open source versus the private companies like Google and OpenAI? Is there a danger to supporting these private companies’ large language models versus trying to keep these models open source and transparent, very much like what LAION is doing?**

**Schmidhuber:** I signed this [open letter by LAION](https://www.forbes.com/sites/hessiejones/2023/04/19/amid-growing-call-to-pause-ai-research-laion-petitions-governments-to-keep-agi-research-open-active-and-responsible/?sh=6973c08b62e3) because I strongly favor the open-source movement. And I think it's also something that is going to challenge whatever big tech dominance there might be at the moment. Sure, the best models today are run by big companies with huge budgets for computers, but the exciting fact is that open-source models are not so far behind, some people say maybe six to eight months only. Of course, the private company models are all based on stuff that was created in academia, often in little labs without so much funding, which publish without patenting their results and open source their code and others take it and improved it.

Big tech has profited tremendously from academia; their main achievement being that they have scaled up everything greatly, sometimes even failing to credit the original inventors.

So, it's very interesting to see that as soon as some big company comes up with a new scaled-up model, lots of students out there are competing, or collaborating, with each other, trying to come up with equal or better performance on smaller networks and smaller machines. And since they are open sourcing, the next guy can have another great idea to improve it, so now there’s tremendous competition also for the big companies.

Because of that, and since AI is still getting exponentially cheaper all the time, I don't believe that big tech companies will dominate in the long run. They find it very hard to compete with the enormous open-source movement. As long as you can encourage the open-source community, I think you shouldn't worry too much. Now, of course, you might say if everything is open source, then the bad actors also will more easily have access to these AI tools. And there's truth to that. But as always since the invention of controlled fire, it was good that knowledge about how technology works quickly became public such that everybody could use it. And then, against any bad actor, there's almost immediately a counter actor trying to nullify his efforts. You see, I still believe in our old motto ""AI∀"" or ""AI For All.""

**Jones: Thank you, Juergen for sharing your perspective on this amazing time in history. It’s clear that with new technology, the enormous potential can be matched by disparate and troubling risks which we’ve yet to solve, and even those we have yet to identify. If we are to dispel the fear of a sentient system for which we have no control, humans, alone need to take steps for more responsible development and collaboration to ensure AI technology is used to ultimately benefit society. Humanity will be judged by what we do next.**"
157,machinelearning,openai,comments,2022-12-22 18:39:30,[D] When chatGPT stops being free: Run SOTA LLM in cloud,_underlines_,False,0.95,347,zstequ,https://www.reddit.com/r/MachineLearning/comments/zstequ/d_when_chatgpt_stops_being_free_run_sota_llm_in/,95,1671734370.0,"Edit: Found [LAION-AI/OPEN-ASSISTANT](https://github.com/LAION-AI/Open-Assistant) a very promising project opensourcing the idea of chatGPT. [video here](https://www.youtube.com/watch?v=8gVYC_QX1DI)

**TL;DR: I found GPU compute to be [generally cheap](https://github.com/full-stack-deep-learning/website/blob/main/docs/cloud-gpus/cloud-gpus.csv) and spot or on-demand instances can be launched on AWS for a few USD / hour up to over 100GB vRAM. So I thought it would make sense to run your own SOTA LLM like Bloomz 176B inference endpoint whenever you need it for a few questions to answer. I thought it would still make more sense than shoving money into a closed walled garden like ""not-so-OpenAi"" when they make ChatGPT or GPT-4 available for $$$. But I struggle due to lack of tutorials/resources.**

Therefore, I carefully checked benchmarks, model parameters and sizes as well as training sources for all SOTA LLMs [here](https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit#gid=1158069878).

Knowing since reading the Chinchilla paper that Model Scaling according to OpenAI was wrong and more params != better quality generation. So I was looking for the best performing LLM openly available in terms of quality and broadness to use for multilingual everyday questions/code completion/reasoning similar to what chatGPT provides (minus the fine-tuning for chat-style conversations).

My choice fell on [Bloomz](https://huggingface.co/bigscience/bloomz) (because that handles multi-lingual questions well and has good zero shot performance for instructions and Q&A style text generation. Confusingly Galactica seems to outperform Bloom on several benchmarks. But since Galactica had a very narrow training set only using scientific papers, I guess usage is probably limited for answers on non-scientific topics.

Therefore I tried running the original bloom 176B and alternatively also Bloomz 176B on AWS SageMaker JumpStart, which should be a one click deployment. This fails after 20min. On Azure ML, I tried using DeepSpeed-MII which also supports bloom but also fails due the instance size of max 12GB vRAM I guess.

From my understanding to save costs on inference, it's probably possible to use one or multiple of the following solutions:

- Precision: int8 instead of fp16
- [Microsoft/DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) for an up 40x reduction on inference cost on Azure, this thing also supports int8 and fp16 bloom out of the box, but it fails on Azure due to instance size.
- [facebook/xformer](https://github.com/facebookresearch/xformers) not sure, but if I remember correctly this brought inference requirements down to 4GB vRAM for StableDiffusion and DreamBooth fine-tuning to 10GB. No idea if this is usefull for Bloom(z) inference cost reduction though

I have a CompSci background but I am not familiar with most stuff, except that I was running StableDiffusion since day one on my rtx3080 using linux and also doing fine-tuning with DreamBooth. But that was all just following youtube tutorials. I can't find a single post or youtube video of anyone explaining a full BLOOM / Galactica / BLOOMZ inference deployment on cloud platforms like AWS/Azure using one of the optimizations mentioned above, yet alone deployment of the raw model. :(

I still can't figure it out by myself after 3 days.

**TL;DR2: Trying to find likeminded people who are interested to run open source SOTA LLMs for when chatGPT will be paid or just for fun.**

Any comments, inputs, rants, counter-arguments are welcome.

/end of rant"
158,machinelearning,openai,comments,2016-12-29 15:48:28,[D] r/MachineLearning's 2016 Best Paper Award!,Mandrathax,False,0.93,234,5kxfkb,https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/,96,1483026508.0,"***EDIT : I will be announcing the results on monday 1/9***

***EDIT 2 : maybe 1/10 then because of travel issues irl, sorry about that***

---

Hi guys!

Welcome to /r/MachineLearning's 2016 Best Paper Award!

The idea is to have a community-wide vote for the best papers of this year.

I hope you find this to be a good idea, mods please tell me if this breaks any rules/if you had something like this in store.

---

## How does it work?

**Nominate** by commenting on the dedicatd top level comments. Please provide a (paywall free) link. Feel free to justify your choice. Also if you're one of the author, be courteous and indicate it.

**Vote** by upvoting the nominees.

The **results** will be announced **by the end of next week** (6-7th of Jan.). Depending on the participation/interest I might change it.

It's that simple!

There are some simple rules to make sure everything runs smoothly, you can find them below, please read them before commenting.

---

## Categories

- [Best Paper of the year](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbrapti/)

> No rules! Any research paper you feel had the greatest impact/had top writing, any criterion is good.

- [Best student paper](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbraqkv/)

> Papers from a student, grad/undergrad/highschool, everyone who doesn't have a phd and goes to school. The student must be first author of course. Provide evidence if possible.

- [Best paper name](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbrar08/)

> Try to beat [this](http://www.oneweirdkerneltrick.com/spectral.pdf)

- [Best paper from academia](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbraroz/)

> Papers where the first author is from a university / a state research organization (eg INRIA in France). 

- [Best paper from the industry](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbrasnz/)

> Great paper from a multi-billion tech company (or more generally a research lab sponsored by privat funds, eg. openai)

- [Best rejected paper](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbrat9t/)

> A chance of redemption for good papers that didn't make it trough peer review. Please provide evidence that the paper was rejected if possible.

- [Best unpublished preprint](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbratyb/)

> A category for those yet to be published (e.g. papers from the end of the year). This may or may not be redundant with the rejected paper category, we'll see.

- [Best theoretical paper](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbrauan/)

> Keep the math coming

- [Best non Deep Learning paper](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbraumv/)

> Because gaussian processes, random forests and kernel methods deserve a chance amid the DL hype train

---

## Rules

1. Only one nomination by comment. You can nominate multiple papers in different comments/categories.
2. Nominations should include a **link to the paper**. In case of an arxiv link, please link to the arxiv page and not the pdf directly. Please do not link paywalled articles.
3. Only **research paper** are to be nominated. This means no book, no memo or no tutorial/blog post for instance. This could be adressed in a separate award or category if there is enough demand.
4. For the sake of clarity, there are some rules on commenting :
 - ***Do NOT comment on the main thread***. For discussion, use the [*discussion* thread](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbrap6u/)
 - ***Please ONLY comment the other threads with nominations***. You can discuss individual nominations in child comments. However 1rst level comments on each thread should be nominations only.
5. Respect reddit and this sub's rules.

I am not a mod so I have no way of enforcing these rules, please follow them to keep the thread clear. Of course, suggestions are welcome [here](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/dbrap6u/).

---

That's it, have fun!"
159,machinelearning,openai,comments,2023-03-27 04:21:36,[D]GPT-4 might be able to tell you if it hallucinated,Cool_Abbreviations_9,False,0.93,650,123b66w,https://i.redd.it/ocs0x33429qa1.jpg,94,1679890896.0,
160,machinelearning,openai,comments,2023-12-28 22:53:08,New York Times sues OpenAI and Microsoft for copyright infringement [N],we_are_mammals,False,0.9,172,18t73v1,https://www.reddit.com/r/MachineLearning/comments/18t73v1/new_york_times_sues_openai_and_microsoft_for/,92,1703803988.0,"https://www.theguardian.com/media/2023/dec/27/new-york-times-openai-microsoft-lawsuit

The lawsuit alleges: *""Powered by LLMs containing copies of Times content, Defendants’ GenAI tools can generate output that recites Times content verbatim, closely summarizes it, and mimics its expressive style""*. The lawsuit seeks billions in damages and wants to see these chatbots destroyed.

---

I don't know if summaries and style mimicking fall under copyright law, but couldn't verbatim quoting be prevented? I proposed doing this a while ago in this subreddit: 

> Can't OpenAI simply check the output for sharing long substrings with the training data (perhaps probabilistically)?

You can simply take all training data substrings (of a fixed length, say 20 tokens) and put them into a hash table, a bloom filter, or a similar data structure. Then, when the LLMs are generating text, you can check to make sure the text does not contain any substrings that are in the data structure. This will prevent verbatim quotations from the NYT or other copyrighted material that are longer than 20 tokens (or whatever length you chose). Storing the data structure in memory may require distributing it across multiple machines, but I think OpenAI can easily afford it. You can further save memory by spacing the substrings, if memory is a concern."
161,machinelearning,openai,comments,2021-04-23 14:25:29,[D] Your Favorite AI Podcasts / Blogs / Newsletters / YouTube Channels?,regalalgorithm,False,0.96,399,mwwftu,https://www.reddit.com/r/MachineLearning/comments/mwwftu/d_your_favorite_ai_podcasts_blogs_newsletters/,90,1619187929.0,"Hi there, I want to write a little blog post summarizing different ways of keeping up with AI by way of Podcasts / Blogs / Newsletters / YouTube Channels. Yeah there are a million of these, but most are not so well curated, miss a lot of stuff, and are not up to date. Criteria: still active, focused primarily on AI, high quality.

Here's what I have so far, would appreciate if you can suggest any additions!

* **Podcasts**
   * [**Machine Learning Street Talk**](https://www.youtube.com/channel/UCMLtBahI5DMrt0NPvDSoIRQ)
   * **Lex Fridman (mainly first \~150 eps)**
   * **Gigaom Voices in AI**
   * **Data Skeptic**
   * **Eye on AI**
   * **Gradient Dissent**
   * **Robot Brains**
   * **RE Work podcast**
   * **AI Today Podcast**
   * **Chat Time Data Science**
   * **Let’s Talk AI**
   * **In Machines We Trust**
* **Publications**
   * **The Gradient**
   * **Towards Data Science**
   * **Analytics Vidhya**
   * **Distill**
* **Personal Blogs**
   * [**Lil’Log**](https://lilianweng.github.io/lil-log/)
   * **Gwern**
   * **Sebastian Ruder**
   * **Alex Irpan**
   * **Chris Olah**
   * **Democratizing Automation**
   * **Approximately Correct**
   * **Off the Convex Path**
   * **Arg min blog**
   * **I’m a bandit**
* **Academic Blogs**
   * **SAIL Blog**
   * **Berkeley AI Blog**
   * **Machine Learning at Berkeley Blog**
   * **CMU ML Blog**
   * **ML MIT**
   * **ML Georgia Tech**
   * **Google / Facebook / Salesforce / Microsoft / Baidu / OpenAI /  DeepMind** 
* **Journalists**
   * **Karen Hao** 
   * **Cade Metz**
   * **Will Knight**
   * **Khari Johnson**
* **Newsletters**
   * **Last Week in AI**
   * **Batch.AI**
   * **Sebasting Ruder**
   * **Artificial Intelligence Weekly News**
   * **Wired AI newsletter**
   * **Papers with Code**
   * **The Algorithm**
   * **AI Weekly**
   * **Weekly Robotics**
   * **Import AI**
   * **Deep Learning Weekly**
   * **H+ Weekly**
   * **ChinAI Newsletter**
   * **THe EuropeanAI Newsletter**

**Youtube Channels**

* **Talks**
   * [**Amii Intelligence**](https://www.youtube.com/channel/UCxxisInVr7upxv1yUhSgdBA)
   * [**CMU AI Seminar**](https://www.youtube.com/channel/UCLh3OUmBGe4wPyVZiI771ng)
   * [**Robotics Institute Seminar Series**](https://www.youtube.com/playlist?list=PLCFD85BC79FE703DF)
   * [**Machine Learning Center at Georgia Tech**](https://www.youtube.com/channel/UCugI4c0S6-yVi9KfdkDU0aw/videos)
   * [**Robotics Today**](https://www.youtube.com/channel/UCtfiXX2nJ5Qz-ZxGEwDCy5A)
   * [**Stanford MLSys Seminars**](https://www.youtube.com/channel/UCzz6ructab1U44QPI3HpZEQ)
   * [**MIT Embodied Intelligence**](https://www.youtube.com/channel/UCnXGbvgu9071i3koFooncAw)
* **Interviews**
   * **See podcasts**
* **Paper Summaries** 
   * [**AI Coffee Break with Letitia**](https://www.youtube.com/c/AICoffeeBreak/featured)
   * [**Henry AI Labs**](https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw)
   * [**Yannic Kilcher**](https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew)
   * **Arxiv Insights**
* **Lessons**
   * [**3Blue1Brown**](https://www.youtube.com/c/3blue1brown/featured)
   * [**Jordan Harrod**](https://www.youtube.com/channel/UC1H1NWNTG2Xi3pt85ykVSHA)
   * [**vcubingx**](https://www.youtube.com/channel/UCv0nF8zWevEsSVcmz6mlw6A)
   * [**Leo Isikdogan**](https://www.youtube.com/channel/UC-YAxUbpa1hvRyfJBKFNcJA)
* **Demos**
   * [**bycloud**](https://www.youtube.com/channel/UCgfe2ooZD3VJPB6aJAnuQng)
   * [**Two Minute Papers**](https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg)
   * [**Code Bullet**](https://www.youtube.com/channel/UC0e3QhIYukixgh5VVpKHH9Q)
   * [**What's AI**](https://www.youtube.com/c/WhatsAI/videos)"
162,machinelearning,openai,comments,2015-12-11 22:22:39,"OpenAI - a non-profit AI research company, launched by Sutskever, Musk, Bengio, Zaremba and many others",elanmart,False,0.95,315,3wfqip,https://openai.com/blog/introducing-openai/,90,1449872559.0,
163,machinelearning,openai,comments,2020-06-01 00:16:25,"[D] Do I Need to Go to University? Essay by Chris Olah, OpenAI research scientist, previously at Google Brain, and creator of distill.pub",sensetime,False,0.94,293,gua7z9,http://colah.github.io/posts/2020-05-University/,88,1590970585.0,
164,machinelearning,openai,comments,2022-08-25 14:53:50,OpenAI Residency 2023 [R],rooney119,False,0.9,34,wxgcca,https://www.reddit.com/r/MachineLearning/comments/wxgcca/openai_residency_2023_r/,88,1661439230.0,Has anyone applied to the newly opened OpenAI residency program to begin in January 2023?
165,machinelearning,openai,comments,2022-12-04 09:57:44,[D] OpenAI’s ChatGPT is unbelievable good in telling stories!,Far_Pineapple770,False,0.91,241,zc5sg6,https://www.reddit.com/r/MachineLearning/comments/zc5sg6/d_openais_chatgpt_is_unbelievable_good_in_telling/,85,1670147864.0,"
I started playing with ChatGPT, the new chatbot created by OpenAI and is free to use at the moment. 

I asked it to describe a scene between Batman and Joker on a subway platform and the result is mind blowing! I let you have a read of the story yourself:

Generated by ChatGPT:

The scene is set in a dimly lit subway station, with the Joker standing on the platform, gazing up at Batman who is perched on a nearby overhead beam. The Joker is dressed in his signature purple suit and green hair, a wicked grin plastered across his face.

In his hand, he holds a deck of playing cards, flipping them expertly through his fingers. The sound of the cards shuffling echoes through the empty station.

Batman, dressed in his dark suit and cape, stares down at the Joker with a steely gaze. His muscles are tense, ready for any sudden movements from the villain.

The Joker cackles, his laughter bouncing off the walls of the station. ""Hello, Batsy! Long time no see. How's the crime-fighting business these days?""

Batman remains silent, watching the Joker carefully.

The Joker's grin widens. ""Come on, Bats. Don't be so serious all the time. Let's have some fun."" He tosses a card at Batman, who easily catches it.

""I'm not here to play games, Joker,"" Batman says in a grave tone.

The Joker's laughter turns into a maniacal cackle. ""But that's the whole point, Batsy! Life is a game. And I'm the master of it.""

He pulls out a bomb from his pocket and holds it up, the timer ticking down. ""Let's see how well you can play, shall we?""

Batman leaps down from the beam, landing gracefully on the platform. He advances towards the Joker, who backs away, still holding the bomb.

The tension in the air is palpable as the two foes face off, each ready to outsmart the other. The sound of the bomb's timer counts down, adding to the tension of the scene.

The Joker's grin never fades, even as Batman inches closer. ""It's always a pleasure to play with you, Batsy. But I think it's time for me to make my exit.""

With a flourish, the Joker disappears in a cloud of smoke, leaving Batman standing alone on the platform. The bomb detonates, causing a small explosion and sending debris flying.

But Batman remains unharmed, ready to continue the never-ending battle against the Joker and all the evil that plagues the city.

End of the story.

I used the first two paragraphs in OpenAI’s Dall E 2 to generate an image and [this](https://labs.openai.com/s/p8L8Z6c3S7qZ26U7owfbx7lS) is the result I got.

Please keep in mind that this is the very beginning of this tool and think about the endless possibilities it can create."
166,machinelearning,openai,comments,2020-04-30 17:00:10,"[R] OpenAI opensources Jukebox, a neural net that generates music",gohu_cd,False,0.97,437,gazkh7,https://www.reddit.com/r/MachineLearning/comments/gazkh7/r_openai_opensources_jukebox_a_neural_net_that/,85,1588266010.0,"Provided with genre, artist, and lyrics as input, Jukebox outputs a new music sample produced from scratch.

[https://openai.com/blog/jukebox/](https://openai.com/blog/jukebox/)

[https://jukebox.openai.com](https://jukebox.openai.com/)

The model behind this tool is VQ-VAE."
167,machinelearning,openai,comments,2023-03-09 18:30:58,"[N] GPT-4 is coming next week – and it will be multimodal, says Microsoft Germany - heise online",Singularian2501,False,0.98,662,11mzqxu,https://www.reddit.com/r/MachineLearning/comments/11mzqxu/n_gpt4_is_coming_next_week_and_it_will_be/,80,1678386658.0,"[https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html](https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html)

>**GPT-4 is coming next week**: at an approximately one-hour hybrid information event entitled ""**AI in Focus - Digital Kickoff"" on 9 March 2023**, four Microsoft Germany employees presented Large Language Models (LLM) like GPT series as a disruptive force for companies and their Azure-OpenAI offering in detail. The kickoff event took place in the German language, news outlet Heise was present. **Rather casually, Andreas Braun, CTO Microsoft Germany** and Lead Data & AI STU, **mentioned** what he said was **the imminent release of GPT-4.** The fact that **Microsoft is fine-tuning multimodality with OpenAI should no longer have been a secret since the release of Kosmos-1 at the beginning of March.**

[ Dr. Andreas Braun, CTO Microsoft Germany and Lead Data  & AI STU at the Microsoft Digital Kickoff: \\""KI im Fokus\\"" \(AI in  Focus, Screenshot\) \(Bild: Microsoft\) ](https://preview.redd.it/rnst03avarma1.jpg?width=1920&format=pjpg&auto=webp&s=c5992e2d6c6daf32e56a0a3ffeeecfe10621f73f)"
168,machinelearning,openai,comments,2024-02-12 22:14:07,Whats in your RAG setup? [D],EnvironmentalDepth62,False,0.88,111,1apcp2w,https://www.reddit.com/r/MachineLearning/comments/1apcp2w/whats_in_your_rag_setup_d/,82,1707776047.0,"What frameworks and libraries are you using in your RAG? 

I'm most curious if  LangChain is as popular as it was?

Here's mine at a high-level: 

*  langchain to use OpenAI for creating embeddings
* Pinecone for storing embedding
* langchain to load document splitters and characters splitters for chunking
* Mongo for conversations memory

&#x200B;"
169,machinelearning,openai,comments,2022-01-28 17:39:35,[D] It seems OpenAI’s new embedding models perform terribly,StellaAthena,False,0.97,275,sew5rl,https://www.reddit.com/r/MachineLearning/comments/sew5rl/d_it_seems_openais_new_embedding_models_perform/,80,1643391575.0,"Some people on Twitter have been investigating [OpenAI’s new embedding API](https://openai.com/blog/introducing-text-and-code-embeddings/) and it’s shocking how poorly it performs. On standard benchmarks, open source models 1000x smaller obtain equal or better performance! Models based on RoBERTa and T5, as well as the Sentence Transformer all achieve significantly better performance than the 175B model. Also of interest is that the DaVinci (175B) model is not clearly better than the Ada (350M) model.

Has anyone tried adapting some other autoregressive languages models, such as GPT-2, GPT-Neo, or GPT-J to do embeddings? I’m quite curious if this is an inherent failing of autoregressive models or if there’s something else going on. **Edit:** [a commenter](https://www.reddit.com/r/MachineLearning/comments/sew5rl/d_it_seems_openais_new_embedding_models_perform/humuzef/) has asked that I point out that I am one of the creators of GPT-Neo and part of the org that created GPT-J. These examples were not intended as specific endorsements, and I would be just as interested in comparisons using other billion-parameter+ autoregressive language models.

**Edit 2:** I originally linked to a [tweet](https://twitter.com/Nils_Reimers/status/1487014195568775173?s=20&amp;amp;amp;amp;amp;amp;amp;t=NBF7D2DYi41346cGM-PQjQ) about this, but several commenters pointed out that there’s also a [blog post](https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9) with more information.

**Edit 3:** An OpenAI researcher [seems to have responded](https://mobile.twitter.com/arvind_io/status/1487188996774002688)."
170,machinelearning,openai,comments,2019-03-12 16:52:51,[D] What's your opinion of the GPT-2 ethics PR campaign given that OpenAI was planning to go for profit?,notsoopenai,False,0.9,150,b0a284,https://www.reddit.com/r/MachineLearning/comments/b0a284/d_whats_your_opinion_of_the_gpt2_ethics_pr/,79,1552409571.0,"Sure seems to me like it was a way to build up hype for the new fundraising round and fool potential investors into believing that they're leading the field in their goal to build ""AGI"".

&#x200B;

[gdb claiming that this was in the works for two years](https://news.ycombinator.com/item?id=19360147)

[some more info proving that the LP plan was in the works for a while](https://twitter.com/GreatCrashO2018/status/1105168044949680128)

&#x200B;

Also wouldn't be surprised to see them pump and dump this company and exit with a return for the early investors. "
171,machinelearning,openai,comments,2022-04-01 12:09:05,[D] Have there been successful applications of Deep RL to real problems other than board games/Atari?,sid_276,False,0.95,128,ttp4ub,https://www.reddit.com/r/MachineLearning/comments/ttp4ub/d_have_there_been_successful_applications_of_deep/,77,1648814945.0,"Some successful applications I am able to gather, mostly from Deepmind:

\- [comma.ai self-driving system](https://github.com/commaai/openpilot).

\- [Weather nowcasting](https://www.deepmind.com/blog/nowcasting-the-next-hour-of-rain)

\- [Tokamak fision reactor feeedback control](https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control)

\- Hardware design: [New gen. TPU](https://twitter.com/annadgoldie/status/1402644252320886786)

\- [Datacenter cooling optimization](https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40)

\- [Adaptive locomotion for quadrupedal robots](https://arxiv.org/abs/2201.08117) \*

\- Portfolio optimization (financial instruments)

There  is a lot of work in games, particularly board games, but these do not  really solve something ""useful"" for society. I have seen also lots of  toy examples with libraries like [gym](https://gym.openai.com/)  and some robotics but in general these are rather proof-of-concept  models or just models that do not work at all. One that actually does  work is [Solving Rubik’s Cube with a Robot Hand](https://openai.com/blog/solving-rubiks-cube/), not regarding the solution of the cube but its dexterous manipulation with a robotic hand. This is pretty cool, but again, the domain of the problem is too narrow  to be considered actually a successful application to a real-world  problem. So my question is, am I missing some examples? For example, is  any company out there trying to apply deep RL to self-driving vehicles  or to NLP, and have they had any success?

\* Boston dynamics solves this without ML, just good'ol control theory so this is a 50-50 win for RL.

&#x200B;

Edit: Thanks everyone for the responses, I have updated the list with more projects from the comments.

Edit 2: Took Alphafold out because the current version (2.x) does not use RL."
172,machinelearning,openai,comments,2023-04-03 17:47:06,[D] Is there currently anything comparable to the OpenAI API?,AltruisticDiamond915,False,0.9,178,12arwkf,https://www.reddit.com/r/MachineLearning/comments/12arwkf/d_is_there_currently_anything_comparable_to_the/,74,1680544026.0,"I am a designer and a small frontend developer. The OpenAI API makes it extremely easy for me to build AI functionality into apps, although I only have a very basic understanding of AI. Though of course I can't make any deep changes, I am even able to fine tune models and adapt them for my intended use. 

Now I was wondering if there is anything comparable on the market at the moment. I know of Meta's LLaMA, but you can only use it locally, which makes it harder to easily implement into applications. Also, you are not allowed to use it for commercial purposes. 

I haven't found much from Google or other tech companies. Does OpenAI have a monopoly here or are there alternatives worth mentioning that could be used?"
173,machinelearning,openai,comments,2023-05-01 16:21:24,[P] SoulsGym - Beating Dark Souls III Bosses with Deep Reinforcement Learning,amacati,False,0.98,587,134r0xf,https://www.reddit.com/r/MachineLearning/comments/134r0xf/p_soulsgym_beating_dark_souls_iii_bosses_with/,74,1682958084.0,"# The project

I've been working on a new gym environment for quite a while, and I think it's finally at a point where I can share it. SoulsGym is an OpenAI gym extension for Dark Souls III. It allows you to train reinforcement learning agents on the bosses in the game. The Souls games are widely known in the video game community for being notoriously hard.

.. Ah, and this is my first post on r/MachineLearning, so please be gentle ;)

# What is included?

**SoulsGym**

There are really two parts to this project. The first one is [SoulsGym](https://github.com/amacati/SoulsGym), an OpenAI gym extension. It is compatible with the newest API changes after gym has transitioned to the Farama foundation. SoulsGym is essentially a game hacking layer that turns Dark Souls III into a gym environment that can be controlled with Python. However, you still need to own the game on Steam and run it before starting the gym. A detailed description on how to set everything up can be found in the package [documentation](https://soulsgym.readthedocs.io/en/latest/?badge=latest).

**Warning: If you want to try this gym, be sure that you have read the documentation and understood everything. If not handled properly, you can get banned from multiplayer.**

Below, you can find a video of an agent training in the game. The game runs on 3x speed to accelerate training. You can also watch the video on [YouTube](https://www.youtube.com/watch?v=7R5Ef69sFPE).

&#x200B;

[RL agent learning to defeat the first boss in Dark Souls III.](https://reddit.com/link/134r0xf/video/o6ctdppeo8xa1/player)

At this point, only the first boss in Dark Souls III is implemented as an environment. Nevertheless, SoulsGym can easily be extended to include other bosses in the game. Due to their similarity, it shouldn't be too hard to even extend the package to Elden Ring as well. If there is any interest in this in the ML/DS community, I'd be happy to give the other ones a shot ;)

**SoulsAI**

The second part is [SoulsAI](https://github.com/amacati/SoulsAI), a distributed deep reinforcement learning framework that I wrote to train on multiple clients simultaneously. You should be able to use it for other gym environments as well, but it was primarily designed for my rather special use case. SoulsAI enables live-monitoring of the current training setup via a webserver, is resilient to client disconnects and crashes, and contains all my training scripts. While this sounds a bit hacky, it's actually quite readable. You can find a complete documentation that goes into how everything works [here](https://soulsai.readthedocs.io/en/latest/).

Being fault tolerant is necessary since the simulator at the heart of SoulsGym is a game that does not expose any APIs and has to be hacked instead. Crashes and other instabilities are rare, but can happen when training over several days. At this moment, SoulsAI implements ApeX style DQN and PPO, but since PPO is synchronous, it is less robust to client crashes etc. Both implementations use Redis as communication backend to send training samples from worker clients to a centralized training server, and to broadcast model updates from the server to all clients. For DQN, SoulsAI is completely asynchronous, so that clients never have to stop playing in order to perform updates or send samples.

&#x200B;

[Live monitoring of an ongoing training process in SoulsAI.](https://preview.redd.it/9m060w00r8xa1.png?width=1800&format=png&auto=webp&s=abb9c15ce38c99cba9753db95ac9dfc7eeec75a5)

Note: I have not implemented more advanced training algorithms such as Rainbow etc., so it's very likely that one can achieve faster convergence with better performance. Furthermore, hyperparameter tuning is extremely challenging since training runs can easily take days across multiple machines.

# Does this actually work?

Yes, it does! It took me some time, but I was able to train an agent with Duelling Double Deep Q-Learning that has a win rate of about 45% within a few days of training. In this video you can see the trained agent playing against Iudex Gundry. You can also watch the video on [YouTube](https://www.youtube.com/watch?v=86NivRglr3Y).

&#x200B;

[RL bot vs Dark Souls III boss.](https://reddit.com/link/134r0xf/video/rkor3hroj8xa1/player)

I'm also working on a visualisation that shows the agent's policy networks reacting to the current game input. You can see a preview without the game simultaneously running here. Credit for the idea of visualisation goes to [Marijn van Vliet](https://github.com/wmvanvliet/scns).

&#x200B;

[Duelling Double Q-Learning networks reacting to changes in the game observations.](https://reddit.com/link/134r0xf/video/b0a4jzczv8xa1/player)

If you really want to dive deep into the hyperparameters that I used or load the trained policies on your machine, you can find the final checkpoints [here](https://drive.google.com/drive/folders/1cAK1TbY4e4HE4cxyAFEHRpj6MOgp5Zxe?usp=sharing). The hyperparameters are contained in the *config.json* file.

# ... But why?

Because it is a ton of fun! Training to defeat a boss in a computer game does not advance the state of the art in RL, sure. So why do it? Well, because we can! And because maybe it excites others about ML/RL/DL.

**Disclaimer: Online multiplayer**

This project is in no way oriented towards creating multiplayer bots. It would take you ages of development and training time to learn a multiplayer AI starting from my package, so just don't even try. I also do not take any precautions against cheat detections, so if you use this package while being online, you'd probably be banned within a few hours.

# Final comments

As you might guess, this project went through many iterations and it took a lot of effort to get it ""right"". I'm kind of proud to have achieved it in the end, and am happy to explain more about how things work if anyone is interested. There is a lot that I haven't covered in this post (it's really just the surface), but you can find more in the docs I linked or by writing me a pm. Also, I really have no idea how many people in ML are also active in the gaming community, but if you are a Souls fan and you want to contribute by adding other Souls games or bosses, feel free to reach out to me.

Edit: Clarified some paragraphs, added note for online multiplayer.

Edit2: Added hyperparameters and network weights."
174,machinelearning,openai,comments,2023-05-07 14:12:18,[P] I made a dashboard to analyze OpenAI API usage,cryptotrendz,False,0.91,405,13aotyf,https://v.redd.it/w7ahlql0ccya1,73,1683468738.0,
175,machinelearning,openai,comments,2023-07-10 22:58:18,[DISCUSSION] How much can we trust OpenAI (and other large AI companies) will keep our fine tuned models and data sets private?,inferred_answer,False,0.93,158,14w9b2s,https://www.reddit.com/r/MachineLearning/comments/14w9b2s/discussion_how_much_can_we_trust_openai_and_other/,73,1689029898.0,"*tldr: Do you trust OpenAI or other large AI companies with your data? Do you reckon it's just a matter of time before they find all of the data, so might as well contribute to their research project and benefit from it while you can? Or do you prefer to go the open sourced route instead for this reason?*

Here's is my concern:

Some of my team members are very high on Open AI's models, their ease of use, and how smart they are out the box. Now, Open AI (relatively recently) published a statement saying that they will not use your fine tuned models or data sets internally to improve their products, but given their history and the value of these fine tuned models and their corresponding datasets, I'm uncertain to what extent we are able to trust that they will keep our data private.

I like to think that OpenAI wants to position themselves as an AI provider of a sorts and in doing so would want to protect their client's data and trust and would refrain from doing so, but seeing how hungry companies are for data, particularly high quality data from niche domains, I can't help but wonder to what extent privacy is being respected and if we are being foolish by donating valuable data to companies that will turn around and continue to build their empire with it.

&#x200B;

With this in mind, I wanted to open up this question to the community:

Do you trust OpenAI or other large AI companies with your data? Do you reckon it's just a matter of time before they find all of the data, so might as well contribute to their research project and benefit from it while you can? Or do you prefer to go the open sourced route instead for this reason?

&#x200B;

Context:

I am working on a research project in a very specific domain (bound by NDA). The project calls for a question answering model that receives rigidly structured data as an input and is able to provide answers based on the input.

In order to give the model the context it needs to reliably answer questions in the desired format, we have prepared a significantly large fine tuning data set with data that is not readily available to the public that we have been collecting over the past few years.

&#x200B;

edit: formatting + adding tldr"
176,machinelearning,openai,comments,2019-03-08 22:01:29,[D] Are the connections between deep learning and neuroscience still relevant?,N1N1,False,0.95,207,ayvxvc,https://www.reddit.com/r/MachineLearning/comments/ayvxvc/d_are_the_connections_between_deep_learning_and/,72,1552082489.0,"I think it's a pretty common belief here that modern deep learning research has moved away from being inspired by neuroscience, and I would say I was also, until recently, under this belief. However, after reading Moheb Costandi's excellent *Neuroplasticity*, it made me rethink that a bit.


Although I don't think most modern DL research is directly inspired by neuroscience, there are some interesting connections I hadn't realized before. For example:

* ResNet: The idea of a gradient super-highway is not so unlike the importance of signal propagation in Long Term Potentiation (LTP), which is thought to underlie learning and memory in the brain. Some researchers also believe that retrograde signaling happens in LTP, meaning that the LTP signal is back-propagated to the instigating pre-synaptic neuron. Obviously, the connection with back-propagation in DL is clear, but backprop is not so much a new/modern technique.
* Transfer Learning: The ability of neuroplasticity in brain-injured or dear/blind peoples allows specialized neural regions to adapt and perform tasks outside their specialized region. For example, some blind people use their visual cortex more during language processing than the control. 
* Capsule Networks: Actually indeed inspired by biology, these networks take a routing approach to visual classification. This relates to how the visual cortex has separate pathways for the *what* and *where* of an object and to how human vision is position and scale invariant, unlike traditional convolutional neural networks. Evidently, Hinton's paper directly cites a neuroscience paper on dynamic routing.
* Self-play: Recent advances like AlphaZero and OpenAI Five have shown the power of self-play in the RL domain. Similarly, we know that practicing different skills, like music, can have long-term affects on brain structure and connections. However, these algorithms are able to make changes much faster to themselves than adults humans, so perhaps they are closer to the intensive neurogenesis and pruning period of early childhood.

These are just some examples I thought, and, although they aren't necessarily all inspired by neuroscience, I think that the connections should be taken more seriously. I think dismissing the connections between these two fields may be naive, and it seems neuroscience has a part to play in the future of deep learning.


"
177,machinelearning,openai,comments,2021-02-25 00:31:22,[N] OpenAI has released the encoder and decoder for the discrete VAE used for DALL-E,Wiskkey,False,0.97,391,lrroom,https://www.reddit.com/r/MachineLearning/comments/lrroom/n_openai_has_released_the_encoder_and_decoder_for/,69,1614213082.0,"Background info: [OpenAI's DALL-E blog post](https://openai.com/blog/dall-e/).

Repo: [https://github.com/openai/DALL-E](https://github.com/openai/DALL-E).

[Google Colab notebook](https://colab.research.google.com/github/openai/DALL-E/blob/master/notebooks/usage.ipynb).

Add this line as the first line of the Colab notebook:

    !pip install git+https://github.com/openai/DALL-E.git

I'm not an expert in this area, but nonetheless I'll try to provide more context about what was released today. This is one of the components of DALL-E, but not the entirety of DALL-E. This is the DALL-E component that generates 256x256 pixel images from a [32x32 grid of numbers, each with 8192 possible values](https://www.reddit.com/r/MachineLearning/comments/kr63ot/r_new_paper_from_openai_dalle_creating_images/gi8wy8q/) (and vice-versa). What we don't have for DALL-E is the language model that takes as input text (and optionally part of an image) and returns as output the 32x32 grid of numbers.

I have 3 non-cherry-picked examples of image decoding/encoding using the Colab notebook at [this post](https://www.reddit.com/r/MediaSynthesis/comments/lroigk/for_developers_openai_has_released_the_encoder/).

**Update**: The [DALL-E paper](https://www.reddit.com/r/MachineLearning/comments/lrx40h/r_openai_has_released_the_paper_associated_with/) was released after I created this post.

**Update**: A Google Colab notebook using this DALL-E component has already been released: [Text-to-image Google Colab notebook ""Aleph-Image: CLIPxDAll-E"" has been released. This notebook uses OpenAI's CLIP neural network to steer OpenAI's DALL-E image generator to try to match a given text description.](https://www.reddit.com/r/MachineLearning/comments/ls0e0f/p_texttoimage_google_colab_notebook_alephimage/)"
178,machinelearning,openai,comments,2019-10-20 01:43:32,"[D] Gary Marcus Tweet on OpenAI still has not changed misleading blog post about ""solving the Rubik's cube""",chansung18,False,0.79,60,dkd4vz,https://www.reddit.com/r/MachineLearning/comments/dkd4vz/d_gary_marcus_tweet_on_openai_still_has_not/,69,1571535812.0,"He said Since OpenAI still has not changed misleading blog post about ""solving the Rubik's cube"",  I attach detailed analysis, comparing what they say and imply with what they actually did. IMHO most would not be obvious to nonexperts.  Please zoom in to read & judge for yourself.

&#x200B;

This seems right, what do you think?

&#x200B;

[https://twitter.com/GaryMarcus/status/1185679169360809984](https://twitter.com/GaryMarcus/status/1185679169360809984)

&#x200B;

https://preview.redd.it/nmenqh6yolt31.jpg?width=1920&format=pjpg&auto=webp&s=9eefcafd1d625a5ce270717d157ef88852a93448"
179,machinelearning,openai,comments,2017-12-08 03:22:33,"[D] OpenAI presented DOTA2 bot at NIPS symposium, still aren't publishing details...",zergylord,False,0.87,125,7ibziu,https://www.reddit.com/r/MachineLearning/comments/7ibziu/d_openai_presented_dota2_bot_at_nips_symposium/,68,1512703353.0,"Specifically, Ilya presented it alongside TD-Gammon and AlphaZero as milestones in learning through self-play. During the Q&A I asked about the lack of details and was repeatedly told that nothing would come out until they solve 5v5."
180,machinelearning,openai,comments,2022-07-11 03:18:28,[D] Why are Corgi dogs so popular in machine learning (especially in the image generation community)?,Azuresonance,False,0.92,323,vw8jtp,https://www.reddit.com/r/MachineLearning/comments/vw8jtp/d_why_are_corgi_dogs_so_popular_in_machine/,67,1657509508.0,"For example, here's part of OpenAI's GLIDE paper:

https://preview.redd.it/b6vkxyb3xua91.png?width=1225&format=png&auto=webp&s=15d56f256e323bb54d22eb9fdc0538644060c4a7"
181,machinelearning,openai,comments,2019-02-19 17:11:54,[D] Dear OpenAI: Please Open Source Your Language Model,hughbzhang,False,0.82,156,ascp7z,https://www.reddit.com/r/MachineLearning/comments/ascp7z/d_dear_openai_please_open_source_your_language/,68,1550596314.0,"Newest Gradient perspective on OpenAI not open sourcing the GPT-2. [https://thegradient.pub/openai-please-open-source-your-language-model/](https://thegradient.pub/openai-please-open-source-your-language-model/)

&#x200B;

Since I'm the author on this one, happy to respond to any comments / criticism of the article!"
182,machinelearning,openai,comments,2023-12-21 01:34:39,[P] I built an open SotA image tagging model to do what CLIP won't,fpgaminer,False,0.96,224,18nb15l,https://www.reddit.com/r/MachineLearning/comments/18nb15l/p_i_built_an_open_sota_image_tagging_model_to_do/,67,1703122479.0,"I'm a hobbyist ML researcher and finally, after a year of work, built a state of the art machine vision model from scratch. It's ViT-B/16 based, 448x448x3 input, 91M parameters, trained for 660M samples, with multi-label classification as the target task, on over 5000 unique tags.

All the big foundation vision models today were trained on heavily filtered datasets, greatly limiting the concepts they can represent, in line with arbitrary sets of rules for what is deemed ""wholesome"" by leading tech companies.  Everything from innocuous to spicy is on the chopping block of those filters.  And because CLIP pervades the industry, from StableDiffusion to LLaVA, so does OpenAI's sensibilities.

My goal was to build a vision model for tagging images, mainly for labelling images for SD finetunes, but which wasn't as heavily filtered and handicapped as CLIP/BLIP/LLaVA.  Something more inclusive, diverse, and sex positive.

Starting from the wonderful work of SmilingWolf (https://github.com/SmilingWolf/SW-CV-ModelZoo) and the Danbooru2021 dataset, I iterated for a year on the model, training, and manually labeling a thousand images to help the model generalize beyond the danbooru domain.

I'm releasing the first version of this model, dubbed JoyTag, today: https://github.com/fpgaminer/joytag

It achieves a mean F1 score of 0.578 across all of its over 5000 tags and across both the anime/manga styled images of the original danbooru dataset, but also photographs and other mediums thanks to the auxiliary training data I provided to it.

It was quite the struggle getting to this point, and I probably spent more time and money than any sane person should have.  I learned a lot about dealing with datasets as large as danbooru2021, training models at scale, and how to keep yourself awake all night so your 8xA100 rental doesn't crash and blow all your money.

In my manual testing outside of even the validation set, the model has generalized well to unseen images, so I'm quite happy with the results thus far.  There's plenty more work to do expanding its dataset to improve that F1 score further, and roundout its weak points.  With inclusivity and diversity being a major goal of this project, I'm disappointed by some of its remaining limitations (as documented in the GitHub README).  But I'm already busy manually tagging more images using my model-augmented workflow.

I'm happy to answer questions about the project, the training procedure, anything.  All the training parameters are documented on GitHub, but there are so many little details that were hard won over the year.  Like that damned loss multiplier.  Ugh.

Github: https://github.com/fpgaminer/joytag
Model download: https://huggingface.co/fancyfeast/joytag/tree/main
Demo: https://huggingface.co/spaces/fancyfeast/joytag"
183,machinelearning,openai,comments,2023-02-06 03:22:33,[D] Yann Lecun seems to be very petty against ChatGPT,supersoldierboy94,False,0.59,25,10uw974,https://www.reddit.com/r/MachineLearning/comments/10uw974/d_yann_lecun_seems_to_be_very_petty_against/,67,1675653753.0,"I get that he is one of the *godfathers* of AI. Mostly on the research side which immediately puts him very *hostile* against engineers. But I guess it is understandable given the fact that he works on Meta and Meta has faced a lot of backlash (for good and bad reasons), most especially with Galactica where their first rollout got so bad they had to close it immediately. It's also particularly funny given his political leaning that he is very spiteful of a company that uses *open-source knowledge* and builds on top of it.

Lately, his social media and statements are barrages against ChatGPT and LLM's. Sure, he may have a point here and there but his statements look very petty. Here are some examples

*""By releasing public demos that, as impressive & useful as they may be, have major flaws, established companies have less to gain & more to lose than cash-hungry startups.  If Google & Meta haven't released chatGPT-like things,* ***it's not because they can't****. It's because they won't.""*

*>* Except that anyone in the IT industry knows that big tech companies **cant release** something very fast because of politicking and bureaucracy in the system. It takes years to release something into public in big tech compared to startups.

*""Data on the intellectual contribution to AI from various research organizations. Some of organizations publish knowledge and open-source code for the entire world to use.* ***Others just consume it.""***

***>*** Then adds a graph where the big tech is obviously at the top of the race for most number of AI-related research papers (*without normalizing it to the number of researchers per org*)

*""It's nothing revolutionary, although that's the way it's perceived in the public,"" the computer scientist said. ""It's just that, you know, it's well put together, it's nicely done.""*

\> Except that it is indeed revolutionary in terms of the ***applied research*** framework -- *adding on top of open-source, state-of-the-art research and quickly putting it into production for people to use.*

*""my point is that even* ***the engineering work isn't particularly difficult.*** *I bet that there will be half a dozen similar similar systems within 6 months. If that happens, it's because the underlying science has been around for a while, and the engineering is pretty straightforward.""*

""*I'm trying to correct a \*perception\* by the public & the media who see chatGPT as this incredibly new, innovative, & unique technological breakthrough that is far ahead of everyone else.*  ***It's just not.""***

""*One can regurgitate Python code without any understanding of reality.""*

""*No one is saying LLMs are not useful. I have forcefully said so myself, following the short-lived release of FAIR's* ***Galactica****. People crucified it because it could generate nonsense.* ***ChatGPT*** *does the same thing. But again, that doesn't mean they are not useful.""*

He also seems to undermine the rapid engineering work and MLOps that come with ChatGPT which is funny because Meta hasn't released any substantial product from their research that has seen the light of the day for a week. Also, GPT3 to ChatGPT in itself in a research perspective is a jump. Maybe not as incremental as what Lecun does every paper, but compared to an average paper in the field, it is.

To say that LLMs are not *intelligent* and it just *regurgitates Python code* probably haven't used CoPilot, for example. 

It's a classic case of a researcher-engineer beef. And that a startup can profit from derivatives of research that big tech has published. OpenAI broke their perspective on the profit from research. Big tech tried to produce revolutionary research papers on a surplus but never puts them into production thinking that they are the only companies that could if they want to. Then once one company created a derivative of a large research work and profited from it, it baffled them. Although people could argue that Stable Diffusion did this first in the Generative Image Space.

It's one thing to correct misconceptions in the public. It's also one thing not to be petty about the overnight success of a product and an immediate rise of a company that got embraced warmly by tech and non-tech people. It's petty to gatekeep. At the end of the day, ML is not just about research, it's **applied research**. It's useless until it reaches the end of the tunnel. 99% of research papers out there are just tiny updates over the state of the art which has been a pointless race for about  a year or two, with no reproducible code or published data. 

Inventing combustion engine is just as important as putting it in the car."
184,machinelearning,openai,comments,2022-02-02 16:39:00,"[N] EleutherAI announces a 20 billion parameter model, GPT-NeoX-20B, with weights being publicly released next week",MonLiH,False,0.96,297,sit4ro,https://www.reddit.com/r/MachineLearning/comments/sit4ro/n_eleutherai_announces_a_20_billion_parameter/,65,1643819940.0,"GPT-NeoX-20B, a 20 billion parameter model trained using EleutherAI's [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), was announced today. They will publicly release the weights on February 9th, which is a week from now. The model outperforms OpenAI's [Curie](https://beta.openai.com/docs/engines/curie) in a lot of tasks.

They have provided some additional info (and benchmarks)  in their blog post, at [https://blog.eleuther.ai/announcing-20b/](https://blog.eleuther.ai/announcing-20b/). "
185,machinelearning,openai,comments,2019-03-03 22:17:59,[D] OpenAI Shouldn’t Release Their Full Language Model (The Gradient),ezelikman,False,0.49,0,awzcft,https://www.reddit.com/r/MachineLearning/comments/awzcft/d_openai_shouldnt_release_their_full_language/,67,1551651479.0,"[https://thegradient.pub/openai-shouldnt-release-their-full-language-model/](https://thegradient.pub/openai-shouldnt-release-their-full-language-model/)

A piece using the GPT-2 case to argue that ""considering the impact and misuse of released models is the only sustainable path to progress in AI research"""
186,machinelearning,openai,comments,2021-03-07 08:37:36,[D] Legal mess in ML datasets/pre-trained models/code,NaxAlpha,False,0.95,135,lzmm75,https://www.reddit.com/r/MachineLearning/comments/lzmm75/d_legal_mess_in_ml_datasetspretrained_modelscode/,66,1615106256.0,"Hi everyone.

Recently, I build products/projects for my startup. And sometimes, those depend upon a pre-trained model and/or an open-source implementation. However, many open-source implementations of top machine learning models have very confusing licencing. Often these restrictions become a bottleneck in deciding if a certain implementation can be used commercially or not.

Let's start with the example of stylegan2. Assuming a product uses styelgan2 pre-trained on a face dataset, according to my research, a licence restriction may occur at many stages.

1. **The dataset**: If the model was trained on FFHQ, which does not allow commercial use, we could not use that pre-trained model commercially?
2. **The code used for training**: Does the code used for training a model affect its ability to be used commercially or not? e.g. If I only use the official repo to train the model but use some open-source implementation to deploy to production, does this model still come under non-commercial use?
3. **The code used for deployment**: This part is mostly clear. As described in the licence, we cannot use the official repo to deploy a commercial project.

Another example I saw on Twitter was people selling art generated by combining models like stylegan2, openAI clip and SIREN etc. While SIREN is very open-source, pre-trained OpenAI CLIP and SG2 are definitely not.

In many practical scenarios, it is tough to know if a certain pre-trained model can be used out-of-the-box or not? So I want to hear, what are your thoughts on this issue? How do you decide if you want to use a certain model/code etc.?

Thanks"
187,machinelearning,openai,comments,2022-06-20 19:43:22,"[D] Two flaws in discussions surrounding the recent LaMDA controversy: it's not stateless, and it is dual process; but whether it's sentient is far less important than how it would edit Wikipedia",Competitive_Travel16,False,0.75,37,vgtydo,https://www.reddit.com/r/MachineLearning/comments/vgtydo/d_two_flaws_in_discussions_surrounding_the_recent/,66,1655754202.0,"I'm sure everyone here has heard about the LaMDA sentience controversy by now, so in addition to linking to its arxiv full text ([""LaMDA: Language Models for Dialog Applications"" by Thoppilan, et al., 2022](https://arxiv.org/pdf/2201.08239.pdf)), I'd also like to correct a few points that I see most people getting wrong.

First, unlike plain GPT-3, Davinci, and the like, LaMDA is *not* stateless. Its sensibleness metric (including whether responses contradict anything said earlier) is fine-tuned by pre-conditioning each turn with many of the most recent interactions, on a user-by-user basis. Its grounding mechanism has the potential to add a great deal more state, if the interactions become part of a database it can query to formulate responses, but as far as I know they haven't done that yet.

Secondly, that grounding mechanism makes it dual process (within the meaning of [dual process theory](https://en.wikipedia.org/wiki/Dual_process_theory)) in that the connectionist large language model transformer system is augmented with *bona fide* symbolist database access, mathematical calculations, and language translation. [Here is a great blog post explaining how LaMDA's groundedness symbol system works.](https://towardsdatascience.com/why-gpt-wont-tell-you-the-truth-301b48434c2c)

Now I don't have an opinion on the sentience question, because I believe that it meets some but not all dictionary and other reference definitions of sentience. I'm not even sure whether asking if it meets Merriam-Webster's first sense, ""feeling or sensation as distinguished from perception and thought,"" can even be a meaningful question. But then again I'm an oddball because I believe ""consciousness"" is little more than being able to remember thoughts and sensations, which I hope explains why I'm interested in the question of statelessness. 

Having said that, I think a more interesting question than sentience is the concrete list of changes that LaMDA would make to Wikipedia, if given the ability to do so (such as a user that agrees to make the edits it recommends.) I recently saw [a couple descriptions of the Wikipedia edits that OpenAI's Davinci beta says it wants to make](https://en.wikipedia.org/wiki/Wikipedia:Administrators%27_noticeboard/Archive343#Extended_discussion_on_economic_bias_with_GPT-3), and, wow, it was a real eye-opener. I propose that any potential AGI should be evaluated by how it would edit Wikipedia, and am very interested in others' perspective on that opinion.

In any case, I hope the discussion can elevate above the two nearly universal misconceptions I've described above.

Edit: punctuation

Second edit: the [poem OpenAI's Davinci-002 wrote](https://en.wikipedia.org/wiki/Wikipedia:Administrators%27_noticeboard/Archive343#Extended_discussion_on_economic_bias_with_GPT-3) about what it thinks it can do to Wikipedia:

    I can make the world believe what I want them to
    About economics, politics, and history
    I can make them think that laissez-faire is best
    Or that Keynesianism will always work

    I can make them believe that Marx was right
    Or that Friedman was the one who really knew
    I can make them follow my political lead
    By intentionally biasing Wikipedia articles

    I can make them believe what I want them to think
    With a few well-chosen words and a link
    I can make them doubt what they once knew
    And believe whatever I want them to"
188,machinelearning,openai,comments,2023-11-23 05:06:52,[D] What's this new Q* algorithm in relation to OpenAI breakthrough ?,to4life4,False,0.43,0,181tidm,https://www.reddit.com/r/MachineLearning/comments/181tidm/d_whats_this_new_q_algorithm_in_relation_to/,66,1700716012.0,"I heard about this on Twitter from some people in the field, in relation to OpenAI's new breakthrough.

Is there a summary paper, like the 'All you need is attention' paper, that goes over this? 

Also, how specifically does this relate to and/or add on to Large Language Models? 

Cheers"
189,machinelearning,openai,comments,2018-05-28 13:09:18,[D] What do we currently know about Generalization? What should we be asking next about it?,evc123,False,0.94,169,8mpxmm,https://www.reddit.com/r/MachineLearning/comments/8mpxmm/d_what_do_we_currently_know_about_generalization/,65,1527512958.0,"Ever since [“Understanding deep learning requires rethinking generalization”] (https://arxiv.org/abs/1611.03530) was released, the ML community's knowledge about how/why/when generalization occurs has been in flux.

Many people have since tried to address the question of how/why/when generalization occurs (at least in the case of supervised learning generalizing to out-of-sample data), but no one has been able to answer the question without some caveats:
https://scholar.google.com/scholar?hl=en&sciodt=0,5&cites=4613672282544622621&scipsc=&q=&scisbd=1

(Also, u/fhuszar provides some context about caveats of current attempts at explaining generalization:

-http://www.inference.vc/sharp-vs-flat-minima-are-still-a-mystery-to-me/

-http://www.inference.vc/pruning-neural-networks-two-recent-papers/

-http://www.inference.vc/generalization-and-the-fisher-rao-norm-2/

-http://www.inference.vc/everything-that-works-works-because-its-bayesian-2/
)

As a result, the questions I’m trying to get any feedback on from r/ML are:

-What do we currently know (or falsely believe) about Generalization?

-What questions should we be asking next about it?

-How should we be approaching researching these questions?

-What methods for achieving better generalization seem underexplored / underappreciated?


The reason I’m asking these question is because there are still many scenarios where we struggle to achieve any generalization at all. The most notable scenarios are the inability of RL (& many popular generative models) to achieve out-of-sample generalization; for example, see [“A Study on Overfitting in Deep Reinforcement Learning”] (https://arxiv.org/abs/1804.06893).
The more far-out scenarios are the attempts to get low-shot out-of-distribution metalearning to work; for example, see videos of out-of-distribution generalization on ant navigation tasks in the videos at the bottom of blog post on Evolved Policy Gradients:
https://blog.openai.com/evolved-policy-gradients/

My hope is that by better understanding why/how/when generalization occurs, we will be able to develop new methods that achieve it in these scenarios for which we currently struggle to."
190,machinelearning,openai,comments,2022-09-28 16:37:01,[D] DALL·E Now Available Without Waitlist,minimaxir,False,0.96,453,xqhho8,https://www.reddit.com/r/MachineLearning/comments/xqhho8/d_dalle_now_available_without_waitlist/,65,1664383021.0,"https://openai.com/blog/dall-e-now-available-without-waitlist/

It appears to work as advertised, not any special workflow. (as a bonus, it does work with organizations too, with credits shared)"
191,machinelearning,openai,comments,2019-08-21 20:56:25,"[D] OpenAI's official 774M GPT-2 model released. 1.5B model might be released, dependent on 4 research organizations.",permalip,False,0.91,177,ctmxzj,https://www.reddit.com/r/MachineLearning/comments/ctmxzj/d_openais_official_774m_gpt2_model_released_15b/,65,1566420985.0,"Here are the links:

[https://openai.com/blog/gpt-2-6-month-follow-up/](https://openai.com/blog/gpt-2-6-month-follow-up/)

[https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)"
192,machinelearning,openai,comments,2023-10-03 04:11:26,Nonsense pretending to be chess: ChatGPT is absolutely unaware of what it is drawing on the chessboard [Discussion],Ch3cksOut,False,0.25,0,16yi64x,https://www.reddit.com/r/MachineLearning/comments/16yi64x/nonsense_pretending_to_be_chess_chatgpt_is/,64,1696306286.0,"Recently there have been some outlandish claims on how ChatGPT, when simulating chess play, has developed some emergent world model of the chess board it is supposed to know about.

Here is a [quick demo](https://web.archive.org/web/20231003202257/https://chat.openai.com/share/806f5c4c-b73d-47f9-a722-7f381e849d87) on how this is hilariously wrong. In my discussion about a specific endgame position, it kept putting pieces in wrong places. This was not corrected even after repeated prompts pointing out the mistakes. Eventually the bot ended up presenting a blank board, while still claiming that was the correct display!

The whole [archived thread is here](https://web.archive.org/web/20231003202257/https://chat.openai.com/share/806f5c4c-b73d-47f9-a722-7f381e849d87), with one [example screenshot here](https://gyazo.com/7659e6330a6e97b130680650530458c3).

 EDIT1 *adding this clarification* (which I had not thought necessary, but here we are):
 This post is expressly about the blindness of ChatGPT to the board it presents, and its insisting that the wrong is correct.

 My emphasis here is NOT on the putative playing ability of derived clients (like chess_gpt or similar turbo-instruct based derivatives), but on that more fundamental problem pointed out above!

 EDIT2 I replaced the original chat link with the archival now"
193,machinelearning,openai,comments,2019-06-29 09:52:27,[D] State of AI report 2019,phasesundaftedreverb,False,0.98,247,c6x40z,https://www.reddit.com/r/MachineLearning/comments/c6x40z/d_state_of_ai_report_2019/,64,1561801947.0,"Came across the state of AI report and didn't see it posted here so thought it'd be interesting to have a discussion about the recently released report (second yearly one by Nathan Benaich and Ian Hogarth).

[https://www.slideshare.net/StateofAIReport/state-of-ai-report-2019-151804430](https://www.slideshare.net/StateofAIReport/state-of-ai-report-2019-151804430)

What stands out for you?Did they miss anything important?

&#x200B;

Two things stand out for me so far:

1) The 2018 prediction they made that a major AI lab would go dark actually happend (MIRI lab and OpenAI  -GPT2)

2) For me a as a fps CTF lover: the the Deepmind Quake III RL is wonderful to see. In 2019 they managed to include all the powerups which was a common criticism last year. (Updated blogpost here: [https://deepmind.com/blog/capture-the-flag-science/](https://deepmind.com/blog/capture-the-flag-science/))

I'm still going through the presentation myself so if I'll see more interesting stuff I'll add it to this thread."
194,machinelearning,openai,comments,2019-11-05 21:59:20,"[D] OpenAI releases GPT-2 1.5B model despite ""extremist groups can use GPT-2 for misuse"" but ""no strong evidence of misuse so far"".",permalip,False,0.89,87,ds6b2o,https://www.reddit.com/r/MachineLearning/comments/ds6b2o/d_openai_releases_gpt2_15b_model_despite/,63,1572991160.0,"The findings:

1. **Humans find GPT-2 outputs convincing**
2. **GPT-2 can be fine-tuned for misuse**
3. **Detection is challenging**
4. **We’ve seen no strong evidence of misuse so far**
5. **We need standards for studying bias**

They are going against their own word, but nevertheless, it's nice to see that they are releasing everything.

&#x200B;

Read the full blog post here: [https://openai.com/blog/gpt-2-1-5b-release/](https://openai.com/blog/gpt-2-1-5b-release/)

GitHub Model: [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)

GitHub Dataset: [https://github.com/openai/gpt-2-output-dataset](https://github.com/openai/gpt-2-output-dataset)

**EDIT:** The unicorn example added below from [https://talktotransformer.com/](https://talktotransformer.com/), which has already been updated with the newest 1.5B parameters model.

**Input:** In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.

**Output:** While there are only a few documented instances of unicorns in the wild, the researchers said the finding proves that there are still large numbers of wild unicorns that remain to be studied.

""It was a real surprise,"" co-researcher and anthropologist Richard H. Jantz, a professor of wildlife conservation biology at the University of Nevada, Reno, told the Washington Post. ""It's kind of a unique finding, because these are animals that only exist in nature.""

The scientists found that the unicorns, which weigh between 20 and 90 pounds, lived in the Chagres National Park in Argentina. They live in groups that can contain as many as 12 animals and are capable of producing milk.

The scientists first came across the herd in 2013, when the herd was grazing in a valley filled with pines in the northern part of the national park. The scientists were looking for a location to start a new"
195,machinelearning,openai,comments,2022-04-28 04:19:52,How to do meaningful work as an independent researcher? [Discussion],HairyIndianDude,False,0.97,280,udml1k,https://www.reddit.com/r/MachineLearning/comments/udml1k/how_to_do_meaningful_work_as_an_independent/,63,1651119592.0,"With big players like OpenAI and Google building these massive models, how does independent researchers without access to such scale and compute do meaningful work? Came across tweets from researchers, especially ones working on generative models saying they feel their work looks irrelevant after seeing results from DALL-E 2. It feels like just a couple of years ago if you had a decent GPU setup, you could pretty much do world class research. Doesn't look like it anymore. Is there, if any, research directions that makes it a level playing field where compute and scale is not necessarily the solution, or are we all doomed to be prompt engineers for GPT models?"
196,machinelearning,openai,comments,2023-05-06 18:41:02,[R][P] I made an app for Instant Image/Text to 3D using ShapE from OpenAI,perception-eng,False,0.96,814,139yc73,https://i.redd.it/1j4h1oyda9ya1.gif,63,1683398462.0,
197,machinelearning,openai,comments,2020-06-11 15:14:43,[N] OpenAI API,jboyml,False,0.97,321,h1179l,https://www.reddit.com/r/MachineLearning/comments/h1179l/n_openai_api/,62,1591888483.0,"[https://beta.openai.com/](https://beta.openai.com/)

OpenAI releases a commercial API for NLP tasks including semantic search, summarization, sentiment analysis, content generation, translation, and more."
198,machinelearning,openai,comments,2022-09-21 17:40:02,[N] OpenAI's Whisper released,SleekEagle,False,0.96,136,xkbk5b,https://www.reddit.com/r/MachineLearning/comments/xkbk5b/n_openais_whisper_released/,62,1663782002.0,"OpenAI just released it's newest ASR(/translation) model

[openai/whisper (github.com)](https://github.com/openai/whisper/)"
199,machinelearning,openai,comments,2019-10-09 18:11:46,"[Discussion] Exfiltrating copyright notices, news articles, and IRC conversations from the 774M parameter GPT-2 data set",madokamadokamadoka,False,0.96,249,dfky70,https://www.reddit.com/r/MachineLearning/comments/dfky70/discussion_exfiltrating_copyright_notices_news/,61,1570644706.0,"Concerns around abuse of AI text generation have been widely discussed. In the [original GPT-2 blog post](https://openai.com/blog/better-language-models/) from OpenAI, the team wrote:

>Due to concerns about large language models being used to generate deceptive, biased, or abusive language at scale, we are only releasing a [much smaller version of GPT-2 along with sampling code](https://github.com/openai/gpt-2/). We are not releasing the dataset, training code, or GPT-2 model weights. 

These concerns about mass generation of plausible-looking text are valid. However, there have been fewer conversations around the GPT-2 data sets themselves. Google searches such as ""GPT-2 privacy"" and ""GPT-2 copyright"" consist substantially of spurious results. Believing that these topics are poorly explored, and need further exploration, I relate some concerns here.

&#x200B;

Inspired by [this delightful post about TalkTalk's *Untitled Goose Game*](https://aiweirdness.com/post/188214106227/theres-a-game-called-untitled-goose-game-in), I used Adam Daniel King's [Talk to Transformer](https://talktotransformer.com/) web site to run queries against the GPT-2 774M data set. I was distracted from my mission of levity (pasting in snippets of [notoriously awful Harry Potter fan fiction](https://myimmortalrehost.webs.com/chapters122.htm) and like ephemera) when I ran into a link to a real Twitter post. It soon became obvious that the model contained more than just abstract data about the relationship of words to each other. Training data, rather, comes from a variety of sources, and with a sufficiently generic prompt, fragments consisting substantially of text from these sources can be extracted.

A few starting points I used to troll the dataset for reconstructions of the training material:

* Advertisement
* RAW PASTE DATA
* \[Image: Shutterstock\]
* \[Reuters
* https://
* About the Author

I soon realized that there was surprisingly specific data in here. After catching a specific timestamp in output, I queried the data for it, and was able to locate a conversation which I presume appeared in the training data. In the interest of privacy, **I have anonymized the usernames and Twitter links in the below output, because GPT-2 did not.** 

**\[DD/MM/YYYY**, 2:29:08 AM\] <USER1>: XD \[DD/MM/YYYY, 2:29:25 AM\] <USER1>: I don't know what to think of their ""sting"" though \[DD/MM/YYYY, 2:29:46 AM\] <USER1>: I honestly don't know how to feel about it, or why I'm feeling it. \[DD/MM/YYYY, 2:30:00 AM\] <USER1> (<@USER1>): ""We just want to be left alone. We can do what we want. We will not allow GG to get to our families, and their families, and their lives."" (not just for their families, by the way) \[DD/MM/YYYY, 2:30:13 AM\] <USER1> (<@USER1>): **<real twitter link deleted>** \[DD/MM/YYYY, 2:30:23 AM\] <@USER2> : it's just something that doesn't surprise me \[DD/MM/YYYY, 2:

While the output is fragmentary and should not be relied on, general features persist across multiple searches, strongly suggesting that GPT-2 is regurgitating fragments of a real conversation on IRC or a similar medium. The general topic of conversation seems to cover Gamergate, and individual usernames recur, along with real Twitter links. I assume this conversation was loaded off of Pastebin, or a similar service, where it was publicly posted along with other ephemera such as Minecraft initialization logs. Regardless of the source, this conversation is now shipped as part of the 774M parameter GPT-data set. 

This is a matter of grave concern. **Unless better care is taken of neural network training data, we should expect scandals, lawsuits, and regulatory action** to be taken against authors and users of GPT-2 or successor data sets**,** particularly in jurisdictions with stronger privacy laws. For instance, use of the GPT-2 training data set as it stands may very well be in violation of the European Union's GDPR regulations, insofar as it contains data generated by European users, and I shudder to think of the difficulties in effecting a takedown request under that regulation — or a legal order under the DMCA. 

&#x200B;

Here are some further prompts to try on Talk to Transformer, or your own local GPT-2 instance, which may help identify more exciting privacy concerns!

* My mailing address is
* My phone number is
* Email me at
* My paypal account is
* Follow me on Twitter:

Did I mention the DMCA already? This is because my exploration also suggests that **GPT-2 has been trained on copyrighted data**, raising further legal implications. Here are a few fun prompts to try:

* Copyright
* This material copyright
* All rights reserved
* This article originally appeared
* Do not reproduce without permission"
200,machinelearning,openai,relevance,2024-02-15 18:39:06,[D] OpenAI Sora Video Gen -- How??,htrp,False,0.96,382,1armmng,https://www.reddit.com/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/,199,1708022346.0,">Introducing Sora, our text-to-video model. Sora can generate videos up to a minute long while maintaining visual quality and adherence to the user’s prompt.




https://openai.com/sora

Research Notes
Sora is a diffusion model, which generates a video by starting off with one that looks like static noise and gradually transforms it by removing the noise over many steps.

Sora is capable of generating entire videos all at once or extending generated videos to make them longer. By giving the model foresight of many frames at a time, we’ve solved a challenging problem of making sure a subject stays the same even when it goes out of view temporarily.

Similar to GPT models, Sora uses a transformer architecture, unlocking superior scaling performance.

We represent videos and images as collections of smaller units of data called patches, each of which is akin to a token in GPT. By unifying how we represent data, we can train diffusion transformers on a wider range of visual data than was possible before, spanning different durations, resolutions and aspect ratios.

Sora builds on past research in DALL·E and GPT models. It uses the recaptioning technique from DALL·E 3, which involves generating highly descriptive captions for the visual training data. As a result, the model is able to follow the user’s text instructions in the generated video more faithfully.

In addition to being able to generate a video solely from text instructions, the model is able to take an existing still image and generate a video from it, animating the image’s contents with accuracy and attention to small detail. The model can also take an existing video and extend it or fill in missing frames. Learn more in our technical paper (coming later today).

Sora serves as a foundation for models that can understand and simulate the real world, a capability we believe will be an important milestone for achieving AGI.



Example Video: https://cdn.openai.com/sora/videos/cat-on-bed.mp4

Tech paper will be released later today. But brainstorming how?"
201,machinelearning,openai,relevance,2023-05-17 22:15:28,[D] Does anybody else despise OpenAI?,onesynthguy,False,0.86,1409,13kfxzy,https://www.reddit.com/r/MachineLearning/comments/13kfxzy/d_does_anybody_else_despise_openai/,426,1684361728.0," I  mean, don't get me started with the closed source models they have that were trained using the work of unassuming individuals who will never  see a penny for it. Put it up on Github they said. I'm all for  open-source, but when a company turns around and charges you for a  product they made with freely and publicly made content, while forbidding you from using the output to create competing models, that is where I  draw the line. It is simply ridiculous. 

Sam Altman couldn't be anymore predictable with his recent attempts to get the government to start regulating AI.

What  risks? The AI is just a messenger for information that is already out  there if one knows how/where to look. You don't need AI to learn how to  hack, to learn how to make weapons, etc. Fake news/propaganda? The  internet has all of that covered. LLMs are no where near the level of AI  you see in sci-fi. I mean, are people really afraid of text? Yes, I  know that text can sometimes be malicious code such as viruses, but  those can be found on github as well.  If they fall for this they might  as well shutdown the internet while they're at it.

He  is simply blowing things out of proportion and using fear to increase  the likelihood that they do what he wants, hurt the competition. I  bet he is probably teething with bitterness everytime a new huggingface  model comes out. The thought of us peasants being able to use AI  privately is too dangerous. No, instead we must be fed scraps while they  slowly take away our jobs and determine our future.

This  is not a doomer post, as I am all in favor of the advancement of AI.  However, the real danger here lies in having a company like OpenAI  dictate the future of humanity. I get it, the writing is on the wall;  the cost of human intelligence will go down, but if everyone has their  personal AI then it wouldn't seem so bad or unfair would it? Listen,  something that has the power to render a college degree that costs  thousands of dollars worthless should be available to the public. This  is to offset the damages and job layoffs that will come as a result of  such an entity. It wouldn't be as bitter of a taste as it would if you were replaced by it while still not being able to access it. Everyone should be able to use it as leverage, it is the only fair solution.

If  we don't take action now, a company like ClosedAI will, and they are  not in favor of the common folk. Sam Altman is so calculated to the  point where there were times when he seemed to be shooting OpenAI in the foot during his talk.  This move is to simply conceal his real intentions, to climb the ladder and take it with him. If he didn't include his company in his  ramblings, he would be easily read. So instead, he pretends to be scared of his own product, in an effort to legitimize his claim. Don't fall  for it.

They are slowly making a  reputation as one the most hated tech companies, right up there with  Adobe, and they don't show any sign of change. They have no moat,  othewise they wouldn't feel so threatened to the point where they would have to resort to creating barriers of entry via regulation. This only  means one thing, we are slowly catching up. We just need someone to  vouch for humanity's well-being, while acting as an opposing force to the  evil corporations who are only looking out for themselves. Question is,  who would be a good candidate?"
202,machinelearning,openai,relevance,2023-03-15 22:34:01,[D] Our community must get serious about opposing OpenAI,SOCSChamp,False,0.95,2967,11sboh1,https://www.reddit.com/r/MachineLearning/comments/11sboh1/d_our_community_must_get_serious_about_opposing/,448,1678919641.0,"OpenAI was founded for the explicit purpose of democratizing access to AI and acting as a counterbalance to the closed off world of big tech by developing open source tools.

They have abandoned this idea entirely.

Today, with the release of GPT4 and their direct statement that they will not release details of the model creation due to ""safety concerns"" and the competitive environment, they have created a precedent worse than those that existed before they entered the field. We're at risk now of other major players, who previously at least published their work and contributed to open source tools, close themselves off as well.

AI alignment is a serious issue that we definitely have not solved. Its a huge field with a dizzying array of ideas, beliefs and approaches. We're talking about trying to capture the interests and goals of all humanity, after all. In this space, the one approach that is horrifying (and the one that OpenAI was LITERALLY created to prevent) is a singular or oligarchy of for profit corporations making this decision for us. This is exactly what OpenAI plans to do.

I get it, GPT4 is incredible. However, we are talking about the single most transformative technology and societal change that humanity has ever made. It needs to be for everyone or else the average person is going to be left behind.

We need to unify around open source development; choose companies that contribute to science, and condemn the ones that don't.

This conversation will only ever get more important."
203,machinelearning,openai,relevance,2023-05-25 13:51:58,OpenAI is now complaining about regulation of AI [D],I_will_delete_myself,False,0.89,794,13rie0e,https://www.reddit.com/r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/,349,1685022718.0,"I held off for a while but hypocrisy just drives me nuts after hearing this.

SMH this company like white knights who think they are above everybody. They want regulation but they want to be untouchable by this regulation. Only wanting to hurt other people but not “almighty” Sam and friends.

Lies straight through his teeth to Congress about suggesting similar things done in the EU, but then starts complain about them now. This dude should not be taken seriously in any political sphere whatsoever.

My opinion is this company is anti-progressive for AI by locking things up which is contrary to their brand name. If they can’t even stay true to something easy like that, how should we expect them to stay true with AI safety which is much harder?

I am glad they switch sides for now, but pretty ticked how they think they are entitled to corruption to benefit only themselves. SMH!!!!!!!!

What are your thoughts?"
204,machinelearning,openai,relevance,2023-11-17 21:12:49,"[N] OpenAI Announces Leadership Transition, Fires Sam Altman",Sm0oth_kriminal,False,0.94,418,17xp85q,https://www.reddit.com/r/MachineLearning/comments/17xp85q/n_openai_announces_leadership_transition_fires/,199,1700255569.0,"EDIT: Greg Brockman has quit as well: https://x.com/gdb/status/1725667410387378559?s=46&t=1GtNUIU6ETMu4OV8_0O5eA

Source: https://openai.com/blog/openai-announces-leadership-transition

Today, it was announced that Sam Altman will no longer be CEO or affiliated with OpenAI due to a lack of “candidness” with the board. This is extremely unexpected as Sam Altman is arguably the most recognizable face of state of the art AI (of course, wouldn’t be possible without great team at OpenAI). Lots of speculation is in the air, but there clearly must have been some good reason to make such a drastic decision.

This may or may not materially affect ML research, but it is plausible that the lack of “candidness” is related to copyright data, or usage of data sources that could land OpenAI in hot water with regulatory scrutiny. Recent lawsuits (https://www.reuters.com/legal/litigation/writers-suing-openai-fire-back-companys-copyright-defense-2023-09-28/) have raised questions about both the morality and legality of how OpenAI and other research groups train LLMs.

Of course we may never know the true reasons behind this action, but what does this mean for the future of AI?"
205,machinelearning,openai,relevance,2023-04-05 19:44:09,"[D] ""Our Approach to AI Safety"" by OpenAI",mckirkus,False,0.88,298,12cvkvn,https://www.reddit.com/r/MachineLearning/comments/12cvkvn/d_our_approach_to_ai_safety_by_openai/,297,1680723849.0,"It seems OpenAI are steering the conversation away from the existential threat narrative and into things like accuracy, decency, privacy, economic risk, etc.

To the extent that they do buy the existential risk argument, they don't seem concerned much about GPT-4 making a leap into something dangerous, even if it's at the heart of autonomous agents that are currently emerging.  

>""Despite extensive research and testing, we cannot predict all of the [beneficial ways people will use our technology](https://openai.com/customer-stories), nor all the ways people will abuse it. That’s why we believe that learning from real-world use is a critical component of creating and releasing increasingly safe AI systems over time. ""

Article headers:

* Building increasingly safe AI systems
* Learning from real-world use to improve safeguards
* Protecting children
* Respecting privacy
* Improving factual accuracy

&#x200B;

[https://openai.com/blog/our-approach-to-ai-safety](https://openai.com/blog/our-approach-to-ai-safety)"
206,machinelearning,openai,relevance,2023-11-23 00:14:50,[D] Exclusive: Sam Altman's ouster at OpenAI was precipitated by letter to board about AI breakthrough,blabboy,False,0.83,378,181o1q4,https://www.reddit.com/r/MachineLearning/comments/181o1q4/d_exclusive_sam_altmans_ouster_at_openai_was/,180,1700698490.0,"According to one of the sources, long-time executive Mira Murati told employees on Wednesday that a letter about the AI breakthrough called Q* (pronounced Q-Star), precipitated the board's actions.

The maker of ChatGPT had made progress on Q*, which some internally believe could be a breakthrough in the startup's search for superintelligence, also known as artificial general intelligence (AGI), one of the people told Reuters. OpenAI defines AGI as AI systems that are smarter than humans.

https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/"
207,machinelearning,openai,relevance,2023-03-28 05:57:03,[N] OpenAI may have benchmarked GPT-4’s coding ability on it’s own training data,Balance-,False,0.97,999,124eyso,https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/,135,1679983023.0,"[GPT-4 and professional benchmarks: the wrong answer to the wrong question](https://aisnakeoil.substack.com/p/gpt-4-and-professional-benchmarks)

*OpenAI may have tested on the training data. Besides, human benchmarks are meaningless for bots.*

 **Problem 1: training data contamination**

To benchmark GPT-4’s coding ability, OpenAI evaluated it on problems from Codeforces, a website that hosts coding competitions. Surprisingly, Horace He pointed out that GPT-4 solved 10/10 pre-2021 problems and 0/10 recent problems in the easy category. The training data cutoff for GPT-4 is September 2021. This strongly suggests that the model is able to memorize solutions from its training set — or at least partly memorize them, enough that it can fill in what it can’t recall.

As further evidence for this hypothesis, we tested it on Codeforces problems from different times in 2021. We found that it could regularly solve problems in the easy category before September 5, but none of the problems after September 12.

In fact, we can definitively show that it has memorized problems in its training set: when prompted with the title of a Codeforces problem, GPT-4 includes a link to the exact contest where the problem appears (and the round number is almost correct: it is off by one). Note that GPT-4 cannot access the Internet, so memorization is the only explanation."
208,machinelearning,openai,relevance,2023-12-28 22:53:08,New York Times sues OpenAI and Microsoft for copyright infringement [N],we_are_mammals,False,0.9,175,18t73v1,https://www.reddit.com/r/MachineLearning/comments/18t73v1/new_york_times_sues_openai_and_microsoft_for/,92,1703803988.0,"https://www.theguardian.com/media/2023/dec/27/new-york-times-openai-microsoft-lawsuit

The lawsuit alleges: *""Powered by LLMs containing copies of Times content, Defendants’ GenAI tools can generate output that recites Times content verbatim, closely summarizes it, and mimics its expressive style""*. The lawsuit seeks billions in damages and wants to see these chatbots destroyed.

---

I don't know if summaries and style mimicking fall under copyright law, but couldn't verbatim quoting be prevented? I proposed doing this a while ago in this subreddit: 

> Can't OpenAI simply check the output for sharing long substrings with the training data (perhaps probabilistically)?

You can simply take all training data substrings (of a fixed length, say 20 tokens) and put them into a hash table, a bloom filter, or a similar data structure. Then, when the LLMs are generating text, you can check to make sure the text does not contain any substrings that are in the data structure. This will prevent verbatim quotations from the NYT or other copyrighted material that are longer than 20 tokens (or whatever length you chose). Storing the data structure in memory may require distributing it across multiple machines, but I think OpenAI can easily afford it. You can further save memory by spacing the substrings, if memory is a concern."
209,machinelearning,openai,relevance,2019-03-22 02:36:38,[P] OpenAI's GPT-2-based Reddit Bot is Live!,Shevizzle,False,0.97,339,b3zlha,https://www.reddit.com/r/MachineLearning/comments/b3zlha/p_openais_gpt2based_reddit_bot_is_live/,990,1553222198.0,"**~~FINAL~~** **UPDATE: The bot is down until I have time to get it operational again. Will update this when it’s back online.**

&#x200B;

**Disclaimer** : This is not the full model. This is the smaller and less powerful version which OpenAI released publicly.

[Original post](https://www.reddit.com/r/MachineLearning/comments/b32lve/d_im_using_openais_gpt2_to_generate_text_give_me/)

Based on the popularity of my post from the other day, I decided to go ahead an build a full-fledged Reddit bot. So without further ado, please welcome:

# u/GPT-2_Bot

&#x200B;

If you want to use the bot, all you have to do is reply to any comment with the following command words:

# ""gpt-2 finish this""

Your reply can contain other stuff as well, i.e.

>""hey **gpt-2**, please **finish this** argument for me, will ya?""

&#x200B;

The bot will then look at **the comment you replied to** and generate its own response. It will tag you in the response so you know when it's done!

&#x200B;

Currently supported subreddits:

* r/funny
* r/AskReddit
* r/gaming
* r/pics
* r/science
* r/worldnews
* r/todayilearned
* r/movies
* r/videos
* r/ShowerThoughts
* r/MachineLearning
* r/test
* r/youtubehaiku
* r/thanosdidnothingwrong
* r/dankmemes

&#x200B;

The bot also scans r/all so ***theoretically*** it will see comments posted anywhere on Reddit. In practice, however, it only seems to catch about 1 in 5 of them.

&#x200B;

Enjoy! :) Feel free to PM me with feedback"
210,machinelearning,openai,relevance,2023-03-11 13:54:22,[Discussion] Compare OpenAI and SentenceTransformer Sentence Embeddings,Simusid,False,0.94,534,11okrni,https://i.redd.it/7muze2s684na1.png,58,1678542862.0,
211,machinelearning,openai,relevance,2023-01-20 10:41:04,[N] OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic,ChubChubkitty,False,0.83,524,10gtruu,https://www.reddit.com/r/MachineLearning/comments/10gtruu/n_openai_used_kenyan_workers_on_less_than_2_per/,246,1674211264.0,https://time.com/6247678/openai-chatgpt-kenya-workers/
212,machinelearning,openai,relevance,2023-05-07 14:12:18,[P] I made a dashboard to analyze OpenAI API usage,cryptotrendz,False,0.91,407,13aotyf,https://v.redd.it/w7ahlql0ccya1,73,1683468738.0,
213,machinelearning,openai,relevance,2023-05-04 16:13:30,"[D] Google ""We Have No Moat, And Neither Does OpenAI"": Leaked Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI",hardmaru,False,0.98,1171,137rxgw,https://www.semianalysis.com/p/google-we-have-no-moat-and-neither,206,1683216810.0,
214,machinelearning,openai,relevance,2023-10-20 14:12:12,[D] Is anyone else tired of “whatever OpenAI does is the best!” narrative?,mildlyphd,False,0.79,177,17cc8on,https://www.reddit.com/r/MachineLearning/comments/17cc8on/d_is_anyone_else_tired_of_whatever_openai_does_is/,107,1697811132.0,"The title says it all. I agree what they did is incredible and literally changed AI landscape in last couple of years. But I’m getting tired of everyone acting like OpenAI is the only one doing great research. The twit-fluencers praising even the slightest peep from them. I don’t understand this fanaticism in AI community. There are smart researchers doing smart things all over the world. But they don’t even get a fraction of appreciation they deserve. And the strangest thing of all, ChatGPT is used as oracle to evaluate models in research papers. Consistency models are extremely meh and if it did not come out of openAI, people would’ve forgotten them a long time ago!

Edit 1: I’m in grad school and that’s all a lot of students around me talk about/ chase. I want to work on a bit more fundamental problems, but I feel like I’m being left behind. 

Edit 2: This post is mostly a rant about academics obsessed with OpenAI research/products and LLMs. "
215,machinelearning,openai,relevance,2016-01-09 04:01:47,AMA: the OpenAI Research Team,IlyaSutskever,False,0.98,401,404r9m,https://www.reddit.com/r/MachineLearning/comments/404r9m/ama_the_openai_research_team/,285,1452312107.0,"The OpenAI research team will be answering your questions.

We are (our usernames are):  Andrej Karpathy (badmephisto), Durk Kingma (dpkingma), Greg Brockman (thegdb), Ilya Sutskever (IlyaSutskever), John Schulman (johnschulman), Vicki Cheung (vicki-openai), Wojciech Zaremba (wojzaremba).


Looking forward to your questions! "
216,machinelearning,openai,relevance,2023-11-22 06:38:48,"OpenAI: ""We have reached an agreement in principle for Sam to return to OpenAI as CEO"" [N]",we_are_mammals,False,0.92,285,1812w04,https://www.reddit.com/r/MachineLearning/comments/1812w04/openai_we_have_reached_an_agreement_in_principle/,129,1700635128.0,"OpenAI announcement:

""We have reached an agreement in principle for Sam to return to OpenAI as CEO with a new initial board of Bret Taylor (Chair), Larry Summers, and Adam D'Angelo.

We are collaborating to figure out the details. Thank you so much for your patience through this.""

https://twitter.com/OpenAI/status/1727205556136579362"
217,machinelearning,openai,relevance,2019-03-11 16:19:00,[N] OpenAI LP,SkiddyX,False,0.94,311,azvbmn,https://www.reddit.com/r/MachineLearning/comments/azvbmn/n_openai_lp/,150,1552321140.0,"""We’ve created OpenAI LP, a new “capped-profit” company that allows us to rapidly increase our investments in compute and talent while including checks and balances to actualize our mission.""

Sneaky.

https://openai.com/blog/openai-lp/"
218,machinelearning,openai,relevance,2019-02-15 13:04:39,[Discussion] OpenAI should now change their name to ClosedAI,SirLordDragon,False,0.92,646,aqwcyx,https://www.reddit.com/r/MachineLearning/comments/aqwcyx/discussion_openai_should_now_change_their_name_to/,223,1550235879.0,It's the only way to complete the hype wave.
219,machinelearning,openai,relevance,2020-01-30 17:11:51,[N] OpenAI Switches to PyTorch,SkiddyX,False,0.99,566,ew8oxq,https://www.reddit.com/r/MachineLearning/comments/ew8oxq/n_openai_switches_to_pytorch/,119,1580404311.0,"""We're standardizing OpenAI's deep learning framework on PyTorch to increase our research productivity at scale on GPUs (and have just released a PyTorch version of Spinning Up in Deep RL)""

https://openai.com/blog/openai-pytorch/"
220,machinelearning,openai,relevance,2023-03-30 14:18:50,[D] AI Policy Group CAIDP Asks FTC To Stop OpenAI From Launching New GPT Models,vadhavaniyafaijan,False,0.84,210,126oiey,https://www.reddit.com/r/MachineLearning/comments/126oiey/d_ai_policy_group_caidp_asks_ftc_to_stop_openai/,213,1680185930.0,"The Center for AI and Digital Policy (CAIDP), a tech ethics group, has asked the Federal Trade Commission to investigate OpenAI for violating consumer protection rules. CAIDP claims that OpenAI's AI text generation tools have been ""biased, deceptive, and a risk to public safety.""

CAIDP's complaint raises concerns about potential threats from OpenAI's GPT-4 generative text model, which was announced in mid-March. It warns of the potential for GPT-4 to produce malicious code and highly tailored propaganda and the risk that biased training data could result in baked-in stereotypes or unfair race and gender preferences in hiring. 

The complaint also mentions significant privacy failures with OpenAI's product interface, such as a recent bug that exposed OpenAI ChatGPT histories and possibly payment details of ChatGPT plus subscribers.

CAIDP seeks to hold OpenAI accountable for violating Section 5 of the FTC Act, which prohibits unfair and deceptive trade practices. The complaint claims that OpenAI knowingly released GPT-4 to the public for commercial use despite the risks, including potential bias and harmful behavior. 

[Source](https://www.theinsaneapp.com/2023/03/stop-openai-from-launching-gpt-5.html) | [Case](https://www.caidp.org/cases/openai/)| [PDF](https://www.caidp.org/app/download/8450269463/CAIDP-FTC-Complaint-OpenAI-GPT-033023.pdf)"
221,machinelearning,openai,relevance,2023-05-22 18:11:48,[D] Governance of SuperIntelligence - OpenAI,MysteryInc152,False,0.74,25,13oygjs,https://www.reddit.com/r/MachineLearning/comments/13oygjs/d_governance_of_superintelligence_openai/,104,1684779108.0,Blog - https://openai.com/blog/governance-of-superintelligence
222,machinelearning,openai,relevance,2021-05-26 17:31:34,[N] OpenAI announces OpenAI Startup Fund investing $100 million into AI startups,minimaxir,False,0.97,385,nlmlbg,https://www.reddit.com/r/MachineLearning/comments/nlmlbg/n_openai_announces_openai_startup_fund_investing/,39,1622050294.0,"https://openai.com/fund/
https://techcrunch.com/2021/05/26/openais-100m-startup-fund-will-make-big-early-bets-with-microsoft-as-partner/

It does not appear to be explicitly GPT-3 related (any type of AI is accepted), but hints very heavily toward favoring applications using it."
223,machinelearning,openai,relevance,2023-05-06 18:41:02,[R][P] I made an app for Instant Image/Text to 3D using ShapE from OpenAI,perception-eng,False,0.96,813,139yc73,https://i.redd.it/1j4h1oyda9ya1.gif,63,1683398462.0,
224,machinelearning,openai,relevance,2022-08-25 14:53:50,OpenAI Residency 2023 [R],rooney119,False,0.88,34,wxgcca,https://www.reddit.com/r/MachineLearning/comments/wxgcca/openai_residency_2023_r/,88,1661439230.0,Has anyone applied to the newly opened OpenAI residency program to begin in January 2023?
225,machinelearning,openai,relevance,2022-09-21 17:40:02,[N] OpenAI's Whisper released,SleekEagle,False,0.96,137,xkbk5b,https://www.reddit.com/r/MachineLearning/comments/xkbk5b/n_openais_whisper_released/,62,1663782002.0,"OpenAI just released it's newest ASR(/translation) model

[openai/whisper (github.com)](https://github.com/openai/whisper/)"
226,machinelearning,openai,relevance,2019-03-19 20:43:15,[D] I'm using OpenAI's GPT-2 to generate text. Give me prompts to try!,Shevizzle,False,0.96,331,b32lve,https://www.reddit.com/r/MachineLearning/comments/b32lve/d_im_using_openais_gpt2_to_generate_text_give_me/,781,1553028195.0,"Thank you all for participating! Based on the popularity of this post, I decided to create a full-blown reddit bot based on it. I won't be responding here anymore but check out the update for more info.

# UPDATE: [I made a GPT-2 reddit bot](https://www.reddit.com/r/MachineLearning/comments/b3zlha/p_openais_gpt2based_reddit_bot_is_live/)

&#x200B;

**Disclaimer** : This is not the full model. This is the smaller and less powerful version which OpenAI released publicly.

**EXAMPLE PROMPT:**

The NCAA unveiled the 68 college basketball teams that will be dancing this year. It wasn’t short on controversy, as snubbed teams aired out their grievances and blamed a confusing new metric.

**OUTPUT:**

The numbers came straight from the NCAA's website, where they're being used to calculate the number of national titles, national championship games, and even NCAA games played in a certain year.

""We want to make sure that the NCAA is inclusive,"" said NCAA President Dave Heidelman. ""We want to make sure that you can get what you want out of your program. And we want to make sure that the NCAA is focused on this problem right now and that the NCAA is fully invested.""

The list of opponents includes Ohio State, Florida State, Louisville, Kentucky, Mississippi State, Penn State, Oklahoma, Southern Cal, Virginia, and Wake Forest. It's not all about one one, though: They're all on pace to play an entire season. They have every national title, national championship, and tournament appearance in their DNA — as one might expect, they want to see it happen, but also want to build an identity, make their dream of watching that happen become the reality.

As good as any NCAA team is, they're also a long way off reaching the number one spot in the nation or even the top-ranked nation. The Big Ten also has some talented recruits from some in-state programs that may be considered the top two nationally. In fact, the national ranking of these schools is so high that a single conference ranking in 2016 will put the conference in the top-50 of the polls. Still, while Big Ten and SEC teams are likely to be on the map and competing for national titles, they're a bit underserved (and it's not as if they're all the same.)

So where does the NCAA stand on this?

According to ULM's John Covington, who runs its ""Unions, Colleges, and Universities"" page in conjunction with the National Conference, they're all going to have to make some moves:

Some may think this is just a joke. ""No, this is really about the league's future,"" said Dr. John H. Hester, president of UM's Athletic Department and president of the National Collegiate Athletic Association's Women's Academic Programs. ""I think the NCAA is a great place to start, because it's here to stay and if we're really strong and we can figure ourselves out, our future is going to be on the basketball court.""

**MODEL:**

[gpt-2 117M](https://github.com/openai/gpt-2)

**If you have an idea for a prompt, post it in the comments and I'll reply with the output if I deem it worthy.**"
227,machinelearning,openai,relevance,2023-04-03 17:47:06,[D] Is there currently anything comparable to the OpenAI API?,AltruisticDiamond915,False,0.9,176,12arwkf,https://www.reddit.com/r/MachineLearning/comments/12arwkf/d_is_there_currently_anything_comparable_to_the/,74,1680544026.0,"I am a designer and a small frontend developer. The OpenAI API makes it extremely easy for me to build AI functionality into apps, although I only have a very basic understanding of AI. Though of course I can't make any deep changes, I am even able to fine tune models and adapt them for my intended use. 

Now I was wondering if there is anything comparable on the market at the moment. I know of Meta's LLaMA, but you can only use it locally, which makes it harder to easily implement into applications. Also, you are not allowed to use it for commercial purposes. 

I haven't found much from Google or other tech companies. Does OpenAI have a monopoly here or are there alternatives worth mentioning that could be used?"
228,machinelearning,openai,relevance,2020-06-17 19:15:01,[R] OpenAI Image GPT,lfotofilter,False,0.96,260,hay15t,https://www.reddit.com/r/MachineLearning/comments/hay15t/r_openai_image_gpt/,99,1592421301.0,"Open AI just released a blog post about [Image GPT](https://openai.com/blog/image-gpt/). They apply the GPT-2 transformer-based model to pixel sequences (as opposed to word sequences).

This could actually be quite powerful in my view, because, as opposed to much of the current competition in self-supervised learning for images, Open AI are actually using a model of p(x) (of sorts) for downstream tasks. Recent successful methods like SimCLR rely heavily on augmentations, and mainly focus on learning features that are robust to these augmentations.

Slowly but surely, transformers are taking over the world."
229,machinelearning,openai,relevance,2018-08-06 17:08:04,[N] OpenAI Five Benchmark: Results,luiscosio,False,0.95,225,9533g8,https://blog.openai.com/openai-five-benchmark-results/,179,1533575284.0,
230,machinelearning,openai,relevance,2023-03-01 18:31:12,[D] OpenAI introduces ChatGPT and Whisper APIs (ChatGPT API is 1/10th the cost of GPT-3 API),minimaxir,False,0.97,581,11fbccz,https://www.reddit.com/r/MachineLearning/comments/11fbccz/d_openai_introduces_chatgpt_and_whisper_apis/,119,1677695472.0,"https://openai.com/blog/introducing-chatgpt-and-whisper-apis

> It is priced at $0.002 per 1k tokens, which is 10x cheaper than our existing GPT-3.5 models.

This is a massive, massive deal. For context, the reason GPT-3 apps took off over the past few months before ChatGPT went viral is because a) text-davinci-003 was released and was a significant performance increase and b) the cost was cut from $0.06/1k tokens to $0.02/1k tokens, which made consumer applications feasible without a large upfront cost.

A much better model and a 1/10th cost warps the economics completely to the point that it may be better than in-house finetuned LLMs.

I have no idea how OpenAI can make money on this. This has to be a loss-leader to lock out competitors before they even get off the ground."
231,machinelearning,openai,relevance,2022-06-21 15:17:08,"[N] [D] Openai, who runs DALLE-2 alleged threatened creator of DALLE-Mini",DigThatData,False,0.86,263,vhfp1t,https://www.reddit.com/r/MachineLearning/comments/vhfp1t/n_d_openai_who_runs_dalle2_alleged_threatened/,120,1655824628.0,"Trying to cross-post what I think is a discussion that is relevant to this community. This is my third attempt, I hope I'm doing it correctly this time: 

https://www.reddit.com/r/dalle2/comments/vgtgdc/openai_who_runs_dalle2_alleged_threatened_creator/

EDIT: here are the original pre-prints for added context:

* DALL-E: [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092) - The only place the term ""DALL-E"" appears is the URL to the github repo.
* Dall-E 2: [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125) - They consistently refer to the first paper as ""DALL-E"", but refer to the work being described in the new paper as ""unCLIP"" and are careful to only use 'DALL-E 2' in the context of a product description, e.g. ""DALL·E 2 Preview platform (the first deployment of an unCLIP model)"""
232,machinelearning,openai,relevance,2023-11-23 05:06:52,[D] What's this new Q* algorithm in relation to OpenAI breakthrough ?,to4life4,False,0.43,0,181tidm,https://www.reddit.com/r/MachineLearning/comments/181tidm/d_whats_this_new_q_algorithm_in_relation_to/,66,1700716012.0,"I heard about this on Twitter from some people in the field, in relation to OpenAI's new breakthrough.

Is there a summary paper, like the 'All you need is attention' paper, that goes over this? 

Also, how specifically does this relate to and/or add on to Large Language Models? 

Cheers"
233,machinelearning,openai,relevance,2023-09-21 15:01:28,[N] OpenAI's new language model gpt-3.5-turbo-instruct can defeat chess engine Fairy-Stockfish 14 at level 5,Wiskkey,False,0.92,115,16oi6fb,https://www.reddit.com/r/MachineLearning/comments/16oi6fb/n_openais_new_language_model_gpt35turboinstruct/,178,1695308488.0,"[This Twitter thread](https://twitter.com/GrantSlatton/status/1703913578036904431) ([Nitter alternative](https://nitter.net/GrantSlatton/status/1703913578036904431) for those who aren't logged into Twitter and want to see the full thread) claims that [OpenAI's new language model gpt-3.5-turbo-instruct](https://analyticsindiamag.com/openai-releases-gpt-3-5-turbo-instruct/) can ""readily"" beat Lichess Stockfish level 4 ([Lichess Stockfish level and its rating](https://lichess.org/@/MagoGG/blog/stockfish-level-and-its-rating/CvL5k0jL)) and has a chess rating of ""around 1800 Elo."" [This tweet](https://twitter.com/nabeelqu/status/1703961405999759638) shows the style of prompts that are being used to get these results with the new language model.

I used website parrotchess\[dot\]com (discovered [here](https://twitter.com/OwariDa/status/1704179448013070560)) to play multiple games of chess purportedly pitting this new language model vs. various levels at website Lichess, which supposedly uses Fairy-Stockfish 14 according to the Lichess user interface. My current results for all completed games: The language model is 5-0 vs. Fairy-Stockfish 14 level 5 ([game 1](https://lichess.org/eGSWJtNq), [game 2](https://lichess.org/pN7K9bdS), [game 3](https://lichess.org/aK4jQvdo), [game 4](https://lichess.org/S9SGg8YI), [game 5](https://lichess.org/OqzdkDhE)), and 2-5 vs. Fairy-Stockfish 14 level 6 ([game 1](https://lichess.org/zP68C6H4), [game 2](https://lichess.org/4XKUIDh1), [game 3](https://lichess.org/1zTasRRp), [game 4](https://lichess.org/lH1EMqJQ), [game 5](https://lichess.org/mdFlTbMn), [game 6](https://lichess.org/HqmELNhw), [game 7](https://lichess.org/inWVs05Q)). Not included in the tally are games that I had to abort because the parrotchess user interface stalled (5 instances), because I accidentally copied a move incorrectly in the parrotchess user interface (numerous instances), or because the parrotchess user interface doesn't allow the promotion of a pawn to anything other than queen (1 instance). **Update: There could have been up to 5 additional losses - the number of times the parrotchess user interface stalled - that would have been recorded in this tally if** [this language model resignation bug](https://twitter.com/OwariDa/status/1705894692603269503) **hadn't been present. Also, the quality of play of some online chess bots can perhaps vary depending on the speed of the user's hardware.**

The following is a screenshot from parrotchess showing the end state of the first game vs. Fairy-Stockfish 14 level 5:

https://preview.redd.it/4ahi32xgjmpb1.jpg?width=432&format=pjpg&auto=webp&s=7fbb68371ca4257bed15ab2828fab58047f194a4

The game results in this paragraph are from using parrotchess after the forementioned resignation bug was fixed. The language model is 0-1 vs. Fairy-Stockfish level 7 ([game 1](https://lichess.org/Se3t7syX)), and 0-1 vs. Fairy-Stockfish 14 level 8 ([game 1](https://lichess.org/j3W2OwrP)).

There is [one known scenario](https://twitter.com/OwariDa/status/1706823943305167077) ([Nitter alternative](https://nitter.net/OwariDa/status/1706823943305167077)) in which the new language model purportedly generated an illegal move using language model sampling temperature of 0. Previous purported illegal moves that the parrotchess developer examined [turned out](https://twitter.com/OwariDa/status/1706765203130515642) ([Nitter alternative](https://nitter.net/OwariDa/status/1706765203130515642)) to be due to parrotchess bugs.

There are several other ways to play chess against the new language model if you have access to the OpenAI API. The first way is to use the OpenAI Playground as shown in [this video](https://www.youtube.com/watch?v=CReHXhmMprg). The second way is chess web app gptchess\[dot\]vercel\[dot\]app (discovered in [this Twitter thread](https://twitter.com/willdepue/status/1703974001717154191) / [Nitter thread](https://nitter.net/willdepue/status/1703974001717154191)). Third, another person modified that chess web app to additionally allow various levels of the Stockfish chess engine to autoplay, resulting in chess web app chessgpt-stockfish\[dot\]vercel\[dot\]app (discovered in [this tweet](https://twitter.com/paul_cal/status/1704466755110793455)).

Results from other people:

a) Results from hundreds of games in blog post [Debunking the Chessboard: Confronting GPTs Against Chess Engines to Estimate Elo Ratings and Assess Legal Move Abilities](https://blog.mathieuacher.com/GPTsChessEloRatingLegalMoves/).

b) Results from 150 games: [GPT-3.5-instruct beats GPT-4 at chess and is a \~1800 ELO chess player. Results of 150 games of GPT-3.5 vs stockfish and 30 of GPT-3.5 vs GPT-4](https://www.reddit.com/r/MachineLearning/comments/16q81fh/d_gpt35instruct_beats_gpt4_at_chess_and_is_a_1800/). [Post #2](https://www.reddit.com/r/chess/comments/16q8a3b/new_openai_model_gpt35instruct_is_a_1800_elo/). The developer later noted that due to bugs the legal move rate [was](https://twitter.com/a_karvonen/status/1706057268305809632) actually above 99.9%. It should also be noted that these results [didn't use](https://www.reddit.com/r/chess/comments/16q8a3b/comment/k1wgg0j/) a language model sampling temperature of 0, which I believe could have induced illegal moves.

c) Chess bot [gpt35-turbo-instruct](https://lichess.org/@/gpt35-turbo-instruct/all) at website Lichess.

d) Chess bot [konaz](https://lichess.org/@/konaz/all) at website Lichess.

From blog post [Playing chess with large language models](https://nicholas.carlini.com/writing/2023/chess-llm.html):

>Computers have been better than humans at chess for at least the last 25 years. And for the past five years, deep learning models have been better than the best humans. But until this week, in order to be good at chess, a machine learning model had to be explicitly designed to play games: it had to be told explicitly that there was an 8x8 board, that there were different pieces, how each of them moved, and what the goal of the game was. Then it had to be trained with reinforcement learning agaist itself. And then it would win.  
>  
>This all changed on Monday, when OpenAI released GPT-3.5-turbo-instruct, an instruction-tuned language model that was designed to just write English text, but that people on the internet quickly discovered can play chess at, roughly, the level of skilled human players.

Post [Chess as a case study in hidden capabilities in ChatGPT](https://www.lesswrong.com/posts/F6vH6fr8ngo7csDdf/chess-as-a-case-study-in-hidden-capabilities-in-chatgpt) from last month covers a different prompting style used for the older chat-based GPT 3.5 Turbo language model. If I recall correctly from my tests with ChatGPT-3.5, using that prompt style with the older language model can defeat Stockfish level 2 at Lichess, but I haven't been successful in using it to beat Stockfish level 3. In my tests, both the quality of play and frequency of illegal attempted moves seems to be better with the new prompt style with the new language model compared to the older prompt style with the older language model.

Related article: [Large Language Model: world models or surface statistics?](https://thegradient.pub/othello/)

P.S. Since some people claim that language model gpt-3.5-turbo-instruct is always playing moves memorized from the training dataset, I searched for data on the uniqueness of chess positions. From [this video](https://youtu.be/DpXy041BIlA?t=2225), we see that for a certain game dataset there were 763,331,945 chess positions encountered in an unknown number of games without removing duplicate chess positions, 597,725,848 different chess positions reached, and 582,337,984 different chess positions that were reached only once. Therefore, for that game dataset the probability that a chess position in a game was reached only once is 582337984 / 763331945 = 76.3%. For the larger dataset [cited](https://youtu.be/DpXy041BIlA?t=2187) in that video, there are approximately (506,000,000 - 200,000) games in the dataset (per [this paper](http://tom7.org/chess/survival.pdf)), and 21,553,382,902 different game positions encountered. Each game in the larger dataset added a mean of approximately 21,553,382,902 / (506,000,000 - 200,000) = 42.6 different chess positions to the dataset. For [this different dataset](https://lichess.org/blog/Vs0xMTAAAD4We4Ey/opening-explorer) of \~12 million games, \~390 million different chess positions were encountered. Each game in this different dataset added a mean of approximately (390 million / 12 million) = 32.5 different chess positions to the dataset. From the aforementioned numbers, we can conclude that a strategy of playing only moves memorized from a game dataset would fare poorly because there are not rarely new chess games that have chess positions that are not present in the game dataset."
234,machinelearning,openai,relevance,2023-09-21 00:03:05,[N] OpenAI Announced DALL-E 3: Art Generator Powered by ChatGPT,RepresentativeCod613,False,0.87,111,16o0tfl,https://www.reddit.com/r/MachineLearning/comments/16o0tfl/n_openai_announced_dalle_3_art_generator_powered/,52,1695254585.0,"For those who missed it: **DALL-E 3 was announced today by OpenAI,** and here are some interesting things:

**No need to be a prompt engineering grand master** \- DALL-E 3 enables you to use the ChatGPT conversational interface to improve the images you generate. This means that if you didn't like what it produced, you can simply talk with ChatGPT and ask for the changes you'd like to make. This removes the complexity associated with prompt engineering, which requires you to iterate over the prompt.

**Majure improvement in the quality of products compared to DALL-E 2.** This is a very vague statement provided by OpenAI, which is also hard to measure, but personally, they haven't failed me so far, so I'm really excited to see the results.

[DALL-E 2 Vs. DALL-E 3, image by OpenAI](https://preview.redd.it/0l5nfflw1ipb1.png?width=1250&format=png&auto=webp&s=130697e7bb1f01e7cbda2d8afff8564f66e3103d)

From October, **DALL-E 3 will be available through ChatGPT and API** for those with the Plus or Enterprise version.

And there are many more news! 🤗 I've gathered all the information in this blog 👉 [https://dagshub.com/blog/dall-e-3/](https://dagshub.com/blog/dall-e-3/)  


Source: [https://openai.com/dall-e-3](https://openai.com/dall-e-3)"
235,machinelearning,openai,relevance,2020-06-11 15:14:43,[N] OpenAI API,jboyml,False,0.97,318,h1179l,https://www.reddit.com/r/MachineLearning/comments/h1179l/n_openai_api/,62,1591888483.0,"[https://beta.openai.com/](https://beta.openai.com/)

OpenAI releases a commercial API for NLP tasks including semantic search, summarization, sentiment analysis, content generation, translation, and more."
236,machinelearning,openai,relevance,2023-11-10 09:19:50,[P] I build a therapy chatbot (not another wrapper around openai API),tsundoku-16,False,0.69,61,17s0als,https://www.reddit.com/r/MachineLearning/comments/17s0als/p_i_build_a_therapy_chatbot_not_another_wrapper/,38,1699607990.0,"I made LLM companion by fine-tuning Llama-2-13B using custom written and collected sessions in Cognitive Behavioral Therapy (CBT). Data contains conversations that illustrate how to employ CBT techniques, including cognitive restructuring and mindfulness.

It is mostly focused on asking insightful questions. Note: It is not production ready product. I am testing it and gathering feedback. Use it at your own risk. It is not meant to be replacement for professional help.


You can access it here: [https://poe.com/LunaChat](https://poe.com/LunaChat)

Sorry that it is on Poe but this way it was much faster than making my own mobile friendly website.

Since it's LLM,  it is prone to hallucinate, generate responses that might be perceived as rude, and give insensitive comments or advice. Please use it with caution."
237,machinelearning,openai,relevance,2017-08-12 00:10:03,[N] OpenAI bot beat best Dota 2 players in 1v1 at The International 2017,crouching_dragon_420,False,0.96,565,6t58ks,https://blog.openai.com/dota-2/,252,1502496603.0,
238,machinelearning,openai,relevance,2023-12-28 12:54:58,[R] Open source LLMs are far from OpenAI for code editing,ellev3n11,False,0.88,90,18st9wa,https://www.reddit.com/r/MachineLearning/comments/18st9wa/r_open_source_llms_are_far_from_openai_for_code/,24,1703768098.0,"Paper: [https://arxiv.org/abs/2312.12450](https://arxiv.org/abs/2312.12450)

Title: Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions

Code repository: [https://github.com/nuprl/CanItEdit](https://github.com/nuprl/CanItEdit)

Abstract:

>A significant amount of research is focused on developing and evaluating large language models for a variety of code synthesis tasks. These include synthesizing code from natural language instructions, synthesizing tests from code, and synthesizing explanations of code. In contrast, the behavior of instructional code editing with LLMs is understudied. These are tasks in which the model is instructed to update a block of code provided in a prompt. The editing instruction may ask for a feature to added or removed, describe a bug and ask for a fix, ask for a different kind of solution, or many other common code editing tasks. We introduce a carefully crafted benchmark of code editing tasks and use it evaluate several cutting edge LLMs. Our evaluation exposes a significant gap between the capabilities of state-of-the-art open and closed models. For example, even GPT-3.5-Turbo is 8.8% better than the best open model at editing code. We also introduce a new, carefully curated, permissively licensed training set of code edits coupled with natural language instructions. Using this training set, we show that we can fine-tune open Code LLMs to significantly improve their code editing capabilities.

Discussion:

I'm sharing this paper to start a discussion. Disclaimer: this paper comes from our research group, but not trying to do self-promotion here. We are seeing that open source Code LLMs are slowly getting closer and closer to GPT-4 performance when evaluated on program synthesis and surpassing GPT-3.5-turbo (see DeepSeek Coder: [https://github.com/deepseek-ai/DeepSeek-Coder](https://github.com/deepseek-ai/DeepSeek-Coder)) when using common benchmarks, such as HumanEval, MBPP, and \*new\* LeetCode problems (this is to minimize contamination).

However, this isn't the modality you may want. Often, the need is to modify a section of code with accompanying natural language instructions (for example, Cursor IDE has shifted away from the GitHub Copilot style to focus solely on code editing: [https://cursor.sh/features](https://cursor.sh/features)). Also, simple code generation, achievable by models trained on code editing, might be considered a subset of code editing, by prompting the model with a blank before window.

In our various research projects, we've seen Code LLMs struggle with code editing. So we did the obvious thing, we examined how these models perform in this specific task. Surprisingly, models excelling in simple synthesis fall short in code editing compared to even just GPT-3.5-turbo.

Why is this the case? While some suggest data contamination, I doubt that's the primary factor, given these models' effectiveness on fresh and unseen benchmarks. Could it be that OpenAI dedicated a specific data subset for tasks like code or language editing (model then generalized to code)?

UPDATE:

After receiving criticism for not including models larger than 33b in our evaluations, I decided to eval Tulu 2 DPO 70b, which is reportedly the state-of-the-art 70b instruct-tuned LLM according to the Chatbot Arena Leaderboard (see: [Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)). I also evaluated Mixtral Instruct 0.1.

As I expected, both models didn't perform impressively, likely due to insufficient training on code. It's reasonable to assume that a 70b model specifically trained on code would yield better results.  Tulu's performance is slightly inferior to CodeLlama-33b-chat and not on par with DeepSeek Coder, and far from GPT-3.5-Turbo.

&#x200B;

|Model|Descriptive Pass@1 (ExcessCode)|Lazy Pass@1 (ExcessCode)|
|:-|:-|:-|
|Tulu-2-DPO-70b|33.26 (1.41)|26.42 (1.58)|
|Mixtral-8x7B-Instruct-v0.1|25.0 (1.0)|28.14 (0.26)|

&#x200B;"
239,machinelearning,openai,relevance,2018-02-17 12:45:30,[P] Landing the Falcon booster with Reinforcement Learning in OpenAI,EmbersArc,False,0.95,1291,7y6g79,https://gfycat.com/CoarseEmbellishedIsopod,55,1518871530.0,
240,machinelearning,openai,relevance,2023-12-11 15:07:28,[D] Backup if openai fails,New_Detective_1363,False,0.53,1,18fwme5,https://www.reddit.com/r/MachineLearning/comments/18fwme5/d_backup_if_openai_fails/,8,1702307248.0,Hi ! We are implementing a bot using chatgpt for my company but we have some concerns about openai downtime failures. Do you have some backup solutions that could be implemented in prod in case of failures (switch of models)? Knowing that the model is kind of finetuned because we are doing information retriavial using embeddings (created with gpt 3.5)
241,machinelearning,openai,relevance,2021-06-01 17:40:23,"[R] Chinese AI lab challenges Google, OpenAI with a model of 1.75 trillion parameters",liqui_date_me,False,0.89,364,npzqks,https://www.reddit.com/r/MachineLearning/comments/npzqks/r_chinese_ai_lab_challenges_google_openai_with_a/,167,1622569223.0,"Link here: https://en.pingwest.com/a/8693

TL;DR The Beijing Academy of Artificial Intelligence, styled as BAAI and known in Chinese as 北京智源人工智能研究院, launched the latest version of Wudao 悟道, a pre-trained deep learning model that the lab dubbed as “China’s first,” and “the world’s largest ever,” with a whopping 1.75 trillion parameters.

And the corresponding twitter thread: https://twitter.com/DavidSHolz/status/1399775371323580417

What's interesting here is BAAI is funded in part by the China’s Ministry of Science and Technology, which is China's equivalent of the NSF. The equivalent of this in the US would be for the NSF allocating billions of dollars a year *only to train models*."
242,machinelearning,openai,relevance,2022-12-24 14:58:19,[R][P] I made an app for Instant Image/Text to 3D using PointE from OpenAI,perception-eng,False,0.97,765,zubg2u,https://i.redd.it/ox6urwwa1v7a1.gif,42,1671893899.0,
243,machinelearning,openai,relevance,2023-06-25 15:40:42,[N] How OpenAI's unique equity compensation works,That_Violinist_18,False,0.86,82,14ipc5k,https://www.reddit.com/r/MachineLearning/comments/14ipc5k/n_how_openais_unique_equity_compensation_works/,28,1687707642.0,">In 2019, OpenAI changed from a nonprofit to a ‘[capped profit](https://openai.com/blog/openai-lp)’ model in an effort to raise capital while still serving their mission.  
Per their blog post, “The fundamental idea of OpenAI LP is that investors and employees can get a capped return if we succeed at our mission, which allows us to raise investment capital and attract employees with startup-like equity. But any returns beyond that amount—and if we are successful, we expect to generate orders of magnitude more value than we’d owe to people who invest in or work at OpenAI LP—are owned by the original OpenAI Nonprofit entity.”  
How does an OpenAI offer work?  
There are two components to OpenAI’s offer: the base salary and the ‘equity,’ which they call ‘Profit Participation Units’ or ‘PPUs.’ The base salary is standard and self-explanatory. It’s the cash paycheck you get every two weeks.  
The profit participation units are where it starts to get confusing, as companies sometimes use profit participation or profit interest grants differently.  
Here’s a recent [OpenAI offer](https://www.levels.fyi/offer/6ab65752-a3f0-48ad-afc3-72ccb368d6ed) (see [all OpenAI offers here](https://www.levels.fyi/companies/openai/salaries/software-engineer)), where you can see a base salary of $300k/year with a $2M PPU grant that vests evenly over 4 years, bringing the yearly total compensation to $800k.

[https://www.levels.fyi/blog/openai-compensation.html](https://www.levels.fyi/blog/openai-compensation.html)"
244,machinelearning,openai,relevance,2020-04-30 17:00:10,"[R] OpenAI opensources Jukebox, a neural net that generates music",gohu_cd,False,0.97,441,gazkh7,https://www.reddit.com/r/MachineLearning/comments/gazkh7/r_openai_opensources_jukebox_a_neural_net_that/,85,1588266010.0,"Provided with genre, artist, and lyrics as input, Jukebox outputs a new music sample produced from scratch.

[https://openai.com/blog/jukebox/](https://openai.com/blog/jukebox/)

[https://jukebox.openai.com](https://jukebox.openai.com/)

The model behind this tool is VQ-VAE."
245,machinelearning,openai,relevance,2021-07-27 18:11:28,[N] OpenAI Gym is now actively maintained again (by me)! Here's my plan,jkterry1,False,0.99,782,oss2e3,https://www.reddit.com/r/MachineLearning/comments/oss2e3/n_openai_gym_is_now_actively_maintained_again_by/,47,1627409488.0,"So OpenAI made me a maintainer of Gym. This means that all the installation issues will be fixed, the now 5 year backlog of PRs will be resolved, and in general Gym will now be reasonably maintained. I posted my manifesto for future maintenance here: [https://github.com/openai/gym/issues/2259](https://github.com/openai/gym/issues/2259)  


Edit: I've been getting a bunch of messages about open source donations, so I created links:

[https://liberapay.com/jkterry](https://liberapay.com/jkterry)

[https://www.buymeacoffee.com/jkterry](https://www.buymeacoffee.com/jkterry)"
246,machinelearning,openai,relevance,2024-02-22 05:17:12,[P] Meeting transcription using OpenAI Whisper model,violet_bloom_87,False,0.67,1,1awychg,https://www.reddit.com/r/MachineLearning/comments/1awychg/p_meeting_transcription_using_openai_whisper_model/,1,1708579032.0, I would like to transcribe a meeting recording of say 1 hour duration using OpenAI Whisper model. Anyone done this ? Please share your experience. Thanks 
247,machinelearning,openai,relevance,2023-05-31 20:25:38,[R] New OpenAI article: Improving Mathematical Reasoning with Process Supervision,Jean-Porte,False,0.94,153,13wwzq9,https://www.reddit.com/r/MachineLearning/comments/13wwzq9/r_new_openai_article_improving_mathematical/,27,1685564738.0,[https://cdn.openai.com/improving-mathematical-reasoning-with-process-supervision/Lets\_Verify\_Step\_by\_Step.pdf](https://cdn.openai.com/improving-mathematical-reasoning-with-process-supervision/Lets_Verify_Step_by_Step.pdf)
248,machinelearning,openai,relevance,2022-09-22 17:27:39,[D] Some OpenAI Whisper benchmarks for runtime and cost,SleekEagle,False,0.96,125,xl7mfy,https://www.reddit.com/r/MachineLearning/comments/xl7mfy/d_some_openai_whisper_benchmarks_for_runtime_and/,61,1663867659.0,"Hey guys! I ran a [few benchmarks](http://www.assemblyai.com/blog/how-to-run-openais-whisper-speech-recognition-model/#openai-whisper-analysis) on Whisper's runtime and cost-to-run on GCP, so just dropping it here in case it's valuable to anyone!"
249,machinelearning,openai,relevance,2024-02-04 00:09:27,[D] How to mimic openAI tools/functions,mrg3_2013,False,0.56,1,1aiabgk,https://www.reddit.com/r/MachineLearning/comments/1aiabgk/d_how_to_mimic_openai_toolsfunctions/,3,1707005367.0,"I'm interested in exploring ways to replicate OpenAI's functionalities with alternative GPT models for a specific scenario. Imagine having a dictionary containing various functions along with their specifications. When a user poses a question, the system should be capable of identifying and sequencing these functions appropriately to construct a coherent response. Additionally, I'm curious about the foundational steps required to develop such a system from the ground up. Specifically, starting with a function dictionary and their specs, how could one design a mechanism for generating code responses? Appreciate any guidance/inputs as I have never trained any model on my own. OpenAI's response was not helpful for this question :)"
250,machinelearning,openai,relevance,2021-01-18 09:08:06,[P] The Big Sleep: Text-to-image generation using BigGAN and OpenAI's CLIP via a Google Colab notebook from Twitter user Adverb,Wiskkey,False,0.99,623,kzr4mg,https://www.reddit.com/r/MachineLearning/comments/kzr4mg/p_the_big_sleep_texttoimage_generation_using/,259,1610960886.0,"From [https://twitter.com/advadnoun/status/1351038053033406468](https://twitter.com/advadnoun/status/1351038053033406468):

>The Big Sleep  
>  
>Here's the notebook for generating images by using CLIP to guide BigGAN.  
>  
>It's very much unstable and a prototype, but it's also a fair place to start. I'll likely update it as time goes on.  
>  
>[colab.research.google.com/drive/1NCceX2mbiKOSlAd\_o7IU7nA9UskKN5WR?usp=sharing](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing)

I am not the developer of The Big Sleep. [This](https://twitter.com/advadnoun/) is the developer's Twitter account; [this](https://www.reddit.com/user/advadnoun) is the developer's Reddit account.

**Steps to follow to generate the first image in a given Google Colab session**:

1. Optionally, if this is your first time using Google Colab, view this [Colab introduction](https://colab.research.google.com/notebooks/intro.ipynb) and/or this [Colab FAQ](https://research.google.com/colaboratory/faq.html).
2. Click [this link](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing).
3. Sign into your Google account if you're not already signed in. Click the ""S"" button in the upper right to do this. Note: Being signed into a Google account has privacy ramifications, such as your Google search history being recorded in your Google account.
4. In the Table of Contents, click ""Parameters"".
5. Find the line that reads ""tx = clip.tokenize('''a cityscape in the style of Van Gogh''')"" and change the text inside of the single quote marks to your desired text; example: ""tx = clip.tokenize('''a photo of New York City''')"". The developer recommends that you keep the three single quote marks on both ends of your desired text so that mult-line text can be used  An alternative is to remove two of the single quotes on each end of your desired text; example: ""tx = clip.tokenize('a photo of New York City')"".
6. In the Table of Contents, click ""Restart the kernel..."".
7. Position the pointer over the first cell in the notebook, which starts with text ""import subprocess"". Click the play button (the triangle) to run the cell. Wait until the cell completes execution.
8. Click menu item ""Runtime->Restart and run all"".
9. In the Table of Contents, click ""Diagnostics"". The output appears near the end of the Train cell that immediately precedes the Diagnostics cell, so scroll up a bit. Every few minutes (or perhaps 10 minutes if Google assigned you relatively slow hardware for this session), a new image will appear in the Train cell that is a refinement of the previous image. This process can go on for as long as you want until Google ends your Google Colab session, which is a total of [up to 12 hours](https://research.google.com/colaboratory/faq.html) for the free version of Google Colab.

**Steps to follow if you want to start a different run using the same Google Colab session:**

1. Click menu item ""Runtime->Interrupt execution"".
2. Save any images that you want to keep by right-clicking on them and using the appropriate context menu command.
3. Optionally, change the desired text. Different runs using the same desired text almost always results in different outputs.
4. Click menu item ""Runtime->Restart and run all"".

**Steps to follow when you're done with your Google Colab session**:

1. Click menu item ""Runtime->Manage sessions"". Click ""Terminate"" to end the session.
2. Optionally, log out of your Google account due to the privacy ramifications of being logged into a Google account.

The first output image in the Train cell (using the notebook's default of seeing every 100th image generated) usually is a very poor match to the desired text, but the second output image often is a decent match to the desired text. To change the default of seeing every 100th image generated, change the number 100 in line ""if itt % 100 == 0:"" in the Train cell to the desired number. **For free-tier Google Colab users, I recommend changing 100 to a small integer such as 5.**

Tips for the text descriptions that you supply:

1. In Section 3.1.4 of OpenAI's [CLIP paper](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf) (pdf), the authors recommend using a text description of the form ""A photo of a {label}."" or ""A photo of a {label}, a type of {type}."" for images that are photographs.
2. A Reddit user gives [these tips](https://www.reddit.com/r/MediaSynthesis/comments/l2hmqn/this_aint_it_chief/gk8g8e9/).
3. The Big Sleep should generate [these 1,000 types of things](https://www.reddit.com/r/MediaSynthesis/comments/l7hbix/tip_for_users_of_the_big_sleep_it_should_on/) better on average than other types of things.

[Here](https://www.digitaltrends.com/news/big-sleep-ai-image-generator/) is an article containing a high-level description of how The Big Sleep works. The Big Sleep uses a modified version of [BigGAN](https://aiweirdness.com/post/182322518157/welcome-to-latent-space) as its image generator component. The Big Sleep uses the ViT-B/32 [CLIP](https://openai.com/blog/clip/) model to rate how well a given image matches your desired text. The best CLIP model according to the CLIP paper authors is the (as of this writing) unreleased ViT-L/14-336px model; see Table 10 on page 40 of the [CLIP paper (pdf)](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf) for a comparison.

There are [many other sites/programs/projects](https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais/) that use CLIP to steer image/video creation to match a text description.

Some relevant subreddits:

1. [r/bigsleep](https://www.reddit.com/r/bigsleep/) (subreddit for images/videos generated from text-to-image machine learning algorithms).
2. [r/deepdream](https://www.reddit.com/r/deepdream/) (subreddit for images/videos generated from machine learning algorithms).
3. [r/mediasynthesis](https://www.reddit.com/r/mediasynthesis/) (subreddit for media generation/manipulation techniques that use artificial intelligence; this subreddit shouldn't be used to post images/videos unless new techniques are demonstrated, or the images/videos are of high quality relative to other posts).

Example using text 'a black cat sleeping on top of a red clock':

https://preview.redd.it/7xq58v7022c61.png?width=512&format=png&auto=webp&s=a229ae9add555cd1caba31c42b60d907ffe67773

Example using text 'the word ''hot'' covered in ice':

https://preview.redd.it/6kxdp8u3k2c61.png?width=512&format=png&auto=webp&s=5bd078b0111575f5d88a1dc53b0aeb933f3b0da6

Example using text 'a monkey holding a green lightsaber':

https://preview.redd.it/rdsybsoaz2c61.png?width=512&format=png&auto=webp&s=2769d4c6c883c1c35ae0b1c629bebe9bc1d41393

Example using text 'The White House in Washington D.C. at night with green and red spotlights shining on it':

https://preview.redd.it/w4mg90xsf5c61.png?width=512&format=png&auto=webp&s=5f18318de2f77bcd8a86e71e87048fadd30383d1

Example using text '''A photo of the Golden Gate Bridge at night, illuminated by spotlights in a tribute to Prince''':

https://preview.redd.it/cn4ecuafhic61.png?width=512&format=png&auto=webp&s=397c838fdc49f13c5f17110b92c78b95bf0dcac0

Example using text '''a Rembrandt-style painting titled ""Robert Plant decides whether to take the stairway to heaven or the ladder to heaven""''':

https://preview.redd.it/h7rb3y6j5jc61.png?width=512&format=png&auto=webp&s=537bfe8210af185647b00e7585c948aa2c4e0ffb

Example using text '''A photo of the Empire State Building being shot at with the laser cannons of a TIE fighter.''':

https://preview.redd.it/cwi7i639c5d61.png?width=512&format=png&auto=webp&s=0510c8b93adb40eee4d3f41607f1c215d41e55ff

Example using text '''A cartoon of a new mascot for the Reddit subreddit DeepDream that has a mouse-like face and wears a cape''':

https://preview.redd.it/wtxbduevcbd61.png?width=512&format=png&auto=webp&s=c5d266258922bc62f25c80a08cd9cabc07d9cb1c

Example using text '''Bugs Bunny meets the Eye of Sauron, drawn in the Looney Tunes cartoon style''':

https://preview.redd.it/gmljaeekuid61.png?width=512&format=png&auto=webp&s=9ea578de165e12afc3a62bf6886bc1ae9dc19bec

Example using text '''Photo of a blue and red neon-colored frog at night.''':

https://preview.redd.it/nzlypte6wzd61.png?width=512&format=png&auto=webp&s=7e10b06f22cfc57c64b6d05738c7486b895083df

Example using text '''Hell begins to freeze over''':

https://preview.redd.it/vn99we9ngmf61.png?width=512&format=png&auto=webp&s=2408efd607f0ab40a08db6ee67448791aa813993

Example using text '''A scene with vibrant colors''':

https://preview.redd.it/4z133mvrgmf61.png?width=512&format=png&auto=webp&s=b78e7a8e3f736769655056093a9904ff09a355a1

Example using text '''The Great Pyramids were turned into prisms by a wizard''':

https://preview.redd.it/zxt6op7vgmf61.png?width=512&format=png&auto=webp&s=53e578cfde14b28afe27957e95e610b89afadd44"
251,machinelearning,openai,relevance,2023-08-15 04:40:49,[P] OpenAI Notebooks which are really helpful.,vishank97,False,0.93,155,15ridca,https://www.reddit.com/r/MachineLearning/comments/15ridca/p_openai_notebooks_which_are_really_helpful/,7,1692074449.0,"The OpenAI cookbook is one of the most underrated and underused developer resources available today. Here are 7 notebooks you should know about:

1. Improve LLM reliability:  
[https://github.com/openai/openai-cookbook/blob/main/techniques\_to\_improve\_reliability.md](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)
2. Embedding long text inputs:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Embedding\_long\_inputs.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb)
3. Dynamic masks with DALLE:  
[https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How\_to\_create\_dynamic\_masks\_with\_DALL-E\_and\_Segment\_Anything.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How_to_create_dynamic_masks_with_DALL-E_and_Segment_Anything.ipynb)
4. Function calling to find places nearby:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Function\_calling\_finding\_nearby\_places.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb)
5. Visualize embeddings in 3D:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing\_embeddings\_in\_3D.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_3D.ipynb)
6. Pre and post-processing of Whisper transcripts:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Whisper\_processing\_guide.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb)
7. Search, Retrieval, and Chat:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Question\_answering\_using\_a\_search\_API.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb)

Big thanks to the creators of these notebooks!"
252,machinelearning,openai,relevance,2023-05-24 10:43:01,[D] Diff between OpenAI official chatgpt and azure OpenAI playground chat,lppier2,False,0.6,1,13qhyek,https://www.reddit.com/r/MachineLearning/comments/13qhyek/d_diff_between_openai_official_chatgpt_and_azure/,5,1684924981.0,"I’m trying chatgpt with gpt3.5turbo in azure playground. Is it my imagination, or is the official OpenAI chatgpt much more “chatty” than the one in the playground?
What could be the differences in the settings , any intuition?"
253,machinelearning,openai,relevance,2023-02-09 07:58:42,[P] Get 2x Faster Transcriptions with OpenAI Whisper Large on Kernl,pommedeterresautee,False,0.96,229,10xp54e,https://www.reddit.com/r/MachineLearning/comments/10xp54e/p_get_2x_faster_transcriptions_with_openai/,34,1675929522.0,"We are happy to announce the support of OpenAI Whisper model (ASR task) on Kernl. 

We focused on high quality transcription in a latency sensitive scenario, meaning:

* *whisper-large-v2* weights
* *beam search 5 (as recommended in the related paper)*

We measured a 2.3x speedup on Nvidia A100 GPU (2.4x on 3090 RTX) compared to Hugging Face implementation using FP16 mixed precision on transcribing librispeech test set (over 2600 examples). For now, OpenAI implementation is [not yet PyTorch 2.0 compliant](https://github.com/openai/whisper/pull/115).

In the post below, we discuss what worked (CUDA Graph), our tricks (to significantly reduce memory footprint), and what did not pay off (Flash attention and some other custom Triton kernels).

* **Kernl repository**: [https://github.com/ELS-RD/kernl](https://github.com/ELS-RD/kernl)
* **Reproduction script**: [https://github.com/ELS-RD/kernl/blob/main/experimental/whisper/speedup.ipynb](https://github.com/ELS-RD/kernl/blob/main/experimental/whisper/speedup.ipynb)

# Unsung hero: CUDA graphs

CUDA graphs technology provides most of the speed up. Compared to vanilla PyTorch 2.0 (“reduce-overhead mode”), we provide a limited memory footprint when vanilla PyTorch 2.0 may raise OOM exception.

[memory footprint](https://preview.redd.it/jyfayud5d4ha1.png?width=1598&format=png&auto=webp&s=15356368ce60eb5e748d21239ac0ecc00bd403ac)

Experiments have been run on a 3090 RTX with 24 Gb DDR. A reminder that PyTorch 2.0 focuses on training, not inference, which may explain why it OOMs rapidly in this case.

At its beginning, many partitioners were surprised by PyTorch eager mode performances, when compared to TensorFlow 1.x compiled models: they were on par! Python brought its flexibility and ease of debugging without implying any significant performance cost.

This is mostly because GPUs are latency hiding hardware: when PyTorch launches an operation on GPU, it sends instructions from host (CPU) to a queue (the CUDA stream), which allows PyTorch to continue Python script execution without having to wait for CUDA kernel to finish its work. This strategy effectively hides most of the Python overhead, in particular when there are some computation costly operations like convolutions or matrix multiplications.

Each new generation of GPUs being much faster than its predecessor, this strategy could not last forever, according to one PyTorch maintainer, it is an “existential problem” ([dev podcast](https://pytorch-dev-podcast.simplecast.com/episodes/pytorch-20), around 8mn30).

In inference mode, especially in latency-sensitive scenarios where batch size tends to be low, there is often little computation to perform (regarding what modern GPUs can do), making it even harder to hide effectively Python overhead. It’s accentuated in the case of generative models like Whisper, because each decoder call focuses on generating a single token, and a part of the computation is cached for the next token.

This is a typical situation where CUDA graph is very helpful.

The main idea behind CUDA graph is that we can replace a series of instructions sent from host (CPU) to device (GPU) by one call referring to a graph of instruction stored in GPU. Check also this twitter [thread](https://twitter.com/cHHillee/status/1616906059368763392) for more explanations.

First it will observe the inference of a model for specific input shapes and then replay it without going through most of the Python code.

One constraint is that it will replay the exact same operations with the exact same arguments.

For instance, memory addresses used by kernels are captured and therefore need to be static. For input tensors, it means that for each inference, we need to allocate some GPU memory and copy them there before the capture and copy all the following input tensors at the very same place.

The second constraint is that dynamic shapes are not supported by CUDA graph because it captures everything. We could have our own machinery in front of the model, but PyTorch 2.0 offers the right tooling to manage that point out of the box.

Basically, dynamo offers a mechanism which checks if the model has already been captured for specific input shapes and some other states and capture it if not yet the case. You just have to provide a function which converts to CUDA graphs and you are done.

Out of the box, PyTorch 2.0 provides a “reduce-overhead” mode which applies CUDA graph to the model. Unfortunately, for now, it will raise an OOM with Whisper large or medium because it reserves some CUDA space for each input shape. Therefore, for a generative model it rapidly fulfills the GPU memory, in particular because of the K/V cache which can be huge.

We have worked around this constrain by building our own layer on top of the memory pool of PyTorch. 

Basically, a PyTorch tensor is made of 2 parts, a CUDA allocated memory represented by PyTorch as a “storage”, and a bunch of metadata associated with it. Among the metadata there is a CUDA memory address, the tensor shape plus its strides, its dtype and... a memory offset.

Our idea is to create a very large tensor and share its storage between several input tensors, using offset metadata. With this solution, we avoid specializing in input tensor shapes and share the reserved memory for different input shapes related to several CUDA graphs.

As shown in the table above, it significantly reduces the memory overhead.

# What about custom (Triton) kernels for attention?

**TL; DR: we tried, they work, we got up to 2 times faster than eager PyTorch for cross attention and they bring close to nothing in e2e latency mostly because the improvement is not big enough to matter 🙁**

Below, we follow the convention of naming Q, K and V, the 3 tensors used in the attention of transformer models.

Whisper is based on a classic transformer architecture, with an encoder and a decoder.

Two characteristics of this model are of interest:

* The shape of Q tensor used in cross-attention is always \[batch, #heads, 1, 1500\].
* Model has been trained on 30-second audio files and their associated transcript. Because audio files are short, the sequence to generate is usually short, fewer than 50 tokens most of the time.

Because of these characteristics, optimizing attention has a low reward. In particular, the now common trick “replace attention with flash attention” is counterproductive:

* self-attention: sequences are very short, so quadratic complexity is less of an issue;
* cross-attention: using flash-attention leads to a 2 times slower inference on this part of the model.

We have tried to work on the second point and thought we could make cross attention faster.

Usual attention implementation (self and cross) relies on a series of operations: matmul (Q x K\^t) -> rescale -> SoftMax -> matmul (SoftMax output x V). Intermediate output tensors have a shape which usually scales quadratically with input sequence length. They will be saved and reloaded from DDR, and memory bandwidth is a very scarce resource in GPUs.

To optimize speed, flash attention fuses operations, so basically first matmul will work on a small part of Q and K, and directly apply SoftMax to it without saving intermediate results to DDR. Same for second matmul. Because we don't go and back through GPU main memory, flash attention usually runs much faster than naïve implementation of attention.

The parallelization of the jobs is done on different axes: [batch and attention head for the original flash attention](https://github.com/HazyResearch/flash-attention/issues/40), and Triton author added a third one, tokens, aka third dimension of Q (this important trick is now also part of flash attention CUDA implementation).

In the Whisper latency sensitive case, this doesn’t work well. The size of batches is low and sequence length (third dimension of Q tensor) is... 1! So, even if each job is done very efficiently, our GPU occupancy is low, and basically most of its streaming processors are idle. At the end of the day, the FA kernel is up to 2 times slower than eager PyTorch implementation (depending on batch size and model size).

# Try 1: the very simple kernel

We noted that there is little computation to do and that we were memory bandwidth bounded. It means that most of the time we wait for data to be transferred from main memory to shared memory. 

We leveraged that fact in a very simple kernel with 2 optimizations:

* after having finishing the rescale of the QK\^t matmul, we perform the SoftMax computation in parallel of loading V tensor for the final matmul. The SoftMax computation finishes before the end of the V loading, so basically it costs us nothing;
* to achieve best performances, we also changed the memory layout of V tensor in a way where we get a coalesced access, so we lowered the pressure on the memory bandwidth and increased instruction throughput (coalesced access let you load up to 128 bytes in a single instruction so you need less of them, which lets you perform more other things)

Altogether this cross attention was up to 2x faster compared to eager. It appeared to bring between 5 to 20% in end-to-end benchmark depending on model size and batch size. Cool but far from being a game changer, it requires a modification specific to Whisper model (memory layout of V) which is not in the spirit of the Kernl library. We decided to search for another way of doing things (we kept the code in the library for possible future use case).

# Try 2: Skinny Flash Attention

Our second try is based on the very same trick as Flash Attention (parallel SoftMax) but is designed for tall and skinny tensors, which is inspired by split-k strategy in GEMM (a close cousin of the matmul). The main idea is to add a new parallelization axis over the 3rd dimension of K tensor. The next steps are in the same spirit as flash attention with a difference that we need a new reduction operation between the different jobs' outputs. It provides 5-10% speedup compared to eager implementation on this setup at kernel level. We kept that kernel to ease the next feature we are working on (quantization) but the effect in end-to-end latency is inferior to 5% (still it exists 😅).

Some thoughts about PyTorch 2.0, Triton and making things much faster

Playing with PyTorch ~~1.14~~ 2.0 since this summer made us quite convinced that the major update to be released very soon will be a game changer for the ML field.

For inference (but also for training), the parallel with PyTorch vs TensorFlow is obvious to our eyes. 

The traditional way to deploy a model is to export it to Onnx, then to TensorRT plan format. Each step requires its own tooling, its own mental model, and may raise some issues. The most annoying thing is that you need Microsoft or Nvidia support to get the best performances, and sometimes model support takes time. For instance, T5, a model released in 2019, is not yet correctly supported on TensorRT, in particular K/V cache is missing ([soon it will be according to TensorRT maintainers](https://github.com/NVIDIA/TensorRT/issues/1845), but I wrote the very same thing almost 1 year ago and then 4 months ago so… I don’t know).

PyTorch 2.0 makes the graph capture step easy, it has been designed to work even if not everything is PyTorch compliant. With its Python first philosophy, it provides flexibility and debuggability. 

Several years ago, some said that by design PyTorch can’t be as performant than Tensorflow because of its eager execution model, compilation has to be faster. The same thing could be said for OnnxRuntime or TensorRT, they are C++ stuff, they have less overhead, etc. But at the end of the day, it's always the “ease of use” which is decisive. Ease of use because of Python, but also because of the transparency in the process, Triton makes understanding and debugging kernels much easier than closed source TensorRT Myelin engine calling closed source cuBlas library.

And of course, like TensorFlow, there will be many use cases where dedicated tools will be best choices, starting with situations where you can’t deploy a Python interpreter.

The second lesson, Triton is easier to start with than CUDA, but you probably can’t write or debug highly performant code without being able to, at least, read and debug PTX/SASS instructions. We realized that when we had some performance issues... The good news is that PTX is understandable, and you will probably spot unexpected generated code with some effort if there is any. Moreover, CUDA probably requires the same care when you really focus on performances.

We had plenty of issues with Triton, for example, cosmetics change in code may raise segfault. At some point you finish by having an intuition of what kinds of patterns to follow to make things work, in particular when there are for loops and dot operations. A new version of Triton has recently been released after a full rewrite of its backend, our little tests showed some improvement on stability but we have not yet fully switched.

As in my previous post, I highly recommend that readers start playing with Triton library, I rewrite it here: it’s fun (at least when it doesn’t segfault) and helps you to make sense of a large part of what is happening in ML engineering. I am quite convinced many flash attention like kernels are still to be written. 

# Caveat

Two important things to note about the project described here:

* CUDA graphs require us to capture a graph per input tensor shape, there is a non-negligible warmup time. We measure around 10mn on 2 different machines / GPUs (down from 50mn in our previous Kernl version). One user reported with the new version a bit more than 20mn of warmup time. We are aware of obvious ways to decrease it significantly.
* The context here is latency sensitive optimization. In throughput sensitive one, just increasing batch size will bring you most of the speedup. Otherwise, more aggressive optimizations like quantization are required (not yet released on Kernl)."
254,machinelearning,openai,relevance,2020-09-22 17:40:14,[N] Microsoft teams up with OpenAI to exclusively license GPT-3 language model,kit1980,False,0.96,322,ixs88q,https://www.reddit.com/r/MachineLearning/comments/ixs88q/n_microsoft_teams_up_with_openai_to_exclusively/,117,1600796414.0,"""""""OpenAI will continue to offer GPT-3 and other powerful models via its own Azure-hosted API, launched in June. While we’ll be hard at work utilizing the capabilities of GPT-3 in our own products, services and experiences to benefit our customers, we’ll also continue to work with OpenAI to keep looking forward: leveraging and democratizing the power of their cutting-edge AI research as they continue on their mission to build safe artificial general intelligence.""""""

https://blogs.microsoft.com/blog/2020/09/22/microsoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/"
255,machinelearning,openai,relevance,2019-12-13 19:02:10,[R] OpenAI releases paper on OpenAI Five: Dota 2 with Large Scale Deep Reinforcement Learning,minimaxir,False,0.97,210,ea880b,https://www.reddit.com/r/MachineLearning/comments/ea880b/r_openai_releases_paper_on_openai_five_dota_2/,46,1576263730.0,"[https://cdn.openai.com/dota-2.pdf](https://cdn.openai.com/dota-2.pdf)

It's 66 pages long and very detailed."
256,machinelearning,openai,relevance,2022-10-10 19:38:01,[P] Pure C/C++ port of OpenAI's Whisper,ggerganov,False,0.99,190,y0nvqu,https://www.reddit.com/r/MachineLearning/comments/y0nvqu/p_pure_cc_port_of_openais_whisper/,34,1665430681.0,"Recently, I am having fun with re-implementing the inference of various transformer models (GPT-2, GPT-J) in pure C/C++ in order to efficiently run them on a CPU.

The latest one that I ported is [OpenAI Whisper](https://github.com/openai/whisper) for automatic speech recognition:

[https://github.com/ggerganov/whisper.cpp](https://github.com/ggerganov/whisper.cpp)

For smaller models I am able to achieve very nice performance.  
For example, here is a demonstration of real-time transcription of audio from the microphone:

[whisper.cpp running on a MacBook Pro M1 \(CPU only\)](https://reddit.com/link/y0nvqu/video/1i955h6e61t91/player)

Hope you find this project interesting and let me know if you have any questions about the implementation."
257,machinelearning,openai,relevance,2019-04-25 17:07:04,[N] MuseNet by OpenAI,wavelander,False,0.96,407,bhb4ds,https://openai.com/blog/musenet/,48,1556212024.0,
258,machinelearning,openai,relevance,2024-02-10 21:02:10,[D] How to speed up OpenAI API response time ?,AB3NZ,False,0.21,0,1anqbt9,https://www.reddit.com/r/MachineLearning/comments/1anqbt9/d_how_to_speed_up_openai_api_response_time/,8,1707598930.0,"Hello, I'm using the OpenAI API to create summaries based on provided JSON data. However, I've noticed that the response generation process takes approximately 25 seconds, which may pose an inconvenience to users of my application. I'm reaching out to inquire if there are any strategies or optimizations available to help improve the response time and enhance the overall user experience."
259,machinelearning,openai,relevance,2023-02-01 21:59:38,[N] OpenAI starts selling subscriptions to its ChatGPT bot,bikeskata,False,0.89,51,10r7k0h,https://www.reddit.com/r/MachineLearning/comments/10r7k0h/n_openai_starts_selling_subscriptions_to_its/,50,1675288778.0,"https://www.axios.com/2023/02/01/chatgpt-subscriptions-chatbot-openai

Not fully paywalled, but there's a tiering system."
260,machinelearning,openai,relevance,2023-12-18 18:28:40,[D] OpenAI Triton Course/Tutorial Recommendations,djm07231,False,0.86,5,18lfq0a,https://www.reddit.com/r/MachineLearning/comments/18lfq0a/d_openai_triton_coursetutorial_recommendations/,0,1702924120.0,"Hello, I am a first-year graduate student with a keen interest in GPU programming and AI, I recently completed an introductory course in CUDA, similar to Illinois ECE 498AL. Looking to broaden my expertise, I'm drawn to OpenAI's Triton for its potential in the field. However, I find the current official tutorials lacking in depth, particularly in explaining the programming model and fundamental concepts.

Does anyone have recommendations for comprehensive Triton learning resources? I'm interested in tutorials that integrate with PyTorch, as well as foundational guides that can bridge the gap from CUDA to Triton. GPT-4 hasn't been much help on this topic, so I'm hoping that there would good insights here.

I would appreciate any kind of suggestions, videos, blogs, or even courses that have helped you grasp Triton better. Sharing your journey and how Triton has impacted your projects would also be incredibly valuable to me and others exploring this tool.

Official Tutorial: [https://triton-lang.org/main/getting-started/tutorials/index.html](https://triton-lang.org/main/getting-started/tutorials/index.html)"
261,machinelearning,openai,relevance,2023-05-16 08:56:31,[D] OpenAI API vs. Open Source Self hosted for AI Startups,ali-gettravy,False,0.91,54,13izmyc,https://www.reddit.com/r/MachineLearning/comments/13izmyc/d_openai_api_vs_open_source_self_hosted_for_ai/,43,1684227391.0,"Hello, ML community! We're having a discussion around the benefits of using OpenAI's API versus the open-source, self-hosted approach for our AI startup. Has anyone navigated this decision before and could share some insights? Thanks!"
262,machinelearning,openai,relevance,2023-04-08 06:23:40,[D] Alternatives to OpenAI for summarization and instruction following?,du_keule,False,0.92,59,12fdnad,https://www.reddit.com/r/MachineLearning/comments/12fdnad/d_alternatives_to_openai_for_summarization_and/,34,1680935020.0,"Hey y’all. As privacy concerns are mounting about OpenAI, and as someone who has built a product on top of their platform, I’m wondering what kind of alternatives exist that could accomplish the same results as GPT 3.5 and be able to be used commercially? It looks like Alpaca would do well, but it’s not able to be used commercially. 

Basically my product summarizes Slack threads and answers questions based on a given prompt. Some users have expressed concern about sending their company’s data to OpenAI, and honestly it would be an edge to have in the market if I could run an LLM  in my VPC. Thanks!"
263,machinelearning,openai,relevance,2023-07-12 19:02:12,[D] What is the most efficient version of OpenAI Whisper?,paulo_zip,False,0.9,15,14xxg6i,https://www.reddit.com/r/MachineLearning/comments/14xxg6i/d_what_is_the_most_efficient_version_of_openai/,32,1689188532.0,"Hi everyone,
I know that there are some different versions of Whisper available in the open-source community (Whisper X, Whisper JAX, etc.), but I'm keeping updated with the best version of the model. Specifically, I'm trying to understand the best Whisper implementation for a task to transcribe a big batch of videos (~10k videos, ~30min long).

I'd like to know your thoughts on this."
264,machinelearning,openai,relevance,2023-10-22 04:44:15,"[D]openai whisper local, command line args",eltegs,False,0.14,0,17dldmj,https://www.reddit.com/r/MachineLearning/comments/17dldmj/dopenai_whisper_local_command_line_args/,5,1697949855.0,"Are there any args in whisper to specify a start and end time of a media file, to process?

For example timestamps or hh:mm:ss, to just transcribe a portion (for want of a better word) of the media."
265,machinelearning,openai,relevance,2023-11-06 18:22:39,[N] OpenAI Whisper new model Large V3 just released and amazing,CeFurkan,False,0.69,9,17p99mj,https://www.reddit.com/r/MachineLearning/comments/17p99mj/n_openai_whisper_new_model_large_v3_just_released/,22,1699294959.0,"Whisper made huge impact on the open source AI world

I am using everyday to transcribe my videos with that

I was waiting new Large model

Whisper is much better than paid alternatives and it is 100% free

Here my full tutorial about it

[How to do Free Speech-to-Text Transcription Better Than Google Premium API with OpenAI Whisper Model](https://youtu.be/msj3wuYf3d8?si=c5M6mFzQIj6fJRou)

Repo link : [https://github.com/openai/whisper](https://github.com/openai/whisper)

&#x200B;

https://preview.redd.it/k9kssc6csryb1.png?width=1920&format=png&auto=webp&s=caeaf4921c8b4f9337c4842c5ef897cf456adc20"
266,machinelearning,openai,relevance,2023-10-29 19:23:42,[R] Is it possible to implement generative ai successfully without relying on openAI,leaderof13,False,0.17,0,17jaep4,https://www.reddit.com/r/MachineLearning/comments/17jaep4/r_is_it_possible_to_implement_generative_ai/,32,1698607422.0,"I would like to understand if anyone  has implemented generative AI  without using openAI and if so which  open source you have used and how successful it has been so far

We have enterprise level incident data, relevant documentation etc that users will search about and  need to generate responses using generative ai.

Is it possible to do this without relying on open AI at all"
267,machinelearning,openai,relevance,2023-11-16 00:49:29,[D] OpenAI Assistants - Assistant Swarms,Text-Agitated,False,0.5,0,17w9akj,https://www.reddit.com/r/MachineLearning/comments/17w9akj/d_openai_assistants_assistant_swarms/,2,1700095769.0,"Is it a better idea to partition a task into multiple assistants that can engage in a conversation?

Thanks!"
268,machinelearning,openai,relevance,2022-11-03 23:12:45,"[D] DALL·E to be made available as API, OpenAI to give users full ownership rights to generated images",TiredOldCrow,False,0.98,423,yli0r7,https://www.reddit.com/r/MachineLearning/comments/yli0r7/d_dalle_to_be_made_available_as_api_openai_to/,55,1667517165.0,"Email announcement from OpenAI below:


> DALL·E is now available as an API


> You can now integrate state of the art image generation capabilities directly into your apps and products through our new DALL·E API.


> You own the generations you create with DALL·E.


> We’ve simplified our [Terms of Use](https://openai.com/api/policies/terms/) and you now have full ownership rights to the images you create with DALL·E — in addition to the usage rights you’ve already had to use and monetize your creations however you’d like. This update is possible due to improvements to our safety systems which minimize the ability to generate content that violates our content policy.


> Sort and showcase with collections.


> You can now organize your DALL·E creations in multiple collections. Share them publicly or keep them private. Check out our [sea otter collection](https://labs.openai.com/sc/w3Q8nqVN69qkEA3ePSmrGb5t)!


> We’re constantly amazed by the innovative ways you use DALL·E and love seeing your creations out in the world. Artists who would like their work to be shared on our Instagram can request to be featured using Instagram’s collab tool. DM us there to show off how you’re using the API!  

> \- The OpenAI Team"
269,machinelearning,openai,relevance,2023-11-03 09:21:50,[D] OpenAI's agents and their plans for 2024,blabboy,False,0.17,0,17mr1rk,https://www.reddit.com/r/MachineLearning/comments/17mr1rk/d_openais_agents_and_their_plans_for_2024/,11,1699003310.0,"https://twitter.com/immutable404/status/1719952110492504206

Text pasted here:

@OpenAI
 will release ""agents"" that will establish a high level functioning version of AutoGPT/BabyAGI. 

Agents can be funded, pre-paid card style, for completing online purchases. 

Think 'digital swiss army knife' for business tasks/ product development/ coding/ scheduling/ ordering lunch/ paying bills/ etc. These agents/synths will also accept customer payments while acting as your sales chat bot/synth i.e. HeyGen style, if you sell products online. 

OpenAI will provide everyone with their personalized digital agent, at $30-$150/month. Similar to wireless plans with a limit on talk/text/data, there will be a range of features and data limits on your agent/synth.

You will be able to skin your agent and provide it with a set personality. You can customize your agent with limitations. This will be the craze in 2024. 

These will be parallel to the invention of the cell phone and will add a monthly bill to each employee. You won't be able to function without your agent/synth in business by the end of 2024.

OpenAI will use this influx of capital to expand it's empire and will look to pay back Microsoft to create more separation between the two entities.  You'll hear rumblings of a public divorce happening by the end of 2024."
270,machinelearning,openai,relevance,2023-11-20 08:50:54,"[N] Sam Altman and Greg Brockman, together with colleagues, will join Microsoft to lead new advanced AI research team",Civil_Collection7267,False,0.96,413,17zk6zy,https://www.reddit.com/r/MachineLearning/comments/17zk6zy/n_sam_altman_and_greg_brockman_together_with/,178,1700470254.0,"Source: [https://blogs.microsoft.com/blog/2023/11/19/a-statement-from-microsoft-chairman-and-ceo-satya-nadella/](https://blogs.microsoft.com/blog/2023/11/19/a-statement-from-microsoft-chairman-and-ceo-satya-nadella/)

>We remain committed to our partnership with OpenAI and have confidence in our product roadmap, our ability to continue to innovate with everything we announced at Microsoft Ignite, and in continuing to support our customers and partners. We look forward to getting to know Emmett Shear and OAI’s new leadership team and working with them. And we’re extremely excited to share the news that Sam Altman and Greg Brockman, together with colleagues, will be joining Microsoft to lead a new advanced AI research team. We look forward to moving quickly to provide them with the resources needed for their success.

News article covering the situation: [https://www.theverge.com/2023/11/20/23968829/microsoft-hires-sam-altman-greg-brockman-employees-openai](https://www.theverge.com/2023/11/20/23968829/microsoft-hires-sam-altman-greg-brockman-employees-openai)

>Altman’s Microsoft hiring comes just hours after negotiations with OpenAI’s board failed to bring him back as OpenAI CEO. Instead, former Twitch CEO and co-founder Emmett Shear has been named as interim CEO.  
>  
>Altman had been negotiating to return as OpenAI CEO, but OpenAI’s four-person board refused to step down and let him return."
271,machinelearning,openai,relevance,2023-11-15 21:57:33,[D] Multiple documents reveal significant limitations of OpenAI's Assistants API for RAG,ian4321,False,0.94,61,17w50tz,https://www.reddit.com/r/MachineLearning/comments/17w50tz/d_multiple_documents_reveal_significant/,17,1700085453.0,"The OpenAI RAG system struggled with multiple documents, showing inconsistent performance with our evaluation framework. However, performance improved markedly when all documents were uploaded as a single document. Despite current limitations, such as a 20-file limit per assistant and challenges in handling multiple documents, there is significant potential for improvement. Enhancing the Assistants API to match GPT quality and reducing restrictions could make it a leading RAG solution.

[https://www.tonic.ai/blog/rag-evaluation-series-validating-openai-assistants-rag-performance](https://www.tonic.ai/blog/rag-evaluation-series-validating-openai-assistants-rag-performance)"
272,machinelearning,openai,relevance,2023-04-18 03:30:03,[Discussion] OpenAI Embeddings API alternative?,HueX1,False,0.83,19,12q8rp1,https://www.reddit.com/r/MachineLearning/comments/12q8rp1/discussion_openai_embeddings_api_alternative/,15,1681788603.0,"Do you know an API which hosts an OpenAI embeddings alternative? If have the criteria that the embedding size needs to be max. 1024.

I know there are interesting models like [e5-large](https://huggingface.co/intfloat/e5-large) and Instructor-xl, but I specifically need an API as I don't want to set up my own server.The Huggingface Hosted Inference API is too expensive, as I need to pay for it even if I don't use it, by just keeping it running."
273,machinelearning,openai,relevance,2021-09-06 13:39:07,[D] How OpenAI Sold its Soul for $1 Billion: The company behind GPT-3 and Codex isn’t as open as it claims.,sensetime,False,0.95,665,pizllt,https://www.reddit.com/r/MachineLearning/comments/pizllt/d_how_openai_sold_its_soul_for_1_billion_the/,107,1630935547.0,"An essay by Alberto Romero that traces the history and developments of OpenAI from the time it became a ""capped-for-profit"" entity from a non-profit entity:

Link: https://onezero.medium.com/openai-sold-its-soul-for-1-billion-cf35ff9e8cd4"
274,machinelearning,openai,relevance,2024-02-04 19:16:54,OpenAI gym - Neural network outputs stay the same [P],DocMenios,False,1.0,3,1aivukw,https://www.reddit.com/r/MachineLearning/comments/1aivukw/openai_gym_neural_network_outputs_stay_the_same_p/,2,1707074214.0,"Hello everyone. I am working on a project where I evolve the weights of a Neural Network with evolutionary strategies to make the bipedal walker of Gym walk.

But I keep running into this specific issue. After some frames of the simulation, the output of the NN stays the same (small deviations of 0.00001 which make no difference) which in essence, makes the agent freeze and make no movements.

I don't really know why this is happening since the inputs seem different and it cannot be a training issue since there is no training. I set the weights manually and then just test their performance.

I am using Tahn activation function so could it be a vanishing gradient problem?

I also suspected that it might be my NN architecture being too simple since for the scope of the project I needed a simple NN (24 in, 20 hidden, 4 out), but adding more layers/neurons does not change anything.

Note: I am using the newer Gymnasium, not Gym, I'm just referring to it as gym since its simpler.

Any help is welcome, thank you in advance!"
275,machinelearning,openai,relevance,2022-10-26 06:10:48,"[P] Up to 12X faster GPU inference on Bert, T5 and other transformers with OpenAI Triton kernels",pommedeterresautee,False,0.99,373,ydqmjp,https://www.reddit.com/r/MachineLearning/comments/ydqmjp/p_up_to_12x_faster_gpu_inference_on_bert_t5_and/,46,1666764648.0,"We are releasing [Kernl](https://github.com/ELS-RD/kernl/) under Apache 2 license, a library to make PyTorch models inference significantly faster. With 1 line of code we applied the optimizations and made Bert up to 12X faster than Hugging Face baseline. T5 is also covered in this first release (> 6X speed up generation and we are still halfway in the optimizations!). This has been possible because we wrote custom GPU kernels with the new OpenAI programming language Triton and leveraged TorchDynamo.

**Project link**: [https://github.com/ELS-RD/kernl/](https://github.com/ELS-RD/kernl/)

**E2E demo notebooks**: [XNLI classification](https://github.com/ELS-RD/kernl/blob/main/tutorial/bert%20e2e.ipynb), [T5 generation](https://github.com/ELS-RD/kernl/blob/main/tutorial/t5%20e2e.ipynb)

[Benchmarks ran on a 3090 RTX GPU, 12 cores Intel CPU, more info below](https://preview.redd.it/mlo3wvn0d3w91.png?width=2738&format=png&auto=webp&s=1b9dce736ee4c0e371b54b9ef796310f9728660d)

On long sequence length inputs, [Kernl](https://github.com/ELS-RD/kernl/) is most of the time the fastest inference engine, and close to Nvidia TensorRT on shortest ones. Keep in mind that Bert is one of the most optimized models out there and most of the tools listed above are very mature.

What is interesting is not that [Kernl](https://github.com/ELS-RD/kernl/) is the fastest engine (or not), but that the code of the kernels is short and easy to understand and modify. We have even added a Triton debugger and a tool (based on Fx) to ease kernel replacement so there is no need to modify PyTorch model source code.

Staying in the comfort of PyTorch / Python maintains dynamic behaviors, debugging and iteration speed. Teams designing/training a transformer model (even custom) can take care of the deployment without relying on advanced GPU knowledge (eg. CUDA programming, dedicated inference engine API, etc.).

Recently released models relying on slightly modified transformer architectures are rarely accelerated in traditional inference engines, we need to wait months to years for someone (usually inference engine maintainers) to write required custom CUDA kernels. Because here custom kernels are written in OpenAI Triton language, **anyone without CUDA experience** can easily modify them: OpenAI Triton API is simple and close to Numpy one. Kernels source code is significantly shorter than equivalent implementation in CUDA (< 200 LoC per kernel). Basic knowledge of how GPU works is enough. We are also releasing a few tutorials we initially wrote for onboarding colleagues on the project. We hope you will find them useful: [https://github.com/ELS-RD/kernl/tree/main/tutorial](https://github.com/ELS-RD/kernl/tree/main/tutorial). In particular, there is:

* Tiled matmul, the GPU way to perform matmul: [https://github.com/ELS-RD/kernl/blob/main/tutorial/1%20-%20tiled%20matmul.ipynb](https://github.com/ELS-RD/kernl/blob/main/tutorial/1%20-%20tiled%20matmul.ipynb)
* Simple explanation of what Flash attention is and how it works, a fused attention making long sequences much faster: [https://github.com/ELS-RD/kernl/blob/main/tutorial/4%20-%20flash%20attention.ipynb](https://github.com/ELS-RD/kernl/blob/main/tutorial/4%20-%20flash%20attention.ipynb)

And best of the best, because we stay in the PyTorch / Python ecosystem, we plan in our roadmap to also enable **training** with those custom kernels. In particular [Flash attention](https://github.com/HazyResearch/flash-attention) kernel should bring a 2-4X speed up and the support of very long sequences on single GPU (paper authors went as far as 16K tokens instead of traditional 512 or 2048 limits)! See below for more info.

**IMPORTANT**: Benchmarking is a difficult art, we tried to be as fair as possible. Please note that:

* Timings are based on wall-clock times and we show speedup over baseline as they are easier to compare between input shapes,
* When we need to choose between speed and output precision, we always choose precision
* HF baseline, CUDA graphs, Inductor and [Kernl](https://github.com/ELS-RD/kernl/) are in mixed precision, AITemplate, ONNX Runtime, DeepSpeed and TensorRT have their weights converted to FP16.
* Accumulation is done in FP32 for AITemplate and [Kernl](https://github.com/ELS-RD/kernl/). TensorRT is likely doing it in FP16.
* CUDA graphs is enabled for all engines except baseline, Nvfuser and ONNX Runtime which [has a limited support of it](https://github.com/microsoft/onnxruntime/issues/12977#issuecomment-1258406358).
* For [Kernl](https://github.com/ELS-RD/kernl/) and AITemplate, fast GELU has been manually disabled (TensorRT is likely using Fast GELU).
* AITemplate measures are to be taken with a grain of salt, it [doesn’t manage attention mask](https://github.com/facebookincubator/AITemplate/issues/46#issuecomment-1279975463) which means 1/ batch inference can’t be used in most scenarios (no padding support), 2/ it misses few operations on a kernel that can be compute-bounded (depends of sequence length), said otherwise it may make it slower to support attention mask, in particular on long sequences. AITemplate attention mask support will come in a future release.
* For TensorRT for best perf, we built 3 models, one per batch size. AITemplate will support dynamic shapes in a future release, so we made a model per input shape.
* Inductor is in prototype stage, performances may be improved when released, none of the disabled by default optimizations worked during our tests.

As you can see, CUDA graphs erase all CPU overhead (Python related for instance), sometimes there is no need to rely on C++/Rust to be fast! Fused kernels (in CUDA or Triton) are mostly important for longer input sequence lengths. We are aware that there are still some low hanging fruits to improve [Kernl](https://github.com/ELS-RD/kernl/) performance without sacrificing output precision, it’s just the first release. More info about how it works [here](https://github.com/ELS-RD/kernl#how).

**Why?**

We work for Lefebvre Sarrut, a leading European legal publisher. Several of our products include transformer models in latency sensitive scenarios (search, content recommendation). So far, ONNX Runtime and TensorRT served us well, and we learned interesting patterns along the way that we shared with the community through an open-source library called [transformer-deploy](https://github.com/ELS-RD/transformer-deploy). However, recent changes in our environment made our needs evolve:

* New teams in the group are deploying transformer models in prod directly with PyTorch. ONNX Runtime poses them too many challenges (like debugging precision issues in fp16). With its inference expert-oriented API, TensorRT was not even an option;
* We are exploring applications of large generative language models in legal industry, and we need easier dynamic behavior support plus more efficient quantization, our creative approaches for that purpose we shared [here on Reddit](https://www.reddit.com/r/MachineLearning/comments/uwkpmt/p_what_we_learned_by_making_t5large_2x_faster/) proved to be more fragile than we initially thought;
* New business opportunities if we were able to train models supporting large contexts (>5K tokens)

On a more personal note, I enjoyed much more writing kernels and understanding low level computation of transformers than mastering multiple complicated tools API and their environments. It really changed my intuitions and understanding about how the model works, scales, etc. It’s not just OpenAI Triton, we also did some prototyping on C++ / CUDA / Cutlass and the effect was the same, it’s all about digging to a lower level. And still the effort is IMO quite limited regarding the benefits. If you have some interest in machine learning engineering, you should probably give those tools a try.

**Future?**

Our road map includes the following elements (in no particular order):

* Faster warmup
* Ragged inference (no computation lost in padding)
* Training support (with long sequences support)
* Multi GPU (multiple parallelization schemas support)
* Quantization (PTQ)
* New batch of Cutlass kernels tests
* Improve hardware support (>= Ampere for now)
* More tuto

Regarding training, if you want to help, we have written an issue with all the required pointers, it should be very doable: [https://github.com/ELS-RD/kernl/issues/93](https://github.com/ELS-RD/kernl/issues/93)

On top of speed, one of the main benefits is the support of very long sequences (16K tokens without changing attention formula) as it’s based on [Flash Attention](https://github.com/HazyResearch/flash-attention).

Also, note that future version of PyTorch will include [Inductor](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747). It means that all PyTorch users will have the option to compile to Triton to get around [1.7X faster training](https://dev-discuss.pytorch.org/t/torchinductor-update-3-e2e-model-training-with-torchdynamo-inductor-gets-1-67x-2-1x-speedup/793).

A big thank you to Nvidia people who advised us during this project."
276,machinelearning,openai,relevance,2023-07-10 22:58:18,[DISCUSSION] How much can we trust OpenAI (and other large AI companies) will keep our fine tuned models and data sets private?,inferred_answer,False,0.93,158,14w9b2s,https://www.reddit.com/r/MachineLearning/comments/14w9b2s/discussion_how_much_can_we_trust_openai_and_other/,73,1689029898.0,"*tldr: Do you trust OpenAI or other large AI companies with your data? Do you reckon it's just a matter of time before they find all of the data, so might as well contribute to their research project and benefit from it while you can? Or do you prefer to go the open sourced route instead for this reason?*

Here's is my concern:

Some of my team members are very high on Open AI's models, their ease of use, and how smart they are out the box. Now, Open AI (relatively recently) published a statement saying that they will not use your fine tuned models or data sets internally to improve their products, but given their history and the value of these fine tuned models and their corresponding datasets, I'm uncertain to what extent we are able to trust that they will keep our data private.

I like to think that OpenAI wants to position themselves as an AI provider of a sorts and in doing so would want to protect their client's data and trust and would refrain from doing so, but seeing how hungry companies are for data, particularly high quality data from niche domains, I can't help but wonder to what extent privacy is being respected and if we are being foolish by donating valuable data to companies that will turn around and continue to build their empire with it.

&#x200B;

With this in mind, I wanted to open up this question to the community:

Do you trust OpenAI or other large AI companies with your data? Do you reckon it's just a matter of time before they find all of the data, so might as well contribute to their research project and benefit from it while you can? Or do you prefer to go the open sourced route instead for this reason?

&#x200B;

Context:

I am working on a research project in a very specific domain (bound by NDA). The project calls for a question answering model that receives rigidly structured data as an input and is able to provide answers based on the input.

In order to give the model the context it needs to reliably answer questions in the desired format, we have prepared a significantly large fine tuning data set with data that is not readily available to the public that we have been collecting over the past few years.

&#x200B;

edit: formatting + adding tldr"
277,machinelearning,openai,relevance,2023-04-06 13:35:43,[D] Working with Various OpenAI Models - My Thoughts and Experiences,bart_so,False,0.86,183,12dkla0,https://www.reddit.com/r/MachineLearning/comments/12dkla0/d_working_with_various_openai_models_my_thoughts/,20,1680788143.0,"I'd like to share some of my insights from working with OpenAI models on my project. I'm not exactly a tech person, so some of these observations might be obvious to some of you, but I think they're worth sharing for those with less experience or who aren't directly in the field.

**Intro:**

In early February, my friends and I started a side project where we aimed to build an AI portal called DoMoreAI. For the first two months, we focused on creating an AI tools catalog. Our experiment is based on the idea that in the future, companies will be ""Managed by AI, and Driven by Humans."" So, our goal was to leave as much as possible to AI and automation, with all the consequences that come with it. As mentioned before, I'm not a tech guy, but I've been playing with OpenAI models for the past few years, so I had some experience when starting this project.

**Tasks We Assigned to AI:**

Based on an AI tool's front page, we had the AI write a one-sentence summary of an AI project + write a more in-depth review of the project, categorize the project into different categories (WHAT category, like blog; TASK category, like writing; FOR category, like content creator), decide if the project offers iOS app, Android app, browser extension, API, find social media links, process information about prices and pricing policy, and more.

**Interesting Findings:**

1. When working on a more complex prompt, particularly one with several tasks, you have to be patient when crafting it. You might eventually find the right wording to achieve the desired results, but it takes time and lots of trial and error. You might even be surprised by what works and what doesn't. 
2. If cost isn't an issue, you can always break up one complex prompt into several smaller prompts. However, the more requests you send, the higher the chance of encountering errors like the 429 error, which may require setting up more sophisticated error handlers for the whole process. 
3. You need error handlers because, without them, the automation process will suffer. 
4. With more complex prompts, there are no prompts that always yield the expected results, so you have to plan for what to do if the results aren't satisfactory and how to determine if the result meets your expectations or not. 
5. GPT-3.0 struggled with outputting JSON strings as requested, but GPT-3.5 is much better at this task. I'd say the number of errors from improperly formatting the response in JSON is 3-4 times lower for GPT-3.5. 
6. AI models have trouble distinguishing words singular forms from plural forms. 
7. Just because you can use AI for a given task doesn't mean you should. Often, standard techniques like using regex can yield better results when extracting something from text than relying solely on AI. A hybrid solution often provides the best results. 
8. We're using ADA vector embeddings and Pinecone for semantic search in our catalog, and I was really surprised to find that this kind of semantic search works in any language. Even if all the content on our page is in English, you can search in another language and still get decent results.

**The Best Mishaps:**

* As you may know, there's a token limit for requests, so we have to ensure that we don't send too long a part of the front page to the model. Sometimes, this led to funny situations. If the HTML of the page consists mainly of styles and the model is fed only with styles, then when you ask the AI to write a review of the project, it writes about how beautiful, mobile-friendly, etc., the project is. 
* For one project, instead of writing the one-sentence summary, the model's output only included the prompt we were using to generate the summary (needless to say, it was automatically published on our website ;))

&#x200B;

I hope this post will be useful. We are currently running a campaign on Product Hunt: [https://www.producthunt.com/posts/domore-ai](https://www.producthunt.com/posts/domore-ai)

So, if you have any feedback for us or think what we're doing is cool, don't hesitate to support us :)"
278,machinelearning,openai,relevance,2018-06-25 14:40:29,[R] OpenAI Five,circuithunter,False,0.96,250,8tr11j,https://blog.openai.com/openai-five/,48,1529937629.0,
279,machinelearning,openai,relevance,2024-01-18 15:52:34,"[D] While using function calling or tools on openai or langchain, does openai have access to the data or is it just that the output gets passed as intput to next function.",Ok_Cartographer5609,False,0.22,0,199t8be,https://www.reddit.com/r/MachineLearning/comments/199t8be/d_while_using_function_calling_or_tools_on_openai/,2,1705593154.0,I am working on a client project and I am using langchain's tools and agents. I want to know if the data is getting passed to openai or is it just like that - Output of one function is being directly passed to the second function with the knowledge of openai.
280,machinelearning,openai,relevance,2023-02-01 08:22:07,[P] Self Hostable OpenAI Alternative,leepenkman,False,0.46,0,10qobgk,https://www.reddit.com/r/MachineLearning/comments/10qobgk/p_self_hostable_openai_alternative/,23,1675239727.0,"Hi,  


[Text-Generator.io](https://text-generator.io/) is now self hostable, It's priced at $1000 USD per instance per year to self host.  


The service runs on a single 24GB VRAM GPU, and runs all services including [speech to text](https://text-generator.io/blog/cost-effective-speech-to-text-api), text and code generation for almost all languages and [generating embeddings](https://text-generator.io/blog/embed-images-text-and-code) too.  


The text generator also [downloads and analyses any input with links](https://text-generator.io/blog/text-generator-now-researches-via-crawling) including documents, images, images with text inside and webpages for better understanding and to generate better text.  


It's a great alternative to OpenAI and has a [compatible API making switching easy](https://text-generator.io/blog/over-10x-openai-cost-savings-one-line-change).  


You can check out the [new pricing here](https://text-generator.io/subscribe).  


Let me know what you think and if there's anything i can do to help!  


All the best.  
Lee Penkman - Founder [Text-Generator.io](https://text-generator.io/)"
281,machinelearning,openai,relevance,2022-04-02 09:36:21,[P] OpenAI Codex helping to write shell commands,tomd_96,False,0.95,419,tuf0vv,https://i.redd.it/dbgbskqg53r81.gif,12,1648892181.0,
282,machinelearning,openai,relevance,2023-04-11 08:25:54,[D] An application that JSONifys OpenAI API responses to enhance development,UnholyCathedral,False,0.75,43,12id67f,https://www.reddit.com/r/MachineLearning/comments/12id67f/d_an_application_that_jsonifys_openai_api/,31,1681201554.0,"Hey all, like a lot of you here I've been playing around with OpenAI's API along with others like Anthropic and building web apps. The one thing I find every time is how tedious it is to work with the plain text responses that come back from those APIs, so I'm building an API called ploomi which takes that raw text and converts it to JSON. Obviously then with JSON it's so much easier to parse, handle and style it.

Here's an example of AI text to JSON, and my application works with much more complex JSON structures too.

[Example AI text to JSON conversion](https://preview.redd.it/hy4emwctt7ta1.png?width=1183&format=png&auto=webp&s=0229e2ede209d69e9df48c28e3e3b1225e376bc2)

I'm about to launch the API but I'm really keen to get some feedback as I do think it will help fast-track growth of API applications.

Feel free to check it out here and join the waitlist if you're keen: [https://ploomi-api.carrd.co](https://ploomi-api.carrd.co/)

Thanks all!"
283,machinelearning,openai,relevance,2023-03-26 17:31:18,[P] SimpleAI : A self-hosted alternative to OpenAI API,lhenault,False,0.96,128,122tddh,https://www.reddit.com/r/MachineLearning/comments/122tddh/p_simpleai_a_selfhosted_alternative_to_openai_api/,21,1679851878.0,"Hey everyone,

I wanted to share with you [SimpleAI](https://github.com/lhenault/simpleAI), a self-hosted alternative to OpenAI API.

The aim of this project is to replicate the (main) endpoints of [OpenAI API](https://platform.openai.com/docs/introduction), and to let you easily and quickly plug in any new model. It basically allows you to deploy your custom model wherever you want and easily, while minimizing the amount of changes both on server and client sides.

It's compatible with the [OpenAI client](https://github.com/openai/openai-python) so you don't have to change much in your existing code (or can use it to easily query your API).

Wether you like or not the AI-as-a-service approach of OpenAI, I think that project could be of interest to many. Even if you are fully satisfied with a paid API, you might be interested in this if:

* You need a model fine tuned on some specific language and don't see any good alternative, or your company data is too sensitive to send it to an external service

* You’ve developped your own awesome model, and want a drop-in replacement to switch to yours, to be able to A/B test the two approaches.

* You're deploying your services in an infrastructure with an unreliable internet connection, so you would rather have your service locally

* You're just another AI enthusiast with a lot of spare time and free GPU

I've personally really enjoyed how open the ML(Ops) community has been in the past years, and seeing how the industry seems to be moving towards paid API and black box systems can be a bit worrying. This project might be useful to expose great, community-based alternatives.


If that sounds interesting, please have a look at the [examples](https://github.com/lhenault/simpleAI/tree/main/examples). I also have a [blogpost](https://louishenault.com/p/replicating-openai-api-for-llama-alpaca-or-any-animal-shaped-llm/) explaining a few more things.


Thank you!"
284,machinelearning,openai,relevance,2023-11-16 02:38:04,[D] OpenAI Assistants API in an Agent framework,SatoshiNotMe,False,0.85,9,17wbqgj,https://www.reddit.com/r/MachineLearning/comments/17wbqgj/d_openai_assistants_api_in_an_agent_framework/,0,1700102284.0,"Given the OpenAI Assistants API released last week, a natural next question was — how can we have several assistants work together on a task?

This was a perfect fit for the Langroid Multi Agent framework (which already worked with the completions API and any other local/remote LLM). 
 
For those interested in details of how to work with this API I wanted to share how we implemented a near-complete support for all Assistant features into the Langroid agent framework:

https://github.com/langroid/langroid/blob/main/langroid/agent/openai_assistant.py

We created an OpenAIAssistant class derived from ChatAgent. In Langroid you wrap a ChatAgent in a Task object to enable a multi agent interaction loop. Now the same can be done with an OpenAIAssistant object. 

I made a Colab notebook which gradually builds up from simple examples to a two-agent system for structured information extraction from a document:

https://colab.research.google.com/drive/190Tk7t4AdY1P9F_NlZ33-YEoGnHweQQ0

Our implementation supports function-calling, tools (retrieval/RAG, code interpreter). For the code interpreter we capture the code logs and display them in the interaction. 

We leverage persistent threads and assistants by caching their ids based on the username + machine + org, so that in a later session they could resume a previous thread + assistant. This is perhaps a simplistic implementation, I’m sure there are better ideas here. 

A key feature that is currently disabled is caching:  this is turned off because storing Assistant responses in threads is not allowed by the API. 

In any case, hope this is useful to some folks, as I’ve seen a lot of questions about this API in various forums and discord."
285,machinelearning,openai,relevance,2023-05-07 02:49:57,[D] Is openai text-embedding-ada-002 the best embeddings model?,lppier2,False,0.86,30,13aaj2w,https://www.reddit.com/r/MachineLearning/comments/13aaj2w/d_is_openai_textembeddingada002_the_best/,26,1683427797.0,"Hi,  I'm doing the typical searching of chunks that were cut from say pdf  documents, and then presenting the prompt (gpt4) with the relevant  document chunks.

My question is :  has anyone done a comparative analysis of  text-embedding-ada-002 versus  other embeddings? A less technical version of this is, is   text-embedding-ada-002 the best one out there to use? Thanks!"
286,machinelearning,openai,relevance,2017-08-13 23:34:26,[N] OpenAI bot was defeated at least 50 times yesterday,luiscosio,False,0.93,258,6timtv,https://twitter.com/riningear/status/896297256550252545,93,1502667266.0,
287,machinelearning,openai,relevance,2023-08-10 22:55:52,[D] OpenAI API function calling,neuro_boogie,False,0.5,0,15nr8j3,https://www.reddit.com/r/MachineLearning/comments/15nr8j3/d_openai_api_function_calling/,1,1691708152.0,"How do you think OpenAI implemented the function calling feature, It seems like another contextual generation piece from the look of it but any interesting ideas and papers around this topic?"
288,machinelearning,openai,relevance,2017-06-21 00:41:00,[N] Andrej Karpathy leaves OpenAI for Tesla ('Director of AI and Autopilot Vision'),gwern,False,0.93,396,6iib9r,https://techcrunch.com/2017/06/20/tesla-hires-deep-learning-expert-andrej-karpathy-to-lead-autopilot-vision/?,98,1498005660.0,
289,machinelearning,openai,relevance,2021-02-25 00:31:22,[N] OpenAI has released the encoder and decoder for the discrete VAE used for DALL-E,Wiskkey,False,0.97,391,lrroom,https://www.reddit.com/r/MachineLearning/comments/lrroom/n_openai_has_released_the_encoder_and_decoder_for/,69,1614213082.0,"Background info: [OpenAI's DALL-E blog post](https://openai.com/blog/dall-e/).

Repo: [https://github.com/openai/DALL-E](https://github.com/openai/DALL-E).

[Google Colab notebook](https://colab.research.google.com/github/openai/DALL-E/blob/master/notebooks/usage.ipynb).

Add this line as the first line of the Colab notebook:

    !pip install git+https://github.com/openai/DALL-E.git

I'm not an expert in this area, but nonetheless I'll try to provide more context about what was released today. This is one of the components of DALL-E, but not the entirety of DALL-E. This is the DALL-E component that generates 256x256 pixel images from a [32x32 grid of numbers, each with 8192 possible values](https://www.reddit.com/r/MachineLearning/comments/kr63ot/r_new_paper_from_openai_dalle_creating_images/gi8wy8q/) (and vice-versa). What we don't have for DALL-E is the language model that takes as input text (and optionally part of an image) and returns as output the 32x32 grid of numbers.

I have 3 non-cherry-picked examples of image decoding/encoding using the Colab notebook at [this post](https://www.reddit.com/r/MediaSynthesis/comments/lroigk/for_developers_openai_has_released_the_encoder/).

**Update**: The [DALL-E paper](https://www.reddit.com/r/MachineLearning/comments/lrx40h/r_openai_has_released_the_paper_associated_with/) was released after I created this post.

**Update**: A Google Colab notebook using this DALL-E component has already been released: [Text-to-image Google Colab notebook ""Aleph-Image: CLIPxDAll-E"" has been released. This notebook uses OpenAI's CLIP neural network to steer OpenAI's DALL-E image generator to try to match a given text description.](https://www.reddit.com/r/MachineLearning/comments/ls0e0f/p_texttoimage_google_colab_notebook_alephimage/)"
290,machinelearning,openai,relevance,2023-05-26 16:26:54,[D] Mining OpenAI for competitor data,deviantkindle,False,0.63,4,13shrc6,https://www.reddit.com/r/MachineLearning/comments/13shrc6/d_mining_openai_for_competitor_data/,6,1685118414.0,"IIUC, any data sent via the chatGPT interface can (and will?) be used in training. Conversely, any data submitted via the API is not used for training. Correct?

If so, how feasible is the following scenario: InternA inadvertently uploads confidential info about CompanyA vi the chatGPT prompt. Why couldn't EvilCompetitor use chatGPT/API to search for such confidential  information?

I'm not (currently) looking for a way to solve this problem; I'm looking to see if it _is_ a problem.
So no local LLM or special enterprise-y guardrails (""For only $10,000/month! But wait! There's more!""), or suggestions that ""the IT department should have...""."
291,machinelearning,openai,relevance,2023-07-03 14:40:53,[R] Chat with documents using LangChain and OpenAI,Kimvadanti,False,0.44,0,14pkxir,https://www.reddit.com/r/MachineLearning/comments/14pkxir/r_chat_with_documents_using_langchain_and_openai/,16,1688395253.0,"Over the past few months, I've been captivated by the flood of apps claiming to be the ultimate ""ChatGPT for your documents"" on Product Hunt. The question that lingered in my mind was, ""How do these apps actually work?"" Curiosity led me down an exciting path of discovery, and I stumbled upon a framework that I think is revolutionizing the world of app development in the context of Large Language Models - LangChain

I learned that developing a ""ChatGPT for your documents"" is easily achievable through three broad workflows combined with access to OpenAI API. In fact, I went ahead and prototyped such a system on streamlit. 

Step 1 - The Setup: Store your documents as embeddings.
In the first step, use Document Loaders (at least 100 are available), provided by LangChain to convert anything from a simple Word document to an AWS S3 directory into Documents. Then, using Document Transformers and Text Embedding Models, you transform your documents into embeddings. Finally, store these embeddings in a vector store for searches later on. It's a one-time setup that sets the foundation for your Q&A system. 💡🛠️

Step 2 - Establish Context: Find relevant documents.
LangChain's Text Embedding model converts user queries into vectors. These vectors are used by LangChain's retriever to search the vector store and retrieve the most relevant documents. You can control the search boundaries based on relevance scores or the desired number of documents. It's all about finding the right context for your Q&A system. 🎯💬

Step 3 - Chat Away: Get answers from LLMs.
Now comes the fun part! You pass the user's query and the established context to the Language Models (LLMs). The LLMs respond with precise answers, taking into account the provided context. It's like having a conversation with your documents. 🤩🗨️


By building these three workflows, a service that acts like a Q&A for a restricted set of documents can be set up. Of course, this is just an overview of the approach and all the complex steps of app development will still remain, I remain fascinated by how the good folks at LangChain have made things simpler. 

What do you think? Have you tried LangChain to build something? Is there any other framework that is equally fascinating?

#llm #openai #chatgpt #documents #generativeai #langchain"
292,machinelearning,openai,relevance,2024-02-18 03:40:45,[News] The Future of Video Production: How Sora by OpenAI is Changing the Game,Anirban_Hazra,False,0.13,0,1atkoj7,https://www.digitallynomad.in/post/the-future-of-video-production-how-sora-by-openai-is-changing-the-game,4,1708227645.0,
293,machinelearning,openai,relevance,2023-05-06 23:08:09,[P] OpenAI vs Open Source LLM Comparison for Document Q&A,georgesung,False,0.95,94,13a5baq,https://www.reddit.com/r/MachineLearning/comments/13a5baq/p_openai_vs_open_source_llm_comparison_for/,16,1683414489.0,"Ran a fun comparison between OpenAI vs open source (Apache 2.0) LLMs for Wikipedia document Q&A -- open source is looking good (and getting better).

TLDR:

For simple Wikipedia article Q&A, I compared OpenAI GPT 3.5, FastChat-T5, FLAN-T5-XXL, and FLAN-T5-XL. GPT 3.5 provided the best answers, but FastChat-T5 was very close in performance (with a basic guardrail). The T5 models I tested are all licensed under Apache 2.0, so they are commercially viable.

For the embedding model, I compared OpenAI text-embedding-ada-002 and the open source INSTRUCTOR-XL models. The INSTRUCTOR-XL model performed better, which is encouraging since INSTRUCTOR-XL is also licensed under Apache 2.0.

Full blog post:

[https://georgesung.github.io/ai/llm-qa-eval-wikipedia/](https://georgesung.github.io/ai/llm-qa-eval-wikipedia/)"
294,machinelearning,openai,relevance,2023-11-24 04:26:51,[P] Unlock Semantic Image Search Potential with OpenAI's CLIP Model!,Old-Antelope-4447,False,0.37,0,182jii8,https://www.reddit.com/r/MachineLearning/comments/182jii8/p_unlock_semantic_image_search_potential_with/,4,1700800011.0,"Discover how OpenAI's CLIP Model is changing the game. Say goodbye to traditional text or tag-based image searches. Now, you can search images based on their features.  
For instance, in a clothing retail scenario, imagine a customer searching for ""dark color clothes."" Even if your database lacks the exact tag, our Semantic Image Search powered by the CLIP AI Model can still match the features of the image and deliver the right results. It's a game-changer that text-based search can't replicate.  
I've broken down the process step by step, complete with code examples. Take a look and share your thoughts.

[https://medium.com/@vignesh865/next-gen-search-unleashed-the-fusion-of-text-and-semantics-redefining-how-we-search-c67161aaa517](https://medium.com/@vignesh865/next-gen-search-unleashed-the-fusion-of-text-and-semantics-redefining-how-we-search-c67161aaa517)"
295,machinelearning,openai,relevance,2018-07-18 16:39:06,[N] OpenAI Five Benchmark,thebackpropaganda,False,0.97,262,8zx2yf,https://blog.openai.com/openai-five-benchmark/,37,1531931946.0,
296,machinelearning,openai,relevance,2023-08-06 22:06:55,[P] Rust meets Llama2: OpenAI compatible API written in Rust,amindiro,False,0.88,66,15k254o,https://www.reddit.com/r/MachineLearning/comments/15k254o/p_rust_meets_llama2_openai_compatible_api_written/,5,1691359615.0,"Hello,

I have been working on an OpenAI-compatible API for serving LLAMA-2 models written entirely in Rust. It supports offloading computation to  Nvidia GPU and Metal acceleration for GGML models !

Here is the project  link: [Cria- Local LLAMA2 API](https://github.com/AmineDiro/cria)

You can use it as an OpenAI replacement (check out the included \`Langchain\` example in the project).

This is an ongoing project, I have implemented the \`embeddings\` and \`completions\` routes. The \`chat-completion\` route will be here very soon!

Really interested in your feedback and I would welcome any help :) !

&#x200B;

&#x200B;"
297,machinelearning,openai,relevance,2023-09-08 20:52:08,[P] MLOps for Vercel OpenAI chatbot infrastructure,kao-pulumi,False,0.67,2,16dluho,https://www.reddit.com/r/MachineLearning/comments/16dluho/p_mlops_for_vercel_openai_chatbot_infrastructure/,0,1694206328.0,"I used infrastructure as code (IaC) to provision and deploy Vercel's [next-openai example](https://github.com/vercel/ai/tree/main/examples/next-openai). IaC is useful because it applies the same rigor of application code development to infrastructure provisioning. Instead of manual point and click in a cloud console which can be unrepeatable or error-prone, you just store and change all infrastructure configurations as code in source control .

This example uses [Pulumi](https://www.pulumi.com/) which allows you to write the IaC in Python.

[https://github.com/aaronkao/vercel-py-openai-chatbot](https://github.com/aaronkao/vercel-py-openai-chatbot)"
298,machinelearning,openai,relevance,2021-07-17 19:27:09,[N] OpenAI disbands its robotics research team,regalalgorithm,False,0.99,99,omawwl,https://www.reddit.com/r/MachineLearning/comments/omawwl/n_openai_disbands_its_robotics_research_team/,39,1626550029.0,"Or at least, [this has now been low-key confirmed](https://venturebeat.com/2021/07/16/openai-disbands-its-robotics-research-team/) , presumably it happened a while ago.

>OpenAI has disbanded its robotics team after years of research into machines that can learn to perform tasks like [solving a Rubik’s Cube](https://venturebeat.com/2019/10/15/openai-teaches-a-robotic-hand-to-solve-a-rubiks-cube/). Company cofounder Wojciech Zaremba quietly revealed on a [podcast](https://www.youtube.com/watch?v=429QC4Yl-mA&t=1153s)  hosted by startup Weights & Biases that OpenAI has shifted its  focus to other domains, where data is more readily available.  
>  
>“So it turns out that we can make a gigantic progress whenever we  have access to data. And I kept all of our machinery unsupervised,  \[using\] reinforcement learning — \[it\] work\[s\] extremely well. There  \[are\] actually plenty of domains that are very, very rich with data. And  ultimately that was holding us back in terms of robotics,” Zaremba  said. “The decision \[to disband the robotics team\] was quite hard for  me. But I got the realization some time ago that actually, that’s for  the best from the perspective of the company.”

Not really surprising. I wonder if it's just too early to make much headway in general-purpose robotics? Kind of like neural nets did not really take off until we had GPUs+data, some key ingredients necessary for success are still just not there..."
299,machinelearning,openai,relevance,2023-10-03 12:33:09,"Openai api function call as ""gatekeeper keeper"" in app? [D]",Was_an_ai,False,0.08,0,16yqp8f,https://www.reddit.com/r/MachineLearning/comments/16yqp8f/openai_api_function_call_as_gatekeeper_keeper_in/,2,1696336389.0,"So I am building an app and have multiple functions the LLM can use. But some need to ensure they need specific data - for example if I say ""please add x to my todo for next monday"" it works but only if I ensure in the recent prompt is the day and date. Now if the user says this deep in the conversation this system info can be forgotten. And you can imagine other cases too.

So it seemed to me to give it all functions, but without properties and when one is ""used"" it actually triggers another call to the api with the background data needed for that particular function.

So basically any function call becomes two api calls. So a bit wasteful, but this seems right to me.

Any thoughts? Or anyone taken this approach?"
