,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,id,url,num_comments,created,body
0,deeplearning,chatgpt,top,2023-01-27 10:45:48,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.95,117,10mhyek,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
1,deeplearning,chatgpt,top,2023-04-05 01:36:40,Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT,Time_Key8052,False,0.95,84,12c43uu,https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/,19,1680658600.0,"Vicuna : ChatGPT Alternative, Open-Source, High Quality and Low Cost 

&#x200B;

[ Relative Response Quality Assessed by GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a072b91)

Vicuna-13B has demonstrated competitive performance against other open-source models, such as Stanford Alpaca, by fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.

Evaluation using GPT-4 as a judge shows that Vicuna-13B achieves more than 90% of the quality of OpenAI ChatGPT and Google Bard AI, while outperforming other models such as Meta LLaMA (Large Language Model Meta AI) and Stanford Alpaca in more than 90% of cases.

The cost of training Vicuna-13B is approximately $300.

The training and serving code, along with an online demo, are publicly available for non-commercial use.

&#x200B;

More Information : [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT](https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)

Discord Server : [https://discord.gg/h6kCZb72G7](https://discord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysorg](https://twitter.com/lmsysorg)"
2,deeplearning,chatgpt,top,2023-11-16 08:28:46,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,0.84,70,17whz6e,https://v.redd.it/qx68wbuf7o0c1,7,1700123326.0,
3,deeplearning,chatgpt,top,2023-01-29 22:39:07,[P] We built a browser extension that unlocks browser mode capabilities using ChatGPT: MULTI·ON: AI Web Co-Pilot powered by ChatGPT,DragonLord9,False,0.95,65,10okyg3,https://v.redd.it/2hw47h0b82fa1,8,1675031947.0,
4,deeplearning,chatgpt,top,2023-01-20 08:53:58,Gotcha,actual_rocketman,False,0.94,63,10gs1ik,https://i.redd.it/yyh41pnje7da1.jpg,5,1674204838.0,
5,deeplearning,chatgpt,top,2023-03-25 04:24:49,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.93,49,121agx4,https://www.reddit.com/r/deeplearning/comments/121agx4/do_we_really_need_100b_parameters_in_a_large/,54,1679718289.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
6,deeplearning,chatgpt,top,2023-02-11 06:59:00,⭕ New Open-Source Version Of ChatGPT,LesleyFair,False,0.88,41,10zepkt,https://www.reddit.com/r/deeplearning/comments/10zepkt/new_opensource_version_of_chatgpt/,3,1676098740.0,"GPT is getting competition from open-source.

A group of researchers, around the YouTuber [Yannic Kilcher](https://www.ykilcher.com/), have announced that they are working on [Open Assistant](https://github.com/LAION-AI/Open-Assistant). The goal is to produce a chat-based language model that is much smaller than GPT-3 while maintaining similar performance.

If you want to support them, they are crowd-sourcing training data [here](https://open-assistant.io/).

**What Does This Mean?**

Current language models are too big.

They require millions of dollars of hardware to train and use. Hence, access to this technology is limited to big organizations. Smaller firms and universities are effectively shut out from the developments.

Shrinking and open-sourcing models will facilitate academic research and niche applications.

Projects such as Open Assistant will help to make language models a commodity. Lowering the barrier to entry will increase access and accelerate innovation.

What an exciting time to be alive! 

Thank you for reading! I really enjoyed making this for you!  
The Decoding ⭕ is a thoughtful weekly 5-minute email that keeps you in the loop about machine research and the data economy. [Click here to sign up](https://thedecoding.net/)!"
7,deeplearning,chatgpt,top,2022-12-12 17:29:39,ChatGPT context length,Wild-Ad3931,False,0.95,29,zk5esp,https://www.reddit.com/r/deeplearning/comments/zk5esp/chatgpt_context_length/,10,1670866179.0,"How come ChatGPT can follow entire discussions whereas nowaday's LLM are limited (to the best of my knowledge) 4096 tokens ?

I asked it and it is not able to answer neither, and I found nothing on Google because no paper is published. I was also curious to understand how come Beamsearch was so fast with ChatGPT.

https://preview.redd.it/o6djsmpb5i5a1.png?width=1126&format=png&auto=webp&s=98baae5bf6fa294db408f0530214f8afa8a32a0b"
8,deeplearning,chatgpt,top,2023-03-05 11:10:56,LLaMA model parallelization and server configuration,ChristmasInOct,False,0.97,24,11ium8l,https://www.reddit.com/r/deeplearning/comments/11ium8l/llama_model_parallelization_and_server/,8,1678014656.0,"Hey everyone,

First of all, tldr at bottom, typed more than expected here.  

Please excuse the rather naive perspective I have here.  I've followed along with great interest, but this is not my industry.

Regardless, I have spent the past 3-4 days falling down a brutally obsessive rabbit hole, and I cannot seem to find this information.  I'm assuming it's just that I am missing context of course, and regardless of whether there is a clear answer, I'm trying to get a better understanding of this topic so that I could better appraise the situation myself.

Really I suppose I have two questions.  **The first** is regarding model parallelization.

I'm assuming this is not generic whatsoever.  What is the typical process engineers go about for designing such a pipeline?  Specifically in regards to these new LLaMA models, is something like ALPA relevant?  Deepspeed?

More importantly, what information should I be seeking to determine this myself?

This roughly segues to my **second inquiry**.

The reason I'm curious about splitting the model pipeline etc., is that I am potentially in interested in standing a server up for this.  Although I don't have much of a budget for this build (\~$30-40K is the rough top-end, but I'd be a lot happier around $20-25K), the money is there if I can genuinely satisfy my use-case.

I work at a small, but borderline manic startup working on enterprise software; 90% of the work we're doing based in the react/node ecosystem, some low-level work for backend services, and some very interesting database work that I have very little to do with.  I am a fullstack engineer that grew up playing with C++ => C#, and somehow ended up spending all of my time r/w'ing javascript.  Lol.  Anyways.

Part of our roadmap since GPT-3 and the playground were made publicly accessible, involves usage of these transformer models, and their ability to interpret natural language inputs, whether from user inputs, or scraped input values generated somewhere in a chain of requests / operations.

Seeing GPT-3 in action made me specifically realize that my estimations on this technology had been wildly off.  Seeing ChatGPT in action and uptick, the API's becoming available, has me further panicked.

Running our inference through their API has never really been an option for us.  I haven't even really looked that far into it, but bottom line the data running through our platform is all back-office, highly sensitive business information, and many have agreements explicitly restricting the movement of data to or from any cloud services, with Microsoft, Amazon, and Google all specifically mentioned.

Regardless of the reasoning for these contracts, the LLaMA release has had me obsessed over this topic in more detail than before, and whether or not I would be able to get this setup privately, for our use-case.

**To get to the actual second inquiry**:

Say I want to throw a budget rig together for this in a server cabinet.  Am I able to effectively parallelize the LLaMA model, well enough to justify going with 24GB VRAM 4090's in the rig?  Say I do so with DeepSpeed, or some of the standard model parallelization libraries.

Is the performance cost low enough to justify taking the extra compute here over 1/3 - 1/2 as many RTX6000 ADA's?

Or should I be grabbing the 48GB ADA's?

Like I said, I apologize for the naivety, I'm really looking for more information so that I can start to put this picture together better on my own.  It really isn't the easiest topic to research with how quickly things seem to move, and the giant gap between conversation depths (gamer || phd in a lot of the most interesting or niche discussions, little between).

Thank you very much for your time.

TL;DR - Any information on LLaMA model parallelization at the moment?  Will it be compatible with things like zero or alpa?  How about for throwing a rig together right now for fine-tuning and then running inference on the LLaMA models?  48GB 6000 ADA's, or 24GB 4090's?

Planning on putting it in a mostly empty 42U cabinet that also houses our primary web server and networking hardware, so if there is a sales pitch for 4090's across multiple nodes here, I do have a massive bias as the kind of nerd that finds that kind of hardware borderline erotic.

Hydro and cooling are not an issue, just usage of the budget and understanding the requirements / approach given memory limitations, and how to avoid communication bottlenecks or even balance them against raw compute.

Thanks again everyone!"
9,deeplearning,chatgpt,top,2023-04-25 17:53:47,"Microsoft releases SynapseMl v0.11 with support for ChatGPT, GPT-4, causal learning, and more",mhamilton723,False,0.88,24,12yqpnp,https://www.reddit.com/r/deeplearning/comments/12yqpnp/microsoft_releases_synapseml_v011_with_support/,0,1682445227.0,"Today Microsoft launched SynapseML v0.11 with support for ChatGPT, GPT-4, distributed training of huggingface and torchvision models, an ONNX Model hub integration, Causal Learning with EconML, 10x memory reductions for LightGBM, and a newly refactored integration with Vowpal Wabbit. To learn more check out our release notes and please feel give us a star if you enjoy the project!

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.11.0](https://github.com/microsoft/SynapseML/releases/tag/v0.11.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919)

Thank you to all the contributors in the community who made the release possible!

&#x200B;

[What's new in SynapseML v0.11](https://preview.redd.it/9pqj1mowj2wa1.png?width=4125&format=png&auto=webp&s=a358e73760c847a09cc76f2ed17dc58e15aed5ed)"
10,deeplearning,chatgpt,top,2023-02-01 09:00:18,"Python wrapper of OpenAI's New AI classifier tool, which detects whether the paragraph was generated by ChatGPT, GPT models, or written by humans",StoicBatman,False,0.85,17,10qouv9,https://www.reddit.com/r/deeplearning/comments/10qouv9/python_wrapper_of_openais_new_ai_classifier_tool/,3,1675242018.0,"OpenAI has developed a new AI classifier tool which detects whether the content (paragraph, code, etc.) was generated by #ChatGPT, #GPT-based large language models or written by humans.

Here is a python wrapper of openai model to detect if a text is written by humans or generated by ChatGPT, GPT models

Github: [https://github.com/promptslab/openai-detector](https://github.com/promptslab/openai-detector)  
Openai release: [https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/](https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/)

If you are interested in #PromptEngineering, #LLMs, #ChatGPT and other latest research discussions, please consider joining our discord [discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)  


https://preview.redd.it/9d8ooeg2ljfa1.png?width=1358&format=png&auto=webp&s=0de7ccc5cd16d6bbad3dcfe7d8441547a6196fc8"
11,deeplearning,chatgpt,top,2023-03-22 21:55:31,ChatLLaMA – A ChatGPT style chatbot for Facebook's LLaMA,imgonnarelph,False,0.93,18,11yy5es,https://chatllama.baseten.co/,2,1679522131.0,
12,deeplearning,chatgpt,top,2023-03-09 00:50:22,"AI generated video chapter titles (YouTube, Vimeo, etc)",happybirthday290,False,0.91,19,11mdvb9,https://i.redd.it/h6utxsxg2mma1.png,1,1678323022.0,
13,deeplearning,chatgpt,top,2023-09-29 14:02:33,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.88,17,16vch0x,https://www.reddit.com/r/deeplearning/comments/16vch0x/this_week_in_ai_all_the_major_ai_developments_in/,2,1695996153.0,"1. **Meta AI** presents **Emu**, a quality-tuned latent diffusion model for generating highly aesthetic images. Emu significantly outperforms SDXLv1.0 on visual appeal.
2. **Meta AI** researchers present a series of long-context LLMs with context windows of up to 32,768 tokens. LLAMA 2 70B variant surpasses gpt-3.5-turbo-16k’s overall performance on a suite of long-context tasks.
3. **Abacus AI** released a larger 70B version of **Giraffe**. Giraffe is a family of models that are finetuned from base Llama 2 and have a larger context length of 32K tokens\].
4. **Cerebras** and **Opentensor** released Bittensor Language Model, ‘**BTLM-3B-8K**’, a new 3 billion parameter open-source language model with an 8k context length trained on 627B tokens of SlimPajama. It outperforms models trained on hundreds of billions more tokens and achieves comparable performance to open 7B parameter models. The model needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models and is available with an Apache 2.0 license for commercial use.
5. **OpenAI** is rolling out, over the next two weeks, new voice and image capabilities in ChatGPT enabling ChatGPT to understand images, understand speech and speak. The new voice capability is powered by a new text-to-speech model, capable of generating human-like audio from just text and a few seconds of sample speech. .
6. **Mistral AI**, a French startup, released its first 7B-parameter model, **Mistral 7B**, which outperforms all currently available open models up to 13B parameters on all standard English and code benchmarks. Mistral 7B is released in Apache 2.0, making it usable without restrictions anywhere.
7. **Microsoft** has released **AutoGen** \- an open-source framework that enables development of LLM applications using multiple agents that can converse with each other to solve a task. Agents can operate in various modes that employ combinations of LLMs, human inputs and tools.
8. **LAION** released **LeoLM**, the first open and commercially available German foundation language model built on Llama-2
9. Researchers from **Google** and **Cornell University** present and release code for DynIBaR (Neural Dynamic Image-Based Rendering) - a novel approach that generates photorealistic renderings from complex, dynamic videos taken with mobile device cameras, overcoming fundamental limitations of prior methods and enabling new video effects.
10. **Cloudflare** launched **Workers AI** (an AI inference as a service platform), **Vectorize** (a vector Database) and **AI Gateway** with tools to cache, rate limit and observe AI deployments. Llama2 is available on Workers AI.
11. **Amazon** announced the general availability of **Bedrock**, its service that offers a choice of generative AI models from Amazon itself and third-party partners through an API.
12. **Spotify** has launched a pilot program for AI-powered voice translations of podcasts in other languages - in the podcaster’s voic. It uses OpenAI’s newly released voice generation model.
13. **Getty Images** has launched a generative AI image tool, ‘**Generative AI by Getty Images**’, that is ‘commercially‑safe’. It’s powered by Nvidia Picasso, a custom model trained exclusively using Getty’s images library.
14. **Optimus**, Tesla’s humanoid robot, can now sort objects autonomously and do yoga. Its neural network is trained fully end-to-end.
15. **Amazon** will invest up to $4 billion in Anthropic. Developers and engineers will be able to build on top of Anthropic’s models via Amazon Bedrock.
16. **Google Search** indexed shared Bard conversational links into its search results pages. Google says it is working on a fix.

&#x200B;

  
My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
14,deeplearning,chatgpt,top,2023-03-10 05:22:26,[POC] ChatGPT Audio Bot (like Google Assistant and Alexa) - Opensource,JC1DA,False,0.83,18,11nfrhw,https://www.reddit.com/r/deeplearning/comments/11nfrhw/poc_chatgpt_audio_bot_like_google_assistant_and/,2,1678425746.0,"Hey guys, just wanna share this bot that I quickly built using ChatGPT API.

GitHub page: [https://github.com/LanyTek/ChatGPT\_Audio\_Bot](https://github.com/LanyTek/ChatGPT_Audio_Bot)

This service allows you to keep talking to the bot using your voice like you often do with Google Assistant or Alexa. It can be used in a lot of scenarios like teaching, gossiping, or quickly retrieving information without the need of typing. 

I have a quick video demo here if you wanna check [https://www.youtube.com/watch?v=e9n0BJfMyKw](https://www.youtube.com/watch?v=e9n0BJfMyKw)

It's open source so feel free to use it for anything you like :)"
15,deeplearning,chatgpt,top,2023-04-07 10:58:54,Text-to-image Diffusion Models in Generative AI: A Survey,Learningforeverrrrr,False,0.87,14,12ehc2m,https://www.reddit.com/r/deeplearning/comments/12ehc2m/texttoimage_diffusion_models_in_generative_ai_a/,0,1680865134.0,"Diffusion models have become a SOTA generative modeling method for numerous content types, such as images, audio, graph, etc. As the number of articles on diffusion models has grown exponentially over the past few years, there is an increasing need for survey works to summarize them. Recognizing the existence of such works, our team has completed multiple field-specific surveys on diffusion models. We promote our works here and hope they can be helpful to researchers in relative fields: text-to-image diffusion models [\[a survey\]](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey), audio diffusion models [\[a survey\]](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI), and graph diffusion models [\[a survey\]](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material) .

In the following, we briefly summarize our survey on text-to-image diffusion models.

[Text-to-image Diffusion Models in Generative AI: A Survey](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)

As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis, i.e., text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.

Moreover, we have also completed two survey works on generative AI (AIGC) [\[a survey\]](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need) and ChatGPT [\[a survey\]](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era), respectively. Interested readers may give it a look."
16,deeplearning,chatgpt,top,2023-03-13 12:50:10,Learning logical relationships with neural networks with differential ILP,Neurosymbolic,False,1.0,13,11q8tir,https://www.reddit.com/r/deeplearning/comments/11q8tir/learning_logical_relationships_with_neural/,3,1678711810.0,"Since last week’s post on my lab’s software package [PyReason](https://neurosymoblic.asu.edu/pyreason/), I got a lot of questions on if it would be possible for a neural network to learn logical relationships from data. After all, ChatGPT seems to be able to generate Python code, and Meta released a NeurIPS paper to show you can learn math equations from data using a transformer-based model, so why not logic? In this article, we will one line of research in this area – differential ILP.

The research in this area really kicked off with a 2018 [paper from DeepMind](https://www.reddit.com/r/deeplearning/jair.org/index.php/jair/article/view/11172) where Richard Evans and Edward Grefenstette showed that you could adapt techniques from “inductive logic programming” to use gradient descent, and learn logical rules from data. Previous (non-neural) work on inductive logic programming was generally not designed to work with noisy data and instead fit the historical examples in a precise manner. Evans and Grefenstette utilized a neural architecture and a loss function – and they showed they could handle noisy data and even do some level of integration with CNN’s. Their neural architecture mimicked a set of candidate logical rules – and the rules assigned higher weights by gradient descent would be thought to best fit the data. However, a downside to this approach is that the neural network was [quintic in the size of the input](https://www.youtube.com/watch?v=SOnAE0EyX8c&list=PLpqh-PUKX-i7URwnkTqpAkSchJHvbxZHB&index=5). This is why they only applied their approach on very small problems – it did not see very wide adoption.

That said, in the last two years, there have been some notable follow-ons to this work. Researchers out of Kyoto University and NTT introduced a manner to learn rules that are more expressive in a different manner by allowing function symbols in the logical language ([Shindo et al., AAAI 2021](https://ojs.aaai.org/index.php/AAAI/article/view/16637/16444)). They leverage a clause search and refinement process to limit the number of candidate rules – hence limiting the size of the neural network. A student team from ASU created a presentation on their work for our recent seminar course on neuro symbolic AI. We released a three part video series from their talk:

[Part 1: Review of differentiable inductive logic programming](https://www.youtube.com/watch?v=JIS78a40q8U&t=270s)

[Part 2: Clause search and refinement In our recent video series](https://www.youtube.com/watch?v=nzfbxlHUwuE&t=345s)

[Part 3: Experiments](https://www.youtube.com/watch?v=-fKWNtHUIN0&t=27s)

[Slides](https://labs.engineering.asu.edu/labv2/wp-content/uploads/sites/82/2022/10/Shindo_dILP.pdf)

Some think that the ability to learn such relationships will represent a significant advancement in ML, specifically addressing shortcomings in areas such as knowledge graph completion and reasoning about scene graphs. However, the gap still remains wide, and ILP techniques, including differentiable ILP still have a ways to go. Really interested in what your thoughts are, feel free to comment below."
17,deeplearning,chatgpt,top,2023-04-02 18:10:37,Should we draw inspiration from Deep learning/Computer vision world for fine-tuning LLMs?,Vegetable-Skill-9700,False,0.74,10,129t3tl,https://www.reddit.com/r/deeplearning/comments/129t3tl/should_we_draw_inspiration_from_deep/,8,1680459037.0,"With HuggingGPT, BloombergGPT, and OpenAI's chatGPT store, it looks like the world is moving towards specialized GPTs for specialized tasks. What do you think are the best tips & tricks when it comes to fine-tuning and refining these task-specific GPTs?

Over the last decade, I have built many computer vision models (for human pose estimation, action classification, etc.), and our general approach was always based on Transfer learning. Take a state-of-the-art public model and fine-tune it by collecting data for the given use case.

Do you think that paradigm still holds true for LLMs?

Based on my experience, I believe observing the model's performance as it interacts with real-world data, identifying failure cases (where the model's outputs are wrong), and using them to create a high-quality retraining dataset will be the key.

&#x200B;

P.S. I am building an open-source project UpTrain ([https://github.com/uptrain-ai/uptrain](https://github.com/uptrain-ai/uptrain)), which helps data scientists to do so. We just wrote a blog on how this principle can be applied to fine-tune an LLM for a conversation summarization task. Check it out here: [https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation\_summarization](https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation_summarization)"
18,deeplearning,chatgpt,top,2023-11-15 20:27:19,"Nvidia introduces the H200, an AI-crunching monster GPU that may speed up ChatGPT",keghn,False,0.88,12,17w2vgj,https://arstechnica.com/information-technology/2023/11/nvidia-introduces-its-most-powerful-gpu-yet-designed-for-accelerating-ai/,0,1700080039.0,
19,deeplearning,chatgpt,top,2024-01-10 06:21:19,Are NLP based AI skills useless?,SnooBeans7516,False,0.71,12,1931m9b,https://www.reddit.com/r/deeplearning/comments/1931m9b/are_nlp_based_ai_skills_useless/,9,1704867679.0,"So something's been on my mind recently and I wanted to get Reddit's thoughts.

The thing that is troubling me as I get deeper, is it seems that for a lot of language based NLP tasks, ChatGPT or these other foundation models seem SOTA for 99% of tasks. In terms of developing valuable skills and products, it feels like the two options are: 

1. Research and training foundation models at large research labs 

2. Using GPT APIs in some sort of system (RAG or just raw prompts)

I feel like this issue is rooted in language itself being a somewhat static medium that can be mostly mastered by a single model. As opposed to images where we see a lot of cool projects from people that have to employ many different techniques to complete a specific task.

\## Example

Unlike images, the use cases seem much more straightforward, and don't require any special work other than throwing GPT at it. 

For example, I wanted to create a BERT like model to replace ingredients in a recipe, conditioned on a user's requirements or desires. The problem is, the more I worked on this and trained some models, the more it seemed that ChatGPT could just do it better with some prompt engineering...

==================

All this to say, I was previously motivated to be a part-time NLP researcher, implementing research into products in cool ways. Now it kind of feels like the only actually valuable investment is into ""AI engineering"", building systems using a GPT API:/"
20,deeplearning,chatgpt,top,2023-02-17 02:54:52,How likely is ChatGPT to be weaponized as an information pollution tool? What are the possible implementation paths? How to prevent possible attacks?,zcwang0702,False,0.71,10,1148t20,https://www.reddit.com/r/deeplearning/comments/1148t20/how_likely_is_chatgpt_to_be_weaponized_as_an/,14,1676602492.0,
21,deeplearning,chatgpt,top,2023-08-02 11:33:32,Using PDFs with GPT Models,Disastrous_Look_1745,False,0.8,12,15g6i4x,https://www.reddit.com/r/deeplearning/comments/15g6i4x/using_pdfs_with_gpt_models/,2,1690976012.0,Found a blog talking about how we can interact with PDFs in Python by using GPT API & Langchain. It talks about some pretty cool automations you can build involving PDFs - [https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/](https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/)
22,deeplearning,chatgpt,top,2023-03-15 00:07:51,GPTMinusOne - AI that hides the use of ChatGPT and GPT4,tomd_96,False,0.82,10,11rfgbs,https://github.com/tom-doerr/gpt_minus_one,3,1678838871.0,
23,deeplearning,chatgpt,top,2023-04-07 10:28:52,"Series of Surveys on ChatGPT, Generative AI (AIGC), and Diffusion Models",Learningforeverrrrr,False,0.87,10,12egmab,https://www.reddit.com/r/deeplearning/comments/12egmab/series_of_surveys_on_chatgpt_generative_ai_aigc/,0,1680863332.0,"* **A survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)
* **A survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)
* **A survey on Text-to-image diffusion models:** [**Text-to-image Diffusion Models in Generative AI: A Survey**](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)
* **A survey on Audio diffusion models:** [**A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI**](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI)
* **A survey on Graph diffusion models:** [**A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material**](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material)

**ChatGPT goes viral.** Launched by OpenAI on November 30, 2022, ChatGPT has attracted unprecedented attention due to its powerful abilities all over the world.  It took only 5 days \[1\] and 2 months \[2\] for ChatGPT to have 1 million users and 100 million monthly users after launch, making it the fastest-growing consumer application in history. ChatGPT can be seen as the milestone for the GPT family to go viral. In academia, ChatGPT has also inspired a large number of works discussing its applications in multiple fields, with **more than 500 papers within four months** after release and **the number is still increasing rapidly.**  This brings a huge challenge for a researcher who hopes to have an overview of ChatGPT applications or hopes to start his or her journey with ChatGPT in their own field.  **To help more people keep up with the latest progress of the GPT family,** we’re glad to share a self-contained survey that not only summarizes **the recent applications** of ChatGPT and other GPT variants like GPT-4, but also introduces the **underlying techniques** and **challenges.** Please refer to the following link for the paper: [One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era).

&#x200B;

**From ChatGPT to Generative AI.**  One highlighting ability of the GPT family is that it can generate natural languages, which falls into the area of Generative AI. Apart from text, Generative AI can also generate content in other modalities, such as image, audio, and graph. More excitingly, Generative AI is able to convert data from one modality to another one, such as the text-to-image task (generating images from text). **To help readers have a better overview of Generative AI,** we provide a complete survey on underlying **techniques,** summary and development of **typical tasks in academia**, and also **industrial applications.** Please refer to the following link for the paper.  [A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

&#x200B;

**From Generative AI to Diffusion Models.** The prosperity of a field is always driven by the development of technology, and so is Generative AI.  Different from ChatGPT which generates text based on the transformer, **diffuson models** have greatly accelerated the development of other fields in Generative AI, such as image synthesis.  Although we provide a summary of diffusion models and typical tasks in the Generative AI survey, we cannot include detailed discussions due to paper length limitations. **For those who are interested in the technical details of diffusion models and the recent progress of their applications in Generative AI,** we provide three self-contained surveys on **how diffusion models are applied in three typical areas: Text-to-image diffusion models** (also includes related tasks such as image editing)**, Audio diffusion models** (including text to speech synthesis and enhancement), and **Graph diffusion models** (including molecule, protein and material areas). Please refer to the following links for the paper.

* [Text-to-image Diffusion Models in Generative AI: A Survey](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)
* [A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI)
* [A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material)

We hope our survey series will help people for a better understanding of ChatGPT and Generative AI, and we will update the survey regularly to include the latest progress. Please refer to the personal pages of the authors for the latest updates on surveys. If you have any suggestions or problems, please feel free to contact us.

\[1\] Greg Brockman, co-founder of OpenAI, [https://twitter.com/gdb/status/1599683104142430208?lang=en](https://twitter.com/gdb/status/1599683104142430208?lang=en)

\[2\] Reuters, [https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/)"
24,deeplearning,chatgpt,top,2023-05-13 22:42:26,Domain specific chatbot. Semantic search isn't enough.,mldlbr,False,0.82,7,13gv1zj,https://www.reddit.com/r/deeplearning/comments/13gv1zj/domain_specific_chatbot_semantic_search_isnt/,8,1684017746.0,"Hi guys, I'm struggling to find a reliable solution to this specific problem.

I  have a huge dataset with chat conversations, about several topics. I  want to ask questions and retrieve information about these conversations in a chatbot way.

I have tried  semantic search with chatGPT to answer questions about these  conversations. The problem is that semantic search only returns top  similar sentences, and doesn't ‘read’ all conversations, that’s not  enough to answer generic questions, just very specific ones. For  example, if I ask “What are these people talking about person X?” it  will return only the top sentences (through semantic similarity) and  that will not tell the whole story. The LLM’s models have a limit of  tokens, so I can’t send the whole dataset as context.

Is there any approach to giving a reliable answer based on reading all the messages?

Any ideas on how to approach this problem?"
25,deeplearning,chatgpt,top,2023-01-14 14:48:43,Scaling Language Models Shines Light On The Future Of AI ⭕,LesleyFair,False,0.75,8,10bq685,https://www.reddit.com/r/deeplearning/comments/10bq685/scaling_language_models_shines_light_on_the/,1,1673707723.0,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs might allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* reduce hallucination and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example is words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (a proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/jlh726eku0ca1.png?width=800&format=png&auto=webp&s=55d170251a967f31b36f01864af6bb7e2dbda253)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week.No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)"
26,deeplearning,chatgpt,top,2023-03-11 00:38:10,Generate READMEs Using ChatGPT,tomd_96,False,0.86,9,11o5zyl,https://www.reddit.com/r/deeplearning/comments/11o5zyl/generate_readmes_using_chatgpt/,0,1678495090.0,"&#x200B;

https://i.redd.it/k375our2a0na1.gif

&#x200B;

You can use this program I wrote to generate readmes: [https://github.com/tom-doerr/codex-readme](https://github.com/tom-doerr/codex-readme)

&#x200B;

It's far from perfect, but I now added ChatGPT and it is surprisingly good at inferring what the project is about. It often generates interesting usage examples and explains the available command line options.

&#x200B;

You probably won't yet use this for larger projects, but I think this can make sense for small projects or single scripts. Many small scripts are very useful but might never be published because of the work that is required to document and explain it. Using this AI might assist you with that.

&#x200B;

Reportedly GPT-4 is coming out next week, which probably would make it even better.

&#x200B;

What do you think?"
27,deeplearning,chatgpt,top,2023-04-09 19:34:12,"ChatGPT for free now , GPT4ALL is now here",oridnary_artist,False,0.79,8,12gt77m,https://www.youtube.com/watch?v=WiCYfi3SUTE&t=1s,0,1681068852.0,
28,deeplearning,chatgpt,top,2022-12-10 22:44:46,InstructGPT,MRMohebian,False,0.73,5,zi62fr,https://www.reddit.com/r/deeplearning/comments/zi62fr/instructgpt/,1,1670712286.0,"Recently, ChatGPT became trendy. It was adopting an algorithm by the name of InstructGPT. 
We go through the paper ""Training Language Models to Follow Instructions with Human Feedback"" in this video and go into extensive detail about how InstructGPT works. 

Please subscribe, leave a comment and share with your friends.

[R]
https://youtu.be/lYRWzCPGM2Q"
29,deeplearning,chatgpt,top,2023-03-02 07:25:30,Good news for builders! OpenAI Releases APIs To ChatGPT and Whisper,LesleyFair,False,0.69,6,11fwcxf,https://www.reddit.com/r/deeplearning/comments/11fwcxf/good_news_for_builders_openai_releases_apis_to/,0,1677741930.0,"If you were as disappointed as I was when you saw that access to Meta's LLaMA models is limited to researchers, you are going to like this.  


[APIs to ChatGPT and OpenAI's speech-to-text model whisper](https://openai.com/blog/introducing-chatgpt-and-whisper-apis) are available as of yesterday. Through system-wide optimizations, they claim to have reduced inference costs by 90%. They now price ChatGPT at $0.002 per 1000 tokens. Dedicated instances are available for speedup and make economic sense if you process \~450M tokens a day.  


Machine learning progress continues to be as fast as a banana peal skating on warm vaseline. 

If you found this useful and want to stay in the loop, consider subscribing to The Decoding. I send out a weekly 5-minute newsletter that keeps professionals in the loop about machine learning and the data economy. [Click here to subscribe!](https://thedecoding.net/)"
30,deeplearning,chatgpt,top,2022-12-05 01:45:02,Thread: Top 10 ways you can use ChatGPT for Music related stuff,dicklesworth,False,0.65,4,zct1o6,/r/musictheory/comments/zcso1s/thread_top_10_ways_you_can_use_chatgpt_for_music/,0,1670204702.0,
31,deeplearning,chatgpt,top,2023-04-25 23:50:32,Research on the political biases of ChatGPT,Mysterious_Potato132,False,0.87,6,12z08ni,/r/ChatGPT/comments/12yz49i/research_on_the_political_biases_of_chatgpt/,6,1682466632.0,
32,deeplearning,chatgpt,top,2023-04-19 13:51:18,Alpaca Electron: ChatGPT Locally!,oridnary_artist,False,0.73,7,12rtzak,https://youtu.be/0oz3RaLlTlM,0,1681912278.0,
33,deeplearning,chatgpt,top,2023-09-27 17:55:06,Advanced Query Engines in LlamaIndex - Concepts Explained + E2E Python Code Notebooks,CShorten,False,0.83,4,16trdr4,https://www.reddit.com/r/deeplearning/comments/16trdr4/advanced_query_engines_in_llamaindex_concepts/,0,1695837306.0,"Hey everyone! I am super excited to share Erika's 3rd Episode in our Llama Index and Weaviate series, covering the Advanced Query Engines in Llama Index. Here is a quick overview, the video will explain the concepts in further detail and then an End-to-End Python code demo (I am particularly proud of the SQL Router demo)

• SQL Router -- one of the most interesting products in the latest boom of LLMs is that we can connect Vector Search with SQL systems, routing queries with an LLM!!! We can also use the LLM to format the queries with Text-to-SQL prompts! Such an amazing thing that I didn't have on my bingo card before ChatGPT haha.

• Recursive Retrieval -- Even aside from LLMs, we can create more advanced search indexes by connecting indexes with each other - for example, first searching through descriptions of the tools available and then stepping into the documentation within that tool to find the more particular thing you need! This also can involve LLMs if for example the high-level search takes us into a structured table -- and now we call upon our good old Text-to-SQL LLM again.

• Self-Correcting Query Engine -- Quite a bizarre phenomenon of LLMs is that they are able to correct themselves by simply reflecting on their output. Llama Index presents a nice and simple solution to get running with this.

• Lastly is the most open-ended of the Advanced Query Engines... the Sub Question Query Engine. This describes asking the LLM to decompose the question or task into it's constituent sub-questions or sub-tasks and then compose the results together to serve the larger goal. For example, ""Did Aristotle Use a Laptop?"" --> ""When did Aristotle Live?"" & ""When were Laptops invented?""

I hope you find this video useful, we are more than happy to answer any questions or discuss any ideas you have about the content in the video!

https://www.youtube.com/watch?v=Su-ROQMaiaw"
34,deeplearning,chatgpt,top,2023-09-28 11:01:22,ChatGPT's Latest Upgrade: Access to Real-Time Information,marilaane,False,0.83,4,16ucwlq,https://www.brainyai.online/2023/09/chatgpts-latest-upgrade-access-to-real.html?m=1,0,1695898882.0,
35,deeplearning,chatgpt,top,2023-07-22 18:37:01,"LLAMA 2 - Model Explained, Demo and Comparison to ChatGPT",Combination-Fun,False,0.83,4,156roiv,https://www.reddit.com/r/deeplearning/comments/156roiv/llama_2_model_explained_demo_and_comparison_to/,0,1690051021.0,"LLAMA 2 is the largest and best opensource LLM every released free for commercial use. There have been several improvements that make LLAMA 2 better than LLAMA 1. Here is a video explaining the LLAMA 2 Model, a quick Demo of the model along with a Comparison to ChatGPT:

[https://youtu.be/TiloR3qRogs](https://youtu.be/TiloR3qRogs)

Hope its useful!"
36,deeplearning,chatgpt,top,2023-12-24 00:01:05,MLX Mixtral 8x7b on M3 max 128GB | Better than chatgpt?,gimel1213,False,1.0,3,18pildv,https://youtube.com/watch?v=mFIsZHSAzJ0&si=wq6uLlHzXdg-yL72,0,1703376065.0,
37,deeplearning,chatgpt,top,2023-06-17 20:54:57,LLMs for small projects,KrazedRook,False,0.67,2,14c1hgq,https://www.reddit.com/r/deeplearning/comments/14c1hgq/llms_for_small_projects/,3,1687035297.0,"I am looking fo a LLM to use for an app that I'm working on (for myself, it wont be published). I was originally goin to use ChatGPT but I have "" exceeded my current quota "" and I don't want to have to pay for this, so thats out of the question. I want to see if there are any others that I could use for my project, I'm using VS Code. If you have any I can use please explain it to me if you can in summary."
38,deeplearning,chatgpt,top,2023-02-06 00:07:43,ChatGPT: From Nowhere to Knowledgeable,crawfa,False,0.61,3,10urypd,https://www.reddit.com/r/deeplearning/comments/10urypd/chatgpt_from_nowhere_to_knowledgeable/,0,1675642063.0,"ChatGPT is taking the world by storm, and is now the fastest growing software application ever, eclipsing TikTok, which may soon be the fastest shrinking software application ever if it gets banned in the US. This article explains at a high level what ChatGPT is, how it works at a high level, what you can do with it, as well as some developer choices they have made and identifies some things that ChatGPT does not do well.

[https://open.substack.com/pub/stayblog/p/chatgpt-from-nowhere-to-knowledgeable?r=1qxkwn&utm\_campaign=post&utm\_medium=web](https://open.substack.com/pub/stayblog/p/chatgpt-from-nowhere-to-knowledgeable?r=1qxkwn&utm_campaign=post&utm_medium=web)"
39,deeplearning,chatgpt,top,2023-08-05 17:09:44,The Quest to Have Endless Conversations with Llama and ChatGPT 🗣️💬,JClub,False,0.8,3,15j1117,https://medium.com/@joaolages/the-quest-to-have-endless-conversations-with-llama-and-chatgpt-%EF%B8%8F-81360b9b34b2,0,1691255384.0,
40,deeplearning,chatgpt,top,2023-05-31 07:08:07,Question about Neural Nets,yanggang20202024,False,0.55,1,13wf18y,https://www.reddit.com/r/deeplearning/comments/13wf18y/question_about_neural_nets/,3,1685516887.0,"I recently read an article about how the supercomputer used to train Chatgpt consisted of something like 10,000 gpus.

My question is, do these supercomputers that train neural nets always get better when more gpus are added? Or is it a situation where progress flattens to such a degree at some point that it makes no sense to make the supercomputer any bigger?"
41,deeplearning,chatgpt,top,2022-12-05 02:22:37,Building A Virtual Machine Inside ChatGPT,x_abyss,False,0.81,3,zctzmf,https://www.engraved.blog/building-a-virtual-machine-inside/,0,1670206957.0,
42,deeplearning,chatgpt,top,2023-02-17 12:18:21,"ChatGPT - model, alignment and training explained",Combination-Fun,False,0.67,2,114j09j,/r/ChatGPT/comments/114izlj/chatgpt_model_alignment_and_training_explained/,0,1676636301.0,
43,deeplearning,chatgpt,top,2023-05-29 13:38:03,"Ortus - Chat with YouTube | ChatGPT Chrome extension -> use it to learn ML, supports The AI Epiphany YT channel, and more to come!",gordicaleksa,False,0.56,1,13uv79y,https://www.youtube.com/watch?v=0V9Jw6haJHQ,1,1685367483.0,
44,deeplearning,chatgpt,top,2024-02-10 03:18:15,Building an AI that can mimic the Spectator Method by Benjamin Franklin?,kiwifreeze,False,1.0,2,1an6mfj,https://www.reddit.com/r/deeplearning/comments/1an6mfj/building_an_ai_that_can_mimic_the_spectator/,2,1707535095.0,"Hello, I'm a recent CS grad and aspiring game dev. 

 I want to get better at writing and one of the ways I've been doing this was using the [Spectator method by Benjamin Franklin](https://shanesnow.com/research/how-to-be-a-better-writer-ben-franklin) 

I have been doing this by hand recently and I've found it to be incredibly helpful for my writing chops. I do everything by hand, that is take notes on each individual sentence of whichever book and then try to rewrite after offsetting the time a bit so I've forgotten it and then compare my writing to the original to see what I'm lacking. 

Taking notes on each individual sentence has been tedious though, and I tried to get a way for ChatGPT to do this for me but it can't read PDFs and yet alone other book files (I'm guessing). It does have the capability however to make short sentiments of the meanings of each sentence in any paragraph of a book (I tested this with a public domain book like Pride and Prejudice but it stopped after awhile). 

Is it possible to write an AI to do this for me automatically instead so I don't have to take notes sentence by sentence? Like use the OpenAI api to build a personal app around, or even build an LLM (which I don't even know where I would start with tbh). 

I do have much experience with coding and have taken an Intro to AI course at my university, though I can't say how much I really paid attention. That being said, I'm willing and capable of learning. 

Any advice would be appreciated!"
45,deeplearning,chatgpt,top,2023-04-05 13:21:51,"New Weaviate Podcast (#42) - ChatGPT Plugin Marketplace, Alpaca Models, Semantic Search on S3, and more!",CShorten,False,0.76,2,12ck7ae,https://www.reddit.com/r/deeplearning/comments/12ck7ae/new_weaviate_podcast_42_chatgpt_plugin/,0,1680700911.0," I am beyond excited to share our latest Weaviate Podcast with Ethan Steininger! Ethan is the founder of Mixpeek and creator of Collie.ai!

Ethan began by explaining how he came into search through integrating MongoDB with the Lucene inverted index. Ethan continued explaining how his background in Sales Engineering helped him to see the recurring problems businesses are facing when trying to utilize the latest LLM and Vector Database technologies to solve their problems.

We then continued to take a tour of all sorts of topics in the AI Landscape from the impact of the ChatGPT Plugin Marketplace / New App Store for AI to the Stanford Alpaca models, the impact of LLMs for coding productivity and many more, even ending with Ethan's advice on stress management by getting into nature and our thoughts on the existential fear technologies like GPT-4 inspire in many and the implications of it on society.

I hope you enjoy the podcast, please let us know what you think!

[https://www.youtube.com/watch?v=EDPk1umuge0](https://www.youtube.com/watch?v=EDPk1umuge0)"
46,deeplearning,chatgpt,top,2023-02-01 15:20:25,Launching my first-ever open-source project and it might make your ChatGPT answers better,Vegetable-Skill-9700,False,0.75,2,10qx9po,https://www.reddit.com/r/deeplearning/comments/10qx9po/launching_my_firstever_opensource_project_and_it/,6,1675264825.0,"I am building UpTrain - an open-source ML diagnostic toolkit that recently got investment from YCombinator.

As you know no ML model is 100% accurate, and, further, their accuracy deteriorates over time 😣. Additionally, due to the black boxiness ⬛ nature of Large Language models, it's challenging to identify and fix their problems.

The tool helps ML practitioners to:
1. Understand how their models are performing in production
2. Catch edge cases and outliers to help them refine their models
3. Allow them to define custom monitors to catch under-performing data-points
4. Retrain the model on them to improve its accuracy

You can check out the project here: https://github.com/uptrain-ai/uptrain. Would love to hear feedback from the community!"
47,deeplearning,chatgpt,top,2023-12-21 15:21:16,Subjectivity in AI with Dan Shipper,CShorten,False,0.75,2,18npf5z,https://www.reddit.com/r/deeplearning/comments/18npf5z/subjectivity_in_ai_with_dan_shipper/,0,1703172076.0,"Hey everyone!! I am SUPER excited to publish the fourth and final episode of the AI-Native Database podcast series with Dan Shipper and Bob van Luijt!

I thought Dan brought such a unique perspective on how we write and create with ChatGPT and Vector DBs! We dove into how subjectivity, or personality, is reflected in AI systems. For example, imagine you ask an LLM: ""Could you plan a vacation to Miami?"". Compared to previous database or search engine technologies that will just spit out relevant information to the query, the LLM might respond: ""Are you sure you want to go to Miami? I think Fort Lauderdale is nicer at this time of year"".

The podcast is titled ""Subjectivity in AI"" because this is such a novelty of AI compared to previous technologies. How will this unique ""personality"" of LLMs (as well as vector embeddings, classifiers, and other model types) seep into all these new applications? Can we control it with prompts and RAG, training data, or some combination of the above? What do multi-agent systems look like, each with their own personality and intrinsic motivation? Further, what new art forms will arise because of this property of AI, what will the ""overdrive"" of AI use look like?

I hope you enjoy the podcast, the last in our series! This has been so much fun to record and publish!

[https://www.youtube.com/watch?v=prV5R3T6UqM](https://www.youtube.com/watch?v=prV5R3T6UqM)"
48,deeplearning,chatgpt,top,2023-09-18 12:40:11,"DeepMind co-founder predicts ""third wave"" of AI: machines talking to machines and people",Nalix01,False,0.75,2,16lugx7,https://www.reddit.com/r/deeplearning/comments/16lugx7/deepmind_cofounder_predicts_third_wave_of_ai/,2,1695040811.0,"DeepMind's co-founder, Mustafa Suleyman, anticipates a ""third wave"" of AI evolution where machines will interact with both humans and other machines.

If you want to stay ahead of the curve in AI and tech, [look here first](https://dupple.com/techpresso?utm_source=reddit&utm_medium=social&utm_campaign=post).

**The Evolution of AI Phases**

* **Initial Classification Phase**: This was the first wave, focusing on deep learning that classifies different types of input data, such as images and audio.
* **Current Generative Phase**: AI uses input data to create new data.
* **Upcoming Interactive Phase**: Machines will be able to perform tasks by conversing with other machines and humans. Users will give high-level objectives to their AI systems which will then take necessary actions, involving dialogues with other AIs and individuals.

**Interactive AI's Potential**

* **More than Just Automation**: This AI won't just be about following commands but will have the freedom and agency to execute tasks.
* **Closer to Sci-Fi**: Interactive AI is anticipated to be more similar to the artificial intelligence depicted in science fiction, with dynamic capabilities rather than being static.

**Current AI Landscape**:

* **Generative AI's Popularity**: Despite being a game-changer, enthusiasm for generative AI seems to be waning, with declining user growth and web traffic for tools like ChatGPT.
* **Inflection AI's ""Pi""**: Earlier this year, Suleyman's company released a ChatGPT rival named Pi, emphasizing its polite and conversational nature.

**PS:** **If you enjoyed this post**, you’ll love my [ML-powered newsletter](https://dupple.com/techpresso?utm_source=reddit&utm_medium=social&utm_campaign=post) that summarizes the best AI/tech news from 50+ media. It’s already being read by **6,500+** **professionals** from **OpenAI, Google, Meta**…"
49,deeplearning,chatgpt,top,2023-04-07 08:41:41,A survey on graph diffusion models,Learningforeverrrrr,False,1.0,2,12eejpe,https://www.reddit.com/r/deeplearning/comments/12eejpe/a_survey_on_graph_diffusion_models/,0,1680856901.0,"Diffusion models have become a SOTA generative modeling method for numerous content types, such as images, audio, graph, etc. As the number of articles on diffusion models has grown exponentially over the past few years, there is an increasing need for survey works to summarize them. Recognizing the existence of such works, our team has completed multiple field-specific surveys on diffusion models. We promote our works here and hope they can be helpful to researchers in relative fields: text-to-image diffusion models [\[a survey\]](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey), audio diffusion models [\[a survey\]](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI), and graph diffusion models [\[a survey\]](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material) .

In the following, we briefly summarize our survey work on graph diffusion models.

[https://www.researchgate.net/publication/369716257\_A\_Survey\_on\_Graph\_Diffusion\_Models\_Generative\_AI\_in\_Science\_for\_Molecule\_Protein\_and\_Material](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material)

We start with a summary of the progress of graph generation before diffusion models. The diffusion models are then concisely presented and graph generation is discussed in depth from a structural and application perspective. Moreover,  the currently popular evaluation datasets and metrics are covered. Finally, we summarize the challenges and research questions still facing the research community. This survey work might be a useful guidebook for researchers who are interested in exploring the potential of diffusion models for graph generation and related tasks.

Moreover, we have also completed two survey works on generative AI (AIGC) [\[a survey\]](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need) and ChatGPT [\[a survey\]](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era), respectively. Interested readers may give it a look."
50,deeplearning,chatgpt,top,2023-12-14 02:03:03,[D] Constructing an Efficient Knowledge Graph RAG Pipeline with LlamaIndex,Fit_Maintenance_2455,False,1.0,2,18hxo48,https://www.reddit.com/r/deeplearning/comments/18hxo48/d_constructing_an_efficient_knowledge_graph_rag/,4,1702519383.0,"Large Language Models (LLMs) such as ChatGPT and Bard exhibit remarkable abilities within their specialized areas of training. However, their constraints in handling new or private data inquiries are widely recognized.

Retrieval Augmented Generation (RAG) emerges as a solution to bridge this gap, allowing LLMs to access external knowledge sources. This article delves into RAG, examines its elements, and constructs a usable RAG workflow that harnesses the potential of LlamaIndex, a knowledge graph.

&#x200B;

Link: [https://medium.com/ai-advances/constructing-an-efficient-knowledge-graph-rag-pipeline-with-llamaindex-81a0a0b105b7](https://medium.com/ai-advances/constructing-an-efficient-knowledge-graph-rag-pipeline-with-llamaindex-81a0a0b105b7)  "
51,deeplearning,chatgpt,top,2023-05-23 14:14:45,New Weaviate Podcast - Unstructured!,CShorten,False,0.76,2,13ppstr,https://www.reddit.com/r/deeplearning/comments/13ppstr/new_weaviate_podcast_unstructured/,0,1684851285.0,"ChatWithPDF has been one of the most captivating applications of the latest wave of ChatGPT and pairing ChatGPT with Retrieval-Augmentation and Vector Databases! As exciting as this is, there is a glaring problem... how do I get the text data out of my PDFs?

This is the problem Unstructured is solving with 3 core abstractions: (1) Partitioning (visually looking at elements on a PDF / Webpage / Resume / Slidedeck / Receipt / ... extracting the text data and adding metadata such as ""header"", ""body"", or ""image caption"", (2) Cleaning (I'm sure everyone in this group who has worked with text data has seen these ridiculous character encoding problems, regex, and whitespace removals we need to clean our text data for NLP pipelines), and (3) Staging (this describes extracting the JSONs / etc. to pass this data into another system such as Weaviate as an example)

I really hope you enjoy the podcast -- I think these innovations are so exciting for unlocking our data into these LLM systems!

[https://www.youtube.com/watch?v=b84Q2cJ6po8](https://www.youtube.com/watch?v=b84Q2cJ6po8)"
52,deeplearning,chatgpt,top,2023-08-27 23:55:12,GPT4 Contextual Decomposition Template,InevitableSky2801,False,0.67,1,1636b99,https://www.reddit.com/r/deeplearning/comments/1636b99/gpt4_contextual_decomposition_template/,0,1693180512.0,"Complex tasks with LLMs like ChatGPT/GPT4 are best broken down by first asking ChatGPT to outline the steps and then asking the LLM to execute against those steps that it defined. I first came across this interesting technique on Twitter recently.

While it’s OK to do this once in OpenAI’s playground, it's difficult to make this repeatable and streamlined. When I wanted an LLM to do something complex, I wanted to be able to plug into a template instead of thinking about and setting up the contextual decomposition process.

I made this Contextual Decomposition Template to help solve this problem: [https://lastmileai.dev/workbooks/cllqfl5c600rdpgnhh2su2fa0](https://lastmileai.dev/workbooks/cllqfl5c600rdpgnhh2su2fa0)

With a document and objective, this template allows you to quickly get to the answer through defining intermediate steps and executing according. Parameters are set up so you can easily change the goal, document, and objective and click 'Run All' to get the final results.

Please let me know if you have feedback! I'm also very curious if you have other interesting techniques with complex tasks and workflows working with LLMs."
53,deeplearning,chatgpt,top,2023-06-07 14:12:57,New Weaviate Podcast - LLM Agents!,CShorten,False,1.0,1,143edrc,https://www.reddit.com/r/deeplearning/comments/143edrc/new_weaviate_podcast_llm_agents/,0,1686147177.0,"Hey everyone! I am SUPER excited to share our 51st Weaviate Podcast on keeping up with the latest in LLM Agents -- featuring Greg Kamradt from Data Independent and Colin Harmon from Nesh, we discussed all the abstractions and emerging ideas from LangChain, LlamaIndex, and miscellaneous other sources!

We discussed everything under the sun related to LLMs from Tool Use to Databases, Multi-Agent LLMs, Privacy, Personalization, the ChatGPT Marketplace, Fine-Tuning, Long Context Length LLMs, and more! I really hope you find it useful!

https://www.youtube.com/watch?v=iB4ki6gdAdc"
54,deeplearning,chatgpt,top,2023-10-16 14:41:46,Nobel laureate Dr Michael Levitt: AI will change everything forever,kirst31,False,1.0,1,1797kte,https://www.reddit.com/r/deeplearning/comments/1797kte/nobel_laureate_dr_michael_levitt_ai_will_change/,0,1697467306.0,"[https://www.youtube.com/watch?v=8ZV6o0EiuTo&t=61s](https://www.youtube.com/watch?v=8ZV6o0EiuTo&t=61s)

The decorated and respected scientist, an early adopter of ChatGPT and other AI technologies, reveals whether the rapid emergence of ever-more powerful machine learning tools will ultimately help or harm humanity as it changes our world beyond recognition."
55,deeplearning,chatgpt,top,2023-05-17 15:10:45,New Weaviate Podcast - ChatArena!,CShorten,False,0.6,1,13k4i21,https://www.reddit.com/r/deeplearning/comments/13k4i21/new_weaviate_podcast_chatarena/,0,1684336245.0,"Hey everyone! I am super excited to publish our newest Weaviate Podcast on ChatArena!  One of the most exciting ideas with the emergence of LLM capabilities is to plug multiple LLMs together in Multi-Agent games or environments! I think the world is collectively still scrambling to understand systems like this, but 2 applications are immediately obvious:  


1. Create intelligent artifacts by simulating conversations between role-playing agents -- say an LLM impersonator of Sam Altman and Gary Marcus debate proposals for AI safety or Michael Bronstein and Jure Leskovec discuss the future of Geometric Deep Learning and Graph Neural Networks
2. Evaluating LLMs -- Benchmarks unfortunately don't really capture the nuances of intelligence, but having say ChatGPT chat with Claude moderated by a Cohere LLM that rates which LLM was more intelligent across thousands of simulated conversations 😂 -- looks like an incredibly promising way to keep up with the flood of new LLMs entering the market!  

I learned so much from this podcast, it was easily one of my favorite conversations I've ever had across the 47 Weaviate podcast episodes we have published so far, I really hope you find it useful and interesting!https://www.youtube.com/watch?v=\_0ww8Q0Bq2w"
56,deeplearning,chatgpt,top,2024-01-11 11:25:53,What are some tips of curating a dataset to fine-tune a code-completion LLM?,janissary2016,False,1.0,1,193zhur,https://www.reddit.com/r/deeplearning/comments/193zhur/what_are_some_tips_of_curating_a_dataset_to/,0,1704972353.0,"Hi.

There is a new SDK that I am working on and I want to know what are some ways of automatically curating a dataset to train a code-completing LLM to deploy as a VSCode plugin? Hacky ways are appreciated. I was thinking of using chatgpt API to make numerous API calls to inflate a CSV with artificially generated prompts - code entries. There are available datasets of course but I want to tailor the code completion for this particular SDK.

Appreciate all answers."
57,deeplearning,chatgpt,top,2023-10-25 01:25:27,How we Built an Open-Source RAG-based ChatGPT Web App: Meet Our new AI Tutor!,OnlyProggingForFun,False,0.6,1,17ft66e,https://youtu.be/7ytyK6u3aAk,0,1698197127.0,
58,deeplearning,chatgpt,top,2023-01-19 11:43:34,Join us this Friday 6 pm EST for a fascinating discussion about the societal impact of large language models (LLMs) like ChatGPT,OnlyProggingForFun,False,1.0,1,10fzn3l,https://discord.gg/ehPqT6rym8?event=1063903315974443162,0,1674128614.0,
59,deeplearning,chatgpt,top,2024-01-18 05:16:45,How can I make LLM plot graphs/figures on my database with RAG?,HappyDataGuy,False,0.67,1,199ieeu,https://www.reddit.com/r/deeplearning/comments/199ieeu/how_can_i_make_llm_plot_graphsfigures_on_my/,1,1705555005.0,I want for LLM like ChatGPT to plot graph but not with code-interpretor which requires uploading the data to openai. is there any way to achieve this? 
60,deeplearning,chatgpt,top,2023-06-11 15:56:18,ChatGPT interrogating bugs and errors!!,Available-Bass-7575,False,0.55,1,146xgyf,https://youtu.be/zfIyIScu0oo,1,1686498978.0,
61,deeplearning,chatgpt,top,2023-05-02 23:42:54,Longgboi 64K+ Context Size / Tokens Trained Open Source LLM and ChatGPT / GPT4 with Code Interpreter - Trained Voice Generated Speech,CeFurkan,False,1.0,1,13649d4,https://www.youtube.com/watch?v=v6TBtyO5Sxg&deeplearning,0,1683070974.0,
62,deeplearning,chatgpt,top,2023-12-22 11:45:32,Team GPT's Picks: Top 15 AI Tools for 2024 Productivity,LongjmpingShower,False,0.67,1,18od3vg,https://www.reddit.com/r/deeplearning/comments/18od3vg/team_gpts_picks_top_15_ai_tools_for_2024/,0,1703245532.0,"Some of the best AI tools to increase productivity are Team-GPT, Notion AI, Asana Intelligence, Salesforce Einstein, Zia, HubSpot AI, Jasper, Canva AI, Copy AI, Zapier, IFTTT, Outreach, Otter, Rev, Trint. In this article, I review all the AI software mentioned above in detail by telling you their features, pros, cons, and pricing categorically.  
**Learn more>>>**[https://team-gpt.com/learn/chatgpt-for-work-course](https://team-gpt.com/learn/chatgpt-for-work-course)  


https://preview.redd.it/nxwv3gff3u7c1.png?width=824&format=png&auto=webp&s=72a5fd8feb3ee0199df5c25962014a0b45ed3678"
63,deeplearning,chatgpt,top,2023-06-28 15:56:28,"[SEEKING FEEDBACK] I built a Chrome Extension AI tool that brings ChatGPT directly to any website. It's powered by GPT4 with access to the web, email reply, auto-suggest prompt, summarize and read screen features.",RidiculusRex,False,0.67,1,14ld8h3,https://v.redd.it/xuumhvhc5s8b1,1,1687967788.0,
64,deeplearning,chatgpt,top,2023-11-29 16:18:08,Rudy Lai on Tactic Generate - Weaviate Podcast #78!,CShorten,False,0.67,1,186t892,https://www.reddit.com/r/deeplearning/comments/186t892/rudy_lai_on_tactic_generate_weaviate_podcast_78/,0,1701274688.0,"Hey everyone! I am SUPER excited to publish our 78th Weaviate Podcast with Rudy Lai, Co-Founder and CEO of Tactic Generate!

I think most people would agree that the viral success of ChatGPT has greatly aided by **pairing the LLM with a Chat GUI** (as well as of course instruction tuning it for chat). ChatGPT marked the dawn of entirely new user experiences with computers enabled by AI.

Tactic Generate is similarly pioneering a new user experience for ""Chat with Documents"", presenting a multi-column view where LLMs answer questions over each of your documents (or folders / collections in a database) in **parallel!**

So imagine grabbing 5 papers from ArXiv about LoRA and asking, ""how does merging LoRA weights with the base model work?"", you can get an **answer to each of these questions in parallel grounded in each of the 5 papers!** Tactic Generate's GUI then enables you to remove the column boundaries and sync up the answers!

I am writing this message from the AWS Re:Invent conference where I have been showing the Verba demo over and over again haha. It has really taught me the power of the GUI for explaining concepts in RAG and Vector Databases -- I think Tactic Generate can have the same impact for demonstrating Parallel LLM Execution and Multi-Document Agents!

I hope you enjoy the podcast with Rudy, as always more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

[https://www.youtube.com/watch?v=igK4JN2-1m4](https://www.youtube.com/watch?v=igK4JN2-1m4)"
65,deeplearning,chatgpt,top,2023-06-02 16:17:33,"Decent laptops for machine learning (GPU on cloud, but still decent local performance)?",bot_exe,False,0.6,1,13yh3b0,https://www.reddit.com/r/deeplearning/comments/13yh3b0/decent_laptops_for_machine_learning_gpu_on_cloud/,2,1685722653.0,"I'm looking for laptop recommendations where I can easily use cloud services like colab, paperspaces, runpod, etc. when I need a powerful GPU to training train neural networks or use a big opensource models, but also smoothly run code locally using VScode and jupyterlab/jupyter notebooks for everything else with good performance.

So far I have been using an ipad pro 12.9"" (2020) using google colab on the Safari browser (with chatGPT on splitscreen) and it has worked surprisingly well (I was forced to code on the ipad, because my old gaming PC died recently). The portability is amazing, yet some of the limitations of the OS, available software and the hardware are a pain in the butt.... I really want to be able to do simple things locally and quickly, like i could in my old PC, so I think I need decent CPU, RAM and storage for that (but not sure how much exactly????), the ability to use windows (and maybe linux), while also having a better interface (proper keyboard and mouse), while keeping portability.

Hence why I'm looking for decent laptop recommendations (good CPU, RAM, storage, form factor and connectors). I don't have enough money to buy a new gaming PC or workstation or GPU laptop (which I think are dumb anyway, if I wanted a GPU I would get a desktop PC), so I'm also trying to keep the price below something like a strong gaming PC, but still willing to invest in a good laptop if it will really solve my problems as I'm thinking this might... anyway since I'm still rather knew in this space I also welcome opinions on whether this is even feasible or if I should go for a different setup or rethink my requirements."
66,deeplearning,chatgpt,top,2023-02-07 19:42:25,New Weaviate Podcast - Adding ChatGPT to Weaviate!,HenryAILabs,False,1.0,1,10wb0ed,https://www.reddit.com/r/deeplearning/comments/10wb0ed/new_weaviate_podcast_adding_chatgpt_to_weaviate/,0,1675798945.0,"This podcast debuts the Weaviate generate module! The generate module is a new API in Weaviate that facilitates passing data from the Weaviate database to ChatGPT. Here is a snippet from Bob around the 43 minute mark I really enjoyed, describing how this kind of LLM technology is changing the world of database technology, ""Yeah so, what I’m really excited about and this is something that it’s just so funny right because if you see it, you have this huge epiphany. I’ve always been thinking of working with these models on input. Right so that they we can solve the problem of not having 100% keyword based search, so that we can have semantic search, image search, and those kind of things. I saw that as this beautiful uniqueness coming from a vector search engine or vector search database. So now what we’re adding is not only the input in the database but the output. So we’re basically saying we’re going to give you relevant information coming from the database, but that’s not per se stored inside the database. That’s new! I mean, just think about the most used databases in the world, Postgres, or MySQL, those kind of databases. It only outputs what’s in there. It makes sense. Because that’s how you use it. But now what we’re saying, is that’s fine you can do that, but also it can give you information, give you data that’s generated based on a task or prompt that you’re giving it. Having databases that make sense of it at input and generate new relevant content if that’s something you want as a user is amazing, and it’s just getting started. We should do this podcast like a half a year from now again and see how it's evolved because this is just too exciting man."". I really hope you enjoy the podcast, we are more than happy to answer any questions or help you get started with Weaviate!  


[https://www.youtube.com/watch?v=ro3ln4A9N8w](https://www.youtube.com/watch?v=ro3ln4A9N8w)"
67,deeplearning,chatgpt,top,2023-10-09 16:07:35,[?] Local Chatgpt Alternative Hardware,gyrene2083,False,0.67,1,173vmgn,https://www.reddit.com/r/deeplearning/comments/173vmgn/local_chatgpt_alternative_hardware/,4,1696867655.0,"Hello all, I'm new to the Chatgpt world, my son showed me this at the beginning of ther year and I just now started messing around with it and have to say it's a tool.

That said I started thinking, I want to run this locally. This is the following hardware I have and I would appreciate your input if this is at all possible on my end.

2950 Threadripper

64GB Ram

Nvidia 1070 w/8gb Ram

\*EDIT just got my hands on an Nvidia RTX 3090 w/24gbs Ram

I use this computer for video transcoding, but that is rare these days. I want to put this computer to good use and figured why not learn more about local AI.

Anyway, thank you for all your help in advance."
68,deeplearning,chatgpt,top,2023-06-30 20:42:58,Learn how to leverage custom NLP models and chatGPT to analyze risk factors from SEC 10-K reports in this insightful tutorial,Molly_Knight0,False,1.0,1,14nbjqu,https://ubiai.tools/analyze-company-risk-factors-from-sec-reports-with-ai/,0,1688157778.0,
69,deeplearning,chatgpt,top,2023-09-28 16:54:46,"The meme of man on chair with cardboard offering model parameter tuning for a few bucks, where to find?",tvtavtat,False,1.0,1,16ul8ej,https://www.reddit.com/r/deeplearning/comments/16ul8ej/the_meme_of_man_on_chair_with_cardboard_offering/,1,1695920086.0,"I'm sure you must know what I am talking about. I just can't find it, image search, bard, chatgpt, no one works. I need to real intelligence to help."
70,deeplearning,chatgpt,top,2023-04-06 07:43:06,A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?,Learningforeverrrrr,False,0.6,2,12dcnrm,https://www.reddit.com/r/deeplearning/comments/12dcnrm/a_complete_survey_on_generative_ai_aigc_is/,0,1680766986.0,"We recently completed two surveys: one on generative AI and the other on ChatGPT. Generative AI and ChatGPT are two fast-evolving research fields, and we will update the content soon, for which your feedback is appreciated (you can reach out to us through emails on the paper).

The title of this post refers to the first one, however, we put both links below.

**Link to a survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

**Link to a survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)

The following is the **abstract** of the **survey on generative AI** with a summary **figure**.

As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible to miss the opportunity to glimpse AIGC from a certain angle.  In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? To answer this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its **techniques** to **applications**. Modern generative AI relies on various technical foundations, ranging from **model architecture** and **self-supervised pretraining** to **generative modeling** methods (like GAN and diffusion models). After introducing the fundamental techniques, this work focuses on the technological development of various AIGC tasks based on their output type, including **text**, **images**, **videos**, **3D content**, **etc**., which depicts the full potential of ChatGPT's future. Moreover, we summarize their significant applications in some mainstream **industries**, such as **education** and **creativity** content. Finally, we discuss the **challenges** currently faced and present an **outlook** on how generative AI might evolve in the near future.

&#x200B;

https://preview.redd.it/scbpeabnx7sa1.png?width=1356&format=png&auto=webp&s=445da6a707ceb6af75e5305137ad30dcd06c32fe

**Link to a survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

**Link to a survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)"
71,deeplearning,chatgpt,top,2022-12-22 09:57:14,"Show ChatGPT's response next to the search results from Google, Bing, and DuckDuckGo.",Harrypham22,False,1.0,1,zsics7,https://www.reddit.com/r/deeplearning/comments/zsics7/show_chatgpts_response_next_to_the_search_results/,0,1671703034.0," **Do you know about ChatGPT?** 

It is a variant of the GPT-3 language model developed by OpenAI specifically designed for generating responses to user input in chat or messaging applications. It is trained on a large dataset of conversation data and is able to understand the context of a conversation and generate appropriate responses. ChatGPT is particularly well-suited for use in chat or messaging applications, but it can also be used for a wide range of other natural language processing tasks, such as language translation, summarization, and question answering.

**Display ChatGPT response alongside other search engine results (Google,Bing,Duck go go,..)**

***ChatGPT for Search Engines*** is an AI-based extension that could potentially become a real threat to any search engine.

It basically shows results to all sorts of queries next to the Google results (or other search engine). The precision is very impressive. This extension makes it possible everywhere you browse

This is a simple extension that show response from ChatGPT alongside Google and other search engines

Features:

\* Markdown rendering

\* Code hightlights

\* Feedback buttons

\* Custom trigger mode

Maybe you should try this extension: [shorturl.at/eqZ78](https://shorturl.at/eqZ78)

Watch this video to see how it work: [https://www.tiktok.com/@ai\_life26/video/7179865803003579674](https://www.tiktok.com/@ai_life26/video/7179865803003579674)

https://preview.redd.it/ojjxcy2q9f7a1.png?width=1294&format=png&auto=webp&s=e3b02aba9ba03f37dbdcd0a2c6265a95b9f11ed8"
72,deeplearning,chatgpt,top,2023-11-16 08:28:39,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,1.0,1,17whz4d,https://v.redd.it/qx68wbuf7o0c1,0,1700123319.0,
73,deeplearning,chatgpt,top,2023-03-08 01:18:38,"You probably have heard of chatgpt, have you heard of MarioGPT?",Shubhra22,False,0.67,1,11lhwsp,https://youtu.be/3h_kNjbWrdw,0,1678238318.0,
74,deeplearning,chatgpt,top,2023-03-01 05:52:05,Career pivot for a SWE considering chatGPT disruption?,Which-Distance1384,False,0.6,1,11evrik,https://www.reddit.com/r/deeplearning/comments/11evrik/career_pivot_for_a_swe_considering_chatgpt/,5,1677649925.0,"tl;dr : what to do to board GPT ship?

&#x200B;

I really dont think ChatGPT will replace jobs very soon. Not until many software and apps are developed to replace workers, e.g., something that can do HR job, read internal docs and summarize, code from a design, etc etc. And all that needs experts. Given the field is pretty new and there are not many experts in this field, at least for the scale needed, I think the process of job loss takes some time. For near future probably we all SWEs can enjoy the more efficient coding, documentation readintg, etc. But for sure in the future there will be changes.

&#x200B;

I have been bored with my job as a SWE for a while. I have worked mostly in Cloud all my career (AWS and GCP) and always wanted to try something new.

&#x200B;

I find ChatGPT disruptive and interesting and want to be a part of it. I wonder what is the best way for me to join Large Language Model efforts? Which ways have best ROI?

\- Going to a PhD program and becoming a researcher?

\- Getting an MSC and become some sort of NLP AI engineer?

\- Finding internal teams that are willing to give me a try on their research/product?

&#x200B;

&#x200B;"
75,deeplearning,chatgpt,top,2022-11-30 19:14:16,OpenAI's new impressive Conversational LLM - ChatGPT,dulldata,False,1.0,1,z90966,https://www.youtube.com/watch?v=2VJZky25rIs,0,1669835656.0,
76,deeplearning,chatgpt,top,2022-12-14 15:04:39,ChatGPT and Search Technology - New Weaviate Podcast with CEO Bob van Luijt and FAQx co-founders Chris Dossman and Marco Bianoc,HenryAILabs,False,0.5,0,zltb3s,https://www.reddit.com/r/deeplearning/comments/zltb3s/chatgpt_and_search_technology_new_weaviate/,0,1671030279.0,"ChatGPT has landed, leaving a massive impact on the world of technology! This podcast features visionaries and builders at the cutting edge of Search technology, discussing how recent advances like ChatGPT will change the way we use Search Engines! 

Link: [https://www.youtube.com/watch?v=s9aVAgk-6Ww](https://www.youtube.com/watch?v=s9aVAgk-6Ww) (also on Spotify - Weaviate Podcast)"
77,deeplearning,chatgpt,top,2023-04-15 16:29:02,Generative Agents: Interactive Simulacra of Human Behavior - Discover a Town Run by 25 ChatGPTs,deeplearningperson,False,0.5,0,12na1yq,https://youtu.be/9LzuqQkXEjo,0,1681576142.0,
78,deeplearning,chatgpt,top,2023-04-10 08:13:15,Summarize documents with ChatGPT via Python scripts,rottoneuro,False,0.5,0,12hbq89,https://levelup.gitconnected.com/summarize-documents-with-chatgpt-a43456841cc4,1,1681114395.0,
79,deeplearning,chatgpt,top,2023-09-30 12:23:31,[D] How to train a seq2seq model to rephrase input text following given rules.,3Ammar404,False,0.5,0,16w5g5p,https://www.reddit.com/r/deeplearning/comments/16w5g5p/d_how_to_train_a_seq2seq_model_to_rephrase_input/,2,1696076611.0,"Hi guys,

I want to train (fine-tune) a seq2seq model to perform the task of rephrasing input following these rules :

1- always follow the pattern ""Entity Verb Entity""

2- only use simple sentences : never combine sentences

3- Don't replace existing words

4- Don't lose the overall meaning of the text or any information in it.

For example:

text = ""Project Risk Management includes the processes of conducting risk management planning, identification, analysis, response planning, response implementation, and monitoring risk on a project""

Standardized Text = ""Project Risk Management conducts risk management planning. Project Risk Management conducts risk identification. Project Risk Management conducts risk analysis. Project Risk Management plans responses. Project Risk Management implements responses. Project Risk Management monitors risk on a project.""

Using ChatGPT the results were very good, but I want to know if I can fine tune a model (BERT, T5, any LM) locally, what should be the data format for training such a model, evaluation metrics ?"
80,deeplearning,chatgpt,top,2023-01-12 06:26:20,"Hi friends, we bring you the first bilingual ChatGPT detection toolset and would love your feedback~",Ok_Firefighter_2106,False,0.5,0,109sgrl,https://www.reddit.com/r/deeplearning/comments/109sgrl/hi_friends_we_bring_you_the_first_bilingual/,0,1673504780.0,"On 9 December 2022, we started a project on collecting comparison data from humans and ChatGPT, and methods of detecting ChatGPT-generated content.

Now, after a month of effort, we launched the **first bilingual** (EN/ZH)  [\#ChatGPT](https://twitter.com/hashtag/ChatGPT?src=hashtag_click) detecting tool set, consisting of **three** different models! 🎉  

We've also collected nearly 40K questions and their corresponding **human vs. ChatGPT comparison responses**, which will be released soon for future research!

&#x200B;

Detectors on 🤗 [@huggingface](https://twitter.com/huggingface) :

* [**QA version**](https://huggingface.co/spaces/Hello-SimpleAI/chatgpt-detector-qa)**:** detect whether an **answer** is generated by ChatGPT for a certain **question**, using PLM-based classifiers
* [**Single-text version**](https://huggingface.co/spaces/Hello-SimpleAI/chatgpt-detector-single): detect whether a **single** piece of text is ChatGPT generated, using PLM-based classifiers
* [**Linguistic version**](https://huggingface.co/spaces/Hello-SimpleAI/chatgpt-detector-ling)**:** detect whether a piece of text is ChatGPT generated, using **linguistic** features

Our **models and dataset will be open-sourced** in about a week. We look forward to receiving feedback from the community to help improve the models and make contributions to **open** academic research together:)

Project GitHub page: [ChatGPT Comparison Corpus (C3), Detectors, and more! 🔥](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection)

&#x200B;

https://preview.redd.it/3uqy9svtzjba1.png?width=2038&format=png&auto=webp&s=b6be9f3985111fba2337c7919761542d5a896b18

&#x200B;

https://preview.redd.it/etz77l3frtba1.png?width=2800&format=png&auto=webp&s=ea45f907e57dce8f35c5bb3bf2e66558bfd100a6

&#x200B;

https://preview.redd.it/y5rk8zwjrtba1.png?width=2584&format=png&auto=webp&s=5a4fcd814f4118a01ec05173e9d4b8f8efe1a310

https://preview.redd.it/hayhbjlszjba1.png?width=1454&format=png&auto=webp&s=5a627f0c42a120fcdce5ed152dc3f16e073b5f0f

&#x200B;"
81,deeplearning,chatgpt,top,2023-06-12 16:29:03,"Openai Leak Docs, Possible for New Update? 😳",KSSolomon,False,0.53,1,147r660,https://i.redd.it/fqym39c36m5b1.jpg,0,1686587343.0,"Someone who uses Reddit, which is a big online forum where people can talk about all kinds of stuff, found out about this update by looking at some of the computer code that makes up the program.

They found out that the new version will have something called ""workspaces"". This would be like if you could make different profiles in a game, and the game would remember each one and what you did with them.

They also found that the program might be able to take files, which are like digital pieces of paper with stuff written on them.

OpenAI talked about making this business version of ChatGPT in April 2023, and they also said they would make it more private. This means that the things people say to ChatGPT wouldn't be used to make it smarter anymore.

Now, here are the implications of this update:

Businesses could use this version of ChatGPT to help them with their work. They might be able to give it files with information, and ChatGPT could use that to help answer questions or solve problems.

The ""workspaces"" feature could make it easier for people to use ChatGPT in different ways. For example, a business might have one workspace for customer service, and another for helping with paperwork.

The new privacy measures would mean that what you say to ChatGPT stays private. This is important because it helps keep people's information safe. It also means that OpenAI is listening to people's concerns about privacy and doing something about it.

This literally just happened if you want Ai news as it drops it launched [here first](https://www.therundown.ai/subscribe?utm_source=al). The whole article has been extrapolated here as well for convenience."
82,deeplearning,chatgpt,top,2023-02-16 11:58:35,Create youtube video using ChatGPT and Pictory AI only.,coder4mzero,False,0.5,0,113oxh5,https://youtu.be/iSz4Q_d7JR8,0,1676548715.0,
83,deeplearning,chatgpt,top,2023-04-16 18:03:39,ChatGPT Math Problem Challenge! (AAAI-MAKE 2023),Neurosymbolic,False,0.33,0,12oj6bi,https://youtube.com/watch?v=iRhbOE9U_Tk&feature=share,0,1681668219.0,
84,deeplearning,chatgpt,top,2022-12-04 20:11:36,5 ChatGPT Tutorial for Total Beginners,dulldata,False,0.4,0,zck620,https://www.youtube.com/watch?v=gMb4iYHaONQ,0,1670184696.0,
85,deeplearning,chatgpt,top,2022-12-04 09:59:19,OpenAI’s ChatGPT is unbelievable good in telling stories!,Far_Pineapple770,False,0.33,0,zc5tc3,/r/MachineLearning/comments/zc5sg6/d_openais_chatgpt_is_unbelievable_good_in_telling/,0,1670147959.0,
86,deeplearning,chatgpt,top,2022-12-06 01:38:42,ChatGPT explained in 5 minutes,OnlyProggingForFun,False,0.43,0,zdr6l7,https://youtu.be/AsFgn8vU-tQ,0,1670290722.0,
87,deeplearning,chatgpt,top,2023-10-21 04:19:19,"Is there any tool or LLM like chatgpt,midjourney that can help us train and generate custom sounds",Beginning_Finding_98,False,0.43,0,17cu8ah,https://www.reddit.com/r/deeplearning/comments/17cu8ah/is_there_any_tool_or_llm_like_chatgptmidjourney/,1,1697861959.0,"&#x200B;

**Generating a Wide Variety of Sounds**

I'm a non-technical person with very little knowledge to develop AI tools and intending to learn Python  and based on that My question is as follows:

&#x200B;

Are there tools or chatgpt like platforms that can help people like me to generate couple of sounds like dog barks, cat meows. I want either something that can generate a variety of sounds or I want to work towards making something that cane help me generate audios  like dog barks, such as fierce, aggressive ones but not just limited to dog barks but also sound focused on nature, other animals, vehicles, machinery(e.g., honks, engine sounds ), and possibly human sounds (though that's not my primary focus for now).

**The amount of technical Assistance Needed**

I also came across a  tool like Teachable Machine and was wondering if it could be a solution as it does offer tools for audio. I am also aware that I would need datasets for such a task but apart from that I am not too sure about the nitty gritty or should I say the intricacies involved as well as the knowledge needed as I do assume it is likely not very easy [https://www.youtube.com/watch?v=L4GOmYPPqn8&t=1854s](https://www.youtube.com/watch?v=L4GOmYPPqn8&t=1854s)

&#x200B;

\[Teachable Machine\]([https://teachablemachine.withgoogle.com/](https://teachablemachine.withgoogle.com/))

&#x200B;

**Inspiration**

I was inspired by a project I found here: \[[https://x.com/TheAIAnonGuy/status/1684443155448360961?s=20](https://x.com/TheAIAnonGuy/status/1684443155448360961?s=20)\] 

&#x200B;

&#x200B;

Can anyone provide insights, guidance, or recommendations on how to accomplish this?

**To be fair, I'm not really sure if this is an audio-related or neural/machine learning (ML)/deep learning related learning question.**

 But I would like more insight if this is possible on an individual scale either with teachable, code or AI or a combination of all approaches and if there are any beginner friendly ways to achieve this

Thank you all for your assistance!"
88,deeplearning,chatgpt,top,2023-04-30 03:53:26,Why do neural networks like ChatGPT use memory and processing power while at rest?,Will_Tomos_Edwards,False,0.38,0,133f4m4,https://www.reddit.com/r/deeplearning/comments/133f4m4/why_do_neural_networks_like_chatgpt_use_memory/,13,1682826806.0,"It is surprising to me that the default behavior of many neural networks is to run processes and use resources (RAM, CPU or their equivalent) . Why are they not just stored in memory by default? Obviously they must take up a lot of memory, but I don't understand why the default behaviour is to be active."
89,deeplearning,chatgpt,top,2023-07-20 17:20:22,What’s hot nowadays,Gold-Act-7366,False,0.25,0,154x5xu,https://www.reddit.com/r/deeplearning/comments/154x5xu/whats_hot_nowadays/,1,1689873622.0,"I just wanted to ask which kind of projects are hot these, like before gpt it was text to image(mid journey etc) and at the time of chatgpt there were autogpt, agentgpt, gpt-engineer.

I’m just a teenager who is exploring in tech, and made some dl projects."
90,deeplearning,chatgpt,top,2023-12-13 10:06:50,Durchbruch in der KI mit Gemini: ChatGPT 4.0 in Benchmark-Tests übertroffen!,Webglobic_tech,False,0.25,0,18hdkrs,https://webglobic.com/2023/12/12/gemini-uebertrifft-chatgpt4-0-in-benchmark-tests-ein-neuer-meilenstein-in-der-ki-entwicklung/,0,1702462010.0,
91,deeplearning,chatgpt,top,2023-04-22 08:22:10,"ChatGPT TED talk is the hottest discussion! Almost 370+ comments and 400k views in just 20 hours, if you are interested in AI, come talk!",Ok-Judgment-1181,False,0.2,0,12uzdhn,/r/ChatGPT/comments/12tycz4/chatgpt_ted_talk_is_mind_blowing/,0,1682151730.0,
92,deeplearning,chatgpt,top,2023-06-01 14:21:15,How Does ChatGPT Learn: Reinforcement Learning Explained,OnlyProggingForFun,False,0.4,0,13xiv73,https://youtu.be/lWK9T56t-YM,0,1685629275.0,
93,deeplearning,chatgpt,top,2022-12-13 02:04:24,How to Talk to ChatGPT | An introduction to prompt,OnlyProggingForFun,False,0.33,0,zkj3z6,https://youtu.be/pZsJbYIFCCw,0,1670897064.0,
94,deeplearning,chatgpt,top,2023-10-05 19:31:50,AI And Math,Gla-l,False,0.25,0,170r03q,https://www.reddit.com/r/deeplearning/comments/170r03q/ai_and_math/,4,1696534310.0," Hello i guys i have some questions. I want learn AI but i don't know math too much so what can i do for this i asked to chatgpt and he is give me some subject and i searched, listen but some times i don't understand some arguments in lesson i guess this problem from my high school because we didn't have math lessons. Well what can i do for this how can i start to math? "
95,deeplearning,chatgpt,top,2023-04-26 13:19:51,Report: ChatGPT's Myers-Briggs personality type is ENFJ and it shows strong signs of Egoism and Sadism,Excellent_Cup3709,False,0.25,0,12zhck0,https://www.researchgate.net/publication/370071092_The_Self-Perception_and_Political_Biases_of_ChatGPT,2,1682515191.0,
96,deeplearning,chatgpt,top,2023-04-24 13:14:39,Applications of GPT,AcornWizard,False,0.3,0,12xfegq,https://www.reddit.com/r/deeplearning/comments/12xfegq/applications_of_gpt/,5,1682342079.0,"Hello. Is ChatGPT currently the only implemented application that uses GPT? Looking on the internet I see a lot of flashy but often vague talk about potential applications, yet I have not found more implemented uses."
97,deeplearning,chatgpt,top,2023-01-17 16:06:37,What nobody tells you about chatGPT and GPT-4,thomas999999,False,0.27,0,10efwno,https://www.reddit.com/r/deeplearning/comments/10efwno/what_nobody_tells_you_about_chatgpt_and_gpt4/,3,1673971597.0,i wanted to share a nice writeup my friend made about chatGPT [https://medium.com/@christian.bernhard97/what-nobody-tells-you-about-chatgpt-and-gpt-4-c8d97ae9f92d](https://medium.com/@christian.bernhard97/what-nobody-tells-you-about-chatgpt-and-gpt-4-c8d97ae9f92d)
98,deeplearning,chatgpt,top,2023-04-29 09:34:51,Connecting assistants to ChatGPT is nuts! JARVIS is ever closer!,Lewenhart87,False,0.35,0,132ogn4,https://v.redd.it/4257us79jswa1,1,1682760891.0,
99,deeplearning,chatgpt,top,2023-12-15 04:50:13,OpenAI's Next Move: ChatGPT 4.5 Upgrade in the Works? Sam Altman Clarifies,Damanjain,False,0.13,0,18is4sm,https://thebuzz.news/article/openai-chatgpt-4-5-leak/11787/,1,1702615813.0,
100,deeplearning,chatgpt,comments,2023-03-25 04:24:49,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.93,48,121agx4,https://www.reddit.com/r/deeplearning/comments/121agx4/do_we_really_need_100b_parameters_in_a_large/,54,1679718289.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
101,deeplearning,chatgpt,comments,2023-04-05 01:36:40,Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT,Time_Key8052,False,0.95,85,12c43uu,https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/,19,1680658600.0,"Vicuna : ChatGPT Alternative, Open-Source, High Quality and Low Cost 

&#x200B;

[ Relative Response Quality Assessed by GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a072b91)

Vicuna-13B has demonstrated competitive performance against other open-source models, such as Stanford Alpaca, by fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.

Evaluation using GPT-4 as a judge shows that Vicuna-13B achieves more than 90% of the quality of OpenAI ChatGPT and Google Bard AI, while outperforming other models such as Meta LLaMA (Large Language Model Meta AI) and Stanford Alpaca in more than 90% of cases.

The cost of training Vicuna-13B is approximately $300.

The training and serving code, along with an online demo, are publicly available for non-commercial use.

&#x200B;

More Information : [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT](https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)

Discord Server : [https://discord.gg/h6kCZb72G7](https://discord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysorg](https://twitter.com/lmsysorg)"
102,deeplearning,chatgpt,comments,2023-01-27 10:45:48,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.95,119,10mhyek,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
103,deeplearning,chatgpt,comments,2023-02-17 02:54:52,How likely is ChatGPT to be weaponized as an information pollution tool? What are the possible implementation paths? How to prevent possible attacks?,zcwang0702,False,0.7,9,1148t20,https://www.reddit.com/r/deeplearning/comments/1148t20/how_likely_is_chatgpt_to_be_weaponized_as_an/,14,1676602492.0,
104,deeplearning,chatgpt,comments,2023-04-30 03:53:26,Why do neural networks like ChatGPT use memory and processing power while at rest?,Will_Tomos_Edwards,False,0.44,0,133f4m4,https://www.reddit.com/r/deeplearning/comments/133f4m4/why_do_neural_networks_like_chatgpt_use_memory/,13,1682826806.0,"It is surprising to me that the default behavior of many neural networks is to run processes and use resources (RAM, CPU or their equivalent) . Why are they not just stored in memory by default? Obviously they must take up a lot of memory, but I don't understand why the default behaviour is to be active."
105,deeplearning,chatgpt,comments,2022-12-12 17:29:39,ChatGPT context length,Wild-Ad3931,False,0.97,30,zk5esp,https://www.reddit.com/r/deeplearning/comments/zk5esp/chatgpt_context_length/,10,1670866179.0,"How come ChatGPT can follow entire discussions whereas nowaday's LLM are limited (to the best of my knowledge) 4096 tokens ?

I asked it and it is not able to answer neither, and I found nothing on Google because no paper is published. I was also curious to understand how come Beamsearch was so fast with ChatGPT.

https://preview.redd.it/o6djsmpb5i5a1.png?width=1126&format=png&auto=webp&s=98baae5bf6fa294db408f0530214f8afa8a32a0b"
106,deeplearning,chatgpt,comments,2023-01-29 22:39:07,[P] We built a browser extension that unlocks browser mode capabilities using ChatGPT: MULTI·ON: AI Web Co-Pilot powered by ChatGPT,DragonLord9,False,0.96,66,10okyg3,https://v.redd.it/2hw47h0b82fa1,8,1675031947.0,
107,deeplearning,chatgpt,comments,2024-01-10 06:21:19,Are NLP based AI skills useless?,SnooBeans7516,False,0.7,11,1931m9b,https://www.reddit.com/r/deeplearning/comments/1931m9b/are_nlp_based_ai_skills_useless/,9,1704867679.0,"So something's been on my mind recently and I wanted to get Reddit's thoughts.

The thing that is troubling me as I get deeper, is it seems that for a lot of language based NLP tasks, ChatGPT or these other foundation models seem SOTA for 99% of tasks. In terms of developing valuable skills and products, it feels like the two options are: 

1. Research and training foundation models at large research labs 

2. Using GPT APIs in some sort of system (RAG or just raw prompts)

I feel like this issue is rooted in language itself being a somewhat static medium that can be mostly mastered by a single model. As opposed to images where we see a lot of cool projects from people that have to employ many different techniques to complete a specific task.

\## Example

Unlike images, the use cases seem much more straightforward, and don't require any special work other than throwing GPT at it. 

For example, I wanted to create a BERT like model to replace ingredients in a recipe, conditioned on a user's requirements or desires. The problem is, the more I worked on this and trained some models, the more it seemed that ChatGPT could just do it better with some prompt engineering...

==================

All this to say, I was previously motivated to be a part-time NLP researcher, implementing research into products in cool ways. Now it kind of feels like the only actually valuable investment is into ""AI engineering"", building systems using a GPT API:/"
108,deeplearning,chatgpt,comments,2023-05-13 22:42:26,Domain specific chatbot. Semantic search isn't enough.,mldlbr,False,0.91,9,13gv1zj,https://www.reddit.com/r/deeplearning/comments/13gv1zj/domain_specific_chatbot_semantic_search_isnt/,8,1684017746.0,"Hi guys, I'm struggling to find a reliable solution to this specific problem.

I  have a huge dataset with chat conversations, about several topics. I  want to ask questions and retrieve information about these conversations in a chatbot way.

I have tried  semantic search with chatGPT to answer questions about these  conversations. The problem is that semantic search only returns top  similar sentences, and doesn't ‘read’ all conversations, that’s not  enough to answer generic questions, just very specific ones. For  example, if I ask “What are these people talking about person X?” it  will return only the top sentences (through semantic similarity) and  that will not tell the whole story. The LLM’s models have a limit of  tokens, so I can’t send the whole dataset as context.

Is there any approach to giving a reliable answer based on reading all the messages?

Any ideas on how to approach this problem?"
109,deeplearning,chatgpt,comments,2023-04-02 18:10:37,Should we draw inspiration from Deep learning/Computer vision world for fine-tuning LLMs?,Vegetable-Skill-9700,False,0.76,12,129t3tl,https://www.reddit.com/r/deeplearning/comments/129t3tl/should_we_draw_inspiration_from_deep/,8,1680459037.0,"With HuggingGPT, BloombergGPT, and OpenAI's chatGPT store, it looks like the world is moving towards specialized GPTs for specialized tasks. What do you think are the best tips & tricks when it comes to fine-tuning and refining these task-specific GPTs?

Over the last decade, I have built many computer vision models (for human pose estimation, action classification, etc.), and our general approach was always based on Transfer learning. Take a state-of-the-art public model and fine-tune it by collecting data for the given use case.

Do you think that paradigm still holds true for LLMs?

Based on my experience, I believe observing the model's performance as it interacts with real-world data, identifying failure cases (where the model's outputs are wrong), and using them to create a high-quality retraining dataset will be the key.

&#x200B;

P.S. I am building an open-source project UpTrain ([https://github.com/uptrain-ai/uptrain](https://github.com/uptrain-ai/uptrain)), which helps data scientists to do so. We just wrote a blog on how this principle can be applied to fine-tune an LLM for a conversation summarization task. Check it out here: [https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation\_summarization](https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation_summarization)"
110,deeplearning,chatgpt,comments,2024-01-05 08:27:31,6 ways AI can make your life easier in 2024,PoetryOne4804,False,0.33,0,18z212l,https://www.reddit.com/r/deeplearning/comments/18z212l/6_ways_ai_can_make_your_life_easier_in_2024/,8,1704443251.0,"Artificial intelligence is developing every day. ChatGPT was a game changer for millions of people, but it is not the only one. Advances in AI are coming, and they're coming FAST. Very fast. There’re so many tasks AI can help with and make this year less stressful. Let me show you these ways:

**1) Chatbots for answering questions and brainstorming**

Except ChatGPT, you can use Google Bard, SpinBot, and YouChat.

**2) AI Essay writers**

Many people use [essay writing services](https://www.reddit.com/r/deeplearning/comments/16gnuwy/best_essay_writing_services_top_5/) but not all think that AI can also help in academic writing. AI essay writers like [Textero.ai](https://Textero.ai) can be faster and generate ideas or find sources for your topic.

**3) Daily life tools**

There’re AI planners to schedule meetings and integrate with your calendars. You can also keep track of finances using PocketGuard, Wally, or Cleo.

**4) Tools for social networks**

There’re various AI tools tailored for social networks, such as Postwise for Twitter posts and Steve.ai for YouTube.

**5) Tools to improve health and fitness goals**

AI tools like Apple Watches and Fitbits can monitor your fitness and health. They can even track your sleep and offer suggestions to improve sleep quality.

**6) Tools for academic needs**

Even though some professors are against using AI while studying, students look for ways to make academic life easier. Useful tools for school life you can find here:  [ai tools for students](https://www.reddit.com/r/artificial/comments/1716t0y/ai_tools_for_students_from_ai_essay_generators_to/)

Any other tools to share? Feel free to write about them, I’m ready to try more new services."
111,deeplearning,chatgpt,comments,2023-03-05 11:10:56,LLaMA model parallelization and server configuration,ChristmasInOct,False,1.0,25,11ium8l,https://www.reddit.com/r/deeplearning/comments/11ium8l/llama_model_parallelization_and_server/,8,1678014656.0,"Hey everyone,

First of all, tldr at bottom, typed more than expected here.  

Please excuse the rather naive perspective I have here.  I've followed along with great interest, but this is not my industry.

Regardless, I have spent the past 3-4 days falling down a brutally obsessive rabbit hole, and I cannot seem to find this information.  I'm assuming it's just that I am missing context of course, and regardless of whether there is a clear answer, I'm trying to get a better understanding of this topic so that I could better appraise the situation myself.

Really I suppose I have two questions.  **The first** is regarding model parallelization.

I'm assuming this is not generic whatsoever.  What is the typical process engineers go about for designing such a pipeline?  Specifically in regards to these new LLaMA models, is something like ALPA relevant?  Deepspeed?

More importantly, what information should I be seeking to determine this myself?

This roughly segues to my **second inquiry**.

The reason I'm curious about splitting the model pipeline etc., is that I am potentially in interested in standing a server up for this.  Although I don't have much of a budget for this build (\~$30-40K is the rough top-end, but I'd be a lot happier around $20-25K), the money is there if I can genuinely satisfy my use-case.

I work at a small, but borderline manic startup working on enterprise software; 90% of the work we're doing based in the react/node ecosystem, some low-level work for backend services, and some very interesting database work that I have very little to do with.  I am a fullstack engineer that grew up playing with C++ => C#, and somehow ended up spending all of my time r/w'ing javascript.  Lol.  Anyways.

Part of our roadmap since GPT-3 and the playground were made publicly accessible, involves usage of these transformer models, and their ability to interpret natural language inputs, whether from user inputs, or scraped input values generated somewhere in a chain of requests / operations.

Seeing GPT-3 in action made me specifically realize that my estimations on this technology had been wildly off.  Seeing ChatGPT in action and uptick, the API's becoming available, has me further panicked.

Running our inference through their API has never really been an option for us.  I haven't even really looked that far into it, but bottom line the data running through our platform is all back-office, highly sensitive business information, and many have agreements explicitly restricting the movement of data to or from any cloud services, with Microsoft, Amazon, and Google all specifically mentioned.

Regardless of the reasoning for these contracts, the LLaMA release has had me obsessed over this topic in more detail than before, and whether or not I would be able to get this setup privately, for our use-case.

**To get to the actual second inquiry**:

Say I want to throw a budget rig together for this in a server cabinet.  Am I able to effectively parallelize the LLaMA model, well enough to justify going with 24GB VRAM 4090's in the rig?  Say I do so with DeepSpeed, or some of the standard model parallelization libraries.

Is the performance cost low enough to justify taking the extra compute here over 1/3 - 1/2 as many RTX6000 ADA's?

Or should I be grabbing the 48GB ADA's?

Like I said, I apologize for the naivety, I'm really looking for more information so that I can start to put this picture together better on my own.  It really isn't the easiest topic to research with how quickly things seem to move, and the giant gap between conversation depths (gamer || phd in a lot of the most interesting or niche discussions, little between).

Thank you very much for your time.

TL;DR - Any information on LLaMA model parallelization at the moment?  Will it be compatible with things like zero or alpa?  How about for throwing a rig together right now for fine-tuning and then running inference on the LLaMA models?  48GB 6000 ADA's, or 24GB 4090's?

Planning on putting it in a mostly empty 42U cabinet that also houses our primary web server and networking hardware, so if there is a sales pitch for 4090's across multiple nodes here, I do have a massive bias as the kind of nerd that finds that kind of hardware borderline erotic.

Hydro and cooling are not an issue, just usage of the budget and understanding the requirements / approach given memory limitations, and how to avoid communication bottlenecks or even balance them against raw compute.

Thanks again everyone!"
112,deeplearning,chatgpt,comments,2023-11-16 08:28:46,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,0.84,67,17whz6e,https://v.redd.it/qx68wbuf7o0c1,7,1700123326.0,
113,deeplearning,chatgpt,comments,2023-04-25 23:50:32,Research on the political biases of ChatGPT,Mysterious_Potato132,False,0.78,5,12z08ni,/r/ChatGPT/comments/12yz49i/research_on_the_political_biases_of_chatgpt/,6,1682466632.0,
114,deeplearning,chatgpt,comments,2023-02-01 15:20:25,Launching my first-ever open-source project and it might make your ChatGPT answers better,Vegetable-Skill-9700,False,0.75,2,10qx9po,https://www.reddit.com/r/deeplearning/comments/10qx9po/launching_my_firstever_opensource_project_and_it/,6,1675264825.0,"I am building UpTrain - an open-source ML diagnostic toolkit that recently got investment from YCombinator.

As you know no ML model is 100% accurate, and, further, their accuracy deteriorates over time 😣. Additionally, due to the black boxiness ⬛ nature of Large Language models, it's challenging to identify and fix their problems.

The tool helps ML practitioners to:
1. Understand how their models are performing in production
2. Catch edge cases and outliers to help them refine their models
3. Allow them to define custom monitors to catch under-performing data-points
4. Retrain the model on them to improve its accuracy

You can check out the project here: https://github.com/uptrain-ai/uptrain. Would love to hear feedback from the community!"
115,deeplearning,chatgpt,comments,2023-01-20 08:53:58,Gotcha,actual_rocketman,False,0.95,66,10gs1ik,https://i.redd.it/yyh41pnje7da1.jpg,5,1674204838.0,
116,deeplearning,chatgpt,comments,2023-04-24 13:14:39,Applications of GPT,AcornWizard,False,0.27,0,12xfegq,https://www.reddit.com/r/deeplearning/comments/12xfegq/applications_of_gpt/,5,1682342079.0,"Hello. Is ChatGPT currently the only implemented application that uses GPT? Looking on the internet I see a lot of flashy but often vague talk about potential applications, yet I have not found more implemented uses."
117,deeplearning,chatgpt,comments,2023-03-01 05:52:05,Career pivot for a SWE considering chatGPT disruption?,Which-Distance1384,False,0.6,1,11evrik,https://www.reddit.com/r/deeplearning/comments/11evrik/career_pivot_for_a_swe_considering_chatgpt/,5,1677649925.0,"tl;dr : what to do to board GPT ship?

&#x200B;

I really dont think ChatGPT will replace jobs very soon. Not until many software and apps are developed to replace workers, e.g., something that can do HR job, read internal docs and summarize, code from a design, etc etc. And all that needs experts. Given the field is pretty new and there are not many experts in this field, at least for the scale needed, I think the process of job loss takes some time. For near future probably we all SWEs can enjoy the more efficient coding, documentation readintg, etc. But for sure in the future there will be changes.

&#x200B;

I have been bored with my job as a SWE for a while. I have worked mostly in Cloud all my career (AWS and GCP) and always wanted to try something new.

&#x200B;

I find ChatGPT disruptive and interesting and want to be a part of it. I wonder what is the best way for me to join Large Language Model efforts? Which ways have best ROI?

\- Going to a PhD program and becoming a researcher?

\- Getting an MSC and become some sort of NLP AI engineer?

\- Finding internal teams that are willing to give me a try on their research/product?

&#x200B;

&#x200B;"
118,deeplearning,chatgpt,comments,2023-10-05 19:31:50,AI And Math,Gla-l,False,0.22,0,170r03q,https://www.reddit.com/r/deeplearning/comments/170r03q/ai_and_math/,4,1696534310.0," Hello i guys i have some questions. I want learn AI but i don't know math too much so what can i do for this i asked to chatgpt and he is give me some subject and i searched, listen but some times i don't understand some arguments in lesson i guess this problem from my high school because we didn't have math lessons. Well what can i do for this how can i start to math? "
119,deeplearning,chatgpt,comments,2023-06-17 20:54:57,LLMs for small projects,KrazedRook,False,0.8,3,14c1hgq,https://www.reddit.com/r/deeplearning/comments/14c1hgq/llms_for_small_projects/,3,1687035297.0,"I am looking fo a LLM to use for an app that I'm working on (for myself, it wont be published). I was originally goin to use ChatGPT but I have "" exceeded my current quota "" and I don't want to have to pay for this, so thats out of the question. I want to see if there are any others that I could use for my project, I'm using VS Code. If you have any I can use please explain it to me if you can in summary."
120,deeplearning,chatgpt,comments,2023-10-09 16:07:35,[?] Local Chatgpt Alternative Hardware,gyrene2083,False,0.67,1,173vmgn,https://www.reddit.com/r/deeplearning/comments/173vmgn/local_chatgpt_alternative_hardware/,4,1696867655.0,"Hello all, I'm new to the Chatgpt world, my son showed me this at the beginning of ther year and I just now started messing around with it and have to say it's a tool.

That said I started thinking, I want to run this locally. This is the following hardware I have and I would appreciate your input if this is at all possible on my end.

2950 Threadripper

64GB Ram

Nvidia 1070 w/8gb Ram

\*EDIT just got my hands on an Nvidia RTX 3090 w/24gbs Ram

I use this computer for video transcoding, but that is rare these days. I want to put this computer to good use and figured why not learn more about local AI.

Anyway, thank you for all your help in advance."
121,deeplearning,chatgpt,comments,2023-12-14 02:03:03,[D] Constructing an Efficient Knowledge Graph RAG Pipeline with LlamaIndex,Fit_Maintenance_2455,False,1.0,2,18hxo48,https://www.reddit.com/r/deeplearning/comments/18hxo48/d_constructing_an_efficient_knowledge_graph_rag/,4,1702519383.0,"Large Language Models (LLMs) such as ChatGPT and Bard exhibit remarkable abilities within their specialized areas of training. However, their constraints in handling new or private data inquiries are widely recognized.

Retrieval Augmented Generation (RAG) emerges as a solution to bridge this gap, allowing LLMs to access external knowledge sources. This article delves into RAG, examines its elements, and constructs a usable RAG workflow that harnesses the potential of LlamaIndex, a knowledge graph.

&#x200B;

Link: [https://medium.com/ai-advances/constructing-an-efficient-knowledge-graph-rag-pipeline-with-llamaindex-81a0a0b105b7](https://medium.com/ai-advances/constructing-an-efficient-knowledge-graph-rag-pipeline-with-llamaindex-81a0a0b105b7)  "
122,deeplearning,chatgpt,comments,2023-01-17 16:06:37,What nobody tells you about chatGPT and GPT-4,thomas999999,False,0.31,0,10efwno,https://www.reddit.com/r/deeplearning/comments/10efwno/what_nobody_tells_you_about_chatgpt_and_gpt4/,3,1673971597.0,i wanted to share a nice writeup my friend made about chatGPT [https://medium.com/@christian.bernhard97/what-nobody-tells-you-about-chatgpt-and-gpt-4-c8d97ae9f92d](https://medium.com/@christian.bernhard97/what-nobody-tells-you-about-chatgpt-and-gpt-4-c8d97ae9f92d)
123,deeplearning,chatgpt,comments,2023-03-15 00:07:51,GPTMinusOne - AI that hides the use of ChatGPT and GPT4,tomd_96,False,0.81,9,11rfgbs,https://github.com/tom-doerr/gpt_minus_one,3,1678838871.0,
124,deeplearning,chatgpt,comments,2023-05-31 07:08:07,Question about Neural Nets,yanggang20202024,False,0.62,3,13wf18y,https://www.reddit.com/r/deeplearning/comments/13wf18y/question_about_neural_nets/,3,1685516887.0,"I recently read an article about how the supercomputer used to train Chatgpt consisted of something like 10,000 gpus.

My question is, do these supercomputers that train neural nets always get better when more gpus are added? Or is it a situation where progress flattens to such a degree at some point that it makes no sense to make the supercomputer any bigger?"
125,deeplearning,chatgpt,comments,2023-02-01 09:00:18,"Python wrapper of OpenAI's New AI classifier tool, which detects whether the paragraph was generated by ChatGPT, GPT models, or written by humans",StoicBatman,False,0.89,20,10qouv9,https://www.reddit.com/r/deeplearning/comments/10qouv9/python_wrapper_of_openais_new_ai_classifier_tool/,3,1675242018.0,"OpenAI has developed a new AI classifier tool which detects whether the content (paragraph, code, etc.) was generated by #ChatGPT, #GPT-based large language models or written by humans.

Here is a python wrapper of openai model to detect if a text is written by humans or generated by ChatGPT, GPT models

Github: [https://github.com/promptslab/openai-detector](https://github.com/promptslab/openai-detector)  
Openai release: [https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/](https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/)

If you are interested in #PromptEngineering, #LLMs, #ChatGPT and other latest research discussions, please consider joining our discord [discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)  


https://preview.redd.it/9d8ooeg2ljfa1.png?width=1358&format=png&auto=webp&s=0de7ccc5cd16d6bbad3dcfe7d8441547a6196fc8"
126,deeplearning,chatgpt,comments,2023-03-13 12:50:10,Learning logical relationships with neural networks with differential ILP,Neurosymbolic,False,0.94,12,11q8tir,https://www.reddit.com/r/deeplearning/comments/11q8tir/learning_logical_relationships_with_neural/,3,1678711810.0,"Since last week’s post on my lab’s software package [PyReason](https://neurosymoblic.asu.edu/pyreason/), I got a lot of questions on if it would be possible for a neural network to learn logical relationships from data. After all, ChatGPT seems to be able to generate Python code, and Meta released a NeurIPS paper to show you can learn math equations from data using a transformer-based model, so why not logic? In this article, we will one line of research in this area – differential ILP.

The research in this area really kicked off with a 2018 [paper from DeepMind](https://www.reddit.com/r/deeplearning/jair.org/index.php/jair/article/view/11172) where Richard Evans and Edward Grefenstette showed that you could adapt techniques from “inductive logic programming” to use gradient descent, and learn logical rules from data. Previous (non-neural) work on inductive logic programming was generally not designed to work with noisy data and instead fit the historical examples in a precise manner. Evans and Grefenstette utilized a neural architecture and a loss function – and they showed they could handle noisy data and even do some level of integration with CNN’s. Their neural architecture mimicked a set of candidate logical rules – and the rules assigned higher weights by gradient descent would be thought to best fit the data. However, a downside to this approach is that the neural network was [quintic in the size of the input](https://www.youtube.com/watch?v=SOnAE0EyX8c&list=PLpqh-PUKX-i7URwnkTqpAkSchJHvbxZHB&index=5). This is why they only applied their approach on very small problems – it did not see very wide adoption.

That said, in the last two years, there have been some notable follow-ons to this work. Researchers out of Kyoto University and NTT introduced a manner to learn rules that are more expressive in a different manner by allowing function symbols in the logical language ([Shindo et al., AAAI 2021](https://ojs.aaai.org/index.php/AAAI/article/view/16637/16444)). They leverage a clause search and refinement process to limit the number of candidate rules – hence limiting the size of the neural network. A student team from ASU created a presentation on their work for our recent seminar course on neuro symbolic AI. We released a three part video series from their talk:

[Part 1: Review of differentiable inductive logic programming](https://www.youtube.com/watch?v=JIS78a40q8U&t=270s)

[Part 2: Clause search and refinement In our recent video series](https://www.youtube.com/watch?v=nzfbxlHUwuE&t=345s)

[Part 3: Experiments](https://www.youtube.com/watch?v=-fKWNtHUIN0&t=27s)

[Slides](https://labs.engineering.asu.edu/labv2/wp-content/uploads/sites/82/2022/10/Shindo_dILP.pdf)

Some think that the ability to learn such relationships will represent a significant advancement in ML, specifically addressing shortcomings in areas such as knowledge graph completion and reasoning about scene graphs. However, the gap still remains wide, and ILP techniques, including differentiable ILP still have a ways to go. Really interested in what your thoughts are, feel free to comment below."
127,deeplearning,chatgpt,comments,2023-09-28 16:54:46,"The meme of man on chair with cardboard offering model parameter tuning for a few bucks, where to find?",tvtavtat,False,1.0,1,16ul8ej,https://www.reddit.com/r/deeplearning/comments/16ul8ej/the_meme_of_man_on_chair_with_cardboard_offering/,1,1695920086.0,"I'm sure you must know what I am talking about. I just can't find it, image search, bard, chatgpt, no one works. I need to real intelligence to help."
128,deeplearning,chatgpt,comments,2023-02-11 06:59:00,⭕ New Open-Source Version Of ChatGPT,LesleyFair,False,0.87,39,10zepkt,https://www.reddit.com/r/deeplearning/comments/10zepkt/new_opensource_version_of_chatgpt/,3,1676098740.0,"GPT is getting competition from open-source.

A group of researchers, around the YouTuber [Yannic Kilcher](https://www.ykilcher.com/), have announced that they are working on [Open Assistant](https://github.com/LAION-AI/Open-Assistant). The goal is to produce a chat-based language model that is much smaller than GPT-3 while maintaining similar performance.

If you want to support them, they are crowd-sourcing training data [here](https://open-assistant.io/).

**What Does This Mean?**

Current language models are too big.

They require millions of dollars of hardware to train and use. Hence, access to this technology is limited to big organizations. Smaller firms and universities are effectively shut out from the developments.

Shrinking and open-sourcing models will facilitate academic research and niche applications.

Projects such as Open Assistant will help to make language models a commodity. Lowering the barrier to entry will increase access and accelerate innovation.

What an exciting time to be alive! 

Thank you for reading! I really enjoyed making this for you!  
The Decoding ⭕ is a thoughtful weekly 5-minute email that keeps you in the loop about machine research and the data economy. [Click here to sign up](https://thedecoding.net/)!"
129,deeplearning,chatgpt,comments,2023-09-29 14:02:33,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.91,18,16vch0x,https://www.reddit.com/r/deeplearning/comments/16vch0x/this_week_in_ai_all_the_major_ai_developments_in/,2,1695996153.0,"1. **Meta AI** presents **Emu**, a quality-tuned latent diffusion model for generating highly aesthetic images. Emu significantly outperforms SDXLv1.0 on visual appeal.
2. **Meta AI** researchers present a series of long-context LLMs with context windows of up to 32,768 tokens. LLAMA 2 70B variant surpasses gpt-3.5-turbo-16k’s overall performance on a suite of long-context tasks.
3. **Abacus AI** released a larger 70B version of **Giraffe**. Giraffe is a family of models that are finetuned from base Llama 2 and have a larger context length of 32K tokens\].
4. **Cerebras** and **Opentensor** released Bittensor Language Model, ‘**BTLM-3B-8K**’, a new 3 billion parameter open-source language model with an 8k context length trained on 627B tokens of SlimPajama. It outperforms models trained on hundreds of billions more tokens and achieves comparable performance to open 7B parameter models. The model needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models and is available with an Apache 2.0 license for commercial use.
5. **OpenAI** is rolling out, over the next two weeks, new voice and image capabilities in ChatGPT enabling ChatGPT to understand images, understand speech and speak. The new voice capability is powered by a new text-to-speech model, capable of generating human-like audio from just text and a few seconds of sample speech. .
6. **Mistral AI**, a French startup, released its first 7B-parameter model, **Mistral 7B**, which outperforms all currently available open models up to 13B parameters on all standard English and code benchmarks. Mistral 7B is released in Apache 2.0, making it usable without restrictions anywhere.
7. **Microsoft** has released **AutoGen** \- an open-source framework that enables development of LLM applications using multiple agents that can converse with each other to solve a task. Agents can operate in various modes that employ combinations of LLMs, human inputs and tools.
8. **LAION** released **LeoLM**, the first open and commercially available German foundation language model built on Llama-2
9. Researchers from **Google** and **Cornell University** present and release code for DynIBaR (Neural Dynamic Image-Based Rendering) - a novel approach that generates photorealistic renderings from complex, dynamic videos taken with mobile device cameras, overcoming fundamental limitations of prior methods and enabling new video effects.
10. **Cloudflare** launched **Workers AI** (an AI inference as a service platform), **Vectorize** (a vector Database) and **AI Gateway** with tools to cache, rate limit and observe AI deployments. Llama2 is available on Workers AI.
11. **Amazon** announced the general availability of **Bedrock**, its service that offers a choice of generative AI models from Amazon itself and third-party partners through an API.
12. **Spotify** has launched a pilot program for AI-powered voice translations of podcasts in other languages - in the podcaster’s voic. It uses OpenAI’s newly released voice generation model.
13. **Getty Images** has launched a generative AI image tool, ‘**Generative AI by Getty Images**’, that is ‘commercially‑safe’. It’s powered by Nvidia Picasso, a custom model trained exclusively using Getty’s images library.
14. **Optimus**, Tesla’s humanoid robot, can now sort objects autonomously and do yoga. Its neural network is trained fully end-to-end.
15. **Amazon** will invest up to $4 billion in Anthropic. Developers and engineers will be able to build on top of Anthropic’s models via Amazon Bedrock.
16. **Google Search** indexed shared Bard conversational links into its search results pages. Google says it is working on a fix.

&#x200B;

  
My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
130,deeplearning,chatgpt,comments,2023-08-02 11:33:32,Using PDFs with GPT Models,Disastrous_Look_1745,False,0.79,11,15g6i4x,https://www.reddit.com/r/deeplearning/comments/15g6i4x/using_pdfs_with_gpt_models/,2,1690976012.0,Found a blog talking about how we can interact with PDFs in Python by using GPT API & Langchain. It talks about some pretty cool automations you can build involving PDFs - [https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/](https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/)
131,deeplearning,chatgpt,comments,2024-02-10 03:18:15,Building an AI that can mimic the Spectator Method by Benjamin Franklin?,kiwifreeze,False,1.0,2,1an6mfj,https://www.reddit.com/r/deeplearning/comments/1an6mfj/building_an_ai_that_can_mimic_the_spectator/,2,1707535095.0,"Hello, I'm a recent CS grad and aspiring game dev. 

 I want to get better at writing and one of the ways I've been doing this was using the [Spectator method by Benjamin Franklin](https://shanesnow.com/research/how-to-be-a-better-writer-ben-franklin) 

I have been doing this by hand recently and I've found it to be incredibly helpful for my writing chops. I do everything by hand, that is take notes on each individual sentence of whichever book and then try to rewrite after offsetting the time a bit so I've forgotten it and then compare my writing to the original to see what I'm lacking. 

Taking notes on each individual sentence has been tedious though, and I tried to get a way for ChatGPT to do this for me but it can't read PDFs and yet alone other book files (I'm guessing). It does have the capability however to make short sentiments of the meanings of each sentence in any paragraph of a book (I tested this with a public domain book like Pride and Prejudice but it stopped after awhile). 

Is it possible to write an AI to do this for me automatically instead so I don't have to take notes sentence by sentence? Like use the OpenAI api to build a personal app around, or even build an LLM (which I don't even know where I would start with tbh). 

I do have much experience with coding and have taken an Intro to AI course at my university, though I can't say how much I really paid attention. That being said, I'm willing and capable of learning. 

Any advice would be appreciated!"
132,deeplearning,chatgpt,comments,2023-04-26 13:19:51,Report: ChatGPT's Myers-Briggs personality type is ENFJ and it shows strong signs of Egoism and Sadism,Excellent_Cup3709,False,0.25,0,12zhck0,https://www.researchgate.net/publication/370071092_The_Self-Perception_and_Political_Biases_of_ChatGPT,2,1682515191.0,
133,deeplearning,chatgpt,comments,2023-09-18 12:40:11,"DeepMind co-founder predicts ""third wave"" of AI: machines talking to machines and people",Nalix01,False,0.75,2,16lugx7,https://www.reddit.com/r/deeplearning/comments/16lugx7/deepmind_cofounder_predicts_third_wave_of_ai/,2,1695040811.0,"DeepMind's co-founder, Mustafa Suleyman, anticipates a ""third wave"" of AI evolution where machines will interact with both humans and other machines.

If you want to stay ahead of the curve in AI and tech, [look here first](https://dupple.com/techpresso?utm_source=reddit&utm_medium=social&utm_campaign=post).

**The Evolution of AI Phases**

* **Initial Classification Phase**: This was the first wave, focusing on deep learning that classifies different types of input data, such as images and audio.
* **Current Generative Phase**: AI uses input data to create new data.
* **Upcoming Interactive Phase**: Machines will be able to perform tasks by conversing with other machines and humans. Users will give high-level objectives to their AI systems which will then take necessary actions, involving dialogues with other AIs and individuals.

**Interactive AI's Potential**

* **More than Just Automation**: This AI won't just be about following commands but will have the freedom and agency to execute tasks.
* **Closer to Sci-Fi**: Interactive AI is anticipated to be more similar to the artificial intelligence depicted in science fiction, with dynamic capabilities rather than being static.

**Current AI Landscape**:

* **Generative AI's Popularity**: Despite being a game-changer, enthusiasm for generative AI seems to be waning, with declining user growth and web traffic for tools like ChatGPT.
* **Inflection AI's ""Pi""**: Earlier this year, Suleyman's company released a ChatGPT rival named Pi, emphasizing its polite and conversational nature.

**PS:** **If you enjoyed this post**, you’ll love my [ML-powered newsletter](https://dupple.com/techpresso?utm_source=reddit&utm_medium=social&utm_campaign=post) that summarizes the best AI/tech news from 50+ media. It’s already being read by **6,500+** **professionals** from **OpenAI, Google, Meta**…"
134,deeplearning,chatgpt,comments,2023-09-30 12:23:31,[D] How to train a seq2seq model to rephrase input text following given rules.,3Ammar404,False,0.5,0,16w5g5p,https://www.reddit.com/r/deeplearning/comments/16w5g5p/d_how_to_train_a_seq2seq_model_to_rephrase_input/,2,1696076611.0,"Hi guys,

I want to train (fine-tune) a seq2seq model to perform the task of rephrasing input following these rules :

1- always follow the pattern ""Entity Verb Entity""

2- only use simple sentences : never combine sentences

3- Don't replace existing words

4- Don't lose the overall meaning of the text or any information in it.

For example:

text = ""Project Risk Management includes the processes of conducting risk management planning, identification, analysis, response planning, response implementation, and monitoring risk on a project""

Standardized Text = ""Project Risk Management conducts risk management planning. Project Risk Management conducts risk identification. Project Risk Management conducts risk analysis. Project Risk Management plans responses. Project Risk Management implements responses. Project Risk Management monitors risk on a project.""

Using ChatGPT the results were very good, but I want to know if I can fine tune a model (BERT, T5, any LM) locally, what should be the data format for training such a model, evaluation metrics ?"
135,deeplearning,chatgpt,comments,2023-03-09 09:27:13,ChatGPT vs Other Chatbots!!,Genius_feed,False,0.29,0,11mnvcr,https://i.redd.it/7eqx02homoma1.png,2,1678354033.0,
136,deeplearning,chatgpt,comments,2023-03-10 05:22:26,[POC] ChatGPT Audio Bot (like Google Assistant and Alexa) - Opensource,JC1DA,False,0.85,18,11nfrhw,https://www.reddit.com/r/deeplearning/comments/11nfrhw/poc_chatgpt_audio_bot_like_google_assistant_and/,2,1678425746.0,"Hey guys, just wanna share this bot that I quickly built using ChatGPT API.

GitHub page: [https://github.com/LanyTek/ChatGPT\_Audio\_Bot](https://github.com/LanyTek/ChatGPT_Audio_Bot)

This service allows you to keep talking to the bot using your voice like you often do with Google Assistant or Alexa. It can be used in a lot of scenarios like teaching, gossiping, or quickly retrieving information without the need of typing. 

I have a quick video demo here if you wanna check [https://www.youtube.com/watch?v=e9n0BJfMyKw](https://www.youtube.com/watch?v=e9n0BJfMyKw)

It's open source so feel free to use it for anything you like :)"
137,deeplearning,chatgpt,comments,2023-06-02 16:17:33,"Decent laptops for machine learning (GPU on cloud, but still decent local performance)?",bot_exe,False,0.6,1,13yh3b0,https://www.reddit.com/r/deeplearning/comments/13yh3b0/decent_laptops_for_machine_learning_gpu_on_cloud/,2,1685722653.0,"I'm looking for laptop recommendations where I can easily use cloud services like colab, paperspaces, runpod, etc. when I need a powerful GPU to training train neural networks or use a big opensource models, but also smoothly run code locally using VScode and jupyterlab/jupyter notebooks for everything else with good performance.

So far I have been using an ipad pro 12.9"" (2020) using google colab on the Safari browser (with chatGPT on splitscreen) and it has worked surprisingly well (I was forced to code on the ipad, because my old gaming PC died recently). The portability is amazing, yet some of the limitations of the OS, available software and the hardware are a pain in the butt.... I really want to be able to do simple things locally and quickly, like i could in my old PC, so I think I need decent CPU, RAM and storage for that (but not sure how much exactly????), the ability to use windows (and maybe linux), while also having a better interface (proper keyboard and mouse), while keeping portability.

Hence why I'm looking for decent laptop recommendations (good CPU, RAM, storage, form factor and connectors). I don't have enough money to buy a new gaming PC or workstation or GPU laptop (which I think are dumb anyway, if I wanted a GPU I would get a desktop PC), so I'm also trying to keep the price below something like a strong gaming PC, but still willing to invest in a good laptop if it will really solve my problems as I'm thinking this might... anyway since I'm still rather knew in this space I also welcome opinions on whether this is even feasible or if I should go for a different setup or rethink my requirements."
138,deeplearning,chatgpt,comments,2023-03-22 21:55:31,ChatLLaMA – A ChatGPT style chatbot for Facebook's LLaMA,imgonnarelph,False,1.0,21,11yy5es,https://chatllama.baseten.co/,2,1679522131.0,
139,deeplearning,chatgpt,comments,2023-12-15 04:50:13,OpenAI's Next Move: ChatGPT 4.5 Upgrade in the Works? Sam Altman Clarifies,Damanjain,False,0.11,0,18is4sm,https://thebuzz.news/article/openai-chatgpt-4-5-leak/11787/,1,1702615813.0,
140,deeplearning,chatgpt,comments,2023-01-14 14:48:43,Scaling Language Models Shines Light On The Future Of AI ⭕,LesleyFair,False,0.75,8,10bq685,https://www.reddit.com/r/deeplearning/comments/10bq685/scaling_language_models_shines_light_on_the/,1,1673707723.0,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs might allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* reduce hallucination and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example is words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (a proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/jlh726eku0ca1.png?width=800&format=png&auto=webp&s=55d170251a967f31b36f01864af6bb7e2dbda253)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week.No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)"
141,deeplearning,chatgpt,comments,2023-07-20 17:20:22,What’s hot nowadays,Gold-Act-7366,False,0.25,0,154x5xu,https://www.reddit.com/r/deeplearning/comments/154x5xu/whats_hot_nowadays/,1,1689873622.0,"I just wanted to ask which kind of projects are hot these, like before gpt it was text to image(mid journey etc) and at the time of chatgpt there were autogpt, agentgpt, gpt-engineer.

I’m just a teenager who is exploring in tech, and made some dl projects."
142,deeplearning,chatgpt,comments,2023-04-29 09:34:51,Connecting assistants to ChatGPT is nuts! JARVIS is ever closer!,Lewenhart87,False,0.38,0,132ogn4,https://v.redd.it/4257us79jswa1,1,1682760891.0,
143,deeplearning,chatgpt,comments,2023-03-09 00:50:22,"AI generated video chapter titles (YouTube, Vimeo, etc)",happybirthday290,False,0.88,17,11mdvb9,https://i.redd.it/h6utxsxg2mma1.png,1,1678323022.0,
144,deeplearning,chatgpt,comments,2023-06-11 15:56:18,ChatGPT interrogating bugs and errors!!,Available-Bass-7575,False,0.55,1,146xgyf,https://youtu.be/zfIyIScu0oo,1,1686498978.0,
145,deeplearning,chatgpt,comments,2023-05-29 13:38:03,"Ortus - Chat with YouTube | ChatGPT Chrome extension -> use it to learn ML, supports The AI Epiphany YT channel, and more to come!",gordicaleksa,False,0.63,2,13uv79y,https://www.youtube.com/watch?v=0V9Jw6haJHQ,1,1685367483.0,
146,deeplearning,chatgpt,comments,2022-12-10 22:44:46,InstructGPT,MRMohebian,False,0.75,6,zi62fr,https://www.reddit.com/r/deeplearning/comments/zi62fr/instructgpt/,1,1670712286.0,"Recently, ChatGPT became trendy. It was adopting an algorithm by the name of InstructGPT. 
We go through the paper ""Training Language Models to Follow Instructions with Human Feedback"" in this video and go into extensive detail about how InstructGPT works. 

Please subscribe, leave a comment and share with your friends.

[R]
https://youtu.be/lYRWzCPGM2Q"
147,deeplearning,chatgpt,comments,2023-06-28 15:56:28,"[SEEKING FEEDBACK] I built a Chrome Extension AI tool that brings ChatGPT directly to any website. It's powered by GPT4 with access to the web, email reply, auto-suggest prompt, summarize and read screen features.",RidiculusRex,False,0.67,1,14ld8h3,https://v.redd.it/xuumhvhc5s8b1,1,1687967788.0,
148,deeplearning,chatgpt,comments,2024-01-18 05:16:45,How can I make LLM plot graphs/figures on my database with RAG?,HappyDataGuy,False,0.67,1,199ieeu,https://www.reddit.com/r/deeplearning/comments/199ieeu/how_can_i_make_llm_plot_graphsfigures_on_my/,1,1705555005.0,I want for LLM like ChatGPT to plot graph but not with code-interpretor which requires uploading the data to openai. is there any way to achieve this? 
149,deeplearning,chatgpt,comments,2023-04-10 08:13:15,Summarize documents with ChatGPT via Python scripts,rottoneuro,False,0.44,0,12hbq89,https://levelup.gitconnected.com/summarize-documents-with-chatgpt-a43456841cc4,1,1681114395.0,
150,deeplearning,chatgpt,comments,2023-10-21 04:19:19,"Is there any tool or LLM like chatgpt,midjourney that can help us train and generate custom sounds",Beginning_Finding_98,False,0.44,0,17cu8ah,https://www.reddit.com/r/deeplearning/comments/17cu8ah/is_there_any_tool_or_llm_like_chatgptmidjourney/,1,1697861959.0,"&#x200B;

**Generating a Wide Variety of Sounds**

I'm a non-technical person with very little knowledge to develop AI tools and intending to learn Python  and based on that My question is as follows:

&#x200B;

Are there tools or chatgpt like platforms that can help people like me to generate couple of sounds like dog barks, cat meows. I want either something that can generate a variety of sounds or I want to work towards making something that cane help me generate audios  like dog barks, such as fierce, aggressive ones but not just limited to dog barks but also sound focused on nature, other animals, vehicles, machinery(e.g., honks, engine sounds ), and possibly human sounds (though that's not my primary focus for now).

**The amount of technical Assistance Needed**

I also came across a  tool like Teachable Machine and was wondering if it could be a solution as it does offer tools for audio. I am also aware that I would need datasets for such a task but apart from that I am not too sure about the nitty gritty or should I say the intricacies involved as well as the knowledge needed as I do assume it is likely not very easy [https://www.youtube.com/watch?v=L4GOmYPPqn8&t=1854s](https://www.youtube.com/watch?v=L4GOmYPPqn8&t=1854s)

&#x200B;

\[Teachable Machine\]([https://teachablemachine.withgoogle.com/](https://teachablemachine.withgoogle.com/))

&#x200B;

**Inspiration**

I was inspired by a project I found here: \[[https://x.com/TheAIAnonGuy/status/1684443155448360961?s=20](https://x.com/TheAIAnonGuy/status/1684443155448360961?s=20)\] 

&#x200B;

&#x200B;

Can anyone provide insights, guidance, or recommendations on how to accomplish this?

**To be fair, I'm not really sure if this is an audio-related or neural/machine learning (ML)/deep learning related learning question.**

 But I would like more insight if this is possible on an individual scale either with teachable, code or AI or a combination of all approaches and if there are any beginner friendly ways to achieve this

Thank you all for your assistance!"
151,deeplearning,chatgpt,comments,2022-12-04 09:59:19,OpenAI’s ChatGPT is unbelievable good in telling stories!,Far_Pineapple770,False,0.33,0,zc5tc3,/r/MachineLearning/comments/zc5sg6/d_openais_chatgpt_is_unbelievable_good_in_telling/,0,1670147959.0,
152,deeplearning,chatgpt,comments,2023-08-05 17:09:44,The Quest to Have Endless Conversations with Llama and ChatGPT 🗣️💬,JClub,False,0.67,2,15j1117,https://medium.com/@joaolages/the-quest-to-have-endless-conversations-with-llama-and-chatgpt-%EF%B8%8F-81360b9b34b2,0,1691255384.0,
153,deeplearning,chatgpt,comments,2022-12-14 15:04:39,ChatGPT and Search Technology - New Weaviate Podcast with CEO Bob van Luijt and FAQx co-founders Chris Dossman and Marco Bianoc,HenryAILabs,False,0.5,0,zltb3s,https://www.reddit.com/r/deeplearning/comments/zltb3s/chatgpt_and_search_technology_new_weaviate/,0,1671030279.0,"ChatGPT has landed, leaving a massive impact on the world of technology! This podcast features visionaries and builders at the cutting edge of Search technology, discussing how recent advances like ChatGPT will change the way we use Search Engines! 

Link: [https://www.youtube.com/watch?v=s9aVAgk-6Ww](https://www.youtube.com/watch?v=s9aVAgk-6Ww) (also on Spotify - Weaviate Podcast)"
154,deeplearning,chatgpt,comments,2023-11-15 20:27:19,"Nvidia introduces the H200, an AI-crunching monster GPU that may speed up ChatGPT",keghn,False,0.88,12,17w2vgj,https://arstechnica.com/information-technology/2023/11/nvidia-introduces-its-most-powerful-gpu-yet-designed-for-accelerating-ai/,0,1700080039.0,
155,deeplearning,chatgpt,comments,2023-07-22 18:37:01,"LLAMA 2 - Model Explained, Demo and Comparison to ChatGPT",Combination-Fun,False,0.8,3,156roiv,https://www.reddit.com/r/deeplearning/comments/156roiv/llama_2_model_explained_demo_and_comparison_to/,0,1690051021.0,"LLAMA 2 is the largest and best opensource LLM every released free for commercial use. There have been several improvements that make LLAMA 2 better than LLAMA 1. Here is a video explaining the LLAMA 2 Model, a quick Demo of the model along with a Comparison to ChatGPT:

[https://youtu.be/TiloR3qRogs](https://youtu.be/TiloR3qRogs)

Hope its useful!"
156,deeplearning,chatgpt,comments,2023-06-07 14:12:57,New Weaviate Podcast - LLM Agents!,CShorten,False,1.0,1,143edrc,https://www.reddit.com/r/deeplearning/comments/143edrc/new_weaviate_podcast_llm_agents/,0,1686147177.0,"Hey everyone! I am SUPER excited to share our 51st Weaviate Podcast on keeping up with the latest in LLM Agents -- featuring Greg Kamradt from Data Independent and Colin Harmon from Nesh, we discussed all the abstractions and emerging ideas from LangChain, LlamaIndex, and miscellaneous other sources!

We discussed everything under the sun related to LLMs from Tool Use to Databases, Multi-Agent LLMs, Privacy, Personalization, the ChatGPT Marketplace, Fine-Tuning, Long Context Length LLMs, and more! I really hope you find it useful!

https://www.youtube.com/watch?v=iB4ki6gdAdc"
157,deeplearning,chatgpt,comments,2023-04-16 18:03:39,ChatGPT Math Problem Challenge! (AAAI-MAKE 2023),Neurosymbolic,False,0.33,0,12oj6bi,https://youtube.com/watch?v=iRhbOE9U_Tk&feature=share,0,1681668219.0,
158,deeplearning,chatgpt,comments,2023-10-16 14:41:46,Nobel laureate Dr Michael Levitt: AI will change everything forever,kirst31,False,1.0,1,1797kte,https://www.reddit.com/r/deeplearning/comments/1797kte/nobel_laureate_dr_michael_levitt_ai_will_change/,0,1697467306.0,"[https://www.youtube.com/watch?v=8ZV6o0EiuTo&t=61s](https://www.youtube.com/watch?v=8ZV6o0EiuTo&t=61s)

The decorated and respected scientist, an early adopter of ChatGPT and other AI technologies, reveals whether the rapid emergence of ever-more powerful machine learning tools will ultimately help or harm humanity as it changes our world beyond recognition."
159,deeplearning,chatgpt,comments,2023-12-24 00:01:05,MLX Mixtral 8x7b on M3 max 128GB | Better than chatgpt?,gimel1213,False,1.0,3,18pildv,https://youtube.com/watch?v=mFIsZHSAzJ0&si=wq6uLlHzXdg-yL72,0,1703376065.0,
160,deeplearning,chatgpt,comments,2023-05-17 15:10:45,New Weaviate Podcast - ChatArena!,CShorten,False,0.6,1,13k4i21,https://www.reddit.com/r/deeplearning/comments/13k4i21/new_weaviate_podcast_chatarena/,0,1684336245.0,"Hey everyone! I am super excited to publish our newest Weaviate Podcast on ChatArena!  One of the most exciting ideas with the emergence of LLM capabilities is to plug multiple LLMs together in Multi-Agent games or environments! I think the world is collectively still scrambling to understand systems like this, but 2 applications are immediately obvious:  


1. Create intelligent artifacts by simulating conversations between role-playing agents -- say an LLM impersonator of Sam Altman and Gary Marcus debate proposals for AI safety or Michael Bronstein and Jure Leskovec discuss the future of Geometric Deep Learning and Graph Neural Networks
2. Evaluating LLMs -- Benchmarks unfortunately don't really capture the nuances of intelligence, but having say ChatGPT chat with Claude moderated by a Cohere LLM that rates which LLM was more intelligent across thousands of simulated conversations 😂 -- looks like an incredibly promising way to keep up with the flood of new LLMs entering the market!  

I learned so much from this podcast, it was easily one of my favorite conversations I've ever had across the 47 Weaviate podcast episodes we have published so far, I really hope you find it useful and interesting!https://www.youtube.com/watch?v=\_0ww8Q0Bq2w"
161,deeplearning,chatgpt,comments,2022-12-05 01:45:02,Thread: Top 10 ways you can use ChatGPT for Music related stuff,dicklesworth,False,0.72,6,zct1o6,/r/musictheory/comments/zcso1s/thread_top_10_ways_you_can_use_chatgpt_for_music/,0,1670204702.0,
162,deeplearning,chatgpt,comments,2024-01-11 11:25:53,What are some tips of curating a dataset to fine-tune a code-completion LLM?,janissary2016,False,1.0,1,193zhur,https://www.reddit.com/r/deeplearning/comments/193zhur/what_are_some_tips_of_curating_a_dataset_to/,0,1704972353.0,"Hi.

There is a new SDK that I am working on and I want to know what are some ways of automatically curating a dataset to train a code-completing LLM to deploy as a VSCode plugin? Hacky ways are appreciated. I was thinking of using chatgpt API to make numerous API calls to inflate a CSV with artificially generated prompts - code entries. There are available datasets of course but I want to tailor the code completion for this particular SDK.

Appreciate all answers."
163,deeplearning,chatgpt,comments,2023-02-06 00:07:43,ChatGPT: From Nowhere to Knowledgeable,crawfa,False,0.61,3,10urypd,https://www.reddit.com/r/deeplearning/comments/10urypd/chatgpt_from_nowhere_to_knowledgeable/,0,1675642063.0,"ChatGPT is taking the world by storm, and is now the fastest growing software application ever, eclipsing TikTok, which may soon be the fastest shrinking software application ever if it gets banned in the US. This article explains at a high level what ChatGPT is, how it works at a high level, what you can do with it, as well as some developer choices they have made and identifies some things that ChatGPT does not do well.

[https://open.substack.com/pub/stayblog/p/chatgpt-from-nowhere-to-knowledgeable?r=1qxkwn&utm\_campaign=post&utm\_medium=web](https://open.substack.com/pub/stayblog/p/chatgpt-from-nowhere-to-knowledgeable?r=1qxkwn&utm_campaign=post&utm_medium=web)"
164,deeplearning,chatgpt,comments,2023-10-25 01:25:27,How we Built an Open-Source RAG-based ChatGPT Web App: Meet Our new AI Tutor!,OnlyProggingForFun,False,0.6,1,17ft66e,https://youtu.be/7ytyK6u3aAk,0,1698197127.0,
165,deeplearning,chatgpt,comments,2023-01-19 11:43:34,Join us this Friday 6 pm EST for a fascinating discussion about the societal impact of large language models (LLMs) like ChatGPT,OnlyProggingForFun,False,1.0,1,10fzn3l,https://discord.gg/ehPqT6rym8?event=1063903315974443162,0,1674128614.0,
166,deeplearning,chatgpt,comments,2022-12-04 20:11:36,5 ChatGPT Tutorial for Total Beginners,dulldata,False,0.4,0,zck620,https://www.youtube.com/watch?v=gMb4iYHaONQ,0,1670184696.0,
167,deeplearning,chatgpt,comments,2023-04-06 07:43:06,A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?,Learningforeverrrrr,False,0.55,1,12dcnrm,https://www.reddit.com/r/deeplearning/comments/12dcnrm/a_complete_survey_on_generative_ai_aigc_is/,0,1680766986.0,"We recently completed two surveys: one on generative AI and the other on ChatGPT. Generative AI and ChatGPT are two fast-evolving research fields, and we will update the content soon, for which your feedback is appreciated (you can reach out to us through emails on the paper).

The title of this post refers to the first one, however, we put both links below.

**Link to a survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

**Link to a survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)

The following is the **abstract** of the **survey on generative AI** with a summary **figure**.

As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible to miss the opportunity to glimpse AIGC from a certain angle.  In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? To answer this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its **techniques** to **applications**. Modern generative AI relies on various technical foundations, ranging from **model architecture** and **self-supervised pretraining** to **generative modeling** methods (like GAN and diffusion models). After introducing the fundamental techniques, this work focuses on the technological development of various AIGC tasks based on their output type, including **text**, **images**, **videos**, **3D content**, **etc**., which depicts the full potential of ChatGPT's future. Moreover, we summarize their significant applications in some mainstream **industries**, such as **education** and **creativity** content. Finally, we discuss the **challenges** currently faced and present an **outlook** on how generative AI might evolve in the near future.

&#x200B;

https://preview.redd.it/scbpeabnx7sa1.png?width=1356&format=png&auto=webp&s=445da6a707ceb6af75e5305137ad30dcd06c32fe

**Link to a survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

**Link to a survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)"
168,deeplearning,chatgpt,comments,2023-04-07 10:58:54,Text-to-image Diffusion Models in Generative AI: A Survey,Learningforeverrrrr,False,0.92,17,12ehc2m,https://www.reddit.com/r/deeplearning/comments/12ehc2m/texttoimage_diffusion_models_in_generative_ai_a/,0,1680865134.0,"Diffusion models have become a SOTA generative modeling method for numerous content types, such as images, audio, graph, etc. As the number of articles on diffusion models has grown exponentially over the past few years, there is an increasing need for survey works to summarize them. Recognizing the existence of such works, our team has completed multiple field-specific surveys on diffusion models. We promote our works here and hope they can be helpful to researchers in relative fields: text-to-image diffusion models [\[a survey\]](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey), audio diffusion models [\[a survey\]](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI), and graph diffusion models [\[a survey\]](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material) .

In the following, we briefly summarize our survey on text-to-image diffusion models.

[Text-to-image Diffusion Models in Generative AI: A Survey](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)

As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis, i.e., text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.

Moreover, we have also completed two survey works on generative AI (AIGC) [\[a survey\]](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need) and ChatGPT [\[a survey\]](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era), respectively. Interested readers may give it a look."
169,deeplearning,chatgpt,comments,2023-04-22 08:22:10,"ChatGPT TED talk is the hottest discussion! Almost 370+ comments and 400k views in just 20 hours, if you are interested in AI, come talk!",Ok-Judgment-1181,False,0.17,0,12uzdhn,/r/ChatGPT/comments/12tycz4/chatgpt_ted_talk_is_mind_blowing/,0,1682151730.0,
170,deeplearning,chatgpt,comments,2023-03-11 00:38:10,Generate READMEs Using ChatGPT,tomd_96,False,0.8,8,11o5zyl,https://www.reddit.com/r/deeplearning/comments/11o5zyl/generate_readmes_using_chatgpt/,0,1678495090.0,"&#x200B;

https://i.redd.it/k375our2a0na1.gif

&#x200B;

You can use this program I wrote to generate readmes: [https://github.com/tom-doerr/codex-readme](https://github.com/tom-doerr/codex-readme)

&#x200B;

It's far from perfect, but I now added ChatGPT and it is surprisingly good at inferring what the project is about. It often generates interesting usage examples and explains the available command line options.

&#x200B;

You probably won't yet use this for larger projects, but I think this can make sense for small projects or single scripts. Many small scripts are very useful but might never be published because of the work that is required to document and explain it. Using this AI might assist you with that.

&#x200B;

Reportedly GPT-4 is coming out next week, which probably would make it even better.

&#x200B;

What do you think?"
171,deeplearning,chatgpt,comments,2023-08-27 23:55:12,GPT4 Contextual Decomposition Template,InevitableSky2801,False,0.67,1,1636b99,https://www.reddit.com/r/deeplearning/comments/1636b99/gpt4_contextual_decomposition_template/,0,1693180512.0,"Complex tasks with LLMs like ChatGPT/GPT4 are best broken down by first asking ChatGPT to outline the steps and then asking the LLM to execute against those steps that it defined. I first came across this interesting technique on Twitter recently.

While it’s OK to do this once in OpenAI’s playground, it's difficult to make this repeatable and streamlined. When I wanted an LLM to do something complex, I wanted to be able to plug into a template instead of thinking about and setting up the contextual decomposition process.

I made this Contextual Decomposition Template to help solve this problem: [https://lastmileai.dev/workbooks/cllqfl5c600rdpgnhh2su2fa0](https://lastmileai.dev/workbooks/cllqfl5c600rdpgnhh2su2fa0)

With a document and objective, this template allows you to quickly get to the answer through defining intermediate steps and executing according. Parameters are set up so you can easily change the goal, document, and objective and click 'Run All' to get the final results.

Please let me know if you have feedback! I'm also very curious if you have other interesting techniques with complex tasks and workflows working with LLMs."
172,deeplearning,chatgpt,comments,2022-12-06 01:38:42,ChatGPT explained in 5 minutes,OnlyProggingForFun,False,0.43,0,zdr6l7,https://youtu.be/AsFgn8vU-tQ,0,1670290722.0,
173,deeplearning,chatgpt,comments,2023-04-05 13:21:51,"New Weaviate Podcast (#42) - ChatGPT Plugin Marketplace, Alpaca Models, Semantic Search on S3, and more!",CShorten,False,0.76,2,12ck7ae,https://www.reddit.com/r/deeplearning/comments/12ck7ae/new_weaviate_podcast_42_chatgpt_plugin/,0,1680700911.0," I am beyond excited to share our latest Weaviate Podcast with Ethan Steininger! Ethan is the founder of Mixpeek and creator of Collie.ai!

Ethan began by explaining how he came into search through integrating MongoDB with the Lucene inverted index. Ethan continued explaining how his background in Sales Engineering helped him to see the recurring problems businesses are facing when trying to utilize the latest LLM and Vector Database technologies to solve their problems.

We then continued to take a tour of all sorts of topics in the AI Landscape from the impact of the ChatGPT Plugin Marketplace / New App Store for AI to the Stanford Alpaca models, the impact of LLMs for coding productivity and many more, even ending with Ethan's advice on stress management by getting into nature and our thoughts on the existential fear technologies like GPT-4 inspire in many and the implications of it on society.

I hope you enjoy the podcast, please let us know what you think!

[https://www.youtube.com/watch?v=EDPk1umuge0](https://www.youtube.com/watch?v=EDPk1umuge0)"
174,deeplearning,chatgpt,comments,2023-12-22 11:45:32,Team GPT's Picks: Top 15 AI Tools for 2024 Productivity,LongjmpingShower,False,0.67,1,18od3vg,https://www.reddit.com/r/deeplearning/comments/18od3vg/team_gpts_picks_top_15_ai_tools_for_2024/,0,1703245532.0,"Some of the best AI tools to increase productivity are Team-GPT, Notion AI, Asana Intelligence, Salesforce Einstein, Zia, HubSpot AI, Jasper, Canva AI, Copy AI, Zapier, IFTTT, Outreach, Otter, Rev, Trint. In this article, I review all the AI software mentioned above in detail by telling you their features, pros, cons, and pricing categorically.  
**Learn more>>>**[https://team-gpt.com/learn/chatgpt-for-work-course](https://team-gpt.com/learn/chatgpt-for-work-course)  


https://preview.redd.it/nxwv3gff3u7c1.png?width=824&format=png&auto=webp&s=72a5fd8feb3ee0199df5c25962014a0b45ed3678"
175,deeplearning,chatgpt,comments,2023-04-09 19:34:12,"ChatGPT for free now , GPT4ALL is now here",oridnary_artist,False,0.74,7,12gt77m,https://www.youtube.com/watch?v=WiCYfi3SUTE&t=1s,0,1681068852.0,
176,deeplearning,chatgpt,comments,2023-03-08 01:18:38,"You probably have heard of chatgpt, have you heard of MarioGPT?",Shubhra22,False,0.67,1,11lhwsp,https://youtu.be/3h_kNjbWrdw,0,1678238318.0,
177,deeplearning,chatgpt,comments,2023-11-29 16:18:08,Rudy Lai on Tactic Generate - Weaviate Podcast #78!,CShorten,False,0.67,1,186t892,https://www.reddit.com/r/deeplearning/comments/186t892/rudy_lai_on_tactic_generate_weaviate_podcast_78/,0,1701274688.0,"Hey everyone! I am SUPER excited to publish our 78th Weaviate Podcast with Rudy Lai, Co-Founder and CEO of Tactic Generate!

I think most people would agree that the viral success of ChatGPT has greatly aided by **pairing the LLM with a Chat GUI** (as well as of course instruction tuning it for chat). ChatGPT marked the dawn of entirely new user experiences with computers enabled by AI.

Tactic Generate is similarly pioneering a new user experience for ""Chat with Documents"", presenting a multi-column view where LLMs answer questions over each of your documents (or folders / collections in a database) in **parallel!**

So imagine grabbing 5 papers from ArXiv about LoRA and asking, ""how does merging LoRA weights with the base model work?"", you can get an **answer to each of these questions in parallel grounded in each of the 5 papers!** Tactic Generate's GUI then enables you to remove the column boundaries and sync up the answers!

I am writing this message from the AWS Re:Invent conference where I have been showing the Verba demo over and over again haha. It has really taught me the power of the GUI for explaining concepts in RAG and Vector Databases -- I think Tactic Generate can have the same impact for demonstrating Parallel LLM Execution and Multi-Document Agents!

I hope you enjoy the podcast with Rudy, as always more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

[https://www.youtube.com/watch?v=igK4JN2-1m4](https://www.youtube.com/watch?v=igK4JN2-1m4)"
178,deeplearning,chatgpt,comments,2022-12-05 02:22:37,Building A Virtual Machine Inside ChatGPT,x_abyss,False,0.81,3,zctzmf,https://www.engraved.blog/building-a-virtual-machine-inside/,0,1670206957.0,
179,deeplearning,chatgpt,comments,2023-06-01 14:21:15,How Does ChatGPT Learn: Reinforcement Learning Explained,OnlyProggingForFun,False,0.38,0,13xiv73,https://youtu.be/lWK9T56t-YM,0,1685629275.0,
180,deeplearning,chatgpt,comments,2022-12-22 09:57:14,"Show ChatGPT's response next to the search results from Google, Bing, and DuckDuckGo.",Harrypham22,False,1.0,1,zsics7,https://www.reddit.com/r/deeplearning/comments/zsics7/show_chatgpts_response_next_to_the_search_results/,0,1671703034.0," **Do you know about ChatGPT?** 

It is a variant of the GPT-3 language model developed by OpenAI specifically designed for generating responses to user input in chat or messaging applications. It is trained on a large dataset of conversation data and is able to understand the context of a conversation and generate appropriate responses. ChatGPT is particularly well-suited for use in chat or messaging applications, but it can also be used for a wide range of other natural language processing tasks, such as language translation, summarization, and question answering.

**Display ChatGPT response alongside other search engine results (Google,Bing,Duck go go,..)**

***ChatGPT for Search Engines*** is an AI-based extension that could potentially become a real threat to any search engine.

It basically shows results to all sorts of queries next to the Google results (or other search engine). The precision is very impressive. This extension makes it possible everywhere you browse

This is a simple extension that show response from ChatGPT alongside Google and other search engines

Features:

\* Markdown rendering

\* Code hightlights

\* Feedback buttons

\* Custom trigger mode

Maybe you should try this extension: [shorturl.at/eqZ78](https://shorturl.at/eqZ78)

Watch this video to see how it work: [https://www.tiktok.com/@ai\_life26/video/7179865803003579674](https://www.tiktok.com/@ai_life26/video/7179865803003579674)

https://preview.redd.it/ojjxcy2q9f7a1.png?width=1294&format=png&auto=webp&s=e3b02aba9ba03f37dbdcd0a2c6265a95b9f11ed8"
181,deeplearning,chatgpt,comments,2023-04-19 13:51:18,Alpaca Electron: ChatGPT Locally!,oridnary_artist,False,0.71,6,12rtzak,https://youtu.be/0oz3RaLlTlM,0,1681912278.0,
182,deeplearning,chatgpt,comments,2023-04-07 10:28:52,"Series of Surveys on ChatGPT, Generative AI (AIGC), and Diffusion Models",Learningforeverrrrr,False,0.79,7,12egmab,https://www.reddit.com/r/deeplearning/comments/12egmab/series_of_surveys_on_chatgpt_generative_ai_aigc/,0,1680863332.0,"* **A survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)
* **A survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)
* **A survey on Text-to-image diffusion models:** [**Text-to-image Diffusion Models in Generative AI: A Survey**](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)
* **A survey on Audio diffusion models:** [**A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI**](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI)
* **A survey on Graph diffusion models:** [**A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material**](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material)

**ChatGPT goes viral.** Launched by OpenAI on November 30, 2022, ChatGPT has attracted unprecedented attention due to its powerful abilities all over the world.  It took only 5 days \[1\] and 2 months \[2\] for ChatGPT to have 1 million users and 100 million monthly users after launch, making it the fastest-growing consumer application in history. ChatGPT can be seen as the milestone for the GPT family to go viral. In academia, ChatGPT has also inspired a large number of works discussing its applications in multiple fields, with **more than 500 papers within four months** after release and **the number is still increasing rapidly.**  This brings a huge challenge for a researcher who hopes to have an overview of ChatGPT applications or hopes to start his or her journey with ChatGPT in their own field.  **To help more people keep up with the latest progress of the GPT family,** we’re glad to share a self-contained survey that not only summarizes **the recent applications** of ChatGPT and other GPT variants like GPT-4, but also introduces the **underlying techniques** and **challenges.** Please refer to the following link for the paper: [One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era).

&#x200B;

**From ChatGPT to Generative AI.**  One highlighting ability of the GPT family is that it can generate natural languages, which falls into the area of Generative AI. Apart from text, Generative AI can also generate content in other modalities, such as image, audio, and graph. More excitingly, Generative AI is able to convert data from one modality to another one, such as the text-to-image task (generating images from text). **To help readers have a better overview of Generative AI,** we provide a complete survey on underlying **techniques,** summary and development of **typical tasks in academia**, and also **industrial applications.** Please refer to the following link for the paper.  [A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

&#x200B;

**From Generative AI to Diffusion Models.** The prosperity of a field is always driven by the development of technology, and so is Generative AI.  Different from ChatGPT which generates text based on the transformer, **diffuson models** have greatly accelerated the development of other fields in Generative AI, such as image synthesis.  Although we provide a summary of diffusion models and typical tasks in the Generative AI survey, we cannot include detailed discussions due to paper length limitations. **For those who are interested in the technical details of diffusion models and the recent progress of their applications in Generative AI,** we provide three self-contained surveys on **how diffusion models are applied in three typical areas: Text-to-image diffusion models** (also includes related tasks such as image editing)**, Audio diffusion models** (including text to speech synthesis and enhancement), and **Graph diffusion models** (including molecule, protein and material areas). Please refer to the following links for the paper.

* [Text-to-image Diffusion Models in Generative AI: A Survey](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)
* [A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI)
* [A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material)

We hope our survey series will help people for a better understanding of ChatGPT and Generative AI, and we will update the survey regularly to include the latest progress. Please refer to the personal pages of the authors for the latest updates on surveys. If you have any suggestions or problems, please feel free to contact us.

\[1\] Greg Brockman, co-founder of OpenAI, [https://twitter.com/gdb/status/1599683104142430208?lang=en](https://twitter.com/gdb/status/1599683104142430208?lang=en)

\[2\] Reuters, [https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/)"
183,deeplearning,chatgpt,comments,2023-12-21 15:21:16,Subjectivity in AI with Dan Shipper,CShorten,False,0.75,2,18npf5z,https://www.reddit.com/r/deeplearning/comments/18npf5z/subjectivity_in_ai_with_dan_shipper/,0,1703172076.0,"Hey everyone!! I am SUPER excited to publish the fourth and final episode of the AI-Native Database podcast series with Dan Shipper and Bob van Luijt!

I thought Dan brought such a unique perspective on how we write and create with ChatGPT and Vector DBs! We dove into how subjectivity, or personality, is reflected in AI systems. For example, imagine you ask an LLM: ""Could you plan a vacation to Miami?"". Compared to previous database or search engine technologies that will just spit out relevant information to the query, the LLM might respond: ""Are you sure you want to go to Miami? I think Fort Lauderdale is nicer at this time of year"".

The podcast is titled ""Subjectivity in AI"" because this is such a novelty of AI compared to previous technologies. How will this unique ""personality"" of LLMs (as well as vector embeddings, classifiers, and other model types) seep into all these new applications? Can we control it with prompts and RAG, training data, or some combination of the above? What do multi-agent systems look like, each with their own personality and intrinsic motivation? Further, what new art forms will arise because of this property of AI, what will the ""overdrive"" of AI use look like?

I hope you enjoy the podcast, the last in our series! This has been so much fun to record and publish!

[https://www.youtube.com/watch?v=prV5R3T6UqM](https://www.youtube.com/watch?v=prV5R3T6UqM)"
184,deeplearning,chatgpt,comments,2023-09-27 17:55:06,Advanced Query Engines in LlamaIndex - Concepts Explained + E2E Python Code Notebooks,CShorten,False,0.72,3,16trdr4,https://www.reddit.com/r/deeplearning/comments/16trdr4/advanced_query_engines_in_llamaindex_concepts/,0,1695837306.0,"Hey everyone! I am super excited to share Erika's 3rd Episode in our Llama Index and Weaviate series, covering the Advanced Query Engines in Llama Index. Here is a quick overview, the video will explain the concepts in further detail and then an End-to-End Python code demo (I am particularly proud of the SQL Router demo)

• SQL Router -- one of the most interesting products in the latest boom of LLMs is that we can connect Vector Search with SQL systems, routing queries with an LLM!!! We can also use the LLM to format the queries with Text-to-SQL prompts! Such an amazing thing that I didn't have on my bingo card before ChatGPT haha.

• Recursive Retrieval -- Even aside from LLMs, we can create more advanced search indexes by connecting indexes with each other - for example, first searching through descriptions of the tools available and then stepping into the documentation within that tool to find the more particular thing you need! This also can involve LLMs if for example the high-level search takes us into a structured table -- and now we call upon our good old Text-to-SQL LLM again.

• Self-Correcting Query Engine -- Quite a bizarre phenomenon of LLMs is that they are able to correct themselves by simply reflecting on their output. Llama Index presents a nice and simple solution to get running with this.

• Lastly is the most open-ended of the Advanced Query Engines... the Sub Question Query Engine. This describes asking the LLM to decompose the question or task into it's constituent sub-questions or sub-tasks and then compose the results together to serve the larger goal. For example, ""Did Aristotle Use a Laptop?"" --> ""When did Aristotle Live?"" & ""When were Laptops invented?""

I hope you find this video useful, we are more than happy to answer any questions or discuss any ideas you have about the content in the video!

https://www.youtube.com/watch?v=Su-ROQMaiaw"
185,deeplearning,chatgpt,comments,2023-06-30 20:42:58,Learn how to leverage custom NLP models and chatGPT to analyze risk factors from SEC 10-K reports in this insightful tutorial,Molly_Knight0,False,1.0,1,14nbjqu,https://ubiai.tools/analyze-company-risk-factors-from-sec-reports-with-ai/,0,1688157778.0,
186,deeplearning,chatgpt,comments,2023-04-25 17:53:47,"Microsoft releases SynapseMl v0.11 with support for ChatGPT, GPT-4, causal learning, and more",mhamilton723,False,0.9,23,12yqpnp,https://www.reddit.com/r/deeplearning/comments/12yqpnp/microsoft_releases_synapseml_v011_with_support/,0,1682445227.0,"Today Microsoft launched SynapseML v0.11 with support for ChatGPT, GPT-4, distributed training of huggingface and torchvision models, an ONNX Model hub integration, Causal Learning with EconML, 10x memory reductions for LightGBM, and a newly refactored integration with Vowpal Wabbit. To learn more check out our release notes and please feel give us a star if you enjoy the project!

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.11.0](https://github.com/microsoft/SynapseML/releases/tag/v0.11.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919)

Thank you to all the contributors in the community who made the release possible!

&#x200B;

[What's new in SynapseML v0.11](https://preview.redd.it/9pqj1mowj2wa1.png?width=4125&format=png&auto=webp&s=a358e73760c847a09cc76f2ed17dc58e15aed5ed)"
187,deeplearning,chatgpt,comments,2023-01-12 06:26:20,"Hi friends, we bring you the first bilingual ChatGPT detection toolset and would love your feedback~",Ok_Firefighter_2106,False,0.5,0,109sgrl,https://www.reddit.com/r/deeplearning/comments/109sgrl/hi_friends_we_bring_you_the_first_bilingual/,0,1673504780.0,"On 9 December 2022, we started a project on collecting comparison data from humans and ChatGPT, and methods of detecting ChatGPT-generated content.

Now, after a month of effort, we launched the **first bilingual** (EN/ZH)  [\#ChatGPT](https://twitter.com/hashtag/ChatGPT?src=hashtag_click) detecting tool set, consisting of **three** different models! 🎉  

We've also collected nearly 40K questions and their corresponding **human vs. ChatGPT comparison responses**, which will be released soon for future research!

&#x200B;

Detectors on 🤗 [@huggingface](https://twitter.com/huggingface) :

* [**QA version**](https://huggingface.co/spaces/Hello-SimpleAI/chatgpt-detector-qa)**:** detect whether an **answer** is generated by ChatGPT for a certain **question**, using PLM-based classifiers
* [**Single-text version**](https://huggingface.co/spaces/Hello-SimpleAI/chatgpt-detector-single): detect whether a **single** piece of text is ChatGPT generated, using PLM-based classifiers
* [**Linguistic version**](https://huggingface.co/spaces/Hello-SimpleAI/chatgpt-detector-ling)**:** detect whether a piece of text is ChatGPT generated, using **linguistic** features

Our **models and dataset will be open-sourced** in about a week. We look forward to receiving feedback from the community to help improve the models and make contributions to **open** academic research together:)

Project GitHub page: [ChatGPT Comparison Corpus (C3), Detectors, and more! 🔥](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection)

&#x200B;

https://preview.redd.it/3uqy9svtzjba1.png?width=2038&format=png&auto=webp&s=b6be9f3985111fba2337c7919761542d5a896b18

&#x200B;

https://preview.redd.it/etz77l3frtba1.png?width=2800&format=png&auto=webp&s=ea45f907e57dce8f35c5bb3bf2e66558bfd100a6

&#x200B;

https://preview.redd.it/y5rk8zwjrtba1.png?width=2584&format=png&auto=webp&s=5a4fcd814f4118a01ec05173e9d4b8f8efe1a310

https://preview.redd.it/hayhbjlszjba1.png?width=1454&format=png&auto=webp&s=5a627f0c42a120fcdce5ed152dc3f16e073b5f0f

&#x200B;"
188,deeplearning,chatgpt,comments,2023-03-02 07:25:30,Good news for builders! OpenAI Releases APIs To ChatGPT and Whisper,LesleyFair,False,0.69,6,11fwcxf,https://www.reddit.com/r/deeplearning/comments/11fwcxf/good_news_for_builders_openai_releases_apis_to/,0,1677741930.0,"If you were as disappointed as I was when you saw that access to Meta's LLaMA models is limited to researchers, you are going to like this.  


[APIs to ChatGPT and OpenAI's speech-to-text model whisper](https://openai.com/blog/introducing-chatgpt-and-whisper-apis) are available as of yesterday. Through system-wide optimizations, they claim to have reduced inference costs by 90%. They now price ChatGPT at $0.002 per 1000 tokens. Dedicated instances are available for speedup and make economic sense if you process \~450M tokens a day.  


Machine learning progress continues to be as fast as a banana peal skating on warm vaseline. 

If you found this useful and want to stay in the loop, consider subscribing to The Decoding. I send out a weekly 5-minute newsletter that keeps professionals in the loop about machine learning and the data economy. [Click here to subscribe!](https://thedecoding.net/)"
189,deeplearning,chatgpt,comments,2023-12-13 10:06:50,Durchbruch in der KI mit Gemini: ChatGPT 4.0 in Benchmark-Tests übertroffen!,Webglobic_tech,False,0.25,0,18hdkrs,https://webglobic.com/2023/12/12/gemini-uebertrifft-chatgpt4-0-in-benchmark-tests-ein-neuer-meilenstein-in-der-ki-entwicklung/,0,1702462010.0,
190,deeplearning,chatgpt,comments,2023-02-17 12:18:21,"ChatGPT - model, alignment and training explained",Combination-Fun,False,0.8,3,114j09j,/r/ChatGPT/comments/114izlj/chatgpt_model_alignment_and_training_explained/,0,1676636301.0,
191,deeplearning,chatgpt,comments,2023-06-12 16:29:03,"Openai Leak Docs, Possible for New Update? 😳",KSSolomon,False,0.5,0,147r660,https://i.redd.it/fqym39c36m5b1.jpg,0,1686587343.0,"Someone who uses Reddit, which is a big online forum where people can talk about all kinds of stuff, found out about this update by looking at some of the computer code that makes up the program.

They found out that the new version will have something called ""workspaces"". This would be like if you could make different profiles in a game, and the game would remember each one and what you did with them.

They also found that the program might be able to take files, which are like digital pieces of paper with stuff written on them.

OpenAI talked about making this business version of ChatGPT in April 2023, and they also said they would make it more private. This means that the things people say to ChatGPT wouldn't be used to make it smarter anymore.

Now, here are the implications of this update:

Businesses could use this version of ChatGPT to help them with their work. They might be able to give it files with information, and ChatGPT could use that to help answer questions or solve problems.

The ""workspaces"" feature could make it easier for people to use ChatGPT in different ways. For example, a business might have one workspace for customer service, and another for helping with paperwork.

The new privacy measures would mean that what you say to ChatGPT stays private. This is important because it helps keep people's information safe. It also means that OpenAI is listening to people's concerns about privacy and doing something about it.

This literally just happened if you want Ai news as it drops it launched [here first](https://www.therundown.ai/subscribe?utm_source=al). The whole article has been extrapolated here as well for convenience."
192,deeplearning,chatgpt,comments,2022-12-13 02:04:24,How to Talk to ChatGPT | An introduction to prompt,OnlyProggingForFun,False,0.3,0,zkj3z6,https://youtu.be/pZsJbYIFCCw,0,1670897064.0,
193,deeplearning,chatgpt,comments,2023-09-28 11:01:22,ChatGPT's Latest Upgrade: Access to Real-Time Information,marilaane,False,0.83,4,16ucwlq,https://www.brainyai.online/2023/09/chatgpts-latest-upgrade-access-to-real.html?m=1,0,1695898882.0,
194,deeplearning,chatgpt,comments,2023-11-16 08:28:39,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,1.0,1,17whz4d,https://v.redd.it/qx68wbuf7o0c1,0,1700123319.0,
195,deeplearning,chatgpt,comments,2023-02-07 19:42:25,New Weaviate Podcast - Adding ChatGPT to Weaviate!,HenryAILabs,False,1.0,1,10wb0ed,https://www.reddit.com/r/deeplearning/comments/10wb0ed/new_weaviate_podcast_adding_chatgpt_to_weaviate/,0,1675798945.0,"This podcast debuts the Weaviate generate module! The generate module is a new API in Weaviate that facilitates passing data from the Weaviate database to ChatGPT. Here is a snippet from Bob around the 43 minute mark I really enjoyed, describing how this kind of LLM technology is changing the world of database technology, ""Yeah so, what I’m really excited about and this is something that it’s just so funny right because if you see it, you have this huge epiphany. I’ve always been thinking of working with these models on input. Right so that they we can solve the problem of not having 100% keyword based search, so that we can have semantic search, image search, and those kind of things. I saw that as this beautiful uniqueness coming from a vector search engine or vector search database. So now what we’re adding is not only the input in the database but the output. So we’re basically saying we’re going to give you relevant information coming from the database, but that’s not per se stored inside the database. That’s new! I mean, just think about the most used databases in the world, Postgres, or MySQL, those kind of databases. It only outputs what’s in there. It makes sense. Because that’s how you use it. But now what we’re saying, is that’s fine you can do that, but also it can give you information, give you data that’s generated based on a task or prompt that you’re giving it. Having databases that make sense of it at input and generate new relevant content if that’s something you want as a user is amazing, and it’s just getting started. We should do this podcast like a half a year from now again and see how it's evolved because this is just too exciting man."". I really hope you enjoy the podcast, we are more than happy to answer any questions or help you get started with Weaviate!  


[https://www.youtube.com/watch?v=ro3ln4A9N8w](https://www.youtube.com/watch?v=ro3ln4A9N8w)"
196,deeplearning,chatgpt,comments,2023-04-07 08:41:41,A survey on graph diffusion models,Learningforeverrrrr,False,1.0,2,12eejpe,https://www.reddit.com/r/deeplearning/comments/12eejpe/a_survey_on_graph_diffusion_models/,0,1680856901.0,"Diffusion models have become a SOTA generative modeling method for numerous content types, such as images, audio, graph, etc. As the number of articles on diffusion models has grown exponentially over the past few years, there is an increasing need for survey works to summarize them. Recognizing the existence of such works, our team has completed multiple field-specific surveys on diffusion models. We promote our works here and hope they can be helpful to researchers in relative fields: text-to-image diffusion models [\[a survey\]](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey), audio diffusion models [\[a survey\]](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI), and graph diffusion models [\[a survey\]](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material) .

In the following, we briefly summarize our survey work on graph diffusion models.

[https://www.researchgate.net/publication/369716257\_A\_Survey\_on\_Graph\_Diffusion\_Models\_Generative\_AI\_in\_Science\_for\_Molecule\_Protein\_and\_Material](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material)

We start with a summary of the progress of graph generation before diffusion models. The diffusion models are then concisely presented and graph generation is discussed in depth from a structural and application perspective. Moreover,  the currently popular evaluation datasets and metrics are covered. Finally, we summarize the challenges and research questions still facing the research community. This survey work might be a useful guidebook for researchers who are interested in exploring the potential of diffusion models for graph generation and related tasks.

Moreover, we have also completed two survey works on generative AI (AIGC) [\[a survey\]](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need) and ChatGPT [\[a survey\]](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era), respectively. Interested readers may give it a look."
197,deeplearning,chatgpt,comments,2023-04-15 16:29:02,Generative Agents: Interactive Simulacra of Human Behavior - Discover a Town Run by 25 ChatGPTs,deeplearningperson,False,0.5,0,12na1yq,https://youtu.be/9LzuqQkXEjo,0,1681576142.0,
198,deeplearning,chatgpt,comments,2023-05-02 23:42:54,Longgboi 64K+ Context Size / Tokens Trained Open Source LLM and ChatGPT / GPT4 with Code Interpreter - Trained Voice Generated Speech,CeFurkan,False,1.0,1,13649d4,https://www.youtube.com/watch?v=v6TBtyO5Sxg&deeplearning,0,1683070974.0,
199,deeplearning,chatgpt,comments,2023-05-23 14:14:45,New Weaviate Podcast - Unstructured!,CShorten,False,0.76,2,13ppstr,https://www.reddit.com/r/deeplearning/comments/13ppstr/new_weaviate_podcast_unstructured/,0,1684851285.0,"ChatWithPDF has been one of the most captivating applications of the latest wave of ChatGPT and pairing ChatGPT with Retrieval-Augmentation and Vector Databases! As exciting as this is, there is a glaring problem... how do I get the text data out of my PDFs?

This is the problem Unstructured is solving with 3 core abstractions: (1) Partitioning (visually looking at elements on a PDF / Webpage / Resume / Slidedeck / Receipt / ... extracting the text data and adding metadata such as ""header"", ""body"", or ""image caption"", (2) Cleaning (I'm sure everyone in this group who has worked with text data has seen these ridiculous character encoding problems, regex, and whitespace removals we need to clean our text data for NLP pipelines), and (3) Staging (this describes extracting the JSONs / etc. to pass this data into another system such as Weaviate as an example)

I really hope you enjoy the podcast -- I think these innovations are so exciting for unlocking our data into these LLM systems!

[https://www.youtube.com/watch?v=b84Q2cJ6po8](https://www.youtube.com/watch?v=b84Q2cJ6po8)"
200,deeplearning,chatgpt,relevance,2023-10-09 16:07:35,[?] Local Chatgpt Alternative Hardware,gyrene2083,False,0.67,1,173vmgn,https://www.reddit.com/r/deeplearning/comments/173vmgn/local_chatgpt_alternative_hardware/,4,1696867655.0,"Hello all, I'm new to the Chatgpt world, my son showed me this at the beginning of ther year and I just now started messing around with it and have to say it's a tool.

That said I started thinking, I want to run this locally. This is the following hardware I have and I would appreciate your input if this is at all possible on my end.

2950 Threadripper

64GB Ram

Nvidia 1070 w/8gb Ram

\*EDIT just got my hands on an Nvidia RTX 3090 w/24gbs Ram

I use this computer for video transcoding, but that is rare these days. I want to put this computer to good use and figured why not learn more about local AI.

Anyway, thank you for all your help in advance."
201,deeplearning,chatgpt,relevance,2022-12-12 17:29:39,ChatGPT context length,Wild-Ad3931,False,0.97,30,zk5esp,https://www.reddit.com/r/deeplearning/comments/zk5esp/chatgpt_context_length/,10,1670866179.0,"How come ChatGPT can follow entire discussions whereas nowaday's LLM are limited (to the best of my knowledge) 4096 tokens ?

I asked it and it is not able to answer neither, and I found nothing on Google because no paper is published. I was also curious to understand how come Beamsearch was so fast with ChatGPT.

https://preview.redd.it/o6djsmpb5i5a1.png?width=1126&format=png&auto=webp&s=98baae5bf6fa294db408f0530214f8afa8a32a0b"
202,deeplearning,chatgpt,relevance,2023-12-24 00:01:05,MLX Mixtral 8x7b on M3 max 128GB | Better than chatgpt?,gimel1213,False,1.0,3,18pildv,https://youtube.com/watch?v=mFIsZHSAzJ0&si=wq6uLlHzXdg-yL72,0,1703376065.0,
203,deeplearning,chatgpt,relevance,2023-11-16 08:28:46,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,0.84,67,17whz6e,https://v.redd.it/qx68wbuf7o0c1,7,1700123326.0,
204,deeplearning,chatgpt,relevance,2023-09-28 11:01:22,ChatGPT's Latest Upgrade: Access to Real-Time Information,marilaane,False,0.85,5,16ucwlq,https://www.brainyai.online/2023/09/chatgpts-latest-upgrade-access-to-real.html?m=1,0,1695898882.0,
205,deeplearning,chatgpt,relevance,2023-06-11 15:56:18,ChatGPT interrogating bugs and errors!!,Available-Bass-7575,False,0.5,0,146xgyf,https://youtu.be/zfIyIScu0oo,1,1686498978.0,
206,deeplearning,chatgpt,relevance,2023-04-19 13:51:18,Alpaca Electron: ChatGPT Locally!,oridnary_artist,False,0.69,5,12rtzak,https://youtu.be/0oz3RaLlTlM,0,1681912278.0,
207,deeplearning,chatgpt,relevance,2023-03-09 09:27:13,ChatGPT vs Other Chatbots!!,Genius_feed,False,0.29,0,11mnvcr,https://i.redd.it/7eqx02homoma1.png,2,1678354033.0,
208,deeplearning,chatgpt,relevance,2023-04-25 23:50:32,Research on the political biases of ChatGPT,Mysterious_Potato132,False,0.86,5,12z08ni,/r/ChatGPT/comments/12yz49i/research_on_the_political_biases_of_chatgpt/,6,1682466632.0,
209,deeplearning,chatgpt,relevance,2023-03-11 00:38:10,Generate READMEs Using ChatGPT,tomd_96,False,0.86,9,11o5zyl,https://www.reddit.com/r/deeplearning/comments/11o5zyl/generate_readmes_using_chatgpt/,0,1678495090.0,"&#x200B;

https://i.redd.it/k375our2a0na1.gif

&#x200B;

You can use this program I wrote to generate readmes: [https://github.com/tom-doerr/codex-readme](https://github.com/tom-doerr/codex-readme)

&#x200B;

It's far from perfect, but I now added ChatGPT and it is surprisingly good at inferring what the project is about. It often generates interesting usage examples and explains the available command line options.

&#x200B;

You probably won't yet use this for larger projects, but I think this can make sense for small projects or single scripts. Many small scripts are very useful but might never be published because of the work that is required to document and explain it. Using this AI might assist you with that.

&#x200B;

Reportedly GPT-4 is coming out next week, which probably would make it even better.

&#x200B;

What do you think?"
210,deeplearning,chatgpt,relevance,2023-12-13 10:06:50,Durchbruch in der KI mit Gemini: ChatGPT 4.0 in Benchmark-Tests übertroffen!,Webglobic_tech,False,0.25,0,18hdkrs,https://webglobic.com/2023/12/12/gemini-uebertrifft-chatgpt4-0-in-benchmark-tests-ein-neuer-meilenstein-in-der-ki-entwicklung/,0,1702462010.0,
211,deeplearning,chatgpt,relevance,2023-12-15 04:50:13,OpenAI's Next Move: ChatGPT 4.5 Upgrade in the Works? Sam Altman Clarifies,Damanjain,False,0.13,0,18is4sm,https://thebuzz.news/article/openai-chatgpt-4-5-leak/11787/,1,1702615813.0,
212,deeplearning,chatgpt,relevance,2023-02-11 06:59:00,⭕ New Open-Source Version Of ChatGPT,LesleyFair,False,0.85,36,10zepkt,https://www.reddit.com/r/deeplearning/comments/10zepkt/new_opensource_version_of_chatgpt/,3,1676098740.0,"GPT is getting competition from open-source.

A group of researchers, around the YouTuber [Yannic Kilcher](https://www.ykilcher.com/), have announced that they are working on [Open Assistant](https://github.com/LAION-AI/Open-Assistant). The goal is to produce a chat-based language model that is much smaller than GPT-3 while maintaining similar performance.

If you want to support them, they are crowd-sourcing training data [here](https://open-assistant.io/).

**What Does This Mean?**

Current language models are too big.

They require millions of dollars of hardware to train and use. Hence, access to this technology is limited to big organizations. Smaller firms and universities are effectively shut out from the developments.

Shrinking and open-sourcing models will facilitate academic research and niche applications.

Projects such as Open Assistant will help to make language models a commodity. Lowering the barrier to entry will increase access and accelerate innovation.

What an exciting time to be alive! 

Thank you for reading! I really enjoyed making this for you!  
The Decoding ⭕ is a thoughtful weekly 5-minute email that keeps you in the loop about machine research and the data economy. [Click here to sign up](https://thedecoding.net/)!"
213,deeplearning,chatgpt,relevance,2023-06-01 14:21:15,How Does ChatGPT Learn: Reinforcement Learning Explained,OnlyProggingForFun,False,0.29,0,13xiv73,https://youtu.be/lWK9T56t-YM,0,1685629275.0,
214,deeplearning,chatgpt,relevance,2023-11-15 20:27:19,"Nvidia introduces the H200, an AI-crunching monster GPU that may speed up ChatGPT",keghn,False,0.93,13,17w2vgj,https://arstechnica.com/information-technology/2023/11/nvidia-introduces-its-most-powerful-gpu-yet-designed-for-accelerating-ai/,0,1700080039.0,
215,deeplearning,chatgpt,relevance,2023-04-05 01:36:40,Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT,Time_Key8052,False,0.95,88,12c43uu,https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/,19,1680658600.0,"Vicuna : ChatGPT Alternative, Open-Source, High Quality and Low Cost 

&#x200B;

[ Relative Response Quality Assessed by GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a072b91)

Vicuna-13B has demonstrated competitive performance against other open-source models, such as Stanford Alpaca, by fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.

Evaluation using GPT-4 as a judge shows that Vicuna-13B achieves more than 90% of the quality of OpenAI ChatGPT and Google Bard AI, while outperforming other models such as Meta LLaMA (Large Language Model Meta AI) and Stanford Alpaca in more than 90% of cases.

The cost of training Vicuna-13B is approximately $300.

The training and serving code, along with an online demo, are publicly available for non-commercial use.

&#x200B;

More Information : [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT](https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)

Discord Server : [https://discord.gg/h6kCZb72G7](https://discord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysorg](https://twitter.com/lmsysorg)"
216,deeplearning,chatgpt,relevance,2023-11-16 08:28:39,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,1.0,1,17whz4d,https://v.redd.it/qx68wbuf7o0c1,0,1700123319.0,
217,deeplearning,chatgpt,relevance,2023-04-10 08:13:15,Summarize documents with ChatGPT via Python scripts,rottoneuro,False,0.5,0,12hbq89,https://levelup.gitconnected.com/summarize-documents-with-chatgpt-a43456841cc4,1,1681114395.0,
218,deeplearning,chatgpt,relevance,2023-07-22 18:37:01,"LLAMA 2 - Model Explained, Demo and Comparison to ChatGPT",Combination-Fun,False,0.67,2,156roiv,https://www.reddit.com/r/deeplearning/comments/156roiv/llama_2_model_explained_demo_and_comparison_to/,0,1690051021.0,"LLAMA 2 is the largest and best opensource LLM every released free for commercial use. There have been several improvements that make LLAMA 2 better than LLAMA 1. Here is a video explaining the LLAMA 2 Model, a quick Demo of the model along with a Comparison to ChatGPT:

[https://youtu.be/TiloR3qRogs](https://youtu.be/TiloR3qRogs)

Hope its useful!"
219,deeplearning,chatgpt,relevance,2023-03-01 05:52:05,Career pivot for a SWE considering chatGPT disruption?,Which-Distance1384,False,0.6,1,11evrik,https://www.reddit.com/r/deeplearning/comments/11evrik/career_pivot_for_a_swe_considering_chatgpt/,5,1677649925.0,"tl;dr : what to do to board GPT ship?

&#x200B;

I really dont think ChatGPT will replace jobs very soon. Not until many software and apps are developed to replace workers, e.g., something that can do HR job, read internal docs and summarize, code from a design, etc etc. And all that needs experts. Given the field is pretty new and there are not many experts in this field, at least for the scale needed, I think the process of job loss takes some time. For near future probably we all SWEs can enjoy the more efficient coding, documentation readintg, etc. But for sure in the future there will be changes.

&#x200B;

I have been bored with my job as a SWE for a while. I have worked mostly in Cloud all my career (AWS and GCP) and always wanted to try something new.

&#x200B;

I find ChatGPT disruptive and interesting and want to be a part of it. I wonder what is the best way for me to join Large Language Model efforts? Which ways have best ROI?

\- Going to a PhD program and becoming a researcher?

\- Getting an MSC and become some sort of NLP AI engineer?

\- Finding internal teams that are willing to give me a try on their research/product?

&#x200B;

&#x200B;"
220,deeplearning,chatgpt,relevance,2023-04-16 18:03:39,ChatGPT Math Problem Challenge! (AAAI-MAKE 2023),Neurosymbolic,False,0.33,0,12oj6bi,https://youtube.com/watch?v=iRhbOE9U_Tk&feature=share,0,1681668219.0,
221,deeplearning,chatgpt,relevance,2023-08-05 17:09:44,The Quest to Have Endless Conversations with Llama and ChatGPT 🗣️💬,JClub,False,0.8,3,15j1117,https://medium.com/@joaolages/the-quest-to-have-endless-conversations-with-llama-and-chatgpt-%EF%B8%8F-81360b9b34b2,0,1691255384.0,
222,deeplearning,chatgpt,relevance,2022-12-06 01:38:42,ChatGPT explained in 5 minutes,OnlyProggingForFun,False,0.5,0,zdr6l7,https://youtu.be/AsFgn8vU-tQ,0,1670290722.0,
223,deeplearning,chatgpt,relevance,2023-02-17 12:18:21,"ChatGPT - model, alignment and training explained",Combination-Fun,False,0.67,2,114j09j,/r/ChatGPT/comments/114izlj/chatgpt_model_alignment_and_training_explained/,0,1676636301.0,
224,deeplearning,chatgpt,relevance,2023-04-30 03:53:26,Why do neural networks like ChatGPT use memory and processing power while at rest?,Will_Tomos_Edwards,False,0.38,0,133f4m4,https://www.reddit.com/r/deeplearning/comments/133f4m4/why_do_neural_networks_like_chatgpt_use_memory/,13,1682826806.0,"It is surprising to me that the default behavior of many neural networks is to run processes and use resources (RAM, CPU or their equivalent) . Why are they not just stored in memory by default? Obviously they must take up a lot of memory, but I don't understand why the default behaviour is to be active."
225,deeplearning,chatgpt,relevance,2023-04-09 19:34:12,"ChatGPT for free now , GPT4ALL is now here",oridnary_artist,False,0.79,8,12gt77m,https://www.youtube.com/watch?v=WiCYfi3SUTE&t=1s,0,1681068852.0,
226,deeplearning,chatgpt,relevance,2023-10-25 01:25:27,How we Built an Open-Source RAG-based ChatGPT Web App: Meet Our new AI Tutor!,OnlyProggingForFun,False,0.6,1,17ft66e,https://youtu.be/7ytyK6u3aAk,0,1698197127.0,
227,deeplearning,chatgpt,relevance,2023-04-29 09:34:51,Connecting assistants to ChatGPT is nuts! JARVIS is ever closer!,Lewenhart87,False,0.33,0,132ogn4,https://v.redd.it/4257us79jswa1,1,1682760891.0,
228,deeplearning,chatgpt,relevance,2022-12-04 20:11:36,5 ChatGPT Tutorial for Total Beginners,dulldata,False,0.4,0,zck620,https://www.youtube.com/watch?v=gMb4iYHaONQ,0,1670184696.0,
229,deeplearning,chatgpt,relevance,2022-12-05 02:22:37,Building A Virtual Machine Inside ChatGPT,x_abyss,False,0.81,3,zctzmf,https://www.engraved.blog/building-a-virtual-machine-inside/,0,1670206957.0,
230,deeplearning,chatgpt,relevance,2023-02-07 19:42:25,New Weaviate Podcast - Adding ChatGPT to Weaviate!,HenryAILabs,False,1.0,1,10wb0ed,https://www.reddit.com/r/deeplearning/comments/10wb0ed/new_weaviate_podcast_adding_chatgpt_to_weaviate/,0,1675798945.0,"This podcast debuts the Weaviate generate module! The generate module is a new API in Weaviate that facilitates passing data from the Weaviate database to ChatGPT. Here is a snippet from Bob around the 43 minute mark I really enjoyed, describing how this kind of LLM technology is changing the world of database technology, ""Yeah so, what I’m really excited about and this is something that it’s just so funny right because if you see it, you have this huge epiphany. I’ve always been thinking of working with these models on input. Right so that they we can solve the problem of not having 100% keyword based search, so that we can have semantic search, image search, and those kind of things. I saw that as this beautiful uniqueness coming from a vector search engine or vector search database. So now what we’re adding is not only the input in the database but the output. So we’re basically saying we’re going to give you relevant information coming from the database, but that’s not per se stored inside the database. That’s new! I mean, just think about the most used databases in the world, Postgres, or MySQL, those kind of databases. It only outputs what’s in there. It makes sense. Because that’s how you use it. But now what we’re saying, is that’s fine you can do that, but also it can give you information, give you data that’s generated based on a task or prompt that you’re giving it. Having databases that make sense of it at input and generate new relevant content if that’s something you want as a user is amazing, and it’s just getting started. We should do this podcast like a half a year from now again and see how it's evolved because this is just too exciting man."". I really hope you enjoy the podcast, we are more than happy to answer any questions or help you get started with Weaviate!  


[https://www.youtube.com/watch?v=ro3ln4A9N8w](https://www.youtube.com/watch?v=ro3ln4A9N8w)"
231,deeplearning,chatgpt,relevance,2023-10-21 04:19:19,"Is there any tool or LLM like chatgpt,midjourney that can help us train and generate custom sounds",Beginning_Finding_98,False,0.38,0,17cu8ah,https://www.reddit.com/r/deeplearning/comments/17cu8ah/is_there_any_tool_or_llm_like_chatgptmidjourney/,1,1697861959.0,"&#x200B;

**Generating a Wide Variety of Sounds**

I'm a non-technical person with very little knowledge to develop AI tools and intending to learn Python  and based on that My question is as follows:

&#x200B;

Are there tools or chatgpt like platforms that can help people like me to generate couple of sounds like dog barks, cat meows. I want either something that can generate a variety of sounds or I want to work towards making something that cane help me generate audios  like dog barks, such as fierce, aggressive ones but not just limited to dog barks but also sound focused on nature, other animals, vehicles, machinery(e.g., honks, engine sounds ), and possibly human sounds (though that's not my primary focus for now).

**The amount of technical Assistance Needed**

I also came across a  tool like Teachable Machine and was wondering if it could be a solution as it does offer tools for audio. I am also aware that I would need datasets for such a task but apart from that I am not too sure about the nitty gritty or should I say the intricacies involved as well as the knowledge needed as I do assume it is likely not very easy [https://www.youtube.com/watch?v=L4GOmYPPqn8&t=1854s](https://www.youtube.com/watch?v=L4GOmYPPqn8&t=1854s)

&#x200B;

\[Teachable Machine\]([https://teachablemachine.withgoogle.com/](https://teachablemachine.withgoogle.com/))

&#x200B;

**Inspiration**

I was inspired by a project I found here: \[[https://x.com/TheAIAnonGuy/status/1684443155448360961?s=20](https://x.com/TheAIAnonGuy/status/1684443155448360961?s=20)\] 

&#x200B;

&#x200B;

Can anyone provide insights, guidance, or recommendations on how to accomplish this?

**To be fair, I'm not really sure if this is an audio-related or neural/machine learning (ML)/deep learning related learning question.**

 But I would like more insight if this is possible on an individual scale either with teachable, code or AI or a combination of all approaches and if there are any beginner friendly ways to achieve this

Thank you all for your assistance!"
232,deeplearning,chatgpt,relevance,2023-01-17 16:06:37,What nobody tells you about chatGPT and GPT-4,thomas999999,False,0.27,0,10efwno,https://www.reddit.com/r/deeplearning/comments/10efwno/what_nobody_tells_you_about_chatgpt_and_gpt4/,3,1673971597.0,i wanted to share a nice writeup my friend made about chatGPT [https://medium.com/@christian.bernhard97/what-nobody-tells-you-about-chatgpt-and-gpt-4-c8d97ae9f92d](https://medium.com/@christian.bernhard97/what-nobody-tells-you-about-chatgpt-and-gpt-4-c8d97ae9f92d)
233,deeplearning,chatgpt,relevance,2023-03-10 05:22:26,[POC] ChatGPT Audio Bot (like Google Assistant and Alexa) - Opensource,JC1DA,False,0.79,15,11nfrhw,https://www.reddit.com/r/deeplearning/comments/11nfrhw/poc_chatgpt_audio_bot_like_google_assistant_and/,2,1678425746.0,"Hey guys, just wanna share this bot that I quickly built using ChatGPT API.

GitHub page: [https://github.com/LanyTek/ChatGPT\_Audio\_Bot](https://github.com/LanyTek/ChatGPT_Audio_Bot)

This service allows you to keep talking to the bot using your voice like you often do with Google Assistant or Alexa. It can be used in a lot of scenarios like teaching, gossiping, or quickly retrieving information without the need of typing. 

I have a quick video demo here if you wanna check [https://www.youtube.com/watch?v=e9n0BJfMyKw](https://www.youtube.com/watch?v=e9n0BJfMyKw)

It's open source so feel free to use it for anything you like :)"
234,deeplearning,chatgpt,relevance,2023-02-16 11:58:35,Create youtube video using ChatGPT and Pictory AI only.,coder4mzero,False,0.5,0,113oxh5,https://youtu.be/iSz4Q_d7JR8,0,1676548715.0,
235,deeplearning,chatgpt,relevance,2023-03-22 21:55:31,ChatLLaMA – A ChatGPT style chatbot for Facebook's LLaMA,imgonnarelph,False,0.96,18,11yy5es,https://chatllama.baseten.co/,2,1679522131.0,
236,deeplearning,chatgpt,relevance,2023-03-15 00:07:51,GPTMinusOne - AI that hides the use of ChatGPT and GPT4,tomd_96,False,0.87,11,11rfgbs,https://github.com/tom-doerr/gpt_minus_one,3,1678838871.0,
237,deeplearning,chatgpt,relevance,2022-11-30 19:14:16,OpenAI's new impressive Conversational LLM - ChatGPT,dulldata,False,1.0,1,z90966,https://www.youtube.com/watch?v=2VJZky25rIs,0,1669835656.0,
238,deeplearning,chatgpt,relevance,2023-03-08 01:18:38,"You probably have heard of chatgpt, have you heard of MarioGPT?",Shubhra22,False,0.67,1,11lhwsp,https://youtu.be/3h_kNjbWrdw,0,1678238318.0,
239,deeplearning,chatgpt,relevance,2023-03-02 07:25:30,Good news for builders! OpenAI Releases APIs To ChatGPT and Whisper,LesleyFair,False,0.69,6,11fwcxf,https://www.reddit.com/r/deeplearning/comments/11fwcxf/good_news_for_builders_openai_releases_apis_to/,0,1677741930.0,"If you were as disappointed as I was when you saw that access to Meta's LLaMA models is limited to researchers, you are going to like this.  


[APIs to ChatGPT and OpenAI's speech-to-text model whisper](https://openai.com/blog/introducing-chatgpt-and-whisper-apis) are available as of yesterday. Through system-wide optimizations, they claim to have reduced inference costs by 90%. They now price ChatGPT at $0.002 per 1000 tokens. Dedicated instances are available for speedup and make economic sense if you process \~450M tokens a day.  


Machine learning progress continues to be as fast as a banana peal skating on warm vaseline. 

If you found this useful and want to stay in the loop, consider subscribing to The Decoding. I send out a weekly 5-minute newsletter that keeps professionals in the loop about machine learning and the data economy. [Click here to subscribe!](https://thedecoding.net/)"
240,deeplearning,chatgpt,relevance,2022-12-13 02:04:24,How to Talk to ChatGPT | An introduction to prompt,OnlyProggingForFun,False,0.33,0,zkj3z6,https://youtu.be/pZsJbYIFCCw,0,1670897064.0,
241,deeplearning,chatgpt,relevance,2022-12-04 09:59:19,OpenAI’s ChatGPT is unbelievable good in telling stories!,Far_Pineapple770,False,0.33,0,zc5tc3,/r/MachineLearning/comments/zc5sg6/d_openais_chatgpt_is_unbelievable_good_in_telling/,0,1670147959.0,
242,deeplearning,chatgpt,relevance,2023-02-01 15:20:25,Launching my first-ever open-source project and it might make your ChatGPT answers better,Vegetable-Skill-9700,False,0.75,2,10qx9po,https://www.reddit.com/r/deeplearning/comments/10qx9po/launching_my_firstever_opensource_project_and_it/,6,1675264825.0,"I am building UpTrain - an open-source ML diagnostic toolkit that recently got investment from YCombinator.

As you know no ML model is 100% accurate, and, further, their accuracy deteriorates over time 😣. Additionally, due to the black boxiness ⬛ nature of Large Language models, it's challenging to identify and fix their problems.

The tool helps ML practitioners to:
1. Understand how their models are performing in production
2. Catch edge cases and outliers to help them refine their models
3. Allow them to define custom monitors to catch under-performing data-points
4. Retrain the model on them to improve its accuracy

You can check out the project here: https://github.com/uptrain-ai/uptrain. Would love to hear feedback from the community!"
243,deeplearning,chatgpt,relevance,2023-04-15 16:29:02,Generative Agents: Interactive Simulacra of Human Behavior - Discover a Town Run by 25 ChatGPTs,deeplearningperson,False,0.5,0,12na1yq,https://youtu.be/9LzuqQkXEjo,0,1681576142.0,
244,deeplearning,chatgpt,relevance,2022-12-05 01:45:02,Thread: Top 10 ways you can use ChatGPT for Music related stuff,dicklesworth,False,0.77,7,zct1o6,/r/musictheory/comments/zcso1s/thread_top_10_ways_you_can_use_chatgpt_for_music/,0,1670204702.0,
245,deeplearning,chatgpt,relevance,2023-04-26 13:19:51,Report: ChatGPT's Myers-Briggs personality type is ENFJ and it shows strong signs of Egoism and Sadism,Excellent_Cup3709,False,0.22,0,12zhck0,https://www.researchgate.net/publication/370071092_The_Self-Perception_and_Political_Biases_of_ChatGPT,2,1682515191.0,
246,deeplearning,chatgpt,relevance,2023-04-05 13:21:51,"New Weaviate Podcast (#42) - ChatGPT Plugin Marketplace, Alpaca Models, Semantic Search on S3, and more!",CShorten,False,0.76,2,12ck7ae,https://www.reddit.com/r/deeplearning/comments/12ck7ae/new_weaviate_podcast_42_chatgpt_plugin/,0,1680700911.0," I am beyond excited to share our latest Weaviate Podcast with Ethan Steininger! Ethan is the founder of Mixpeek and creator of Collie.ai!

Ethan began by explaining how he came into search through integrating MongoDB with the Lucene inverted index. Ethan continued explaining how his background in Sales Engineering helped him to see the recurring problems businesses are facing when trying to utilize the latest LLM and Vector Database technologies to solve their problems.

We then continued to take a tour of all sorts of topics in the AI Landscape from the impact of the ChatGPT Plugin Marketplace / New App Store for AI to the Stanford Alpaca models, the impact of LLMs for coding productivity and many more, even ending with Ethan's advice on stress management by getting into nature and our thoughts on the existential fear technologies like GPT-4 inspire in many and the implications of it on society.

I hope you enjoy the podcast, please let us know what you think!

[https://www.youtube.com/watch?v=EDPk1umuge0](https://www.youtube.com/watch?v=EDPk1umuge0)"
247,deeplearning,chatgpt,relevance,2023-02-17 02:54:52,How likely is ChatGPT to be weaponized as an information pollution tool? What are the possible implementation paths? How to prevent possible attacks?,zcwang0702,False,0.7,9,1148t20,https://www.reddit.com/r/deeplearning/comments/1148t20/how_likely_is_chatgpt_to_be_weaponized_as_an/,14,1676602492.0,
248,deeplearning,chatgpt,relevance,2023-01-29 22:39:07,[P] We built a browser extension that unlocks browser mode capabilities using ChatGPT: MULTI·ON: AI Web Co-Pilot powered by ChatGPT,DragonLord9,False,0.95,67,10okyg3,https://v.redd.it/2hw47h0b82fa1,8,1675031947.0,
249,deeplearning,chatgpt,relevance,2023-04-06 07:43:06,A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?,Learningforeverrrrr,False,0.56,1,12dcnrm,https://www.reddit.com/r/deeplearning/comments/12dcnrm/a_complete_survey_on_generative_ai_aigc_is/,0,1680766986.0,"We recently completed two surveys: one on generative AI and the other on ChatGPT. Generative AI and ChatGPT are two fast-evolving research fields, and we will update the content soon, for which your feedback is appreciated (you can reach out to us through emails on the paper).

The title of this post refers to the first one, however, we put both links below.

**Link to a survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

**Link to a survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)

The following is the **abstract** of the **survey on generative AI** with a summary **figure**.

As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible to miss the opportunity to glimpse AIGC from a certain angle.  In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? To answer this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its **techniques** to **applications**. Modern generative AI relies on various technical foundations, ranging from **model architecture** and **self-supervised pretraining** to **generative modeling** methods (like GAN and diffusion models). After introducing the fundamental techniques, this work focuses on the technological development of various AIGC tasks based on their output type, including **text**, **images**, **videos**, **3D content**, **etc**., which depicts the full potential of ChatGPT's future. Moreover, we summarize their significant applications in some mainstream **industries**, such as **education** and **creativity** content. Finally, we discuss the **challenges** currently faced and present an **outlook** on how generative AI might evolve in the near future.

&#x200B;

https://preview.redd.it/scbpeabnx7sa1.png?width=1356&format=png&auto=webp&s=445da6a707ceb6af75e5305137ad30dcd06c32fe

**Link to a survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

**Link to a survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)"
250,deeplearning,chatgpt,relevance,2022-12-22 09:57:14,"Show ChatGPT's response next to the search results from Google, Bing, and DuckDuckGo.",Harrypham22,False,1.0,1,zsics7,https://www.reddit.com/r/deeplearning/comments/zsics7/show_chatgpts_response_next_to_the_search_results/,0,1671703034.0," **Do you know about ChatGPT?** 

It is a variant of the GPT-3 language model developed by OpenAI specifically designed for generating responses to user input in chat or messaging applications. It is trained on a large dataset of conversation data and is able to understand the context of a conversation and generate appropriate responses. ChatGPT is particularly well-suited for use in chat or messaging applications, but it can also be used for a wide range of other natural language processing tasks, such as language translation, summarization, and question answering.

**Display ChatGPT response alongside other search engine results (Google,Bing,Duck go go,..)**

***ChatGPT for Search Engines*** is an AI-based extension that could potentially become a real threat to any search engine.

It basically shows results to all sorts of queries next to the Google results (or other search engine). The precision is very impressive. This extension makes it possible everywhere you browse

This is a simple extension that show response from ChatGPT alongside Google and other search engines

Features:

\* Markdown rendering

\* Code hightlights

\* Feedback buttons

\* Custom trigger mode

Maybe you should try this extension: [shorturl.at/eqZ78](https://shorturl.at/eqZ78)

Watch this video to see how it work: [https://www.tiktok.com/@ai\_life26/video/7179865803003579674](https://www.tiktok.com/@ai_life26/video/7179865803003579674)

https://preview.redd.it/ojjxcy2q9f7a1.png?width=1294&format=png&auto=webp&s=e3b02aba9ba03f37dbdcd0a2c6265a95b9f11ed8"
251,deeplearning,chatgpt,relevance,2023-06-30 20:42:58,Learn how to leverage custom NLP models and chatGPT to analyze risk factors from SEC 10-K reports in this insightful tutorial,Molly_Knight0,False,1.0,1,14nbjqu,https://ubiai.tools/analyze-company-risk-factors-from-sec-reports-with-ai/,0,1688157778.0,
252,deeplearning,chatgpt,relevance,2023-01-12 06:26:20,"Hi friends, we bring you the first bilingual ChatGPT detection toolset and would love your feedback~",Ok_Firefighter_2106,False,0.5,0,109sgrl,https://www.reddit.com/r/deeplearning/comments/109sgrl/hi_friends_we_bring_you_the_first_bilingual/,0,1673504780.0,"On 9 December 2022, we started a project on collecting comparison data from humans and ChatGPT, and methods of detecting ChatGPT-generated content.

Now, after a month of effort, we launched the **first bilingual** (EN/ZH)  [\#ChatGPT](https://twitter.com/hashtag/ChatGPT?src=hashtag_click) detecting tool set, consisting of **three** different models! 🎉  

We've also collected nearly 40K questions and their corresponding **human vs. ChatGPT comparison responses**, which will be released soon for future research!

&#x200B;

Detectors on 🤗 [@huggingface](https://twitter.com/huggingface) :

* [**QA version**](https://huggingface.co/spaces/Hello-SimpleAI/chatgpt-detector-qa)**:** detect whether an **answer** is generated by ChatGPT for a certain **question**, using PLM-based classifiers
* [**Single-text version**](https://huggingface.co/spaces/Hello-SimpleAI/chatgpt-detector-single): detect whether a **single** piece of text is ChatGPT generated, using PLM-based classifiers
* [**Linguistic version**](https://huggingface.co/spaces/Hello-SimpleAI/chatgpt-detector-ling)**:** detect whether a piece of text is ChatGPT generated, using **linguistic** features

Our **models and dataset will be open-sourced** in about a week. We look forward to receiving feedback from the community to help improve the models and make contributions to **open** academic research together:)

Project GitHub page: [ChatGPT Comparison Corpus (C3), Detectors, and more! 🔥](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection)

&#x200B;

https://preview.redd.it/3uqy9svtzjba1.png?width=2038&format=png&auto=webp&s=b6be9f3985111fba2337c7919761542d5a896b18

&#x200B;

https://preview.redd.it/etz77l3frtba1.png?width=2800&format=png&auto=webp&s=ea45f907e57dce8f35c5bb3bf2e66558bfd100a6

&#x200B;

https://preview.redd.it/y5rk8zwjrtba1.png?width=2584&format=png&auto=webp&s=5a4fcd814f4118a01ec05173e9d4b8f8efe1a310

https://preview.redd.it/hayhbjlszjba1.png?width=1454&format=png&auto=webp&s=5a627f0c42a120fcdce5ed152dc3f16e073b5f0f

&#x200B;"
253,deeplearning,chatgpt,relevance,2023-05-02 23:42:54,Longgboi 64K+ Context Size / Tokens Trained Open Source LLM and ChatGPT / GPT4 with Code Interpreter - Trained Voice Generated Speech,CeFurkan,False,1.0,1,13649d4,https://www.youtube.com/watch?v=v6TBtyO5Sxg&deeplearning,0,1683070974.0,
254,deeplearning,chatgpt,relevance,2023-05-29 13:38:03,"Ortus - Chat with YouTube | ChatGPT Chrome extension -> use it to learn ML, supports The AI Epiphany YT channel, and more to come!",gordicaleksa,False,0.63,2,13uv79y,https://www.youtube.com/watch?v=0V9Jw6haJHQ,1,1685367483.0,
255,deeplearning,chatgpt,relevance,2023-04-22 08:22:10,"ChatGPT TED talk is the hottest discussion! Almost 370+ comments and 400k views in just 20 hours, if you are interested in AI, come talk!",Ok-Judgment-1181,False,0.17,0,12uzdhn,/r/ChatGPT/comments/12tycz4/chatgpt_ted_talk_is_mind_blowing/,0,1682151730.0,
256,deeplearning,chatgpt,relevance,2023-01-19 11:43:34,Join us this Friday 6 pm EST for a fascinating discussion about the societal impact of large language models (LLMs) like ChatGPT,OnlyProggingForFun,False,1.0,1,10fzn3l,https://discord.gg/ehPqT6rym8?event=1063903315974443162,0,1674128614.0,
257,deeplearning,chatgpt,relevance,2022-12-14 15:04:39,ChatGPT and Search Technology - New Weaviate Podcast with CEO Bob van Luijt and FAQx co-founders Chris Dossman and Marco Bianoc,HenryAILabs,False,0.5,0,zltb3s,https://www.reddit.com/r/deeplearning/comments/zltb3s/chatgpt_and_search_technology_new_weaviate/,0,1671030279.0,"ChatGPT has landed, leaving a massive impact on the world of technology! This podcast features visionaries and builders at the cutting edge of Search technology, discussing how recent advances like ChatGPT will change the way we use Search Engines! 

Link: [https://www.youtube.com/watch?v=s9aVAgk-6Ww](https://www.youtube.com/watch?v=s9aVAgk-6Ww) (also on Spotify - Weaviate Podcast)"
258,deeplearning,chatgpt,relevance,2023-12-22 11:45:32,Team GPT's Picks: Top 15 AI Tools for 2024 Productivity,LongjmpingShower,False,0.67,1,18od3vg,https://www.reddit.com/r/deeplearning/comments/18od3vg/team_gpts_picks_top_15_ai_tools_for_2024/,0,1703245532.0,"Some of the best AI tools to increase productivity are Team-GPT, Notion AI, Asana Intelligence, Salesforce Einstein, Zia, HubSpot AI, Jasper, Canva AI, Copy AI, Zapier, IFTTT, Outreach, Otter, Rev, Trint. In this article, I review all the AI software mentioned above in detail by telling you their features, pros, cons, and pricing categorically.  
**Learn more>>>**[https://team-gpt.com/learn/chatgpt-for-work-course](https://team-gpt.com/learn/chatgpt-for-work-course)  


https://preview.redd.it/nxwv3gff3u7c1.png?width=824&format=png&auto=webp&s=72a5fd8feb3ee0199df5c25962014a0b45ed3678"
259,deeplearning,chatgpt,relevance,2023-06-28 15:56:28,"[SEEKING FEEDBACK] I built a Chrome Extension AI tool that brings ChatGPT directly to any website. It's powered by GPT4 with access to the web, email reply, auto-suggest prompt, summarize and read screen features.",RidiculusRex,False,0.67,1,14ld8h3,https://v.redd.it/xuumhvhc5s8b1,1,1687967788.0,
260,deeplearning,chatgpt,relevance,2023-06-12 16:29:03,"Openai Leak Docs, Possible for New Update? 😳",KSSolomon,False,0.5,0,147r660,https://i.redd.it/fqym39c36m5b1.jpg,0,1686587343.0,"Someone who uses Reddit, which is a big online forum where people can talk about all kinds of stuff, found out about this update by looking at some of the computer code that makes up the program.

They found out that the new version will have something called ""workspaces"". This would be like if you could make different profiles in a game, and the game would remember each one and what you did with them.

They also found that the program might be able to take files, which are like digital pieces of paper with stuff written on them.

OpenAI talked about making this business version of ChatGPT in April 2023, and they also said they would make it more private. This means that the things people say to ChatGPT wouldn't be used to make it smarter anymore.

Now, here are the implications of this update:

Businesses could use this version of ChatGPT to help them with their work. They might be able to give it files with information, and ChatGPT could use that to help answer questions or solve problems.

The ""workspaces"" feature could make it easier for people to use ChatGPT in different ways. For example, a business might have one workspace for customer service, and another for helping with paperwork.

The new privacy measures would mean that what you say to ChatGPT stays private. This is important because it helps keep people's information safe. It also means that OpenAI is listening to people's concerns about privacy and doing something about it.

This literally just happened if you want Ai news as it drops it launched [here first](https://www.therundown.ai/subscribe?utm_source=al). The whole article has been extrapolated here as well for convenience."
261,deeplearning,chatgpt,relevance,2023-08-02 11:33:32,Using PDFs with GPT Models,Disastrous_Look_1745,False,0.76,11,15g6i4x,https://www.reddit.com/r/deeplearning/comments/15g6i4x/using_pdfs_with_gpt_models/,2,1690976012.0,Found a blog talking about how we can interact with PDFs in Python by using GPT API & Langchain. It talks about some pretty cool automations you can build involving PDFs - [https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/](https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/)
262,deeplearning,chatgpt,relevance,2024-01-10 06:21:19,Are NLP based AI skills useless?,SnooBeans7516,False,0.71,12,1931m9b,https://www.reddit.com/r/deeplearning/comments/1931m9b/are_nlp_based_ai_skills_useless/,9,1704867679.0,"So something's been on my mind recently and I wanted to get Reddit's thoughts.

The thing that is troubling me as I get deeper, is it seems that for a lot of language based NLP tasks, ChatGPT or these other foundation models seem SOTA for 99% of tasks. In terms of developing valuable skills and products, it feels like the two options are: 

1. Research and training foundation models at large research labs 

2. Using GPT APIs in some sort of system (RAG or just raw prompts)

I feel like this issue is rooted in language itself being a somewhat static medium that can be mostly mastered by a single model. As opposed to images where we see a lot of cool projects from people that have to employ many different techniques to complete a specific task.

\## Example

Unlike images, the use cases seem much more straightforward, and don't require any special work other than throwing GPT at it. 

For example, I wanted to create a BERT like model to replace ingredients in a recipe, conditioned on a user's requirements or desires. The problem is, the more I worked on this and trained some models, the more it seemed that ChatGPT could just do it better with some prompt engineering...

==================

All this to say, I was previously motivated to be a part-time NLP researcher, implementing research into products in cool ways. Now it kind of feels like the only actually valuable investment is into ""AI engineering"", building systems using a GPT API:/"
263,deeplearning,chatgpt,relevance,2023-04-07 10:28:52,"Series of Surveys on ChatGPT, Generative AI (AIGC), and Diffusion Models",Learningforeverrrrr,False,0.86,9,12egmab,https://www.reddit.com/r/deeplearning/comments/12egmab/series_of_surveys_on_chatgpt_generative_ai_aigc/,0,1680863332.0,"* **A survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)
* **A survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)
* **A survey on Text-to-image diffusion models:** [**Text-to-image Diffusion Models in Generative AI: A Survey**](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)
* **A survey on Audio diffusion models:** [**A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI**](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI)
* **A survey on Graph diffusion models:** [**A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material**](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material)

**ChatGPT goes viral.** Launched by OpenAI on November 30, 2022, ChatGPT has attracted unprecedented attention due to its powerful abilities all over the world.  It took only 5 days \[1\] and 2 months \[2\] for ChatGPT to have 1 million users and 100 million monthly users after launch, making it the fastest-growing consumer application in history. ChatGPT can be seen as the milestone for the GPT family to go viral. In academia, ChatGPT has also inspired a large number of works discussing its applications in multiple fields, with **more than 500 papers within four months** after release and **the number is still increasing rapidly.**  This brings a huge challenge for a researcher who hopes to have an overview of ChatGPT applications or hopes to start his or her journey with ChatGPT in their own field.  **To help more people keep up with the latest progress of the GPT family,** we’re glad to share a self-contained survey that not only summarizes **the recent applications** of ChatGPT and other GPT variants like GPT-4, but also introduces the **underlying techniques** and **challenges.** Please refer to the following link for the paper: [One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era).

&#x200B;

**From ChatGPT to Generative AI.**  One highlighting ability of the GPT family is that it can generate natural languages, which falls into the area of Generative AI. Apart from text, Generative AI can also generate content in other modalities, such as image, audio, and graph. More excitingly, Generative AI is able to convert data from one modality to another one, such as the text-to-image task (generating images from text). **To help readers have a better overview of Generative AI,** we provide a complete survey on underlying **techniques,** summary and development of **typical tasks in academia**, and also **industrial applications.** Please refer to the following link for the paper.  [A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

&#x200B;

**From Generative AI to Diffusion Models.** The prosperity of a field is always driven by the development of technology, and so is Generative AI.  Different from ChatGPT which generates text based on the transformer, **diffuson models** have greatly accelerated the development of other fields in Generative AI, such as image synthesis.  Although we provide a summary of diffusion models and typical tasks in the Generative AI survey, we cannot include detailed discussions due to paper length limitations. **For those who are interested in the technical details of diffusion models and the recent progress of their applications in Generative AI,** we provide three self-contained surveys on **how diffusion models are applied in three typical areas: Text-to-image diffusion models** (also includes related tasks such as image editing)**, Audio diffusion models** (including text to speech synthesis and enhancement), and **Graph diffusion models** (including molecule, protein and material areas). Please refer to the following links for the paper.

* [Text-to-image Diffusion Models in Generative AI: A Survey](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)
* [A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI)
* [A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material)

We hope our survey series will help people for a better understanding of ChatGPT and Generative AI, and we will update the survey regularly to include the latest progress. Please refer to the personal pages of the authors for the latest updates on surveys. If you have any suggestions or problems, please feel free to contact us.

\[1\] Greg Brockman, co-founder of OpenAI, [https://twitter.com/gdb/status/1599683104142430208?lang=en](https://twitter.com/gdb/status/1599683104142430208?lang=en)

\[2\] Reuters, [https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/)"
264,deeplearning,chatgpt,relevance,2023-02-06 00:07:43,ChatGPT: From Nowhere to Knowledgeable,crawfa,False,0.61,3,10urypd,https://www.reddit.com/r/deeplearning/comments/10urypd/chatgpt_from_nowhere_to_knowledgeable/,0,1675642063.0,"ChatGPT is taking the world by storm, and is now the fastest growing software application ever, eclipsing TikTok, which may soon be the fastest shrinking software application ever if it gets banned in the US. This article explains at a high level what ChatGPT is, how it works at a high level, what you can do with it, as well as some developer choices they have made and identifies some things that ChatGPT does not do well.

[https://open.substack.com/pub/stayblog/p/chatgpt-from-nowhere-to-knowledgeable?r=1qxkwn&utm\_campaign=post&utm\_medium=web](https://open.substack.com/pub/stayblog/p/chatgpt-from-nowhere-to-knowledgeable?r=1qxkwn&utm_campaign=post&utm_medium=web)"
265,deeplearning,chatgpt,relevance,2024-01-18 05:16:45,How can I make LLM plot graphs/figures on my database with RAG?,HappyDataGuy,False,0.67,1,199ieeu,https://www.reddit.com/r/deeplearning/comments/199ieeu/how_can_i_make_llm_plot_graphsfigures_on_my/,1,1705555005.0,I want for LLM like ChatGPT to plot graph but not with code-interpretor which requires uploading the data to openai. is there any way to achieve this? 
266,deeplearning,chatgpt,relevance,2023-09-28 16:54:46,"The meme of man on chair with cardboard offering model parameter tuning for a few bucks, where to find?",tvtavtat,False,1.0,1,16ul8ej,https://www.reddit.com/r/deeplearning/comments/16ul8ej/the_meme_of_man_on_chair_with_cardboard_offering/,1,1695920086.0,"I'm sure you must know what I am talking about. I just can't find it, image search, bard, chatgpt, no one works. I need to real intelligence to help."
267,deeplearning,chatgpt,relevance,2024-01-11 11:25:53,What are some tips of curating a dataset to fine-tune a code-completion LLM?,janissary2016,False,1.0,1,193zhur,https://www.reddit.com/r/deeplearning/comments/193zhur/what_are_some_tips_of_curating_a_dataset_to/,0,1704972353.0,"Hi.

There is a new SDK that I am working on and I want to know what are some ways of automatically curating a dataset to train a code-completing LLM to deploy as a VSCode plugin? Hacky ways are appreciated. I was thinking of using chatgpt API to make numerous API calls to inflate a CSV with artificially generated prompts - code entries. There are available datasets of course but I want to tailor the code completion for this particular SDK.

Appreciate all answers."
268,deeplearning,chatgpt,relevance,2023-11-29 16:18:08,Rudy Lai on Tactic Generate - Weaviate Podcast #78!,CShorten,False,0.67,1,186t892,https://www.reddit.com/r/deeplearning/comments/186t892/rudy_lai_on_tactic_generate_weaviate_podcast_78/,0,1701274688.0,"Hey everyone! I am SUPER excited to publish our 78th Weaviate Podcast with Rudy Lai, Co-Founder and CEO of Tactic Generate!

I think most people would agree that the viral success of ChatGPT has greatly aided by **pairing the LLM with a Chat GUI** (as well as of course instruction tuning it for chat). ChatGPT marked the dawn of entirely new user experiences with computers enabled by AI.

Tactic Generate is similarly pioneering a new user experience for ""Chat with Documents"", presenting a multi-column view where LLMs answer questions over each of your documents (or folders / collections in a database) in **parallel!**

So imagine grabbing 5 papers from ArXiv about LoRA and asking, ""how does merging LoRA weights with the base model work?"", you can get an **answer to each of these questions in parallel grounded in each of the 5 papers!** Tactic Generate's GUI then enables you to remove the column boundaries and sync up the answers!

I am writing this message from the AWS Re:Invent conference where I have been showing the Verba demo over and over again haha. It has really taught me the power of the GUI for explaining concepts in RAG and Vector Databases -- I think Tactic Generate can have the same impact for demonstrating Parallel LLM Execution and Multi-Document Agents!

I hope you enjoy the podcast with Rudy, as always more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

[https://www.youtube.com/watch?v=igK4JN2-1m4](https://www.youtube.com/watch?v=igK4JN2-1m4)"
269,deeplearning,chatgpt,relevance,2023-12-14 02:03:03,[D] Constructing an Efficient Knowledge Graph RAG Pipeline with LlamaIndex,Fit_Maintenance_2455,False,1.0,2,18hxo48,https://www.reddit.com/r/deeplearning/comments/18hxo48/d_constructing_an_efficient_knowledge_graph_rag/,4,1702519383.0,"Large Language Models (LLMs) such as ChatGPT and Bard exhibit remarkable abilities within their specialized areas of training. However, their constraints in handling new or private data inquiries are widely recognized.

Retrieval Augmented Generation (RAG) emerges as a solution to bridge this gap, allowing LLMs to access external knowledge sources. This article delves into RAG, examines its elements, and constructs a usable RAG workflow that harnesses the potential of LlamaIndex, a knowledge graph.

&#x200B;

Link: [https://medium.com/ai-advances/constructing-an-efficient-knowledge-graph-rag-pipeline-with-llamaindex-81a0a0b105b7](https://medium.com/ai-advances/constructing-an-efficient-knowledge-graph-rag-pipeline-with-llamaindex-81a0a0b105b7)  "
270,deeplearning,chatgpt,relevance,2023-10-16 14:41:46,Nobel laureate Dr Michael Levitt: AI will change everything forever,kirst31,False,1.0,1,1797kte,https://www.reddit.com/r/deeplearning/comments/1797kte/nobel_laureate_dr_michael_levitt_ai_will_change/,0,1697467306.0,"[https://www.youtube.com/watch?v=8ZV6o0EiuTo&t=61s](https://www.youtube.com/watch?v=8ZV6o0EiuTo&t=61s)

The decorated and respected scientist, an early adopter of ChatGPT and other AI technologies, reveals whether the rapid emergence of ever-more powerful machine learning tools will ultimately help or harm humanity as it changes our world beyond recognition."
271,deeplearning,chatgpt,relevance,2023-10-05 19:31:50,AI And Math,Gla-l,False,0.22,0,170r03q,https://www.reddit.com/r/deeplearning/comments/170r03q/ai_and_math/,4,1696534310.0," Hello i guys i have some questions. I want learn AI but i don't know math too much so what can i do for this i asked to chatgpt and he is give me some subject and i searched, listen but some times i don't understand some arguments in lesson i guess this problem from my high school because we didn't have math lessons. Well what can i do for this how can i start to math? "
272,deeplearning,chatgpt,relevance,2023-05-23 14:14:45,New Weaviate Podcast - Unstructured!,CShorten,False,0.76,2,13ppstr,https://www.reddit.com/r/deeplearning/comments/13ppstr/new_weaviate_podcast_unstructured/,0,1684851285.0,"ChatWithPDF has been one of the most captivating applications of the latest wave of ChatGPT and pairing ChatGPT with Retrieval-Augmentation and Vector Databases! As exciting as this is, there is a glaring problem... how do I get the text data out of my PDFs?

This is the problem Unstructured is solving with 3 core abstractions: (1) Partitioning (visually looking at elements on a PDF / Webpage / Resume / Slidedeck / Receipt / ... extracting the text data and adding metadata such as ""header"", ""body"", or ""image caption"", (2) Cleaning (I'm sure everyone in this group who has worked with text data has seen these ridiculous character encoding problems, regex, and whitespace removals we need to clean our text data for NLP pipelines), and (3) Staging (this describes extracting the JSONs / etc. to pass this data into another system such as Weaviate as an example)

I really hope you enjoy the podcast -- I think these innovations are so exciting for unlocking our data into these LLM systems!

[https://www.youtube.com/watch?v=b84Q2cJ6po8](https://www.youtube.com/watch?v=b84Q2cJ6po8)"
273,deeplearning,chatgpt,relevance,2023-03-25 04:24:49,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.93,47,121agx4,https://www.reddit.com/r/deeplearning/comments/121agx4/do_we_really_need_100b_parameters_in_a_large/,54,1679718289.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
274,deeplearning,chatgpt,relevance,2023-04-24 13:14:39,Applications of GPT,AcornWizard,False,0.36,0,12xfegq,https://www.reddit.com/r/deeplearning/comments/12xfegq/applications_of_gpt/,5,1682342079.0,"Hello. Is ChatGPT currently the only implemented application that uses GPT? Looking on the internet I see a lot of flashy but often vague talk about potential applications, yet I have not found more implemented uses."
275,deeplearning,chatgpt,relevance,2023-07-20 17:20:22,What’s hot nowadays,Gold-Act-7366,False,0.25,0,154x5xu,https://www.reddit.com/r/deeplearning/comments/154x5xu/whats_hot_nowadays/,1,1689873622.0,"I just wanted to ask which kind of projects are hot these, like before gpt it was text to image(mid journey etc) and at the time of chatgpt there were autogpt, agentgpt, gpt-engineer.

I’m just a teenager who is exploring in tech, and made some dl projects."
276,deeplearning,chatgpt,relevance,2023-05-31 07:08:07,Question about Neural Nets,yanggang20202024,False,0.64,3,13wf18y,https://www.reddit.com/r/deeplearning/comments/13wf18y/question_about_neural_nets/,3,1685516887.0,"I recently read an article about how the supercomputer used to train Chatgpt consisted of something like 10,000 gpus.

My question is, do these supercomputers that train neural nets always get better when more gpus are added? Or is it a situation where progress flattens to such a degree at some point that it makes no sense to make the supercomputer any bigger?"
277,deeplearning,chatgpt,relevance,2023-06-17 20:54:57,LLMs for small projects,KrazedRook,False,0.8,3,14c1hgq,https://www.reddit.com/r/deeplearning/comments/14c1hgq/llms_for_small_projects/,3,1687035297.0,"I am looking fo a LLM to use for an app that I'm working on (for myself, it wont be published). I was originally goin to use ChatGPT but I have "" exceeded my current quota "" and I don't want to have to pay for this, so thats out of the question. I want to see if there are any others that I could use for my project, I'm using VS Code. If you have any I can use please explain it to me if you can in summary."
278,deeplearning,chatgpt,relevance,2023-09-30 12:23:31,[D] How to train a seq2seq model to rephrase input text following given rules.,3Ammar404,False,0.5,0,16w5g5p,https://www.reddit.com/r/deeplearning/comments/16w5g5p/d_how_to_train_a_seq2seq_model_to_rephrase_input/,2,1696076611.0,"Hi guys,

I want to train (fine-tune) a seq2seq model to perform the task of rephrasing input following these rules :

1- always follow the pattern ""Entity Verb Entity""

2- only use simple sentences : never combine sentences

3- Don't replace existing words

4- Don't lose the overall meaning of the text or any information in it.

For example:

text = ""Project Risk Management includes the processes of conducting risk management planning, identification, analysis, response planning, response implementation, and monitoring risk on a project""

Standardized Text = ""Project Risk Management conducts risk management planning. Project Risk Management conducts risk identification. Project Risk Management conducts risk analysis. Project Risk Management plans responses. Project Risk Management implements responses. Project Risk Management monitors risk on a project.""

Using ChatGPT the results were very good, but I want to know if I can fine tune a model (BERT, T5, any LM) locally, what should be the data format for training such a model, evaluation metrics ?"
279,deeplearning,chatgpt,relevance,2023-06-07 14:12:57,New Weaviate Podcast - LLM Agents!,CShorten,False,1.0,1,143edrc,https://www.reddit.com/r/deeplearning/comments/143edrc/new_weaviate_podcast_llm_agents/,0,1686147177.0,"Hey everyone! I am SUPER excited to share our 51st Weaviate Podcast on keeping up with the latest in LLM Agents -- featuring Greg Kamradt from Data Independent and Colin Harmon from Nesh, we discussed all the abstractions and emerging ideas from LangChain, LlamaIndex, and miscellaneous other sources!

We discussed everything under the sun related to LLMs from Tool Use to Databases, Multi-Agent LLMs, Privacy, Personalization, the ChatGPT Marketplace, Fine-Tuning, Long Context Length LLMs, and more! I really hope you find it useful!

https://www.youtube.com/watch?v=iB4ki6gdAdc"
280,deeplearning,chatgpt,relevance,2022-12-10 22:44:46,InstructGPT,MRMohebian,False,0.82,7,zi62fr,https://www.reddit.com/r/deeplearning/comments/zi62fr/instructgpt/,1,1670712286.0,"Recently, ChatGPT became trendy. It was adopting an algorithm by the name of InstructGPT. 
We go through the paper ""Training Language Models to Follow Instructions with Human Feedback"" in this video and go into extensive detail about how InstructGPT works. 

Please subscribe, leave a comment and share with your friends.

[R]
https://youtu.be/lYRWzCPGM2Q"
281,deeplearning,chatgpt,relevance,2023-05-13 22:42:26,Domain specific chatbot. Semantic search isn't enough.,mldlbr,False,0.9,8,13gv1zj,https://www.reddit.com/r/deeplearning/comments/13gv1zj/domain_specific_chatbot_semantic_search_isnt/,8,1684017746.0,"Hi guys, I'm struggling to find a reliable solution to this specific problem.

I  have a huge dataset with chat conversations, about several topics. I  want to ask questions and retrieve information about these conversations in a chatbot way.

I have tried  semantic search with chatGPT to answer questions about these  conversations. The problem is that semantic search only returns top  similar sentences, and doesn't ‘read’ all conversations, that’s not  enough to answer generic questions, just very specific ones. For  example, if I ask “What are these people talking about person X?” it  will return only the top sentences (through semantic similarity) and  that will not tell the whole story. The LLM’s models have a limit of  tokens, so I can’t send the whole dataset as context.

Is there any approach to giving a reliable answer based on reading all the messages?

Any ideas on how to approach this problem?"
282,deeplearning,chatgpt,relevance,2024-01-05 08:27:31,6 ways AI can make your life easier in 2024,PoetryOne4804,False,0.34,0,18z212l,https://www.reddit.com/r/deeplearning/comments/18z212l/6_ways_ai_can_make_your_life_easier_in_2024/,8,1704443251.0,"Artificial intelligence is developing every day. ChatGPT was a game changer for millions of people, but it is not the only one. Advances in AI are coming, and they're coming FAST. Very fast. There’re so many tasks AI can help with and make this year less stressful. Let me show you these ways:

**1) Chatbots for answering questions and brainstorming**

Except ChatGPT, you can use Google Bard, SpinBot, and YouChat.

**2) AI Essay writers**

Many people use [essay writing services](https://www.reddit.com/r/deeplearning/comments/16gnuwy/best_essay_writing_services_top_5/) but not all think that AI can also help in academic writing. AI essay writers like [Textero.ai](https://Textero.ai) can be faster and generate ideas or find sources for your topic.

**3) Daily life tools**

There’re AI planners to schedule meetings and integrate with your calendars. You can also keep track of finances using PocketGuard, Wally, or Cleo.

**4) Tools for social networks**

There’re various AI tools tailored for social networks, such as Postwise for Twitter posts and Steve.ai for YouTube.

**5) Tools to improve health and fitness goals**

AI tools like Apple Watches and Fitbits can monitor your fitness and health. They can even track your sleep and offer suggestions to improve sleep quality.

**6) Tools for academic needs**

Even though some professors are against using AI while studying, students look for ways to make academic life easier. Useful tools for school life you can find here:  [ai tools for students](https://www.reddit.com/r/artificial/comments/1716t0y/ai_tools_for_students_from_ai_essay_generators_to/)

Any other tools to share? Feel free to write about them, I’m ready to try more new services."
283,deeplearning,chatgpt,relevance,2024-02-10 03:18:15,Building an AI that can mimic the Spectator Method by Benjamin Franklin?,kiwifreeze,False,1.0,2,1an6mfj,https://www.reddit.com/r/deeplearning/comments/1an6mfj/building_an_ai_that_can_mimic_the_spectator/,2,1707535095.0,"Hello, I'm a recent CS grad and aspiring game dev. 

 I want to get better at writing and one of the ways I've been doing this was using the [Spectator method by Benjamin Franklin](https://shanesnow.com/research/how-to-be-a-better-writer-ben-franklin) 

I have been doing this by hand recently and I've found it to be incredibly helpful for my writing chops. I do everything by hand, that is take notes on each individual sentence of whichever book and then try to rewrite after offsetting the time a bit so I've forgotten it and then compare my writing to the original to see what I'm lacking. 

Taking notes on each individual sentence has been tedious though, and I tried to get a way for ChatGPT to do this for me but it can't read PDFs and yet alone other book files (I'm guessing). It does have the capability however to make short sentiments of the meanings of each sentence in any paragraph of a book (I tested this with a public domain book like Pride and Prejudice but it stopped after awhile). 

Is it possible to write an AI to do this for me automatically instead so I don't have to take notes sentence by sentence? Like use the OpenAI api to build a personal app around, or even build an LLM (which I don't even know where I would start with tbh). 

I do have much experience with coding and have taken an Intro to AI course at my university, though I can't say how much I really paid attention. That being said, I'm willing and capable of learning. 

Any advice would be appreciated!"
284,deeplearning,chatgpt,relevance,2023-12-21 15:21:16,Subjectivity in AI with Dan Shipper,CShorten,False,0.75,2,18npf5z,https://www.reddit.com/r/deeplearning/comments/18npf5z/subjectivity_in_ai_with_dan_shipper/,0,1703172076.0,"Hey everyone!! I am SUPER excited to publish the fourth and final episode of the AI-Native Database podcast series with Dan Shipper and Bob van Luijt!

I thought Dan brought such a unique perspective on how we write and create with ChatGPT and Vector DBs! We dove into how subjectivity, or personality, is reflected in AI systems. For example, imagine you ask an LLM: ""Could you plan a vacation to Miami?"". Compared to previous database or search engine technologies that will just spit out relevant information to the query, the LLM might respond: ""Are you sure you want to go to Miami? I think Fort Lauderdale is nicer at this time of year"".

The podcast is titled ""Subjectivity in AI"" because this is such a novelty of AI compared to previous technologies. How will this unique ""personality"" of LLMs (as well as vector embeddings, classifiers, and other model types) seep into all these new applications? Can we control it with prompts and RAG, training data, or some combination of the above? What do multi-agent systems look like, each with their own personality and intrinsic motivation? Further, what new art forms will arise because of this property of AI, what will the ""overdrive"" of AI use look like?

I hope you enjoy the podcast, the last in our series! This has been so much fun to record and publish!

[https://www.youtube.com/watch?v=prV5R3T6UqM](https://www.youtube.com/watch?v=prV5R3T6UqM)"
285,deeplearning,chatgpt,relevance,2023-08-27 23:55:12,GPT4 Contextual Decomposition Template,InevitableSky2801,False,0.67,1,1636b99,https://www.reddit.com/r/deeplearning/comments/1636b99/gpt4_contextual_decomposition_template/,0,1693180512.0,"Complex tasks with LLMs like ChatGPT/GPT4 are best broken down by first asking ChatGPT to outline the steps and then asking the LLM to execute against those steps that it defined. I first came across this interesting technique on Twitter recently.

While it’s OK to do this once in OpenAI’s playground, it's difficult to make this repeatable and streamlined. When I wanted an LLM to do something complex, I wanted to be able to plug into a template instead of thinking about and setting up the contextual decomposition process.

I made this Contextual Decomposition Template to help solve this problem: [https://lastmileai.dev/workbooks/cllqfl5c600rdpgnhh2su2fa0](https://lastmileai.dev/workbooks/cllqfl5c600rdpgnhh2su2fa0)

With a document and objective, this template allows you to quickly get to the answer through defining intermediate steps and executing according. Parameters are set up so you can easily change the goal, document, and objective and click 'Run All' to get the final results.

Please let me know if you have feedback! I'm also very curious if you have other interesting techniques with complex tasks and workflows working with LLMs."
286,deeplearning,chatgpt,relevance,2023-09-29 14:02:33,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.91,18,16vch0x,https://www.reddit.com/r/deeplearning/comments/16vch0x/this_week_in_ai_all_the_major_ai_developments_in/,2,1695996153.0,"1. **Meta AI** presents **Emu**, a quality-tuned latent diffusion model for generating highly aesthetic images. Emu significantly outperforms SDXLv1.0 on visual appeal.
2. **Meta AI** researchers present a series of long-context LLMs with context windows of up to 32,768 tokens. LLAMA 2 70B variant surpasses gpt-3.5-turbo-16k’s overall performance on a suite of long-context tasks.
3. **Abacus AI** released a larger 70B version of **Giraffe**. Giraffe is a family of models that are finetuned from base Llama 2 and have a larger context length of 32K tokens\].
4. **Cerebras** and **Opentensor** released Bittensor Language Model, ‘**BTLM-3B-8K**’, a new 3 billion parameter open-source language model with an 8k context length trained on 627B tokens of SlimPajama. It outperforms models trained on hundreds of billions more tokens and achieves comparable performance to open 7B parameter models. The model needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models and is available with an Apache 2.0 license for commercial use.
5. **OpenAI** is rolling out, over the next two weeks, new voice and image capabilities in ChatGPT enabling ChatGPT to understand images, understand speech and speak. The new voice capability is powered by a new text-to-speech model, capable of generating human-like audio from just text and a few seconds of sample speech. .
6. **Mistral AI**, a French startup, released its first 7B-parameter model, **Mistral 7B**, which outperforms all currently available open models up to 13B parameters on all standard English and code benchmarks. Mistral 7B is released in Apache 2.0, making it usable without restrictions anywhere.
7. **Microsoft** has released **AutoGen** \- an open-source framework that enables development of LLM applications using multiple agents that can converse with each other to solve a task. Agents can operate in various modes that employ combinations of LLMs, human inputs and tools.
8. **LAION** released **LeoLM**, the first open and commercially available German foundation language model built on Llama-2
9. Researchers from **Google** and **Cornell University** present and release code for DynIBaR (Neural Dynamic Image-Based Rendering) - a novel approach that generates photorealistic renderings from complex, dynamic videos taken with mobile device cameras, overcoming fundamental limitations of prior methods and enabling new video effects.
10. **Cloudflare** launched **Workers AI** (an AI inference as a service platform), **Vectorize** (a vector Database) and **AI Gateway** with tools to cache, rate limit and observe AI deployments. Llama2 is available on Workers AI.
11. **Amazon** announced the general availability of **Bedrock**, its service that offers a choice of generative AI models from Amazon itself and third-party partners through an API.
12. **Spotify** has launched a pilot program for AI-powered voice translations of podcasts in other languages - in the podcaster’s voic. It uses OpenAI’s newly released voice generation model.
13. **Getty Images** has launched a generative AI image tool, ‘**Generative AI by Getty Images**’, that is ‘commercially‑safe’. It’s powered by Nvidia Picasso, a custom model trained exclusively using Getty’s images library.
14. **Optimus**, Tesla’s humanoid robot, can now sort objects autonomously and do yoga. Its neural network is trained fully end-to-end.
15. **Amazon** will invest up to $4 billion in Anthropic. Developers and engineers will be able to build on top of Anthropic’s models via Amazon Bedrock.
16. **Google Search** indexed shared Bard conversational links into its search results pages. Google says it is working on a fix.

&#x200B;

  
My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
287,deeplearning,chatgpt,relevance,2023-04-02 18:10:37,Should we draw inspiration from Deep learning/Computer vision world for fine-tuning LLMs?,Vegetable-Skill-9700,False,0.79,13,129t3tl,https://www.reddit.com/r/deeplearning/comments/129t3tl/should_we_draw_inspiration_from_deep/,8,1680459037.0,"With HuggingGPT, BloombergGPT, and OpenAI's chatGPT store, it looks like the world is moving towards specialized GPTs for specialized tasks. What do you think are the best tips & tricks when it comes to fine-tuning and refining these task-specific GPTs?

Over the last decade, I have built many computer vision models (for human pose estimation, action classification, etc.), and our general approach was always based on Transfer learning. Take a state-of-the-art public model and fine-tune it by collecting data for the given use case.

Do you think that paradigm still holds true for LLMs?

Based on my experience, I believe observing the model's performance as it interacts with real-world data, identifying failure cases (where the model's outputs are wrong), and using them to create a high-quality retraining dataset will be the key.

&#x200B;

P.S. I am building an open-source project UpTrain ([https://github.com/uptrain-ai/uptrain](https://github.com/uptrain-ai/uptrain)), which helps data scientists to do so. We just wrote a blog on how this principle can be applied to fine-tune an LLM for a conversation summarization task. Check it out here: [https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation\_summarization](https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation_summarization)"
288,deeplearning,chatgpt,relevance,2023-05-17 15:10:45,New Weaviate Podcast - ChatArena!,CShorten,False,0.6,1,13k4i21,https://www.reddit.com/r/deeplearning/comments/13k4i21/new_weaviate_podcast_chatarena/,0,1684336245.0,"Hey everyone! I am super excited to publish our newest Weaviate Podcast on ChatArena!  One of the most exciting ideas with the emergence of LLM capabilities is to plug multiple LLMs together in Multi-Agent games or environments! I think the world is collectively still scrambling to understand systems like this, but 2 applications are immediately obvious:  


1. Create intelligent artifacts by simulating conversations between role-playing agents -- say an LLM impersonator of Sam Altman and Gary Marcus debate proposals for AI safety or Michael Bronstein and Jure Leskovec discuss the future of Geometric Deep Learning and Graph Neural Networks
2. Evaluating LLMs -- Benchmarks unfortunately don't really capture the nuances of intelligence, but having say ChatGPT chat with Claude moderated by a Cohere LLM that rates which LLM was more intelligent across thousands of simulated conversations 😂 -- looks like an incredibly promising way to keep up with the flood of new LLMs entering the market!  

I learned so much from this podcast, it was easily one of my favorite conversations I've ever had across the 47 Weaviate podcast episodes we have published so far, I really hope you find it useful and interesting!https://www.youtube.com/watch?v=\_0ww8Q0Bq2w"
289,deeplearning,chatgpt,relevance,2023-09-18 12:40:11,"DeepMind co-founder predicts ""third wave"" of AI: machines talking to machines and people",Nalix01,False,0.75,2,16lugx7,https://www.reddit.com/r/deeplearning/comments/16lugx7/deepmind_cofounder_predicts_third_wave_of_ai/,2,1695040811.0,"DeepMind's co-founder, Mustafa Suleyman, anticipates a ""third wave"" of AI evolution where machines will interact with both humans and other machines.

If you want to stay ahead of the curve in AI and tech, [look here first](https://dupple.com/techpresso?utm_source=reddit&utm_medium=social&utm_campaign=post).

**The Evolution of AI Phases**

* **Initial Classification Phase**: This was the first wave, focusing on deep learning that classifies different types of input data, such as images and audio.
* **Current Generative Phase**: AI uses input data to create new data.
* **Upcoming Interactive Phase**: Machines will be able to perform tasks by conversing with other machines and humans. Users will give high-level objectives to their AI systems which will then take necessary actions, involving dialogues with other AIs and individuals.

**Interactive AI's Potential**

* **More than Just Automation**: This AI won't just be about following commands but will have the freedom and agency to execute tasks.
* **Closer to Sci-Fi**: Interactive AI is anticipated to be more similar to the artificial intelligence depicted in science fiction, with dynamic capabilities rather than being static.

**Current AI Landscape**:

* **Generative AI's Popularity**: Despite being a game-changer, enthusiasm for generative AI seems to be waning, with declining user growth and web traffic for tools like ChatGPT.
* **Inflection AI's ""Pi""**: Earlier this year, Suleyman's company released a ChatGPT rival named Pi, emphasizing its polite and conversational nature.

**PS:** **If you enjoyed this post**, you’ll love my [ML-powered newsletter](https://dupple.com/techpresso?utm_source=reddit&utm_medium=social&utm_campaign=post) that summarizes the best AI/tech news from 50+ media. It’s already being read by **6,500+** **professionals** from **OpenAI, Google, Meta**…"
290,deeplearning,chatgpt,relevance,2023-06-04 06:58:24,Setting up remote instances for training.,zen_zen_zen_zen,False,0.67,1,1404bqs,https://www.reddit.com/r/deeplearning/comments/1404bqs/setting_up_remote_instances_for_training/,2,1685861904.0,I created a deep learning library with many dependencies and its been a pain in the ass to setup my environment in any remote compute instance for training. I use a conda environment.yml for intial setup but installing some modules requires more downloading .whl files and using pip to install them. How does everyone do it?
291,deeplearning,chatgpt,relevance,2023-09-27 17:55:06,Advanced Query Engines in LlamaIndex - Concepts Explained + E2E Python Code Notebooks,CShorten,False,0.83,4,16trdr4,https://www.reddit.com/r/deeplearning/comments/16trdr4/advanced_query_engines_in_llamaindex_concepts/,0,1695837306.0,"Hey everyone! I am super excited to share Erika's 3rd Episode in our Llama Index and Weaviate series, covering the Advanced Query Engines in Llama Index. Here is a quick overview, the video will explain the concepts in further detail and then an End-to-End Python code demo (I am particularly proud of the SQL Router demo)

• SQL Router -- one of the most interesting products in the latest boom of LLMs is that we can connect Vector Search with SQL systems, routing queries with an LLM!!! We can also use the LLM to format the queries with Text-to-SQL prompts! Such an amazing thing that I didn't have on my bingo card before ChatGPT haha.

• Recursive Retrieval -- Even aside from LLMs, we can create more advanced search indexes by connecting indexes with each other - for example, first searching through descriptions of the tools available and then stepping into the documentation within that tool to find the more particular thing you need! This also can involve LLMs if for example the high-level search takes us into a structured table -- and now we call upon our good old Text-to-SQL LLM again.

• Self-Correcting Query Engine -- Quite a bizarre phenomenon of LLMs is that they are able to correct themselves by simply reflecting on their output. Llama Index presents a nice and simple solution to get running with this.

• Lastly is the most open-ended of the Advanced Query Engines... the Sub Question Query Engine. This describes asking the LLM to decompose the question or task into it's constituent sub-questions or sub-tasks and then compose the results together to serve the larger goal. For example, ""Did Aristotle Use a Laptop?"" --> ""When did Aristotle Live?"" & ""When were Laptops invented?""

I hope you find this video useful, we are more than happy to answer any questions or discuss any ideas you have about the content in the video!

https://www.youtube.com/watch?v=Su-ROQMaiaw"
292,deeplearning,chatgpt,relevance,2023-06-02 16:17:33,"Decent laptops for machine learning (GPU on cloud, but still decent local performance)?",bot_exe,False,0.6,1,13yh3b0,https://www.reddit.com/r/deeplearning/comments/13yh3b0/decent_laptops_for_machine_learning_gpu_on_cloud/,2,1685722653.0,"I'm looking for laptop recommendations where I can easily use cloud services like colab, paperspaces, runpod, etc. when I need a powerful GPU to training train neural networks or use a big opensource models, but also smoothly run code locally using VScode and jupyterlab/jupyter notebooks for everything else with good performance.

So far I have been using an ipad pro 12.9"" (2020) using google colab on the Safari browser (with chatGPT on splitscreen) and it has worked surprisingly well (I was forced to code on the ipad, because my old gaming PC died recently). The portability is amazing, yet some of the limitations of the OS, available software and the hardware are a pain in the butt.... I really want to be able to do simple things locally and quickly, like i could in my old PC, so I think I need decent CPU, RAM and storage for that (but not sure how much exactly????), the ability to use windows (and maybe linux), while also having a better interface (proper keyboard and mouse), while keeping portability.

Hence why I'm looking for decent laptop recommendations (good CPU, RAM, storage, form factor and connectors). I don't have enough money to buy a new gaming PC or workstation or GPU laptop (which I think are dumb anyway, if I wanted a GPU I would get a desktop PC), so I'm also trying to keep the price below something like a strong gaming PC, but still willing to invest in a good laptop if it will really solve my problems as I'm thinking this might... anyway since I'm still rather knew in this space I also welcome opinions on whether this is even feasible or if I should go for a different setup or rethink my requirements."
293,deeplearning,chatgpt,relevance,2023-04-25 17:53:47,"Microsoft releases SynapseMl v0.11 with support for ChatGPT, GPT-4, causal learning, and more",mhamilton723,False,0.87,22,12yqpnp,https://www.reddit.com/r/deeplearning/comments/12yqpnp/microsoft_releases_synapseml_v011_with_support/,0,1682445227.0,"Today Microsoft launched SynapseML v0.11 with support for ChatGPT, GPT-4, distributed training of huggingface and torchvision models, an ONNX Model hub integration, Causal Learning with EconML, 10x memory reductions for LightGBM, and a newly refactored integration with Vowpal Wabbit. To learn more check out our release notes and please feel give us a star if you enjoy the project!

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.11.0](https://github.com/microsoft/SynapseML/releases/tag/v0.11.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919)

Thank you to all the contributors in the community who made the release possible!

&#x200B;

[What's new in SynapseML v0.11](https://preview.redd.it/9pqj1mowj2wa1.png?width=4125&format=png&auto=webp&s=a358e73760c847a09cc76f2ed17dc58e15aed5ed)"
294,deeplearning,chatgpt,relevance,2023-01-27 10:45:48,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.95,118,10mhyek,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
295,deeplearning,chatgpt,relevance,2024-02-18 11:37:33,How to break in the field; Advice wanted ,Exact-Committee-8613,False,0.59,5,1ats6qb,https://www.reddit.com/r/deeplearning/comments/1ats6qb/how_to_break_in_the_field_advice_wanted/,8,1708256253.0,"Hello everyone,

I hold a Master's degree in Business Analytics and Data Science from a globally recognized university ranked in the top 50 by QS, maintaining a GPA of 3.67. While not the highest, it reflects a solid academic performance.

I am eager to enter the field, armed with extensive knowledge and a diverse portfolio of projects spanning analytics, machine learning, and computer vision.

My resume has been well-received, leading to numerous interview opportunities. However, I've encountered challenges, particularly in coding/technical assessments. While I now pass them 70-80% of the time, progressing to final rounds remains elusive as they often opt for candidates deemed a better fit.

I occasionally struggle with rapid-fire questioning during interviews, where my data science expertise is put to the test across various domains, from A/B testing to deep learning. Despite thorough preparation, I acknowledge the inherent unpredictability of such encounters.

Two years post-graduation, I find myself frustrated by the lack of job offers. Whether it's insufficient experience hindering my initial candidacy or faltering in subsequent rounds, the result is the same.

I seek advice on how to generate income as my financial situation is dire, with my bank balance in the negative due to deductions for insufficient funds.

Any guidance or suggestions would be greatly appreciated.

Thank you."
296,deeplearning,chatgpt,relevance,2023-12-12 04:00:24,"Where can I get in touch with more ""friendlier"" papers?",Whole_Reference_96,False,0.29,0,18gd15d,https://www.reddit.com/r/deeplearning/comments/18gd15d/where_can_i_get_in_touch_with_more_friendlier/,2,1702353624.0,"I'm trying to get more into chatbot academically.

 Also, do u guys think its a good idea to start specializing in chatbots now? Am I too late?"
297,deeplearning,chatgpt,relevance,2023-04-03 14:02:20,Are any Pre-trained Conditional-GAN models available?,Maverick_5112,False,0.33,0,12alfmp,https://www.reddit.com/r/deeplearning/comments/12alfmp/are_any_pretrained_conditionalgan_models_available/,1,1680530540.0,I would love to use a pre-trained Conditional-GAN for my project but haven't come across one yet. Could anyone please point me to the right resources please?
298,deeplearning,chatgpt,relevance,2024-01-30 12:45:52,How can I build a voice chatbot which can answer limited questions?,mszahan,False,0.5,0,1aenl8o,https://www.reddit.com/r/deeplearning/comments/1aenl8o/how_can_i_build_a_voice_chatbot_which_can_answer/,4,1706618752.0,"I am Web developer, but have no ideas about AI or machine learning. I want to build a voice chatbot type thing that will answer only particular questions. It could be app or web application. I have a set of 50 questions. When user will ask the question if it's within that 50 questions it will answer the question otherwise it will say 'sorry I can't answer the question.'   


Can anyone give me direction how can I build such a thing? If it requires to learn anything I am ready to do that, I got almost 1 year to build this. And if there is any other shortcut like paid or opensource API, I also can afford that. "
299,deeplearning,chatgpt,relevance,2023-04-24 01:17:58,Can an average person learn how to build a LLM model?,sch1zoph_,False,0.72,27,12wxrrd,https://www.reddit.com/r/deeplearning/comments/12wxrrd/can_an_average_person_learn_how_to_build_a_llm/,29,1682299078.0,"Hello everyone. I am a 30-year-old Korean male.

To be honest, I have never really studied properly in my life. It's a little embarrassing, but that's the truth.

Recently, while using ChatGPT, I had a dream for the first time. I want to create a chatbot that can provide a light comfort to people who come for advice. I would like to create an LLM model using Transformer, and use our country's beginner's counseling manual as the basis for the database.

I am aware that there are clear limits to the level of comfort that can be provided. Therefore, if the problem is too complex or serious for this chatbot to handle, I would like to recommend the nearest mental hospital or counseling center based on the user's location. And, if the user can prove that they have visited the hospital (currently considering a direction where the hospital or counseling center can provide direct certification), I would like to create a program that provides simple benefits (such as a free Starbucks coffee coupon).

I also thought about collecting a database of categories related to people's problems (excluding personal information) and selling it to counseling or psychiatric societies. I think this could be a great help to these societies.

The problem is that I have never studied ""even once,"" and I feel scared and fearful of the unfamiliar sensation. I have never considered myself a smart person.

However, I really want to make this happen! Our country is now in a state of constant conflict, and people hate and despise each other due to strong propaganda.

As a result, the birth rate has dropped to less than 1%, leading to a decline in the population. Many people hide their pain inside and have no will to solve it. They just drink with their friends to relieve their pain. This is obviously not a solution. Therefore, Korea has a really serious suicide rate.

I may not be able to solve this problem, but I want to put one small brick to build a big barrier to stop hatred. Can an ordinary person who knows nothing learn the common sense and study needed to build an LLM model? And what direction should one take to study one by one?"
