,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,id,url,num_comments,created,body
0,deeplearning,openai,top,2023-01-27 10:45:48,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.95,122,10mhyek,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
1,deeplearning,openai,top,2020-08-05 10:58:06,image-GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model,OnlyProggingForFun,False,0.94,95,i437pt,https://www.youtube.com/watch?v=FwXQ568_io0,6,1596625086.0,
2,deeplearning,openai,top,2021-12-29 08:03:22,I wrote a program with OpenAI's Codex that fixes errors,tomd_96,False,0.94,97,rr2wme,https://v.redd.it/jupdtry6vf881,6,1640765002.0,
3,deeplearning,openai,top,2023-04-05 01:36:40,Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT,Time_Key8052,False,0.95,85,12c43uu,https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/,19,1680658600.0,"Vicuna : ChatGPT Alternative, Open-Source, High Quality and Low Cost 

&#x200B;

[ Relative Response Quality Assessed by GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a072b91)

Vicuna-13B has demonstrated competitive performance against other open-source models, such as Stanford Alpaca, by fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.

Evaluation using GPT-4 as a judge shows that Vicuna-13B achieves more than 90% of the quality of OpenAI ChatGPT and Google Bard AI, while outperforming other models such as Meta LLaMA (Large Language Model Meta AI) and Stanford Alpaca in more than 90% of cases.

The cost of training Vicuna-13B is approximately $300.

The training and serving code, along with an online demo, are publicly available for non-commercial use.

&#x200B;

More Information : [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT](https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)

Discord Server : [https://discord.gg/h6kCZb72G7](https://discord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysorg](https://twitter.com/lmsysorg)"
4,deeplearning,openai,top,2023-01-19 07:55:49,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.9,70,10fw22o,https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/,11,1674114949.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=webp&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
5,deeplearning,openai,top,2023-11-16 08:28:46,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,0.84,70,17whz6e,https://v.redd.it/qx68wbuf7o0c1,7,1700123326.0,
6,deeplearning,openai,top,2021-07-28 17:45:57,"OpenAI Releases Triton, An Open-Source Python-Like GPU Programming Language For Neural Networks",techsucker,False,0.94,65,otf0fs,https://www.reddit.com/r/deeplearning/comments/otf0fs/openai_releases_triton_an_opensource_pythonlike/,5,1627494357.0,"OpenAI released their newest language, [Triton](https://github.com/openai/triton). This open-source programming language that enables researchers to write highly efficient GPU code for AI workloads is Python-compatible and comes with the ability of a user to write in as few as 25 lines, something on par with what an expert could achieve. OpenAI claims this makes it possible to reach peak hardware performance without much effort, making creating more complex workflows easier than ever before!

Researchers in the field of Deep Learning often rely on native framework operators. However, this can be problematic because it requires many temporary tensors to work, which may hurt performance at scale for neural networks. Writing specialized GPU kernels is a more convenient solution, but surprisingly difficult due to intricacies when programming them according to GPUs. It was challenging to find a system that provides the flexibility and speed required while also being easy enough for developers to understand. This has led researchers at OpenAI in improving Triton, which was initially founded by one of their teammates.

Quick Read: [https://www.marktechpost.com/2021/07/28/openai-releases-triton-an-open-source-python-like-gpu-programming-language-for-neural-networks/](https://www.marktechpost.com/2021/07/28/openai-releases-triton-an-open-source-python-like-gpu-programming-language-for-neural-networks/) 

Paper: http://www.eecs.harvard.edu/\~htk/publication/2019-mapl-tillet-kung-cox.pdf

Github: https://github.com/openai/triton"
7,deeplearning,openai,top,2023-01-20 08:53:58,Gotcha,actual_rocketman,False,0.94,64,10gs1ik,https://i.redd.it/yyh41pnje7da1.jpg,5,1674204838.0,
8,deeplearning,openai,top,2020-01-30 18:53:20,OpenAI goes all-in on Facebook's Pytorch machine learning framework,emptyplate,False,0.95,66,ewa9jc,https://venturebeat.com/2020/01/30/openai-facebook-pytorch-google-tensorflow/,0,1580410400.0,
9,deeplearning,openai,top,2020-09-11 15:37:20,[R] OpenAI ‘GPT-f’ Delivers SOTA Performance in Automated Mathematical Theorem Proving,Yuqing7,False,0.94,43,iqsul7,https://www.reddit.com/r/deeplearning/comments/iqsul7/r_openai_gptf_delivers_sota_performance_in/,2,1599838640.0,"San Francisco-based AI research laboratory OpenAI has added another member to its popular GPT (Generative Pre-trained Transformer) family. In a new paper, OpenAI researchers introduce GPT-f, an automated prover and proof assistant for the Metamath formalization language.

Here is a quick read: [OpenAI ‘GPT-f’ Delivers SOTA Performance in Automated Mathematical Theorem Proving](https://syncedreview.com/2020/09/10/openai-gpt-f-delivers-sota-performance-in-automated-mathematical-theorem-proving/)

The paper *Generative Language Modeling for Automated Theorem Proving* is on [arXiv](https://arxiv.org/pdf/2009.03393.pdf)."
10,deeplearning,openai,top,2020-10-25 19:26:17,Google AI Introduces Performer: A Generalized Attention Framework based on the Transformer architecture,ai-lover,False,0.9,43,jhzd4q,https://www.reddit.com/r/deeplearning/comments/jhzd4q/google_ai_introduces_performer_a_generalized/,2,1603653977.0,"[Transformer model](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html), a deep learning framework, has achieved state-of-the-art results across diverse domains, including [natural language](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html), [conversation](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html), [images](https://openai.com/blog/image-gpt/), and even [music](https://magenta.tensorflow.org/music-transformer). The core block of any Transformer architecture is the [attention module](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html), which computes similarity scores for all pairs of positions in an input sequence. Since it requires quadratic computation time and quadratic memory size of the storing matrix, with the increase in the input sequence’s length, its efficiency decreases.

Thus, for long-range attention, one of the most common methods is [sparse attention](https://openai.com/blog/sparse-transformer/). It reduces the complexity by computing selective similarity scores from the sequence, based on various methods. There are still certain limitations like unavailability of efficient sparse-matrix multiplication operations on all accelerators, lack of theoretical guarantees, insufficiency to address the full range of problems, etc.

**Introduction to “Performer”**

To resolve these issues, Google AI introduces the [Performer](https://arxiv.org/abs/2009.14794), a Transformer architecture with attention mechanisms that scale linearly. The framework is implemented by [Fast Attention Via Positive Orthogonal Random Features (FAVOR+) algorithm](https://github.com/google-research/google-research/tree/master/performer/fast_self_attention), providing scalable low-variance and unbiased estimation of attention mechanisms expressed by random feature maps decompositions (in particular, regular softmax-attention). Mapping helps in preserving linear space and time complexity.

**Full Summary:** [https://www.marktechpost.com/2020/10/25/google-ai-introduces-performer-a-generalized-attention-framework-based-on-the-transformer-architecture/](https://www.marktechpost.com/2020/10/25/google-ai-introduces-performer-a-generalized-attention-framework-based-on-the-transformer-architecture/) 

**Github:** [https://github.com/google-research/google-research](https://github.com/google-research/google-research) 

**Paper:** [https://arxiv.org/pdf/2009.14794.pdf](https://arxiv.org/pdf/2009.14794.pdf)"
11,deeplearning,openai,top,2021-10-10 02:22:26,Generate READMEs Using AI,tomd_96,False,0.97,38,q4z8wm,https://www.reddit.com/r/deeplearning/comments/q4z8wm/generate_readmes_using_ai/,2,1633832546.0,"&#x200B;

https://i.redd.it/nbnqpn0f9js71.gif

You can use this program I wrote to generate readmes: [https://github.com/tom-doerr/codex-readme](https://github.com/tom-doerr/codex-readme)

It's far from perfect, but I'm still surprised how well it works if you give it a few tries. It often generates interesting usage examples and explains the available command line options.

You probably won't use this for larger projects, but I think this can make sense for small projects or single scripts. Many small scripts are very useful but might never be published because of the work that is required to document and explain it. Using this AI might assist you with that.

Be aware that you need to get access to OpenAI's Codex API to use it.

What do you think?"
12,deeplearning,openai,top,2021-02-18 14:52:00,The world's largest scale Turing Test / Do you think OpenAI's GPT3 is good enough to pass the Turing Test?,theaicore,False,0.91,34,lmog2d,https://www.theaicore.com/imitationgame?utm_source=reddit,11,1613659920.0,
13,deeplearning,openai,top,2020-04-21 23:00:56,How does the talktotransformer website work so fast?,parrot15,False,0.97,32,g5prf5,https://www.reddit.com/r/deeplearning/comments/g5prf5/how_does_the_talktotransformer_website_work_so/,5,1587510056.0,"At [https://talktotransformer.com/](https://talktotransformer.com/), you can type a prompt and the transformer will autogenerate the text for you using OpenAI's GPT-2 1.5 billion parameter model.

I'm not asking how GPT-2 works, I'm asking something else. When I ran the GPT-2 1.5B model on the free TPU in Google Colab, it took around 20 to 40ish seconds to generate around the same about of text as the website generates per prompt.

And yet, the website is somehow generating text from the prompt almost instantaneously using the 1.5B model. This is on top of all the X number of people who must be using the website at the same time I am, so it is doing text generation concurrently and near-instantaneously using a gigantic model.

I am very confused. Did the creator of the website just use a lot more TPUs/GPUs behind the scenes and is letting them run 24/7 (I don't think so because that would cost a shit ton of money), or am I missing something fundamental here?

This same question can be applied to this website too: [https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is](https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is)

Please keep in mind that I'm relatively new to all of this. Thanks in advance!"
14,deeplearning,openai,top,2022-03-12 04:56:16,Microsoft’s Latest Machine Learning Research Introduces μTransfer: A New Technique That Can Tune The 6.7 Billion Parameter GPT-3 Model Using Only 7% Of The Pretraining Compute,No_Coffee_4638,False,1.0,33,tc8u6k,https://www.reddit.com/r/deeplearning/comments/tc8u6k/microsofts_latest_machine_learning_research/,0,1647060976.0,"Scientists conduct trial and error procedures which experimenting, that many times lear to freat scientific breakthroughs. Similarly, foundational research provides for developing large-scale AI systems theoretical insights that reduce the amount of trial and error required and can be very cost-effective.

Microsoft team tunes massive neural networks that are too expensive to train several times. For this, they employed a specific parameterization that maintains appropriate hyperparameters across varied model sizes. The used µ-Parametrization (or µP, pronounced “myu-P”) is a unique way to learn all features in the infinite-width limit. The researchers collaborated with the OpenAI team to test the method’s practical benefit on various realistic cases.

Studies have shown that training large neural networks because their behavior changes as they grow in size are uncertain. Many works suggest heuristics that attempt to maintain consistency in the activation scales at initialization. However, as training progresses, this uniformity breaks off at various model widths.

[**CONTINUE READING MY SUMMARY ON THIS RESEARCH**](https://www.marktechpost.com/2022/03/11/microsofts-latest-machine-learning-research-introduces-%ce%bctransfer-a-new-technique-that-can-tune-the-6-7-billion-parameter-gpt-3-model-using-only-7-of-the-pretraining-compute/)

Paper: https://www.microsoft.com/en-us/research/uploads/prod/2021/11/TP5.pdf

Github:https://github.com/microsoft/mup

https://i.redd.it/7jrt9r3awvm81.gif"
15,deeplearning,openai,top,2019-05-15 23:29:08,Where to learn neural network architecture design,DongDilly,False,1.0,31,bp5931,https://www.reddit.com/r/deeplearning/comments/bp5931/where_to_learn_neural_network_architecture_design/,12,1557962948.0,"So people at Deep Mind and OpenAi comes up with great models that are really innovatively designed. Can someone please tell me how and where I can learn the skill to design a neural network for a specific problem.
 Thank you"
16,deeplearning,openai,top,2021-07-06 16:56:22,What OpenAI and GitHub’s “AI pair programmer” means for the software industry,bendee983,False,0.93,29,oez127,https://bdtechtalks.com/2021/07/05/openai-github-gpt-3-copilot/,6,1625590582.0,
17,deeplearning,openai,top,2018-11-14 15:56:16,OpenAI Founder: Short-Term AGI Is a Serious Possibility,gwen0927,False,0.91,27,9x1aqi,https://medium.com/syncedreview/openai-founder-short-term-agi-is-a-serious-possibility-368424f7462f,5,1542210976.0,
18,deeplearning,openai,top,2023-04-12 05:21:13,Is OpenAI’s Study On The Labor Market Impacts Of AI Flawed?,LesleyFair,False,0.96,26,12jb4xz,https://www.reddit.com/r/deeplearning/comments/12jb4xz/is_openais_study_on_the_labor_market_impacts_of/,1,1681276873.0,"[Example img\_name](https://preview.redd.it/f3hrmeet1eta1.png?width=1451&format=png&auto=webp&s=20e20b142a2f88c3d495177e540f34bc8ea4312b)

We all have heard an uncountable amount of predictions about how AI will ***terk err jerbs!***

However, here we have a proper study on the topic from OpenAI and the University of Pennsylvania. They investigate how Generative Pre-trained Transformers (GPTs) could automate tasks across different occupations \[1\].

Although I’m going to discuss how the study comes with a set of “imperfections”, the findings still make me really excited. The findings suggest that machine learning is going to deliver some serious productivity gains.

People in the data science world fought tooth and nail for years to squeeze some value out of incomplete data sets from scattered sources while hand-holding people on their way toward a data-driven organization. At the same time, the media was flooded with predictions of omniscient AI right around the corner.

*Let’s dive in and take an* exciting glimpse into the future of labor markets\*!\*

# What They Did

The study looks at all US occupations. It breaks them down into tasks and assesses the possible level of for each task. They use that to estimate how much automation is possible for a given occupation.

The researchers used the [O\*NET database,](https://www.onetcenter.org/database.html) which is an occupation database specifically for the U.S. market. It lists 1,016 occupations along with its standardized descriptions of tasks.

The researchers annotated each task once manually and once using GPT-4. Thereby, each task was labeled as either somewhat (<50%) or significantly (>50%) automatable through LLMs. In their judgment, they considered both the direct “exposure” of a task to GPT as well as to a secondary GPT-powered system, e.g. LLMs integrated with image generation systems.

To reiterate, a higher “exposure” means that an occupation is more likely to get automated.

Lastly, they enriched the occupation data with wages and demographic information. This was used to determine whether e. g. high or low-paying jobs are at higher risk to be automated.

So far so good. This all sounds pretty decent. Sure, there is a lot of qualitative judgment going into their data acquisition process. However, we gotta cut them some slag. These kinds of studies always struggle to get any hard data and so far they did a good job.

However, there are a few obvious things to criticize. But before we get to that let’s look at their results.

# Key Findings

The study finds that 80% of the US workforce, across all industries, could have at least some tasks affected. Even more significantly, 19% of occupations are expected to have at least half of their tasks significantly automated!

Furthermore, they find that higher levels of automation exposure are associated with:

* Programming and writing skills
* Higher wages (contrary to previous research!)
* Higher levels of education (Bachelor’s and up)

Lower levels of exposure are associated with:

* Science and critical thinking skills
* Manual work and tasks that might potentially be done using physical robots

This is somewhat unsurprising. We of course know that LLMs will likely not increase productivity in the plumbing business. However, their findings underline again how different this wave is. In the past, simple and repetitive tasks fell prey to automation.

*This time it’s the suits!*

If we took this study at face value, many of us could start thinking about life as full-time pensioners.

But not so fast! This, like all the other studies on the topic, has a number of flaws.

# Necessary Criticism

First, let’s address the elephant in the room!

OpenAI co-authored the study. They have a vested interest in the hype around AI, both for commercial and regulatory reasons. Even if the external researchers performed their work with the utmost thoroughness and integrity, which I am sure they did, the involvement of OpenAI could have introduced an unconscious bias.

*But there’s more!*

The occupation database contains over 1000 occupations broken down into tasks. Neither GPT-4 nor the human labelers can possibly have a complete understanding of all the tasks across all occupations. Hence, their judgment about how much a certain task can be automated has to be rather hand-wavy in many cases.

Flaws in the data also arise from the GPT-based labeling itself.

The internet is flooded with countless sensationalist articles about how AI will replace jobs. It is hard to gauge whether this actually causes GPT models to be more optimistic when it comes to their own impact on society. However, it is possible and should not be neglected.

The authors do also not really distinguish between labor-augmenting and labor-displacing effects and it is hard to know what “affected by” or “exposed to LLMs” actually means. Will people be replaced or will they just be able to do more?

Last but not least, lists of tasks most likely do not capture all requirements in a given occupation. For instance ""making someone feel cared for"" can be an essential part of a job but might be neglected in such a list.

# Take-Away And Implications

GPT models have the world in a frenzy - rightfully so.

Nobody knows whether 19% of knowledge work gets heavily automated or if it is only 10%.

As the dust settles, we will begin to see how the ecosystem develops and how productivity in different industries can be increased. Time will tell whether foundational LLMs, specialized smaller models, or vertical tools built on top of APIs will be having the biggest impact.

In any case, these technologies have the potential to create unimaginable value for the world. At the same time, change rarely happens without pain. I strongly believe in human ingenuity and our ability to adapt to change. All in all, the study - flaws aside - represents an honest attempt at gauging the future.

Efforts like this and their scrutiny are our best shot at navigating the future. Well, or we all get chased out of the city by pitchforks.

Jokes aside!

What an exciting time for science and humanity!

As always, I really enjoyed making this for you and I sincerely hope you found value in it!

If you are not subscribed to the newsletter yet, [click here to sign up](https://thedecoding.net/)! I send out a thoughtful 5-minute email every week to keep you in the loop about machine learning research and the data economy.

*Thank you for reading and I see you next week ⭕!*

**References:**

\[1\] [https://arxiv.org/abs/2303.10130](https://arxiv.org/abs/2303.10130)"
19,deeplearning,openai,top,2023-12-16 15:22:29,Is there any alternative for OpenAI API?,CrazyProgramm,False,0.9,25,18jtffj,https://www.reddit.com/r/deeplearning/comments/18jtffj/is_there_any_alternative_for_openai_api/,24,1702740149.0, So I am from Sri Lanka and our university is going to organize a competition and we need OpenAI API for it but we don't have money to afford it. Is there any alternative API you guys know 
20,deeplearning,openai,top,2023-05-17 02:26:44,OpenAI CEO asking for government's license for building AI . WHAT THE ACTUAL FUCK?,Angry_Grandpa_,False,0.82,23,13joq2b,/r/singularity/comments/13jbc76/openai_ceo_asking_for_governments_license_for/,8,1684290404.0,
21,deeplearning,openai,top,2021-12-24 15:48:32,[R] OpenAI Releases GLIDE: A Scaled-Down Text-to-Image Model That Rivals DALL-E Performance,Yuqing7,False,0.93,24,rnovk0,https://www.reddit.com/r/deeplearning/comments/rnovk0/r_openai_releases_glide_a_scaleddown_texttoimage/,1,1640360912.0,"An OpenAI research team proposes GLIDE (Guided Language-to-Image Diffusion for Generation and Editing) for high-quality synthetic image generation. Human evaluators prefer GLIDE samples over DALL-E’s, and the model size is much smaller (3.5 billion vs. 12 billion parameters). 

Here is a quick read: [OpenAI Releases GLIDE: A Scaled-Down Text-to-Image Model That Rivals DALL-E Performance.](https://syncedreview.com/2021/12/24/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-173/)

The paper *GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models* is on [arXiv](https://arxiv.org/abs/2112.10741)."
22,deeplearning,openai,top,2023-04-25 17:53:47,"Microsoft releases SynapseMl v0.11 with support for ChatGPT, GPT-4, causal learning, and more",mhamilton723,False,0.85,21,12yqpnp,https://www.reddit.com/r/deeplearning/comments/12yqpnp/microsoft_releases_synapseml_v011_with_support/,0,1682445227.0,"Today Microsoft launched SynapseML v0.11 with support for ChatGPT, GPT-4, distributed training of huggingface and torchvision models, an ONNX Model hub integration, Causal Learning with EconML, 10x memory reductions for LightGBM, and a newly refactored integration with Vowpal Wabbit. To learn more check out our release notes and please feel give us a star if you enjoy the project!

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.11.0](https://github.com/microsoft/SynapseML/releases/tag/v0.11.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919)

Thank you to all the contributors in the community who made the release possible!

&#x200B;

[What's new in SynapseML v0.11](https://preview.redd.it/9pqj1mowj2wa1.png?width=4125&format=png&auto=webp&s=a358e73760c847a09cc76f2ed17dc58e15aed5ed)"
23,deeplearning,openai,top,2022-12-07 00:33:41,Are currently state of art model for logical/common-sense reasoning all based on NLP(LLM)?,Accomplished-Bill-45,False,1.0,24,zen8l4,https://www.reddit.com/r/deeplearning/comments/zen8l4/are_currently_state_of_art_model_for/,6,1670373221.0,"Not very familiar with NLP, but I'm playing around with OpenAI's ChatGPT; particularly impressed by its reasoning, and its thought-process.

Are all good reasoning models derived from NLP (LLM) models with RL training method at the moment?

What are some papers/research team to read/follow to understand this area better and stay on updated?

&#x200B;

&#x200B;

for ChatGPT. I've tested it with following cases

Social reasoning ( which does a good job; such as: if I'm going to attend meeting tonight. I have a suit, but its dirty and size doesn't fit. another option is just wear underwear, the underwear is clean and fit in size. Which one should I wear to attend the meeting. )

Psychological reasoning ( it did a bad job.I asked it to infer someone's intention given his behaviours, expression, talks etc.)

Solving math question ( it’s ok, better then Minerva)

Asking LSAT logic game questions ( it gives its thought process, but failed to give correct answers)

I also wrote up a short mystery novel, ( like 200 words, with context) ask if it can tell is the victim is murdered or committed suicide; if its murdered, does victim knows the killer etc. It actually did ok job on this one if the context is clearly given that everyone can deduce some conclusion using common sense."
24,deeplearning,openai,top,2019-11-09 23:25:27,TensorLayer Team Released Reinforcement Learning Algorithm Baseline-RLzoo,quantumiracle,False,0.97,25,du3jol,https://www.reddit.com/r/deeplearning/comments/du3jol/tensorlayer_team_released_reinforcement_learning/,1,1573341927.0,"Recently,  in order to enable the industry to better use the cutting-edge  reinforcement learning algorithms, the TensorLayer Reinforcement   Learning Team has released a complete library of reinforcement learning   baseline algorithms for the industry — RLzoo. TensorLayer is an  extended  library based on TensorFlow for better supports of basic  neural network  construction and diverse neural network applications.  The RLzoo project  is the first comprehensive open source algorithm  library with  TensorLayer 2.0 and TensorFlow 2.0 since the release of  TensorFlow 2.0.  The library currently supports OpenAI Gym, DeepMind  Control Suite and  other large-scale simulation environments, such as  the robotic learning  environment RLBench, etc.

Link of full post: [https://medium.com/@zhding96/tensorlayer-team-released-reinforcement-learning-algorithm-baseline-rlzoo-2663cfb77904](https://medium.com/@zhding96/tensorlayer-team-released-reinforcement-learning-algorithm-baseline-rlzoo-2663cfb77904)

Link of RLzoo: [https://github.com/tensorlayer/RLzoo](https://github.com/tensorlayer/RLzoo)

Link of RL tutorial: [https://github.com/tensorlayer/tensorlayer/tree/master/examples/reinforcement\_learning](https://github.com/tensorlayer/tensorlayer/tree/master/examples/reinforcement_learning)

Slack group: [https://app.slack.com/client/T5SHUUKNJ/D5SJDERU7](https://app.slack.com/client/T5SHUUKNJ/D5SJDERU7)"
25,deeplearning,openai,top,2020-07-02 18:38:37,[N] Top US AI Research Institutes and Tech Companies Support National AI Research Cloud,Yuqing7,False,0.86,22,hk2nuf,https://www.reddit.com/r/deeplearning/comments/hk2nuf/n_top_us_ai_research_institutes_and_tech/,0,1593715117.0,"Leading US universities engaged in AI research have joined tech giants Google, Amazon Web Services, Microsoft, IBM, and NVIDIA in backing legislation aimed at establishing a roadmap for the creation of a national AI research cloud. Organizations such as the Allen Institute for AI, IEEE, and OpenAI are also supporting the bipartisan and bicameral *National AI Research Resource Task Force Act.*

 Here is a quick read: [Top US AI Research Institutes and Tech Companies Support National AI Research Cloud](https://syncedreview.com/2020/07/02/top-us-ai-research-institutes-and-tech-companies-support-national-ai-research-cloud/)"
26,deeplearning,openai,top,2019-02-14 17:19:15,OpenAI Guards Its ML Model Code & Data to Thwart Malicious Usage,gwen0927,False,0.96,22,aqm35j,https://medium.com/syncedreview/openai-guards-its-ml-model-code-data-to-thwart-malicious-usage-d9f7e9c43cd0,4,1550164755.0,
27,deeplearning,openai,top,2021-07-08 04:51:47,"Exploration of GitHub Copilot, OpenAI Codex-based AI coding assistant that translates natural language into code",techn0_cratic,False,0.76,22,og08xq,https://youtu.be/GTG_bcFdcLQ,0,1625719907.0,
28,deeplearning,openai,top,2022-10-12 20:21:40,"I've built an Auto Subtitled Video Generator using Streamlit and OpenAI Whisper, hosted on HuggingFace spaces.",Batuhan_Y,False,0.93,21,y2edmn,https://www.reddit.com/r/deeplearning/comments/y2edmn/ive_built_an_auto_subtitled_video_generator_using/,0,1665606100.0,"All you have to do is input a YouTube video link and get a video with subtitles (alongside with .txt, .vtt, .srt files).

Whisper can translate 98 different languages to English. If you want to give it a try;

Link of the app: [https://huggingface.co/spaces/BatuhanYilmaz/Auto-Subtitled-Video-Generator](https://huggingface.co/spaces/BatuhanYilmaz/Auto-Subtitled-Video-Generator)

&#x200B;

https://reddit.com/link/y2edmn/video/r49plzsgoft91/player"
29,deeplearning,openai,top,2021-12-03 15:41:55,"[R] Warsaw U, Google & OpenAI’s Terraformer Achieves a 37x Speedup Over Dense Baselines on 17B Transformer Decoding",Yuqing7,False,0.93,21,r81wny,https://www.reddit.com/r/deeplearning/comments/r81wny/r_warsaw_u_google_openais_terraformer_achieves_a/,1,1638546115.0,"In the new paper Sparse is Enough in Scaling Transformers, a research team from the University of Warsaw, Google Research and OpenAI proposes Scaling Transformers, a family of novel transformers that leverage sparse layers to scale efficiently and perform unbatched decoding much faster than original transformers, enabling fast inference on long sequences even with limited memory. 

Here is a quick read: [Warsaw U, Google & OpenAI’s Terraformer Achieves a 37x Speedup Over Dense Baselines on 17B Transformer Decoding.](https://syncedreview.com/2021/12/03/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-158/)

The paper *Sparse is Enough in Scaling Transformers* is on [arXiv](https://arxiv.org/abs/2111.12763)."
30,deeplearning,openai,top,2023-02-01 09:00:18,"Python wrapper of OpenAI's New AI classifier tool, which detects whether the paragraph was generated by ChatGPT, GPT models, or written by humans",StoicBatman,False,0.86,19,10qouv9,https://www.reddit.com/r/deeplearning/comments/10qouv9/python_wrapper_of_openais_new_ai_classifier_tool/,3,1675242018.0,"OpenAI has developed a new AI classifier tool which detects whether the content (paragraph, code, etc.) was generated by #ChatGPT, #GPT-based large language models or written by humans.

Here is a python wrapper of openai model to detect if a text is written by humans or generated by ChatGPT, GPT models

Github: [https://github.com/promptslab/openai-detector](https://github.com/promptslab/openai-detector)  
Openai release: [https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/](https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/)

If you are interested in #PromptEngineering, #LLMs, #ChatGPT and other latest research discussions, please consider joining our discord [discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)  


https://preview.redd.it/9d8ooeg2ljfa1.png?width=1358&format=png&auto=webp&s=0de7ccc5cd16d6bbad3dcfe7d8441547a6196fc8"
31,deeplearning,openai,top,2023-08-15 04:46:43,OpenAI Notebooks which are really helpful.,vishank97,False,0.91,18,15rihgo,https://www.reddit.com/r/deeplearning/comments/15rihgo/openai_notebooks_which_are_really_helpful/,3,1692074803.0,"The OpenAI cookbook is one of the most underrated and underused developer resources available today. Here are 7 notebooks you should know about:

1. Improve LLM reliability:  
[https://github.com/openai/openai-cookbook/blob/main/techniques\_to\_improve\_reliability.md](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)
2. Embedding long text inputs:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Embedding\_long\_inputs.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb)
3. Dynamic masks with DALLE:  
[https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How\_to\_create\_dynamic\_masks\_with\_DALL-E\_and\_Segment\_Anything.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How_to_create_dynamic_masks_with_DALL-E_and_Segment_Anything.ipynb)
4. Function calling to find places nearby:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Function\_calling\_finding\_nearby\_places.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb)
5. Visualize embeddings in 3D:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing\_embeddings\_in\_3D.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_3D.ipynb)
6. Pre and post-processing of Whisper transcripts:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Whisper\_processing\_guide.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb)
7. Search, Retrieval, and Chat:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Question\_answering\_using\_a\_search\_API.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb)

Big thanks to the creators of these notebooks!"
32,deeplearning,openai,top,2022-07-15 13:18:45,Have fun and learn AI at the Reinforcement Learning Hackathon on July 23rd!,zakrzzz,False,0.96,17,vzojmv,https://www.reddit.com/r/deeplearning/comments/vzojmv/have_fun_and_learn_ai_at_the_reinforcement/,1,1657891125.0,"One day to immerse yourself in technology that is a first for companies and engineers around the world!

To help you begin your immersion in AI as effectively as possible, we've prepared experts to assist you all the way.

Not without a competitive component, the winners will receive worthy prizes that will help them successfully use advanced technologies for their projects.

So come join us and learn everything you need to know about RL!

[Register here](https://lablab.ai/event/reinforcement-learning-openai-gym?utm_medium=23&utm_source=Reddit&utm_campaign=RL1&utm_term=Hackathon)

[Reinforcement Learning OpenAI Gym Hackathon](https://preview.redd.it/vm4gf3pviqb91.png?width=1920&format=png&auto=webp&s=bb3ba85c6e4d5b3069c742a3e38e6c3d70d6843a)"
33,deeplearning,openai,top,2023-01-28 06:34:28,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,1.0,17,10n8c80,https://www.reddit.com/r/deeplearning/comments/10n8c80/a_python_module_to_generate_optimized_prompts/,0,1674887668.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features 🚀

* 🧙‍♀️ NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* 🔨 Easily add one-shot, two-shot, or few-shot examples to the prompt
* ✌ Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* 💥 Custom examples and samples can be easily added to the prompt
* 💰 Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Example](https://preview.redd.it/sjvhtd8b3nea1.png?width=1236&format=png&auto=webp&s=e9a3a28f41f59cd25fe8e95bd1fca56b15f27a6e)

&#x200B;

https://preview.redd.it/fnb05bys3nea1.png?width=1398&format=png&auto=webp&s=096f4e2cbd0a71e795f30cc5e3720316b5e5caf6"
34,deeplearning,openai,top,2023-06-07 15:08:34,How does Openai's CLIP avoids dimensional collapse?,souhaielbensalem,False,0.95,17,143frxx,https://www.reddit.com/r/deeplearning/comments/143frxx/how_does_openais_clip_avoids_dimensional_collapse/,1,1686150514.0,"According to this paper from FAIR : [https://arxiv.org/abs/2110.09348](https://arxiv.org/abs/2110.09348)  , contrastive learning methods suffer from the problem of dimensional  collapse where ""the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available  embedding"". According to the authors, this problem is caused by  two  main reason which are strong augmentations and the implicit  regularization of deep neural networks that tend to converge to low rank  solutions. I am not sure how Openai's CLIP avoids this problem. Is it just the  sheer scale and the fact they used a batch size of 32k smaples?"
35,deeplearning,openai,top,2023-11-15 10:30:36,"GPT-4 Turbo: Die Zukunft der Künstlichen Intelligenz, entwickelt von OpenAI",Webglobic_tech,False,0.82,14,17vqtlk,https://webglobic.com/magazine/,0,1700044236.0,
36,deeplearning,openai,top,2022-07-06 11:22:15,Reinforcement Learning without Reward Engineering (reproducing OpenAI paper with crowdsourcing),Euphetar,False,0.91,15,vsnnv9,https://medium.com/p/60c63402c59f,0,1657106535.0,
37,deeplearning,openai,top,2021-11-01 14:33:01,"[R] Warsaw U, OpenAI and Google’s Hourglass Hierarchical Transformer Model Outperforms Transformer Baselines",Yuqing7,False,0.86,15,qkf9xu,https://www.reddit.com/r/deeplearning/comments/qkf9xu/r_warsaw_u_openai_and_googles_hourglass/,2,1635777181.0,"A team from the University of Warsaw, OpenAI and Google Research proposes Hourglass, a hierarchical transformer language model that operates on shortened sequences to alleviate transformers’ huge computation burdens. 

Here is a quick read: [Warsaw U, OpenAI and Google’s Hourglass Hierarchical Transformer Model Outperforms Transformer Baselines.](https://syncedreview.com/2021/11/01/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-135/)

The paper *Hierarchical Transformers Are More Efficient Language Models* is on [arXiv](https://arxiv.org/abs/2110.13711)."
38,deeplearning,openai,top,2021-09-26 16:13:31,I added Codex (GitHub Copilot) to the terminal,tomd_96,False,1.0,13,pvwvz7,https://www.reddit.com/r/deeplearning/comments/pvwvz7/i_added_codex_github_copilot_to_the_terminal/,1,1632672811.0,"&#x200B;

https://i.redd.it/8ww6msiugvp71.gif

You can now let Zsh write code for you using the plugin I wrote: [https://github.com/tom-doerr/zsh\_codex](https://github.com/tom-doerr/zsh_codex)

All you need to provide is a comment or a variable name and the plugin will use OpenAI's Codex AI (powers GitHub Copilot) to write the corresponding code.

Be aware that you do need to get access to the Codex API."
39,deeplearning,openai,top,2023-12-20 21:36:11,[Blogpost] Top Python Libraries of 2023,No_Dig_7017,False,0.88,13,18n5wzb,https://www.reddit.com/r/deeplearning/comments/18n5wzb/blogpost_top_python_libraries_of_2023/,4,1703108171.0,"Hello Python Community!

We're thrilled to present our 9th edition of the **Top Python Libraries and tools**, where we've scoured the Python ecosystem for the most innovative and impactful developments of the year.

This year, it’s been the boom of Generative AI and Large Language Models (LLMs) which have influenced our picks. Our team has meticulously reviewed and categorized over 100 libraries, ensuring we highlight both the mainstream and the hidden gems.

**Explore the entire list with in-depth descriptions here**: [](https://tryolabs.com/blog/top-python-libraries-2023)

Here’s a glimpse of our top 10 picks:

1. [LiteLLM](https://github.com/BerriAI/litellm) — Call any LLM using OpenAI format, and more.
2. [PyApp](https://github.com/ofek/pyapp) — Deploy self-contained Python applications anywhere.
3. [Taipy](https://github.com/Avaiga/taipy) — Build UIs for data apps, even in production.
4. [MLX](https://github.com/ml-explore/mlx) — Machine learning on Apple silicon with NumPy-like API.
5. [Unstructured](https://github.com/Unstructured-IO/unstructured) — The ultimate toolkit for text preprocessing.
6. [ZenML](https://github.com/zenml-io/zenml) and [AutoMLOps](https://github.com/GoogleCloudPlatform/automlops) — Portable, production-ready MLOps pipelines.
7. [WhisperX](https://github.com/m-bain/whisperX) — Speech recognition with word-level timestamps & diarization.
8. [AutoGen](https://github.com/microsoft/autogen) — LLM conversational collaborative suite.
9. [Guardrails](https://github.com/guardrails-ai/guardrails) — Babysit LLMs so they behave as intended.
10. [Temporian](https://github.com/google/temporian) — The “Pandas” built for preprocessing temporal data.

Our selection criteria prioritize innovation, robust maintenance, and the potential to spark interest across a variety of programming fields. Alongside our top picks, we've put significant effort into the long tail, showcasing a wide range of tools and libraries that are valuable to the Python community.

A huge thank you to the individuals and teams behind these libraries. Your contributions are the driving force behind the Python community's growth and innovation. 🚀🚀🚀

**What do you think of our 2023 lineup? Did we miss any library that deserves recognition?** Your feedback is vital to help us refine our selection each year.

Edit: updated the post body so the links are directly here in reddit."
40,deeplearning,openai,top,2023-04-02 18:10:37,Should we draw inspiration from Deep learning/Computer vision world for fine-tuning LLMs?,Vegetable-Skill-9700,False,0.75,11,129t3tl,https://www.reddit.com/r/deeplearning/comments/129t3tl/should_we_draw_inspiration_from_deep/,8,1680459037.0,"With HuggingGPT, BloombergGPT, and OpenAI's chatGPT store, it looks like the world is moving towards specialized GPTs for specialized tasks. What do you think are the best tips & tricks when it comes to fine-tuning and refining these task-specific GPTs?

Over the last decade, I have built many computer vision models (for human pose estimation, action classification, etc.), and our general approach was always based on Transfer learning. Take a state-of-the-art public model and fine-tune it by collecting data for the given use case.

Do you think that paradigm still holds true for LLMs?

Based on my experience, I believe observing the model's performance as it interacts with real-world data, identifying failure cases (where the model's outputs are wrong), and using them to create a high-quality retraining dataset will be the key.

&#x200B;

P.S. I am building an open-source project UpTrain ([https://github.com/uptrain-ai/uptrain](https://github.com/uptrain-ai/uptrain)), which helps data scientists to do so. We just wrote a blog on how this principle can be applied to fine-tune an LLM for a conversation summarization task. Check it out here: [https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation\_summarization](https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation_summarization)"
41,deeplearning,openai,top,2022-10-18 16:52:15,Fully automated video generation - connecting OpenAI's Whisper with Stable Diffusion. Tutorial & code coming soon!,hayAbhay,False,1.0,13,y7cbf4,https://youtu.be/xRcoeUgD4GY,8,1666111935.0,
42,deeplearning,openai,top,2021-09-03 00:19:43,OpenAI's Codex in Vim,tomd_96,False,0.88,12,pgu4ez,https://www.reddit.com/r/deeplearning/comments/pgu4ez/openais_codex_in_vim/,1,1630628383.0,"&#x200B;

https://i.redd.it/7xui0u2ml6l71.gif

You can now let your editor write Python code for you using the Vim plugin I wrote: [https://github.com/tom-doerr/vim\_codex](https://github.com/tom-doerr/vim_codex)

All you need to provide is a docstring and the plugin will use OpenAI's Codex AI (powers GitHub Copilot) to write the corresponding code.

Be aware that you do need to get access to the Codex API."
43,deeplearning,openai,top,2021-08-10 22:11:44,Demo - Improved OpenAI Codex that translates natural language to code,rshpkamil,False,0.92,11,p1zkqp,https://www.reddit.com/r/deeplearning/comments/p1zkqp/demo_improved_openai_codex_that_translates/,0,1628633504.0,https://openai.com/blog/openai-codex/
44,deeplearning,openai,top,2021-01-13 20:28:24,OpenAI CLIP: ConnectingText and Images (Paper Explained),myyoucef,False,0.86,10,kwp46g,https://www.youtube.com/watch?v=T9XSU0pKX2E,0,1610569704.0,
45,deeplearning,openai,top,2022-09-23 16:28:06,OpenAI Whisper: SOTA Speech To Text With Microphone Demo,l33thaxman,False,1.0,12,xm26oq,https://www.reddit.com/r/deeplearning/comments/xm26oq/openai_whisper_sota_speech_to_text_with/,0,1663950486.0,"OpenAI has released a Speech To Text model that nears human performance.  This video goes over the basics of the model, as well as how to run it with a microphone.

[https://youtu.be/nwPaRSlDSaY](https://youtu.be/nwPaRSlDSaY)"
46,deeplearning,openai,top,2023-08-02 11:33:32,Using PDFs with GPT Models,Disastrous_Look_1745,False,0.75,10,15g6i4x,https://www.reddit.com/r/deeplearning/comments/15g6i4x/using_pdfs_with_gpt_models/,2,1690976012.0,Found a blog talking about how we can interact with PDFs in Python by using GPT API & Langchain. It talks about some pretty cool automations you can build involving PDFs - [https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/](https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/)
47,deeplearning,openai,top,2022-12-08 20:36:26,Daath AI Parser is an open-source application that uses OpenAI to parse visible text of HTML elements.,softcrater,False,1.0,11,zgarkj,https://github.com/kagermanov27/daath-ai-parser,0,1670531786.0,
48,deeplearning,openai,top,2024-02-06 18:19:49,"Edgen: A Local, Open Source GenAI Server Alternative to OpenAI in Rust",EdgenAI,False,0.92,10,1akghpe,https://www.reddit.com/r/deeplearning/comments/1akghpe/edgen_a_local_open_source_genai_server/,0,1707243589.0,"⚡Edgen: Local, private GenAI server alternative to OpenAI. No GPU required. Run AI models locally: LLMs (Llama2, Mistral, Mixtral...), Speech-to-text (whisper) and many others.

Our goal with⚡Edgen is to make privacy-centric, local development accessible to more people, offering full compliance with OpenAI's API. It's made for those who prioritize data privacy and want to experiment with or deploy AI models locally with a Rust based infrastructure.

We'd love for this community to be among the first to try it out, give feedback, and contribute to its growth. 

Check it out here:  [GitHub - edgenai/edgen: ⚡ Edgen: Local, private GenAI server alternative to OpenAI. No GPU required. Run AI models locally: LLMs (Llama2, Mistral, Mixtral...), Speech-to-text (whisper) and many others.](https://github.com/edgenai/edgen) "
49,deeplearning,openai,top,2021-01-07 05:25:23,OpenAI Introduces DALL·E: A Neural Network That Creates Images From Text Descriptions,ai-lover,False,0.87,11,ks6jag,https://www.marktechpost.com/2021/01/06/openai-introduces-dall%C2%B7e-a-neural-network-that-creates-images-from-text-descriptions,0,1609997123.0,
50,deeplearning,openai,top,2023-04-07 10:28:52,"Series of Surveys on ChatGPT, Generative AI (AIGC), and Diffusion Models",Learningforeverrrrr,False,0.87,10,12egmab,https://www.reddit.com/r/deeplearning/comments/12egmab/series_of_surveys_on_chatgpt_generative_ai_aigc/,0,1680863332.0,"* **A survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)
* **A survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)
* **A survey on Text-to-image diffusion models:** [**Text-to-image Diffusion Models in Generative AI: A Survey**](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)
* **A survey on Audio diffusion models:** [**A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI**](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI)
* **A survey on Graph diffusion models:** [**A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material**](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material)

**ChatGPT goes viral.** Launched by OpenAI on November 30, 2022, ChatGPT has attracted unprecedented attention due to its powerful abilities all over the world.  It took only 5 days \[1\] and 2 months \[2\] for ChatGPT to have 1 million users and 100 million monthly users after launch, making it the fastest-growing consumer application in history. ChatGPT can be seen as the milestone for the GPT family to go viral. In academia, ChatGPT has also inspired a large number of works discussing its applications in multiple fields, with **more than 500 papers within four months** after release and **the number is still increasing rapidly.**  This brings a huge challenge for a researcher who hopes to have an overview of ChatGPT applications or hopes to start his or her journey with ChatGPT in their own field.  **To help more people keep up with the latest progress of the GPT family,** we’re glad to share a self-contained survey that not only summarizes **the recent applications** of ChatGPT and other GPT variants like GPT-4, but also introduces the **underlying techniques** and **challenges.** Please refer to the following link for the paper: [One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era).

&#x200B;

**From ChatGPT to Generative AI.**  One highlighting ability of the GPT family is that it can generate natural languages, which falls into the area of Generative AI. Apart from text, Generative AI can also generate content in other modalities, such as image, audio, and graph. More excitingly, Generative AI is able to convert data from one modality to another one, such as the text-to-image task (generating images from text). **To help readers have a better overview of Generative AI,** we provide a complete survey on underlying **techniques,** summary and development of **typical tasks in academia**, and also **industrial applications.** Please refer to the following link for the paper.  [A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

&#x200B;

**From Generative AI to Diffusion Models.** The prosperity of a field is always driven by the development of technology, and so is Generative AI.  Different from ChatGPT which generates text based on the transformer, **diffuson models** have greatly accelerated the development of other fields in Generative AI, such as image synthesis.  Although we provide a summary of diffusion models and typical tasks in the Generative AI survey, we cannot include detailed discussions due to paper length limitations. **For those who are interested in the technical details of diffusion models and the recent progress of their applications in Generative AI,** we provide three self-contained surveys on **how diffusion models are applied in three typical areas: Text-to-image diffusion models** (also includes related tasks such as image editing)**, Audio diffusion models** (including text to speech synthesis and enhancement), and **Graph diffusion models** (including molecule, protein and material areas). Please refer to the following links for the paper.

* [Text-to-image Diffusion Models in Generative AI: A Survey](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)
* [A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI)
* [A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material)

We hope our survey series will help people for a better understanding of ChatGPT and Generative AI, and we will update the survey regularly to include the latest progress. Please refer to the personal pages of the authors for the latest updates on surveys. If you have any suggestions or problems, please feel free to contact us.

\[1\] Greg Brockman, co-founder of OpenAI, [https://twitter.com/gdb/status/1599683104142430208?lang=en](https://twitter.com/gdb/status/1599683104142430208?lang=en)

\[2\] Reuters, [https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/)"
51,deeplearning,openai,top,2019-04-10 19:21:52,Creating a Custom OpenAI Gym Environment for Stock Trading,notadamking,False,0.91,9,bbq5mi,https://medium.com/@adamjking3/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e,0,1554924112.0,
52,deeplearning,openai,top,2023-03-03 23:02:37,Meta new large lanugage model (similar to OpenAI one) called LLaMA is leaked via torrent,aipaintr,False,0.81,6,11hhzeg,https://github.com/facebookresearch/llama/pull/73/files,2,1677884557.0,
53,deeplearning,openai,top,2022-06-15 16:07:22,Join us for the OpenAI GPT-3 Deep Learning Labs Hackathon!,zakrzzz,False,0.82,7,vcxzp4,https://www.reddit.com/r/deeplearning/comments/vcxzp4/join_us_for_the_openai_gpt3_deep_learning_labs/,0,1655309242.0,"We are waiting for all of you, AI enthusiasts with coding experience and without, on the 24th - 26th of June to help you turn your ground-breaking ideas into reality!

Register here - [https://lablab.ai/event/gpt3-online](https://lablab.ai/event/gpt3-online)  


https://preview.redd.it/mhr93wgo6t591.png?width=1600&format=png&auto=webp&s=07e23c79830db8061eb300f76b64588b01219ebc"
54,deeplearning,openai,top,2023-02-20 21:27:58,Fine tuning a GPT for text generation,nashcaps2724,False,0.82,7,117l2vf,https://www.reddit.com/r/deeplearning/comments/117l2vf/fine_tuning_a_gpt_for_text_generation/,6,1676928478.0,"Hi all, let me lay out my problem…

Imagine there are two corpora, Corpus A (100,000~) and Corpus B (20,000,000~). 

Individuals create reports for corpus A based on the information in corpus B. 

My idea was to pretrain a GPT on corpus A, and fine tune it to take documents from corpus B as an input, and output text in the style of corpus A (essentially a mix of text generation and summarization). 

Is this something folks think is even feasible? Should I be pretaining the GPT on both corpora or just corpus A? I thought of both fine tuning an OpenAI GPT and training from scratch. 

Any advice would be welcome!"
55,deeplearning,openai,top,2019-10-20 21:26:53,"AI Weekly Update #9 - (October 20th, 2019)",HenryAILabs,False,0.8,9,dkq5m7,https://www.reddit.com/r/deeplearning/comments/dkq5m7/ai_weekly_update_9_october_20th_2019/,3,1571606813.0,"https://www.youtube.com/watch?v=03kCD18H5nQ

This week's update covers OpenAI's amazing robotic hand and exciting updates such as Facebook's Semi-Weakly Supervised learning framework, Google's MASSIVE (50BN params) Multilingual NMT models, and many more!"
56,deeplearning,openai,top,2020-03-17 08:40:17,[P] Simple PyTorch Implementation of OpenAI GPT-1,lyeoni,False,0.9,7,fk1tog,https://www.reddit.com/r/deeplearning/comments/fk1tog/p_simple_pytorch_implementation_of_openai_gpt1/,2,1584434417.0,"Hello :)

Most  of OpenAI GPT codes are for version 2 (GPT-2), and GPT-2 is hard to use  restricted small GPU resources. So, I implemented GPT-1 for someone who  wants to pre-train/fine-tune with very small resources, like me.  Because GPT-1 requires relatively small resources.

And, I also added belows.

* training logs for pre-training on WikiText-103, fine-tuning on IMDb.
* results for the question : *Does auxiliary objective function have a bigger impact?*

I hope that this repo can be a good solution for people who want to train GPT model with restricted resources.

\[LINK\] : [https://github.com/lyeoni/gpt-pytorch](https://github.com/lyeoni/gpt-pytorch)"
57,deeplearning,openai,top,2023-08-01 09:44:05,Learn how to build products with LLMs,sidhusmart,False,0.77,7,15f7qlv,https://www.reddit.com/r/deeplearning/comments/15f7qlv/learn_how_to_build_products_with_llms/,0,1690883045.0,"Hi everyone,

I’ve learnt a lot from various communities on Reddit and wanted to share something that’s free and hopefully useful to you or your team members.

I’m hosting a free, online, cohort-based course in collaboration with OpenAI. I think we are all very impressed by the demos you see on Twitter and wanted to see what is possible to build and really got dug into it - building actual, tangible products with AI. It started off with scratching my own itch and in the process, I discovered a lot of cool tools and startups like [Modal Labs](https://modal.com/) that make the process of building something very easy. At the same time, I also discovered the edges where things don’t work and constraints around response times and how one might work around them. 

The course is structured in the form of two sessions of 1-1.5 hours each accompanied by course content, slides, and Colab notebooks. The focus is very much on the project where we will first work with OpenAI APIs to extract information from podcast episodes. We will build the backend service using Modal Labs and host a front end using Streamlit. At the end, you should have a deployed product that generates a weekly newsletter summarizing your favorite episodes :D

I’m happy to answer any questions you have and feel free to sign up [here](https://www.corise.com/go/building-ai-products-with-openai-6X7XAG)."
58,deeplearning,openai,top,2021-06-09 14:57:40,[R] Pieter Abbeel Team’s Decision Transformer Abstracts RL as Sequence Modelling,Yuqing7,False,0.74,7,nvxwi7,https://www.reddit.com/r/deeplearning/comments/nvxwi7/r_pieter_abbeel_teams_decision_transformer/,2,1623250660.0,"A research team from UC Berkeley, Facebook AI Research and Google Brain abstracts Reinforcement Learning (RL) as a sequence modelling problem. Their proposed Decision Transformer simply outputs optimal actions by leveraging a causally masked transformer, yet matches or exceeds state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.

Here is a quick read: [Pieter Abbeel Team’s Decision Transformer Abstracts RL as Sequence Modelling.](https://syncedreview.com/2021/06/09/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-37/)

The paper *Decision Transformer: Reinforcement Learning via Sequence Modeling* is on [arXiv](https://arxiv.org/abs/2106.01345)."
59,deeplearning,openai,top,2021-01-06 00:55:05,OpenAI successfully trained a network able to generate images from text captions: DALL·E,OnlyProggingForFun,False,0.78,7,krcfkg,https://youtu.be/nLzfDVwQxRU,0,1609894505.0,
60,deeplearning,openai,top,2022-07-24 13:36:42,OpenAI GLIDE (Diffusion) | ML Coding series | Towards Photorealistic Image Generation and Editing,gordicaleksa,False,0.76,6,w6vst8,https://youtu.be/c1GwVg3lt1c,0,1658669802.0,
61,deeplearning,openai,top,2022-12-28 16:01:41,Andrew Huberman transcripts app - high-quality transcription using OpenAI's largest Whisper model (see comment),gordicaleksa,False,1.0,8,zxd7yd,https://www.hubermantranscripts.com/,2,1672243301.0,
62,deeplearning,openai,top,2019-04-14 05:16:57,Humans Call GG! OpenAI Five Bots Beat Top Pros OG in Dota 2,gwen0927,False,0.86,5,bczk3e,https://medium.com/syncedreview/humans-call-gg-openai-five-bots-beat-top-pros-og-in-dota-2-8508e59b8fd5,0,1555219017.0,
63,deeplearning,openai,top,2020-06-16 03:13:01,"I just published ""All you need to need to know about OPENAI's GPT-3 "" on medium . Check out , feedback is highly appreciated..",dharma_m,False,0.64,6,h9ve0q,https://medium.com/@savanidharmam5/all-you-need-to-know-about-openai-gpt-3-d0d879446aeb,0,1592277181.0,
64,deeplearning,openai,top,2022-10-04 01:00:17,Create your own speech to text application with Whisper from OpenAI and Flask,hellopaperspace,False,0.73,5,xv0x55,https://blog.paperspace.com/whisper-openai-flask-application-deployment/,0,1664845217.0,
65,deeplearning,openai,top,2023-11-23 12:13:26,OpenAI Q* Rumours,MIKOLAJslippers,False,0.61,6,181zwb8,https://www.reddit.com/r/deeplearning/comments/181zwb8/openai_q_rumours/,30,1700741606.0,Anyone know any juicy rumours about the capabilities of this internal Q* project at OpenAI that has supposedly catalysed some of the recent dramatics?
66,deeplearning,openai,top,2020-07-24 09:50:27,[Tutorial] Sentence to SQL Converter using GPT-3,bhavesh91,False,0.73,5,hwyz1v,https://www.reddit.com/r/deeplearning/comments/hwyz1v/tutorial_sentence_to_sql_converter_using_gpt3/,1,1595584227.0,"I created a simple Sentence to SQL Converter using GPT - 3. If you want to learn how you can use OpenAI's GPT-3 to generate NLP Applications then this simple tutorial should help.Video Link : [https://www.youtube.com/watch?v=9g66yO0Jues](https://www.youtube.com/watch?v=9g66yO0Jues)

https://reddit.com/link/hwyz1v/video/79gg5vrj1sc51/player"
67,deeplearning,openai,top,2019-02-27 11:37:45,My Thoughts on OpenAI Not Releasing Weights,ericnyamu,False,0.69,7,avcjo4,https://www.mawazoforum.com/viewtopic.php?f=40&t=437,0,1551267465.0,
68,deeplearning,openai,top,2023-11-23 15:47:45,A deeper look at the Q* Model as a combination of A* algorithms and Deep Q-learning networks.,Ok-Judgment-1181,False,0.58,6,18240yl,https://www.reddit.com/r/deeplearning/comments/18240yl/a_deeper_look_at_the_q_model_as_a_combination_of/,5,1700754465.0,"Hey, folks! Buckle up because the recent buzz in the AI sphere has been nothing short of an intense rollercoaster. Rumors about a groundbreaking AI, enigmatically named Q\* (pronounced Q-Star), have been making waves, closely tied to a chaotic series of events that rocked OpenAI and came to light after the [abrupt firing of their CEO](https://edition.cnn.com/2023/11/17/tech/sam-altman-departs-open-ai/index.html) \- Sam Altman ( [u/samaltman](https://www.reddit.com/u/samaltman/) **)**.

There are several questions I would like to entertain, such as the impacts of Sam Altman's firing, the most probable reasons behind it, and the possible monopoly on highly efficient AI technologies that Microsoft is striving to have. However, all these things are too much for 1 Reddit post, so here **I will attempt to explain why Q\* is a BIG DEAL, as well as go more in-depth on the theory of combining Q-learning and A\* algorithms**.

At the core of this whirlwind is an AI (Q\*) that aces grade-school math but does so without relying on external aids like Wolfram. It may possibly be a paradigm-shattering breakthrough, transcending AI stereotypes of information repeaters and stochastic parrots which showcases iterative learning, intricate logic, and highly effective long-term strategizing.

This milestone isn't just about numbers; it's about unlocking an AI's capacity to navigate the single-answer world of mathematics, potentially revolutionizing reasoning across scientific research realms, and breaking barriers previously thought insurmountable.

What are A\* algorithms and Q-learning?:

From both the name and rumored capabilities, the Q\* is very likely to be an AI agent that combines A\* Algorithms for planning and Q-learning for action optimization. Let me explain.

[A\* algorithms](https://theory.stanford.edu/~amitp/GameProgramming/AStarComparison.html) serve as powerful tools for finding the shortest path between two points in a graph or a map while efficiently navigating obstacles. Their primary purpose lies in optimizing route planning in scenarios where finding the most efficient path is crucial. These algorithms are known to balance accuracy and efficiency with the notable capabilities being: Shortest Path Finding, Adaptability to Obstacles, and their computational Efficiency / Optimality (heuristic estimations).

However, applying A\* algorithms to a chatbot AI involves leveraging its pathfinding capabilities in a rather different context. While chatbots typically don’t navigate physical spaces, **they do traverse complex information landscapes to find the most relevant responses or solutions to user queries**. Hope you see where I´m going with this, but just in case let's talk about Q-learning for a bit.

Connecting the dots even further, let's think of [Q-learning](https://builtin.com/artificial-intelligence/deep-q-learning) as us giving the AI a constantly expanding cheat sheet, helping it decide the best actions based on past experiences. However, in complex scenarios with vast states and actions, maintaining a mammoth cheat sheet becomes unwieldy and hinders our progress toward AGI due to elevated compute requirements. Deep Q-learning steps in, utilizing neural networks to approximate the Q-value function rather than storing it outright.

Instead of a colossal Q-table, the network maps input states to action-Q-value pairs. It's like having a compact cheat sheet tailored to navigate complex scenarios efficiently, giving AI agents the ability to pick actions based on the [Epsilon-Greedy approach](https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/)—sometimes randomly exploring, sometimes relying on the best-known actions predicted by the networks. Normally DQNs (or [Deep Q-networks](https://www.tensorflow.org/agents/tutorials/0_intro_rl)), use two neural networks—the main and target networks—sharing the same architecture but differing in weights. Periodically, their weights synchronize, enhancing learning and stabilizing the process, this last point is highly important to understand as it may become the key to a model being capable of **self-improvement** which is quite a tall feat to achieve. This point however is driven further if we consider the [Bellman equation](https://www.geeksforgeeks.org/bellman-equation/), which basically states that with each action, the networks update weights using the equation utilizing Experience replay—a sampling and training technique based on past actions— which helps the AI learn in small batches **without necessitating training after every step**.

*I must also mention that Q\*'s potential is not just a math whiz but rather* ***a gateway to scaling abstract goal navigation*** *as we do in our heads when we plan things, however, if achieved at an AI scale we would likely get highly efficient, realistic and logical plans to virtually any query or goal (highly malicious, unethical or downright savage goals included)...*

Finally, there are certain **pushbacks and challenges** to overcome with these systems which I will underline below, HOWEVER, with the recent news surrounding OpenAI, I have a feeling that smarter people have found ways of tackling these challenges efficiently enough to have a huge impact of the industry if word got out.

To better understand possible challenges I would like to give you a hypothetical example of a robot that is tasked with solving a maze, where the starting point is user queries and the endpoint is a perfectly optimized completion of said query, with the maze being the World Wide Web.

Just like a complex maze, the web can be labyrinthine, filled with myriad paths and dead ends. And although the A\* algorithm helps the model seek the shortest path, certain intricate websites or information silos can confuse the robot, leading it down convoluted pathways instead of directly to the optimal solution (problems with web crawling on certain sites).

By utilizing A\* algorithms the AI is also able to adapt to the ever-evolving landscape of the web, with content updates, new sites, and changing algorithms. However, due to the speed being shorter than the web expansion, it may fall behind as it plans based on an initial representation of the web. When new information emerges or websites alter their structures, the algorithm might fail to adjust promptly, impacting the robot's navigation.

On the other hand, let's talk about the challenges that may arise when applying Q-learning. Firstly it would be limited sample efficiency, where the robot may pivot into a fraction of the web content or stick to a specific subset of websites, it might not gather enough diverse data to make well-informed decisions across the entire breadth of the internet therefore failing to satisfy user query with utmost efficiency.

And secondly, problems may arise when tackling [high-dimensional data](https://www.statology.org/high-dimensional-data/). The web encompasses a vast array of data types, from text to multimedia, interactive elements, and more. Deep Q-learning struggles with high-dimensional data (That is data where the number of features in a dataset exceeds the number of observations, due to this fact we will never have a deterministic answer). In this case, if our robot encounters sites with complex structures or extensive multimedia content, processing all this information efficiently becomes a significant challenge.

To combat these issues and integrate these approaches one must find a balance between optimizing pathfinding efficiency while swiftly adapting to the dynamic, multifaceted nature of the Web to provide users with the most relevant and efficient solutions to their queries.

To conclude, there are plenty of rumors floating around the Q\* and Gemini models as giving AI the ability to plan is highly rewarding due to the increased capabilities however it is also quite a risky move in itself. This point is further supported by the constant reminders that we need better AI safety protocols and guardrails in place before continuing research and risking achieving our goal just for it to turn on us, but I'm sure you've already heard enough of those.So, are we teetering on the brink of a paradigm shift in AI, or are these rumors just a flash in the pan? Share your thoughts on this intricate and evolving AI saga—it's a front-row seat to the future!

I know the post came out lengthy and pretty dense, but I hope this post was helpful to you! Please do remember that this is mere speculation based on multiple news articles, research, and rumors currently speculating regarding the nature of Q\*, take the post with a grain of salt :)

**Edit:** After several requests, I would like to mention an Arxiv paper on a very similar topic I've discussed in the post:

***A\* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks*** ([https://arxiv.org/abs/2102.04518v2](https://arxiv.org/abs/2102.04518v2))

*Let us all push the veil of ignorance back and the frontier of discovery forward.*"
69,deeplearning,openai,top,2023-03-02 07:25:30,Good news for builders! OpenAI Releases APIs To ChatGPT and Whisper,LesleyFair,False,0.65,5,11fwcxf,https://www.reddit.com/r/deeplearning/comments/11fwcxf/good_news_for_builders_openai_releases_apis_to/,0,1677741930.0,"If you were as disappointed as I was when you saw that access to Meta's LLaMA models is limited to researchers, you are going to like this.  


[APIs to ChatGPT and OpenAI's speech-to-text model whisper](https://openai.com/blog/introducing-chatgpt-and-whisper-apis) are available as of yesterday. Through system-wide optimizations, they claim to have reduced inference costs by 90%. They now price ChatGPT at $0.002 per 1000 tokens. Dedicated instances are available for speedup and make economic sense if you process \~450M tokens a day.  


Machine learning progress continues to be as fast as a banana peal skating on warm vaseline. 

If you found this useful and want to stay in the loop, consider subscribing to The Decoding. I send out a weekly 5-minute newsletter that keeps professionals in the loop about machine learning and the data economy. [Click here to subscribe!](https://thedecoding.net/)"
70,deeplearning,openai,top,2023-06-15 11:39:53,OpenAI function calling - tutorial,mildlyoverfitted,False,0.89,7,14a06nr,https://youtu.be/_B7F_6nTVEg,0,1686829193.0,
71,deeplearning,openai,top,2016-12-07 17:26:55,"'A Deep Hierarchical Approach to Lifelong Learning in Minecraft' (AAAI-17), Tessler et al 2016",chentessler,False,0.76,4,5h16m9,https://www.reddit.com/r/deeplearning/comments/5h16m9/a_deep_hierarchical_approach_to_lifelong_learning/,2,1481131615.0,"Hi everyone, 

We would like to share with you our recent work entitled 'A Deep Hierarchical Approach to Lifelong Learning in Minecraft' (AAAI-17) ([paper](https://arxiv.org/abs/1604.07255), [website](http://chentessler.wixsite.com/hdrlnminecraft)). This work presents a lifelong deep reinforcement learning system that is able to efficiently retain as well as transfer knowledge (via reusable skills) to solve new, unseen tasks; two of the key building blocks to lifelong learning.
It is an exciting time for Deep Reinforcement Learning (DRL) research as new, complex gaming environments are being open sourced ([Minecraft - Malmo](https://www.microsoft.com/en-us/research/project/project-malmo/), [OpenAI - Universe](https://universe.openai.com/), [StarCraft - TorchCraft](https://arxiv.org/abs/1611.00625), [StarCraft 2](https://deepmind.com/blog/deepmind-and-blizzard-release-starcraft-ii-ai-research-environment/), [DeepMind Lab](https://deepmind.com/blog/open-sourcing-deepmind-lab/)
) and shared with the AI community. We strongly believe that taking advantage of hierarchy as well as efficient mechanisms to transfer and retain knowledge are soon to play significant roles in the ability of DRL agents to scale in these new exciting environments. 

Cheers"
72,deeplearning,openai,top,2023-05-11 02:01:13,OpenAI & GPT Dictionary of Vocabulary. Generative AI Terms To Know In 2023,OnlyProggingForFun,False,0.86,5,13ea348,https://youtu.be/q4G6X09NEu4,1,1683770473.0,
73,deeplearning,openai,top,2021-01-08 16:12:03,OpenAI's CLIP method explained! (CLIP > DALL-E change my mind),gordicaleksa,False,0.69,5,kt5k3z,https://youtu.be/fQyHEXZB-nM,2,1610122323.0,
74,deeplearning,openai,top,2016-04-27 23:38:30,Train Your Reinforcement Learning Agents At The OpenAI Gym,harrism,False,0.78,5,4graaq,https://devblogs.nvidia.com/parallelforall/train-reinforcement-learning-agents-openai-gym/,0,1461800310.0,
75,deeplearning,openai,top,2022-11-11 16:42:07,We just release a complete open-source solution for accelerating Stable Diffusion pretraining and fine-tuning!,HPCAI-Tech,False,0.87,6,ysfpib,https://www.reddit.com/r/deeplearning/comments/ysfpib/we_just_release_a_complete_opensource_solution/,0,1668184927.0,"Hey folks. We just release a **complete open-source solution** for accelerating Stable Diffusion pretraining and fine-tuning. It help **reduce the pretraining cost by 6.5 times, and the hardware cost of fine-tuning by 7 times, while simultaneously speeding up the processes.**

Open source address: [**https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion**](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion)

Our codebase for the diffusion models builds heavily on [OpenAI's ADM codebase](https://github.com/openai/guided-diffusion) , [lucidrains](https://github.com/lucidrains/denoising-diffusion-pytorch), [Stable Diffusion](https://github.com/CompVis/stable-diffusion), [Lightning](https://github.com/Lightning-AI/lightning) and [Hugging Face](https://huggingface.co/CompVis/stable-diffusion). Thanks for open-sourcing!

We also write a blog post about it. [https://medium.com/@yangyou\_berkeley/diffusion-pretraining-and-hardware-fine-tuning-can-be-almost-7x-cheaper-85e970fe207b](https://medium.com/@yangyou_berkeley/diffusion-pretraining-and-hardware-fine-tuning-can-be-almost-7x-cheaper-85e970fe207b)

Glad to know your thoughts about our work!"
76,deeplearning,openai,top,2023-06-18 21:47:43,Demystifying OpenAI Procurement: An Overview of Processes and Pricing,digital-bolkonsky,False,0.78,5,14cw6o0,/r/ai_cost/comments/14cw61j/demystifying_openai_procurement_an_overview_of/,0,1687124863.0,
77,deeplearning,openai,top,2019-11-23 11:12:37,OpenAI releases Safety Gym for reinforcement learning | VentureBeat,RankLord,False,0.81,3,e0glop,https://venturebeat.com/2019/11/21/openai-safety-gym/,0,1574507557.0,
78,deeplearning,openai,top,2019-05-19 18:20:34,Different writing styles for text generation,tetrix994,False,1.0,5,bqk4sx,https://www.reddit.com/r/deeplearning/comments/bqk4sx/different_writing_styles_for_text_generation/,0,1558290034.0,"Hello everyone,

I understand how text generation works but I was wondering if there is a way to train a model that can generate text based on our desires. For example, I have some context and I would like it to generate more poetic , more philosophical or let's say a mix between the two of them with more accent on philosophy.

I am not sure how that would be achieved. I know that OpenAI GPT of BERT can generate quite good texts but I am not sure even with fine-tuning on those topic if there is a way to achieve that.

Maybe a way to do that is with adjusting the temperature of the LSTM but I am not really sure.

Does anyone have an idea how that would be implemented?"
79,deeplearning,openai,top,2017-11-11 17:08:56,Setting mean and std of REWARDS in reinforcement learning - a question,closedloopy,False,0.84,4,7c9k9y,https://www.reddit.com/r/deeplearning/comments/7c9k9y/setting_mean_and_std_of_rewards_in_reinforcement/,8,1510420136.0,"In the great post [pong to pixels](http://karpathy.github.io/2016/05/31/rl/) by Karpathy, and more explicitly in his code [here](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5#file-pg-pong-py-L108) we see that he sets the mean of the rewards to 0 and the standard deviation to 1. This confuses me because that means that half of the rewards will be greater than zero, and the other less than zero. Now, lets assume this array of rewards came from an episode that we liked (good performance) then we'd want to reinforce the behavior. But as far as I can tell half of the actions will be associated with positive reward (and thus encouraged) and the other half with negative reward (and thus discourage). 


*Can anyone help me get a better intuition about why he does this? An example by Pytorch follows:*


PyTorch has in [their demo](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py) a solution of solving the [cart pole](https://gym.openai.com/envs/CartPole-v0/) from open ai gym, and the solution does the same thing in terms of modifying the rewards:

First we have the raw rewards (all ones. The longer the pole stays balanced, the more reward we get):

    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
      1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
      1.,  1.,  1.,  1.,  1.]

Then we [discount](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L62) them:

    [26.76966303456023, 26.02996266117195, 25.282790566840355, 24.528071279636723, 23.76572856528962, 
    22.995685419484467, 22.21786406008532, 21.4321859192781, 20.638571635634445, 19.8369410460954, 
    19.027213177874142, 18.209306240276913, 17.383137616441328, 16.54862385499124, 
    15.705680661607312, 14.854222890512437, 13.994164535871148, 13.12541872310217, 
    12.247897700103202, 11.361512828387072, 10.466174574128356, 9.561792499119552, 8.64827525163591, 
    7.72553055720799, 6.793465209301, 5.8519850599, 4.90099501, 3.9403989999999998, 2.9701, 1.99, 1.0]

Then finally we apply the [mean of 0 and std of 1](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L65):

     [1.6023, 1.5001, 1.3965, 1.2929, 1.1897,
     1.0875, 0.9859, 0.8842, 0.7819, 0.6796,
     0.5768, 0.4728, 0.3672, 0.2595, 0.1500,
     0.0383, -0.0761, -0.1939, -0.3137, -0.4315,
    -0.5477, -0.6630, -0.7778, -0.8916, -1.0049
    -1.1172, -1.2289, -1.3396, -1.4498, -1.5600, -1.6695]

To be clear, I've seen [this post](http://cs231n.github.io/neural-networks-2/) where we learn about data pre-processing and the value of updating mean and std of INPUT, but that is on the INPUT DATA, not the REWARDS. My question is why we would apply this transformation to the REWARDS.

Thank you!"
80,deeplearning,openai,top,2024-02-21 15:00:34,"⚡Edgen now supports Vulkan, CUDA and Metal | Open Source and local GenAI server alternative to OpenAI's API. Supports all GGUF models, across Windows, Mac and Linux with one 30MB download.",EdgenAI,False,1.0,4,1awe3tn,https://www.reddit.com/r/deeplearning/comments/1awe3tn/edgen_now_supports_vulkan_cuda_and_metal_open/,0,1708527634.0,"Our goal with⚡Edgen is to make privacy-centric, local GenAI app development accessible to more people.

It is compliant with OpenAI's API and built in  🦀 Rust so it can be natively compiled into Windows, Linux and MacOS (with a 30MB executable).

We'd love for this community to be among the first to try it out and provide feedback!

Check out⚡Edgen on GitHub: [GitHub - edgenai/edgen: ⚡ Edgen: Local, private GenAI server alternative to OpenAI.](https://github.com/edgenai/edgen)

And keep an an eye out for future releases:

* Speech to Text
* Embeddings Endpoint
* Multimodal Endpoint
* Text to Image Endpoint

&#x200B;"
81,deeplearning,openai,top,2022-09-22 19:32:49,OpenAI Whisper powered Gradio App for Automatic Subtitle Video Generation,dulldata,False,1.0,4,xlawi6,https://www.youtube.com/watch?v=x_uxzgTg1U0,0,1663875169.0,
82,deeplearning,openai,top,2021-09-03 15:34:29,"Tutorial: Visual Introduction to Deep Reinforcement Learning with OpenAI Gym, Google Colab, and RLlib",mgalarny,False,0.86,5,ph7jzs,https://towardsdatascience.com/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google-colab-48fc1ddfb889,0,1630683269.0,
83,deeplearning,openai,top,2024-01-31 12:26:30,Become an AI Developer (Free 9 Part Series),Competitive_data786,False,0.84,4,1afgp2r,https://www.reddit.com/r/deeplearning/comments/1afgp2r/become_an_ai_developer_free_9_part_series/,0,1706703990.0,"Just sharing a free series I stumbled across on Linkedin - DataCamp's 9-part AI code-along series.

This specific session linked below is ""Building Chatbots with OpenAI API and Pinecone"" but there are 8 others to have a look at and code along to.

*Start from basics to build on skills with GPT, Pinecone and LangChain to create a chatbot that answers questions about research papers. Make use of retrieval augmented generation, and learn how to combine this with conversational memory to hold a conversation with the chatbot. Code Along on DataCamp Workspace:* [*https://www.datacamp.com/code-along/building-chatbots-openai-api-pinecone*](https://www.datacamp.com/code-along/building-chatbots-openai-api-pinecone)

Find all of the sessions at: [https://www.datacamp.com/ai-code-alongs](https://www.datacamp.com/ai-code-alongs)"
84,deeplearning,openai,top,2023-08-28 07:12:44,OpenAI introduces fine-tuning capabilities for GPT-3.5 Turbo,intengineering,False,0.84,4,163f3fp,https://interestingengineering.com/innovation/openai-introduces-fine-tuning-capabilities-for-gpt-35-turbo,1,1693206764.0,
85,deeplearning,openai,top,2024-02-16 03:20:25,Unbelieve New Text To Video AI Model By OpenAI - 37 Demo Videos - Still Can't Believe Real - Watch All Videos 4K With A Nice Music - This Will Change Perception Of Reality Forever,CeFurkan,False,0.67,5,1aryk48,https://www.youtube.com/watch?v=VlJYmHNRQZQ&deeplearning,3,1708053625.0,
86,deeplearning,openai,top,2022-12-16 08:54:22,Research/applied scientist on the sub,rexstiener,False,0.78,5,zna7kb,https://www.reddit.com/r/deeplearning/comments/zna7kb/researchapplied_scientist_on_the_sub/,0,1671180862.0,"Hey mod , can you get research/ applied scientists or phd grad working in MNCs to this sub so that we can discuss about salary negotiations, opportunities & lotmore possibilties. For example i personally have no idea about scientists working in openAI , stabilityAI, midjourney & niether i could find those employees on linkdeln ."
87,deeplearning,openai,top,2019-08-26 06:23:21,"Interview with the creator of MuseNet: Christine Payne all about MuseNet, OpenAI and Deep Learning Research",init__27,False,0.81,3,cvk8vn,https://www.reddit.com/r/deeplearning/comments/cvk8vn/interview_with_the_creator_of_musenet_christine/,0,1566800601.0,"Following are links to an Interview with Christine Mcleavy Payne all about MuseNet, Deep Learning Research, OpenAI  and MOOC(s):  

&#x200B;

We also discuss her journey of starting with the courses by [Andrew Ng](https://twitter.com/AndrewYNg) and [fastdotai](https://twitter.com/fastdotai) to transitioning into Research.    


Audio: [https://anchor.fm/chaitimedatascience/episodes/MuseNet--OpenAI-and-Deep-Learning-Research-Interview-with-Christine-Payne-e4r6hb/a-ak54g2](https://anchor.fm/chaitimedatascience/episodes/MuseNet--OpenAI-and-Deep-Learning-Research-Interview-with-Christine-Payne-e4r6hb/a-ak54g2)

Video: [https://www.youtube.com/watch?v=LSEZXPvEV24&list=PLyMom0n-MBrrGL8w-nD9kc2q5dOwN7Hb1&index=13](https://www.youtube.com/watch?v=LSEZXPvEV24&list=PLyMom0n-MBrrGL8w-nD9kc2q5dOwN7Hb1&index=13)"
88,deeplearning,openai,top,2022-04-12 13:21:41,DALL.E 2 by OpenAI is giving very mind blowing results for image generation from text,imapurplemango,False,0.62,3,u1yzgi,https://www.qblocks.cloud/blog/openai-dall-e-2-generate-images-from-text,0,1649769701.0,
89,deeplearning,openai,top,2021-06-17 19:51:07,[R] Improving Language Model Behavior by Training on a Small Curated Dataset,ClaudeCoulombe,False,0.8,3,o262ql,https://www.reddit.com/r/deeplearning/comments/o262ql/r_improving_language_model_behavior_by_training/,0,1623959467.0,"Interesting research results by [OpenAI](https://openai.com/blog/improving-language-model-behavior/). It seems possible to improve the behavior of  a  GPT-3 language model  by fine tuning it  on a very small dataset. Of course, we are talking about undesirable biases (hateful, agressive, racist, sexist, etc.). They only used 80 texts. On the other hand, they neglect to say that someone can very well adjust the generated texts to favor biased texts with again a very small corpus. The [scientific paper](https://cdn.openai.com/palms.pdf) (PDF)."
90,deeplearning,openai,top,2023-12-18 08:09:14,WhisperS2T: An Optimized Speech-to-Text Pipeline for the Whisper Model,Financial-Beach1587,False,1.0,3,18l3mo6,https://i.redd.it/u1kvmui7h07c1.jpeg,0,1702886954.0,
91,deeplearning,openai,top,2021-01-25 19:31:24,OpenAI's DALL-E Alternatives with Colab Code - Deep Daze & Big Sleep [OG],dulldata,False,1.0,3,l4vfru,https://youtu.be/lVR5kN7SjQ8,0,1611603084.0,
92,deeplearning,openai,top,2021-12-20 16:17:11,[R] OpenAI’s WebGPT Crawls a Text-Based Web Environment to Achieve Human-Level Performance on Long-Form QA,Yuqing7,False,0.8,3,rkqv80,https://www.reddit.com/r/deeplearning/comments/rkqv80/r_openais_webgpt_crawls_a_textbased_web/,0,1640017031.0,"An OpenAI research team fine-tunes the GPT-3 pretrained language model to enable it to answer long-form questions by searching and navigating a text-based web browsing environment, achieving retrieval and synthesis improvements and reaching human-level long-form question-answering performance. 

Here is a quick read:[OpenAI’s WebGPT Crawls a Text-Based Web Environment to Achieve Human-Level Performance on Long-Form QA.](https://syncedreview.com/2021/12/20/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-169/)

The paper *WebGPT: Browser-assisted Question-answering with Human Feedback* is on [OpenAI.com](https://openai.com/blog/improving-factual-accuracy/)."
93,deeplearning,openai,top,2023-11-10 13:55:32,"Order in which OpenAI ""short courses"" should be taken",KA_IL_AS,False,0.75,4,17s4irx,https://www.reddit.com/r/deeplearning/comments/17s4irx/order_in_which_openai_short_courses_should_be/,2,1699624532.0,"As you all know OpenAI has released a whole lot of  ""Short Courses"" lately and they're good too. I've taken their prompt engineering course months ago when it was released, it was super helpful.  
But here's the thing they've released a lot of courses after that, and now I don't know in what order I should be taking them.  
Any thoughts and advices on this ? It'll be super helpful"
94,deeplearning,openai,top,2017-10-22 12:34:28,Why does proximal policy optimization(PPO) not need a replay buffer?,Data-Daddy,False,1.0,3,77zy79,https://www.reddit.com/r/deeplearning/comments/77zy79/why_does_proximal_policy_optimizationppo_not_need/,1,1508675668.0,"How come PPO does not use a replay buffer? I never saw them explicitly address this in the paper, but the openai blogpost does(https://blog.openai.com/openai-baselines-ppo/)"
95,deeplearning,openai,top,2021-06-19 09:55:10,Use OpenAI's CLIP for interesting usecases,adeshgautam,False,1.0,3,o3cjlm,https://www.reddit.com/r/deeplearning/comments/o3cjlm/use_openais_clip_for_interesting_usecases/,0,1624096510.0,"Check out my article if you find it interesting

  
[https://adeshg7.medium.com/build-your-own-search-engine-using-openais-clip-and-fastapi-part-1-89995aefbcdd](https://adeshg7.medium.com/build-your-own-search-engine-using-openais-clip-and-fastapi-part-1-89995aefbcdd)"
96,deeplearning,openai,top,2023-07-10 17:50:18,HNSW-FINGER for faster vector search explained!,CShorten,False,1.0,3,14w0zbn,https://www.reddit.com/r/deeplearning/comments/14w0zbn/hnswfinger_for_faster_vector_search_explained/,0,1689011418.0,"Hey everyone! I am super excited to share a new paper summary video of the HNSW-FINGER algorithm!

FINGER is short for ""Fast Inference for Graph-based Approximate Nearest Neighbor Search"". The authors first highlight that most of the distance calculations in HNSW traversal don't contribute anything to the final search results --- Thus, we get away with approximation error.

So here is the super interesting thing -- how do we approximate vector distances?

The authors present a really interesting decomposition of L2 distance into projected and residual components of the query and a neighbor to be explored ONTO the center node. More details are in the paper summary video, this is honestly a tough one to distill in a short text blurb!

I hope you find this interesting, the biggest benefit will be found with higher dimensional vectors (in the paper the authors test up to GIST 960-d vectors, but for the sake of reference the OpenAI ada-002 embeddings are 1536-d, which I think is an inspiring detail for interest in this algorithm)!

[https://www.youtube.com/watch?v=OsxZG2XfcZA](https://www.youtube.com/watch?v=OsxZG2XfcZA)

&#x200B;"
97,deeplearning,openai,top,2021-01-05 22:42:51,"[N] This Time, OpenAI’s GPT-3 Generates Images From Text",Yuqing7,False,0.63,2,kr9sxx,https://www.reddit.com/r/deeplearning/comments/kr9sxx/n_this_time_openais_gpt3_generates_images_from/,0,1609886571.0,"OpenAI’s popular GPT-3 from last year showed that language can be used to instruct a large neural network to perform a variety of text generation tasks. Entering the new year, OpenAI is moving from pure text generation to image generation from text — its researchers today announce that they have trained a neural network called [DALL·E](https://openai.com/blog/dall-e/) that creates images from text captions for a wide range of concepts expressible in natural language.

Here is a quick read: [This Time, OpenAI’s GPT-3 Generates Images From Text](https://syncedreview.com/2021/01/05/this-time-openais-gpt-3-generates-images-from-text/)"
98,deeplearning,openai,top,2020-09-09 19:51:13,[R] New Multitask Benchmark Suggests Even the Best Language Models Don’t Have a Clue What They’re Doing,Yuqing7,False,1.0,3,ipnoh4,https://www.reddit.com/r/deeplearning/comments/ipnoh4/r_new_multitask_benchmark_suggests_even_the_best/,1,1599681073.0,"The recently published paper, *Measuring Massive Multitask Language Understanding,* introduces a test covering topics such as elementary mathematics, US history, computer science, law, etc., designed to measure language models’ multitask accuracy. The authors, from UC Berkeley, Columbia University, UChicago, and UIUC, conclude that even the top-tier 175-billion-parameter OpenAI GPT-3 language model is a bit daft when it comes to language understanding, especially when encountering topics in greater breadth and depth than explored by previous benchmarks.

Here is a quick read: [New Multitask Benchmark Suggests Even the Best Language Models Don’t Have a Clue What They’re Doing](https://syncedreview.com/2020/09/09/new-multitask-benchmark-suggests-even-the-best-language-models-dont-have-a-clue-what-theyre-doing/)

The paper *Measuring Massive Multitask Language Understanding* is on [arXiv](https://arxiv.org/pdf/2009.03300.pdf)."
99,deeplearning,openai,top,2019-04-30 11:32:10,"[D] How to Build OpenAI's GPT-2: ""The AI That's Too Dangerous to Release""",pirate7777777,False,0.8,3,bj27nk,https://www.reddit.com/r/MachineLearning/comments/bj0dsa/d_how_to_build_openais_gpt2_the_ai_thats_too/,0,1556623930.0,
100,deeplearning,openai,comments,2023-11-23 12:13:26,OpenAI Q* Rumours,MIKOLAJslippers,False,0.61,6,181zwb8,https://www.reddit.com/r/deeplearning/comments/181zwb8/openai_q_rumours/,30,1700741606.0,Anyone know any juicy rumours about the capabilities of this internal Q* project at OpenAI that has supposedly catalysed some of the recent dramatics?
101,deeplearning,openai,comments,2023-12-16 15:22:29,Is there any alternative for OpenAI API?,CrazyProgramm,False,0.9,25,18jtffj,https://www.reddit.com/r/deeplearning/comments/18jtffj/is_there_any_alternative_for_openai_api/,24,1702740149.0, So I am from Sri Lanka and our university is going to organize a competition and we need OpenAI API for it but we don't have money to afford it. Is there any alternative API you guys know 
102,deeplearning,openai,comments,2023-04-05 01:36:40,Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT,Time_Key8052,False,0.95,88,12c43uu,https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/,19,1680658600.0,"Vicuna : ChatGPT Alternative, Open-Source, High Quality and Low Cost 

&#x200B;

[ Relative Response Quality Assessed by GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a072b91)

Vicuna-13B has demonstrated competitive performance against other open-source models, such as Stanford Alpaca, by fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.

Evaluation using GPT-4 as a judge shows that Vicuna-13B achieves more than 90% of the quality of OpenAI ChatGPT and Google Bard AI, while outperforming other models such as Meta LLaMA (Large Language Model Meta AI) and Stanford Alpaca in more than 90% of cases.

The cost of training Vicuna-13B is approximately $300.

The training and serving code, along with an online demo, are publicly available for non-commercial use.

&#x200B;

More Information : [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT](https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)

Discord Server : [https://discord.gg/h6kCZb72G7](https://discord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysorg](https://twitter.com/lmsysorg)"
103,deeplearning,openai,comments,2023-12-24 18:24:47,Q*,sunnymorgue,False,0.17,0,18q0qc1,https://www.reddit.com/r/deeplearning/comments/18q0qc1/q/,17,1703442287.0,"Got a question, a lot of people on this board dumb down OpenAI's Q\* to just reinforcement learning; if that's the case then why did Sam get outed of OpenAI temporarily? If it were just RL then there would be no reason to fire him over some insignificant thing. Making premature remarks like that is pretty crazy if you ask me, just saying. "
104,deeplearning,openai,comments,2023-01-27 10:45:48,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.95,120,10mhyek,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
105,deeplearning,openai,comments,2019-05-15 23:29:08,Where to learn neural network architecture design,DongDilly,False,1.0,31,bp5931,https://www.reddit.com/r/deeplearning/comments/bp5931/where_to_learn_neural_network_architecture_design/,12,1557962948.0,"So people at Deep Mind and OpenAi comes up with great models that are really innovatively designed. Can someone please tell me how and where I can learn the skill to design a neural network for a specific problem.
 Thank you"
106,deeplearning,openai,comments,2021-02-18 14:52:00,The world's largest scale Turing Test / Do you think OpenAI's GPT3 is good enough to pass the Turing Test?,theaicore,False,0.91,33,lmog2d,https://www.theaicore.com/imitationgame?utm_source=reddit,11,1613659920.0,
107,deeplearning,openai,comments,2023-01-19 07:55:49,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.9,71,10fw22o,https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/,11,1674114949.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=webp&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
108,deeplearning,openai,comments,2024-02-13 14:38:25,"This is my foundation block or sandwich stack AI model... Criticisms and inputs are welcome, formulated by openAI scholarGPT - thanks in advance.",AskACapperDOTcom,False,0.25,0,1apuyv4,/r/OpenAI/comments/1apf8jy/this_is_my_foundation_block_or_sandwich_stack_ai/,10,1707835105.0,
109,deeplearning,openai,comments,2023-04-02 18:10:37,Should we draw inspiration from Deep learning/Computer vision world for fine-tuning LLMs?,Vegetable-Skill-9700,False,0.74,10,129t3tl,https://www.reddit.com/r/deeplearning/comments/129t3tl/should_we_draw_inspiration_from_deep/,8,1680459037.0,"With HuggingGPT, BloombergGPT, and OpenAI's chatGPT store, it looks like the world is moving towards specialized GPTs for specialized tasks. What do you think are the best tips & tricks when it comes to fine-tuning and refining these task-specific GPTs?

Over the last decade, I have built many computer vision models (for human pose estimation, action classification, etc.), and our general approach was always based on Transfer learning. Take a state-of-the-art public model and fine-tune it by collecting data for the given use case.

Do you think that paradigm still holds true for LLMs?

Based on my experience, I believe observing the model's performance as it interacts with real-world data, identifying failure cases (where the model's outputs are wrong), and using them to create a high-quality retraining dataset will be the key.

&#x200B;

P.S. I am building an open-source project UpTrain ([https://github.com/uptrain-ai/uptrain](https://github.com/uptrain-ai/uptrain)), which helps data scientists to do so. We just wrote a blog on how this principle can be applied to fine-tune an LLM for a conversation summarization task. Check it out here: [https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation\_summarization](https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation_summarization)"
110,deeplearning,openai,comments,2017-11-11 17:08:56,Setting mean and std of REWARDS in reinforcement learning - a question,closedloopy,False,0.84,4,7c9k9y,https://www.reddit.com/r/deeplearning/comments/7c9k9y/setting_mean_and_std_of_rewards_in_reinforcement/,8,1510420136.0,"In the great post [pong to pixels](http://karpathy.github.io/2016/05/31/rl/) by Karpathy, and more explicitly in his code [here](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5#file-pg-pong-py-L108) we see that he sets the mean of the rewards to 0 and the standard deviation to 1. This confuses me because that means that half of the rewards will be greater than zero, and the other less than zero. Now, lets assume this array of rewards came from an episode that we liked (good performance) then we'd want to reinforce the behavior. But as far as I can tell half of the actions will be associated with positive reward (and thus encouraged) and the other half with negative reward (and thus discourage). 


*Can anyone help me get a better intuition about why he does this? An example by Pytorch follows:*


PyTorch has in [their demo](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py) a solution of solving the [cart pole](https://gym.openai.com/envs/CartPole-v0/) from open ai gym, and the solution does the same thing in terms of modifying the rewards:

First we have the raw rewards (all ones. The longer the pole stays balanced, the more reward we get):

    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
      1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
      1.,  1.,  1.,  1.,  1.]

Then we [discount](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L62) them:

    [26.76966303456023, 26.02996266117195, 25.282790566840355, 24.528071279636723, 23.76572856528962, 
    22.995685419484467, 22.21786406008532, 21.4321859192781, 20.638571635634445, 19.8369410460954, 
    19.027213177874142, 18.209306240276913, 17.383137616441328, 16.54862385499124, 
    15.705680661607312, 14.854222890512437, 13.994164535871148, 13.12541872310217, 
    12.247897700103202, 11.361512828387072, 10.466174574128356, 9.561792499119552, 8.64827525163591, 
    7.72553055720799, 6.793465209301, 5.8519850599, 4.90099501, 3.9403989999999998, 2.9701, 1.99, 1.0]

Then finally we apply the [mean of 0 and std of 1](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L65):

     [1.6023, 1.5001, 1.3965, 1.2929, 1.1897,
     1.0875, 0.9859, 0.8842, 0.7819, 0.6796,
     0.5768, 0.4728, 0.3672, 0.2595, 0.1500,
     0.0383, -0.0761, -0.1939, -0.3137, -0.4315,
    -0.5477, -0.6630, -0.7778, -0.8916, -1.0049
    -1.1172, -1.2289, -1.3396, -1.4498, -1.5600, -1.6695]

To be clear, I've seen [this post](http://cs231n.github.io/neural-networks-2/) where we learn about data pre-processing and the value of updating mean and std of INPUT, but that is on the INPUT DATA, not the REWARDS. My question is why we would apply this transformation to the REWARDS.

Thank you!"
111,deeplearning,openai,comments,2023-05-17 02:26:44,OpenAI CEO asking for government's license for building AI . WHAT THE ACTUAL FUCK?,Angry_Grandpa_,False,0.83,24,13joq2b,/r/singularity/comments/13jbc76/openai_ceo_asking_for_governments_license_for/,8,1684290404.0,
112,deeplearning,openai,comments,2022-12-07 00:33:41,Are currently state of art model for logical/common-sense reasoning all based on NLP(LLM)?,Accomplished-Bill-45,False,0.97,22,zen8l4,https://www.reddit.com/r/deeplearning/comments/zen8l4/are_currently_state_of_art_model_for/,6,1670373221.0,"Not very familiar with NLP, but I'm playing around with OpenAI's ChatGPT; particularly impressed by its reasoning, and its thought-process.

Are all good reasoning models derived from NLP (LLM) models with RL training method at the moment?

What are some papers/research team to read/follow to understand this area better and stay on updated?

&#x200B;

&#x200B;

for ChatGPT. I've tested it with following cases

Social reasoning ( which does a good job; such as: if I'm going to attend meeting tonight. I have a suit, but its dirty and size doesn't fit. another option is just wear underwear, the underwear is clean and fit in size. Which one should I wear to attend the meeting. )

Psychological reasoning ( it did a bad job.I asked it to infer someone's intention given his behaviours, expression, talks etc.)

Solving math question ( it’s ok, better then Minerva)

Asking LSAT logic game questions ( it gives its thought process, but failed to give correct answers)

I also wrote up a short mystery novel, ( like 200 words, with context) ask if it can tell is the victim is murdered or committed suicide; if its murdered, does victim knows the killer etc. It actually did ok job on this one if the context is clearly given that everyone can deduce some conclusion using common sense."
113,deeplearning,openai,comments,2022-10-18 16:52:15,Fully automated video generation - connecting OpenAI's Whisper with Stable Diffusion. Tutorial & code coming soon!,hayAbhay,False,0.93,11,y7cbf4,https://youtu.be/xRcoeUgD4GY,8,1666111935.0,
114,deeplearning,openai,comments,2023-11-16 08:28:46,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,0.84,70,17whz6e,https://v.redd.it/qx68wbuf7o0c1,7,1700123326.0,
115,deeplearning,openai,comments,2023-02-20 21:27:58,Fine tuning a GPT for text generation,nashcaps2724,False,0.91,9,117l2vf,https://www.reddit.com/r/deeplearning/comments/117l2vf/fine_tuning_a_gpt_for_text_generation/,6,1676928478.0,"Hi all, let me lay out my problem…

Imagine there are two corpora, Corpus A (100,000~) and Corpus B (20,000,000~). 

Individuals create reports for corpus A based on the information in corpus B. 

My idea was to pretrain a GPT on corpus A, and fine tune it to take documents from corpus B as an input, and output text in the style of corpus A (essentially a mix of text generation and summarization). 

Is this something folks think is even feasible? Should I be pretaining the GPT on both corpora or just corpus A? I thought of both fine tuning an OpenAI GPT and training from scratch. 

Any advice would be welcome!"
116,deeplearning,openai,comments,2020-08-05 10:58:06,image-GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model,OnlyProggingForFun,False,0.94,95,i437pt,https://www.youtube.com/watch?v=FwXQ568_io0,6,1596625086.0,
117,deeplearning,openai,comments,2021-07-06 16:56:22,What OpenAI and GitHub’s “AI pair programmer” means for the software industry,bendee983,False,0.93,30,oez127,https://bdtechtalks.com/2021/07/05/openai-github-gpt-3-copilot/,6,1625590582.0,
118,deeplearning,openai,comments,2021-12-29 08:03:22,I wrote a program with OpenAI's Codex that fixes errors,tomd_96,False,0.94,97,rr2wme,https://v.redd.it/jupdtry6vf881,6,1640765002.0,
119,deeplearning,openai,comments,2021-07-28 17:45:57,"OpenAI Releases Triton, An Open-Source Python-Like GPU Programming Language For Neural Networks",techsucker,False,0.95,68,otf0fs,https://www.reddit.com/r/deeplearning/comments/otf0fs/openai_releases_triton_an_opensource_pythonlike/,5,1627494357.0,"OpenAI released their newest language, [Triton](https://github.com/openai/triton). This open-source programming language that enables researchers to write highly efficient GPU code for AI workloads is Python-compatible and comes with the ability of a user to write in as few as 25 lines, something on par with what an expert could achieve. OpenAI claims this makes it possible to reach peak hardware performance without much effort, making creating more complex workflows easier than ever before!

Researchers in the field of Deep Learning often rely on native framework operators. However, this can be problematic because it requires many temporary tensors to work, which may hurt performance at scale for neural networks. Writing specialized GPU kernels is a more convenient solution, but surprisingly difficult due to intricacies when programming them according to GPUs. It was challenging to find a system that provides the flexibility and speed required while also being easy enough for developers to understand. This has led researchers at OpenAI in improving Triton, which was initially founded by one of their teammates.

Quick Read: [https://www.marktechpost.com/2021/07/28/openai-releases-triton-an-open-source-python-like-gpu-programming-language-for-neural-networks/](https://www.marktechpost.com/2021/07/28/openai-releases-triton-an-open-source-python-like-gpu-programming-language-for-neural-networks/) 

Paper: http://www.eecs.harvard.edu/\~htk/publication/2019-mapl-tillet-kung-cox.pdf

Github: https://github.com/openai/triton"
120,deeplearning,openai,comments,2023-01-20 08:53:58,Gotcha,actual_rocketman,False,0.94,64,10gs1ik,https://i.redd.it/yyh41pnje7da1.jpg,5,1674204838.0,
121,deeplearning,openai,comments,2018-11-14 15:56:16,OpenAI Founder: Short-Term AGI Is a Serious Possibility,gwen0927,False,0.91,26,9x1aqi,https://medium.com/syncedreview/openai-founder-short-term-agi-is-a-serious-possibility-368424f7462f,5,1542210976.0,
122,deeplearning,openai,comments,2024-01-13 04:24:32,"Idea / Proposal for someone smarter than I am. Devs/Coders, please hear me out.",DriestBum,False,0.2,0,195ff66,https://www.reddit.com/r/deeplearning/comments/195ff66/idea_proposal_for_someone_smarter_than_i_am/,5,1705119872.0,"Disclaimer: you're probably more knowledgeable than me, have more experience, and are far better at coding than me. Keep that in mind.

Here's the TLDR: create a new program (perhaps using Transformers/Tiny LLM, perhaps not needing LLMs at all) that scans, details, and analyzes the users exact hardware setup, and ultimately determines the optimal LLM/quant/settings/config for that specific hardware configuration (and perhaps use case, like general chat/role play/instruct/coding...).

Why? 

Because of the multitude of posts we see of new people trying to get into open source LLMs, but having no understanding of where they should start, what is possible on their machine, and how to configure the model for their needs. 

I was one of these people, and only after long periods of reading guides/tutorials/wikis/model cards/etc was I finally able to get a working model on my machine with decent speed and quality. For a person who is tech minded, but not a coder, and not familiar with anything other than straight forward download .exe and go, I had to start at square 1 and figure out GitHub, Linux, and all the backend llamma.ccp and whatnot. It took forever, I'm just a regular tech consumer, I don't know how to build shit. Mind you, it was a valuable experience, but I guarentee many people would have given up without pressing on and figuring it all out.

If we want greater adoption in the open source space, an easier on boarding process would be a God send for people like me. I would have paid for it! Easily! I put out offers for people to create a docker container so I could just click ""run"". The offers to build it were way out of my budget, so I was forced to grind it out and stumble my way through the steep learning curve. 

I'm not a unique case. I'm someone who has used ChatGPT, played with Spaces on Hugging Face, and really wanted a local LLM to use with sensitive local data that I couldn't trust OpenAI or anyone with. I understood the value, but I didn't have the ability to spin up a model without massive amounts of homework. 

So anyway, that's the idea. A stand alone program, or utility, that analyzes a user's hardware capability, suggests appropriate specs/config for a model, and gives a bullet point list of what to do to get that specific model working in a numbered list of steps. 

I would have paid $100 for that shortcut. Easily. I almost paid 20x that for a container of an old model. Just to get it working. 

To reiterate, the problem is:

New people asking ""what do I need to run x?"" or ""can my x machine run this model?"" Or ""what's the beat coding model I can use with x machine"". 

Solution:

A utility that's only purpose is to examine the user's hardware and recommend optimal models, quants, and settings profile (for webui / lm studio...) 

Just an idea. 

Please DM if anyone is interested in a collaboration. I'm a corporate finance person, with a marketing degree/background, I'm not a developer. I can't build it, but I could hype it, and potentially sell it. I know the target market, and it's large and narrow enough to be worthwhile. 

Thanks for coming to my lunch and learn presentation. And I'm totally aware of how I pitched a close source business idea to an open source sub, I know, but the people who could/want to do this are here. I'm not going to lose sleep over offending a commie or two."
123,deeplearning,openai,comments,2024-01-16 19:44:53,How to make my text digital clone?,Zestyclose_Memory535,False,0.5,0,198bwl5,https://www.reddit.com/r/deeplearning/comments/198bwl5/how_to_make_my_text_digital_clone/,5,1705434293.0,"Google gave me extremely dubious results, so I’m writing here.

For a long time, I’ve had the idea in my head of taking all our correspondence with a friend, 100,000 messages from Telegram, and further training a ready-made model using this. A **small model** was planned, because she should not know about everything in the world, *like me*.

Of course, I understand that the data will have to be sorted and structured, and what is equally important, the model must **speak like me** in my language. In this case, the russian language.

I **don’t have** a lot of computing power for training, the maximum is free Kaggle (analogous to the nasty Google Colab), as well as my local video cards of GTX 8 GB and RTX 16 GB.

*Ideally*, I would like the model to simulate my communication style, my slang and my cursed emoji. It would be interesting to communicate with myself, albeit in a reduced form, and also to leave my clone in case I leave.

Advise the vector **where I should move**, I can figure it out myself. It seems to me that I can’t handle models that weigh 4 gigabytes, and I need a smaller model. I already have experience with Dreambooth, OpenAI API, local LLMs, I know how to program, I understand computer technology, I have initial experience in developing neural networks, and gym openai. So I'm not too afraid of complex things and terms."
124,deeplearning,openai,comments,2023-11-23 15:47:45,A deeper look at the Q* Model as a combination of A* algorithms and Deep Q-learning networks.,Ok-Judgment-1181,False,0.58,6,18240yl,https://www.reddit.com/r/deeplearning/comments/18240yl/a_deeper_look_at_the_q_model_as_a_combination_of/,5,1700754465.0,"Hey, folks! Buckle up because the recent buzz in the AI sphere has been nothing short of an intense rollercoaster. Rumors about a groundbreaking AI, enigmatically named Q\* (pronounced Q-Star), have been making waves, closely tied to a chaotic series of events that rocked OpenAI and came to light after the [abrupt firing of their CEO](https://edition.cnn.com/2023/11/17/tech/sam-altman-departs-open-ai/index.html) \- Sam Altman ( [u/samaltman](https://www.reddit.com/u/samaltman/) **)**.

There are several questions I would like to entertain, such as the impacts of Sam Altman's firing, the most probable reasons behind it, and the possible monopoly on highly efficient AI technologies that Microsoft is striving to have. However, all these things are too much for 1 Reddit post, so here **I will attempt to explain why Q\* is a BIG DEAL, as well as go more in-depth on the theory of combining Q-learning and A\* algorithms**.

At the core of this whirlwind is an AI (Q\*) that aces grade-school math but does so without relying on external aids like Wolfram. It may possibly be a paradigm-shattering breakthrough, transcending AI stereotypes of information repeaters and stochastic parrots which showcases iterative learning, intricate logic, and highly effective long-term strategizing.

This milestone isn't just about numbers; it's about unlocking an AI's capacity to navigate the single-answer world of mathematics, potentially revolutionizing reasoning across scientific research realms, and breaking barriers previously thought insurmountable.

What are A\* algorithms and Q-learning?:

From both the name and rumored capabilities, the Q\* is very likely to be an AI agent that combines A\* Algorithms for planning and Q-learning for action optimization. Let me explain.

[A\* algorithms](https://theory.stanford.edu/~amitp/GameProgramming/AStarComparison.html) serve as powerful tools for finding the shortest path between two points in a graph or a map while efficiently navigating obstacles. Their primary purpose lies in optimizing route planning in scenarios where finding the most efficient path is crucial. These algorithms are known to balance accuracy and efficiency with the notable capabilities being: Shortest Path Finding, Adaptability to Obstacles, and their computational Efficiency / Optimality (heuristic estimations).

However, applying A\* algorithms to a chatbot AI involves leveraging its pathfinding capabilities in a rather different context. While chatbots typically don’t navigate physical spaces, **they do traverse complex information landscapes to find the most relevant responses or solutions to user queries**. Hope you see where I´m going with this, but just in case let's talk about Q-learning for a bit.

Connecting the dots even further, let's think of [Q-learning](https://builtin.com/artificial-intelligence/deep-q-learning) as us giving the AI a constantly expanding cheat sheet, helping it decide the best actions based on past experiences. However, in complex scenarios with vast states and actions, maintaining a mammoth cheat sheet becomes unwieldy and hinders our progress toward AGI due to elevated compute requirements. Deep Q-learning steps in, utilizing neural networks to approximate the Q-value function rather than storing it outright.

Instead of a colossal Q-table, the network maps input states to action-Q-value pairs. It's like having a compact cheat sheet tailored to navigate complex scenarios efficiently, giving AI agents the ability to pick actions based on the [Epsilon-Greedy approach](https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/)—sometimes randomly exploring, sometimes relying on the best-known actions predicted by the networks. Normally DQNs (or [Deep Q-networks](https://www.tensorflow.org/agents/tutorials/0_intro_rl)), use two neural networks—the main and target networks—sharing the same architecture but differing in weights. Periodically, their weights synchronize, enhancing learning and stabilizing the process, this last point is highly important to understand as it may become the key to a model being capable of **self-improvement** which is quite a tall feat to achieve. This point however is driven further if we consider the [Bellman equation](https://www.geeksforgeeks.org/bellman-equation/), which basically states that with each action, the networks update weights using the equation utilizing Experience replay—a sampling and training technique based on past actions— which helps the AI learn in small batches **without necessitating training after every step**.

*I must also mention that Q\*'s potential is not just a math whiz but rather* ***a gateway to scaling abstract goal navigation*** *as we do in our heads when we plan things, however, if achieved at an AI scale we would likely get highly efficient, realistic and logical plans to virtually any query or goal (highly malicious, unethical or downright savage goals included)...*

Finally, there are certain **pushbacks and challenges** to overcome with these systems which I will underline below, HOWEVER, with the recent news surrounding OpenAI, I have a feeling that smarter people have found ways of tackling these challenges efficiently enough to have a huge impact of the industry if word got out.

To better understand possible challenges I would like to give you a hypothetical example of a robot that is tasked with solving a maze, where the starting point is user queries and the endpoint is a perfectly optimized completion of said query, with the maze being the World Wide Web.

Just like a complex maze, the web can be labyrinthine, filled with myriad paths and dead ends. And although the A\* algorithm helps the model seek the shortest path, certain intricate websites or information silos can confuse the robot, leading it down convoluted pathways instead of directly to the optimal solution (problems with web crawling on certain sites).

By utilizing A\* algorithms the AI is also able to adapt to the ever-evolving landscape of the web, with content updates, new sites, and changing algorithms. However, due to the speed being shorter than the web expansion, it may fall behind as it plans based on an initial representation of the web. When new information emerges or websites alter their structures, the algorithm might fail to adjust promptly, impacting the robot's navigation.

On the other hand, let's talk about the challenges that may arise when applying Q-learning. Firstly it would be limited sample efficiency, where the robot may pivot into a fraction of the web content or stick to a specific subset of websites, it might not gather enough diverse data to make well-informed decisions across the entire breadth of the internet therefore failing to satisfy user query with utmost efficiency.

And secondly, problems may arise when tackling [high-dimensional data](https://www.statology.org/high-dimensional-data/). The web encompasses a vast array of data types, from text to multimedia, interactive elements, and more. Deep Q-learning struggles with high-dimensional data (That is data where the number of features in a dataset exceeds the number of observations, due to this fact we will never have a deterministic answer). In this case, if our robot encounters sites with complex structures or extensive multimedia content, processing all this information efficiently becomes a significant challenge.

To combat these issues and integrate these approaches one must find a balance between optimizing pathfinding efficiency while swiftly adapting to the dynamic, multifaceted nature of the Web to provide users with the most relevant and efficient solutions to their queries.

To conclude, there are plenty of rumors floating around the Q\* and Gemini models as giving AI the ability to plan is highly rewarding due to the increased capabilities however it is also quite a risky move in itself. This point is further supported by the constant reminders that we need better AI safety protocols and guardrails in place before continuing research and risking achieving our goal just for it to turn on us, but I'm sure you've already heard enough of those.So, are we teetering on the brink of a paradigm shift in AI, or are these rumors just a flash in the pan? Share your thoughts on this intricate and evolving AI saga—it's a front-row seat to the future!

I know the post came out lengthy and pretty dense, but I hope this post was helpful to you! Please do remember that this is mere speculation based on multiple news articles, research, and rumors currently speculating regarding the nature of Q\*, take the post with a grain of salt :)

**Edit:** After several requests, I would like to mention an Arxiv paper on a very similar topic I've discussed in the post:

***A\* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks*** ([https://arxiv.org/abs/2102.04518v2](https://arxiv.org/abs/2102.04518v2))

*Let us all push the veil of ignorance back and the frontier of discovery forward.*"
125,deeplearning,openai,comments,2020-04-21 23:00:56,How does the talktotransformer website work so fast?,parrot15,False,0.97,32,g5prf5,https://www.reddit.com/r/deeplearning/comments/g5prf5/how_does_the_talktotransformer_website_work_so/,5,1587510056.0,"At [https://talktotransformer.com/](https://talktotransformer.com/), you can type a prompt and the transformer will autogenerate the text for you using OpenAI's GPT-2 1.5 billion parameter model.

I'm not asking how GPT-2 works, I'm asking something else. When I ran the GPT-2 1.5B model on the free TPU in Google Colab, it took around 20 to 40ish seconds to generate around the same about of text as the website generates per prompt.

And yet, the website is somehow generating text from the prompt almost instantaneously using the 1.5B model. This is on top of all the X number of people who must be using the website at the same time I am, so it is doing text generation concurrently and near-instantaneously using a gigantic model.

I am very confused. Did the creator of the website just use a lot more TPUs/GPUs behind the scenes and is letting them run 24/7 (I don't think so because that would cost a shit ton of money), or am I missing something fundamental here?

This same question can be applied to this website too: [https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is](https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is)

Please keep in mind that I'm relatively new to all of this. Thanks in advance!"
126,deeplearning,openai,comments,2019-02-14 17:19:15,OpenAI Guards Its ML Model Code & Data to Thwart Malicious Usage,gwen0927,False,0.96,22,aqm35j,https://medium.com/syncedreview/openai-guards-its-ml-model-code-data-to-thwart-malicious-usage-d9f7e9c43cd0,4,1550164755.0,
127,deeplearning,openai,comments,2017-07-21 13:22:52,Very hard to work around continuous latent code in InfoGAN,kudo1026,False,0.75,2,6onvaq,https://www.reddit.com/r/deeplearning/comments/6onvaq/very_hard_to_work_around_continuous_latent_code/,4,1500643372.0,"It is always a pleasure to work with InfoGAN as it can unsupervisedly decode the structure of the data. The discrete latent worked very well for me to categorize different trajectories. However, I've been a little frustrated recently as I'm trying to implement the continuous latent code.

I first tried the mnist example given by the original InfoGAN paper, and it worked fine as the first continuous latent code represents the leaning angle and the second continuous latent represents the width. However, when I tried it on my own toy data sets,(something like
[first][1], [second][2]) nothing really works out. I expect to see continuous variation on my output, but they are instead completely random, like the latent code doesn't work at all.

I use convolutional and deconvolutional networks to discriminate and generate samples and follow the same manner for the discrete latent code as in the original openAI implementation. My treatment towards the continuous latent code is a little different, I have my discriminating network output continuous output with the same dimension as the input continuous latent code, and then try to minimize the MSE of the two. On the contrary, openaAI was modelling the code as a Gaussian distribution, and calculate its distance based on the distribution property.


    loss_c_cont = tf.reduce_mean(tf.reduce_sum(tf.square(c_cont_fake - c_cont), 1))


I think about it for a while, but don't think it could cause a huge difference as the gaussian distribution distance also mainly takes into account the square of epsilon = (x_var - mean).

    return tf.reduce_sum(
            - 0.5 * np.log(2 * np.pi) - tf.log(stddev + TINY) - 0.5 * tf.square(epsilon),
            reduction_indices=1,
        )

So my question would be: is this really the problem that caused all the problems? Or is there anything I need to pay more attention to but I did not notice?

Or in a more general sense, is there anything special about what kind of characteristics of the data can be represented by the continuous latent code? Is there any limitations to the representation capability of the continuous latent code(translation or rotation)? Can anyone share their experience about what kind of features they represented using the continuous latent code except for the examples given by the openAI paper?

I'd really welcome all your thoughts! Thanks!

P.S. Another thing I noticed was when I try to use a 5-category discrete code(in one-hot encoding) and 1-dim continuous code, it will take 5 digit for the discrete code and only 1 digit for the continuous code in the feeding input noise to the generator. Is it somehow unbalanced? Each of these code actually just represents one characteristic of the dataset. In mnist dataset, it could either number, width or rotation angle. Can this be a problem?


  [1]: https://i.stack.imgur.com/YXAMm.jpg
  [2]: https://i.stack.imgur.com/WNRTM.jpg
"
128,deeplearning,openai,comments,2023-12-20 21:36:11,[Blogpost] Top Python Libraries of 2023,No_Dig_7017,False,0.88,13,18n5wzb,https://www.reddit.com/r/deeplearning/comments/18n5wzb/blogpost_top_python_libraries_of_2023/,4,1703108171.0,"Hello Python Community!

We're thrilled to present our 9th edition of the **Top Python Libraries and tools**, where we've scoured the Python ecosystem for the most innovative and impactful developments of the year.

This year, it’s been the boom of Generative AI and Large Language Models (LLMs) which have influenced our picks. Our team has meticulously reviewed and categorized over 100 libraries, ensuring we highlight both the mainstream and the hidden gems.

**Explore the entire list with in-depth descriptions here**: [](https://tryolabs.com/blog/top-python-libraries-2023)

Here’s a glimpse of our top 10 picks:

1. [LiteLLM](https://github.com/BerriAI/litellm) — Call any LLM using OpenAI format, and more.
2. [PyApp](https://github.com/ofek/pyapp) — Deploy self-contained Python applications anywhere.
3. [Taipy](https://github.com/Avaiga/taipy) — Build UIs for data apps, even in production.
4. [MLX](https://github.com/ml-explore/mlx) — Machine learning on Apple silicon with NumPy-like API.
5. [Unstructured](https://github.com/Unstructured-IO/unstructured) — The ultimate toolkit for text preprocessing.
6. [ZenML](https://github.com/zenml-io/zenml) and [AutoMLOps](https://github.com/GoogleCloudPlatform/automlops) — Portable, production-ready MLOps pipelines.
7. [WhisperX](https://github.com/m-bain/whisperX) — Speech recognition with word-level timestamps & diarization.
8. [AutoGen](https://github.com/microsoft/autogen) — LLM conversational collaborative suite.
9. [Guardrails](https://github.com/guardrails-ai/guardrails) — Babysit LLMs so they behave as intended.
10. [Temporian](https://github.com/google/temporian) — The “Pandas” built for preprocessing temporal data.

Our selection criteria prioritize innovation, robust maintenance, and the potential to spark interest across a variety of programming fields. Alongside our top picks, we've put significant effort into the long tail, showcasing a wide range of tools and libraries that are valuable to the Python community.

A huge thank you to the individuals and teams behind these libraries. Your contributions are the driving force behind the Python community's growth and innovation. 🚀🚀🚀

**What do you think of our 2023 lineup? Did we miss any library that deserves recognition?** Your feedback is vital to help us refine our selection each year.

Edit: updated the post body so the links are directly here in reddit."
129,deeplearning,openai,comments,2021-03-30 10:41:10,"this is not overfitting but something else, right?",Sewing31,False,1.0,1,mgd1up,https://www.reddit.com/r/deeplearning/comments/mgd1up/this_is_not_overfitting_but_something_else_right/,3,1617100870.0,"I trained a VAE on a dataset containing 1k images. The VAE itself is convolutional, downsamples 256x256 rgb images four times before reconstruction and uses both relu and BatchNorm Layers as well as ResNet-like Skip Connection prior and after the bottleneck. Input is normalized to unit scale and no data augmentation takes place. The modal consists in total roughly of about 5e6 parameters and the visual inspection of the reconstructed images look decent enough. This is represented for both training and validation runs by the orange curve. No overfitting occurs as far as I can tell. However, if I use the entire dataset, i.e. \~ 35k training images and 5k validation images, the same model performs like shown by the green curve. Taking into account only the lower validation plot, I had said, this is overfitting, although it would have struck me as odd, that the same model that performed decently on the 1k images dataset now overfits on a much larger dataset. But also the training loss shows a clear inflection point around epoch 15.

https://preview.redd.it/1yh10yel95q61.png?width=846&format=png&auto=webp&s=8db4867ff3c6e334d2bf91a5addd80f29b0f50f6

Can somebody tell me a likely source of error here? Is the models capacity not large enough to sufficiently capture the complexity of the data? I dont think so, since I am using a s[ota convolutional VAE used by OpenAI](https://github.com/openai/DALL-E/blob/master/dall_e/encoder.py) for a dataset comprising millions of images.

The context is, that I am trying to learn a discrete vocabulary of latent codes, i.e. have a discrete learnable embedding to sort of quantize the otherwise continuous latent outputs of the encoder that are then used to reconstruct the input image via the decoder [cf. this code snippet](https://github.com/karpathy/deep-vector-quantization/blob/main/dvq/model/quantize.py). So the idea is not to generate random sampled from noise but to learn an efficient notebook, i.e. bottleneck that captures the essentials of the data set. The decoder then outputs a prob distribution for every pixel over the 255 possible values 8 bit images can take o. The KL (assuming a uniform prior to encourage uniform use of all possible vocabulary entries) is currently weighted with 1.

Any help would be appreciated. Thank you in advance"
130,deeplearning,openai,comments,2021-07-24 12:15:43,OpenAI's New Code Generator: GitHub Copilot (and Codex) | This AI Generates Code From Words,OnlyProggingForFun,False,0.17,0,oqov6e,https://youtu.be/az3oVVkTFB8,3,1627128943.0,
131,deeplearning,openai,comments,2023-02-01 09:00:18,"Python wrapper of OpenAI's New AI classifier tool, which detects whether the paragraph was generated by ChatGPT, GPT models, or written by humans",StoicBatman,False,0.85,17,10qouv9,https://www.reddit.com/r/deeplearning/comments/10qouv9/python_wrapper_of_openais_new_ai_classifier_tool/,3,1675242018.0,"OpenAI has developed a new AI classifier tool which detects whether the content (paragraph, code, etc.) was generated by #ChatGPT, #GPT-based large language models or written by humans.

Here is a python wrapper of openai model to detect if a text is written by humans or generated by ChatGPT, GPT models

Github: [https://github.com/promptslab/openai-detector](https://github.com/promptslab/openai-detector)  
Openai release: [https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/](https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/)

If you are interested in #PromptEngineering, #LLMs, #ChatGPT and other latest research discussions, please consider joining our discord [discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)  


https://preview.redd.it/9d8ooeg2ljfa1.png?width=1358&format=png&auto=webp&s=0de7ccc5cd16d6bbad3dcfe7d8441547a6196fc8"
132,deeplearning,openai,comments,2019-10-20 21:26:53,"AI Weekly Update #9 - (October 20th, 2019)",HenryAILabs,False,0.77,9,dkq5m7,https://www.reddit.com/r/deeplearning/comments/dkq5m7/ai_weekly_update_9_october_20th_2019/,3,1571606813.0,"https://www.youtube.com/watch?v=03kCD18H5nQ

This week's update covers OpenAI's amazing robotic hand and exciting updates such as Facebook's Semi-Weakly Supervised learning framework, Google's MASSIVE (50BN params) Multilingual NMT models, and many more!"
133,deeplearning,openai,comments,2024-02-16 03:20:25,Unbelieve New Text To Video AI Model By OpenAI - 37 Demo Videos - Still Can't Believe Real - Watch All Videos 4K With A Nice Music - This Will Change Perception Of Reality Forever,CeFurkan,False,0.6,3,1aryk48,https://www.youtube.com/watch?v=VlJYmHNRQZQ&deeplearning,3,1708053625.0,
134,deeplearning,openai,comments,2023-08-15 04:46:43,OpenAI Notebooks which are really helpful.,vishank97,False,0.88,18,15rihgo,https://www.reddit.com/r/deeplearning/comments/15rihgo/openai_notebooks_which_are_really_helpful/,3,1692074803.0,"The OpenAI cookbook is one of the most underrated and underused developer resources available today. Here are 7 notebooks you should know about:

1. Improve LLM reliability:  
[https://github.com/openai/openai-cookbook/blob/main/techniques\_to\_improve\_reliability.md](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)
2. Embedding long text inputs:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Embedding\_long\_inputs.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb)
3. Dynamic masks with DALLE:  
[https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How\_to\_create\_dynamic\_masks\_with\_DALL-E\_and\_Segment\_Anything.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How_to_create_dynamic_masks_with_DALL-E_and_Segment_Anything.ipynb)
4. Function calling to find places nearby:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Function\_calling\_finding\_nearby\_places.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb)
5. Visualize embeddings in 3D:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing\_embeddings\_in\_3D.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_3D.ipynb)
6. Pre and post-processing of Whisper transcripts:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Whisper\_processing\_guide.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb)
7. Search, Retrieval, and Chat:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Question\_answering\_using\_a\_search\_API.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb)

Big thanks to the creators of these notebooks!"
135,deeplearning,openai,comments,2020-09-11 15:37:20,[R] OpenAI ‘GPT-f’ Delivers SOTA Performance in Automated Mathematical Theorem Proving,Yuqing7,False,0.93,43,iqsul7,https://www.reddit.com/r/deeplearning/comments/iqsul7/r_openai_gptf_delivers_sota_performance_in/,2,1599838640.0,"San Francisco-based AI research laboratory OpenAI has added another member to its popular GPT (Generative Pre-trained Transformer) family. In a new paper, OpenAI researchers introduce GPT-f, an automated prover and proof assistant for the Metamath formalization language.

Here is a quick read: [OpenAI ‘GPT-f’ Delivers SOTA Performance in Automated Mathematical Theorem Proving](https://syncedreview.com/2020/09/10/openai-gpt-f-delivers-sota-performance-in-automated-mathematical-theorem-proving/)

The paper *Generative Language Modeling for Automated Theorem Proving* is on [arXiv](https://arxiv.org/pdf/2009.03393.pdf)."
136,deeplearning,openai,comments,2021-01-08 16:12:03,OpenAI's CLIP method explained! (CLIP > DALL-E change my mind),gordicaleksa,False,0.69,5,kt5k3z,https://youtu.be/fQyHEXZB-nM,2,1610122323.0,
137,deeplearning,openai,comments,2021-11-01 14:33:01,"[R] Warsaw U, OpenAI and Google’s Hourglass Hierarchical Transformer Model Outperforms Transformer Baselines",Yuqing7,False,0.83,14,qkf9xu,https://www.reddit.com/r/deeplearning/comments/qkf9xu/r_warsaw_u_openai_and_googles_hourglass/,2,1635777181.0,"A team from the University of Warsaw, OpenAI and Google Research proposes Hourglass, a hierarchical transformer language model that operates on shortened sequences to alleviate transformers’ huge computation burdens. 

Here is a quick read: [Warsaw U, OpenAI and Google’s Hourglass Hierarchical Transformer Model Outperforms Transformer Baselines.](https://syncedreview.com/2021/11/01/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-135/)

The paper *Hierarchical Transformers Are More Efficient Language Models* is on [arXiv](https://arxiv.org/abs/2110.13711)."
138,deeplearning,openai,comments,2023-08-02 11:33:32,Using PDFs with GPT Models,Disastrous_Look_1745,False,0.72,9,15g6i4x,https://www.reddit.com/r/deeplearning/comments/15g6i4x/using_pdfs_with_gpt_models/,2,1690976012.0,Found a blog talking about how we can interact with PDFs in Python by using GPT API & Langchain. It talks about some pretty cool automations you can build involving PDFs - [https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/](https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/)
139,deeplearning,openai,comments,2023-03-03 23:02:37,Meta new large lanugage model (similar to OpenAI one) called LLaMA is leaked via torrent,aipaintr,False,1.0,10,11hhzeg,https://github.com/facebookresearch/llama/pull/73/files,2,1677884557.0,
140,deeplearning,openai,comments,2024-02-10 03:18:15,Building an AI that can mimic the Spectator Method by Benjamin Franklin?,kiwifreeze,False,1.0,2,1an6mfj,https://www.reddit.com/r/deeplearning/comments/1an6mfj/building_an_ai_that_can_mimic_the_spectator/,2,1707535095.0,"Hello, I'm a recent CS grad and aspiring game dev. 

 I want to get better at writing and one of the ways I've been doing this was using the [Spectator method by Benjamin Franklin](https://shanesnow.com/research/how-to-be-a-better-writer-ben-franklin) 

I have been doing this by hand recently and I've found it to be incredibly helpful for my writing chops. I do everything by hand, that is take notes on each individual sentence of whichever book and then try to rewrite after offsetting the time a bit so I've forgotten it and then compare my writing to the original to see what I'm lacking. 

Taking notes on each individual sentence has been tedious though, and I tried to get a way for ChatGPT to do this for me but it can't read PDFs and yet alone other book files (I'm guessing). It does have the capability however to make short sentiments of the meanings of each sentence in any paragraph of a book (I tested this with a public domain book like Pride and Prejudice but it stopped after awhile). 

Is it possible to write an AI to do this for me automatically instead so I don't have to take notes sentence by sentence? Like use the OpenAI api to build a personal app around, or even build an LLM (which I don't even know where I would start with tbh). 

I do have much experience with coding and have taken an Intro to AI course at my university, though I can't say how much I really paid attention. That being said, I'm willing and capable of learning. 

Any advice would be appreciated!"
141,deeplearning,openai,comments,2021-06-09 14:57:40,[R] Pieter Abbeel Team’s Decision Transformer Abstracts RL as Sequence Modelling,Yuqing7,False,0.7,5,nvxwi7,https://www.reddit.com/r/deeplearning/comments/nvxwi7/r_pieter_abbeel_teams_decision_transformer/,2,1623250660.0,"A research team from UC Berkeley, Facebook AI Research and Google Brain abstracts Reinforcement Learning (RL) as a sequence modelling problem. Their proposed Decision Transformer simply outputs optimal actions by leveraging a causally masked transformer, yet matches or exceeds state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.

Here is a quick read: [Pieter Abbeel Team’s Decision Transformer Abstracts RL as Sequence Modelling.](https://syncedreview.com/2021/06/09/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-37/)

The paper *Decision Transformer: Reinforcement Learning via Sequence Modeling* is on [arXiv](https://arxiv.org/abs/2106.01345)."
142,deeplearning,openai,comments,2016-12-07 17:26:55,"'A Deep Hierarchical Approach to Lifelong Learning in Minecraft' (AAAI-17), Tessler et al 2016",chentessler,False,0.88,6,5h16m9,https://www.reddit.com/r/deeplearning/comments/5h16m9/a_deep_hierarchical_approach_to_lifelong_learning/,2,1481131615.0,"Hi everyone, 

We would like to share with you our recent work entitled 'A Deep Hierarchical Approach to Lifelong Learning in Minecraft' (AAAI-17) ([paper](https://arxiv.org/abs/1604.07255), [website](http://chentessler.wixsite.com/hdrlnminecraft)). This work presents a lifelong deep reinforcement learning system that is able to efficiently retain as well as transfer knowledge (via reusable skills) to solve new, unseen tasks; two of the key building blocks to lifelong learning.
It is an exciting time for Deep Reinforcement Learning (DRL) research as new, complex gaming environments are being open sourced ([Minecraft - Malmo](https://www.microsoft.com/en-us/research/project/project-malmo/), [OpenAI - Universe](https://universe.openai.com/), [StarCraft - TorchCraft](https://arxiv.org/abs/1611.00625), [StarCraft 2](https://deepmind.com/blog/deepmind-and-blizzard-release-starcraft-ii-ai-research-environment/), [DeepMind Lab](https://deepmind.com/blog/open-sourcing-deepmind-lab/)
) and shared with the AI community. We strongly believe that taking advantage of hierarchy as well as efficient mechanisms to transfer and retain knowledge are soon to play significant roles in the ability of DRL agents to scale in these new exciting environments. 

Cheers"
143,deeplearning,openai,comments,2024-02-05 12:26:35,Is there any tutorial/guide on how to efficiently deploy hugging face open-source LLMs? both for testing and then in production? what is the cost I can expect compared to OpenAI API?,HappyDataGuy,False,1.0,2,1ajfaxw,https://www.reddit.com/r/deeplearning/comments/1ajfaxw/is_there_any_tutorialguide_on_how_to_efficiently/,2,1707135995.0,
144,deeplearning,openai,comments,2018-12-19 07:13:28,ground-truth model of the environment is usually not available to the agent,sriharsha_0806,False,1.0,2,a7k5pc,https://www.reddit.com/r/deeplearning/comments/a7k5pc/groundtruth_model_of_the_environment_is_usually/,2,1545203608.0,"Hi, I am reading Reinforcement Learning spiningup of OpenAI. I came across this sentence which is a con for Model-Based RL ""ground-truth model of the environment is usually not available to the agent. If an agent wants to use a model in this case, it has to learn the model purely from experience, which creates several challenges"". Can anyone help me understand this concept? 

Ref: [https://spinningup.openai.com/en/latest/spinningup/rl\_intro2.html](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html)

&#x200B;"
145,deeplearning,openai,comments,2023-11-10 13:55:32,"Order in which OpenAI ""short courses"" should be taken",KA_IL_AS,False,0.71,3,17s4irx,https://www.reddit.com/r/deeplearning/comments/17s4irx/order_in_which_openai_short_courses_should_be/,2,1699624532.0,"As you all know OpenAI has released a whole lot of  ""Short Courses"" lately and they're good too. I've taken their prompt engineering course months ago when it was released, it was super helpful.  
But here's the thing they've released a lot of courses after that, and now I don't know in what order I should be taking them.  
Any thoughts and advices on this ? It'll be super helpful"
146,deeplearning,openai,comments,2023-01-25 22:06:47,OpenAi's breakthrough,bradasm,False,0.21,0,10lb7k3,https://www.reddit.com/r/deeplearning/comments/10lb7k3/openais_breakthrough/,2,1674684407.0,[https://twitter.com/make\_mhe/status/1618255363580755968](https://twitter.com/make_mhe/status/1618255363580755968)
147,deeplearning,openai,comments,2021-10-10 02:22:26,Generate READMEs Using AI,tomd_96,False,0.97,36,q4z8wm,https://www.reddit.com/r/deeplearning/comments/q4z8wm/generate_readmes_using_ai/,2,1633832546.0,"&#x200B;

https://i.redd.it/nbnqpn0f9js71.gif

You can use this program I wrote to generate readmes: [https://github.com/tom-doerr/codex-readme](https://github.com/tom-doerr/codex-readme)

It's far from perfect, but I'm still surprised how well it works if you give it a few tries. It often generates interesting usage examples and explains the available command line options.

You probably won't use this for larger projects, but I think this can make sense for small projects or single scripts. Many small scripts are very useful but might never be published because of the work that is required to document and explain it. Using this AI might assist you with that.

Be aware that you need to get access to OpenAI's Codex API to use it.

What do you think?"
148,deeplearning,openai,comments,2023-03-07 16:20:57,"We tracked mentions of OpenAI, Bing, and Bard across social media to find out who's the most talked about in Silicon Valley",yachay_ai,False,0.33,0,11l3ofp,https://www.reddit.com/r/deeplearning/comments/11l3ofp/we_tracked_mentions_of_openai_bing_and_bard/,2,1678206057.0,"[Posts about OpenAI, Bing, and Bard in the San Francisco Bay Area and Silicon Valley](https://preview.redd.it/tliq31mjecma1.png?width=1286&format=png&auto=webp&s=c5042544103fbf453172bc7e01c3efb6ad9a9451)

Have you been following the news on the conversational AI race? We used social media data and [geolocation models](https://github.com/1712n/yachay-public/tree/master/conf_geotagging_model) to find posts about OpenAI, Bing, and Bard in the Silicon Valley and San Francisco Bay Area for the last two weeks to see which one received the most mentions.

First, we filtered social media data with the keywords ""openai,"" ""bing,"" ""bard,"" and then we predicted coordinates for the social media posts by using our text-based geolocation models. After selecting texts which received a confidence score higher than 0.8, we plotted their coordinates as company logos on a leaflet map using Python and the folium library, restricting the map to the bounding box of the San Francisco Bay Area and Silicon Valley.

We analyzed over 300 social media posts and found that roughly 54.5% of the time, OpenAI was the most talked about. Bing made second place with around 27.2%, and then Bard came in last with 18.3%.

You can check out the full map [here](https://1712n.github.io/yachay-public/maps/chatbots/).

OpenAI may be winning the AI race at the moment, but it's not the end yet. Let us know what other AI projects you're following, and we'll check them out."
149,deeplearning,openai,comments,2020-03-17 08:40:17,[P] Simple PyTorch Implementation of OpenAI GPT-1,lyeoni,False,0.85,8,fk1tog,https://www.reddit.com/r/deeplearning/comments/fk1tog/p_simple_pytorch_implementation_of_openai_gpt1/,2,1584434417.0,"Hello :)

Most  of OpenAI GPT codes are for version 2 (GPT-2), and GPT-2 is hard to use  restricted small GPU resources. So, I implemented GPT-1 for someone who  wants to pre-train/fine-tune with very small resources, like me.  Because GPT-1 requires relatively small resources.

And, I also added belows.

* training logs for pre-training on WikiText-103, fine-tuning on IMDb.
* results for the question : *Does auxiliary objective function have a bigger impact?*

I hope that this repo can be a good solution for people who want to train GPT model with restricted resources.

\[LINK\] : [https://github.com/lyeoni/gpt-pytorch](https://github.com/lyeoni/gpt-pytorch)"
150,deeplearning,openai,comments,2020-10-25 19:26:17,Google AI Introduces Performer: A Generalized Attention Framework based on the Transformer architecture,ai-lover,False,0.9,43,jhzd4q,https://www.reddit.com/r/deeplearning/comments/jhzd4q/google_ai_introduces_performer_a_generalized/,2,1603653977.0,"[Transformer model](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html), a deep learning framework, has achieved state-of-the-art results across diverse domains, including [natural language](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html), [conversation](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html), [images](https://openai.com/blog/image-gpt/), and even [music](https://magenta.tensorflow.org/music-transformer). The core block of any Transformer architecture is the [attention module](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html), which computes similarity scores for all pairs of positions in an input sequence. Since it requires quadratic computation time and quadratic memory size of the storing matrix, with the increase in the input sequence’s length, its efficiency decreases.

Thus, for long-range attention, one of the most common methods is [sparse attention](https://openai.com/blog/sparse-transformer/). It reduces the complexity by computing selective similarity scores from the sequence, based on various methods. There are still certain limitations like unavailability of efficient sparse-matrix multiplication operations on all accelerators, lack of theoretical guarantees, insufficiency to address the full range of problems, etc.

**Introduction to “Performer”**

To resolve these issues, Google AI introduces the [Performer](https://arxiv.org/abs/2009.14794), a Transformer architecture with attention mechanisms that scale linearly. The framework is implemented by [Fast Attention Via Positive Orthogonal Random Features (FAVOR+) algorithm](https://github.com/google-research/google-research/tree/master/performer/fast_self_attention), providing scalable low-variance and unbiased estimation of attention mechanisms expressed by random feature maps decompositions (in particular, regular softmax-attention). Mapping helps in preserving linear space and time complexity.

**Full Summary:** [https://www.marktechpost.com/2020/10/25/google-ai-introduces-performer-a-generalized-attention-framework-based-on-the-transformer-architecture/](https://www.marktechpost.com/2020/10/25/google-ai-introduces-performer-a-generalized-attention-framework-based-on-the-transformer-architecture/) 

**Github:** [https://github.com/google-research/google-research](https://github.com/google-research/google-research) 

**Paper:** [https://arxiv.org/pdf/2009.14794.pdf](https://arxiv.org/pdf/2009.14794.pdf)"
151,deeplearning,openai,comments,2017-09-12 15:07:47,OpenAI's Dota bot learns through self-play. Can anyone explain and/or provide specifics?,KloppOnThruTheRain,False,1.0,2,6znjip,https://www.reddit.com/r/deeplearning/comments/6znjip/openais_dota_bot_learns_through_selfplay_can/,2,1505228867.0,
152,deeplearning,openai,comments,2022-12-28 16:01:41,Andrew Huberman transcripts app - high-quality transcription using OpenAI's largest Whisper model (see comment),gordicaleksa,False,1.0,7,zxd7yd,https://www.hubermantranscripts.com/,2,1672243301.0,
153,deeplearning,openai,comments,2023-09-11 21:06:33,"Meta sets GPT-4 as the bar for its next AI model, says a new report",Nalix01,False,0.5,0,16g7dh3,https://www.reddit.com/r/deeplearning/comments/16g7dh3/meta_sets_gpt4_as_the_bar_for_its_next_ai_model/,2,1694466393.0,"Meta is reportedly planning to train a new model that it hopes will be as powerful as OpenAI’s GPT-4, by heavily investing in data centers and H100 chips. They hope the AI model will be way more powerful than Llama 2.

If you want to stay ahead of the curve in AI and tech, [look here first](https://dupple.com/techpresso).

**Meta's AI Ambitions**

* **New AI Development**: Meta is working on an AI model, which they hope to be several times more powerful than their recent model, Llama 2.
* **Accelerating Generative AI**: This initiative is spearheaded by a group established by Mark Zuckerberg earlier this year, focusing on AI tools that produce human-like expressions.
* **Expected Timeline**: Meta anticipates the commencement of training for this AI system in early 2024.

**Strategic Positioning in the AI Race**

* **Behind Rivals**: This new model is part of Zuckerberg's strategy to reposition Meta as a leading entity in the AI domain after falling behind competitors.
* **Infrastructure Development**: Meta is investing in data centers and acquiring advanced Nvidia chips (H100s) for AI training.
* **Shift from Microsoft**: While Meta's Llama 2 was integrated with Microsoft's cloud platform, Azure, the new model is intended to be trained on Meta's infrastructure.

**Open-source Approach and Implications**

* **Advocating Open-Source**: Zuckerberg's plan is to make the new AI model open-source, making it freely accessible for companies to build AI-driven tools.
* **Benefits and Risks**: Open-source AI models are favored due to their cost-effectiveness and flexibility. However, they also come with potential downsides, including legal risks and misuse for disseminating false information.
* **Concerns from Experts**: There are raised apprehensions about the unpredictability of the system and its potential vulnerabilities, emphasizing the need for transparency and control.

Sources [(WSJ](https://www.wsj.com/tech/ai/meta-is-developing-a-new-more-powerful-ai-system-as-technology-race-escalates-decf9451) and [TheVerge](https://www.theverge.com/2023/9/10/23867323/meta-new-ai-model-gpt-4-openai-chatbot-google-apple))

**PS:** **If you enjoyed this post**, you’ll love my [ML-powered newsletter](https://dupple.com/techpresso) that summarizes the best AI/tech news from 50+ media. It’s already being read by **6,000+** **professionals** from **OpenAI, Google, Meta**…"
154,deeplearning,openai,comments,2021-12-29 02:58:00,OpenAI GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,gordicaleksa,False,1.0,2,rqxc0g,https://www.youtube.com/watch?v=lvv4N2nf-HU,1,1640746680.0,
155,deeplearning,openai,comments,2023-12-15 04:50:13,OpenAI's Next Move: ChatGPT 4.5 Upgrade in the Works? Sam Altman Clarifies,Damanjain,False,0.11,0,18is4sm,https://thebuzz.news/article/openai-chatgpt-4-5-leak/11787/,1,1702615813.0,
156,deeplearning,openai,comments,2020-11-16 12:57:16,"OpenAI RL workshop, blog link in the comments",OneUpWallStreet,False,0.5,0,jv6axz,https://youtu.be/fdY7dt3ijgY,1,1605531436.0,
157,deeplearning,openai,comments,2023-09-20 14:28:24,Weights and Biases on LLM Fine-Tuning - Weaviate Podcast #68!,CShorten,False,0.5,0,16nmpf5,https://www.reddit.com/r/deeplearning/comments/16nmpf5/weights_and_biases_on_llm_finetuning_weaviate/,1,1695220104.0,"Hey everyone! I am SUPER excited to share our newest Weaviate Podcast with Morgan McGuire, Darek Kleczek, and Thomas Capelle from Weights & Biases!!

LLM Fine-Tuning is such an exciting topic! We dove into all things LLMs from the latest Open-source LLMs, the OpenAI fine-tuning APIs, RAG and Fine-Tuning, Domain Adaptation through fine-tuning, or Task distillation through Fine-Tuning. We also of course discussed the tooling in Weights & Biases to help with this!

I hope you find this podcast interesting and useful! More than happy to answer any questions or discuss any ideas you have about the content in the podcast!

https://www.youtube.com/watch?v=9wJuza0\_ix8"
158,deeplearning,openai,comments,2024-01-23 20:03:12,How to create a project using Langchain which will utilise OpenAI api,Rare-Breed420,False,0.25,0,19dxhp4,https://www.reddit.com/r/deeplearning/comments/19dxhp4/how_to_create_a_project_using_langchain_which/,1,1706040192.0,
159,deeplearning,openai,comments,2022-10-25 09:22:34,Understanding OpenAI's new speech recognition system that can beat humans,Difficult-Race-1188,False,0.63,2,yd0cfh,https://www.reddit.com/r/deeplearning/comments/yd0cfh/understanding_openais_new_speech_recognition/,1,1666689754.0,[https://medium.com/aiguys/openai-whisper-robust-speech-recognition-c103daf9add](https://medium.com/aiguys/openai-whisper-robust-speech-recognition-c103daf9add)
160,deeplearning,openai,comments,2022-10-06 01:31:57,OpenAI's Most Recent Model: Whisper (explained),OnlyProggingForFun,False,0.57,1,xwsibr,https://youtu.be/uFOkMme19Zs,1,1665019917.0,
161,deeplearning,openai,comments,2022-07-15 13:18:45,Have fun and learn AI at the Reinforcement Learning Hackathon on July 23rd!,zakrzzz,False,0.96,17,vzojmv,https://www.reddit.com/r/deeplearning/comments/vzojmv/have_fun_and_learn_ai_at_the_reinforcement/,1,1657891125.0,"One day to immerse yourself in technology that is a first for companies and engineers around the world!

To help you begin your immersion in AI as effectively as possible, we've prepared experts to assist you all the way.

Not without a competitive component, the winners will receive worthy prizes that will help them successfully use advanced technologies for their projects.

So come join us and learn everything you need to know about RL!

[Register here](https://lablab.ai/event/reinforcement-learning-openai-gym?utm_medium=23&utm_source=Reddit&utm_campaign=RL1&utm_term=Hackathon)

[Reinforcement Learning OpenAI Gym Hackathon](https://preview.redd.it/vm4gf3pviqb91.png?width=1920&format=png&auto=webp&s=bb3ba85c6e4d5b3069c742a3e38e6c3d70d6843a)"
162,deeplearning,openai,comments,2022-04-07 03:33:41,OpenAI's DALL·E 2 ! Text-to-Image Generation Explained,OnlyProggingForFun,False,0.75,2,ty3ysp,https://youtu.be/rdGVbPI42sA,1,1649302421.0,
163,deeplearning,openai,comments,2023-04-12 05:21:13,Is OpenAI’s Study On The Labor Market Impacts Of AI Flawed?,LesleyFair,False,0.96,26,12jb4xz,https://www.reddit.com/r/deeplearning/comments/12jb4xz/is_openais_study_on_the_labor_market_impacts_of/,1,1681276873.0,"[Example img\_name](https://preview.redd.it/f3hrmeet1eta1.png?width=1451&format=png&auto=webp&s=20e20b142a2f88c3d495177e540f34bc8ea4312b)

We all have heard an uncountable amount of predictions about how AI will ***terk err jerbs!***

However, here we have a proper study on the topic from OpenAI and the University of Pennsylvania. They investigate how Generative Pre-trained Transformers (GPTs) could automate tasks across different occupations \[1\].

Although I’m going to discuss how the study comes with a set of “imperfections”, the findings still make me really excited. The findings suggest that machine learning is going to deliver some serious productivity gains.

People in the data science world fought tooth and nail for years to squeeze some value out of incomplete data sets from scattered sources while hand-holding people on their way toward a data-driven organization. At the same time, the media was flooded with predictions of omniscient AI right around the corner.

*Let’s dive in and take an* exciting glimpse into the future of labor markets\*!\*

# What They Did

The study looks at all US occupations. It breaks them down into tasks and assesses the possible level of for each task. They use that to estimate how much automation is possible for a given occupation.

The researchers used the [O\*NET database,](https://www.onetcenter.org/database.html) which is an occupation database specifically for the U.S. market. It lists 1,016 occupations along with its standardized descriptions of tasks.

The researchers annotated each task once manually and once using GPT-4. Thereby, each task was labeled as either somewhat (<50%) or significantly (>50%) automatable through LLMs. In their judgment, they considered both the direct “exposure” of a task to GPT as well as to a secondary GPT-powered system, e.g. LLMs integrated with image generation systems.

To reiterate, a higher “exposure” means that an occupation is more likely to get automated.

Lastly, they enriched the occupation data with wages and demographic information. This was used to determine whether e. g. high or low-paying jobs are at higher risk to be automated.

So far so good. This all sounds pretty decent. Sure, there is a lot of qualitative judgment going into their data acquisition process. However, we gotta cut them some slag. These kinds of studies always struggle to get any hard data and so far they did a good job.

However, there are a few obvious things to criticize. But before we get to that let’s look at their results.

# Key Findings

The study finds that 80% of the US workforce, across all industries, could have at least some tasks affected. Even more significantly, 19% of occupations are expected to have at least half of their tasks significantly automated!

Furthermore, they find that higher levels of automation exposure are associated with:

* Programming and writing skills
* Higher wages (contrary to previous research!)
* Higher levels of education (Bachelor’s and up)

Lower levels of exposure are associated with:

* Science and critical thinking skills
* Manual work and tasks that might potentially be done using physical robots

This is somewhat unsurprising. We of course know that LLMs will likely not increase productivity in the plumbing business. However, their findings underline again how different this wave is. In the past, simple and repetitive tasks fell prey to automation.

*This time it’s the suits!*

If we took this study at face value, many of us could start thinking about life as full-time pensioners.

But not so fast! This, like all the other studies on the topic, has a number of flaws.

# Necessary Criticism

First, let’s address the elephant in the room!

OpenAI co-authored the study. They have a vested interest in the hype around AI, both for commercial and regulatory reasons. Even if the external researchers performed their work with the utmost thoroughness and integrity, which I am sure they did, the involvement of OpenAI could have introduced an unconscious bias.

*But there’s more!*

The occupation database contains over 1000 occupations broken down into tasks. Neither GPT-4 nor the human labelers can possibly have a complete understanding of all the tasks across all occupations. Hence, their judgment about how much a certain task can be automated has to be rather hand-wavy in many cases.

Flaws in the data also arise from the GPT-based labeling itself.

The internet is flooded with countless sensationalist articles about how AI will replace jobs. It is hard to gauge whether this actually causes GPT models to be more optimistic when it comes to their own impact on society. However, it is possible and should not be neglected.

The authors do also not really distinguish between labor-augmenting and labor-displacing effects and it is hard to know what “affected by” or “exposed to LLMs” actually means. Will people be replaced or will they just be able to do more?

Last but not least, lists of tasks most likely do not capture all requirements in a given occupation. For instance ""making someone feel cared for"" can be an essential part of a job but might be neglected in such a list.

# Take-Away And Implications

GPT models have the world in a frenzy - rightfully so.

Nobody knows whether 19% of knowledge work gets heavily automated or if it is only 10%.

As the dust settles, we will begin to see how the ecosystem develops and how productivity in different industries can be increased. Time will tell whether foundational LLMs, specialized smaller models, or vertical tools built on top of APIs will be having the biggest impact.

In any case, these technologies have the potential to create unimaginable value for the world. At the same time, change rarely happens without pain. I strongly believe in human ingenuity and our ability to adapt to change. All in all, the study - flaws aside - represents an honest attempt at gauging the future.

Efforts like this and their scrutiny are our best shot at navigating the future. Well, or we all get chased out of the city by pitchforks.

Jokes aside!

What an exciting time for science and humanity!

As always, I really enjoyed making this for you and I sincerely hope you found value in it!

If you are not subscribed to the newsletter yet, [click here to sign up](https://thedecoding.net/)! I send out a thoughtful 5-minute email every week to keep you in the loop about machine learning research and the data economy.

*Thank you for reading and I see you next week ⭕!*

**References:**

\[1\] [https://arxiv.org/abs/2303.10130](https://arxiv.org/abs/2303.10130)"
164,deeplearning,openai,comments,2023-05-11 02:01:13,OpenAI & GPT Dictionary of Vocabulary. Generative AI Terms To Know In 2023,OnlyProggingForFun,False,0.86,5,13ea348,https://youtu.be/q4G6X09NEu4,1,1683770473.0,
165,deeplearning,openai,comments,2016-04-30 16:20:57,Construct ~any deep reinforcement learning agent with AgentNet and Lasagne,justheuristic,False,1.0,2,4h525k,https://www.reddit.com/r/deeplearning/comments/4h525k/construct_any_deep_reinforcement_learning_agent/,1,1462033257.0,"Always wanted to train some deep RL networks to beat Atari game, but got no time to construct a training/testing environment?
OpenAI and our unhumble Lambda team made the boring part for you :)

OpenAI Gym - https://github.com/openai/gym - is a cool project that was covered a few posts down the timeline. In short, it provides you with a common interface to play Atari, text-based Go, robot control problems and many many more.

AgentNet library - https://github.com/yandexdataschool/AgentNet - is a Lasagne-based Deep Reinforcement Learning construction kit with full lasagne integration, window-based and custom RNN/GRU/LSTM-based agent memories, action policies, learning algorithms etc. etc.

Here's a single Ipython Notebook, that builds trains and submits a neural network for Space Invaders -

https://github.com/yandexdataschool/AgentNet/blob/master/examples/Playing%20Atari%20with%20Deep%20Reinforcement%20Learning%20%28OpenAI%20Gym%29.ipynb

It can be transferred onto other Atari games (tested for Air Raid and Kung Fu master) by simply changing game name at the beginning :)
Playing other games successfully may take a few tweaks of the neural network and training algorithm, shown in the notebook.

And yes, we know that the networks we have shown there are far from optimal and a few evenings of you tinkering will likely provide a farr better score. Here is where we need your help most.
We won't be able to create decent networks for all the games in our lifetimes - we just don't have that much hands, weekends and GPUs. Although we shall try our best, we would eagerly accept your assistance.

You can check out your ideas about improving NN architecture, training methods and heuristics, by simply re-running the notebook with some code changes. After submitting your solution, you and other DL researchers will see it published at the OpenAI Gym leaderboard.



Issue with request for help 

https://github.com/openai/gym/issues/16


That notebook, once again

https://github.com/yandexdataschool/AgentNet/blob/master/examples/Playing%20Atari%20with%20Deep%20Reinforcement%20Learning%20%28OpenAI%20Gym%29.ipynb

Neural Kung-Fu using GRU and advantage actor-critic
https://github.com/yandexdataschool/AgentNet/blob/master/examples/Deep%20Kung-Fu%20with%20GRUs%20and%20A2c%20algorithm%20%28OpenAI%20Gym%29.ipynb

Your kung-fu is better that our kung-fu? Get in :)


If you want to get into lasagne from ground up

Official tutorial - http://lasagne.readthedocs.io/en/latest/user/tutorial.html

CRaffel's tutorial - http://colinraffel.com/talks/next2015lasagne.pdf

YSDA course (starting from seminar 4) https://github.com/ddtm/dl-course/

-- guys from Lambda lab (YSDA + HSE)"
166,deeplearning,openai,comments,2023-12-21 18:13:33,Langchain vs. LlamaIndex vs. OpenAI GPTs: Which one should you use?,OnlyProggingForFun,False,0.75,2,18ntbqc,https://youtu.be/g84uWgVXVYg,1,1703182413.0,
167,deeplearning,openai,comments,2020-09-04 22:59:41,[D] Is OpenAI’s GPT-3 API Beta Pricing Too Rich for Researchers?,Yuqing7,False,0.67,1,imqa78,https://www.reddit.com/r/deeplearning/comments/imqa78/d_is_openais_gpt3_api_beta_pricing_too_rich_for/,1,1599260381.0,"OpenAI’s 175 billion parameter language model GPT-3 (Generative Pre-trained Transformer 3) turned heads in the NLP community when it was released in June, and now it’s back in the spotlight. A Reddit [post](https://www.reddit.com/r/GPT3/comments/ikorgs/oa_api_preliminary_beta_pricing_announced/) this week by independent writer and researcher Gwern Branwen detailed the pricing plan OpenAI has provided to GPT-3 Beta API users. The scheme, which goes into effect on October 1, has already raised as many questions as it has answered.

Here is a quick read: [Is OpenAI’s GPT-3 API Beta Pricing Too Rich for Researchers?](https://syncedreview.com/2020/09/04/is-openais-gpt-3-api-beta-pricing-too-rich-for-researchers/)"
168,deeplearning,openai,comments,2024-01-18 05:16:45,How can I make LLM plot graphs/figures on my database with RAG?,HappyDataGuy,False,0.67,1,199ieeu,https://www.reddit.com/r/deeplearning/comments/199ieeu/how_can_i_make_llm_plot_graphsfigures_on_my/,1,1705555005.0,I want for LLM like ChatGPT to plot graph but not with code-interpretor which requires uploading the data to openai. is there any way to achieve this? 
169,deeplearning,openai,comments,2017-10-22 12:34:28,Why does proximal policy optimization(PPO) not need a replay buffer?,Data-Daddy,False,1.0,3,77zy79,https://www.reddit.com/r/deeplearning/comments/77zy79/why_does_proximal_policy_optimizationppo_not_need/,1,1508675668.0,"How come PPO does not use a replay buffer? I never saw them explicitly address this in the paper, but the openai blogpost does(https://blog.openai.com/openai-baselines-ppo/)"
170,deeplearning,openai,comments,2023-03-16 00:03:09,OpenAI's GPT 4 is out and it's multimodal! What we know so far,gordicaleksa,False,0.38,0,11sdx6l,https://www.youtube.com/watch?v=FY9Nlkoq4GI&t=2s&ab_channel=AleksaGordi%C4%87-TheAIEpiphany,1,1678924989.0,
171,deeplearning,openai,comments,2021-12-24 15:48:32,[R] OpenAI Releases GLIDE: A Scaled-Down Text-to-Image Model That Rivals DALL-E Performance,Yuqing7,False,0.91,24,rnovk0,https://www.reddit.com/r/deeplearning/comments/rnovk0/r_openai_releases_glide_a_scaleddown_texttoimage/,1,1640360912.0,"An OpenAI research team proposes GLIDE (Guided Language-to-Image Diffusion for Generation and Editing) for high-quality synthetic image generation. Human evaluators prefer GLIDE samples over DALL-E’s, and the model size is much smaller (3.5 billion vs. 12 billion parameters). 

Here is a quick read: [OpenAI Releases GLIDE: A Scaled-Down Text-to-Image Model That Rivals DALL-E Performance.](https://syncedreview.com/2021/12/24/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-173/)

The paper *GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models* is on [arXiv](https://arxiv.org/abs/2112.10741)."
172,deeplearning,openai,comments,2020-07-24 09:50:27,[Tutorial] Sentence to SQL Converter using GPT-3,bhavesh91,False,0.8,6,hwyz1v,https://www.reddit.com/r/deeplearning/comments/hwyz1v/tutorial_sentence_to_sql_converter_using_gpt3/,1,1595584227.0,"I created a simple Sentence to SQL Converter using GPT - 3. If you want to learn how you can use OpenAI's GPT-3 to generate NLP Applications then this simple tutorial should help.Video Link : [https://www.youtube.com/watch?v=9g66yO0Jues](https://www.youtube.com/watch?v=9g66yO0Jues)

https://reddit.com/link/hwyz1v/video/79gg5vrj1sc51/player"
173,deeplearning,openai,comments,2021-09-26 16:13:31,I added Codex (GitHub Copilot) to the terminal,tomd_96,False,1.0,12,pvwvz7,https://www.reddit.com/r/deeplearning/comments/pvwvz7/i_added_codex_github_copilot_to_the_terminal/,1,1632672811.0,"&#x200B;

https://i.redd.it/8ww6msiugvp71.gif

You can now let Zsh write code for you using the plugin I wrote: [https://github.com/tom-doerr/zsh\_codex](https://github.com/tom-doerr/zsh_codex)

All you need to provide is a comment or a variable name and the plugin will use OpenAI's Codex AI (powers GitHub Copilot) to write the corresponding code.

Be aware that you do need to get access to the Codex API."
174,deeplearning,openai,comments,2024-01-29 12:18:19,Advice on building a tool,Mr_Anonymous_01,False,0.5,0,1adtzmd,https://www.reddit.com/r/deeplearning/comments/1adtzmd/advice_on_building_a_tool/,1,1706530699.0,"Hi all. 

I am working on a project which requires me to build a system which takes in generated analytical insights from tabular data and gives recommendations to the user on what next steps can be taken from a business perspective. 

For example, 
Insights: 1. Company X is 4th in terms of growth percentage for the period 2022 to 2023 in region Y.
2. Company X is leading in revenue terms in segment Z globally. etc.

Recommendation: ### Appropriate Business Recommendation. ###

I have the resources to use openai. I have an approach in mind. But I would love to hear other ideas on how to go about building this.

Thank You."
175,deeplearning,openai,comments,2023-08-28 07:12:44,OpenAI introduces fine-tuning capabilities for GPT-3.5 Turbo,intengineering,False,1.0,5,163f3fp,https://interestingengineering.com/innovation/openai-introduces-fine-tuning-capabilities-for-gpt-35-turbo,1,1693206764.0,
176,deeplearning,openai,comments,2023-09-26 18:24:05,"OpenAI’s GPT-4 with vision still has flaws, paper reveals",Nalix01,False,0.33,0,16svoeg,https://www.reddit.com/r/deeplearning/comments/16svoeg/openais_gpt4_with_vision_still_has_flaws_paper/,1,1695752645.0,"OpenAI initially promoted GPT-4's ability to analyze and interpret images alongside text, but has since limited these features due to concerns about misuse and privacy. A recent paper sheds light on the efforts to mitigate these issues and the ongoing challenges GPT-4 faces in interpreting images accurately and responsibly.

If you want to stay ahead of the curve in AI and tech, [look here first](https://dupple.com/techpresso?utm_source=reddit&utm_medium=social&utm_campaign=post).

**Image Analysis Concerns**

* **Abuse and Privacy Issues:** OpenAI limited GPT-4's image features due to potential misuse and privacy violations.
* **Mitigation Efforts:** The company is working on safeguards to prevent malicious use and bias in GPT-4’s image analysis.

**Performance Issues**

* **Inaccurate Inferences:** GPT-4V can make incorrect inferences, combining text strings wrongly and missing details.
* **Identification Issues:** Struggles with identifying dangerous substances or chemicals and gives wrong medical imaging responses.

**Discrimination and Bias**

* **Misunderstood Symbols:** GPT-4V doesn't grasp the nuances of certain hate symbols.
* **Discrimination:** Shows bias against certain sexes and body types, relating responses mainly to body weight and body positivity.

[Source (Tech Crunch)](https://techcrunch.com/2023/09/26/openais-gpt-4-with-vision-still-has-flaws-paper-reveals/#:~:text=The%20paper%20reveals%20that%20GPT,facts%20in%20an%20authoritative%20tone)

**PS:** **If you enjoyed this post**, you’ll love my [ML-powered newsletter](https://dupple.com/techpresso?utm_source=reddit&utm_medium=social&utm_campaign=post) that summarizes the best AI/tech news from 50+ media. It’s already being read by **7,000+** **professionals** from **OpenAI, Google, Meta**…"
177,deeplearning,openai,comments,2020-09-09 19:51:13,[R] New Multitask Benchmark Suggests Even the Best Language Models Don’t Have a Clue What They’re Doing,Yuqing7,False,1.0,3,ipnoh4,https://www.reddit.com/r/deeplearning/comments/ipnoh4/r_new_multitask_benchmark_suggests_even_the_best/,1,1599681073.0,"The recently published paper, *Measuring Massive Multitask Language Understanding,* introduces a test covering topics such as elementary mathematics, US history, computer science, law, etc., designed to measure language models’ multitask accuracy. The authors, from UC Berkeley, Columbia University, UChicago, and UIUC, conclude that even the top-tier 175-billion-parameter OpenAI GPT-3 language model is a bit daft when it comes to language understanding, especially when encountering topics in greater breadth and depth than explored by previous benchmarks.

Here is a quick read: [New Multitask Benchmark Suggests Even the Best Language Models Don’t Have a Clue What They’re Doing](https://syncedreview.com/2020/09/09/new-multitask-benchmark-suggests-even-the-best-language-models-dont-have-a-clue-what-theyre-doing/)

The paper *Measuring Massive Multitask Language Understanding* is on [arXiv](https://arxiv.org/pdf/2009.03300.pdf)."
178,deeplearning,openai,comments,2021-09-03 00:19:43,OpenAI's Codex in Vim,tomd_96,False,0.88,12,pgu4ez,https://www.reddit.com/r/deeplearning/comments/pgu4ez/openais_codex_in_vim/,1,1630628383.0,"&#x200B;

https://i.redd.it/7xui0u2ml6l71.gif

You can now let your editor write Python code for you using the Vim plugin I wrote: [https://github.com/tom-doerr/vim\_codex](https://github.com/tom-doerr/vim_codex)

All you need to provide is a docstring and the plugin will use OpenAI's Codex AI (powers GitHub Copilot) to write the corresponding code.

Be aware that you do need to get access to the Codex API."
179,deeplearning,openai,comments,2019-10-15 20:37:36,Explaining OpenAI's Robotic Hand Rubik's Cube Solver!,HenryAILabs,False,0.56,1,die18r,https://www.reddit.com/r/deeplearning/comments/die18r/explaining_openais_robotic_hand_rubiks_cube_solver/,1,1571171856.0,"This video will explain some of the details behind the amazing research study from OpenAI using a Meta-Learning Automatic Domain Randomization algorithm to bridge the Sim2Real gap and solve a Rubik's Cube with a Robotic Hand!!

https://youtu.be/2AqGocPOOG4"
180,deeplearning,openai,comments,2021-12-03 15:41:55,"[R] Warsaw U, Google & OpenAI’s Terraformer Achieves a 37x Speedup Over Dense Baselines on 17B Transformer Decoding",Yuqing7,False,0.89,18,r81wny,https://www.reddit.com/r/deeplearning/comments/r81wny/r_warsaw_u_google_openais_terraformer_achieves_a/,1,1638546115.0,"In the new paper Sparse is Enough in Scaling Transformers, a research team from the University of Warsaw, Google Research and OpenAI proposes Scaling Transformers, a family of novel transformers that leverage sparse layers to scale efficiently and perform unbatched decoding much faster than original transformers, enabling fast inference on long sequences even with limited memory. 

Here is a quick read: [Warsaw U, Google & OpenAI’s Terraformer Achieves a 37x Speedup Over Dense Baselines on 17B Transformer Decoding.](https://syncedreview.com/2021/12/03/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-158/)

The paper *Sparse is Enough in Scaling Transformers* is on [arXiv](https://arxiv.org/abs/2111.12763)."
181,deeplearning,openai,comments,2019-11-09 23:25:27,TensorLayer Team Released Reinforcement Learning Algorithm Baseline-RLzoo,quantumiracle,False,0.97,24,du3jol,https://www.reddit.com/r/deeplearning/comments/du3jol/tensorlayer_team_released_reinforcement_learning/,1,1573341927.0,"Recently,  in order to enable the industry to better use the cutting-edge  reinforcement learning algorithms, the TensorLayer Reinforcement   Learning Team has released a complete library of reinforcement learning   baseline algorithms for the industry — RLzoo. TensorLayer is an  extended  library based on TensorFlow for better supports of basic  neural network  construction and diverse neural network applications.  The RLzoo project  is the first comprehensive open source algorithm  library with  TensorLayer 2.0 and TensorFlow 2.0 since the release of  TensorFlow 2.0.  The library currently supports OpenAI Gym, DeepMind  Control Suite and  other large-scale simulation environments, such as  the robotic learning  environment RLBench, etc.

Link of full post: [https://medium.com/@zhding96/tensorlayer-team-released-reinforcement-learning-algorithm-baseline-rlzoo-2663cfb77904](https://medium.com/@zhding96/tensorlayer-team-released-reinforcement-learning-algorithm-baseline-rlzoo-2663cfb77904)

Link of RLzoo: [https://github.com/tensorlayer/RLzoo](https://github.com/tensorlayer/RLzoo)

Link of RL tutorial: [https://github.com/tensorlayer/tensorlayer/tree/master/examples/reinforcement\_learning](https://github.com/tensorlayer/tensorlayer/tree/master/examples/reinforcement_learning)

Slack group: [https://app.slack.com/client/T5SHUUKNJ/D5SJDERU7](https://app.slack.com/client/T5SHUUKNJ/D5SJDERU7)"
182,deeplearning,openai,comments,2023-06-07 15:08:34,How does Openai's CLIP avoids dimensional collapse?,souhaielbensalem,False,0.95,18,143frxx,https://www.reddit.com/r/deeplearning/comments/143frxx/how_does_openais_clip_avoids_dimensional_collapse/,1,1686150514.0,"According to this paper from FAIR : [https://arxiv.org/abs/2110.09348](https://arxiv.org/abs/2110.09348)  , contrastive learning methods suffer from the problem of dimensional  collapse where ""the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available  embedding"". According to the authors, this problem is caused by  two  main reason which are strong augmentations and the implicit  regularization of deep neural networks that tend to converge to low rank  solutions. I am not sure how Openai's CLIP avoids this problem. Is it just the  sheer scale and the fact they used a batch size of 32k smaples?"
183,deeplearning,openai,comments,2020-07-27 00:13:47,"OpenAI's New Language Generator: GPT-3. This AI Generates Code, Websites, Songs & More From Words",OnlyProggingForFun,False,0.6,2,hyhvqi,https://www.youtube.com/watch?v=gDDnTZchKec,1,1595808827.0,
184,deeplearning,openai,comments,2022-06-23 08:39:18,Deep dive into the OpenAI CLIP's code | Machine Learning Coding Series,gordicaleksa,False,0.67,1,vis6xu,https://youtu.be/jwZQD0Cqz4o,0,1655973558.0,
185,deeplearning,openai,comments,2022-02-11 16:47:38,Interview with Arvind Neelakantan about the OpenAI Embeddings API,HenryAILabs,False,1.0,2,sq3s05,https://www.reddit.com/r/deeplearning/comments/sq3s05/interview_with_arvind_neelakantan_about_the/,0,1644598058.0,"Hey everyone!   


The release of OpenAI's Embeddings API has been quite the story! I had the pleasure to interview Arvind Neelakantan on miscellaneous topics pertaining to these new advances in Search: [https://www.youtube.com/watch?v=uFxfZ0vLsoU](https://www.youtube.com/watch?v=uFxfZ0vLsoU)  


Additional Background on this:  
OpenAI Embeddings API Blog Post: [https://openai.com/blog/introducing-text-and-code-embeddings/](https://openai.com/blog/introducing-text-and-code-embeddings/)

Nils Reimers' Response (OpenAI GPT-3 Text Embeddings - Really a new state-of-the-art in dense text embeddings?): [https://medium.com/@nils\_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9](https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9)

Yannic Kilcher on the topic: [https://www.youtube.com/watch?v=5skIqoO3ku0](https://www.youtube.com/watch?v=5skIqoO3ku0)"
186,deeplearning,openai,comments,2022-04-15 16:55:50,DALL-E (Zero-Shot Text-to-Image Generation) -PART(1/2),rakshith291,False,0.75,2,u4cg54,https://www.reddit.com/r/deeplearning/comments/u4cg54/dalle_zeroshot_texttoimage_generation_part12/,0,1650041750.0," OpenAI released DALL E2 in the last week, this system is basically have a capability of generating an image from a text description. Some of the results were truly amazing. In this blog, I tried to discuss the ideas around DALL-E (version 1) .  
DALL-E consist of two main components d-VAE(discrete-Variational Auto Encoder) and Auto-regressive transformer. In Part-1 I focused on d-VAE part where I tried to talk about basic VAE and it's ELBO formulation, VQ-VAE eventually that leads to d-VAE. It's reconstruction loss is formulated from Logit Laplcae (bounded) unlike typical L1 or L2. Overall this part explains about how a discrete vector(token) can be generated for an input image."
187,deeplearning,openai,comments,2020-09-15 18:13:05,[R] Human Feedback Improves OpenAI Model Summarizations,Yuqing7,False,1.0,1,ite8ym,https://www.reddit.com/r/deeplearning/comments/ite8ym/r_human_feedback_improves_openai_model/,0,1600193585.0,"In a new paper, a team of OpenAI researchers sets out to advance methods for training large-scale language models such as BERT on objectives that more closely capture human preferences — and does so by putting humans back into the loop. The work focuses on abstractive English text summarization — a subjective task that’s considered challenging because the notion of what makes a “good summary” is difficult to capture without human input.

Here is a quick read: [Human Feedback Improves OpenAI Model Summarizations](https://syncedreview.com/2020/09/15/human-feedback-improves-openai-model-summarizations/)

The paper *Learning to Summarize from Human Feedback* is on [arXiv](https://arxiv.org/pdf/2009.01325.pdf)."
188,deeplearning,openai,comments,2023-09-09 14:42:30,"Musk once tried to stop Google's DeepMind acquisition in 2014, saying the future of AI shouldn't be controlled by Larry Page",Nalix01,False,0.6,1,16e7g71,https://www.reddit.com/r/deeplearning/comments/16e7g71/musk_once_tried_to_stop_googles_deepmind/,0,1694270550.0,"Elon Musk once attempted to prevent Google's acquisition of AI company DeepMind in 2014, indicating that the future of AI shouldn't be in the hands of Larry Page.

If you want to stay ahead of the curve in AI and tech, [look here first](https://dupple.com/techpresso).

**Background of the Acquisition Attempt**

* **Isaacson's Revelations**: Walter Isaacson, who wrote a biography on Musk, revealed the behind-the-scenes efforts regarding the DeepMind deal.
* **Musk-Page Dispute**: At a 2013 birthday celebration, the two tech magnates disagreed on AI's role in the future, leading to Musk's concerns about Page's influence over AI.

**Musk's Efforts to Buy DeepMind**

* **Direct Approach**: Following his disagreement with Page, Musk approached DeepMind's co-founder to discourage him from accepting Google's deal.
* **Financing Efforts**: Musk, along with PayPal co-founder Luke Nosek, made efforts to acquire DeepMind, but Google ultimately secured the deal in 2014 for $500 million.

**Diverging Views on AI's Future**

* **Subsequent AI Ventures**: Post the DeepMind episode, Musk initiated other AI ventures, co-founding OpenAI in 2015 and later establishing xAI.
* **Industry Concerns**: Not just Musk, but several prominent figures in tech have expressed apprehensions about AI's trajectory and potential dangers. Yet, some AI experts argue that the emphasis should be on present challenges rather than hypothetical future threats.

[Source (Business Insiders)](https://www.businessinsider.com/elon-musk-google-larry-page-future-of-ai-2023-9?r=US&IR=T#:~:text=%22The%20future%20of%20AI%20should,Hassabis%2C%20according%20to%20Isaacson's%20account.&text=Musk%20even%20attempted%20to%20cobble,in%202014%20for%20%24500%20million)

**PS:** **If you enjoyed this post**, you’ll love my [ML-powered newsletter](https://dupple.com/techpresso) that summarizes the best AI/tech news from 50+ media. It’s already being read by **6,000+** **professionals** from **OpenAI, Google, Meta**…"
189,deeplearning,openai,comments,2023-08-15 15:09:36,How to run OpenAI CLIP with UI for Image Retrieval and Filtering your dataset - Supervisely,tdionis,False,1.0,2,15rvdc7,https://supervisely.com/blog/openai-clip-for-image-retrieval-and-filtering-computer-vision-datasets-tutorial/,0,1692112176.0,
190,deeplearning,openai,comments,2021-01-06 00:55:05,OpenAI successfully trained a network able to generate images from text captions: DALL·E,OnlyProggingForFun,False,0.76,6,krcfkg,https://youtu.be/nLzfDVwQxRU,0,1609894505.0,
191,deeplearning,openai,comments,2020-06-16 03:13:01,"I just published ""All you need to need to know about OPENAI's GPT-3 "" on medium . Check out , feedback is highly appreciated..",dharma_m,False,0.64,6,h9ve0q,https://medium.com/@savanidharmam5/all-you-need-to-know-about-openai-gpt-3-d0d879446aeb,0,1592277181.0,
192,deeplearning,openai,comments,2022-10-04 01:00:17,Create your own speech to text application with Whisper from OpenAI and Flask,hellopaperspace,False,0.75,6,xv0x55,https://blog.paperspace.com/whisper-openai-flask-application-deployment/,0,1664845217.0,
193,deeplearning,openai,comments,2021-09-19 08:00:31,AI research papers explainer channel.,gauravc2796,False,0.54,1,pr3syb,https://www.reddit.com/r/deeplearning/comments/pr3syb/ai_research_papers_explainer_channel/,0,1632038431.0,"Hi, I have started a youtube channel where I would provide some explainer on the latest AI research papers as I have happened to read a lot of them.  
If you have any suggestions, comments, or anything, do let me know.   
Your opinion would be highly valuable :)  
Channel: [https://www.youtube.com/channel/UCYEXrPn4gP9RbaSzZvxX6MA](https://www.youtube.com/channel/UCYEXrPn4gP9RbaSzZvxX6MA)  


Some Videos which have been created till now:

Textless NLP: [https://www.youtube.com/watch?v=zw\_QjUptr5o](https://www.youtube.com/watch?v=zw_QjUptr5o)  
Neural DB: [https://www.youtube.com/watch?v=Vo9L0LETMI4](https://www.youtube.com/watch?v=Vo9L0LETMI4)  
Perceiver IO: [https://www.youtube.com/watch?v=AS1Sh-KuNzs](https://www.youtube.com/watch?v=AS1Sh-KuNzs)  
Openai's GPT codex: [https://www.youtube.com/watch?v=8977dybJ7Ro](https://www.youtube.com/watch?v=8977dybJ7Ro)"
194,deeplearning,openai,comments,2020-10-28 11:39:21,How did I get access to GPT-3 OpenAI's API? Tips are shared at 4:45 in the video! The rest of the videos explains what can GPT-3 really do and how it can help you or your company.,OnlyProggingForFun,False,0.46,0,jjm4ep,https://www.youtube.com/watch?v=Gm4AMjV8ErM,0,1603885161.0,
195,deeplearning,openai,comments,2020-05-02 14:55:05,Video Analysis of the OpenAI Jukebox Paper,ykilcher,False,1.0,2,gc7rf5,https://www.reddit.com/r/MachineLearning/comments/gc4qjr/d_video_analysis_jukebox_a_generative_model_for/?utm_medium=android_app&utm_source=share,0,1588431305.0,
196,deeplearning,openai,comments,2019-04-14 05:16:57,Humans Call GG! OpenAI Five Bots Beat Top Pros OG in Dota 2,gwen0927,False,1.0,6,bczk3e,https://medium.com/syncedreview/humans-call-gg-openai-five-bots-beat-top-pros-og-in-dota-2-8508e59b8fd5,0,1555219017.0,
197,deeplearning,openai,comments,2017-02-24 11:09:03,Exploring Machine Learning at OpenAI &amp; Google With the Experts,teamrework,False,1.0,2,5vwx2e,https://re-work.co/blog/video-deep-learning-ilya-sutskever-openai-anjuli-kannan-google,0,1487934543.0,
198,deeplearning,openai,comments,2022-02-09 19:59:11,How to render OpenAi Gym in Google Colaboratory,sakshman,False,1.0,1,someat,https://www.reddit.com/r/deeplearning/comments/someat/how_to_render_openai_gym_in_google_colaboratory/,0,1644436751.0,
199,deeplearning,openai,comments,2016-04-27 23:38:30,Train Your Reinforcement Learning Agents At The OpenAI Gym,harrism,False,0.99,5,4graaq,https://devblogs.nvidia.com/parallelforall/train-reinforcement-learning-agents-openai-gym/,0,1461800310.0,
200,deeplearning,openai,relevance,2023-11-23 12:13:26,OpenAI Q* Rumours,MIKOLAJslippers,False,0.62,7,181zwb8,https://www.reddit.com/r/deeplearning/comments/181zwb8/openai_q_rumours/,30,1700741606.0,Anyone know any juicy rumours about the capabilities of this internal Q* project at OpenAI that has supposedly catalysed some of the recent dramatics?
201,deeplearning,openai,relevance,2023-12-16 15:22:29,Is there any alternative for OpenAI API?,CrazyProgramm,False,0.88,24,18jtffj,https://www.reddit.com/r/deeplearning/comments/18jtffj/is_there_any_alternative_for_openai_api/,24,1702740149.0, So I am from Sri Lanka and our university is going to organize a competition and we need OpenAI API for it but we don't have money to afford it. Is there any alternative API you guys know 
202,deeplearning,openai,relevance,2023-11-04 12:53:57,Controlling Chrome browser using OpenAI APIs,smtabatabaie,False,1.0,2,17nl1v4,https://www.reddit.com/r/deeplearning/comments/17nl1v4/controlling_chrome_browser_using_openai_apis/,0,1699102437.0,"Hi, I wanted to develop a Chrome extension that can do some tasks on the browser for users using OpenAI APIs. I wanted to ask if you guys know any ways to create something that has access to users' browsers and can accomplish tasks. Will really appreciate any help. Thanks"
203,deeplearning,openai,relevance,2023-08-15 04:46:43,OpenAI Notebooks which are really helpful.,vishank97,False,0.94,18,15rihgo,https://www.reddit.com/r/deeplearning/comments/15rihgo/openai_notebooks_which_are_really_helpful/,3,1692074803.0,"The OpenAI cookbook is one of the most underrated and underused developer resources available today. Here are 7 notebooks you should know about:

1. Improve LLM reliability:  
[https://github.com/openai/openai-cookbook/blob/main/techniques\_to\_improve\_reliability.md](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)
2. Embedding long text inputs:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Embedding\_long\_inputs.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb)
3. Dynamic masks with DALLE:  
[https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How\_to\_create\_dynamic\_masks\_with\_DALL-E\_and\_Segment\_Anything.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How_to_create_dynamic_masks_with_DALL-E_and_Segment_Anything.ipynb)
4. Function calling to find places nearby:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Function\_calling\_finding\_nearby\_places.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb)
5. Visualize embeddings in 3D:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing\_embeddings\_in\_3D.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_3D.ipynb)
6. Pre and post-processing of Whisper transcripts:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Whisper\_processing\_guide.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb)
7. Search, Retrieval, and Chat:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Question\_answering\_using\_a\_search\_API.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb)

Big thanks to the creators of these notebooks!"
204,deeplearning,openai,relevance,2023-11-10 13:55:32,"Order in which OpenAI ""short courses"" should be taken",KA_IL_AS,False,0.71,3,17s4irx,https://www.reddit.com/r/deeplearning/comments/17s4irx/order_in_which_openai_short_courses_should_be/,2,1699624532.0,"As you all know OpenAI has released a whole lot of  ""Short Courses"" lately and they're good too. I've taken their prompt engineering course months ago when it was released, it was super helpful.  
But here's the thing they've released a lot of courses after that, and now I don't know in what order I should be taking them.  
Any thoughts and advices on this ? It'll be super helpful"
205,deeplearning,openai,relevance,2024-01-23 20:03:12,How to create a project using Langchain which will utilise OpenAI api,Rare-Breed420,False,0.25,0,19dxhp4,https://www.reddit.com/r/deeplearning/comments/19dxhp4/how_to_create_a_project_using_langchain_which/,1,1706040192.0,
206,deeplearning,openai,relevance,2024-02-06 18:19:49,"Edgen: A Local, Open Source GenAI Server Alternative to OpenAI in Rust",EdgenAI,False,0.86,9,1akghpe,https://www.reddit.com/r/deeplearning/comments/1akghpe/edgen_a_local_open_source_genai_server/,0,1707243589.0,"⚡Edgen: Local, private GenAI server alternative to OpenAI. No GPU required. Run AI models locally: LLMs (Llama2, Mistral, Mixtral...), Speech-to-text (whisper) and many others.

Our goal with⚡Edgen is to make privacy-centric, local development accessible to more people, offering full compliance with OpenAI's API. It's made for those who prioritize data privacy and want to experiment with or deploy AI models locally with a Rust based infrastructure.

We'd love for this community to be among the first to try it out, give feedback, and contribute to its growth. 

Check it out here:  [GitHub - edgenai/edgen: ⚡ Edgen: Local, private GenAI server alternative to OpenAI. No GPU required. Run AI models locally: LLMs (Llama2, Mistral, Mixtral...), Speech-to-text (whisper) and many others.](https://github.com/edgenai/edgen) "
207,deeplearning,openai,relevance,2023-06-15 11:39:53,OpenAI function calling - tutorial,mildlyoverfitted,False,0.89,7,14a06nr,https://youtu.be/_B7F_6nTVEg,0,1686829193.0,
208,deeplearning,openai,relevance,2023-11-16 08:28:46,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,0.84,69,17whz6e,https://v.redd.it/qx68wbuf7o0c1,7,1700123326.0,
209,deeplearning,openai,relevance,2023-12-21 18:13:33,Langchain vs. LlamaIndex vs. OpenAI GPTs: Which one should you use?,OnlyProggingForFun,False,0.75,2,18ntbqc,https://youtu.be/g84uWgVXVYg,1,1703182413.0,
210,deeplearning,openai,relevance,2023-11-15 10:30:36,"GPT-4 Turbo: Die Zukunft der Künstlichen Intelligenz, entwickelt von OpenAI",Webglobic_tech,False,0.82,14,17vqtlk,https://webglobic.com/magazine/,0,1700044236.0,
211,deeplearning,openai,relevance,2023-01-25 22:06:47,OpenAi's breakthrough,bradasm,False,0.15,0,10lb7k3,https://www.reddit.com/r/deeplearning/comments/10lb7k3/openais_breakthrough/,2,1674684407.0,[https://twitter.com/make\_mhe/status/1618255363580755968](https://twitter.com/make_mhe/status/1618255363580755968)
212,deeplearning,openai,relevance,2023-06-07 15:08:34,How does Openai's CLIP avoids dimensional collapse?,souhaielbensalem,False,0.95,18,143frxx,https://www.reddit.com/r/deeplearning/comments/143frxx/how_does_openais_clip_avoids_dimensional_collapse/,1,1686150514.0,"According to this paper from FAIR : [https://arxiv.org/abs/2110.09348](https://arxiv.org/abs/2110.09348)  , contrastive learning methods suffer from the problem of dimensional  collapse where ""the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available  embedding"". According to the authors, this problem is caused by  two  main reason which are strong augmentations and the implicit  regularization of deep neural networks that tend to converge to low rank  solutions. I am not sure how Openai's CLIP avoids this problem. Is it just the  sheer scale and the fact they used a batch size of 32k smaples?"
213,deeplearning,openai,relevance,2023-01-27 10:45:48,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.95,114,10mhyek,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
214,deeplearning,openai,relevance,2023-06-12 16:29:03,"Openai Leak Docs, Possible for New Update? 😳",KSSolomon,False,0.5,0,147r660,https://i.redd.it/fqym39c36m5b1.jpg,0,1686587343.0,"Someone who uses Reddit, which is a big online forum where people can talk about all kinds of stuff, found out about this update by looking at some of the computer code that makes up the program.

They found out that the new version will have something called ""workspaces"". This would be like if you could make different profiles in a game, and the game would remember each one and what you did with them.

They also found that the program might be able to take files, which are like digital pieces of paper with stuff written on them.

OpenAI talked about making this business version of ChatGPT in April 2023, and they also said they would make it more private. This means that the things people say to ChatGPT wouldn't be used to make it smarter anymore.

Now, here are the implications of this update:

Businesses could use this version of ChatGPT to help them with their work. They might be able to give it files with information, and ChatGPT could use that to help answer questions or solve problems.

The ""workspaces"" feature could make it easier for people to use ChatGPT in different ways. For example, a business might have one workspace for customer service, and another for helping with paperwork.

The new privacy measures would mean that what you say to ChatGPT stays private. This is important because it helps keep people's information safe. It also means that OpenAI is listening to people's concerns about privacy and doing something about it.

This literally just happened if you want Ai news as it drops it launched [here first](https://www.therundown.ai/subscribe?utm_source=al). The whole article has been extrapolated here as well for convenience."
215,deeplearning,openai,relevance,2023-08-28 07:12:44,OpenAI introduces fine-tuning capabilities for GPT-3.5 Turbo,intengineering,False,0.81,3,163f3fp,https://interestingengineering.com/innovation/openai-introduces-fine-tuning-capabilities-for-gpt-35-turbo,1,1693206764.0,
216,deeplearning,openai,relevance,2023-12-15 04:50:13,OpenAI's Next Move: ChatGPT 4.5 Upgrade in the Works? Sam Altman Clarifies,Damanjain,False,0.2,0,18is4sm,https://thebuzz.news/article/openai-chatgpt-4-5-leak/11787/,1,1702615813.0,
217,deeplearning,openai,relevance,2023-03-10 13:52:36,OpenAI's Python API walk-through,Combination-Fun,False,0.67,1,11npswy,https://www.reddit.com/r/deeplearning/comments/11npswy/openais_python_api_walkthrough/,0,1678456356.0,"If you are getting started with OpenAI's newest Python API, you don't have to spend too much time familiarising yourself with how to use it. Here is a video that walks through the different possiblities, API parameters and finally shares a demo app: [https://youtu.be/dcnfhtuL7qA](https://youtu.be/dcnfhtuL7qA) 

Hope its useful. 

https://preview.redd.it/z1aczihy2xma1.png?width=1920&format=png&auto=webp&s=d280af7e119ceac77bfdd9196add395037721b8f"
218,deeplearning,openai,relevance,2023-11-16 08:28:39,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,1.0,1,17whz4d,https://v.redd.it/qx68wbuf7o0c1,0,1700123319.0,
219,deeplearning,openai,relevance,2023-04-08 15:25:44,[P] Blog post explaining CLIP by OpenAI,pmgautam_,False,0.67,2,12fpa1l,/r/MachineLearning/comments/12fp9nq/p_blog_post_explaining_clip_by_openai/,0,1680967544.0,
220,deeplearning,openai,relevance,2023-06-18 21:47:43,Demystifying OpenAI Procurement: An Overview of Processes and Pricing,digital-bolkonsky,False,0.8,6,14cw6o0,/r/ai_cost/comments/14cw61j/demystifying_openai_procurement_an_overview_of/,0,1687124863.0,
221,deeplearning,openai,relevance,2024-02-13 14:38:25,"This is my foundation block or sandwich stack AI model... Criticisms and inputs are welcome, formulated by openAI scholarGPT - thanks in advance.",AskACapperDOTcom,False,0.25,0,1apuyv4,/r/OpenAI/comments/1apf8jy/this_is_my_foundation_block_or_sandwich_stack_ai/,10,1707835105.0,
222,deeplearning,openai,relevance,2023-05-17 02:26:44,OpenAI CEO asking for government's license for building AI . WHAT THE ACTUAL FUCK?,Angry_Grandpa_,False,0.82,23,13joq2b,/r/singularity/comments/13jbc76/openai_ceo_asking_for_governments_license_for/,8,1684290404.0,
223,deeplearning,openai,relevance,2022-10-06 01:31:57,OpenAI's Most Recent Model: Whisper (explained),OnlyProggingForFun,False,0.67,2,xwsibr,https://youtu.be/uFOkMme19Zs,1,1665019917.0,
224,deeplearning,openai,relevance,2023-05-11 02:01:13,OpenAI & GPT Dictionary of Vocabulary. Generative AI Terms To Know In 2023,OnlyProggingForFun,False,0.87,6,13ea348,https://youtu.be/q4G6X09NEu4,1,1683770473.0,
225,deeplearning,openai,relevance,2023-08-15 15:09:36,How to run OpenAI CLIP with UI for Image Retrieval and Filtering your dataset - Supervisely,tdionis,False,1.0,2,15rvdc7,https://supervisely.com/blog/openai-clip-for-image-retrieval-and-filtering-computer-vision-datasets-tutorial/,0,1692112176.0,
226,deeplearning,openai,relevance,2022-11-30 19:14:16,OpenAI's new impressive Conversational LLM - ChatGPT,dulldata,False,1.0,1,z90966,https://www.youtube.com/watch?v=2VJZky25rIs,0,1669835656.0,
227,deeplearning,openai,relevance,2022-02-12 16:58:26,OpenAI Embeddings API,HenryAILabs,False,1.0,2,sqvwgq,https://www.reddit.com/r/deeplearning/comments/sqvwgq/openai_embeddings_api/,0,1644685106.0,"Hey everyone! I recently had the opportunity to interview Arvind Neelakantan from OpenAI about these ideas related to their Embeddings API. This video summarizes my takeaways and provides background to the key points discussed on the podcast. I really hope you find this informative / useful!  
[https://youtu.be/vyf2sqhAm4Y](https://youtu.be/vyf2sqhAm4Y)"
228,deeplearning,openai,relevance,2023-03-02 07:25:30,Good news for builders! OpenAI Releases APIs To ChatGPT and Whisper,LesleyFair,False,0.69,6,11fwcxf,https://www.reddit.com/r/deeplearning/comments/11fwcxf/good_news_for_builders_openai_releases_apis_to/,0,1677741930.0,"If you were as disappointed as I was when you saw that access to Meta's LLaMA models is limited to researchers, you are going to like this.  


[APIs to ChatGPT and OpenAI's speech-to-text model whisper](https://openai.com/blog/introducing-chatgpt-and-whisper-apis) are available as of yesterday. Through system-wide optimizations, they claim to have reduced inference costs by 90%. They now price ChatGPT at $0.002 per 1000 tokens. Dedicated instances are available for speedup and make economic sense if you process \~450M tokens a day.  


Machine learning progress continues to be as fast as a banana peal skating on warm vaseline. 

If you found this useful and want to stay in the loop, consider subscribing to The Decoding. I send out a weekly 5-minute newsletter that keeps professionals in the loop about machine learning and the data economy. [Click here to subscribe!](https://thedecoding.net/)"
229,deeplearning,openai,relevance,2021-12-29 08:03:22,I wrote a program with OpenAI's Codex that fixes errors,tomd_96,False,0.94,95,rr2wme,https://v.redd.it/jupdtry6vf881,6,1640765002.0,
230,deeplearning,openai,relevance,2022-09-23 16:28:06,OpenAI Whisper: SOTA Speech To Text With Microphone Demo,l33thaxman,False,0.85,8,xm26oq,https://www.reddit.com/r/deeplearning/comments/xm26oq/openai_whisper_sota_speech_to_text_with/,0,1663950486.0,"OpenAI has released a Speech To Text model that nears human performance.  This video goes over the basics of the model, as well as how to run it with a microphone.

[https://youtu.be/nwPaRSlDSaY](https://youtu.be/nwPaRSlDSaY)"
231,deeplearning,openai,relevance,2023-03-16 00:03:09,OpenAI's GPT 4 is out and it's multimodal! What we know so far,gordicaleksa,False,0.25,0,11sdx6l,https://www.youtube.com/watch?v=FY9Nlkoq4GI&t=2s&ab_channel=AleksaGordi%C4%87-TheAIEpiphany,1,1678924989.0,
232,deeplearning,openai,relevance,2022-08-09 16:28:47,Microsoft Announces new Integrations with OpenAI and MLFlow,mhamilton723,False,1.0,2,wk7r6l,https://www.reddit.com/r/deeplearning/comments/wk7r6l/microsoft_announces_new_integrations_with_openai/,0,1660062527.0,"Today Microsoft launched SynapseML v0.10.0 with 175-billion parameter OpenAI language models, full support for .NET, Python, R, Scala, and Java, and an integration with MLflow, and much more. Check out the full release notes, leave a star, and explore SnyapseML

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.10.0](https://github.com/microsoft/SynapseML/releases/tag/v0.10.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/exciting-new-release-of-synapseml/ba-p/3589606](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/exciting-new-release-of-synapseml/ba-p/3589606)

&#x200B;

https://preview.redd.it/4sfexdqnspg91.jpg?width=4125&format=pjpg&auto=webp&s=d894599f7bf6fa5dd3289c23133903260d00de96"
233,deeplearning,openai,relevance,2022-10-25 09:22:34,Understanding OpenAI's new speech recognition system that can beat humans,Difficult-Race-1188,False,0.67,2,yd0cfh,https://www.reddit.com/r/deeplearning/comments/yd0cfh/understanding_openais_new_speech_recognition/,1,1666689754.0,[https://medium.com/aiguys/openai-whisper-robust-speech-recognition-c103daf9add](https://medium.com/aiguys/openai-whisper-robust-speech-recognition-c103daf9add)
234,deeplearning,openai,relevance,2022-07-16 16:09:00,How OpenAI Reduces risks for DALL·E 2,OnlyProggingForFun,False,1.0,1,w0k20k,https://youtu.be/qh3_DnteGD0,0,1657987740.0,
235,deeplearning,openai,relevance,2022-09-22 19:32:49,OpenAI Whisper powered Gradio App for Automatic Subtitle Video Generation,dulldata,False,1.0,4,xlawi6,https://www.youtube.com/watch?v=x_uxzgTg1U0,0,1663875169.0,
236,deeplearning,openai,relevance,2021-09-03 00:19:43,OpenAI's Codex in Vim,tomd_96,False,0.88,13,pgu4ez,https://www.reddit.com/r/deeplearning/comments/pgu4ez/openais_codex_in_vim/,1,1630628383.0,"&#x200B;

https://i.redd.it/7xui0u2ml6l71.gif

You can now let your editor write Python code for you using the Vim plugin I wrote: [https://github.com/tom-doerr/vim\_codex](https://github.com/tom-doerr/vim_codex)

All you need to provide is a docstring and the plugin will use OpenAI's Codex AI (powers GitHub Copilot) to write the corresponding code.

Be aware that you do need to get access to the Codex API."
237,deeplearning,openai,relevance,2023-03-03 23:02:37,Meta new large lanugage model (similar to OpenAI one) called LLaMA is leaked via torrent,aipaintr,False,0.91,8,11hhzeg,https://github.com/facebookresearch/llama/pull/73/files,2,1677884557.0,
238,deeplearning,openai,relevance,2023-06-30 20:46:28,"What is better for a startup with €0 investment, using the OpenAI API or deploying a model like Falcon40b?",ByMarcosDesigns,False,0.67,1,14nbmxr,https://www.reddit.com/r/deeplearning/comments/14nbmxr/what_is_better_for_a_startup_with_0_investment/,0,1688157988.0,"First of all I would like to wish a good day to all the readers of this message, my name is Marc and I am one of the three boys and girls who are promoting a project called AldQuest, an application focused on helping students with the use of artificial intelligence light to test their abilities or be able to solve their doubts. 

Right now we have verified that our client would pay for the application but we are having a problem with the prices that the client can pay.  We have to make the costs of the application as low as possible but using the OpenAI API is a problem for us, we could say that it makes the prices go up.  

We are thinking of deploying our own fine-tuned model based on Falcon 40b, but as we are new to this we don't know how much money it would take to do it in a service like @runpod or @lambdalabs. 

What would you recommend we do?"
239,deeplearning,openai,relevance,2022-07-06 11:22:15,Reinforcement Learning without Reward Engineering (reproducing OpenAI paper with crowdsourcing),Euphetar,False,0.95,15,vsnnv9,https://medium.com/p/60c63402c59f,0,1657106535.0,
240,deeplearning,openai,relevance,2022-12-28 16:01:41,Andrew Huberman transcripts app - high-quality transcription using OpenAI's largest Whisper model (see comment),gordicaleksa,False,0.88,6,zxd7yd,https://www.hubermantranscripts.com/,2,1672243301.0,
241,deeplearning,openai,relevance,2022-10-18 16:52:15,Fully automated video generation - connecting OpenAI's Whisper with Stable Diffusion. Tutorial & code coming soon!,hayAbhay,False,1.0,13,y7cbf4,https://youtu.be/xRcoeUgD4GY,8,1666111935.0,
242,deeplearning,openai,relevance,2024-02-21 15:00:34,"⚡Edgen now supports Vulkan, CUDA and Metal | Open Source and local GenAI server alternative to OpenAI's API. Supports all GGUF models, across Windows, Mac and Linux with one 30MB download.",EdgenAI,False,1.0,4,1awe3tn,https://www.reddit.com/r/deeplearning/comments/1awe3tn/edgen_now_supports_vulkan_cuda_and_metal_open/,0,1708527634.0,"Our goal with⚡Edgen is to make privacy-centric, local GenAI app development accessible to more people.

It is compliant with OpenAI's API and built in  🦀 Rust so it can be natively compiled into Windows, Linux and MacOS (with a 30MB executable).

We'd love for this community to be among the first to try it out and provide feedback!

Check out⚡Edgen on GitHub: [GitHub - edgenai/edgen: ⚡ Edgen: Local, private GenAI server alternative to OpenAI.](https://github.com/edgenai/edgen)

And keep an an eye out for future releases:

* Speech to Text
* Embeddings Endpoint
* Multimodal Endpoint
* Text to Image Endpoint

&#x200B;"
243,deeplearning,openai,relevance,2022-02-09 19:59:11,How to render OpenAi Gym in Google Colaboratory,sakshman,False,1.0,1,someat,https://www.reddit.com/r/deeplearning/comments/someat/how_to_render_openai_gym_in_google_colaboratory/,0,1644436751.0,
244,deeplearning,openai,relevance,2024-02-05 12:26:35,Is there any tutorial/guide on how to efficiently deploy hugging face open-source LLMs? both for testing and then in production? what is the cost I can expect compared to OpenAI API?,HappyDataGuy,False,1.0,2,1ajfaxw,https://www.reddit.com/r/deeplearning/comments/1ajfaxw/is_there_any_tutorialguide_on_how_to_efficiently/,2,1707135995.0,
245,deeplearning,openai,relevance,2022-10-04 01:00:17,Create your own speech to text application with Whisper from OpenAI and Flask,hellopaperspace,False,0.8,6,xv0x55,https://blog.paperspace.com/whisper-openai-flask-application-deployment/,0,1664845217.0,
246,deeplearning,openai,relevance,2024-02-16 03:20:25,Unbelieve New Text To Video AI Model By OpenAI - 37 Demo Videos - Still Can't Believe Real - Watch All Videos 4K With A Nice Music - This Will Change Perception Of Reality Forever,CeFurkan,False,0.64,4,1aryk48,https://www.youtube.com/watch?v=VlJYmHNRQZQ&deeplearning,3,1708053625.0,
247,deeplearning,openai,relevance,2022-06-15 16:07:22,Join us for the OpenAI GPT-3 Deep Learning Labs Hackathon!,zakrzzz,False,0.82,7,vcxzp4,https://www.reddit.com/r/deeplearning/comments/vcxzp4/join_us_for_the_openai_gpt3_deep_learning_labs/,0,1655309242.0,"We are waiting for all of you, AI enthusiasts with coding experience and without, on the 24th - 26th of June to help you turn your ground-breaking ideas into reality!

Register here - [https://lablab.ai/event/gpt3-online](https://lablab.ai/event/gpt3-online)  


https://preview.redd.it/mhr93wgo6t591.png?width=1600&format=png&auto=webp&s=07e23c79830db8061eb300f76b64588b01219ebc"
248,deeplearning,openai,relevance,2022-02-11 16:47:38,Interview with Arvind Neelakantan about the OpenAI Embeddings API,HenryAILabs,False,1.0,2,sq3s05,https://www.reddit.com/r/deeplearning/comments/sq3s05/interview_with_arvind_neelakantan_about_the/,0,1644598058.0,"Hey everyone!   


The release of OpenAI's Embeddings API has been quite the story! I had the pleasure to interview Arvind Neelakantan on miscellaneous topics pertaining to these new advances in Search: [https://www.youtube.com/watch?v=uFxfZ0vLsoU](https://www.youtube.com/watch?v=uFxfZ0vLsoU)  


Additional Background on this:  
OpenAI Embeddings API Blog Post: [https://openai.com/blog/introducing-text-and-code-embeddings/](https://openai.com/blog/introducing-text-and-code-embeddings/)

Nils Reimers' Response (OpenAI GPT-3 Text Embeddings - Really a new state-of-the-art in dense text embeddings?): [https://medium.com/@nils\_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9](https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9)

Yannic Kilcher on the topic: [https://www.youtube.com/watch?v=5skIqoO3ku0](https://www.youtube.com/watch?v=5skIqoO3ku0)"
249,deeplearning,openai,relevance,2022-04-07 03:33:41,OpenAI's DALL·E 2 ! Text-to-Image Generation Explained,OnlyProggingForFun,False,0.75,2,ty3ysp,https://youtu.be/rdGVbPI42sA,1,1649302421.0,
250,deeplearning,openai,relevance,2021-12-21 16:58:03,OpenAI Gym Custom Environments Dynamically Changing Action Space,zhy1024,False,0.75,2,rlix7g,https://www.reddit.com/r/deeplearning/comments/rlix7g/openai_gym_custom_environments_dynamically/,0,1640105883.0," Hello everyone,

I'm currently doing a robotics grasping project using Reinforcement Learning. My agent's action space is discrete, but the issue is that for different states my action space may change as some actions are invalid for some states (valid action list for one state will be checked and given by some functions in my code), how can I fit my custom environment into openai gym format so that I can test some of their baselines algorithms?

Thank you in advance!"
251,deeplearning,openai,relevance,2021-07-15 19:36:18,OpenAI experiments in Minecraft and Coding,HenryAILabs,False,0.75,2,ol06ad,https://www.reddit.com/r/deeplearning/comments/ol06ad/openai_experiments_in_minecraft_and_coding/,0,1626377778.0,"Here are some ideas on how recent papers from OpenAI may come together: 

[https://www.youtube.com/watch?v=\_84MxLMpA3c](https://www.youtube.com/watch?v=_84MxLMpA3c)

Here are full explanations of the individual papers:

Evaluation of Large Language Models Trained on Code Data: 

[https://www.youtube.com/watch?v=1hJdBNYTNmQ](https://www.youtube.com/watch?v=1hJdBNYTNmQ)

Multi-Task Curriculum Learning in a Complex, Visual, Hard-Exploration Domain: Minecraft: 

[https://www.youtube.com/watch?v=aYBliqTTsGI](https://www.youtube.com/watch?v=aYBliqTTsGI)"
252,deeplearning,openai,relevance,2022-06-23 08:39:18,Deep dive into the OpenAI CLIP's code | Machine Learning Coding Series,gordicaleksa,False,0.67,1,vis6xu,https://youtu.be/jwZQD0Cqz4o,0,1655973558.0,
253,deeplearning,openai,relevance,2022-05-16 16:17:25,OpenAI GPT-3 & Codex Hackathon - Deep Learning Labs Stockholm,zakrzzz,False,1.0,2,uqzmxs,https://www.reddit.com/r/deeplearning/comments/uqzmxs/openai_gpt3_codex_hackathon_deep_learning_labs/,0,1652717845.0," Hello everyone!

Join us this weekend for the Deep Learning Hackathon in Stockholm! We are teaming up with WeWork and OpenAI to bring you an event focused on exploring the latest AI technologies: GPT-3 and Codex. This is a great opportunity to learn, build cool stuff, and meet interesting people. All levels of experience are welcome.

So if you are in Stockholm this weekend, we'll be happy to have you! Also if you know someone who might be interested in participating, let them know, I would be very grateful!

And if you won't be in Stockholm, you can watch the event live at [https://www.twitch.tv/deeplearninglabs](https://www.twitch.tv/deeplearninglabs) We will have some interesting Keynote sessions, fireside chat, and of course teams' demo presentations.

Register here: [https://sthlm.dllhack.com/](https://sthlm.dllhack.com/)

If you have any questions, I'll be happy to answer."
254,deeplearning,openai,relevance,2023-05-09 18:05:08,"Building with LLMs, ChatGPT, and Working at OpenAI With Logan Kilpatrick (Dev Rel @OpenAI) - What's AI episode 11",OnlyProggingForFun,False,0.54,1,13d13qo,https://youtu.be/zz4U3X3PD4s,0,1683655508.0,
255,deeplearning,openai,relevance,2021-06-19 09:55:10,Use OpenAI's CLIP for interesting usecases,adeshgautam,False,1.0,3,o3cjlm,https://www.reddit.com/r/deeplearning/comments/o3cjlm/use_openais_clip_for_interesting_usecases/,0,1624096510.0,"Check out my article if you find it interesting

  
[https://adeshg7.medium.com/build-your-own-search-engine-using-openais-clip-and-fastapi-part-1-89995aefbcdd](https://adeshg7.medium.com/build-your-own-search-engine-using-openais-clip-and-fastapi-part-1-89995aefbcdd)"
256,deeplearning,openai,relevance,2022-07-24 13:36:42,OpenAI GLIDE (Diffusion) | ML Coding series | Towards Photorealistic Image Generation and Editing,gordicaleksa,False,0.77,7,w6vst8,https://youtu.be/c1GwVg3lt1c,0,1658669802.0,
257,deeplearning,openai,relevance,2022-12-08 20:36:26,Daath AI Parser is an open-source application that uses OpenAI to parse visible text of HTML elements.,softcrater,False,0.92,10,zgarkj,https://github.com/kagermanov27/daath-ai-parser,0,1670531786.0,
258,deeplearning,openai,relevance,2021-08-19 04:40:51,Remaking OpenAI Codex space game with Baby Yoda,techn0_cratic,False,1.0,1,p780oe,https://youtu.be/bqt29KmiTpU,0,1629348051.0,
259,deeplearning,openai,relevance,2022-10-12 20:21:40,"I've built an Auto Subtitled Video Generator using Streamlit and OpenAI Whisper, hosted on HuggingFace spaces.",Batuhan_Y,False,0.93,22,y2edmn,https://www.reddit.com/r/deeplearning/comments/y2edmn/ive_built_an_auto_subtitled_video_generator_using/,0,1665606100.0,"All you have to do is input a YouTube video link and get a video with subtitles (alongside with .txt, .vtt, .srt files).

Whisper can translate 98 different languages to English. If you want to give it a try;

Link of the app: [https://huggingface.co/spaces/BatuhanYilmaz/Auto-Subtitled-Video-Generator](https://huggingface.co/spaces/BatuhanYilmaz/Auto-Subtitled-Video-Generator)

&#x200B;

https://reddit.com/link/y2edmn/video/r49plzsgoft91/player"
260,deeplearning,openai,relevance,2021-07-28 17:45:57,"OpenAI Releases Triton, An Open-Source Python-Like GPU Programming Language For Neural Networks",techsucker,False,0.95,70,otf0fs,https://www.reddit.com/r/deeplearning/comments/otf0fs/openai_releases_triton_an_opensource_pythonlike/,5,1627494357.0,"OpenAI released their newest language, [Triton](https://github.com/openai/triton). This open-source programming language that enables researchers to write highly efficient GPU code for AI workloads is Python-compatible and comes with the ability of a user to write in as few as 25 lines, something on par with what an expert could achieve. OpenAI claims this makes it possible to reach peak hardware performance without much effort, making creating more complex workflows easier than ever before!

Researchers in the field of Deep Learning often rely on native framework operators. However, this can be problematic because it requires many temporary tensors to work, which may hurt performance at scale for neural networks. Writing specialized GPU kernels is a more convenient solution, but surprisingly difficult due to intricacies when programming them according to GPUs. It was challenging to find a system that provides the flexibility and speed required while also being easy enough for developers to understand. This has led researchers at OpenAI in improving Triton, which was initially founded by one of their teammates.

Quick Read: [https://www.marktechpost.com/2021/07/28/openai-releases-triton-an-open-source-python-like-gpu-programming-language-for-neural-networks/](https://www.marktechpost.com/2021/07/28/openai-releases-triton-an-open-source-python-like-gpu-programming-language-for-neural-networks/) 

Paper: http://www.eecs.harvard.edu/\~htk/publication/2019-mapl-tillet-kung-cox.pdf

Github: https://github.com/openai/triton"
261,deeplearning,openai,relevance,2021-07-16 11:44:12,OpenAI Codex shows the limits of large language models,bendee983,False,0.76,2,olf7u3,https://bdtechtalks.com/2021/07/15/openai-codex-ai-programming/,0,1626435852.0,
262,deeplearning,openai,relevance,2023-12-24 18:24:47,Q*,sunnymorgue,False,0.18,0,18q0qc1,https://www.reddit.com/r/deeplearning/comments/18q0qc1/q/,17,1703442287.0,"Got a question, a lot of people on this board dumb down OpenAI's Q\* to just reinforcement learning; if that's the case then why did Sam get outed of OpenAI temporarily? If it were just RL then there would be no reason to fire him over some insignificant thing. Making premature remarks like that is pretty crazy if you ask me, just saying. "
263,deeplearning,openai,relevance,2021-07-06 16:56:22,What OpenAI and GitHub’s “AI pair programmer” means for the software industry,bendee983,False,0.9,29,oez127,https://bdtechtalks.com/2021/07/05/openai-github-gpt-3-copilot/,6,1625590582.0,
264,deeplearning,openai,relevance,2021-01-13 20:28:24,OpenAI CLIP: ConnectingText and Images (Paper Explained),myyoucef,False,0.93,12,kwp46g,https://www.youtube.com/watch?v=T9XSU0pKX2E,0,1610569704.0,
265,deeplearning,openai,relevance,2023-02-01 09:00:18,"Python wrapper of OpenAI's New AI classifier tool, which detects whether the paragraph was generated by ChatGPT, GPT models, or written by humans",StoicBatman,False,0.86,19,10qouv9,https://www.reddit.com/r/deeplearning/comments/10qouv9/python_wrapper_of_openais_new_ai_classifier_tool/,3,1675242018.0,"OpenAI has developed a new AI classifier tool which detects whether the content (paragraph, code, etc.) was generated by #ChatGPT, #GPT-based large language models or written by humans.

Here is a python wrapper of openai model to detect if a text is written by humans or generated by ChatGPT, GPT models

Github: [https://github.com/promptslab/openai-detector](https://github.com/promptslab/openai-detector)  
Openai release: [https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/](https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/)

If you are interested in #PromptEngineering, #LLMs, #ChatGPT and other latest research discussions, please consider joining our discord [discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)  


https://preview.redd.it/9d8ooeg2ljfa1.png?width=1358&format=png&auto=webp&s=0de7ccc5cd16d6bbad3dcfe7d8441547a6196fc8"
266,deeplearning,openai,relevance,2021-12-29 02:58:00,OpenAI GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,gordicaleksa,False,1.0,2,rqxc0g,https://www.youtube.com/watch?v=lvv4N2nf-HU,1,1640746680.0,
267,deeplearning,openai,relevance,2021-11-01 14:33:01,"[R] Warsaw U, OpenAI and Google’s Hourglass Hierarchical Transformer Model Outperforms Transformer Baselines",Yuqing7,False,0.85,13,qkf9xu,https://www.reddit.com/r/deeplearning/comments/qkf9xu/r_warsaw_u_openai_and_googles_hourglass/,2,1635777181.0,"A team from the University of Warsaw, OpenAI and Google Research proposes Hourglass, a hierarchical transformer language model that operates on shortened sequences to alleviate transformers’ huge computation burdens. 

Here is a quick read: [Warsaw U, OpenAI and Google’s Hourglass Hierarchical Transformer Model Outperforms Transformer Baselines.](https://syncedreview.com/2021/11/01/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-135/)

The paper *Hierarchical Transformers Are More Efficient Language Models* is on [arXiv](https://arxiv.org/abs/2110.13711)."
268,deeplearning,openai,relevance,2021-08-10 22:11:44,Demo - Improved OpenAI Codex that translates natural language to code,rshpkamil,False,0.86,10,p1zkqp,https://www.reddit.com/r/deeplearning/comments/p1zkqp/demo_improved_openai_codex_that_translates/,0,1628633504.0,https://openai.com/blog/openai-codex/
269,deeplearning,openai,relevance,2020-09-15 18:13:05,[R] Human Feedback Improves OpenAI Model Summarizations,Yuqing7,False,1.0,1,ite8ym,https://www.reddit.com/r/deeplearning/comments/ite8ym/r_human_feedback_improves_openai_model/,0,1600193585.0,"In a new paper, a team of OpenAI researchers sets out to advance methods for training large-scale language models such as BERT on objectives that more closely capture human preferences — and does so by putting humans back into the loop. The work focuses on abstractive English text summarization — a subjective task that’s considered challenging because the notion of what makes a “good summary” is difficult to capture without human input.

Here is a quick read: [Human Feedback Improves OpenAI Model Summarizations](https://syncedreview.com/2020/09/15/human-feedback-improves-openai-model-summarizations/)

The paper *Learning to Summarize from Human Feedback* is on [arXiv](https://arxiv.org/pdf/2009.01325.pdf)."
270,deeplearning,openai,relevance,2020-11-16 12:57:16,"OpenAI RL workshop, blog link in the comments",OneUpWallStreet,False,0.5,0,jv6axz,https://youtu.be/fdY7dt3ijgY,1,1605531436.0,
271,deeplearning,openai,relevance,2024-01-31 12:26:30,Become an AI Developer (Free 9 Part Series),Competitive_data786,False,1.0,4,1afgp2r,https://www.reddit.com/r/deeplearning/comments/1afgp2r/become_an_ai_developer_free_9_part_series/,0,1706703990.0,"Just sharing a free series I stumbled across on Linkedin - DataCamp's 9-part AI code-along series.

This specific session linked below is ""Building Chatbots with OpenAI API and Pinecone"" but there are 8 others to have a look at and code along to.

*Start from basics to build on skills with GPT, Pinecone and LangChain to create a chatbot that answers questions about research papers. Make use of retrieval augmented generation, and learn how to combine this with conversational memory to hold a conversation with the chatbot. Code Along on DataCamp Workspace:* [*https://www.datacamp.com/code-along/building-chatbots-openai-api-pinecone*](https://www.datacamp.com/code-along/building-chatbots-openai-api-pinecone)

Find all of the sessions at: [https://www.datacamp.com/ai-code-alongs](https://www.datacamp.com/ai-code-alongs)"
272,deeplearning,openai,relevance,2022-04-12 13:21:41,DALL.E 2 by OpenAI is giving very mind blowing results for image generation from text,imapurplemango,False,0.67,4,u1yzgi,https://www.qblocks.cloud/blog/openai-dall-e-2-generate-images-from-text,0,1649769701.0,
273,deeplearning,openai,relevance,2022-05-11 18:43:47,Nftopia.ai - Visual semantic search for NFTs using OpenAI's CLIP (x-post /r/MachineLearning),hayAbhay,False,0.57,1,ungtzb,https://www.reddit.com/r/deeplearning/comments/ungtzb/nftopiaai_visual_semantic_search_for_nfts_using/,0,1652294627.0,"Hi! My colleagues & I indexed about 1.1 million NFT images from across 21 thousand collections using OpenAI's CLIP and put it behind a website. Please let us know what you think!

Specifically, we embed all these images using clip & expose two functionalities. The search bar embeds the input query & retrieves similar images. The ""+more like this"" is image search that retrieves other NFTs similar to it in the image space. We use Pinecone to power the approximate nearest neighbor search. The web stack uses Django in the front + flask app that interfaces with Pinecone.

(Warning: we've noticed several NSFW NFTs that can pop up occasionally even for unrelated search queries)"
274,deeplearning,openai,relevance,2021-12-24 15:48:32,[R] OpenAI Releases GLIDE: A Scaled-Down Text-to-Image Model That Rivals DALL-E Performance,Yuqing7,False,0.91,24,rnovk0,https://www.reddit.com/r/deeplearning/comments/rnovk0/r_openai_releases_glide_a_scaleddown_texttoimage/,1,1640360912.0,"An OpenAI research team proposes GLIDE (Guided Language-to-Image Diffusion for Generation and Editing) for high-quality synthetic image generation. Human evaluators prefer GLIDE samples over DALL-E’s, and the model size is much smaller (3.5 billion vs. 12 billion parameters). 

Here is a quick read: [OpenAI Releases GLIDE: A Scaled-Down Text-to-Image Model That Rivals DALL-E Performance.](https://syncedreview.com/2021/12/24/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-173/)

The paper *GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models* is on [arXiv](https://arxiv.org/abs/2112.10741)."
275,deeplearning,openai,relevance,2020-05-02 14:55:05,Video Analysis of the OpenAI Jukebox Paper,ykilcher,False,1.0,2,gc7rf5,https://www.reddit.com/r/MachineLearning/comments/gc4qjr/d_video_analysis_jukebox_a_generative_model_for/?utm_medium=android_app&utm_source=share,0,1588431305.0,
276,deeplearning,openai,relevance,2021-01-08 16:12:03,OpenAI's CLIP method explained! (CLIP > DALL-E change my mind),gordicaleksa,False,0.71,6,kt5k3z,https://youtu.be/fQyHEXZB-nM,2,1610122323.0,
277,deeplearning,openai,relevance,2020-09-11 15:37:20,[R] OpenAI ‘GPT-f’ Delivers SOTA Performance in Automated Mathematical Theorem Proving,Yuqing7,False,0.95,44,iqsul7,https://www.reddit.com/r/deeplearning/comments/iqsul7/r_openai_gptf_delivers_sota_performance_in/,2,1599838640.0,"San Francisco-based AI research laboratory OpenAI has added another member to its popular GPT (Generative Pre-trained Transformer) family. In a new paper, OpenAI researchers introduce GPT-f, an automated prover and proof assistant for the Metamath formalization language.

Here is a quick read: [OpenAI ‘GPT-f’ Delivers SOTA Performance in Automated Mathematical Theorem Proving](https://syncedreview.com/2020/09/10/openai-gpt-f-delivers-sota-performance-in-automated-mathematical-theorem-proving/)

The paper *Generative Language Modeling for Automated Theorem Proving* is on [arXiv](https://arxiv.org/pdf/2009.03393.pdf)."
278,deeplearning,openai,relevance,2020-01-30 18:53:20,OpenAI goes all-in on Facebook's Pytorch machine learning framework,emptyplate,False,0.94,64,ewa9jc,https://venturebeat.com/2020/01/30/openai-facebook-pytorch-google-tensorflow/,0,1580410400.0,
279,deeplearning,openai,relevance,2021-09-03 15:34:29,"Tutorial: Visual Introduction to Deep Reinforcement Learning with OpenAI Gym, Google Colab, and RLlib",mgalarny,False,0.86,5,ph7jzs,https://towardsdatascience.com/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google-colab-48fc1ddfb889,0,1630683269.0,
280,deeplearning,openai,relevance,2020-06-16 07:35:17,GPT-3 from OpenAI is here [video review],przemekc,False,0.33,0,h9z59j,https://youtu.be/OznMk5Jexu8,0,1592292917.0,
281,deeplearning,openai,relevance,2020-03-17 08:40:17,[P] Simple PyTorch Implementation of OpenAI GPT-1,lyeoni,False,0.9,7,fk1tog,https://www.reddit.com/r/deeplearning/comments/fk1tog/p_simple_pytorch_implementation_of_openai_gpt1/,2,1584434417.0,"Hello :)

Most  of OpenAI GPT codes are for version 2 (GPT-2), and GPT-2 is hard to use  restricted small GPU resources. So, I implemented GPT-1 for someone who  wants to pre-train/fine-tune with very small resources, like me.  Because GPT-1 requires relatively small resources.

And, I also added belows.

* training logs for pre-training on WikiText-103, fine-tuning on IMDb.
* results for the question : *Does auxiliary objective function have a bigger impact?*

I hope that this repo can be a good solution for people who want to train GPT model with restricted resources.

\[LINK\] : [https://github.com/lyeoni/gpt-pytorch](https://github.com/lyeoni/gpt-pytorch)"
282,deeplearning,openai,relevance,2022-10-01 11:22:32,I filmed myself speaking in 5 languages and then I used OpenAI's Whisper to automatically transcribe and translate the audio into English!,gordicaleksa,False,0.67,4,xssucm,https://youtube.com/shorts/xIbyJhVCmsk?feature=share,0,1664623352.0,
283,deeplearning,openai,relevance,2019-10-15 20:37:36,Explaining OpenAI's Robotic Hand Rubik's Cube Solver!,HenryAILabs,False,0.6,2,die18r,https://www.reddit.com/r/deeplearning/comments/die18r/explaining_openais_robotic_hand_rubiks_cube_solver/,1,1571171856.0,"This video will explain some of the details behind the amazing research study from OpenAI using a Meta-Learning Automatic Domain Randomization algorithm to bridge the Sim2Real gap and solve a Rubik's Cube with a Robotic Hand!!

https://youtu.be/2AqGocPOOG4"
284,deeplearning,openai,relevance,2021-03-22 08:20:29,OpenAI - Solving Rubik's Cube with a Robot Hand | Paper explained,gordicaleksa,False,0.75,2,maie1g,https://youtu.be/eTa-k1pgvnU,0,1616401229.0,
285,deeplearning,openai,relevance,2021-07-24 12:15:43,OpenAI's New Code Generator: GitHub Copilot (and Codex) | This AI Generates Code From Words,OnlyProggingForFun,False,0.17,0,oqov6e,https://youtu.be/az3oVVkTFB8,3,1627128943.0,
286,deeplearning,openai,relevance,2018-11-14 15:56:16,OpenAI Founder: Short-Term AGI Is a Serious Possibility,gwen0927,False,0.87,26,9x1aqi,https://medium.com/syncedreview/openai-founder-short-term-agi-is-a-serious-possibility-368424f7462f,5,1542210976.0,
287,deeplearning,openai,relevance,2021-02-18 14:52:00,The world's largest scale Turing Test / Do you think OpenAI's GPT3 is good enough to pass the Turing Test?,theaicore,False,0.93,34,lmog2d,https://www.theaicore.com/imitationgame?utm_source=reddit,11,1613659920.0,
288,deeplearning,openai,relevance,2021-07-13 15:38:39,[R] OpenAI Fine-Tunes GPT-3 to Unlock Its Code Generation Potential for Difficult Problems,Yuqing7,False,0.75,2,oji5zu,https://www.reddit.com/r/deeplearning/comments/oji5zu/r_openai_finetunes_gpt3_to_unlock_its_code/,0,1626190719.0,"A research team from OpenAI proposes Codex, a specialized GPT model fine-tuned on publicly available code from GitHub that can produce functionally correct Python code bodies from natural language docstrings and could excel at a variety of coding tasks. 

Here is a quick read: [OpenAI Fine-Tunes GPT-3 to Unlock Its Code Generation Potential for Difficult Problems.](https://syncedreview.com/2021/07/13/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-60/)

The paper *Evaluating Large Language Models Trained on Code* is on [arXiv](https://arxiv.org/abs/2107.03374)."
289,deeplearning,openai,relevance,2020-08-05 10:58:06,image-GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model,OnlyProggingForFun,False,0.94,95,i437pt,https://www.youtube.com/watch?v=FwXQ568_io0,6,1596625086.0,
290,deeplearning,openai,relevance,2021-01-07 05:25:23,OpenAI Introduces DALL·E: A Neural Network That Creates Images From Text Descriptions,ai-lover,False,0.92,11,ks6jag,https://www.marktechpost.com/2021/01/06/openai-introduces-dall%C2%B7e-a-neural-network-that-creates-images-from-text-descriptions,0,1609997123.0,
291,deeplearning,openai,relevance,2021-02-03 09:04:49,"Computer vision is leveling up with CLIP, a new neural network by OpenAI",techn0_cratic,False,0.67,1,lbjlgi,https://youtu.be/mKbEofzhHNY,0,1612343089.0,
292,deeplearning,openai,relevance,2021-07-08 04:51:47,"Exploration of GitHub Copilot, OpenAI Codex-based AI coding assistant that translates natural language into code",techn0_cratic,False,0.76,22,og08xq,https://youtu.be/GTG_bcFdcLQ,0,1625719907.0,
293,deeplearning,openai,relevance,2019-02-27 11:37:45,My Thoughts on OpenAI Not Releasing Weights,ericnyamu,False,0.72,8,avcjo4,https://www.mawazoforum.com/viewtopic.php?f=40&t=437,0,1551267465.0,
294,deeplearning,openai,relevance,2021-01-06 00:55:05,OpenAI successfully trained a network able to generate images from text captions: DALL·E,OnlyProggingForFun,False,0.84,8,krcfkg,https://youtu.be/nLzfDVwQxRU,0,1609894505.0,
295,deeplearning,openai,relevance,2019-11-23 11:12:37,OpenAI releases Safety Gym for reinforcement learning | VentureBeat,RankLord,False,1.0,4,e0glop,https://venturebeat.com/2019/11/21/openai-safety-gym/,0,1574507557.0,
296,deeplearning,openai,relevance,2021-01-25 19:31:24,OpenAI's DALL-E Alternatives with Colab Code - Deep Daze & Big Sleep [OG],dulldata,False,1.0,3,l4vfru,https://youtu.be/lVR5kN7SjQ8,0,1611603084.0,
297,deeplearning,openai,relevance,2019-04-10 19:21:52,Creating a Custom OpenAI Gym Environment for Stock Trading,notadamking,False,0.91,9,bbq5mi,https://medium.com/@adamjking3/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e,0,1554924112.0,
298,deeplearning,openai,relevance,2019-07-28 13:31:09,Fine Tuning OpenAI's GPT-2 to Generate Jeopardy Questions,Automato-YT,False,0.75,2,civu6r,https://www.youtube.com/watch?v=DBJRAwnOJoI,0,1564320669.0,
299,deeplearning,openai,relevance,2019-02-14 17:19:15,OpenAI Guards Its ML Model Code & Data to Thwart Malicious Usage,gwen0927,False,0.93,21,aqm35j,https://medium.com/syncedreview/openai-guards-its-ml-model-code-data-to-thwart-malicious-usage-d9f7e9c43cd0,4,1550164755.0,
