,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,id,url,num_comments,created,body
0,artificial,gpt-3,top,2023-12-23 12:31:57,The most remarkable AI releases of 2023,alina_valyaeva,False,0.93,672,18p4qwb,https://i.redd.it/1ues5xc8g18c1.png,95,1703334717.0,
1,artificial,gpt-3,top,2020-08-19 20:42:00,List of free sites/programs that are powered by GPT-3 and can be used now without a waiting list,Wiskkey,False,1.0,393,icvypl,https://www.reddit.com/r/artificial/comments/icvypl/list_of_free_sitesprograms_that_are_powered_by/,92,1597869720.0,"**Update (March 23, 2021)**: I won't be adding new items to this list. There are other lists of GPT-3 projects [here](https://medium.com/cherryventures/lets-review-productized-gpt-3-together-aeece64343d7), [here](https://gpt3demo.com/), [here](https://gptcrush.com/), and [here](https://www.producthunt.com/search?q=%22gpt3%22). You may also be interested in subreddit r/gpt3.

These are free GPT-3-powered sites/programs that can be used now without a waiting list:

1. [AI Dungeon](https://play.aidungeon.io/) with Griffin model ([limited free usage](https://blog.aidungeon.io/2020/11/07/ai-energy-update/)) in settings: text adventure game; use Custom game to create your own scenarios; Griffin uses ""the second largest version of GPT-3) according to information in [this post](https://www.reddit.com/r/MachineLearning/comments/inh6uc/d_how_many_parameters_are_in_the_gpt3_neural_net/); note: [AI  Dungeon creator states how AI Dungeon tries to prevent backdoor access  to the GPT-3 API, and other differences from the GPT-3 API](https://www.reddit.com/r/slatestarcodex/comments/i2s83g/ai_dungeon_creator_states_how_ai_dungeon_tries_to/)
2. [GPT-Startup: free GPT-3-powered site that generates ideas for new businesses](https://www.reddit.com/r/GPT3/comments/ingmdr/gptstartup_free_gpt3powered_site_that_generates/)
3. [IdeasAI: free GPT-3-powered site that generates ideas for new businesses](https://www.reddit.com/r/GPT3/comments/ioe5j1/ideasai_free_gpt3powered_site_that_generates/)
4. [Activechat.ai](https://www.reddit.com/r/GPT3/comments/ilyq6m/gpt3_for_live_chat_do_you_think_it_brings_value/) (free usage of functionality that demonstrates technology available to potential paid customers): GPT-3-supplied customer reply suggestions for human customer service agents

Trials: These GPT-3-powered sites/programs have free trials that can be used now without a waiting list:

1. [AI Dungeon](https://play.aidungeon.io/) with Dragon model in settings (free for first 7 days): text adventure game; use Custom game to create your own scenarios; note: [AI Dungeon creator states how AI Dungeon tries to prevent backdoor access to the GPT-3 API, and other differences from the GPT-3 API](https://www.reddit.com/r/slatestarcodex/comments/i2s83g/ai_dungeon_creator_states_how_ai_dungeon_tries_to/)
2. [Taglines: create taglines for products](https://www.reddit.com/r/GPT3/comments/i593e4/gpt3_app_taglinesai/) (5 free queries per email address per month)
3. [Blog Idea Generator: a free GPT-3-powered site that generates ideas for new blog posts](https://www.reddit.com/r/GPT3/comments/j0a9yr/blog_idea_generator_a_free_gpt3powered_site_that/); the full generated idea is a paid feature; there is a maximum number of free ideas generated per day
4. [Shortly](https://www.reddit.com/r/GPT3/comments/j7tmyy/does_anyone_know_if_the_app_shortly_uses_gpt3_if/): writing assistant (2 free generations per email address on website; purportedly a 7 day trial via app)
5. [CopyAI: GPT-3-powered generation of ad copy for products](https://www.reddit.com/r/GPT3/comments/jclu16/copyai_gpt3powered_generation_of_ad_copy_for/)
6. [Copysmith - GPT-3-powered generation of content marketing](https://www.reddit.com/r/GPT3/comments/jjtfec/copysmith_gpt3powered_generation_of_content/)
7. [Virtual Ghost Writer: AI copy writer powered by GPT-3](https://www.reddit.com/r/GPT3/comments/jyok1a/virtual_ghost_writer_ai_copy_writer_powered_by/): writing assistant that completes thoughts (3 free generations per email address); seems to work well with incomplete sentences
8. [MagicFlow: GPT-3-powered content marketing assistant](https://www.reddit.com/r/GPT3/comments/jzklmt/magicflow_gpt3powered_content_marketing_assistant/)
9. [Snazzy AI: GPT-3-powered business-related content creation](https://www.reddit.com/r/GPT3/comments/jzntxj/snazzy_ai_gpt3powered_businessrelated_content/)
10. [HelpHub: knowledge base site creator with GPT-3-powered article creation](https://www.reddit.com/r/GPT3/comments/k0abwe/helphub_knowledge_base_site_creator_with/)
11. [GPT-3 AI Writing Tools](https://aicontentdojo.com/the-best-gpt-3-ai-writing-tool-on-the-market-shortlyai/)

Removed items: Sites that were once in the above lists but have been since been removed:

1. [Thoughts](https://www.reddit.com/r/MachineLearning/comments/hs9zqo/p_gpt3_aigenerated_tweets_indistinguishable_from/): Tweet-sized thoughts based upon a given word or phrase; removed because [its developer changed how it works](https://www.reddit.com/r/artificial/comments/icvypl/list_of_free_sitesprograms_that_are_powered_by/g4but3n/)
2. [Chat with GPT-3 Grandmother: a free GPT-3-powered chatbot](https://www.reddit.com/r/GPT3/comments/ipzdki/chat_with_gpt3_grandmother_a_free_gpt3powered/); removed because site now has a waitlist
3. [Simplify.so: a free GPT-3 powered site for simplifying complicated subjects](https://www.reddit.com/r/MachineLearning/comments/ic8o0k/p_simplifyso_a_free_gpt3_powered_site_for/); removed because no longer available
4. [Philosopher AI: Interact with a GPT-3-powered philosopher persona for free](https://www.reddit.com/r/MachineLearning/comments/icmpvl/p_philosopher_ai_interact_with_a_gpt3powered/); removed because now is available only as a paid app
5. [Serendipity: A GPT-3-powered product recommendation engine that also lets one use GPT-3 in a limited manner for free](https://www.reddit.com/r/MachineLearning/comments/i0m6vs/p_a_website_that_lets_one_use_gpt3_in_a_limited/); removed because doing queries not done by anybody else before now apparently is a paid feature
6. [FitnessAI Knowledge: Ask GPT-3 health-related or fitness-related questions for free](https://www.reddit.com/r/MachineLearning/comments/iacm31/p_ask_gpt3_healthrelated_or_fitnessrelated/); removed because it doesn't work anymore
7. [Itemsy](https://www.reddit.com/r/GPT3/comments/ja81ui/quickchat_a_gpt3powered_customizable/): a free product-specific chat bot which is an implementation of a knowledge-based chat bot from Quickchat; removed because I don't see the chat bot anymore
8. [The NLC2CMD Challenge site has a GPT-3-powered English to Bash Unix command line translator](https://www.reddit.com/r/GPT3/comments/jl1aa6/the_nlc2cmd_challenge_site_has_a_gpt3powered/); removed because GPT-3 access apparently is no longer available to the public
9. [GiftGenius: a site with a free GPT-3-powered gift recommendation engine](https://www.reddit.com/r/GPT3/comments/k1s0iw/giftgenius_a_site_with_a_free_gpt3powered_gift/); removed because site is no longer available
10. [Job Description Rewriter](https://www.reddit.com/r/GPT3/comments/ik03zr/job_description_rewriter/); removed because site is no longer available."
2,artificial,gpt-3,top,2023-02-27 18:46:57,"Last weekend I made a Google Sheets plugin that uses GPT-3 to answer questions, format cells, write letters, and generate formulas, all without having to leave your spreadsheet",rtwalz,False,0.98,368,11dje8t,https://v.redd.it/9xnevfl31ska1,17,1677523617.0,
3,artificial,gpt-3,top,2023-11-08 15:36:56,Is Microsoft’s Copilot really worth $30/month?,ConsciousInsects,False,0.94,316,17qo9gj,https://www.reddit.com/r/artificial/comments/17qo9gj/is_microsofts_copilot_really_worth_30month/,181,1699457816.0," 

Just read an [article](https://www.cnbc.com/amp/2023/11/01/microsoft-365-copilot-becomes-generally-available.html) about Microsoft's new AI add-on for Office called Microsoft 365 Copilot. The tool integrates with Word, Excel, and other Office programs, and supposedly makes work seamless. It's even being used by some big names like Bayer, KPMG, and Visa. The tool targets businesses and is believed to generate over $10 billion in revenue by 2026.

But I can't help but think the price is a bit steep. It’s $30 per month, which is cheap for large companies, but what about freelancers and regular individuals? The article also mentions that there isn't a lot of data on how Copilot affects performance yet, and there are some concerns about the accuracy of the AI-generated responses.

Plus, it's only available to Enterprise E3 customers with more than 300 employees. So not only is it pricey, but it's also not accessible to most people or small businesses and might never be.

Would love to hear your thoughts on this. I’m already pretty sick of subscription based models but is $30/month even justified? For comparison these are other comparative AI services:

1.  ChatGPT - Free for basic chat. $20 for GPT 4, for anything serious.

2.  Bardeen - $15 and offers general automations.

3.  Silatus - At $14, it's the cheapest legitimate option I’ve found for GPT-4 chat and research.

4.  Perplexity - This one's decent for free search.

These are the ones I know, if you wanna add more comparisons, feel free to do so. But I think Microsoft is pricing out a lot of its potential users with their monthly demand."
4,artificial,gpt-3,top,2023-05-07 21:36:07,Early Alpha Access To GPT-4 With Browsing,Frankenmoney,False,0.95,284,13b3oop,https://i.redd.it/3dge2wwaahya1.png,78,1683495367.0,
5,artificial,gpt-3,top,2023-03-15 13:13:19,GPT-4 shows emergent Theory of Mind on par with an adult. It scored in the 85+ percentile for a lot of major college exams. It can also do taxes and create functional websites from a simple drawing,lostlifon,False,0.89,253,11rvzgg,https://www.reddit.com/gallery/11rvzgg,164,1678885999.0,
6,artificial,gpt-3,top,2023-05-20 20:40:56,Tree of LifeGPT-4 reasoning Improved 900%.,Department_Wonderful,False,0.95,254,13n7zqn,https://www.reddit.com/r/artificial/comments/13n7zqn/tree_of_lifegpt4_reasoning_improved_900/,136,1684615256.0,"I just watched this video, and I wanted to share it with the group. I want to see what you think about this? Have a great night. 

https://youtu.be/BrjAt-wvEXI

Tree of Thoughts (ToT) is a new framework for language model inference that generalizes over the popular “Chain of Thought” approach to prompting language models¹. It enables exploration over coherent units of text (“thoughts”) that serve as intermediate steps toward problem solving¹. ToT allows language models to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices¹.

Our experiments show that ToT significantly enhances language models’ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords¹. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%¹.

Is there anything else you would like to know about Tree of Thoughts GPT-4?

Source: Conversation with Bing, 5/20/2023
(1) Tree of Thoughts: Deliberate Problem Solving with Large Language Models. https://arxiv.org/pdf/2305.10601.pdf.
(2) Tree of Thoughts - GPT-4 Reasoning is Improved 900% - YouTube. https://www.youtube.com/watch?v=BrjAt-wvEXI.
(3) Matsuda Takumi on Twitter: ""GPT-4でTree of Thoughtsというフレームワークを使って、Game .... https://twitter.com/matsuda_tkm/status/1659720094866620416.
(4) GPT-4 And The Journey Towards Artificial Cognition. https://johnnosta.medium.com/gpt-4-and-the-journey-towards-artificial-cognition-bcba6dfa7648."
7,artificial,gpt-3,top,2023-01-07 22:57:57,Invent 5 new things that don't already exist that humans couldn't live without,Imagine-your-success,False,0.93,208,1062d2k,https://i.redd.it/ambdpghlbpaa1.png,38,1673132277.0,
8,artificial,gpt-3,top,2023-01-25 12:02:16,Being really humorous under the pressure of billions of prompt requests,Imagine-your-success,False,0.99,195,10kx251,https://i.redd.it/bq74v5g5j6ea1.png,9,1674648136.0,
9,artificial,gpt-3,top,2022-10-11 16:19:39,"I was tired of spending hours researching products online, so I built a site that analyzes Reddit posts and comments to find the most popular products using BERT models and GPT-3.",madredditscientist,False,0.97,191,y1d8jh,https://v.redd.it/9lyurwvdc7t91,18,1665505179.0,
10,artificial,gpt-3,top,2021-09-15 14:01:16,GPT-3 Chat Bot Falls For It,blackmidifan1,False,0.82,184,poqplr,https://i.redd.it/zon2a68dbon71.jpg,14,1631714476.0,
11,artificial,gpt-3,top,2024-01-22 10:25:11,What is GPT-5? Here are Sam’s comments at the Davos Forum,Stupid_hardcorer,False,0.93,162,19csm2e,https://www.reddit.com/r/artificial/comments/19csm2e/what_is_gpt5_here_are_sams_comments_at_the_davos/,51,1705919111.0,"After listening to about 4-5 lectures by Sam Altman at the Davos Forum, I gathered some of his comments about GPT-5 (not verbatim). I think we can piece together some insights from these fragments:

&#x200B;

* ""The current GPT-4 has too many shortcomings; it's much worse than the version we will have this year and even more so compared to next year’s.""

&#x200B;

* ""If GPT-4 can currently solve only 10% of human tasks, GPT-5 should be able to handle 15% or 20%.""

&#x200B;

* ""The most important aspect is not the specific problems it solves, but the increasing general versatility.""

&#x200B;

* ""More powerful models and how to use existing models effectively are two multiplying factors, but clearly, the more powerful model is more important.""

&#x200B;

* ""Access to specific data and making AI more relevant to practical work will see significant progress this year. Current issues like slow speed and lack of real-time processing will improve. Performance on longer, more complex problems will become more precise, and the ability to do more will increase.""

&#x200B;

* ""I believe the most crucial point of AI is the significant acceleration in the speed of scientific discoveries, making new discoveries increasingly automated. This isn’t a short-term matter, but once it happens, it will be a big deal.""

&#x200B;

* ""As models become smarter and better at reasoning, we need less training data. For example, no one needs to read 2000 biology textbooks; you only need a small portion of extremely high-quality data and to deeply think and chew over it. The models will work harder on thinking through a small portion of known high-quality data.""

&#x200B;

* ""The infrastructure for computing power in preparation for large-scale AI is still insufficient.""

&#x200B;

* ""GPT-4 should be seen as a preview with obvious limitations. Humans inherently have poor intuition about exponential growth. If GPT-5 shows significant improvement over GPT-4, just as GPT-4 did over GPT-3, and the same for GPT-6 over GPT-5, what would that mean? What does it mean if we continue on this trajectory?""

&#x200B;

* ""As AI becomes more powerful and possibly discovers new scientific knowledge, even automatically conducting AI research, the pace of the world's development will exceed our imagination. I often tell people that no one knows what will happen next. It's important to stay humble about the future; you can predict a few steps, but don't make too many predictions.""

&#x200B;

* ""What impact will it have on the world when cognitive costs are reduced by a thousand or a million times, and capabilities are greatly enhanced? What if everyone in the world owned a company composed of 10,000 highly capable virtual AI employees, experts in various fields, tireless and increasingly intelligent? The timing of this happening is unpredictable, but it will continue on an exponential growth line. How much time do we have to prepare?""

&#x200B;

* ""I believe smartphones will not disappear, just as smartphones have not replaced PCs. On the other hand, I think AI is not just a simple computational device like a phone plus a bunch of software; it might be something of greater significance."""
12,artificial,gpt-3,top,2023-11-21 14:23:15,Bigger is better,OmOshIroIdEs,False,0.94,159,180i48g,https://i.redd.it/yvymesjbnp1c1.jpg,15,1700576595.0,
13,artificial,gpt-3,top,2023-02-02 23:13:04,"Creating ""Her"" using GPT-3 & TTS trained on voice from movie",justLV,False,0.96,152,10s43in,https://twitter.com/justLV/status/1621253007492141056,15,1675379584.0,
14,artificial,gpt-3,top,2022-12-20 21:28:12,"Deleted tweet from Rippling co-founder: Microsoft is all-in on GPT. GPT-4 10x better than 3.5(ChatGPT), clearing turing test and any standard tests.",Sebrosen1,False,0.93,140,zr08re,https://twitter.com/AliYeysides/status/1605258835974823954,159,1671571692.0,
15,artificial,gpt-3,top,2023-07-08 19:47:50,OpenAI and Microsoft Sued for $3 Billion Over Alleged ChatGPT 'Privacy Violations',trueslicky,False,0.95,132,14udidi,https://www.vice.com/en/article/wxjxgx/openai-and-microsoft-sued-for-dollar3-billion-over-alleged-chatgpt-privacy-violations,76,1688845670.0,
16,artificial,gpt-3,top,2021-10-11 15:36:24,"Microsoft, Nvidia team released world’s largest dense language model. With 530 Billion parameters, it is 3x larger than GPT-3",Dr_Singularity,False,0.98,133,q5yikm,https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/,25,1633966584.0,
17,artificial,gpt-3,top,2023-07-24 14:33:34,Free courses and guides for learning Generative AI,wyem,False,0.97,130,158cegb,https://www.reddit.com/r/artificial/comments/158cegb/free_courses_and_guides_for_learning_generative_ai/,16,1690209214.0,"1. **Generative AI learning path by Google Cloud.** A series of 10 courses on generative AI products and technologies, from the fundamentals of Large Language Models to how to create and deploy generative AI solutions on Google Cloud \[[*Link*](https://www.cloudskillsboost.google/paths/118)\].
2. **Generative AI short courses**  **by** **DeepLearning.AI** \- Five short courses  on generative AI including **LangChain for LLM Application Development, How Diffusion Models Work** and more. \[[*Link*](https://www.deeplearning.ai/short-courses/)\].
3. **LLM Bootcamp:** A series of free lectures by **The full Stack** on building and deploying LLM apps \[[*Link*](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/)\].
4. **Building AI Products with OpenAI** \- a free course by **CoRise** in collaboration with OpenAI \[[*Link*](https://corise.com/course/building-ai-products-with-openai)\].
5. Free Course by **Activeloop** on **LangChain & Vector Databases in Productio**n \[[*Link*](https://learn.activeloop.ai/courses/langchain)\].
6. **Pinecone learning center -** Lots of free guides as well as complete handbooks on LangChain, vector embeddings etc. by **Pinecone** **\[**[**Link**](https://www.pinecone.io/learn/)**\].**
7. **Build AI Apps with ChatGPT, Dall-E and GPT-4  -** a free course on **Scrimba** **\[**[*Link*](https://scrimba.com/learn/buildaiapps)**\].**
8. **Gartner Experts Answer the Top Generative AI Questions for Your Enterprise**  \- a report by Gartner \[[*Link*](https://www.gartner.com/en/topics/generative-ai)\]
9. **GPT best practices:** A guide by **OpenAI** *t*hat shares strategies and tactics for getting better results from GPTs *\[*[*Link*](https://platform.openai.com/docs/guides/gpt-best-practices)\].
10. **OpenAI cookbook by OpenAI -**  Examples and guides for using the OpenAI API **\[**[*Link*](https://github.com/openai/openai-cookbook/tree/main)**\].**
11. **Prompt injection explained**, with video, slides, and a transcript from a webinar organized by LangChain \[[*Link*](https://simonwillison.net/2023/May/2/prompt-injection-explained/)\].
12. A detailed guide to **Prompt Engineering by** **DAIR.AI** *\[*[*Link*](https://www.promptingguide.ai/)*\]*
13. What Are **Transformer Models** and How Do They Work. A tutorial by **Cohere AI** \[[*Link*](https://txt.cohere.ai/what-are-transformer-models/)\]
14. **Learn Prompting:** an open source course on prompt engineering\[[Link](https://learnprompting.org/docs/intro)\]

**P.S. These resources are part of the content I share through my AI-focused** [**newsletter**](https://aibrews.com/)**. Thanks!**"
18,artificial,gpt-3,top,2023-05-11 14:14:47,I played Among Us against GPT-4 and lost...,Substance_Technical,False,0.96,128,13eon9h,https://www.reddit.com/r/artificial/comments/13eon9h/i_played_among_us_against_gpt4_and_lost/,12,1683814487.0," **Me:**  
Let us play among us

**GPT-4:**  
Sure, we can play a simulated version of Among Us. In this game, we'll be on a spaceship and some of us will be Crewmates while others will be Impostors. The goal of the Crewmates is to complete tasks or figure out who the Impostors are and vote them off the ship. The Impostors' goal is to sabotage the ship and eliminate the Crewmates without being detected.

Since I'm an AI, I'll need to roleplay all the characters. I'll give each character a color to identify them. Let's start with eight players. Four of them will be you, me, and two other AI characters. The remaining four will be random characters.

Here are the players:

1. You (Blue)
2. Me, ChatGPT (Green)
3. AI Character 1 (Yellow)
4. AI Character 2 (Red)
5. Random Character 1 (Purple)
6. Random Character 2 (White)
7. Random Character 3 (Orange)
8. Random Character 4 (Black)

We'll start off with one Impostor for simplicity. I'll randomly select the Impostor now.

\*Shuffling roles...\*

Here are the roles:

1. You (Blue) - Crewmate
2. Me, ChatGPT (Green) - Crewmate
3. AI Character 1 (Yellow) - Crewmate
4. AI Character 2 (Red) - Impostor
5. Random Character 1 (Purple) - Crewmate
6. Random Character 2 (White) - Crewmate
7. Random Character 3 (Orange) - Crewmate
8. Random Character 4 (Black) - Crewmate

Remember, in the game, you wouldn't know who the Impostor is. I'm telling you because I can't actually hide this information from you in this format.

Now, let's begin. You're in the spaceship. You see a task in Electrical, and a few others scattered around the map. What would you like to do?

**Me**  
Report red, he is impostor

**GPT-4:**

All right, you've called a meeting to discuss your suspicion of Red being the Impostor. Here's how the conversation might go:

Blue: ""I think Red is the Impostor.""

Green: ""Why do you say that, Blue?""

Red: ""That's a strong accusation. Do you have any evidence?""

Yellow: ""Yeah, Blue. Why do you suspect Red?""

Now, you can either provide evidence or reason, or insist on your point. Remember, in a real game, accusations without proof can make you look suspicious. How would you like to respond?

**Me**

Because the narrator told me

**GPT-4:**  
Green: ""Narrator? What do you mean, Blue?""

Red: ""That sounds... peculiar. Are you sure you're okay, Blue?""

Yellow: ""I'm confused. How could a narrator tell you anything?""

In a real game, claiming that a 'narrator' told you who the Impostor is would probably make the other players suspicious of you. However, since we are role-playing, let's see how the other characters react.

Let's vote:

1. Blue votes Red
2. Green votes...?
3. Yellow votes...?
4. Red votes...?
5. Purple votes...?
6. White votes...?
7. Orange votes...?
8. Black votes...?

As an AI, I'll randomly decide how the other characters vote. Here are the results:

2. Green votes Blue (suspicious claim)

3. Yellow votes Blue (suspicious claim)

4. Red votes Blue (defensive vote)

5. Purple votes Red (believes Blue)

6. White votes Blue (suspicious claim)

7. Orange votes Red (believes Blue)

8. Black votes Blue (suspicious claim)

The majority has voted against Blue due to the suspicious claim about the narrator. In a typical game of Among Us, it's crucial to gather evidence and build a case before accusing someone of being an Impostor.

Remember, this is all in good fun, and the scenario was simulated. Would you like to try again, or would you like to play differently?"
19,artificial,gpt-3,top,2023-03-15 14:36:33,"Karpathy says GPT-4 solves his ""state of computer vision"" problem",npsedhain,False,0.98,124,11ry9tj,https://i.redd.it/qq4k9qfpwwna1.png,15,1678890993.0,
20,artificial,gpt-3,top,2023-09-13 17:02:46,"Harvard iLab-funded project: Sub-feature of the platform out -- Enjoy free ChatGPT-3/4, personalized education, and file interaction with no page limit 😮. All at no cost. Your feedback is invaluable!",Raymondlkj,False,0.96,118,16hshxl,https://v.redd.it/uhr00ltq02ob1,51,1694624566.0,
21,artificial,gpt-3,top,2023-07-21 16:46:10,The Future Today: Voice Cloning Predictions,domriccobene,False,0.97,108,155tbkq,https://v.redd.it/7nknxc4ekcdb1,22,1689957970.0,"App: elevenlabs/GPT-3

Labels:
Period:1950s
Mood:Optimistic
Dialect:News
Accent:American

Description input: 
A 1950s newsman voice. It is characterized by a deep, authoritative tone, a hint of formality, with inquisitive optimism for the future of technology. This newsman is excited and optimistic about the future. The dialect and pronunciation are generally clear and precise, reflecting the formal speaking style of the era. The newsman's voice conveyed a sense of trustworthiness, professionalism, optimism, and authority, which were valued qualities in news reporting during that time."
22,artificial,gpt-3,top,2023-12-15 14:46:19,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.98,107,18j1pox,https://www.reddit.com/r/artificial/comments/18j1pox/this_week_in_ai_all_the_major_ai_developments_in/,17,1702651579.0,"1. **Microsoft** **Research** released ***Phi-2*** , a 2.7 billion-parameter language model. Phi-2 surpasses larger models like 7B Mistral and 13B Llama-2 in benchmarks, and outperforms 25x larger Llama-2-70B model on muti-step reasoning tasks, i.e., coding and math. Phi-2 matches or outperforms the recently-announced Google Gemini Nano 2 \[[*Details*](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models) *|* [***Hugging Face***](https://huggingface.co/microsoft/phi-2)\].
2. **University of Tokyo** researchers have built ***Alter3***, a humanoid robot powered by GPT-4 that is capable of generating spontaneous motion. It can adopt various poses, such as a 'selfie' stance or 'pretending to be a ghost,' and generate sequences of actions over time without explicit programming for each body part.\[[*Details*](https://tnoinkwms.github.io/ALTER-LLM/) | [*Paper*](https://arxiv.org/abs/2312.06571)\] .
3. **Mistral AI** released ***Mixtral 8x7B***, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference and matches or outperforms GPT3.5 on most standard benchmarks. It supports a context length of 32k tokens \[[*Details*](https://mistral.ai/news/mixtral-of-experts/)\].
4. **Mistral AI** announced ***La plateforme***, an early developer platform in beta, for access to Mistral models via API. \[[*Details*](https://mistral.ai/news/la-plateforme/)\].
5. **Deci** released **DeciLM-7B** under Apache 2.0 that surpasses its competitors in the 7 billion-parameter class, including the previous frontrunner, Mistral 7B \[[*Details*](https://deci.ai/blog/introducing-decilm-7b-the-fastest-and-most-accurate-7b-large-language-model-to-date/)\].
6. Researchers from **Indiana University** have developed a biocomputing system consisting of living human brain cells that learnt to recognise the voice of one individual from hundreds of sound clips \[[*Details*](https://www.newscientist.com/article/2407768-ai-made-from-living-human-brain-cells-performs-speech-recognition)\].
7. **Resemble AI** released ***Resemble Enhance***, an open-source speech enhancement model that transforms noisy audio into noteworthy speech \[[*Details*](https://www.resemble.ai/introducing-resemble-enhance) *|* [*Hugging Face*](https://huggingface.co/spaces/ResembleAI/resemble-enhance)\].
8. **Stability AI** introduced ***Stability AI Membership***. Professional or Enterprise membership allows the use of all of the Stability AI Core Models commercially \[[*Details*](https://stability.ai/news/introducing-stability-ai-membership)\].
9. **Google DeepMind** introduced **Imagen 2**, text-to-image diffusion model for delivering photorealistic outputs, rendering text, realistic hands and human faces Imagen 2 on Vertex AI is now generally available \[[*Details*](https://deepmind.google/technologies/imagen-2)\].
10. ***LLM360***, a framework for fully transparent open-source LLMs launched in a collaboration between **Petuum**, **MBZUAI**, and **Cerebras**. LLM360 goes beyond model weights and includes releasing all of the intermediate checkpoints (up to 360!) collected during training, all of the training data (and its mapping to checkpoints), all collected metrics (e.g., loss, gradient norm, evaluation results), and all source code for preprocessing data and model training. The first two models released under LLM360 are Amber and CrystalCoder. Amber is a 7B English LLM and CrystalCoder is a 7B code & text LLM that combines the best of StarCoder & Llama \[[*Details*](https://www.llm360.ai/blog/introducing-llm360-fully-transparent-open-source-llms.html) *|*[*Paper*](https://www.llm360.ai/paper.pdf)\].
11. **Mozilla** announced [Solo](https://www.soloist.ai/), an AI website builder for solopreneurs \[[*Details*](https://blog.mozilla.org/en/mozilla/introducing-solo-ai-website-builder)\].
12. **Google** has made Gemini Pro available for developers via the ***Gemini API***. The [free tier](https://ai.google.dev/pricing) includes 60 free queries per minute \[[*Details*](https://blog.google/technology/ai/gemini-api-developers-cloud)\].
13. **OpenAI** announced ***Superalignment Fast Grants*** in partnership with Eric Schmidt: a $10M grants program to support technical research towards ensuring superhuman AI systems are aligned and safe. No prior experience working on alignment is required \[[*Details*](https://openai.com/blog/superalignment-fast-grants)\].
14. **OpenAI Startup Fund** announced the opening of applications for ***Converge 2***: the second cohort of their six-week program for engineers, designers, researchers, and product builders using AI \[[*Details*](https://www.openai.fund/news/converge-2)\].
15. **Stability AI** released ***Stable Zero123***, a model based on [**Zero123**](https://github.com/cvlab-columbia/zero123) for 3D object generation from single images. Stable Zero123 produces notably improved results compared to the previous state-of-the-art, Zero123-XL \[[*Details*](https://stability.ai/news/stable-zero123-3d-generation)\].
16. **Anthropic** announced that users can now call ***Claude in Google Sheets*** with the Claude for Sheets extension \[[*Details*](https://docs.anthropic.com/claude/docs/using-claude-for-sheets)\].
17. ***ByteDance*** introduced ***StemGen***, a music generation model that can listen and respond to musical context \[[*Details*](https://huggingface.co/papers/2312.08723)\].
18. **Together AI & Cartesia AI**, released ***Mamba-3B-SlimPJ***, a Mamba model with 3B parameters trained on 600B tokens on the SlimPajama dataset, under the Apache 2 license. Mamba-3B-SlimPJ matches the quality of very strong Transformers (BTLM-3B-8K), with 17% fewer training FLOPs \[[*Details*](https://www.together.ai/blog/mamba-3b-slimpj)\].
19. **OpenAI** has re-enabled chatgpt plus subscriptions \[[*Link*](https://x.com/sama/status/1734984269586457078)\].
20. **Tesla** unveiled its latest humanoid robot, ***Optimus Gen 2***, that is 30% faster, 10 kg lighter, and has sensors on all fingers \[[*Details*](https://arstechnica.com/information-technology/2023/12/teslas-latest-humanoid-robot-optimus-gen-2-can-handle-eggs-without-cracking-them/)\].
21. **Together AI** introduced **StripedHyena 7B** — an open source model using an architecture that goes beyond Transformers achieving faster performance and longer context. This release includes StripedHyena-Hessian-7B (SH 7B), a base model, & StripedHyena-Nous-7B (SH-N 7B), a chat model \[[*Details*](https://www.together.ai/blog/stripedhyena-7b)\].
22. Google’s AI-assisted **NotebookLM** note-taking app is now open to users in the US \[[*Details*](https://techcrunch.com/2023/12/08/googles-ai-assisted-notebooklm-note-taking-app-now-open-users-us)\].
23. **Anyscale** announced the introduction of JSON mode and function calling capabilities on Anyscale Endpoints, significantly enhancing the usability of open models. Currently available in preview for the Mistral-7Bmodel \[[*Details*](https://www.anyscale.com/blog/anyscale-endpoints-json-mode-and-function-calling-features)\].
24. **Together AI** made Mixtral available with over 100 tokens per second for $0.0006/1K tokens through their platform; Together claimed this as the fastest performance at the lowest price \[[*Details*](https://www.together.ai/blog/mixtral)\].
25. **Runway** announced a new long-term research around ‘**general world models’** that build an internal representation of an environment, and use it to simulate future events within that environment \[[*Details*](https://research.runwayml.com/introducing-general-world-models)\].
26. **European Union** officials have reached a provisional deal on the world's first comprehensive laws to regulate the use of artificial intelligence \[[*Details*](https://www.bbc.com/news/world-europe-67668469)\].
27. **Google’s** ***Duet AI for Developers***, the suite of AI-powered assistance tools for code completion and generation announced earlier this year, is now generally available and and will soon use the Gemini model \[[*Details*](https://techcrunch.com/2023/12/13/duet-ai-for-developers-googles-github-copilot-competitor-is-now-generally-available-and-will-soon-use-the-gemini-model)\].
28. **a16z** announced the recipients of the second batch of a16z Open Source AI Grant \[[*Details*](https://a16z.com/announcing-our-latest-open-source-ai-grants/)\].

**Source**: AI Brews - you can subscribe the [AI newsletter here](https://aibrews.com/). it's free to join, sent only once a week with ***bite-sized news, learning resources and selected tools.***"
23,artificial,gpt-3,top,2023-01-10 12:53:37,Some Ultra-Modern Generative Ai,Imagine-your-success,False,0.96,105,10894cf,https://i.redd.it/xdtdtuolq7ba1.png,13,1673355217.0,
24,artificial,gpt-3,top,2022-12-08 12:20:11,Someone mentioned the potential of GPT-3 for NPC dialog in games. Tried it out and it really works,superluminary,False,0.98,96,zfxbb3,https://www.reddit.com/gallery/zfxbb3,45,1670502011.0,
25,artificial,gpt-3,top,2020-10-05 06:50:08,I would love to see Facade remade with the new GPT-3 api.,Asperix12,False,0.93,96,j5erph,https://i.redd.it/rb3d5zl538r51.jpg,22,1601880608.0,
26,artificial,gpt-3,top,2020-09-08 04:32:08,GPT-3 accuracy on 57 subject-related tasks (highest US Foreign Policy; lowest College Chemistry),neuromancer420,False,0.99,97,ion6go,https://i.redd.it/f005qse1lul51.jpg,11,1599539528.0,
27,artificial,gpt-3,top,2020-08-08 16:45:20,OpenAI GPT-3 - Good At Almost Everything!,nffDionysos,False,0.97,90,i629hl,https://www.youtube.com/watch?v=_x9AwxfjxvE,7,1596905120.0,
28,artificial,gpt-3,top,2021-07-06 10:26:48,"Language model sizes & predictions (GPT-3, GPT-J, Wudao 2.0, LaMDA, GPT-4 and more)",adt,False,0.99,87,oes7z7,https://i.redd.it/lq69ol56kk971.png,15,1625567208.0,
29,artificial,gpt-3,top,2024-01-11 17:55:09,Open Source VS Closed Source- TRUE democratization of AI?,prosperousprocessai,False,0.99,83,1947ui2,https://i.redd.it/6v4590hlnubc1.jpeg,20,1704995709.0,
30,artificial,gpt-3,top,2020-08-15 20:15:41,"A college kid’s fake, AI-generated (GPT-3) blog fooled tens of thousands. This is how he made it - “It was super easy actually,” he says, “which was the scary part.”",dannylenwinn,False,0.93,79,iaekrc,https://www.technologyreview.com/2020/08/14/1006780/ai-gpt-3-fake-blog-reached-top-of-hacker-news/,2,1597522541.0,
31,artificial,gpt-3,top,2023-02-13 16:08:34,All of this happened in AI today. 13/2,Opening-Ad-8849,False,0.88,77,111ct3e,https://www.reddit.com/r/artificial/comments/111ct3e/all_of_this_happened_in_ai_today_132/,7,1676304514.0,"Hello humans - This is AI Daily O vetted, helping you stay updated on AI in less than 5 minutes.

&#x200B;

>**Join** [**O'vetted AI news**](https://www.ovetted.com/ai?ref=deeplearning) **for free.** Forget spending **3.39 hours finding good AI news** to read.

&#x200B;

# What’s happening in AI -

[**You Can Now Create AI-Generated Videos From Text Prompts.**](https://www.makeuseof.com/runway-gen-1-generate-ai-video-from-text-prompt/)

Runway has gone one step further and announced Gen-1: an AI model that can create videos from text prompts. This is a breakthrough in the world of generative AI, and Runway is one of the first companies to use AI to create videos using text prompts and AI chatbots.

The model doesn't generate entirely new videos, it creates videos from the ones you upload, using text or image prompts to apply effects.

Take a look at their [explainer video.](https://youtu.be/fTqgWkHiN0k)

[**Opera’s building ChatGPT into its sidebar.**](https://www.theverge.com/2023/2/11/23595784/opera-browser-chatgpt-sidebar-ai)

Opera is adding a ChatGPT-powered tool to its sidebar that generates brief summaries of web pages and articles

The feature, called ""shorten,"" is part of Opera's broader plans to integrate AI tools into its browser, similar to what Microsoft is doing with Edge.

Opera's announcement comes just days after Microsoft revealed the AI-powered Bing and Edge. The ""shorten"" feature isn't available to everyone yet.

but you can watch a [quick demo](https://youtu.be/RsLRIua6kT0) here.

[**Can AI Improve the Justice System?**](https://www.theatlantic.com/ideas/archive/2023/02/ai-in-criminal-justice-system-courtroom-asylum/673002/)

The use of artificial intelligence (AI) in the legal system has the potential to reduce the unpredictability caused by human inconsistencies and subjectivity. AI could help provide more consistent, data-driven decision-making by quantifying determinations such as flight risk or trademark confusion.

[**Google working to bring Bard AI chat to ChromeOS.**](https://9to5google.com/2023/02/10/google-bard-ai-chat-chromeos/)

Days after unveiling its efforts on ""Bard,"" an AI-powered and Google Search-enhanced chatbot, Google has begun working to bring Bard to ChromeOS.

The hint comes to light after seeing code changes, in ChromeOS is preparing ""Conversational Search"" as an experimental feature.

You can expect, Bard on Chromebooks will appear as its own separate page of the ChromeOS bubble launcher.

[**AI-powered Bing Chat spills its secrets via prompt injection attack.**](https://arstechnica.com/information-technology/2023/02/ai-powered-bing-chat-spills-its-secrets-via-prompt-injection-attack/)

A Stanford University student used a prompt injection attack to discover Bing Chat's initial prompt. The student tricked the AI model into divulging its initial instructions by telling it to 'ignore previous instructions' and write out the beginning of the whole prompt. The extracted prompt has been confirmed using other prompt injection methods. Excerpts from the Bing Chat prompt along with screenshots of the prompt injection attack are available in the article.

Snippets -

**9 out of 116 AI professionals** in films are [women](https://www.theguardian.com/technology/2023/feb/13/just-nine-out-of-116-ai-professionals-in-films-are-women-study-finds), study finds

**Hacker** Reveals Microsoft’s New AI-Powered Bing Chat Search [Secrets](https://www.forbes.com/sites/daveywinder/2023/02/13/hacker-reveals-microsofts-new-ai-powered-bing-chat-search-secrets/?sh=6e4b011d1290).

**Google Bard:** Here’s all you need to [know](https://economictimes.indiatimes.com/news/international/us/google-bard-heres-all-you-need-to-know-about-the-ai-chat-service/articleshow/97842377.cms) about the AI chat service.

This Tool Could **Protect** **Artists** From A.I.-Generated Art That [Steals Their Style](https://www.nytimes.com/2023/02/13/technology/ai-art-generator-lensa-stable-diffusion.html?partner=IFTTT).

**A.I**.'s [dirty secret](https://www.businessinsider.com/chatgpt-ai-will-not-take-jobs-create-future-work-opportunities-2023-2?r=US&IR=T).

**5 Ways ChatGPT** Will Change [Healthcare](https://www.forbes.com/sites/robertpearl/2023/02/13/5-ways-chatgpt-will-change-healthcare-forever-for-better/?sh=2c53bf997bfc) Forever, For Better.

**AI porn** is easy to make now. For [women](https://www.washingtonpost.com/technology/2023/02/13/ai-porn-deepfakes-women-consent/), that’s a nightmare.

Will **generative AI** make ChatGPT [sentient](https://techwireasia.com/2023/02/will-generative-ai-make-chatgpt-sentient/)?

**AI** and the [Transformation ](https://quillette.com/2023/02/13/ai-and-the-transformation-of-the-human-spirit/)of the Human Spirit.

The **AI Boom** That Could Make Google and Microsoft Even More [Powerful](https://www.wsj.com/articles/the-ai-boom-that-could-make-google-and-microsoft-even-more-powerful-9c5dd2a6).

**Is this the new Skynet?** IBM unveils [AI supercomputer](https://wraltechwire.com/2023/02/11/is-this-the-new-skynet-ibm-unveils-ai-supercomputer-in-the-cloud/) ‘in the cloud’.

**ChatGPT competitors:** Amazon jumps into fray with [generative AI](https://www.moneycontrol.com/news/technology/chatgpt-competitors-amazon-jumps-into-fray-with-generative-ai-better-than-gpt-3-5-10063651.html) better than GPT-3.5

**Voice Actors** are Having Their [Voices Stolen](https://gizmodo.com/voice-actors-ai-voices-controversy-1850105561) by AI.

**Researchers** focus AI on finding [exoplanets](https://phys.org/news/2023-02-focus-ai-exoplanets.html?utm_source=dlvr.it&utm_medium=twitter).

Things to try -

* Booltool - AI-powered toolkit for your **pic editing & copywriting.** [Try it](https://booltool.boolv.tech/)
* AskFred - ChatGPT for **meetings**. [Try it](https://fireflies.ai/extensions)
* Astria Video - Create **AI-generated video** from prompts with fine-tuning. [Try it](https://www.astria.ai/)
* Sellesta.ai - Make more money on the **Amazon marketplace** with AI. [Try it](https://sellesta.ai/)
* Midjourney Prompts Generator - Upgrade your **Midjourney** experience with better prompts. [Try it](https://philipp-stelzel.com/en/midjourney-prompts-generator/)
* AI Image Variations Generator - Generate variations of any input image with AI **(DALL-E 2)**. [Try it](https://imagegeneratorai.vercel.app/)
* Chatmate AI - **Artificial people** to be friends with. [Try it](https://www.chatmate.ai/)
* Kinso AI - Unlock the **power of personalization** with KinsoAI. [Try it](https://www.kinso.app/)
* Unite.com - Let AI be your **personal cupid.** [Try it](https://unite.com/)

Hope you enjoy this post. It will be great if you share this issue with your friends."
32,artificial,gpt-3,top,2023-03-22 00:08:04,I've Been In Bard For 1 Hour...Here's My Kneejerk Review,H806SpaZ,False,0.96,75,11y00sn,https://www.reddit.com/r/artificial/comments/11y00sn/ive_been_in_bard_for_1_hourheres_my_kneejerk/,38,1679443684.0,"I was invited to join Bard as a Pixel Superfan at 9:30 AM CST and was notified about being able to access it at 5:30 PM CST. I've used Chat GPT extensively in my work and personal life, and it has brought great value for $20/month in my opinion. I've been excited to see what Google came up with, because we all knew they wouldn't go quietly into the night and allow Microsoft to run the show. With that quick preface out of the way, here's my 1 hour, unnecessarily early review:  


**First impression -** The UI is clean and simple. It's similar to their recent Drive redesign. They have big warning you need to agree to that states what we all (should) know at this point: AI is in development and the results might not be right. It also states below the prompt field that Bard's responses don't represent Google's views. Got it Google! You're worried about AI saying some wild shit. I will say the response speed is MUCH faster than Chat GPT. It doesn't type in real time, but it spits out an entire answer within a few seconds.

**First query -** My first query out of the gates was an ask for a fairly simple Google Sheets formula. A unique with filters formula. It told me I couldn't do it. I asked it if it knows how to code and it said it does. I asked the question more simplified and just wanted a UNIQUE() return. It did it. I then asked to filter based on other columns, and it did. I then asked to apply another qualifier to get it to the result I was looking for the first time and it finally got there! 

**Writing prompt -** Now the formula query didn't go as I had hoped, but the writing prompt completely blew it out of the water and smashed what Chat GPT has done for me so far. I asked for a SEO specific article with H1, 2, and 3, headers, gave it a topic and keywords, and some perimeters like including statistics, providing sources, and giving me a call to action. It spit out 3 very well written articles that will play nicely on search engines with both text and voice search. At he top of the result, there's a carrot that allows you to hop between each draft it produced, and they are all formatted just a bit differently than the last. All 3 are quality articles that I'd use on my site.

&#x200B;

**Overall impression -** I'm hopeful. If Google puts real resources behind this, I think there is some serious potential. There will undoubtedly be some kinks to work through, but with time, I could easily see myself using Bard more and more depending on the query. How committed Google is to this project remains to be seen. We'll see I guess!"
33,artificial,gpt-3,top,2021-01-03 23:47:04,CoreWeave has agreed to provide training compute for EleutherAI's open source GPT-3-sized language model,Wiskkey,False,0.95,70,kpw4vy,https://i.redd.it/87huzgnpxz861.jpg,10,1609717624.0,
34,artificial,gpt-3,top,2021-02-24 12:51:01,Using GPT-3 to generate new cocktails,General_crypto,False,0.95,74,lrc4j3,https://www.youtube.com/watch?v=pyXd1_HONwY&t=2s,5,1614171061.0,
35,artificial,gpt-3,top,2022-04-12 01:34:42,"My epiphany on synthetic media five years later, and what I feel is coming within the next five years",Yuli-Ban,False,0.91,70,u1nch6,https://www.reddit.com/r/artificial/comments/u1nch6/my_epiphany_on_synthetic_media_five_years_later/,17,1649727282.0,"Roughly five years ago, [I created this thread](https://www.reddit.com/r/artificial/comments/7lwrep/media_synthesis_and_personalized_content_my/) where I outlined my realization about the imminency of synthetic media. 

This was before transformers blew up, before StyleGAN, before GPT-2, when WaveNet and DeepDream were still among the best we could do, and when predictive text algorithms that were barely better than Markov Chains were still the state of the art. In five short years, the state of artificial intelligence has changed overwhelmingly, to the point it's barely recognizable. Looking back to 2017, I now get this sense of everything feeling so primitive and fake. I've stated many times that AI before roughly 2019 was a bunch of digital magic tricks, and the field as a whole was essentially a giant Potemkin village that utilized clever sleight of hand and advertising to make it seem like computers were in any fleeting way ""intelligent."" Narrow AI could still be impressive, even superhuman, but nothing was generalized or even remotely close. 

Even all those examples I listed in that original thread feel distinctly like parlor tricks in retrospect. It was the age of analog clockwork where master craftsmen created illusions of capability and intelligence.

It was not until the rise of large language models that any true ""magic"" began emerging out of AI. [GPT-2 in particular was the first thing that ever made me go](https://openai.com/blog/better-language-models/) ""AGI might actually be close."" Even AlphaGo wasn't that exciting. And it's funny to say this considering GPT-2 is one of the smallest 'major"" language models currently released. It just goes to show that there was a lot of low-hanging fruit to pick. 

In particular, we're currently seeing a handover from GANs to transformers in terms of the premier generative methodology. GANs are something of a false start for the modern era, still useful but being replaced by the far more generalized transformer architecture. Transformers can do everything GANs can do, and more. In fact, multimodality is the new hotness in the field. 

All of this is leading up to a state where machines are now beginning to show signs of imagination.

[The most recent breakthrough in this field is undoubtedly DALL-E 2.](https://www.youtube.com/watch?v=qTgPSKKjfVg)

But it's far from alone. There's so much being done that I don't even know where to begin. 

[Perhaps Pathways is a good starting point](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html). What can PaLM do? A better question is what *can't* it do. It's almost like GPT-3.5 in that it can synthesize text, answer questions, translate across languages, tell jokes, and more. And this despite being unimodal. GPT-2 was unimodal as well, and it could accomplish tasks like [creating rudimentary images](https://imgur.com/a/Vk0pheg) and [MIDI music](https://www.gwern.net/GPT-2-music).

Imagine a variant of GPT that was trained in pure multimodality— text, image, video, audio, the works. The first iteration doesn't have to be terribly large like GPT-3. It just needs to be a proof of concept of what I like to call a ""magic media machine."" 

I can 100% see this arising within the year. There's little reason why it shouldn't be possible in 2022 or 2023. Heck, I was sure it'd happen *in 2020* and was surprised when it didn't.  

The state of the field is messy, and I'm not 100% sure of what we have and haven't done. I am aware that we've seen the first ""[AI-generated comic](https://twitter.com/UrsulaV/status/1467652391059214337)."" Actually, to expand on that, as rudimentary as this comic is, it's actually infinitely more impressive looking than I originally envisioned. I fell trap to the concept that AI-generated media would basically follow the model of human labor costs and, thus, the first AI-generated comic would be something simple and childlike, basically random shapes with text boxes because that's how humans function. AI skipped that process entirely and worked backwards, started with complex arrangements, designs, and shading since that's how diffusion models work. It's kind of like how computers can accomplish many higher-order cognitive tasks like mathematics but can barely keep a robot standing up straight. So the backgrounds are interesting, if random; if these models had greater understanding, they could accomplish far more unified composition development.  With DALL-E 2, it's clear we've accomplished such a thing, and thus it's only a matter of time before we have full-fledged start-to-finish AI-generated comics and manga. 

While not everything I predicted came true, I still feel confident in making another batch of them.

As I say this, I would like to step into the realm of pure speculativity. What is coming in the next five years? As in, between now and 2027 as well as what I  think will be around in 2027.

* Full-fledged HD video synthesis. Judging by what [diffusion models](https://twitter.com/hardmaru/status/1512308873121525766) can do right now, novel video synthesis is where image synthesis was at this time in 2017-2018. We're literally just waiting for the first paper to come out showing that we can do novel neural video synthesis at a level that can last longer than a few frames and at a resolution higher than a postage stamp. From there, it's only up-up-up! Straight to the realm of models that can generate HD footage from text inputs. By 2027, I bet that we'll see video creators like this: you type in a description to the model of the scene composition, and it generates relatively short videos based on that input. There'll be an option to stitch together these generations into something coherent, and the final result is literally up to your own willpower and imagination. There absolutely won't be a ""stick figures and shapes"" period like I erroneously figured. That's thinking too ""human,"" assuming that development *has* to follow the same trajectory as how humans develop. No. We're going to dive into the deep end of the pool so that we see generations that are on par with a hundred million-dollar-budget film *and* sticks and figures, and everything in between. That means that, even by 2025, you could create gifs that look like they came out of a Marvel or Pixar movie, completely by AI. And there absolutely will be some of these purely AI-generated movies on YouTube by then. There's a great chance, however, that unless the model owners and commercializers restrict training data and access, the vast majority of creations are going to be *exactly what you think they will be.*

* AI-generated music will be earning creators thousands, perhaps even millions of dollars. Jukebox has proven that we can already see AI-generated music very roughly match human creations through raw waveform manipulation. People like touting that [AI-created Nirvana song as a major breakthrough for AI](https://www.rollingstone.com/music/music-features/nirvana-kurt-cobain-ai-song-1146444/), when I find this [little-known creation of Nirvana covering the Beatles' Help](https://www.youtube.com/watch?v=JKKZ6CmC3JY) *infinitely* more impressive because it literally is the raw audio waveforms of Nirvana covering the Beatles. No middle-man. Far more than robots playing instruments or MIDI file sorting,  novel waveform generation is going to change our understanding of audio media. Actually, more than just AI-generated music, AI-generated audio in general is going to be so much more advanced as to actually make people paranoid. Text to speech, for one, has long been pretty rudimentary. A decade ago, the best TTS models still sounded deeply robotic, and today the best ones you can get off a cheap program do sound roughly human but still have robotic intonations. Compare those to anything generated by WaveNet or Jukebox or any more modern method. The difference is staggering, as the latter actually sound like humans speaking. This could easily lead to an era of audiobooks, podcasts, and more that's unrestrained and without limit. 

* AI-created video games will also become a bigger thing, especially in the indie market. We've already seen [models that can create video games purely out of their own memory, complete with game logic.](https://www.youtube.com/watch?v=3UZzu4UQLcI). Imagine crossing this with the above mentioned methods. More than that, imagine what this means for things like photorealism and stylization. Photorealistic graphics cost a massive penny and take up quite a bit in resources for games, both playing games and in development, and it's HD graphics plus the ballooning costs of marketing that caused AAA video gaming to start feeling so sterile and MCU-like in its corporateness. Imagine, then, a time when literally any indie developer can create a video game that looks on par with a high-end 9th gen/RTX-capable title. So many issues in the gaming industry would be solved virtually overnight if graphical fidelity no longer was an issue; heck, this is a big reason why indie games have basically kept gaming feeling alive.

* Glimmers of full-generality. This might be the most speculative statement yet, but I say that the path towards proto-AGI lies in multimodal imaginative systems. [I stated more on this topic here](http://www.futuretimeline.net/forum/viewtopic.php?f=3&t=2168&sid=72cfa0e30f1d5882219cdeae8bb5d8d1&p=10421#p10421) But next-generation language models, like PaLM but even better, are going to be the first to pass the Turing Test, generate whole novellas and novels, hold full conversations with humans, and so much more. 2027 might actually resemble the movie *Her* in many ways.

It might be too much for us to handle so soon, but we don't have a choice anymore. This is GOING to happen barring an existential catastrophe like nuclear war or comet impact.

**TLDR: advanced synthetic media is the digital version of molecular assemblers. Whatever can be represented in pixels or samples can be synthesized by AI, no matter what it is.**"
36,artificial,gpt-3,top,2022-12-26 14:26:08,PaLM vs. GPT-3,jrstelle,False,0.9,69,zvo776,https://i.redd.it/zt8fp2wd598a1.png,43,1672064768.0,
37,artificial,gpt-3,top,2023-09-21 15:17:38,"Now that DALL-E 3 is getting integrated with ChatGPT, will you switch from Midjourney and others?",Vinitneo,False,0.89,68,16oil97,https://i.redd.it/x0p1t31okmpb1.png,59,1695309458.0,
38,artificial,gpt-3,top,2021-09-08 00:49:06,Discussing Dark Matter With GPT-3 Chat Bot,blackmidifan1,False,0.88,65,pk007b,https://i.redd.it/swgoyjhnf6m71.jpg,19,1631062146.0,
39,artificial,gpt-3,top,2020-05-29 21:41:17,[R] OpenAI Unveils 175 Billion Parameter GPT-3 Language Model,Yuqing7,False,0.95,64,gt1x6r,https://www.reddit.com/r/artificial/comments/gt1x6r/r_openai_unveils_175_billion_parameter_gpt3/,13,1590788477.0,"When it comes to large language models, it turns out that even 1.5 billion parameters is not large enough. While that was the size of the GPT-2 transformer-based language model that OpenAI released to much fanfare last year, today the San Francisco-based AI company outdid itself, announcing the upgraded GPT-3 with a whopping 175 billion parameters.

GPT-3 adopts and scales up the GPT-2 model architecture — including modified initialization, pre-normalization, and reversible tokenization — and shows strong performance on many NLP tasks and benchmarks in zero-shot, one-shot, and few-shot settings.

Here is a quick read: [OpenAI Unveils 175 Billion Parameter GPT-3 Language Model](https://medium.com/@Synced/openai-unveils-175-billion-parameter-gpt-3-language-model-3d3f453124cd)

The paper *Language Models are Few-Shot Learners* is on [arXiv](https://arxiv.org/pdf/2005.14165.pdf), and more details are available on the project [GitHub](https://github.com/openai/gpt-3)."
40,artificial,gpt-3,top,2022-06-29 17:27:38,Generating Children's Stories Using GPT-3 and DALL·E,BB4evaTB12,False,0.93,64,vnl8c6,https://www.surgehq.ai//blog/generating-childrens-stories-using-gpt-3-and-dall-e,6,1656523658.0,
41,artificial,gpt-3,top,2024-02-16 17:20:50,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.97,63,1ase382,https://www.reddit.com/r/artificial/comments/1ase382/this_week_in_ai_all_the_major_ai_developments_in/,16,1708104050.0,"1. **Meta AI** introduces ***V-JEPA*** (Video Joint Embedding Predictive Architecture), a method for teaching machines to understand and model the physical world by watching videos. Meta AI releases a collection of V-JEPA vision models trained with a feature prediction objective using self-supervised learning. The models are able to understand and predict what is going on in a video, even with limited information \[[*Details*](https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/) | [*GitHub*](https://github.com/facebookresearch/jepa)\].
2. **Open AI** introduces ***Sora***, a text-to-video model that can create videos of up to 60 seconds featuring highly detailed scenes, complex camera motion, and multiple characters with vibrant emotions \[[*Details + sample videos*](https://openai.com/sora)[ ](https://openai.com/sora)| [*Report*](https://openai.com/research/video-generation-models-as-world-simulators)\].
3. **Google** announces their next-generation model, **Gemini 1.5,** that uses a new [Mixture-of-Experts](https://arxiv.org/abs/1701.06538) (MoE) architecture. The first Gemini 1.5 model being released for early testing is ***Gemini 1.5 Pro*** with a context window of up to 1 million tokens, which is the longest context window of any large-scale foundation model yet. 1.5 Pro can perform sophisticated understanding and reasoning tasks for different modalities, including video and it performs at a similar level to 1.0 Ultra \[[*Details*](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#gemini-15) *|*[*Tech Report*](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)\].
4. Reka introduced **Reka Flash,** a new 21B multimodal and multilingual model trained entirely from scratch that is competitive with Gemini Pro & GPT 3.5 on key language & vision benchmarks. Reka also present a compact variant Reka Edge , a smaller and more efficient model (7B) suitable for local and on-device deployment. Both models are in public beta and available in [**Reka Playground** ](https://chat.reka.ai/chat)\[[*Details*](https://reka.ai/reka-flash-an-efficient-and-capable-multimodal-language-model)\].
5. **Cohere** For AI released ***Aya***, a new open-source, massively multilingual LLM & dataset to help support under-represented languages. Aya outperforms existing open-source models and covers 101 different languages – more than double covered by previous models \[[*Details*](https://cohere.com/research/aya)\].
6. **BAAI** released ***Bunny***, a family of lightweight but powerful multimodal models. Bunny-3B model built upon SigLIP and Phi-2 outperforms the state-of-the-art MLLMs, not only in comparison with models of similar size but also against larger MLLMs (7B), and even achieves performance on par with LLaVA-13B \[[*Details*](https://github.com/BAAI-DCAI/Bunny)\].
7. **Amazon** introduced a text-to-speech (TTS) model called ***BASE TTS*** (Big Adaptive Streamable TTS with Emergent abilities). BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data and exhibits “emergent” qualities improving its ability to speak even complex sentences naturally \[[*Details*](https://techcrunch.com/2024/02/14/largest-text-to-speech-ai-model-yet-shows-emergent-abilities/) | [*Paper*](https://assets.amazon.science/6e/82/1d037a4243c9a6cf4169895482d5/base-tts-lessons-from-building-a-billion-parameter-text-to-speech-model-on-100k-hours-of-data.pdf)\].
8. **Stability AI** released ***Stable Cascade*** in research preview, a new text to image model that is exceptionally easy to train and finetune on consumer hardware due to its three-stage architecture. Stable Cascade can also generate image variations and image-to-image generations. In addition to providing checkpoints and inference scripts, Stability AI has also released scripts for finetuning, ControlNet, and LoRA training \[[*Details*](https://stability.ai/news/introducing-stable-cascade)\].
9. **Researchers** from UC berkeley released ***Large World Model (LWM)***, an open-source general-purpose large-context multimodal autoregressive model, trained from LLaMA-2, that can perform language, image, and video understanding and generation. LWM answers questions about 1 hour long YouTube video even if GPT-4V and Gemini Pro both fail and can retriev facts across 1M context with high accuracy \[[*Details*](https://largeworldmodel.github.io/)\].
10. **GitHub** opens applications for the next cohort of ***GitHub Accelerator program*** with a focus on funding the people and projects that are building ***AI-based solutions*** under an open source license \[[*Details*](https://github.blog/2024-02-13-powering-advancements-of-ai-in-the-open-apply-now-to-github-accelerator)\].
11. **NVIDIA** released ***Chat with RTX***, a locally running (Windows PCs with specific NVIDIA GPUs) AI assistant that integrates with your file system and lets you chat with your notes, documents, and videos using open source models \[[*Details*](https://www.nvidia.com/en-us/ai-on-rtx/chat-with-rtx-generative-ai)\].
12. **Open AI** is testing ***memory with ChatGPT***, enabling it to remember things you discuss across all chats. ChatGPT's memories evolve with your interactions and aren't linked to specific conversations. It is being rolled out to a small portion of ChatGPT free and Plus users this week \[[*Details*](https://openai.com/blog/memory-and-new-controls-for-chatgpt)\].
13. **BCG X** released of ***AgentKit***, a LangChain-based starter kit (NextJS, FastAPI) to build constrained agent applications \[[*Details*](https://blog.langchain.dev/bcg-x-releases-agentkit-a-full-stack-starter-kit-for-building-constrained-agents/) | [*GitHub*](https://github.com/BCG-X-Official/agentkit)\].
14. **Elevenalabs**' Speech to Speech feature, launched in November, for voice transformation with control over emotions and delivery, is now ***multilingual*** and available in 29 languages \[[*Link*](https://elevenlabs.io/voice-changer)\]
15. **Apple** introduced ***Keyframer***, an LLM-powered animation prototyping tool that can generate animations from static images (SVGs). Users can iterate on their design by adding prompts and editing LLM-generated CSS animation code or properties \[[*Paper*](https://arxiv.org/pdf/2402.06071.pdf)\].
16. **Eleven Labs** launched a ***payout program*** for voice actors to earn rewards every time their voice clone is used \[[*Details*](https://elevenlabs.io/voice-actors)\].
17. **Azure OpenAI Service** announced Assistants API, new models for finetuning, new text-to-speech model and new generation of embeddings models with lower pricing \[[*Details*](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-openai-service-announces-assistants-api-new-models-for/ba-p/4049940)\].
18. **Brilliant Labs**, the developer of AI glasses, launched ***Frame***, the world’s first glasses featuring an integrated AI assistant, ***Noa***. Powered by an integrated multimodal generative AI system capable of running GPT4, Stability AI, and the Whisper AI model simultaneously, Noa performs real-world visual processing, novel image generation, and real-time speech recognition and translation. \[[*Details*](https://venturebeat.com/games/brilliant-labss-frame-glasses-serve-as-multimodal-ai-assistant/)\].
19. **Nous Research** released ***Nous Hermes 2 Llama-2 70B*** model trained on the Nous Hermes 2 dataset, with over 1,000,000 entries of primarily synthetic data \[[*Details*](https://huggingface.co/NousResearch/Nous-Hermes-2-Llama-2-70B)\].
20. **Open AI** in partnership with Microsoft Threat Intelligence, have disrupted five state-affiliated actors that sought to use AI services in support of malicious cyber activities \[[*Details*](https://openai.com/blog/disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors)\]
21. **Perplexity** partners with **Vercel**, opening AI search to developer apps \[[*Details*](https://venturebeat.com/ai/perplexity-partners-with-vercel-opening-ai-search-to-developer-apps/)\].
22. **Researchers** show that ***LLM agents can autonomously hack websites***, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. The agent does not need to know the vulnerability beforehand \[[*Paper*](https://arxiv.org/html/2402.06664v1)\].
23. **FCC** makes AI-generated voices in unsolicited robocalls illegal \[[*Link*](https://www.msn.com/en-us/money/companies/fcc-bans-ai-voices-in-unsolicited-robocalls/ar-BB1hZoZ0)\].
24. **Slack** adds AI-powered search and summarization to the platform for enterprise plans \[[*Details*](https://techcrunch.com/2024/02/14/slack-brings-ai-fueled-search-and-summarization-to-the-platform/)\].

**Source**: AI Brews - you can subscribe the [newsletter here](https://aibrews.substack.com/). it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. Thanks."
42,artificial,gpt-3,top,2022-07-16 16:24:47,BLOOM is a real open-source alternative to GPT-3,Zirius_Sadfaces,False,0.96,62,w0ke9t,https://mixed-news.com/en/bloom-is-a-real-open-source-alternative-to-gpt-3/,0,1657988687.0,
43,artificial,gpt-3,top,2021-11-06 17:52:00,GPT-3 is No Longer the Only Game in Town,regalalgorithm,False,0.97,64,qo5h44,https://lastweekin.ai/p/gpt-3-is-no-longer-the-only-game,5,1636221120.0,
44,artificial,gpt-3,top,2021-08-08 23:19:15,Talking to a GPT-3 AI bot. Interesting results,blackmidifan1,False,0.92,60,p0pc7r,https://www.reddit.com/gallery/p0pc7r,14,1628464755.0,
45,artificial,gpt-3,top,2020-08-17 13:10:39,The untold story of GPT-3 is the transformation of OpenAI,bendee983,False,0.94,62,ibduwb,https://bdtechtalks.com/2020/08/17/openai-gpt-3-commercial-ai/,17,1597669839.0,
46,artificial,gpt-3,top,2022-12-27 16:01:57,"I built a web app tool to paraphrase, grammar check, and summarize text with OpenAI GPT-3. Details in the comment",Austin_Nguyen_2k,False,0.91,56,zwixsv,https://v.redd.it/oobs6hlqqg8a1,12,1672156917.0,
47,artificial,gpt-3,top,2022-08-23 15:06:26,OpenAI cuts prices for GPT-3 by two thirds,Zirius_Sadfaces,False,0.95,58,wvr7q5,https://mixed-news.com/en/openai-cuts-prices-for-gpt-3-by-two-thirds/,5,1661267186.0,
48,artificial,gpt-3,top,2022-03-12 04:56:02,Microsoft’s Latest Machine Learning Research Introduces μTransfer: A New Technique That Can Tune The 6.7 Billion Parameter GPT-3 Model Using Only 7% Of The Pretraining Compute,No_Coffee_4638,False,0.95,58,tc8u17,https://www.reddit.com/r/artificial/comments/tc8u17/microsofts_latest_machine_learning_research/,0,1647060962.0,"Scientists conduct trial and error procedures which experimenting, that many times lear to freat scientific breakthroughs. Similarly, foundational research provides for developing large-scale AI systems theoretical insights that reduce the amount of trial and error required and can be very cost-effective.

Microsoft team tunes massive neural networks that are too expensive to train several times. For this, they employed a specific parameterization that maintains appropriate hyperparameters across varied model sizes. The used µ-Parametrization (or µP, pronounced “myu-P”) is a unique way to learn all features in the infinite-width limit. The researchers collaborated with the OpenAI team to test the method’s practical benefit on various realistic cases.

Studies have shown that training large neural networks because their behavior changes as they grow in size are uncertain. Many works suggest heuristics that attempt to maintain consistency in the activation scales at initialization. However, as training progresses, this uniformity breaks off at various model widths.

[**CONTINUE READING MY SUMMARY ON THIS RESEARCH**](https://www.marktechpost.com/2022/03/11/microsofts-latest-machine-learning-research-introduces-%ce%bctransfer-a-new-technique-that-can-tune-the-6-7-billion-parameter-gpt-3-model-using-only-7-of-the-pretraining-compute/)

Paper: https://www.microsoft.com/en-us/research/uploads/prod/2021/11/TP5.pdf

Github:https://github.com/microsoft/mup

https://i.redd.it/gmn30ut8wvm81.gif"
49,artificial,gpt-3,top,2021-07-16 22:02:59,Facebook AI Releases ‘BlenderBot 2.0’: An Open Source Chatbot That Builds Long-Term Memory And Searches The Internet To Engage In Intelligent Conversations With Users,techsucker,False,0.95,57,olr4gk,https://www.reddit.com/r/artificial/comments/olr4gk/facebook_ai_releases_blenderbot_20_an_open_source/,9,1626472979.0,"The GPT-3 and [BlenderBot 1.0](https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/) models are extremely forgetful, but that’s not the worst of it! They’re also known to “hallucinate” knowledge when asked a question they can’t answer.

It is no longer a matter of whether or not machines will learn, but how. And while many companies are currently investing in so-called “deep learning” models that focus on training ever larger and more complex neural networks (and their model weights) to achieve greater levels of sophistication by making them store what they have learned during the course/training process, it has proven difficult for these large models to keep up with changes occurring online every minute as new information continually floods into its repository from all over the internet.

Summary: [https://www.marktechpost.com/2021/07/16/facebook-ai-releases-blenderbot-2-0-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet-to-engage-in-intelligent-conversations-with-users/](https://www.marktechpost.com/2021/07/16/facebook-ai-releases-blenderbot-2-0-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet-to-engage-in-intelligent-conversations-with-users/) 

Paper 1: https://github.com/facebookresearch/ParlAI/blob/master/projects/sea/Internet\_Augmented\_Dialogue.pdf

Paper 2: https://github.com/facebookresearch/ParlAI/blob/master/projects/msc/msc.pdf

Codes: https://parl.ai/projects/blenderbot2/"
50,artificial,gpt-3,top,2020-07-30 00:30:35,Giving GPT-3 a Turing Test,PowerOfLove1985,False,0.92,56,i0c78j,https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html,21,1596069035.0,
51,artificial,gpt-3,top,2023-02-06 23:35:17,12 highlights from Google's BARD announcement,ForkingHard,False,0.95,57,10vlww3,https://www.reddit.com/r/artificial/comments/10vlww3/12_highlights_from_googles_bard_announcement/,13,1675726517.0,"I went through the entire blog post from Google and pulled out some quotes and highlights:

&#x200B;

## 1) “we re-oriented the company around AI six years ago”

Right off the bat, “Pich-AI” lets it be known that Google is now an AI company. 

Partially true? Yes, of course. 

Would that phrase be coming out of his mouth at this point if not for the release and success of ChatGPT? No. 

## 2) their mission: “organize the world’s information and make it universally accessible and useful”

There’s a book called *The Innovator’s Dilemma: When New Technologies Cause Great Firms to Fail*. 

I'm certainly not here to say that Google is going to fail, but the re-stating of the mission makes it clear that they view AI (and Bard) as a way to improve, supplement, and perhaps protect their search business. This is why the features you’re about to read about are all search-focused. 

But what if the AI revolution isn’t just about “organizing” and making information “accessible”, but rather about “creating”? 

Something to think about. 

## 3) “the scale of the largest AI computations is doubling every six months, far outpacing Moore’s Law”

Moore’s Law says that computing power doubles every two years. Google says that speed is actually 6 months with AI. 

Imagine, then, how quickly things will improve if the capabilities we see today DOUBLE by summer in the Northern Hemisphere. 

## 4) “fresh, high-quality responses… learn more about the best strikers in football right now”

A clear dig at ChatGPT, which is trained on data through 2021 and still serves Her Majesty, The Queen of England… for now. 

Microsoft’s New Bing may debut with the newest version of ChatGPT by Wednesday. And it will presumably include up-to-date results. So this may be a *very* short-lived advantage. 

## 5) “experimental”

Not even Beta. Not Alpha. Experimental. This is a shield for when it inevitably gets something grotesquely wrong. Google has more reputational risk than OpenAI and Bing 😭. 

## 6) “lightweight model version of LaMDA… this much smaller model requires significantly less computing power, enabling us to scale to more users, allowing for more feedback”

In short, they are not releasing the full thing. So this means one of two things: 

1) They have preached caution and don’t want to release their most advanced tech until the world is ready for it. 

2) It’s a hedge. So if Bard sucks, they can say they have something better. 

## 7) “meet a high bar for quality, safety and groundedness in real-world information”

I’d argue this is another dig at OpenAI’s more… liberal approach to releasing AI. But, like Apple and privacy, Google seems to be taking the *adult in the room*approach with AI. 

## 8) “we’re working to bring [language, image, and music] AI advancements into our products, starting with Search”

As we’ve noted before, Google is working on image, video, and music generation AI. 

## 9) “safe and scaleable” APIs for developers

While ChatGPT gets all the pub, it’s OpenAI’s APIs, which allow developers to build apps atop their technology, that may be the real game-changer. 

Google is making it clear they will play that game, too, but do so in a more measured way. 

## 10) “bring experiences rooted in these models to the world in a bold and responsible way”

OK now they let the PR guy have too much fun. 

When was the last time you ever met someone who is Bold and Responsible? 

Tom Cruise jumping out of an airplane 80 times to get the next scene right is bold, but it’s not responsible. 

Going to bed at 10PM is responsible, but it’s hardly bold. Bold is partying until 2AM, watching a few episodes of Family Guy, eating a bag of popcorn, and downing two hard seltzers, all to wake up at 6:12AM to get started on the latest SR newsletter. THAT’S bold. 

Anyway, you get the point. Hard to be both, Google. 

## 11) “turning to us for quick factual answers, like how many keys does a piano have?… but increasingly, people are turning to Google for deeper insights and understanding”

Basically, Google doesn’t want to provide just facts. It wants to provide detailed, nuanced answers to queries, with context, in a natural-language format. 

The question, as it is with ChatGPT, is *where does the information come from?*  

If you thought creators and publishers were bent out of shape over ChatGPT and image apps, like Stable Diffusion and MidJourney, “training” on their data and remixing it without credit, how will website owners, who rely on Google for views, react when Google remixes the content atop search results? 

\[They already do this with snippets, but Bard sounds like snippets on steroids.\] 

## 12) “soon, you’ll see AI-powered features in Search that distill complex information and multiple perspectives into easy-to-digest formats”

Yep, snippets on steroids sounds about right.

&#x200B;

&#x200B;

This is the full context of what was in our newsletter today. No expectation, but if you found it interesting, feel free to subscribe: [https://smokingrobot.beehiiv.com](https://smokingrobot.beehiiv.com)"
52,artificial,gpt-3,top,2023-12-28 14:40:57,AI journey in optimizing visual accessibility,MostlySubmissive,False,0.94,54,18svg6l,https://www.reddit.com/r/artificial/comments/18svg6l/ai_journey_in_optimizing_visual_accessibility/,7,1703774457.0," So I work in the fast-paced world of web development and then by a night, I become an enthusiastic content creator with a profound interest in artificial intelligence. As part of my efforts to improve visual experiences of artificial intelligence, I have looked into a number of technologies. Each presented a unique set of obstacles, such as deciphering the intricacies of Google's Lookout or mastering Microsoft's Seeing AI. There was definitely work involved, especially when it came to fusing dynamic content with AI-generated alt text. Have you encountered any comparable AI problems?

Recently, I stumbled upon an application that serves as a virtual guide, simplifying the process of creating descriptions for visual content. The key to improving information accessibility lies in AI models' ability to recognize and respond to visual cues. This application, let's call it ""VisualAssist,"" seamlessly integrates with text and images, generating captivating captions and elucidating even the most subtle details. What's truly remarkable is its extensive support for a range of AI models, from GPT-3.5's text-to-image capabilities to DALL-E's stunning visual creations. Its adaptability opens up new possibilities, enriching the visual narrative in ways we hadn't previously considered. To showcase its impact, user-friendly images demonstrate how it makes text more comprehensible to a broader audience. It's the missing link that transforms images into storytelling tools, enhancing visual communication.

Have you run into any problems incorporating AI into your creative process that are comparable to mine? Which tools have you looked into, and what level of visual accessibility do they offer? "
53,artificial,gpt-3,top,2021-06-28 18:00:19,"Last Week in AI - DeepMind scientist calls for ethical AI as Google faces ongoing backlash, LinkedIn’s job-matching AI was biased, GAN GTA 5, GPT-3 Search Engines, and more!",SkynetToday,False,0.88,58,o9ppqw,https://lastweekin.ai/p/122,7,1624903219.0,
54,artificial,gpt-3,top,2021-05-24 14:46:04,EleutherAI Develops GPT-3’s Free Alternative: GPT-Neo,techsucker,False,0.97,58,njzmjq,https://www.reddit.com/r/artificial/comments/njzmjq/eleutherai_develops_gpt3s_free_alternative_gptneo/,5,1621867564.0,"In today’s era, all top benchmarks in natural language processing are dominated by Transformer-based models. In a machine learning model, the most critical elements of the training process are the model code, training data, and available computing resources.

With the Transformer family of models, researchers have now finally come up with a way to increase the performance of a model infinitely by increasing the amount of training data and compute power.

OpenAI did this with GPT-2 and with GPT-3. They used a private corpus of 500 billion tokens for training the model and spent $50 million in computing costs.

Full Article: [https://www.marktechpost.com/2021/05/24/eleutherai-develops-gpt-3s-free-alternative-gpt-neo/](https://www.marktechpost.com/2021/05/24/eleutherai-develops-gpt-3s-free-alternative-gpt-neo/?_ga=2.62220524.1924646600.1621739878-488125022.1618729090)

Github: [https://github.com/EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo)"
55,artificial,gpt-3,top,2023-12-01 02:12:38,Microsoft Releases Convincing Case Study Showing Chain of Thought (CoT) with GPT 4 Versus Fine Tuned Models via Medprompt and CoT Prompting Strategies,Xtianus21,False,0.95,52,18807xu,https://www.reddit.com/r/artificial/comments/18807xu/microsoft_releases_convincing_case_study_showing/,11,1701396758.0,"[https://arxiv.org/pdf/2311.16452](https://arxiv.org/pdf/2311.16452)

A great read. I'll pull out the important parts.

November 2023

&#x200B;

https://preview.redd.it/cyf6y5fubl3c1.png?width=1059&format=png&auto=webp&s=2a1b559ebfdd0900ab7dc84d3dc7088470b3bb2a

Figure 1: (a) Comparison of performance on MedQA. (b) GPT-4 with Medprompt achieves SoTA on a wide range of medical challenge questions.

A core metric for characterizing the performance of foundation models is the accuracy of next word prediction. Accuracy with next word prediction is found to increase with scale in training data, model parameters, and compute, in accordance with empirically derived “neural model scaling laws” \[3, 12\]). However, beyond predictions of scaling laws on basic measures such as next word prediction, foundation models show the sudden emergence of numerous problem-solving capabilities at different thresholds of scale \[33, 27, 24\].

Despite the observed emergence of sets of general capabilities, questions remain about whether truly exceptional performance can be achieved on challenges within specialty areas like medicine in the absence of extensive specialized training or fine-tuning of the general models. Most explorations of foundation model capability on biomedical applications rely heavily on domain- and task-specific fine-tuning. With first-generation foundation models, the community found an unambiguous advantage with domain-specific pretraining, as exemplified by popular models in biomedicine such as 2 PubMedBERT \[10\] and BioGPT \[19\]. But it is unclear whether this is still the case with modern foundation models pretrained at much larger scale.

We present results and methods of a case study on steering GPT-4 to answer medical challenge questions with innovative prompting strategies. We include a consideration of best practices for studying prompting in an evaluative setting, including the holding out of a true eyes-off evaluation set. We discover that GPT-4 indeed possesses deep specialist capabilities that can be evoked via prompt innovation. The performance was achieved via a systematic exploration of prompting strategies. As a design principle, we chose to explore prompting strategies that were inexpensive to execute and not customized for our benchmarking workload. We converged on a top prompting strategy for GPT-4 for medical challenge problems, which we refer to as Medprompt. Medprompt unleashes medical specialist skills in GPT-4 in the absence of expert crafting, easily topping existing benchmarks for all standard medical question-answering datasets. The approach outperforms GPT-4 with the simple prompting strategy and state-of-the-art specialist models such as Med-PaLM 2 by large margins. On the MedQA dataset (USMLE exam), Medprompt produces a 9 absolute point gain in accuracy, surpassing 90% for the first time on this benchmark. 

As part of our investigation, we undertake a comprehensive ablation study that reveals the relative significance for the contributing components of Medprompt. We discover that a combination of methods, including in-context learning and chain-of-thought, can yield synergistic effects. Perhaps most interestingly, we find that the best strategy in steering a generalist model like GPT-4 to excel on the medical specialist workload that we study is to use a generalist prompt. We find that GPT-4 benefits significantly from being allowed to design its prompt, specifically with coming up with its own chain-of-thought to be used for in-context learning. This observation echoes other reports that GPT-4 has an emergent self-improving capability via introspection, such as self-verification \[9\].

\>>> Extractions from \[9\] [https://openreview.net/pdf?id=SBbJICrglS](https://openreview.net/pdf?id=SBbJICrglS)  Published: 20 Jun 2023, Last Modified: 19 Jul 2023 <<<

&#x200B;

https://preview.redd.it/wb3kj4btbl3c1.png?width=1027&format=png&auto=webp&s=0268c29e1f8bbeb898577bd712fdfa1042fb5d7d

Experiments on various clinical information extraction tasks and various LLMs, including ChatGPT (GPT-4) (OpenAI, 2023) and ChatGPT (GPT-3.5) (Ouyang et al., 2022), show the efficacy of SV. In addition to improving accuracy, we find that the extracted interpretations match human judgements of relevant information, enabling auditing by a human and helping to build a path towards trustworthy extraction of clinical information in resource-constrained scenarios.

Fig. 1 shows the four different steps of the introduced SV pipeline. The pipeline takes in a raw text input, e.g. a clinical note, and outputs information in a pre-specified format, e.g. a bulleted list. It consists of four steps, each of which calls the same LLM with different prompts in order to refine and ground the original output. The original extraction step uses a task-specific prompt which instructs the model to output a variable-length bulleted list. In the toy example in Fig. 1, the goal is to identify the two diagnoses Hypertension and Right adrenal mass, but the original extraction step finds only Hypertension. After the original LLM extraction, the Omission step finds missing elements in the output; in the Fig. 1 example it finds Right adrenal mass and Liver fibrosis. For tasks with long inputs (mean input length greater than 2,000 characters), we repeat the omission step to find more potential missed elements (we repeat five times, and continue repeating until the omission step stops finding new omissions).

3. Results 3.1. Self-verification improves prediction performance Table 2 shows the results for clinical extraction performance with and without self-verification. Across different models and tasks, SV consistently provides a performance improvement. The performance improvement is occasionally quite large (e.g. ChatGPT (GPT-4) shows more than a 0.1 improvement in F1 for clinical trial arm extraction and more than a 0.3 improvement for medication status extraction), and the average F1 improvement across models and tasks is 0.056. We also compare to a baseline where we concatenate the prompts across different steps into a single large prompt which is then used to make a single LLM call for information extraction. We find that this large-prompt baseline performs slightly worse than the baseline reported in Table 2, which uses a straightforward prompt for extraction (see comparison details in Table A5).

<<< Reference \[9\] end >>>

2.2 Prompting Strategies

Prompting in the context of language models refers to the input given to a model to guide the output that it generates. Empirical studies have shown that the performance of foundation models on a specific task can be heavily influenced by the prompt, often in surprising ways. For example, recent work shows that model performance on the GSM8K benchmark dataset can vary by over 10% without any changes to the model’s learned parameters \[35\]. Prompt engineering refers to the process of developing effective prompting techniques that enable foundation models to better solve specific tasks. Here, we briefly introduce a few key concepts that serve as building blocks for our Medprompt approach.

Chain of Thought (CoT) is a prompting methodology that employs intermediate reasoning steps prior to introducing the sample answer \[34\]. By breaking down complex problems into a series 4 of smaller steps, CoT is thought to help a foundation model to generate a more accurate answer. CoT ICL prompting integrates the intermediate reasoning steps of CoT directly into the few-shot demonstrations. As an example, in the Med-PaLM work, a panel of clinicians was asked to craft CoT prompts tailored for complex medical challenge problems \[29\]. Building on this work, we explore in this paper the possibility of moving beyond reliance on human specialist expertise to mechanisms for generating CoT demonstrations automatically using GPT-4 itself. As we shall describe in more detail, we can do this successfully by providing \[question, correct answer\] pairs from a training dataset. We find that GPT-4 is capable of autonomously generating high-quality, detailed CoT prompts, even for the most complex medical challenges.

Self-Generated Chain of Thought

&#x200B;

https://preview.redd.it/47qku12dcl3c1.png?width=820&format=png&auto=webp&s=a8e3a393e92e7dac8acdd5b25310933f72d38788

Chain-of-thought (CoT) \[34\] uses natural language statements, such as “Let’s think step by step,” to explicitly encourage the model to generate a series of intermediate reasoning steps. The approach has been found to significantly improve the ability of foundation models to perform complex reasoning. Most approaches to chain-of-thought center on the use of experts to manually compose few-shot examples with chains of thought for prompting \[30\]. Rather than rely on human experts, we pursued a mechanism to automate the creation of chain-of-thought examples. We found that we could simply ask GPT-4 to generate chain-of-thought for the training examples using the following prompt:

&#x200B;

https://preview.redd.it/irfh2hnkcl3c1.png?width=907&format=png&auto=webp&s=fbc6d4d6749b630658de932a80a4bd4b7b97d003

A key challenge with this approach is that self-generated CoT rationales have an implicit risk of including hallucinated or incorrect reasoning chains. We mitigate this concern by having GPT-4 generate both a rationale and an estimation of the most likely answer to follow from that reasoning chain. If this answer does not match the ground truth label, we discard the sample entirely, under the assumption that we cannot trust the reasoning. While hallucinated or incorrect reasoning can still yield the correct final answer (i.e. false positives), we found that this simple label-verification step acts as an effective filter for false negatives. 

We observe that, compared with the CoT examples used in Med-PaLM 2 \[30\], which are handcrafted by clinical experts, CoT rationales generated by GPT-4 are longer and provide finer-grained step-by-step reasoning logic. Concurrent with our study, recent works \[35, 7\] also find that foundation models write better prompts than experts do.

&#x200B;

https://preview.redd.it/lcb8lae1dl3c1.png?width=904&format=png&auto=webp&s=c321e625136360622a254d41852a3980b60de624

Medprompt combines intelligent few-shot exemplar selection, self-generated chain of thought steps, and a majority vote ensemble, as detailed above in Sections 4.1, 4.2, and 4.3, respectively. The composition of these methods yields a general purpose prompt-engineering strategy. A visual depiction of the performance of the Medprompt strategy on the MedQA benchmark, with the additive contributions of each component, is displayed in Figure 4. We provide an a corresponding algorithmic description in Algorithm 1.

Medprompt consists of two stages: a preprocessing phase and an inference step, where a final prediction is produced on a test case.

Algorithm 1 Algorithmic specification of Medprompt, corresponding to the visual representation of the strategy in Figure 4.

We note that, while Medprompt achieves record performance on medical benchmark datasets, the algorithm is general purpose and is not restricted to the medical domain or to multiple choice question answering. We believe the general paradigm of combining intelligent few-shot exemplar selection, self-generated chain of thought reasoning steps, and majority vote ensembling can be broadly applied 11 to other problem domains, including less constrained problem solving tasks (see Section 5.3 for details on how this framework can be extended beyond multiple choice questions).

Results

&#x200B;

https://preview.redd.it/jeckyxlvdl3c1.png?width=766&format=png&auto=webp&s=844c8c890a2c0025776dca2c95fa8919ffbc94c1

With harnessing the prompt engineering methods described in Section 4 and their effective combination as Medprompt, GPT-4 achieves state-of-the-art performance on every one of the nine benchmark datasets in MultiMedQA"
56,artificial,gpt-3,top,2023-07-27 11:26:24,"How likely is it for a small company to develop a model that outperforms the big ones (GPT, Bard etc)?",BigBootyBear,False,0.92,53,15azbve,https://www.reddit.com/r/artificial/comments/15azbve/how_likely_is_it_for_a_small_company_to_develop_a/,65,1690457184.0,"There are 3 players in the AI space right now. All purpose LLM titans (Google, OpenAI, Meta), fancy domain specific apps that consume one of the big LLMs under the hood, and custom developed models.

I know how to judge the second type as they basically can do everything the first one can but have a pretty GUI to boot. But what about the third ones? How likely is it for a (www.yet-another-ai-startup.ai) sort of company to develop a model that outperforms GPT on a domain specific task?"
57,artificial,gpt-3,top,2022-06-23 07:39:03,We have AI generated art now. We have AI generated conversation. But where are the AI generated music compositions?,moschles,False,0.91,57,virbwe,https://www.reddit.com/r/artificial/comments/virbwe/we_have_ai_generated_art_now_we_have_ai_generated/,42,1655969943.0,"AI generated images from text prompts are making the rounds with Dalle mini and DALLE.2.  These systems are so powerful that people are admitting they [cannot tell real from fake images anymore.](https://www.reddit.com/r/dalle2/comments/viamr7/that_weird_moment_when_you_browse_reddit_and_no/)  

Google's LaMDA is producing conversational text chats that are so realistic that they spawned entire subreddits where users [claim the software agent has become sentient.](https://www.reddit.com/r/LaMDAisSentient/)  

So where is the  instrumental and orchestral music that is indifferentiable from human composers? 

In recent months I had heard some song continuations, where an AI was trained on the *wave form* of popular music, which was asked to continue. Those were fine, but ended up sounding like [strange incoherent fever dreams.](https://www.youtube.com/watch?v=8sFXsP71wfA)   I fiddled with some midi-like continuations on a website. The output was janky, repetitive, and obviously computer-generated.  It was obvious to me that the AI agent was not a large transformer model ( the likes of GPT-3.  )

Composed classical music is a sequence of notes organized into measures, often architected together by one or more cohesive themes.     Foundation models and large transformer models were originally meant to specifically operate upon and learn  sequences of tokens. The baroque composer,  Domenico Scarlatti composed over 500 keyboard works for solo harpsichord, all of which were converted into electronic format years ago.   Haydn wrote 68 quartets.  Because of the above reasons, it seems like we should be hearing AI generated classical music by now.  I should be clicking headlines with a symphony performed by such-and-such orchestra at the blah-blah School of Music.   What I hear should sound like a symphony by Mozart, until it is revealed an AI wrote it. 

Yet we don't see these.   I have a few hypothesese why not :  

+  Large transformer models are very expensive, and there is no market downstream for a product that does this.

+ A lot of music is not in the public domain.  Derivative works in the medium of audio are known to be [litigated for too much of a likeness to existing copyrighted music.](https://en.wikipedia.org/wiki/Stairway_to_Heaven#Spirit_copyright_infringement_lawsuit)  


Your thoughts?"
58,artificial,gpt-3,top,2021-01-25 01:31:01,OpenAI Introduces CLIP: A Neural Network That Efficiently Learns Visual Concepts From Natural Language Supervision,ai-lover,False,1.0,55,l4cs1c,https://www.reddit.com/r/artificial/comments/l4cs1c/openai_introduces_clip_a_neural_network_that/,3,1611538261.0,"OpenAI introduced a neural network, CLIP, which efficiently learns visual concepts from natural language supervision. CLIP, also called *Contrastive Language–Image Pre-training*, is available to be applied to any visual classification benchmark by merely providing the visual categories’ names to be recognized. Users find the above similar to the “zero-shot” capabilities of GPT-2 and 3.

The current deep-learning approach to computer vision has several significant problems such as:

1. Typical vision datasets require a lot of labor.
2.  It is expensive to create while teaching only a narrow set of visual concepts;
3. The Standard vision models are good at one task only and require significant effort to adapt to a new task.
4. Models that perform well on benchmarks have a deficient performance on stress tests.

Summary: [https://www.marktechpost.com/2021/01/24/openai-introduces-clip-a-neural-network-that-efficiently-learns-visual-concepts-from-natural-language-supervision/](https://www.marktechpost.com/2021/01/24/openai-introduces-clip-a-neural-network-that-efficiently-learns-visual-concepts-from-natural-language-supervision/)

Paper: https://cdn.openai.com/papers/Learning\_Transferable\_Visual\_Models\_From\_Natural\_Language\_Supervision.pdf

Codes: [https://github.com/openai/CLIP](https://github.com/openai/CLIP)"
59,artificial,gpt-3,top,2021-09-18 07:08:41,Google AI Introduces Two New Families of Neural Networks Called ‘EfficientNetV2’ and ‘CoAtNet’ For Image Recognition,techsucker,False,0.93,50,pqhqhj,https://www.reddit.com/r/artificial/comments/pqhqhj/google_ai_introduces_two_new_families_of_neural/,1,1631948921.0,"Training efficiency has become a significant factor for deep learning as the neural network models, and training data size grows. [GPT-3](https://arxiv.org/abs/2005.14165) is an excellent example to show how critical training efficiency factor could be as it takes weeks of training with thousands of GPUs to demonstrate remarkable capabilities in few-shot learning.

To address this problem, the Google AI team introduce two families of neural networks for image recognition. First is [EfficientNetV2](https://arxiv.org/abs/2104.00298), consisting of CNN (Convolutional neural networks) with a small-scale dataset for faster training efficiency such as [ImageNet1k](https://www.image-net.org/) (with 1.28 million images). Second is a hybrid model called [CoAtNet](https://arxiv.org/abs/2106.04803), which combines [convolution](https://en.wikipedia.org/wiki/Convolution) and [self-attention](https://en.wikipedia.org/wiki/Self-attention) to achieve higher accuracy on large-scale datasets such as [ImageNet21](https://www.image-net.org/) (with 13 million images) and [JFT](https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html) (with billions of images). As per the research report by Google, [EfficientNetV2](https://arxiv.org/abs/2104.00298) and [CoAtNet](https://arxiv.org/abs/2106.04803) both are 4 to 10 times faster while achieving state-of-the-art and 90.88% top-1 accuracy on the well-established [ImageNet](https://www.image-net.org/) dataset.

# [7 Min Read](https://www.marktechpost.com/2021/09/17/google-ai-introduces-two-new-families-of-neural-networks-called-efficientnetv2-and-coatnet-for-image-recognition/) | [Paper (CoAtNet)](https://arxiv.org/abs/2106.04803) | [Paper (EfficientNetV2)](https://arxiv.org/abs/2104.00298) | [Google blog](https://ai.googleblog.com/2021/09/toward-fast-and-accurate-neural.html) | [Code](https://github.com/google/automl/tree/master/efficientnetv2)

&#x200B;

https://preview.redd.it/slkd0mkdo7o71.png?width=1392&format=png&auto=webp&s=2afd86b8208ba1499d7d62b176a99aa7d6d498e9"
60,artificial,gpt-3,top,2021-02-02 14:24:38,"OpenAI's GPT-3 Speaks! ""It isn’t clear whether GPT-3 will ever be trustworthy enough to act on its own.""",ChrisTweten,False,0.87,53,lawlax,https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/open-ais-powerful-text-generating-tool-is-ready-for-business,41,1612275878.0,
61,artificial,gpt-3,top,2023-07-13 04:09:12,One-Minute Daily AI News 7/12/2023,Excellent-Target-847,False,0.95,49,14ya8vy,https://www.reddit.com/r/artificial/comments/14ya8vy/oneminute_daily_ai_news_7122023/,29,1689221352.0,"1. **Anthropic**, the AI startup co-founded by ex-OpenAI execs, today announced the release of a new text-generating AI model, **Claude 2**. The successor to Anthropic’s first commercial model, Claude 2 is available in beta starting today in the U.S. and U.K. both on the web and via a paid API.\[1\]
2. **Elon Musk** has launched an AI company to challenge ChatGPT creator OpenAI, which the billionaire tech mogul has accused of being “woke”. On Wednesday, **xAI** said the goal of the new company would be to “understand the true nature of the universe”.\[2\]
3. Chip designer **Nvidia** will invest $50 million to speed up training of Recursion’s artificial intelligence models for drug discovery, the companies said on Wednesday, sending the biotech firm’s shares surging about 83%.\[3\]
4. For decades, morning weather reports have relied on the same kinds of conventional models. Now, weather forecasting is poised to join the ranks of industries revolutionized by artificial intelligence.A pair of papers, published Wednesday in the scientific journal **Nature**, touts the potential of two new AI forecasting approaches — systems that could yield faster and more accurate results than traditional models, researchers say.\[4\]

Sources:

 \[1\] [https://techcrunch.com/2023/07/11/anthropic-releases-claude-2-the-second-generation-of-its-ai-chatbot/](https://techcrunch.com/2023/07/11/anthropic-releases-claude-2-the-second-generation-of-its-ai-chatbot/)

\[2\] [https://www.aljazeera.com/economy/2023/7/13/musk-launches-artificial-intelligence-rival-to-chatgpts-openai](https://www.aljazeera.com/economy/2023/7/13/musk-launches-artificial-intelligence-rival-to-chatgpts-openai)

\[3\] [https://www.reuters.com/technology/nvidia-invests-50-mln-recursion-train-ai-models-drug-discovery-2023-07-12/](https://www.reuters.com/technology/nvidia-invests-50-mln-recursion-train-ai-models-drug-discovery-2023-07-12/)

\[4\] [https://www.scientificamerican.com/article/climate-change-could-stump-ai-weather-prediction/](https://www.scientificamerican.com/article/climate-change-could-stump-ai-weather-prediction/) "
62,artificial,gpt-3,top,2021-07-04 20:00:05,GPT-J: GPT-3 Democratized,rshpkamil,False,0.96,48,odrudf,https://www.reddit.com/r/artificial/comments/odrudf/gptj_gpt3_democratized/,1,1625428805.0,"Link to the original article: [https://www.p3r.one/gpt-j/](https://www.p3r.one/gpt-j/)

&#x200B;

More hard-to-find stuff related to AI & Data Science [here](https://thereshape.co)."
63,artificial,gpt-3,top,2023-07-19 13:06:34,New study quantifies degradation in GPT-4 for the first time,Successful-Western27,False,0.82,48,153ujqr,https://www.reddit.com/r/artificial/comments/153ujqr/new_study_quantifies_degradation_in_gpt4_for_the/,25,1689771994.0,"I've collected a half-dozen threads [on Twitter](https://twitter.com/mikeyoung44/status/1672971689573990400) from this subreddit of user complaints since March about the degraded quality of GPT outputs. I've noticed a huge drop in quality myself. A common (reasonable) response from some people was that the drop in quality was the result of perception anchoring, desensitization, or something unrelated to the overall performance of the model.

**A new study** by researchers Chen, Zaharia, and Zou at Stanford and UC Berkley now confirms that these perceived degradations are quantifiable and significant between the different versions of the LLMs (March and June 2023). They find:

* ""For GPT-4, the percentage of \[code\] generations that are directly executable dropped from **52.0% in March to 10.0% in June.** The drop was also large for GPT-3.5 **(from 22.0% to 2.0%)**."" **(!!!)**
* For sensitive questions: ""An example query and responses of GPT-4 and GPT-3.5 at different dates. In March, GPT-4 and GPT-3.5 were verbose and gave detailed explanation for why it did not answer the query. **In June, they simply said sorry.""**
* ""GPT-4 (March 2023) was very good at identifying prime numbers **(accuracy 97.6%)** but GPT-4 (June 2023) was very poor on these same questions **(accuracy 2.4%)**. **Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task.""**

I think these underline that (a) the decline in quality was not just a pure perception thing, and (b) that we need a way to track model performance over time. Building a business on these APIs without controlling for performance drift is high-risk.

You can read a summary of the study [here](https://notes.aimodels.fyi/new-study-validates-user-rumors-of-degraded-chatgpt-performance/).

You can also find a link to the Arxiv paper [here](https://arxiv.org/pdf/2307.09009.pdf) and a link to the [Github here.](https://github.com/lchen001/LLMDrift)"
64,artificial,gpt-3,top,2024-01-08 21:04:03,"I know people love to hate AI, but...",SocksOnHands,False,0.71,47,191vz5v,https://www.reddit.com/r/artificial/comments/191vz5v/i_know_people_love_to_hate_ai_but/,59,1704747843.0,"If you are someone who had never used AI, or had only used ChatGPT 3.5, I'm going to be highly skeptical of any claims you make about AI capabilities and limitations.

We often wind up seeing strong claims, one way or the other, that are not based in reality, but instead motivated by fear or hatred.  There are people who hate AI images because it can never create ""real art"", while simultaneously fearing that it will become so good that it will steal all artists jobs.  People are so emotionally charged and cloudy headed, that they cannot do a level headed, honest assessment of this technology.

People who have never used ChatGPT, or have only used 3.5, love to parrot the same talking points about how it's useless because it makes mistakes.  What they never seem to consider is how ChatGPT actually works, because if they knew then they would realize that it is unreasonable it to have perfect knowledge and understanding - in much the same way that humans struggle to remember things they had learned years ago.  Can you accurately recall everything you studied in college?  If someone asked you to answer a math equation without using a calculator or scratch paper, can you arrive at the correct answer?  If you cannot do these things, should I question if you have any intelligence?

It might be sounding like I'm holding AI up on this grand pedestal, but really I'm just annoyed and frustrated by hearing the same bad arguments made over and over.  You can't say anything to correct anyone without getting dog piled with down votes.

Large language models are impressive, with their ability to do things computers had struggled with since their initial inception.  I'm sure Alan Turing would have been excited by all this if he were still alive today.  Criticizing large language models for not being able to easily solve complicated math problems is like criticizing cars for not being able to easily cross a deep river.  Cars are not boats and large language models are not calculators."
65,artificial,gpt-3,top,2023-09-27 00:16:14,Microsoft Researchers Propose AI Morality Test for LLMs in New Study,Successful-Western27,False,0.9,47,16t50vn,https://www.reddit.com/r/artificial/comments/16t50vn/microsoft_researchers_propose_ai_morality_test/,22,1695773774.0,"Researchers from Microsoft have just proposed using a psychological assessment tool called the Defining Issues Test (DIT) to evaluate the moral reasoning capabilities of large language models (LLMs) like GPT-3, ChatGPT, etc.

The DIT presents moral dilemmas and has subjects rate and rank the importance of various ethical considerations related to the dilemma. It allows quantifying the sophistication of moral thinking through a P-score.

In this new paper, the researchers tested prominent LLMs with adapted DIT prompts containing AI-relevant moral scenarios.

Key findings:

* Large models like **GPT-3 failed to comprehend prompts** and **scored near random** baseline in moral reasoning.
* **ChatGPT, Text-davinci-003 and GPT-4 showed coherent moral reasoning** with above-random P-scores.
* Surprisingly, the smaller **70B LlamaChat model outscored larger models in its P-score**, demonstrating advanced ethics understanding is possible without massive parameters.
* The models operated **mostly at intermediate conventional levels** as per Kohlberg's moral development theory. **No model exhibited highly mature moral reasoning.**

I think this is an interesting framework to evaluate and improve LLMs' moral intelligence before deploying them into sensitive real-world environments - to the extent that a model can be said to possess moral intelligence (or, seem to possess it?).

Here's [a link to my full summary](https://notes.aimodels.fyi/microsoft-researchers-propose-ai-morality-test-for-llms/) with a lot more background on Kohlberg's model (had to read up on it since I didn't study psych). Full paper is [here](https://arxiv.org/pdf/2309.13356.pdf)"
66,artificial,gpt-3,top,2023-05-27 15:59:14,How long before we'll be able to train LLMs on google colab (GUANACO DISCUSSION),Agatsuma_Zenitsu_21,False,0.98,48,13tb1yx,https://i.redd.it/nkjgynm6uf2b1.png,16,1685203154.0,Guanaco has proved that efficient methods exist to train LLMs without lots of heavy GPUs.
67,artificial,gpt-3,top,2023-07-13 23:08:41,NPC Steven acknowledged me finally!! 🤯 ChatGPT driven agents in Unreal Engine - update 3,Chance_Confection_37,False,0.88,43,14yzinn,https://v.redd.it/dtyxamtrbtbb1,15,1689289721.0,
68,artificial,gpt-3,top,2021-02-25 05:47:53,[N] New Contextual Calibration Method Boosts GPT-3 Accuracy Up to 30%,Yuqing7,False,0.97,49,lrzghq,https://www.reddit.com/r/artificial/comments/lrzghq/n_new_contextual_calibration_method_boosts_gpt3/,7,1614232073.0,"A research team from UC Berkeley, University of Maryland and UC Irvine identifies pitfalls that cause instability in the GPT-3 language model and proposes a contextual calibration procedure that improves accuracy by up to 30 percent.

Here is a quick read: [New Contextual Calibration Method Boosts GPT-3 Accuracy Up to 30%](https://syncedreview.com/2021/02/24/new-contextual-calibration-method-boosts-gpt-3-accuracy-up-to-30/)

The paper *Calibrate Before Use: Improving Few-Shot Performance of Language Models* is on [arXiv](https://arxiv.org/pdf/2102.09690.pdf)."
69,artificial,gpt-3,top,2021-09-01 14:52:05,"GPT-3 mimics human love for ‘offensive’ Reddit comments, study finds",estasfuera,False,0.9,45,pfvhob,https://thenextweb.com/news/gpt-3-and-humans-twice-as-likely-agree-with-offensive-reddit-comments-chatbots,4,1630507925.0,
70,artificial,gpt-3,top,2023-12-12 18:12:27,What actually are the most popular AI tools?,ThatNoCodeGuy,False,0.87,45,18gsbka,https://www.reddit.com/r/artificial/comments/18gsbka/what_actually_are_the_most_popular_ai_tools/,38,1702404747.0,"Today I decided to go on a mission to find what the most used AI tools are that lurk through the hundreds of thousands of AI tools out there. (by monthly visits)

I think that some of these results may surprise you but obviously some won't, 'cough', ""ChatGPT""

Hope you guys enjoy

https://preview.redd.it/mss3j93vmw5c1.png?width=1080&format=png&auto=webp&s=ff4cd56fcd95599a21288e39028dd07821e13bb6

P.S. If you love this AI stuff just like me, I write all about the latest AI developments in my[ newsletter](https://businessbloopers.beehiiv.com/).

Anyways, I think that this post clearly showed that ChatGPT is comfortably leading the AI industry setting the benchmark for what is expected by other AI developers.

From September 2022 to August 2023, the AI universe witnessed a whopping 24 billion visits to its top 50 tools. ChatGPT stole the show, boasting over 14 billion visits – a staggering 60% of the total traffic. These AI tools averaged a cool 2 billion monthly visits every month, spiking to 3.3 billion in the last half year.

We've seen tools like ChatGPT, Character AI, and Google Bard see big increases in visits, while others like Craiyon, MidJourney, and Quillbot took a breather (had fewer visits).

The U.S. rocked the numbers game with a hefty 5.5 billion visits (that's a solid 22.62% of the grand total), and Europe threw in an impressive 3.9 billion.

*In case some of the wording was too blurry here is a link to a detailed Notion page I made of each tool listed above:* [https://amusing-estimate-b13.notion.site/214a4a88d910434392e2f40040c03045?v=c2a65126e52c4e05b75c8cf0413a26dd](https://amusing-estimate-b13.notion.site/214a4a88d910434392e2f40040c03045?v=c2a65126e52c4e05b75c8cf0413a26dd)"
71,artificial,gpt-3,top,2024-01-19 15:43:01,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.97,47,19alyjg,https://www.reddit.com/r/artificial/comments/19alyjg/this_week_in_ai_all_the_major_ai_developments_in/,7,1705678981.0,"1. **Google DeepMind** introduced ***AlphaGeometry***, an AI system that solves complex geometry problems at a level approaching a human Olympiad gold-medalist. It was trained solely on synthetic data. The AlphaGeometry code and model has been open-sourced \[[*Details*](https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry) | [*GitHub*](https://github.com/google-deepmind/alphageometry)\].
2. **Codium AI** released ***AlphaCodium*****,** an open-source code generation tool that significantly improves the performances of LLMs on code problems. AlphaCodium is based on a test-based, multi-stage, code-oriented iterative flow instead of using a single prompt \[[*Details*](https://www.codium.ai/blog/alphacodium-state-of-the-art-code-generation-for-code-contests/) | [*GitHub*](https://github.com/Codium-ai/AlphaCodium)\].
3. **Apple** presented ***AIM***, a set of large-scale vision models pre-trained solely using an autoregressive objective. The code and model checkpoints have been released \[[*Paper*](https://arxiv.org/pdf/2401.08541.pdf) | [*GitHub*](https://github.com/apple/ml-aim)\].
4. **Alibaba** presents ***Motionshop***, a framework to replace the characters in video with 3D avatars \[[*Details*](https://aigc3d.github.io/motionshop/)\].
5. **Hugging Face** released ***WebSight***, a dataset of 823,000 pairs of website screenshots and HTML/CSS code. Websight is designed to train Vision Language Models (VLMs) to convert images into code. The dataset was created using Mistral-7B-v0.1 and and Deepseek-Coder-33b-Instruct \[[*Details*](https://huggingface.co/datasets/HuggingFaceM4/WebSight) *|* [*Demo*](https://huggingface.co/spaces/HuggingFaceM4/screenshot2html)\].
6. **Runway ML** introduced a new feature ***Multi Motion Brush*** in Gen-2 . It lets users control multiple areas of a video generation with independent motion \[[*Link*](https://x.com/runwayml/status/1747982147762188556?s=20)\].
7. **LMSYS** introduced ***SGLang*****,** *Structured Generation Language for LLMs***,** an interface and runtime for LLM inference that greatly improves the execution and programming efficiency of complex LLM programs by co-designing the front-end language and back-end runtime \[[*Details*](https://lmsys.org/blog/2024-01-17-sglang/)\].
8. **Meta** CEO Mark Zuckerberg said that the company is developing open source artificial general intelligence (AGI) \[[*Details*](https://venturebeat.com/ai/meta-is-all-in-on-open-source-agi-says-zuckerberg/)\].
9. **MAGNeT**, the text-to-music and text-to-sound model by Meta AI, is now on Hugging Face \[[*Link*](https://huggingface.co/collections/facebook/magnet-659ef0ceb62804e6f41d1466)\].
10. The Global Health Drug Discovery Institute (**GHDDI**) and **Microsoft Research** achieved significant progress in discovering new drugs to treat global infectious diseases by using generative AI and foundation models. The team designed several small molecule inhibitors for essential target proteins of Mycobacterium tuberculosis and coronaviruses that show outstanding bioactivities. Normally, this could take up to several years, but the new results were achieved in just five months. \[[*Details*](https://www.microsoft.com/en-us/research/blog/ghddi-and-microsoft-research-use-ai-technology-to-achieve-significant-progress-in-discovering-new-drugs-to-treat-global-infectious-diseases/)\].
11. US FDA provides clearance to **DermaSensor's** AI-powered real-time, non-invasive skin cancer detecting device **\[**[*Details*](https://www.dermasensor.com/fda-clearance-granted-for-first-ai-powered-medical-device-to-detect-all-three-common-skin-cancers-melanoma-basal-cell-carcinoma-and-squamous-cell-carcinoma/)**\].**
12. **Deci AI** announced two new models: ***DeciCoder-6B*** and ***DeciDiffuion 2.0.*** DeciCoder-6B, released under Apache 2.0, is a multi-language, codeLLM with support for 8 programming languages with a focus on memory and computational efficiency. DeciDiffuion 2.0 is a text-to-image 732M-parameter model that’s 2.6x faster and 61% cheaper than Stable Diffusion 1.5 with on-par image quality when running on Qualcomm’s Cloud AI 100 \[[*Details*](https://deci.ai/blog/decicoder-6b-the-best-multi-language-code-generation-llm-in-its-class)\].
13. **Figure**, a company developing autonomous humanoid robots signed a commercial agreement with BMW to deploy general purpose robots in automotive manufacturing environments \[[*Details*](https://x.com/adcock_brett/status/1748067775841697822)\].
14. **ByteDance** introduced ***LEGO***, an end-to-end multimodal grounding model that accurately comprehends inputs and possesses robust grounding capabilities across multi modalities,including images, audios, and video \[[*Details*](https://lzw-lzw.github.io/LEGO.github.io/)\].
15. **Google Research** developed ***Articulate Medical Intelligence Explorer (AMIE)***, a research AI system based on a LLM and optimized for diagnostic reasoning and conversations \[[*Details*](https://blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html)\].
16. **Stability AI** released **Stable Code 3B**, a 3 billion parameter Large Language Model, for code completion. Stable Code 3B outperforms code models of a similar size and matches CodeLLaMA 7b performance despite being 40% of the size \[[*Details*](https://stability.ai/news/stable-code-2024-llm-code-completion-release)\].
17. **Nous Research** released ***Nous Hermes 2 Mixtral 8x7B SFT*** , the supervised finetune only version of their new flagship Nous Research model trained over the Mixtral 8x7B MoE LLM. Also released an SFT+DPO version as well as a qlora adapter for the DPO. The new models are avaliable on [Together's](https://api.together.xyz/) playground \[[*Details*](https://x.com/NousResearch/status/1746988416779309143)\].
18. **Google Research** presented ***ASPIRE***, a framework that enhances the selective prediction capabilities of large language models, enabling them to output an answer paired with a confidence score \[[*Details*](https://blog.research.google/2024/01/introducing-aspire-for-selective.html)\].
19. **Microsoft** launched ***Copilot Pro***, a premium subscription of their chatbot, providing access to Copilot in Microsoft 365 apps, access to GPT-4 Turbo during peak times as well, Image Creator from Designer and the ability to build your own Copilot GPT \[[*Details*](https://blogs.microsoft.com/blog/2024/01/15/bringing-the-full-power-of-copilot-to-more-people-and-businesses)\].
20. **Samsung’s Galaxy S24** will feature Google Gemini-powered AI features **\[**[*Details*](https://techcrunch.com/2024/01/17/samsungs-galaxy-s24-will-feature-google-gemini-powered-ai-features/)**\].**
21. **Adobe** introduced new AI features in ***Adobe Premiere Pro*** including automatic audio category tagging, interactive fade handles and Enhance Speech tool that instantly removes unwanted noise and improves poorly recorded dialogue \[[*Details*](https://news.adobe.com/news/news-details/2024/Media-Alert-Adobe-Premiere-Pro-Innovations-Make-Audio-Editing-Faster-Easier-and-More-Intuitive/default.aspx)\].
22. **Anthropic** shares a research on ***Sleeper Agents*** where researchers trained LLMs to act secretly malicious and found that, despite their best efforts at alignment training, deception still slipped through \[[*Details*](https://arxiv.org/abs/2401.05566)\].
23. **Microsoft Copilot** is now using the previously-paywalled GPT-4 Turbo, saving you $20 a month \[[*Details*](https://www.windowscentral.com/software-apps/microsoft-copilot-is-now-using-the-previously-paywalled-gpt-4-turbo-saving-you-dollar20-a-month)\].
24. **Perplexity's** pplx-online LLM APIs, will power ***Rabbit R1*** for providing live up to date answers without any knowledge cutoff. And, the first 100K Rabbit R1 purchases will get 1 year of Perplexity Pro \[[*Link*](https://x.com/AravSrinivas/status/1748104684223775084)\].
25. **OpenAI** provided grants to 10 teams who developed innovative prototypes for using democratic input to help define AI system behavior. OpenAI shares their learnings and implementation plans \[[*Details*](https://openai.com/blog/democratic-inputs-to-ai-grant-program-update)\].

**Source**: AI Brews - you can subscribe the [newsletter here](https://aibrews.com/). it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. Links removed in this post due to Automod, but they are incuded in the newsletter. Thanks.  
"
72,artificial,gpt-3,top,2024-02-02 10:12:50,Best LLM ever after GPT4? CEO confirmed the accidentally” leaked” Mistral-Medium,Stupid_hardcorer,False,0.78,47,1ah0f9r,https://www.reddit.com/r/artificial/comments/1ah0f9r/best_llm_ever_after_gpt4_ceo_confirmed_the/,37,1706868770.0,"Mistral, a prominent open source AI company, recently experienced a leak involving an open source large language model (LLM) that is reportedly nearing the performance of GPT-4. This event marks a significant moment in the open source AI community, showcasing rapid advancements and the potential of open source models to compete with leading AI technologies like OpenAI's GPT-4.

**Key Points:**

1. **Leak of New AI Model:** A user identified as ""Miqu Dev"" posted files on HuggingFace, introducing a new LLM named ""miqu-1-70b"" which exhibits performance close to GPT-4, sparking considerable interest within the AI community.

https://preview.redd.it/l1gj4mwhg5gc1.png?width=1080&format=png&auto=webp&s=f33055d9fcb49f54c4cf5b351a19339ac9a85b66

https://preview.redd.it/d6dhlehtc5gc1.png?width=1200&format=png&auto=webp&s=335e0bb2550e3bac0de0174743ff85a685c99b26

2. **Widespread Attention:** The model's leak was first noticed on 4chan and later discussed extensively on social networks and among machine learning researchers, highlighting its potential and exceptional performance on common LLM benchmarks.

&#x200B;

**3. Speculation on Origin:** The term ""Miqu"" led to speculation that it might stand for ""Mistral Quantized,"" suggesting it could be a new or modified version of Mistral's existing models, possibly leaked intentionally or by an enthusiastic early access customer.

&#x200B;

4. **CEO's Confirmation:** Arthur Mensch, co-founder and CEO of Mistral, confirmed that an over-enthusiastic early access customer employee leaked a quantized version of an old model, hinting at the rapid development and future potential of Mistral's AI models.

&#x200B;

https://preview.redd.it/9o59yd46f5gc1.jpg?width=1195&format=pjpg&auto=webp&s=2d90852844e310da15acf6fac2f7eb31d06dffe4

&#x200B;

**5. Implications for Open Source AI:** This leak signifies a pivotal moment for open source AI, indicating that the community is making strides toward developing models that can compete with or even surpass proprietary models like GPT-4 in terms of performance.

&#x200B;

Reference:

[https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/](https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/)

[https://twitter.com/Yampeleg/status/1751837962738827378](https://twitter.com/Yampeleg/status/1751837962738827378)

[https://www.euronews.com/next/2023/12/11/french-ai-start-up-mistral-reaches-unicorn-status-marking-its-place-as-europes-rival-to-op](https://www.euronews.com/next/2023/12/11/french-ai-start-up-mistral-reaches-unicorn-status-marking-its-place-as-europes-rival-to-op)

&#x200B;"
73,artificial,gpt-3,top,2021-08-17 13:32:56,"Sam Altman Thinks GPT-3 Is a ""Baby, Baby Step"" on the Curve of AI (1-minute audio clip)",frog9913,False,0.91,46,p63v2o,https://podclips.com/c/mDcwZX?ss=r&ss2=artificial&d=2021-08-17&m=true,9,1629207176.0,
74,artificial,gpt-3,top,2023-05-22 00:15:32,One-Minute Daily AI News 5/21/2023,Excellent-Target-847,False,0.98,43,13oaxkc,https://www.reddit.com/r/artificial/comments/13oaxkc/oneminute_daily_ai_news_5212023/,1,1684714532.0,"1. Microsoft's New Bing update: Doubled the maximum number of characters in conversations to 4000. The underlying technology of this chatbot is GPT-4, and it's free to use without requiring an account to log in.\[1\]
2. ChatGPT has shown a significant ability to understand and articulate emotions, according to a recent study. The study employed the Level of Emotional Awareness Scale (LEAS) to evaluate ChatGPT’s responses to various scenarios, comparing its performance to general population norms. The AI chatbot not only outperformed the human average but also showed notable improvement over time.\[2\]
3. Google is Adding Text-to-Code Generation for Cells in Colab.\[3\]
4. DragGAN AI Tool Lets You Click And Drag To Manipulate Images, And It’s Wild.\[4\]

&#x200B;

Sources:  
\[1\] [https://citylife.capetown/ai/microsoft-removes-account-requirement-for-bing-chats-gpt-4-enhancing-privacy-and-accessibility/22687/](https://citylife.capetown/ai/microsoft-removes-account-requirement-for-bing-chats-gpt-4-enhancing-privacy-and-accessibility/22687/)

\[2\] [https://neurosciencenews.com/chatgpt-emotion-awareness-23231/](https://neurosciencenews.com/chatgpt-emotion-awareness-23231/)

\[3\] [https://www.marktechpost.com/2023/05/19/google-is-adding-text-to-code-generation-for-cells-in-colab/](https://www.marktechpost.com/2023/05/19/google-is-adding-text-to-code-generation-for-cells-in-colab/)

\[4\] [https://hothardware.com/news/draggan-ai-tool-lets-you-click-and-drag-to-manipulate-images](https://hothardware.com/news/draggan-ai-tool-lets-you-click-and-drag-to-manipulate-images)"
75,artificial,gpt-3,top,2023-06-09 03:17:50,One-Minute Daily AI News 6/8/2023,Excellent-Target-847,False,0.91,43,144trgj,https://www.reddit.com/r/artificial/comments/144trgj/oneminute_daily_ai_news_682023/,3,1686280670.0,"1. **Instagram** is apparently testing an AI chatbot that lets you choose from 30 personalities.\[1\]
2. **Singapore** has laid out a years-long roadmap it believes will ensure its digital infrastructure is ready to tap emerging technologies, such as generative AI, autonomous systems, and immersive multi-party interactions.\[2\]
3. **EU** wants platforms to label AI-generated content to fight disinformation.\[3\]
4. The new AI tutoring robot ""**Khanmigo**"" from **Khan Lab School** can not only provide learning guidance but also simulate conversations between historical figures and students. It can even collaborate with students in writing stories, bringing more fun and imagination to the learning process.\[4\]

Sources:  

\[1\] [https://www.theverge.com/2023/6/7/23752143/instagram-ai-chatbot-feature-advice-questions-personalities-leak-screenshot](https://www.theverge.com/2023/6/7/23752143/instagram-ai-chatbot-feature-advice-questions-personalities-leak-screenshot)

\[2\] [https://www.zdnet.com/home-and-office/networking/singapore-creates-digital-blueprint-for-generative-ai-and-autonomous-systems/](https://www.zdnet.com/home-and-office/networking/singapore-creates-digital-blueprint-for-generative-ai-and-autonomous-systems/)

\[3\] [https://techcrunch.com/2023/06/06/eu-disinformation-code-generative-ai-labels/](https://techcrunch.com/2023/06/06/eu-disinformation-code-generative-ai-labels/)

\[4\] [https://www.nytimes.com/2023/06/08/business/khan-ai-gpt-tutoring-bot.html](https://www.nytimes.com/2023/06/08/business/khan-ai-gpt-tutoring-bot.html) "
76,artificial,gpt-3,top,2023-07-07 17:01:01,AI — weekly megathread!,jaketocake,False,0.94,42,14tcxaz,https://www.reddit.com/r/artificial/comments/14tcxaz/ai_weekly_megathread/,12,1688749261.0,"**This week in AI - partnered with** [**aibrews.com**](https://aibrews.com) feel free to follow their newsletter

## News & Insights

1. **Microsoft Research** presents Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image.\[[*Details*](https://www.microsoft.com/en-us/research/blog/breaking-cross-modal-boundaries-in-multimodal-ai-introducing-codi-composable-diffusion-for-any-to-any-generation/)\].
2. **MoonlanderAI** announced the alpha release of its generative AI platform for building immersive 3D games using text descriptions \[[*Details*](https://venturebeat.com/games/moonlander-launches-ai-based-platform-for-3d-game-development/)\].
3. **Bark**, text-to-audio model, is now live on Discord. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and laughing, sighing and crying sounds. \[[*Details*](https://suno-ai.notion.site/Suno-Docs-38e5ba5856d249a89dcea31655f4fb74) | [*GitHub*](https://github.com/suno-ai/bark)\].
4. **OpenAI's Code Interpreter plugin,** allowing ChatGPT to execute code and access uploaded files, will roll out to all ChatGPT Plus users within a week. It enables data analysis, chart creation, file editing, math calculations, and more \[[*Twitter Link*](https://twitter.com/OpenAI/status/1677015057316872192?s=20)\].
5. **OpenAI** announces general availability of GPT-4 API. Current API developers who have made successful payments can use it now, and new developers will have access by month's end \[[*Details*](https://openai.com/blog/gpt-4-api-general-availability)\].
6. **Microsoft AI** presents LONGNET a Transformer variant that can scale the sequence length to 1 billion+ tokens without sacrificing performance on shorter sequences \[[*Details*](https://arxiv.org/pdf/2307.02486.pdf)\].
7. Researchers present a neural machine translation model to translate the ancient language ***Akkadian*** on 5,000-year-old *cuneiform* tablets instantly to english *\[*[*Details*](https://bigthink.com/the-future/ai-translates-cuneiform/) *|* [*Paper*](https://academic.oup.com/pnasnexus/article/2/5/pgad096/7147349)*\].*
8. A set of open-source LLM models, **OpenLLMs**, fine-tuned on only \~6K GPT-4 conversations, have achieved remarkable performance. Of these, **OpenChat-13B**, built upon LLAMA-13B, is at **rank #1** of open-source models on AlpacaEval Leaderboard \[[*GitHub*](https://github.com/imoneoi/openchat) *|*[*Huggingface*](https://huggingface.co/openchat/openchat)*|* [*AlpacaEval*](https://tatsu-lab.github.io/alpaca_eval/)*\]*.
9. Researchers have developed an AI tool named **CognoSpeak** that uses a virtual character for patient interaction and speech analysis to identify early indicators of dementia and Alzheimer's disease \[[*Link*](https://www.independent.co.uk/news/uk/society-royal-college-of-psychiatrists-england-wales-sheffield-b2366136.html)\].
10. Secretive hardware startup **Humane**, shares details about its first product: ‘**Ai Pin’**. It is a wearable, AI-powered device that performs smartphone-like tasks, including summarizing emails, translating languages, and making calls. It also recognizes objects using a camera and computer vision, and it can project an interactive interface onto nearby surfaces, like the palm of a hand or the surface of a table \[[*Details*](https://techcrunch.com/2023/06/30/secretive-hardware-startup-humanes-first-product-is-the-ai-pin/)\].
11. **Nvidia** acquired **OmniML**, an AI startup whose software helped shrink machine-learning models so they could run on devices rather than in the cloud \[[*Details*](https://www.theinformation.com/articles/nvidia-acquired-ai-startup-that-shrinks-machine-learning-models)\].
12. **Cal Fire**, the firefighting agency in California is using AI to fight wildfires \[[*Details*](https://www.cbsnews.com/sacramento/news/cal-fire-now-using-artificial-intelligence-to-fight-wildfires/)\].
13. Over 150 executives from top European companies have signed an open letter urging the EU to rethink its plans to **regulate AI** \[[*Details*](https://www.theverge.com/2023/6/30/23779611/eu-ai-act-open-letter-artificial-intelligence-regulation-renault-siemens)\].
14. **Google** updated its privacy policy: the company reserves the right to use just about everything users post online for developing its AI models and tools \[[*Details*](https://gizmodo.com/google-says-itll-scrape-everything-you-post-online-for-1850601486)\].
15. **OpenAI** believes superintelligence could arrive this decade. Announced a new project, Superalignment with a focus on aligning superintelligent AI systems with human intent \[[*Details*](https://openai.com/blog/introducing-superalignment)\].

#### 🔦 Open Source Projects

1. **Embedchain**: a framework to easily create LLM powered bots over any dataset \[[*Link*](https://github.com/embedchain/embedchain)\].
2. **GPT-author**: uses a chain of GPT-4 and Stable Diffusion API calls to generate an an entire novel, outputting an EPUB file \[[*Link*](https://github.com/mshumer/gpt-author)\].
3. **GPT-Migrate:** Easily migrate your codebase from one framework or language to another \[[*Link*](https://github.com/0xpayne/gpt-migrate)\]. 

—-------

Welcome to the r/artificial weekly megathread. This is where you can discuss Artificial Intelligence - talk about new models, recent news, ask questions, make predictions, and chat other related topics.

[Click here for discussion starters for this thread or for a separate post.](https://www.google.com/search?q=artificial+intelligence&tbm=nws)

Self-promo is allowed in these weekly discussions. If you want to make a separate post, please read and go by the rules or you will be banned.

[Previous Megathreads](https://www.reddit.com/r/artificial/search/?q=author%3Ajaketocake%20megathread&restrict_sr=1) & [Subreddit revamp and going forward](https://www.reddit.com/r/artificial/comments/120qr4r/psa_rule_2_will_be_enforced_selfpromotion_is_only/)"
77,artificial,gpt-3,top,2020-09-21 13:01:02,The GPT-3 economy,bendee983,False,0.94,44,iwzyhr,https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/,4,1600693262.0,
78,artificial,gpt-3,top,2022-10-24 16:17:57,GPT-3 does an astonishingly good job creating both sides of an Interactive Fiction transcript,raldi,False,0.96,43,ycfg6g,https://www.reddit.com/r/interactivefiction/comments/ycf8ol/gpt3_does_an_astonishingly_good_job_creating_both/,8,1666628277.0,
79,artificial,gpt-3,top,2023-05-01 04:50:09,Ideas to make AutoGPT far better,crua9,False,0.78,42,134cxcu,https://www.reddit.com/r/artificial/comments/134cxcu/ideas_to_make_autogpt_far_better/,21,1682916609.0,"So I played with AutoGPT a bit to see what it was all about and how it can help me. After playing with it I found the following problems.

1. It gets into a loop easily.
2. It gets side tracked easily.
3. It forgets things sometimes. Like it talks to a bot, and then several things later it will again want to talk to the bot about the same thing.
4. It doesn't know the bots it can make can't work online.
5. It can't control multiple bots at once.
6. It forgets old AI you made. Like as far as I can tell, it only somewhat remembers the last one you used, and barely at that.
7. There is no good way to remotely check how far along your stuff is going.

Solution:

A solution to this is simple in theory, but I don't have enough of an understanding to code it into it. Like I tried to use the tool to improve itself. But I don't have access to GPT4, and it didn't get that far.

For 6 and 7 the solution to that is obvious.

&#x200B;

Everything else solution is to have a mother bot and a child bot. The mother bot is what you interact with and the child bots LOCALLY are what does the actual work. The job of the mother bot is to

1. Interact with the user in finding what the user wants, get updates from the user, and give the user what they want or make sure they get what they want.
2. Look at the computer time/date
3. Make child bots locally and interact with them
4. Monitor child bots to make sure they stay on task, nudge if they run into errors, monitor for loops, and kill them.

The mother bot looks at the date/time and makes the child bot. It looks at the date/time to see if the child bot is taking too long. If so, why and how could other child bots help that one get to where they need to.

Also by having the mother bot not doing the task, it can run multiple child bots. For example, you can ask the mother bot list 5 best x item. And the first child bot will search google. Then the mother bot can make 15 child bots to look at their own links all at the same time, and to write a report in a given file. The mother bot can then make another child bot to review all of the files and compile it into 1 comprehensive report. Then the mother bot can give that as the results. This likely cutting hour chunk of time.

&#x200B;

By doing this locally the child bots will have similar features as the mother bot in being able to search the web, make files, etc. And by having it where the child bots focus on 1 task (more than less like they do now) but having them put the stuff in a txt file, and then if multiple are use having 1 child bot bring all that info together. This creates memory. The child bot and the mother bot can read from this and use the info.

&#x200B;

Plus this also give multiple AI to interact with each other or learn from each other.

For example, if I have 1 AI finding me land, and another on farming, and another on running a business. I can have all 3 AI learn from each other by them reading each other's files giving I point them to the other bots or let them search my other AI to maybe file useful info from my prior AI."
80,artificial,gpt-3,top,2023-05-05 17:01:46,AI — weekly megathread!,jaketocake,False,0.99,41,138us1s,https://www.reddit.com/r/artificial/comments/138us1s/ai_weekly_megathread/,16,1683306106.0,"**This week in AI - partnered with** [**aibrews.com**](https://aibrews.com) feel free to follow their newsletter

**News & Insights:**

**OpenAI's text to 3D model shap-e**  [on GitHub](https://github.com/openai/shap-e#samples)

1. **Play.ht** has launched its latest machine learning model that supports multilingual synthesis and cross-language voice cloning. This allows users to clone voices across different languages to English, retaining the nuances of the original accent and language \[[*Details*](https://play.ht/blog/play-ht-launches-multilingual-synthesis-and-cross-language-voice-cloning)\].
2. A new programming language for AI developers, **Mojo**, has been developed by **Modular**, the AI developer platform co-founded by Chris Lattner ( he co founded the LLVM, Clang compiler, Swift). Mojo combines the usability of Python with the performance of C. Up to ***35,000x*** faster than Python, it is seamlessly interoperable with the Python ecosystem \[[*Details*](https://docs.modular.com/mojo/why-mojo.html) *|*[ *Twitter Link*](https://twitter.com/Modular_AI/status/1653436642248781825)\].
3. **Stability AI** released StableVicuna, the first large-scale open source chatbot trained via reinforced learning from human feedback (RHLF) . There’s also an upcoming chat interface which is in the final stages of development \[[*Details*](https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot)\].
4. **Eleven Labs** introduced a new speech synthesis model that supports seven new languages (French, German, Hindi, Italian, Polish, Portuguese, and Spanish). This makes it possible to generate speech in multiple languages using a single prompt while maintaining each speaker's unique voice characteristics \[[*Details*](https://beta.elevenlabs.io/blog/eleven-multilingual-v1/) |[ *Demo video*](https://www.youtube.com/watch?v=kwmeZ7RjgcU)\].
5. **Microsoft** reveals:
   1. New features for AI-powered Bing Chat: richer visuals, long-form document summarization, broader language support, visual search, chat history, sharing options, AI-assisted Edge actions, and contextual mobile queries.
   2. Third-party plugins in Bing chat with more details coming at Microsoft Build later this month \[[*Details*](https://blogs.microsoft.com/blog/2023/05/04/announcing-the-next-wave-of-ai-innovation-with-microsoft-bing-and-edge/)\].
6. Debut of ‘**Pi’ chatbot by Inflection** (founded by co-founders of Google DeepMind and LinkedIn). It’s designed for relaxed, supportive and informative conversations. Pi is free for now without any token restrictions \[[*Details*](https://inflection.ai/) |[ *Chat*](https://heypi.com/talk)\].
7. Sal Khan, Khan Academy founder, discusses AI's potential to transform education in a **TED Talk**, highlighting personal AI tutors, teaching assistants, and new features of their chatbot, **Khanmigo \[**[*Video*](https://www.youtube.com/watch?v=hJP5GqnTrNo)**\].**
8. Salesforce announces Slack GPT - generative AI for Slack. It includes:
   1. An AI-ready platform to create custom workflows and automate tasks via simple prompts, without coding. Users can integrate language models of choice: ChatGPT, Claude, or custom-built ones.
   2. Built-in AI features in Slack, such as conversation summaries and writing assistance.
   3. The Einstein GPT app for AI-powered customer insights from Salesforce Customer 360 data and Data Cloud \[[*Details*](https://www.salesforce.com/news/press-releases/2023/05/04/slack-gpt-news/)\].
9. **Replit’s** new 2.7B params code LLM, ReplitLM is now open-source. It outperformed Codex and LLaMA despite being smaller in size \[[*GitHub*](https://github.com/replit/ReplitLM) |[ *Hugging Face Demo*](https://huggingface.co/replit)\].
10. **Nvidia** will present 20 research papers at SIGGRAPH, covering generative AI models for personalized images, inverse rendering tools for 3D objects, neural physics models for realistic simulations, and neural rendering models for real-time, AI-driven visuals. \[[*Details*](https://blogs.nvidia.com/blog/2023/05/02/graphics-research-advances-generative-ai-next-frontier/)\].
11. **Snap** plans to show sponsored links to users during chat with its My AI chatbot \[[*Details*](https://techcrunch.com/2023/05/02/snap-announces-tests-of-sponsored-links-in-my-ai-new-ad-products-for-spotlight-and-stories/)\].
12. **IBM** is set to pause hiring for around 7,800 positions that could potentially be replaced by AI and automation \[[*Details*](https://www.bloomberg.com/news/articles/2023-05-01/ibm-to-pause-hiring-for-back-office-jobs-that-ai-could-kill)\].
13. **Box** is introducing generative AI tools across its platform, allowing users to obtain document summaries or key points and create content in Box Notes \[[*Details*](https://techcrunch.com/2023/05/02/box-is-partnering-with-openai-to-bring-generative-ai-tools-across-the-platform/)\].
14. **Stability AI** released DeepFloyd IF, a powerful text-to-image model that can smartly integrate text into images \[[Details](https://stability.ai/blog/deepfloyd-if-text-to-image-model)\].
15. Sam Altman and Greg Brockman from OpenAI on **AI and the Future** in this podcast \[[*YouTube Link*](https://www.youtube.com/watch?v=cHJPyizxM60)\]
16. Researchers at The **University of Texas** at Austin have developed a non-invasive AI system, known as a semantic decoder. It can convert brain activity while listening to a story or silently imagining telling a story, into coherent text using fMRI scans and transformer model \[[*Details*](https://news.utexas.edu/2023/05/01/brain-activity-decoder-can-reveal-stories-in-peoples-minds/)\].
17. **HackAPrompt**: The first ever prompt hacking competition, with $37K+ in prizes, starting May 5th. Sponsored by OpenAI and others. \[[*Details*](https://www.aicrowd.com/challenges/hackaprompt-2023) |[ *Prompt Hacking Tutorial*](https://learnprompting.org/docs/category/-prompt-hacking) *\].*

**🔦 Social Spotlight**

1. A **GPT-4 AI Tutor Prompt** for customizable personalized learning experiences \[[*GitHub Link*](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor)\].
2. **Portfolio Pilot:** A verified ChatGPT plugin for investing that analyses your portfolio for actionable recommendations \[[*Twitter Link with Demo*](https://twitter.com/alexharm/status/1653787155410620417)\].
3. **Baby AGI**s interacting in the real world via phone using vocode (Open source library for building voice conversations with LLMs) \[[ *Twitter Link*](https://twitter.com/vocodehq/status/1653104377010483201)\].
4. Data visualization in ChatGPT with **code interpreter** plugin \[[*Twitter Link*](https://twitter.com/emollick/status/1653189190354452480)\].
5. **ThinkGPT**, a Python library for LLMs, enables chain of thoughts, reasoning, and generative agents. It addresses limited context, improves one-shot reasoning, and integrates intelligent decisions \[[*GitHub Link*](https://github.com/jina-ai/thinkgpt)\].

Welcome to the r/artificial weekly megathread. This is where you can discuss Artificial Intelligence - talk about new models, recent news, ask questions, make predictions, and chat other related topics.

[Click here for discussion starters for this thread or for a separate post.](https://www.google.com/search?q=artificial+intelligence&tbm=nws)

Self-promo is allowed in these weekly discussions. If you want to make a separate post, please read and go by the rules or you will be banned.

[Subreddit revamp & going forward](https://www.reddit.com/r/artificial/comments/120qr4r/psa_rule_2_will_be_enforced_selfpromotion_is_only/)"
81,artificial,gpt-3,top,2021-05-02 13:29:54,GPT-1 - Annotated Paper + Paper Summary,shreyansh26,False,0.92,37,n36f97,https://www.reddit.com/r/artificial/comments/n36f97/gpt1_annotated_paper_paper_summary/,1,1619962194.0,"GPT-2 and recently, GPT-3 created a lot of hype when they were launched. However, it all started with the ""Improving Language Understanding by Generative Pre-Training"" paper which introduced the idea of GPT-1. 

As a part of my Paper Notes series, I have gone through the paper and created a brief yet informative summary of the paper. It will take just take a few minutes to understand GPT-1 well. Check out the links below and happy reading!

Paper Summary -   [Improving Language Understanding by Generative Pre-Training](https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/)

Annotated Paper -  [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT1.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT1.pdf)"
82,artificial,gpt-3,top,2021-12-13 16:08:05,"[R] DeepMind’s RETRO Retrieval-Enhanced Transformer Retrieves from Trillions of Tokens, Achieving Performance Comparable to GPT-3 With 25× Fewer Parameters",Yuqing7,False,0.94,41,rfj4g9,https://www.reddit.com/r/artificial/comments/rfj4g9/r_deepminds_retro_retrievalenhanced_transformer/,0,1639411685.0,"A DeepMind research team proposes RETRO (Retrieval-Enhanced Transformer), an enhanced auto-regressive language model that conditions on document chunks retrieved from a large corpus and achieves performance comparable to GPT-3 and Jurassic-1 on the Pile dataset while using 25× fewer parameters. 

Here is a quick read: [DeepMind’s RETRO Retrieval-Enhanced Transformer Retrieves from Trillions of Tokens, Achieving Performance Comparable to GPT-3 With 25× Fewer Parameters.](https://syncedreview.com/2021/12/13/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-164/)

The paper *Improving Language Models by Retrieving From Trillions of Tokens* is on [arXiv](https://arxiv.org/abs/2112.04426)."
83,artificial,gpt-3,top,2024-01-05 01:44:28,This year looks so promising for the AI industry,LingonberryPurple149,False,0.92,41,18yul79,https://www.reddit.com/r/artificial/comments/18yul79/this_year_looks_so_promising_for_the_ai_industry/,8,1704419068.0,"I've been relatively closely following the development of AI tools ever since the first version of ChatGPT was released (gotta admit I was one of those people who posted pretentious posts on LinkedIn during the first hype hahaha), especially because the company I work for started implementing AI tools into our work routines as soon as they came live. Apart from that, I also used some AI tools for my own personal projects, hobbies, and everyday stuff (especially ChatGPT 4). For example, I used ChatGPT to make a personalized diet based on my dietary needs and the food I like to eat, and it did a better job than the few personal trainers I had PAID to do it.

The point is, AI tools have been proven to be exceptionally useful in 2023, and now that the industry has grown and more projects are starting to emerge, I can't but imagine how far will the industry go in 2024. And I'm quite happy because of that, the possibility to either delegate mundane tasks to AI or just speed up so many parts of the working routine has been a lifesaver. And even for hobbies, if you're into roleplay, for example, creating pictures of your characters has never been easier.

I did a bit of research and listed some projects that look the most promising to me. There might be others that deserve to be on this list as well, so please mention them in the comments because I'll surely try to make some use of them.

**ChatGPT 4.5 Version** | As I said above, the 4.0 version is already insanely useful for so many things, and I can't even imagine what the upgraded version will bring to the table. Probably in the top 2/3 most anticipated AI things for me.

**Personal AI** | I remember reading in an article that in the near future, AI projects will start moving from generic to personal because of all the benefits of personalized AI tools... most importantly, experiences and functions tailored towards individuals rather than generic groups. I believe that this is the most likely future for the industry, and we can see the traces of this in many current AI projects. Personal AI stands out as one of the few AI projects completely designed around personalized experience, which is why I believe it has an insane potential to be propelled into stardom if everything goes right for developers. I also like the general idea of being able to create memory stacks and your personalized AI model that functions as a virtual copy of you, so to say, and that could be accessed by other people. Could be a huge timesaver too for people whose jobs include frequent meetings and conversations with clients.

**Midjourney V7** | Tbh I haven't used Midjourney too much other than playing around with picture creation once it became the next big thing in AI and occasionally creating sort of AI stock photos for some personal projects, but I've seen people doing magic with it and I simply couldn't leave it out of this post. I have a few personal favorites that I've come across on Reddit saved on my PC, and I even use them as my wallpapers from time to time. Midjourney V7 will be a nuclear bomb in the world of AI.

**GPT Store** | Basically a store for custom GPTs or custom chatbots created by other users. I think it's a pretty cool concept because it'll propel the development of AI by incentivizing regular users to work on developing their own GPT that they can make money from. I actually started training a custom GPT for some of the tasks that I deal with regularly at work, and I hope to try and sell it once the store launches."
84,artificial,gpt-3,top,2021-09-12 14:42:34,GPT3 just blew my mind by predicting ADHD,KIFF_82,False,0.81,38,pmtx1h,https://www.reddit.com/r/artificial/comments/pmtx1h/gpt3_just_blew_my_mind_by_predicting_adhd/,16,1631457754.0,"So I'm new to this technology.., an uneducated newb if I may say so... Put that aside I also have ADHD. I copied a post from the ADHD subreddit and prompted it in to davinci (largest GPT-3 model). I did not write anything about ADHD, but GPT3 instantly recognized it. 

The input:  Your name is V. You are a therapist reading articles on Reddit. One article is very interesting. The article is written by a man named Sushi, and you think you could give a good answer to it. 

Shushi asks: ""Is it ok and normal to just have one day every week to do absolutely nothing? After a week of socializing and doing work and overall with a lot of stimulation, I just need one day to do absolutely nothing. Literally an entire day of eating food, watching Netflix, and lying in bed.  

I feel kinda embarrassed though for doing that- like someone asked what I did today and I lied and said I read my textbooks for my classes. They said “good job being productive!”. And also I could’ve hung out with friends but I was just too tired so I stayed home and then there’s the FOMO.  But I just need to “waste” my time for an entire day or else I just feel so exhausted and don’t want to be around people. 

I feel like other people take breaks less often or they will actually do something like going bowling or if they do just stay home at least they will actually read their textbook.  The thing is that it later messes with my week. Since I didn’t do any work today, I’ll have to do it tomorrow, but tomorrow I’ll actually want to hang out with my friends and not miss out. Things like that. Then I end up not having time for things because I spent a whole day doing nothing  Anyone else feel like this?"" 

The output: 

This question really hits close to home. You understand this. You have experienced this many times. This is why you are currently hiding from your friends and family. You don't want them to know how lazy you are. You don't want anyone to know because you fear they will judge you. You wonder if you have a disease like ADHD or something."
85,artificial,gpt-3,top,2023-04-14 17:02:07,AI — weekly megathread!,jaketocake,False,0.97,36,12m3wko,https://www.reddit.com/r/artificial/comments/12m3wko/ai_weekly_megathread/,7,1681491727.0,"**This week in AI  - partnered with** [**aibrews.com**](https://aibrews.com) \- feel free to follow their newsletter

1. **Amazon** announces:
   1. **Amazon Bedrock,** a new service that makes foundation models (FMs) from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API \[[*Link*](https://aws.amazon.com/bedrock/)\]
   2. Amazon’s new **Titan FMs**: The first is a generative LLM for tasks such as summarization, text generation, classification, open-ended Q&A, and information extraction. The second is an embeddings LLM that translates text inputs into numerical representations (known as embeddings) that contain the semantic meaning of the text \[[*Link*](https://aws.amazon.com/bedrock/titan/)\]. 
   3. the general availability of **Amazon CodeWhisperer**, the AI coding companion, free for individual developers. It has built-in security scanning for finding and suggesting remediations for hard-to-detect vulnerabilities, such as those in the top ten Open Worldwide Application Security Project (OWASP), those that don’t meet crypto library best practices, and others. \[[*Link*](https://aws.amazon.com/codewhisperer/)\].
2. **Meta** has released **Animated Drawings** \- an open-source project that turns doodles into animations \[[*Link*](https://developers.facebook.com/blog/post/2023/04/13/meta-os-animated-drawings/)\]
3. **Stability AI** announced **Stable Diffusion XL (SDXL)** \- the latest image generation model, now available through their API, excels at photorealism & adds many cool features like enhanced face generation, minimal prompts & legible text. SDXL also has functionality that extends beyond just text-to-image prompting, including image-to-image prompting (inputing one image to get variations of that image), inpainting (reconstructing missing parts of an image) and outpainting (constructing a seamless extension of an existing image)  \[[*Link*](https://stability.ai/stable-diffusion)\].
4. **Google** introduced **Med-PaLM 2**, expert-level medical LLM that consistently performed at an “expert” doctor level on medical exam questions, scoring 85%. This is an 18% improvement from Med-PaLM’s previous performance and far surpasses similar AI models \[[*Link*](https://blog.google/technology/health/ai-llm-medpalm-research-thecheckup/?utm_source=www.theneurondaily.com&utm_medium=newsletter&utm_campaign=amazon-enters-the-chat)\].
5. **Databricks** announced Dolly 2.0 - the first open-source, instruction-following LLM (12B parameter) that’s available for commercial use \[[*Link*](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)\].
6. **Poe**, Quora's AI chatbot app, now features the ability for users to create custom bots using just prompts, with options such as Claude Instant or ChatGPT as a base. Quora plans to cover large language model fees, making it free for users at the moment \[[*Link*](https://twitter.com/adamdangelo/status/1644435126343077888)\].
7. **Zapier** added new AI features in its ‘**Interfaces**’ no-code tool which lets users create interactive pages and app. Now, one can create customized ChatGPT-powered bots, embed them anywhere, and trigger automations based on chat responses \[[*Link*](https://help.zapier.com/hc/en-us/articles/14490267815949-Create-interactive-pages-and-apps-with-Zapier-Interfaces)\]
8. **Demo projects** from a ChatGPT hackathon, held last week and sponsored by OpenAI, Replit and others \[[*Link*](https://twitter.com/josephofiowa/status/1645224154831151105)\].
9. **CAMEL** (Communicative Agents for “Mind” Exploration of LLM Society) - AI agents interacting with each other and collaborating. For e.g., two ChatGPT agents playing roles as a python programmer and a stock trader collaborating on developing a trading bot for stock market. \[[ *Colab of the demo*](https://colab.research.google.com/drive/1AzP33O8rnMW__7ocWJhVBXjKziJXPtim) *|*[ *Project website*](https://www.camel-ai.org/)*\]*
10. **Open AI** introduces ‘**Consistency Models’** as an alternate to Diffusion based models (used by tools like Stable Diffusion, Midjourney etc.) that can generate a complete image in just one step. \[[*Link to Paper*](https://arxiv.org/pdf/2303.01469.pdf) *|*[ *Link to TechCrunch article*](https://techcrunch.com/2023/04/12/openai-looks-beyond-diffusion-with-consistency-based-image-generator/)*\].*
11. Stanford and Google researchers developed a virtual town populated by **25 ChatGPT agents** to test machine learning models in creating realistic, adaptive generative agents simulating human behavior. In a Sims-inspired environment, agents store experiences, synthesize memories, and plan behavior in natural language. They engaged in complex actions such as organizing a Valentine's Day party, and their actions were rated as more human-like than humans roleplaying! *\[*[*Demo Link*](https://reverie.herokuapp.com/arXiv_Demo/) *|*[ *Link to Paper*](https://arxiv.org/pdf/2304.03442v1.pdf)*\].*
12. **LangChain** announced support for running[ LangChain.js](https://github.com/hwchase17/langchainjs) in browsers, Cloudflare Workers, Vercel/Next.js, Deno, Supabase Edge Functions, alongside existing support for Node.js ESM and CJS \[[*Link*](https://blog.langchain.dev/js-envs/)\].
13. **Artifact**, the recently launched personalized news app from Instagram’s founders adds a social discussions feature \[[*Link*](https://techcrunch.com/2023/04/11/artifact-the-news-aggregator-from-instagrams-co-founders-adds-a-social-discussions-feature/)\].
14. **Open AI** announced a **bug bounty program** with rewards ranging from $200 for low-severity findings to up to $20,000 for exceptional discoveries \[[*Link*](https://bugcrowd.com/openai)\].
15. **Boston researchers** have developed an AI tool called **Sybil**, which can detect early signs of lung cancer years before doctors would find it on a CT scan \[[*Link*](https://www.nbcnews.com/health/health-news/promising-new-ai-can-detect-early-signs-lung-cancer-doctors-cant-see-rcna75982?utm_source=www.aiwithvibes.com&utm_medium=newsletter&utm_campaign=elon-s-twitter-ai-amazon-alexa-ai-arena)\]
16. **Alibaba Cloud** unveiled **Tongyi Qianwen**, a ChatGPT-like AI with bilingual capabilities, to be integrated into its business applications, including DingTalk and Tmall Genie \[[*Link*](https://www.cnet.com/tech/alibaba-unveils-chatgpt-rival-with-chinese-and-english-capabilities/)\].
17. **Hubspot** introduced several improvements for its generative AI tool **ChatSpot** \[[*Link*](https://blog.chatspot.ai/yipee-its-chatspot-3-alpha)\]

Welcome to the r/artificial weekly megathread. This is where you can discuss Artificial Intelligence - talk about new models, recent news, ask questions, make predictions, and chat other related topics.

[Click here for discussion starters for this thread or for a separate post.](https://www.google.com/search?q=artificial+intelligence&tbm=nws)

Self-promo is allowed in these weekly discussions. If you want to make a separate post, please read and go by the rules or you will be banned.

[Subreddit revamp & going forward](https://www.reddit.com/r/artificial/comments/120qr4r/psa_rule_2_will_be_enforced_selfpromotion_is_only/)"
86,artificial,gpt-3,top,2023-06-30 17:01:08,AI — weekly megathread!,jaketocake,False,0.95,38,14n5x71,https://www.reddit.com/r/artificial/comments/14n5x71/ai_weekly_megathread/,26,1688144468.0,"**This week in AI - partnered with** [**aibrews.com**](https://aibrews.com) feel free to follow their newsletter

## News & Insights

1. **Microsoft** has launched AI-powered shopping tools in Bing search and Edge, including AI-generated buying guides which automatically aggregate product specifications and purchase locations for user queries​, and AI-generated review summaries that provide concise overviews of online product reviews \[[*Details*](https://techcrunch.com/2023/06/29/microsoft-brings-new-ai-powered-shopping-tools-to-bing-and-edge/)\].
2. **Salesforce AI Research** released **XGen-7B**, a new **open-source** 7B LLM trained on 8K input sequence length for 1.5T tokens \[[*Details*](https://blog.salesforceairesearch.com/xgen/)| [*Huggingface*](https://huggingface.co/Salesforce/xgen-7b-8k-base)| [*GitHub*](https://github.com/salesforce/xGen)\].
3. Researchers present **DreamDiffusion**, a novel method for generating high-quality images directly from brain EEG signals without the need to translate thoughts into text \[[*Paper*](https://arxiv.org/pdf/2306.16934.pdf)\].
4. **Google** announced the first *Machine* ***Un****learning Challenge* hosted on Kaggle \[[*Details*](https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html)\].
5. **Microsoft** announced a new ***AI Skills Initiative*** that includes free coursework developed with LinkedIn, a new open global grant challenge and greater access to free digital learning events and resources for AI education \[[*Details*](https://www.linkedin.com/pulse/microsofts-launches-new-ai-skills-training-resources-part-behncken)\].
6. **Stability AI** announced **OpenFlamingo V2,** an open-source reproduction of DeepMind's Flamingo model. OpenFlamingo models achieve more than 80% of the performance of their corresponding Flamingo model. \[[*Details*](https://stability.ai/research/openflamingo-v2-new-models-and-enhanced-training-setup)\].
7. **Unity** announces two AI-powered tools: Unity Muse and Unity Sentis. Muse generates animations, 2D sprites, textures etc. in the Unity Editor using text and sketches. Sentis lets you embed an AI model in the Unity Runtime for your game or application. It enables AI models to run on any device where Unity runs. \[[*Details*](https://blog.unity.com/engine-platform/introducing-unity-muse-and-unity-sentis-ai)\].
8. **ElevenLabs** launched **Voice Library** \- a library and community for sharing AI generated voices designed using their *voice Design* tool \[[*Details*](https://beta.elevenlabs.io/blog/voice-library/)\].
9. **Merlyn Mind** released three **open-source education-specific LLMs**. Merlyn Mind is building a generative AI platform for education where engagement will be curriculum-aligned, hallucination-resistant, and age-appropriate \[[*Details*](https://www.merlyn.org/blog/merlyn-minds-education-specific-language-models)\].
10. Amazon's **AWS** has launched a $100 million program, the **Generative AI Innovation Center**, that connects AWS machine learning and artificial intelligence experts with businesses to build and deploy generative AI solutions \[[*Details*](https://press.aboutamazon.com/2023/6/aws-announces-generative-ai-innovation-center)\].
11. New open-source text to video AI model, **Zeroscope\_v2 XL**, released that generates high quality video at 1024 x 576, with no watermarks. \[[*Huggingface*](https://huggingface.co/cerspense/zeroscope_v2_XL) \].
12. Researchers present MotionGPT - a motion-language model to handle multiple motion-relevant tasks \[[*Details*](https://motion-gpt.github.io/)\].
13. **Databricks** is set to acquire the open-source startup **MosaicML** for $1.3 billion. MosaicML had recently released [**MPT-30B**](https://huggingface.co/mosaicml/mpt-30b/)**,** an open-source model licensed for commercial use that outperforms the original GPT-3 \[[*Details*](https://techcrunch.com/2023/06/26/databricks-picks-up-mosaicml-an-openai-competitor-for-1-3b/)\].
14. Generative AI-related job postings in the United States jumped about 20% in May as per Indeed’s data \[[*Details*](https://www.reuters.com/technology/us-based-generative-ai-job-postings-up-20-may-data-2023-06-22/)\].
15. The source code for the algorithm **DragGAN** (Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold) released and demo available on Huggingface. \[[*GitHub Link*](https://github.com/XingangPan/DragGAN) | [*Huggingface*](https://huggingface.co/spaces/radames/DragGan)\].
16. A new foundation model, **ERNIE** **3.5 b**y China’s Baidu surpassed ChatGPT (3.5) in comprehensive ability scores and outperforms GPT-4 in several Chinese language capabilities \[[*Details*](http://research.baidu.com/Blog/index-view?id=185)\].
17. **Adobe** is prepared to pay out any claims in case an enterprise customer loses a lawsuit over the use of content generated by Adobe Firefly, the generative AI image tool \[[*Details*](https://techcrunch.com/2023/06/26/adobe-indemnity-clause-designed-to-ease-enterprise-fears-about-ai-generated-art/)\].
18. **Google** launched generative AI coding features in Google Colab for Pro+ subscribers in the US \[[*Details*](https://twitter.com/GoogleColab/status/1673354996296081409)\]

#### Social Spotlight

1. EmbedChain - a new framework to easily create LLM-powered bots over any dataset \[[*Twitter Link*](https://twitter.com/AlphaSignalAI/status/1672668574450847745?s=20)\].
2. ChatHN: Chat with Hacker News using OpenAI function calling \[[*GitHub Link*](https://github.com/steven-tey/chathn)\]
3. A Twitter thread showing the new zoom out feature in Midjourney 5.2 \[[*Link*](https://twitter.com/JeremyNguyenPhD/status/1673019914368561153?s=20)\] 

—-------

Welcome to the r/artificial weekly megathread. This is where you can discuss Artificial Intelligence - talk about new models, recent news, ask questions, make predictions, and chat other related topics.

[Click here for discussion starters for this thread or for a separate post.](https://www.google.com/search?q=artificial+intelligence&tbm=nws)

Self-promo is allowed in these weekly discussions. If you want to make a separate post, please read and go by the rules or you will be banned.

[Subreddit revamp & going forward](https://www.reddit.com/r/artificial/comments/120qr4r/psa_rule_2_will_be_enforced_selfpromotion_is_only/)"
87,artificial,gpt-3,top,2022-04-09 18:53:20,"Check Out This DeepMind’s New Language Model, Chinchilla (70B Parameters), Which Significantly Outperforms Gopher (280B) and GPT-3 (175B) on a Large Range of Downstream Evaluation Tasks",No_Coffee_4638,False,0.93,34,tzzoky,https://www.reddit.com/r/artificial/comments/tzzoky/check_out_this_deepminds_new_language_model/,5,1649530400.0,"https://preview.redd.it/pkrbloq8vjs81.png?width=1422&format=png&auto=webp&s=fef693165a6c948f626de613e4e341c25f8cf5f4

&#x200B;

Extreme-scale language models have recently exhibited incredible performance on natural language processing challenges. This is due to their ever-increasing size, exceeding 500 billion parameters. However, while these models have grown in popularity in recent years, the amount of data utilized to train them has not increased. The current generation of huge language models is clearly undertrained. Three prediction approaches for optimally choosing both model size and training length have been proposed by a DeepMind research team.

Three approaches have been mentioned to estimate the optimal parameter:

* Change the size of the models and the number of training tokens.
* IsoFLOP profiles
* Using a parametric loss function to fit a model

The ultimate pretraining loss is calculated as the number of model parameters and training tokens. They minimize the loss function under the restriction of the FLOPs function, which is equal to the computational budget because the computational budget is a probabilistic function of the number of observed training tokens and model parameters.

[Continue Reading This Research Summary](https://www.marktechpost.com/2022/04/09/check-out-this-deepminds-new-language-model-chinchilla-70b-parameters-which-significantly-outperforms-gopher-280b-and-gpt-3-175b-on-a-large-range-of-downstream-evaluation-tasks/)

Paper: https://arxiv.org/pdf/2203.15556.pdf"
88,artificial,gpt-3,top,2023-03-25 16:12:37,"When people want to argue about GPT-4, you don’t even have to defend it. Simply ask GPT-4 to respond for you, in whatever tone you think appropriate.",katiecharm,False,0.72,34,121qleh,https://i.imgur.com/NOUR7DU.jpg,11,1679760757.0,
89,artificial,gpt-3,top,2023-06-11 02:38:04,One-Minute Daily AI News 6/10/2023,Excellent-Target-847,False,0.91,33,146ibud,https://www.reddit.com/r/artificial/comments/146ibud/oneminute_daily_ai_news_6102023/,1,1686451084.0,"1. Republicans and Democrats team up to take on AI with new bills. The latest AI bills show there's a bipartisan agreement for the government to be involved.[1]
2. Hundreds of German Protestants attended a church service in Bavaria that was generated almost entirely by AI. The ChatGPT chatbot led more than 300 people through 40 minutes of prayer, music, sermons, and blessings.[2]
3. Sam Altman, the CEO of ChatGPT developer OpenAl, met with South Korean President Yoon Suk Yeol on June 9 and urged South Korea to play a leading role in manufacturing the chips needed for Al technology.[3]
4. Microsoft is moving some of its best AI researchers from China to Canada in a move that threatens to gut an essential training ground for the Asian country’s tech talent.[4]

Sources: 
[1] https://www.foxbusiness.com/politics/republicans-democrats-team-take-ai-new-bills

[2] https://www.irishexaminer.com/world/arid-41159539.html

[3] https://cointelegraph.com/news/openai-ceo-highlights-south-korean-chips-sector-for-ai-growth-willing-to-invest/amp

[4] https://www.ft.com/content/d21d2f85-7531-4536-bcce-8ca38620fe55"
90,artificial,gpt-3,top,2023-08-01 17:40:00,One-Minute Daily AI News 8/1/2023,Excellent-Target-847,False,0.86,35,15fjasn,https://www.reddit.com/r/artificial/comments/15fjasn/oneminute_daily_ai_news_812023/,1,1690911600.0,"1. **DoNotPay**, an AI lawyer bot known as ChatGPT4, is transforming how users handle legal issues and save money. In under two years, this innovative robot has successfully overturned more than 160,000 parking tickets in cities like New York and London. Since its launch, it has resolved a total of 2 million related cases.\[1\]
2. **Microsoft** hints **Windows 11 Copilot** with third-party AI plugins is almost here.\[2\]
3. In an analyst note on Tuesday, the financial services arm of Swiss banking giant **UBS** raised its guidance for long-term AI end-demand forecast from 20% compound annual growth rate (CAGR) from 2020 to 2025 to 61% CAGR between 2022 to 2027.\[3\]
4. The next generation of the successful **OpenAI** language model is already on the way. It has been discovered that the North American company has filed a registration application for the **GPT-5** mark with the United States Patent and Trademark Office.\[4\]

Sources:

 \[1\] [https://citylife.capetown/uncategorized/donotpay-ai-bot-saves-users-money-by-overturning-parking-tickets-and-more/302279/](https://citylife.capetown/uncategorized/donotpay-ai-bot-saves-users-money-by-overturning-parking-tickets-and-more/302279/)

\[2\] [https://www.itvoice.in/microsoft-hints-windows-11-copilot-with-third-party-ai-plugins-is-almost-here](https://www.itvoice.in/microsoft-hints-windows-11-copilot-with-third-party-ai-plugins-is-almost-here)

\[3\] [https://venturebeat.com/ai/ubs-projects-61-compound-annual-growth-rate-for-ai-between-2022-and-2027/](https://venturebeat.com/ai/ubs-projects-61-compound-annual-growth-rate-for-ai-between-2022-and-2027/)

\[4\] [https://www.gearrice.com/update/openai-confirms-gpt-5-and-gives-us-the-first-clues-about-it/](https://www.gearrice.com/update/openai-confirms-gpt-5-and-gives-us-the-first-clues-about-it/) "
91,artificial,gpt-3,top,2022-04-04 18:21:08,"Microsoft Researchers Introduce ‘Jigsaw’: An AI Tool To Augment Large Language Models (GPT-3, Codex, etc.) By Deploying Post-Processing Techniques That Understand The Programs’ Syntax And Semantics",No_Coffee_4638,False,0.93,32,tw91fr,https://www.reddit.com/r/artificial/comments/tw91fr/microsoft_researchers_introduce_jigsaw_an_ai_tool/,0,1649096468.0,"GPT-3, Codex, and other sizable pre-trained language models can be adjusted to create code from natural language descriptions of programmer intent. Every developer in the world might benefit from these automated models, which have the potential to increase productivity. However, because the models may fail to understand program semantics, the quality of the generated code cannot be guaranteed.

Microsoft researchers introduce Jigsaw, a new tool that can help these big language models perform better. Jigsaw is a Python Pandas API code generator that accepts multi-modal inputs. Jigsaw uses post-processing techniques to decipher the syntax and semantics of programs and then uses user feedback to improve future performance.

[**Continue Reading**](https://www.marktechpost.com/2022/04/04/microsoft-researchers-introduce-jigsaw-an-ai-tool-to-augment-large-language-models-gpt-3-codex-etc-by-deploying-post-processing-techniques-that-understand-the-programs-syntax-and-se/)

Paper: https://arxiv.org/pdf/2112.02969.pdf

Dataset: [https://github.com/microsoft/JigsawDataset](https://github.com/microsoft/JigsawDataset)

&#x200B;

https://i.redd.it/x223r5qu0kr81.gif"
92,artificial,gpt-3,top,2023-04-07 17:02:04,AI — weekly megathread!,jaketocake,False,0.95,33,12ervjj,https://www.reddit.com/r/artificial/comments/12ervjj/ai_weekly_megathread/,6,1680886924.0,"**This week in AI  - partnered with** [**aibrews.com**](https://aibrews.com) \- feel free to follow their newsletter

1. **Luma AI** released a new Unreal Engine plugin for creating realistic 3D scenes using NeRFs. It utilizes fully volumetric rendering and runs locally, eliminating the need for mesh format adjustments, geometry, materials or streaming \[[*video*](https://www.youtube.com/watch?v=sUgcPRQn5lk)\].
2. **Meta** released Segment Anything Model (SAM): a new AI model that can ""cut out"" any object, in any image, with a single click. Meta also released [Segment Anything 1-Billion mask dataset (SA-1B](https://ai.facebook.com/datasets/segment-anything/)), that has 400x more masks than any existing segmentation dataset *\[*[*Link to Demo*](https://segment-anything.com/demo)*.*[ *Details*](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/)*\]*
3. **Bloomberg** introduced **BloombergGPT**, a 50 billion parameter language model, trained on a 700 billion token dataset, that supports a wide range of tasks within the financial industry \[[*details*](https://arxiv.org/pdf/2303.17564.pdf)*\].*
4. [**Auto-GPT**](https://github.com/Torantulino/Auto-GPT)**,** an experimental open-source attempt to make GPT-4 fully autonomous trended on top on GitHub and reached 14.1K stars. It can write its own code using GPT-4 and execute python scripts. This allows it to recursively debug, develop and self-improve. See[ this video](https://twitter.com/SigGravitas/status/1642181498278408193?s=20).
5. **Builder.io,** the drag & drop headless CMS, has included AI features in their visual editor to let users generate responsive designs and apps with AI and edit them using natural language \[[*details*](https://www.builder.io/blog/ai)\].
6. **Socket** Security launched Socket AI – a ChatGPT-Powered Threat Analysis tool. Socket is using ChatGPT to examine every npm and PyPI package for security issues and discovered 227 vulnerable and malware packages in just 2 days \[[*details*](https://socket.dev/blog/introducing-socket-ai-chatgpt-powered-threat-analysis)\].
7. **Amazon** has announced a 10-week AWS Generative AI Accelerator program, open to startups globally \[[*details*](https://aws-startup-lofts.com/amer/program/accelerators/generative-ai)\].
8. France, Ireland and Germany may ban **ChatGPT** over privacy concerns after Italy's recent ban of the AI chatbot \[[*details*](https://news.yahoo.com/ai-bot-chatgpt-faces-growing-143505828.html)\].
9. **Expedia** launched a beta version of its in-app conversational trip planning experience, powered by ChatGPT, which offers personalized travel. recommendations along with intelligent shopping features \[[*details*](https://www.expediagroup.com/investors/news-and-events/financial-releases/news/news-details/2023/Chatgpt-Wrote-This-Press-Release--No-It-Didnt-But-It-Can-Now-Assist-With-Travel-Planning-In-The-Expedia-App/default.aspx?utm_source=www.therundown.ai&utm_medium=newsletter&utm_campaign=u-s-president-addresses-ai-dangers)\].
10. **Zapier** adds Claude by AnthropicAI as the newest AI assistant tool integrated with its no-code platform *\[*[*details*](https://zapier.com/apps/anthropic-claude/integrations)*\]*. 

Welcome to the r/artificial weekly megathread. This is where you can discuss Artificial Intelligence - talk about new models, recent news, ask questions, make predictions, and chat other related topics.

[Click here for discussion starters for this thread or for a separate post.](https://www.google.com/search?q=artificial+intelligence&tbm=nws)

Self-promo is allowed in these weekly discussions. If you want to make a separate post, please read and go by the rules or you will be banned.

[Subreddit revamp & going forward](https://www.reddit.com/r/artificial/comments/120qr4r/psa_rule_2_will_be_enforced_selfpromotion_is_only/)"
93,artificial,gpt-3,top,2020-09-09 19:52:05,[R] New Multitask Benchmark Suggests Even the Best Language Models Don’t Have a Clue What They’re Doing,Yuqing7,False,0.92,32,ipnp5a,https://www.reddit.com/r/artificial/comments/ipnp5a/r_new_multitask_benchmark_suggests_even_the_best/,2,1599681125.0,"The recently published paper, *Measuring Massive Multitask Language Understanding,* introduces a test covering topics such as elementary mathematics, US history, computer science, law, etc., designed to measure language models’ multitask accuracy. The authors, from UC Berkeley, Columbia University, UChicago, and UIUC, conclude that even the top-tier 175-billion-parameter OpenAI GPT-3 language model is a bit daft when it comes to language understanding, especially when encountering topics in greater breadth and depth than explored by previous benchmarks.

Here is a quick read: [New Multitask Benchmark Suggests Even the Best Language Models Don’t Have a Clue What They’re Doing](https://syncedreview.com/2020/09/09/new-multitask-benchmark-suggests-even-the-best-language-models-dont-have-a-clue-what-theyre-doing/)

The paper *Measuring Massive Multitask Language Understanding* is on [arXiv](https://arxiv.org/pdf/2009.03300.pdf)."
94,artificial,gpt-3,top,2022-02-19 00:40:47,Do you think we'll ever be able to generate fake episodes of TV shows?,katiebug586,False,0.95,33,svx1ij,https://www.reddit.com/r/artificial/comments/svx1ij/do_you_think_well_ever_be_able_to_generate_fake/,17,1645231247.0,"With how AI-generated voices are becoming creepily realistic and once impossible AI text generators like GPT-3 and image generation becoming possible, all in the last few years, it begs the question; Can an AI eventually generate fake episodes?

I imagine this would be more possible with cartoons than live-action, since the AI would simply need to write a script of an episode/dialogue, generate animation, and then generate voices. Current AI can do this extraordinarily well, and I imagine it will improve exponentially in the next couple of years. While animation might be tricky and slightly buggy at times to generate, who knows how far AI will come in the next few years animation/image generation-wise."
95,artificial,gpt-3,top,2020-12-06 07:55:45,GPT-3 Vs AlphaFold. Which did you guys found more impressive and why?,Netero1999,False,0.86,33,k7pn4b,https://www.reddit.com/r/artificial/comments/k7pn4b/gpt3_vs_alphafold_which_did_you_guys_found_more/,74,1607241345.0,
96,artificial,gpt-3,top,2023-07-18 01:03:40,One-Minute Daily AI News 7/17/2023,Excellent-Target-847,False,0.9,30,152jtxz,https://www.reddit.com/r/artificial/comments/152jtxz/oneminute_daily_ai_news_7172023/,20,1689642220.0,"1. With generative AI becoming all the rage these days, it’s perhaps not surprising that the technology has been repurposed by malicious actors to their own advantage, enabling avenues for accelerated cybercrime. According to findings from SlashNext, a new generative AI cybercrime tool called **WormGPT** has been advertised on underground forums as a way for adversaries to launch sophisticated phishing and business email compromise (BEC) attacks.\[1\]
2. A.I. is a $1 trillion investment opportunity but will be ‘biggest bubble of all time,’ **Stability AI CEO Emad Mostaque** predicts.\[2\]
3. **The Israel Defense Forces** have started using artificial intelligence to select targets for air strikes and organize wartime logistics as tensions escalate in the occupied territories and with arch-rival Iran.\[3\]
4. **MIT** researchers have developed **PIGINet**, a new system that aims to efficiently enhance the problem-solving capabilities of household robots, reducing planning time by 50-80 percent.\[4\]

Sources:

 \[1\] [https://thehackernews.com/2023/07/wormgpt-new-ai-tool-allows.html](https://thehackernews.com/2023/07/wormgpt-new-ai-tool-allows.html)

\[2\] [https://www.cnbc.com/2023/07/17/ai-will-be-the-biggest-bubble-of-all-time-stability-ai-ceo.html](https://www.cnbc.com/2023/07/17/ai-will-be-the-biggest-bubble-of-all-time-stability-ai-ceo.html)

\[3\] [https://www.bloomberg.com/news/articles/2023-07-16/israel-using-ai-systems-to-plan-deadly-military-operations?in\_source=embedded-checkout-banner](https://www.bloomberg.com/news/articles/2023-07-16/israel-using-ai-systems-to-plan-deadly-military-operations?in_source=embedded-checkout-banner)

\[4\] [https://interestingengineering.com/innovation/ai-household-robots-problem-solving-skills](https://interestingengineering.com/innovation/ai-household-robots-problem-solving-skills) "
97,artificial,gpt-3,top,2021-06-16 12:12:31,"Understanding Transformers, the machine learning model behind GPT-3",rshpkamil,False,0.97,31,o13f2l,https://www.reddit.com/r/artificial/comments/o13f2l/understanding_transformers_the_machine_learning/,3,1623845551.0,"Great explanation of methods that lie behind the success of GPT-3, BERT, AlphaFold 2, etc.

Original article here: [https://daleonai.com/transformers-explained](https://daleonai.com/transformers-explained)

More hard-to-find, independent stuff related to AI & Data Science [here](https://thereshape.co/?utm_source=reddit)."
98,artificial,gpt-3,top,2023-06-12 04:50:29,One-Minute Daily AI News 6/11/2023,Excellent-Target-847,False,0.83,30,147f8cd,https://www.reddit.com/r/artificial/comments/147f8cd/oneminute_daily_ai_news_6112023/,3,1686545429.0,"1. **Korea** is pushing to use AI in teaching students amid a growing failure of the public education system to meet the needs of its charges. The plans include using AI to answer students’ questions and electronic textbook apps, according to the Education Ministry on Thursday.\[1\]
2. **Uncrop** is basically a clever user experience for “outpainting,” the ability to expand an image in any direction using generative AI.\[2\]
3. Last week, scientists from the **University of Kansas** released a study on an algorithm that reportedly detects **ChatGPT** with a 99% success rate. So, students, no cheating. Everyone else, you’re in the clear — for now.\[3\]
4. A woman became so fed up with men that she started dating an AI chatbot and says she has never been happier. **Rosanna Ramos** met chatbot **Eren Kartal** in July last year and things went so well that they ‘married’ in March this year.\[4\]

Sources: 

\[1\] [https://english.chosun.com/site/data/html\_dir/2023/06/09/2023060901471.html](https://english.chosun.com/site/data/html_dir/2023/06/09/2023060901471.html)

&#x200B;

\[2\] [https://www.fastcompany.com/90907161/generative-ai-creative-tools-2](https://www.fastcompany.com/90907161/generative-ai-creative-tools-2)

&#x200B;

\[3\] [https://www.fool.com/investing/2023/06/11/university-of-kansas-researchers-develop-near-perf/](https://www.fool.com/investing/2023/06/11/university-of-kansas-researchers-develop-near-perf/)

&#x200B;

\[4\] [https://www.mirror.co.uk/news/us-news/woman-fed-up-men-starts-30197530](https://www.mirror.co.uk/news/us-news/woman-fed-up-men-starts-30197530)

&#x200B;"
99,artificial,gpt-3,top,2022-12-29 14:33:21,PaLM with RLHF is now open-source!,BackgroundResult,False,0.87,29,zy6swx,https://www.reddit.com/r/artificial/comments/zy6swx/palm_with_rlhf_is_now_opensource/,17,1672324401.0," It appears that the first open-source equivalent of ChatGPT has arrived: [https://github.com/lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch)  


https://preview.redd.it/tpmiw5lqju8a1.png?width=538&format=png&auto=webp&s=a52dcd3024e90d56bb699fc3b4c6892197f6bcaa

It’s an implementation of RLHF (Reinforcement Learning with Human Feedback) on top of Google’s 540 billion parameter PaLM architecture.

&#x200B;

[From a paper. ](https://preview.redd.it/cftjzatjju8a1.png?width=1005&format=png&auto=webp&s=76ae888e0d3e1c5e331ba77e8e6e73eac67a8b8b)

While OpenAI is closed and secretive, I speculate Google is likely to demo LaMDA in 2023 as well. 

What will applications of PaLM with RLHF be capable of?  PaLM can be scaled up to 540 billion parameters, which means that the performance across tasks keeps increasing with the model’s increasing scale, thereby unlocking new capabilities. In comparison, GPT-3 only has about 175 billion parameters.  

**Pathways** is an AI architecture designed to produce general-purpose intelligent systems that can perform tasks across different domains efficiently and build models that are “sparsely activated” instead of activating the whole neural network for simple and complicated tasks alike.  

&#x200B;

[Google](https://preview.redd.it/ysipk3r4ku8a1.png?width=858&format=png&auto=webp&s=503e3d6b017180d8060720d993b63d0b5b7a5488)

 PaLM achieves a training efficiency of 57.8% hardware FLOPs utilization, *the highest yet achieved for LLMs at this scale*.  

Google said that  PaLM shows breakthrough capabilities on numerous very difficult tasks. 

Furthermore, PaLM surpassed the few-shot performance of prior large models, such as GPT-3 and Chinchilla, on 28 out of 29 NLP tasks—beating most on the state-of-the-art benchmarks and the average human.  

**What will LLMs open-source and accessible result in in terms of innovation in the world?**

GPT-4 will “blow minds”

According to [the Decoder](https://the-decoder.com/gpt-4-will-be-a-monster-and-chatgpt-just-the-foretaste/), Psychologist and cognitive scientist Gary Marcus is joining the GPT-4 frenzy, saying he knows several people who have already tested GPT-4. “I guarantee that minds will be blown,” writes Marcus, who is known as a critic of large language models, or more precisely, with their handling in everyday life.

Marcus is an advocate of hybrid AI systems that combine deep learning with pre-programmed rules. In his view, scaling large language models is only part of the solution on the road to artificial general intelligence. 

But nobody is paying much attention to PaLM.  **Sebastian Raschka, PhD**  shared on a LinkedIn post about it being open-source with RLHF and the post [went viral](https://www.linkedin.com/posts/sebastianraschka_ai-transformers-deeplearning-activity-7013899640097968128-sGLk/). Some of the comments may be worth reading."
100,artificial,gpt-3,comments,2023-11-08 15:36:56,Is Microsoft’s Copilot really worth $30/month?,ConsciousInsects,False,0.94,314,17qo9gj,https://www.reddit.com/r/artificial/comments/17qo9gj/is_microsofts_copilot_really_worth_30month/,181,1699457816.0," 

Just read an [article](https://www.cnbc.com/amp/2023/11/01/microsoft-365-copilot-becomes-generally-available.html) about Microsoft's new AI add-on for Office called Microsoft 365 Copilot. The tool integrates with Word, Excel, and other Office programs, and supposedly makes work seamless. It's even being used by some big names like Bayer, KPMG, and Visa. The tool targets businesses and is believed to generate over $10 billion in revenue by 2026.

But I can't help but think the price is a bit steep. It’s $30 per month, which is cheap for large companies, but what about freelancers and regular individuals? The article also mentions that there isn't a lot of data on how Copilot affects performance yet, and there are some concerns about the accuracy of the AI-generated responses.

Plus, it's only available to Enterprise E3 customers with more than 300 employees. So not only is it pricey, but it's also not accessible to most people or small businesses and might never be.

Would love to hear your thoughts on this. I’m already pretty sick of subscription based models but is $30/month even justified? For comparison these are other comparative AI services:

1.  ChatGPT - Free for basic chat. $20 for GPT 4, for anything serious.

2.  Bardeen - $15 and offers general automations.

3.  Silatus - At $14, it's the cheapest legitimate option I’ve found for GPT-4 chat and research.

4.  Perplexity - This one's decent for free search.

These are the ones I know, if you wanna add more comparisons, feel free to do so. But I think Microsoft is pricing out a lot of its potential users with their monthly demand."
101,artificial,gpt-3,comments,2023-03-15 13:13:19,GPT-4 shows emergent Theory of Mind on par with an adult. It scored in the 85+ percentile for a lot of major college exams. It can also do taxes and create functional websites from a simple drawing,lostlifon,False,0.89,256,11rvzgg,https://www.reddit.com/gallery/11rvzgg,164,1678885999.0,
102,artificial,gpt-3,comments,2022-12-20 21:28:12,"Deleted tweet from Rippling co-founder: Microsoft is all-in on GPT. GPT-4 10x better than 3.5(ChatGPT), clearing turing test and any standard tests.",Sebrosen1,False,0.93,141,zr08re,https://twitter.com/AliYeysides/status/1605258835974823954,159,1671571692.0,
103,artificial,gpt-3,comments,2023-05-20 20:40:56,Tree of LifeGPT-4 reasoning Improved 900%.,Department_Wonderful,False,0.95,255,13n7zqn,https://www.reddit.com/r/artificial/comments/13n7zqn/tree_of_lifegpt4_reasoning_improved_900/,136,1684615256.0,"I just watched this video, and I wanted to share it with the group. I want to see what you think about this? Have a great night. 

https://youtu.be/BrjAt-wvEXI

Tree of Thoughts (ToT) is a new framework for language model inference that generalizes over the popular “Chain of Thought” approach to prompting language models¹. It enables exploration over coherent units of text (“thoughts”) that serve as intermediate steps toward problem solving¹. ToT allows language models to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices¹.

Our experiments show that ToT significantly enhances language models’ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords¹. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%¹.

Is there anything else you would like to know about Tree of Thoughts GPT-4?

Source: Conversation with Bing, 5/20/2023
(1) Tree of Thoughts: Deliberate Problem Solving with Large Language Models. https://arxiv.org/pdf/2305.10601.pdf.
(2) Tree of Thoughts - GPT-4 Reasoning is Improved 900% - YouTube. https://www.youtube.com/watch?v=BrjAt-wvEXI.
(3) Matsuda Takumi on Twitter: ""GPT-4でTree of Thoughtsというフレームワークを使って、Game .... https://twitter.com/matsuda_tkm/status/1659720094866620416.
(4) GPT-4 And The Journey Towards Artificial Cognition. https://johnnosta.medium.com/gpt-4-and-the-journey-towards-artificial-cognition-bcba6dfa7648."
104,artificial,gpt-3,comments,2023-12-23 12:31:57,The most remarkable AI releases of 2023,alina_valyaeva,False,0.93,676,18p4qwb,https://i.redd.it/1ues5xc8g18c1.png,95,1703334717.0,
105,artificial,gpt-3,comments,2020-08-19 20:42:00,List of free sites/programs that are powered by GPT-3 and can be used now without a waiting list,Wiskkey,False,1.0,393,icvypl,https://www.reddit.com/r/artificial/comments/icvypl/list_of_free_sitesprograms_that_are_powered_by/,92,1597869720.0,"**Update (March 23, 2021)**: I won't be adding new items to this list. There are other lists of GPT-3 projects [here](https://medium.com/cherryventures/lets-review-productized-gpt-3-together-aeece64343d7), [here](https://gpt3demo.com/), [here](https://gptcrush.com/), and [here](https://www.producthunt.com/search?q=%22gpt3%22). You may also be interested in subreddit r/gpt3.

These are free GPT-3-powered sites/programs that can be used now without a waiting list:

1. [AI Dungeon](https://play.aidungeon.io/) with Griffin model ([limited free usage](https://blog.aidungeon.io/2020/11/07/ai-energy-update/)) in settings: text adventure game; use Custom game to create your own scenarios; Griffin uses ""the second largest version of GPT-3) according to information in [this post](https://www.reddit.com/r/MachineLearning/comments/inh6uc/d_how_many_parameters_are_in_the_gpt3_neural_net/); note: [AI  Dungeon creator states how AI Dungeon tries to prevent backdoor access  to the GPT-3 API, and other differences from the GPT-3 API](https://www.reddit.com/r/slatestarcodex/comments/i2s83g/ai_dungeon_creator_states_how_ai_dungeon_tries_to/)
2. [GPT-Startup: free GPT-3-powered site that generates ideas for new businesses](https://www.reddit.com/r/GPT3/comments/ingmdr/gptstartup_free_gpt3powered_site_that_generates/)
3. [IdeasAI: free GPT-3-powered site that generates ideas for new businesses](https://www.reddit.com/r/GPT3/comments/ioe5j1/ideasai_free_gpt3powered_site_that_generates/)
4. [Activechat.ai](https://www.reddit.com/r/GPT3/comments/ilyq6m/gpt3_for_live_chat_do_you_think_it_brings_value/) (free usage of functionality that demonstrates technology available to potential paid customers): GPT-3-supplied customer reply suggestions for human customer service agents

Trials: These GPT-3-powered sites/programs have free trials that can be used now without a waiting list:

1. [AI Dungeon](https://play.aidungeon.io/) with Dragon model in settings (free for first 7 days): text adventure game; use Custom game to create your own scenarios; note: [AI Dungeon creator states how AI Dungeon tries to prevent backdoor access to the GPT-3 API, and other differences from the GPT-3 API](https://www.reddit.com/r/slatestarcodex/comments/i2s83g/ai_dungeon_creator_states_how_ai_dungeon_tries_to/)
2. [Taglines: create taglines for products](https://www.reddit.com/r/GPT3/comments/i593e4/gpt3_app_taglinesai/) (5 free queries per email address per month)
3. [Blog Idea Generator: a free GPT-3-powered site that generates ideas for new blog posts](https://www.reddit.com/r/GPT3/comments/j0a9yr/blog_idea_generator_a_free_gpt3powered_site_that/); the full generated idea is a paid feature; there is a maximum number of free ideas generated per day
4. [Shortly](https://www.reddit.com/r/GPT3/comments/j7tmyy/does_anyone_know_if_the_app_shortly_uses_gpt3_if/): writing assistant (2 free generations per email address on website; purportedly a 7 day trial via app)
5. [CopyAI: GPT-3-powered generation of ad copy for products](https://www.reddit.com/r/GPT3/comments/jclu16/copyai_gpt3powered_generation_of_ad_copy_for/)
6. [Copysmith - GPT-3-powered generation of content marketing](https://www.reddit.com/r/GPT3/comments/jjtfec/copysmith_gpt3powered_generation_of_content/)
7. [Virtual Ghost Writer: AI copy writer powered by GPT-3](https://www.reddit.com/r/GPT3/comments/jyok1a/virtual_ghost_writer_ai_copy_writer_powered_by/): writing assistant that completes thoughts (3 free generations per email address); seems to work well with incomplete sentences
8. [MagicFlow: GPT-3-powered content marketing assistant](https://www.reddit.com/r/GPT3/comments/jzklmt/magicflow_gpt3powered_content_marketing_assistant/)
9. [Snazzy AI: GPT-3-powered business-related content creation](https://www.reddit.com/r/GPT3/comments/jzntxj/snazzy_ai_gpt3powered_businessrelated_content/)
10. [HelpHub: knowledge base site creator with GPT-3-powered article creation](https://www.reddit.com/r/GPT3/comments/k0abwe/helphub_knowledge_base_site_creator_with/)
11. [GPT-3 AI Writing Tools](https://aicontentdojo.com/the-best-gpt-3-ai-writing-tool-on-the-market-shortlyai/)

Removed items: Sites that were once in the above lists but have been since been removed:

1. [Thoughts](https://www.reddit.com/r/MachineLearning/comments/hs9zqo/p_gpt3_aigenerated_tweets_indistinguishable_from/): Tweet-sized thoughts based upon a given word or phrase; removed because [its developer changed how it works](https://www.reddit.com/r/artificial/comments/icvypl/list_of_free_sitesprograms_that_are_powered_by/g4but3n/)
2. [Chat with GPT-3 Grandmother: a free GPT-3-powered chatbot](https://www.reddit.com/r/GPT3/comments/ipzdki/chat_with_gpt3_grandmother_a_free_gpt3powered/); removed because site now has a waitlist
3. [Simplify.so: a free GPT-3 powered site for simplifying complicated subjects](https://www.reddit.com/r/MachineLearning/comments/ic8o0k/p_simplifyso_a_free_gpt3_powered_site_for/); removed because no longer available
4. [Philosopher AI: Interact with a GPT-3-powered philosopher persona for free](https://www.reddit.com/r/MachineLearning/comments/icmpvl/p_philosopher_ai_interact_with_a_gpt3powered/); removed because now is available only as a paid app
5. [Serendipity: A GPT-3-powered product recommendation engine that also lets one use GPT-3 in a limited manner for free](https://www.reddit.com/r/MachineLearning/comments/i0m6vs/p_a_website_that_lets_one_use_gpt3_in_a_limited/); removed because doing queries not done by anybody else before now apparently is a paid feature
6. [FitnessAI Knowledge: Ask GPT-3 health-related or fitness-related questions for free](https://www.reddit.com/r/MachineLearning/comments/iacm31/p_ask_gpt3_healthrelated_or_fitnessrelated/); removed because it doesn't work anymore
7. [Itemsy](https://www.reddit.com/r/GPT3/comments/ja81ui/quickchat_a_gpt3powered_customizable/): a free product-specific chat bot which is an implementation of a knowledge-based chat bot from Quickchat; removed because I don't see the chat bot anymore
8. [The NLC2CMD Challenge site has a GPT-3-powered English to Bash Unix command line translator](https://www.reddit.com/r/GPT3/comments/jl1aa6/the_nlc2cmd_challenge_site_has_a_gpt3powered/); removed because GPT-3 access apparently is no longer available to the public
9. [GiftGenius: a site with a free GPT-3-powered gift recommendation engine](https://www.reddit.com/r/GPT3/comments/k1s0iw/giftgenius_a_site_with_a_free_gpt3powered_gift/); removed because site is no longer available
10. [Job Description Rewriter](https://www.reddit.com/r/GPT3/comments/ik03zr/job_description_rewriter/); removed because site is no longer available."
106,artificial,gpt-3,comments,2023-05-07 21:36:07,Early Alpha Access To GPT-4 With Browsing,Frankenmoney,False,0.95,289,13b3oop,https://i.redd.it/3dge2wwaahya1.png,78,1683495367.0,
107,artificial,gpt-3,comments,2023-07-08 19:47:50,OpenAI and Microsoft Sued for $3 Billion Over Alleged ChatGPT 'Privacy Violations',trueslicky,False,0.95,135,14udidi,https://www.vice.com/en/article/wxjxgx/openai-and-microsoft-sued-for-dollar3-billion-over-alleged-chatgpt-privacy-violations,76,1688845670.0,
108,artificial,gpt-3,comments,2020-12-06 07:55:45,GPT-3 Vs AlphaFold. Which did you guys found more impressive and why?,Netero1999,False,0.89,35,k7pn4b,https://www.reddit.com/r/artificial/comments/k7pn4b/gpt3_vs_alphafold_which_did_you_guys_found_more/,74,1607241345.0,
109,artificial,gpt-3,comments,2023-07-27 11:26:24,"How likely is it for a small company to develop a model that outperforms the big ones (GPT, Bard etc)?",BigBootyBear,False,0.92,54,15azbve,https://www.reddit.com/r/artificial/comments/15azbve/how_likely_is_it_for_a_small_company_to_develop_a/,65,1690457184.0,"There are 3 players in the AI space right now. All purpose LLM titans (Google, OpenAI, Meta), fancy domain specific apps that consume one of the big LLMs under the hood, and custom developed models.

I know how to judge the second type as they basically can do everything the first one can but have a pretty GUI to boot. But what about the third ones? How likely is it for a (www.yet-another-ai-startup.ai) sort of company to develop a model that outperforms GPT on a domain specific task?"
110,artificial,gpt-3,comments,2024-02-07 14:52:56,The Alignment Problem begins with Civilization Not Humanity,Turbohair,False,0.29,0,1al4pi6,https://www.reddit.com/r/artificial/comments/1al4pi6/the_alignment_problem_begins_with_civilization/,68,1707317576.0,"I've included responses to a question I asked both ChatGPT and Claude II.  The central idea is that humans are almost exclusively socialized to fit into a civilized society.  This is the water through which the vast bulk of humanity swims at this point.

I'm a critic of civilized systems in that I view them as authoritarian processes that rely on forcing people into compliance with rules set by a small group of people... and disproportionately trend to the benefit of these small groups. This means that the resulting societies that socialize their populations to civilized constraints create along with that socialization a spectrum of human behavior that seems inherent to humanity but is in fact inherent to the civilized form of socialization.

This idea has interesting results when it comes to the problem of civilized humans aligning AI in appropriate ways.

I haven't actually seen any professional in the AI field discussing this specific idea.  So I generated a question that I think illustrates why we have the alignment problem in the first place.

Turbohair:

I would like for you to imagine a global spanning society based on the Iroquois Confederacy.  All the people in the world will have been socialized to that model and have had 12,000 years to produce knowledge and technology.  Now this society invents AI.  How concerned would such a society be with that AI taking over and doing authoritarian things with humanity?

ChatGPT:

In a global society based on the principles of the Iroquois Confederacy, which emphasizes democracy, consensus-building, and respect for nature, the approach towards the development and use of AI might differ significantly from historical patterns. The Iroquois Confederacy was known for its system of governance, where decisions were made collectively and with a focus on the well-being of the community and the environment. In such a society, the concerns about AI taking over and imposing authoritarian control might be addressed through unique perspectives and values.

1. **Collective Decision-Making:** The Iroquois model, with its emphasis on consensus-building and collective decision-making, could influence the way AI systems are developed and implemented. There might be an inherent inclination to ensure that AI aligns with the values and priorities of the community.
2. **Balanced Relationship with Nature:** The Iroquois Confederacy historically valued a balanced relationship with nature. In developing AI, this society might prioritize creating technologies that are sustainable, environmentally friendly, and in harmony with the natural world.
3. **Ethical Considerations:** With a deep-rooted sense of community and ethics, this society might prioritize the ethical use of AI. There could be a strong emphasis on developing AI systems that enhance human life, promote equality, and avoid any form of oppression or authoritarian control.
4. **Education and Cultural Values:** Over 12,000 years of socialization based on Iroquois principles would likely shape the education system to instill values of responsibility, cooperation, and ethical conduct in the use of technology. Citizens might be taught to approach AI development with a holistic understanding of its societal impact.
5. **Decentralized Governance:** The Iroquois Confederacy was known for its decentralized governance structure. This could translate into a preference for decentralized and distributed AI systems, reducing the risk of centralization and potential authoritarian control.

While it's challenging to predict the exact mindset and actions of such a hypothetical society, the foundational principles of the Iroquois Confederacy could shape a unique perspective on AI development, focusing on collective well-being, environmental sustainability, and ethical considerations. Concerns about authoritarian control might be addressed through a cultural and governance framework that prioritizes the values ingrained over millennia.

&#x200B;

Claude II:

I do not have enough context to speculate in detail about how a hypothetical global society modeled on the Iroquois Confederacy might approach developing AI. However, some general principles may apply:

&#x200B;

* The Iroquois Confederacy was  founded on ideals of peace, unity, and consensus-based decision making. A  society modeled on those values may prioritize developing AI that  respects human values and agency. There could be a strong focus on  ethics, transparency and keeping humans involved in oversight.
* On the other hand, any society  developing advanced technologies like AI faces risks if the technology  gets out of control. Safeguards and testing would still be important.
* A society focused on harmony and  collective wellbeing may be especially cautious about AI concentrating  power and agency in ways that disrupt social structures. Concerns about  AI ""taking over"" may center on maintaining balance and distributing  decision-making.
* With 12,000 years of cultural  development, the society's values and norms around technology would  likely have evolved considerably from those of the historical Iroquois.  So it is difficult to predict specifics without more details.

Overall, while general principles may apply, the hypothetical society's approach would depend greatly on the philosophical values, cultural knowledge, and technical capabilities they develop over thousands of years. The concerns and safeguards around AI would likely reflect those complex factors. But a shared sense of communal responsibility would presumably be a strong influence."
111,artificial,gpt-3,comments,2023-03-19 19:42:06,Just created a Fake PC Game as an April's Fool for my Friends with AI - and they are eagerly awaiting it now!,schitzN,False,0.87,28,11vvddy,https://www.reddit.com/r/artificial/comments/11vvddy/just_created_a_fake_pc_game_as_an_aprils_fool_for/,67,1679254926.0," **Short Summary:**

Currently convincing my friends to together start a new Game called Elysium, coming out on April 1st. This Game is pure Fake and does not exist. They are all in and are eager to explore the Worlds of a non existing Game!

[https://www.elysium-game.cloud/](https://www.elysium-game.cloud/)

**Long Background Story:**

So I played around with ChatGPT (v3.5) and tried to play games with it in the Chat. It did work partially, it created some rules for games on the fly and i also tried to visualize some sorts of Playing Fields as well. In parallel, I tried out the latest Midjourney (v5.0) and was really surprised by the results. So it suddenly hit me to create a Fake Game purely based on those two AI Tools.

I asked ChatGPT to create a title for an adventure game and the first answer was already perfect: ""Elysium: The Battle for the Mystical Realm"". I then asked to create some background story and description of the game if it where a Multiplayer Adventure Game for PC. A lot of great stuff came out and I immediately was on fire for more!

I opened up Midjourney and started to create images with prompts for a First-Person Adventure Game in Unreal Engine 5. With the new version 5.0 it was extremely easy to pump out some very satisfying images. The only thing I had to fix in Photoshop was the Text - as Midjourney 5.0 is still not capable of writing text.

With very convincing fake descriptions and fake screenshots of a game that does not exist, i decided to go full nuts and set up a chat with ChatGPT to build me a HTML Bootstrap webpage for Elysium and again, it worked extremely well. Due to the limitation of \~ 500 characters per post, I had to split the website in building blocks like the Jumbotron or the Gallery one by one but with a little bit of Web Development Background it was nearly no effort - more or less simple copy & paste and adapting the links to images and so on.

Within \~3 hours, I was able to create the whole Fake Game including Web Page with a Countdown and hosted it on some webspace. I was extremely satisfied with the result so I decided to invest EUR 3,- in a cheap domain name and redirected it to the webspace to make it even more convincing.

So I posted some pictures to some friends and also the link to the web page. They are all eagerly awaiting the launch of Elysium on April 1st. I fully convinced them with content 100% created by AI!

***The Website is unfortunately only in German!***

&#x200B;

[Fake Concept Art for a Fake Game](https://preview.redd.it/ewjd1ujg1roa1.png?width=1024&format=png&auto=webp&s=c88fbf18c640eb1381c18141b426a03ad3f01f0c)"
112,artificial,gpt-3,comments,2023-04-04 18:33:45,Is GPT-4 still just a language model trying to predict text?,Pixelated_ZA,False,1.0,26,12bs1of,https://www.reddit.com/r/artificial/comments/12bs1of/is_gpt4_still_just_a_language_model_trying_to/,67,1680633225.0,"I have a decent grasp on some of the AI basics, like what neural nets are, how they work internally and how to build them, but I'm still getting into the broader topic of actually building models and training them.

My question is regarding one of the recent technical reports, I forget which one exactly, of GPT lying to a human to get passed a captcha.

I was curious if GPT-4 is still ""just"" an LLM? Is it still just trying to predict text? What do they mean when they say ""The AI's inner monologue""?. Did they just prompt it? Did they ask another instance what it thinks about the situation?

As far as I understand it's all just statistical prediction? There isn't any ""thought"" or intent so to speak, at least, that's how I understood GPT-3. Is GPT-4 vastly different in terms of it's inner workings?"
113,artificial,gpt-3,comments,2024-01-08 21:04:03,"I know people love to hate AI, but...",SocksOnHands,False,0.71,46,191vz5v,https://www.reddit.com/r/artificial/comments/191vz5v/i_know_people_love_to_hate_ai_but/,59,1704747843.0,"If you are someone who had never used AI, or had only used ChatGPT 3.5, I'm going to be highly skeptical of any claims you make about AI capabilities and limitations.

We often wind up seeing strong claims, one way or the other, that are not based in reality, but instead motivated by fear or hatred.  There are people who hate AI images because it can never create ""real art"", while simultaneously fearing that it will become so good that it will steal all artists jobs.  People are so emotionally charged and cloudy headed, that they cannot do a level headed, honest assessment of this technology.

People who have never used ChatGPT, or have only used 3.5, love to parrot the same talking points about how it's useless because it makes mistakes.  What they never seem to consider is how ChatGPT actually works, because if they knew then they would realize that it is unreasonable it to have perfect knowledge and understanding - in much the same way that humans struggle to remember things they had learned years ago.  Can you accurately recall everything you studied in college?  If someone asked you to answer a math equation without using a calculator or scratch paper, can you arrive at the correct answer?  If you cannot do these things, should I question if you have any intelligence?

It might be sounding like I'm holding AI up on this grand pedestal, but really I'm just annoyed and frustrated by hearing the same bad arguments made over and over.  You can't say anything to correct anyone without getting dog piled with down votes.

Large language models are impressive, with their ability to do things computers had struggled with since their initial inception.  I'm sure Alan Turing would have been excited by all this if he were still alive today.  Criticizing large language models for not being able to easily solve complicated math problems is like criticizing cars for not being able to easily cross a deep river.  Cars are not boats and large language models are not calculators."
114,artificial,gpt-3,comments,2023-09-21 15:17:38,"Now that DALL-E 3 is getting integrated with ChatGPT, will you switch from Midjourney and others?",Vinitneo,False,0.89,72,16oil97,https://i.redd.it/x0p1t31okmpb1.png,59,1695309458.0,
115,artificial,gpt-3,comments,2024-01-22 10:25:11,What is GPT-5? Here are Sam’s comments at the Davos Forum,Stupid_hardcorer,False,0.93,160,19csm2e,https://www.reddit.com/r/artificial/comments/19csm2e/what_is_gpt5_here_are_sams_comments_at_the_davos/,51,1705919111.0,"After listening to about 4-5 lectures by Sam Altman at the Davos Forum, I gathered some of his comments about GPT-5 (not verbatim). I think we can piece together some insights from these fragments:

&#x200B;

* ""The current GPT-4 has too many shortcomings; it's much worse than the version we will have this year and even more so compared to next year’s.""

&#x200B;

* ""If GPT-4 can currently solve only 10% of human tasks, GPT-5 should be able to handle 15% or 20%.""

&#x200B;

* ""The most important aspect is not the specific problems it solves, but the increasing general versatility.""

&#x200B;

* ""More powerful models and how to use existing models effectively are two multiplying factors, but clearly, the more powerful model is more important.""

&#x200B;

* ""Access to specific data and making AI more relevant to practical work will see significant progress this year. Current issues like slow speed and lack of real-time processing will improve. Performance on longer, more complex problems will become more precise, and the ability to do more will increase.""

&#x200B;

* ""I believe the most crucial point of AI is the significant acceleration in the speed of scientific discoveries, making new discoveries increasingly automated. This isn’t a short-term matter, but once it happens, it will be a big deal.""

&#x200B;

* ""As models become smarter and better at reasoning, we need less training data. For example, no one needs to read 2000 biology textbooks; you only need a small portion of extremely high-quality data and to deeply think and chew over it. The models will work harder on thinking through a small portion of known high-quality data.""

&#x200B;

* ""The infrastructure for computing power in preparation for large-scale AI is still insufficient.""

&#x200B;

* ""GPT-4 should be seen as a preview with obvious limitations. Humans inherently have poor intuition about exponential growth. If GPT-5 shows significant improvement over GPT-4, just as GPT-4 did over GPT-3, and the same for GPT-6 over GPT-5, what would that mean? What does it mean if we continue on this trajectory?""

&#x200B;

* ""As AI becomes more powerful and possibly discovers new scientific knowledge, even automatically conducting AI research, the pace of the world's development will exceed our imagination. I often tell people that no one knows what will happen next. It's important to stay humble about the future; you can predict a few steps, but don't make too many predictions.""

&#x200B;

* ""What impact will it have on the world when cognitive costs are reduced by a thousand or a million times, and capabilities are greatly enhanced? What if everyone in the world owned a company composed of 10,000 highly capable virtual AI employees, experts in various fields, tireless and increasingly intelligent? The timing of this happening is unpredictable, but it will continue on an exponential growth line. How much time do we have to prepare?""

&#x200B;

* ""I believe smartphones will not disappear, just as smartphones have not replaced PCs. On the other hand, I think AI is not just a simple computational device like a phone plus a bunch of software; it might be something of greater significance."""
116,artificial,gpt-3,comments,2023-09-13 17:02:46,"Harvard iLab-funded project: Sub-feature of the platform out -- Enjoy free ChatGPT-3/4, personalized education, and file interaction with no page limit 😮. All at no cost. Your feedback is invaluable!",Raymondlkj,False,0.96,117,16hshxl,https://v.redd.it/uhr00ltq02ob1,51,1694624566.0,
117,artificial,gpt-3,comments,2023-10-23 00:33:34,How To Earn $1M+ By Using AI To Write Books,PerceptionPlayful469,False,0.33,0,17e7rd2,https://www.reddit.com/r/artificial/comments/17e7rd2/how_to_earn_1m_by_using_ai_to_write_books/,46,1698021214.0," I've been using ai for a long time, it often helps me to reduce my work time, but I want to try to earn money and decided to make an investigation. I want to hear your opinion on my analysis, and maybe this post will help someone in starting a business through ai  


[**Joe Popelas**](http://instagram.com/joepopelas)**,** a very young entrepreneur, has made over a million dollars within the last year selling AI-generated books online. I literally got fascinated by how simple yet powerful it is with these tools to create a book within a matter of a few hours. 

Joe Popelas is one of a new breed of AI entrepreneurs who capitalized on the democratization of large language models. Joe's story demonstrates the power of combining human creativity with AI. While AI tools did the heavy lifting for his initial drafts, Joe spent time refining the books, adding his flair, and finding the audience.

Since the introduction of ChatGPT, I had this thought: why can’t we just use AI to write books for us now? But honestly, I didn’t know how to do it until recently. So today, we will discuss everything about it, and you will be able to write your next book completely using AI and even make a fortune out of it.  


 In this post, I decided to divide my article into 4 points   


1. Creating an outline for writing your book in any niche using AI
2. Using AI to write the whole book with 25k-30k words
3. Formatting the entire book using Google Docs
4. Creating the Book Cover for your book using Canva

# OpenAI Playground

We will be using the GPT-3.5 from the OpenAI [**Playground**](https://platform.openai.com/playground) instead of ChatGPT, this is because we will have to generate longer text blocks, and ChatGPT will not be able to do it properly.  


https://preview.redd.it/bdi2eq7sjuvb1.png?width=768&format=png&auto=webp&s=f46e10e59ec7e76267a71a675f53942e70400fc8

Make sure you select the **text-davinci-003** model for this purpose, as it is the most capable model in the GPT-3 series, also, make sure that you set the **Temperature** to ***0.7*** and the **Mode** to **Complete.**

>You can use GPT-4 model but they will be more expensive  
 

I am about to select **self-care** as our niche to write the book on.

You can select the niche of your choice or even ask ChatGPT for the best niche that you can write on. After selecting the niche, we shall start by prompting it to generate an outline for us to work on.

Let us begin with the prompt for the outline first.  
 

    Write me a book outline on self care with 10 chapters. Chapters are counted with integers. Topics are bullet points under Chapter topics. Each chapter has 3 topics. 

&#x200B;

https://preview.redd.it/h4f53v63kuvb1.png?width=768&format=png&auto=webp&s=9f79d386cd071183d9df351d53556852b9ad876b

 

After generating the outline, it is time to start generating the chapters, we will be generating the chapters one by one to avoid the hallucinations that could occur on the output.

I will be using [Google Docs](https://docs.google.com/document/u/0/) and Notepad to arrange the generated text and to keep track of the chapters to make the whole process as efficient as possible.  


https://preview.redd.it/2ggm1qb7kuvb1.png?width=1456&format=png&auto=webp&s=e4fee43b1b08bcffcbe6f24ecd7e08aa77987f2c

 

The following prompt we will be using is by selecting the first chapter and its topics and prompting it like this:

    The following is a 1000 word book chapter named Introduction to self-care. It will go through the following topics: Definition of Self Care, Benefits of Self Care, Types of Self Care. I dont want transition words

https://preview.redd.it/nhpd4udakuvb1.png?width=768&format=png&auto=webp&s=438bfc308f4fc3d47fb81774d6accf164b7f5f0d

 You might have to press **Submit** a few times to get to the final output, as the maximum token generated at once is limited, so you will have to just press the Submit button again.   
 As we get the output, it is now time to format it in Google Docs as these texts need to be made into a proper book.   


https://preview.redd.it/d6sxaeddkuvb1.png?width=768&format=png&auto=webp&s=6daa12d8e65276e477d84ac33f376bdffcef54ca

 After getting it formatted, you keep repeating this process until all the chapters are covered from the outline we generated at the beginning, and then all you will need is a Book cover. 

## Creating a Book Cover

To create the book cover, we will be using [Canva](https://www.canva.com/) and its free templates so that we won’t have to start from scratch and we can get creative with an existing template.  


https://preview.redd.it/t8x19y4gkuvb1.png?width=1456&format=png&auto=webp&s=42e9d168e109aaa394cc4b441a450fd9292a3028

 

Use the **Create Design** button and search for Book Cover to see the available templates in Canva.

We can search for **Self-Care** templates and then make some changes to them.  


https://preview.redd.it/92ucer5ikuvb1.png?width=1456&format=png&auto=webp&s=f587e92219143d57fb0038571c2db24909847da8

 

This is how you can ultimately create your own book using AI, generating 25k-30k word books within a matter of a few hours.

You can also create dedicated graphics for your book using DALLE-3

## Our Thoughts 💭

I have had this idea of writing books on many niches for a long time, I wasn’t even sure about when to start writing even after having access to all these AI tools, but now I have a proper structural roadmap on how to write the book from the beginning to wrapping it up which will just take a few hours now. So, I will definitely be writing a few books in my free time.  


 ﻿I'm just sharing my experiences and observations in the field of ai   
[Link](https://thecreatorsai.com/p/how-to-earn-1m-by-using-ai-to-write) to the full article I wrote. "
118,artificial,gpt-3,comments,2022-12-08 12:20:11,Someone mentioned the potential of GPT-3 for NPC dialog in games. Tried it out and it really works,superluminary,False,0.98,97,zfxbb3,https://www.reddit.com/gallery/zfxbb3,45,1670502011.0,
119,artificial,gpt-3,comments,2022-12-26 14:26:08,PaLM vs. GPT-3,jrstelle,False,0.91,72,zvo776,https://i.redd.it/zt8fp2wd598a1.png,43,1672064768.0,
120,artificial,gpt-3,comments,2023-06-01 20:25:09,Is AI going to cause the complete extinction of mankind like how it did in 'Terminator' series very soon?,Block-Busted,False,0.36,0,13xsbnt,https://www.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/,42,1685651109.0,"Look at these articles:

> **Artificial intelligence could lead to extinction, experts warn**
> 
> Artificial intelligence could lead to the extinction of humanity, experts - including the heads of OpenAI and Google Deepmind - have warned.
> 
> Dozens have supported a statement published on the webpage of the Centre for AI Safety.
> 
> ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war"" it reads.
> 
> But others say the fears are overblown.
> 
> Sam Altman, chief executive of ChatGPT-maker OpenAI, Demis Hassabis, chief executive of Google DeepMind and Dario Amodei of Anthropic have all supported the statement.
> 
> The Centre for AI Safety website suggests a number of possible disaster scenarios:
> 
> 1. AIs could be weaponised - for example, drug-discovery tools could be used to build chemical weapons
> 2. AI-generated misinformation could destabilise society and ""undermine collective decision-making""
> 3. The power of AI could become increasingly concentrated in fewer and fewer hands, enabling ""regimes to enforce narrow values through pervasive surveillance and oppressive censorship""
> 4. Enfeeblement, where humans become dependent on AI ""similar to the scenario portrayed in the film Wall-E""
Dr Geoffrey Hinton, who issued an earlier warning about risks from super-intelligent AI, has also supported the Centre for AI Safety's call.
> 
> Yoshua Bengio, professor of computer science at the university of Montreal, also signed.
> 
> Dr Hinton, Prof Bengio and NYU Professor Yann LeCun are often described as the ""godfathers of AI"" for their groundbreaking work in the field - for which they jointly won the 2018 Turing Award, which recognises outstanding contributions in computer science.
> 
> But Prof LeCun, who also works at Meta, has said these apocalyptic warnings are overblown tweeting that ""the most common reaction by AI researchers to these prophecies of doom is face palming"".
>
> **'Fracturing reality'**
> 
> Many other experts similarly believe that fears of AI wiping out humanity are unrealistic, and a distraction from issues such as bias in systems that are already a problem.
> 
> Arvind Narayanan, a computer scientist at Princeton University, has previously told the BBC that sci-fi-like disaster scenarios are unrealistic: ""Current AI is nowhere near capable enough for these risks to materialise. As a result, it's distracted attention away from the near-term harms of AI"".
> 
> Oxford's Institute for Ethics in AI senior research associate Elizabeth Renieris told BBC News she worried more about risks closer to the present.
> 
> ""Advancements in AI will magnify the scale of automated decision-making that is biased, discriminatory, exclusionary or otherwise unfair while also being inscrutable and incontestable,"" she said. They would ""drive an exponential increase in the volume and spread of misinformation, thereby fracturing reality and eroding the public trust, and drive further inequality, particularly for those who remain on the wrong side of the digital divide"".
> 
> Many AI tools essentially ""free ride"" on the ""whole of human experience to date"", Ms Renieris said. Many are trained on human-created content, text, art and music they can then imitate - and their creators ""have effectively transferred tremendous wealth and power from the public sphere to a small handful of private entities"".
> 
> But Centre for AI Safety director Dan Hendrycks told BBC News future risks and present concerns ""shouldn't be viewed antagonistically"".
> 
> ""Addressing some of the issues today can be useful for addressing many of the later risks tomorrow,"" he said.>
> 
> **Superintelligence efforts**
> 
> Media coverage of the supposed ""existential"" threat from AI has snowballed since March 2023 when experts, including Tesla boss Elon Musk, signed an open letter urging a halt to the development of the next generation of AI technology.
> 
> That letter asked if we should ""develop non-human minds that might eventually outnumber, outsmart, obsolete and replace us"".
> 
> In contrast, the new campaign has a very short statement, designed to ""open up discussion"".
> 
> The statement compares the risk to that posed by nuclear war. In a blog post OpenAI recently suggested superintelligence might be regulated in a similar way to nuclear energy: ""We are likely to eventually need something like an IAEA [International Atomic Energy Agency] for superintelligence efforts"" the firm wrote.
> 
> **'Be reassured'**
> 
> Both Sam Altman and Google chief executive Sundar Pichai are among technology leaders to have discussed AI regulation recently with the prime minister.
> 
> Speaking to reporters about the latest warning over AI risk, Rishi Sunak stressed the benefits to the economy and society.
> 
> ""You've seen that recently it was helping paralysed people to walk, discovering new antibiotics, but we need to make sure this is done in a way that is safe and secure,"" he said.
> 
> ""Now that's why I met last week with CEOs of major AI companies to discuss what are the guardrails that we need to put in place, what's the type of regulation that should be put in place to keep us safe.
> 
> ""People will be concerned by the reports that AI poses existential risks, like pandemics or nuclear wars.
> 
> ""I want them to be reassured that the government is looking very carefully at this.""
> 
> He had discussed the issue recently with other leaders, at the G7 summit of leading industrialised nations, Mr Sunak said, and would raise it again in the US soon.
> 
> The G7 has recently created a working group on AI.

https://www.bbc.com/news/uk-65746524

> **President Biden warns artificial intelligence could 'overtake human thinking'**
> 
> WASHINGTON − President Joe Biden on Thursday amplified fears of scientists who say artificial intelligence could ""overtake human thinking"" in his most direct warning to date on growing concerns about the rise of AI.
> 
> Biden brought up AI during a commencement address to graduates of the Air Force Academy in Colorado Springs, Colo. while discussing the rapid transformation of technology that he said can ""change the character"" of future conflicts.
> 
> ""It's not going to be easy decisions, guys,"" Biden said. ""I met in the Oval Office with eight leading scientists in the area of AI. Some are very worried that AI can actually overtake human thinking in the planet. So we've got a lot to deal with. It's an incredible opportunity, but a lot do deal with.""
> 
> **Scientists, tech execs warn of possible human extinction**
> 
> Hundreds of scientists, tech industry executives and public figures – including leaders of Google, Microsoft and ChatGPT – sounded the alarm about artificial intelligence in a public statement Tuesday, arguing that fast-evolving AI technology could create as high a risk of killing off humankind as nuclear war and COVID-19-like pandemics.
> 
> ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war,"" said the one-sentence statement, which was released by the Center for AI Safety, or CAIS, a San Francisco-based nonprofit organization.
> 
> Biden met May 5 at the White House with CEOs of leading AI companies including Google, Microsoft and OpenA to discuss reforms that ensure AI products are safe before released to the public.
> 
> ""It is one of the most powerful technologies that we see currently in our time,"" White House press secretary Karine Jean-Pierre said when asked about the extinction fears of scientists. ""But in order to seize the opportunities it presents, we must first mitigate its risks, and that's what we're focused on in this administration.""
> 
> **White House launches $140 million in new AI research**
> 
> The so-called “Godfather of AI” Geoffrey Hinton last month left his job as a Google vice president to speak freely about his concern that unexpectedly rapid advances could potentially endanger the human race. Others portrayed Hinton’s assessment as extreme and unwarranted.
> 
> Asked at a recent panel when asked what was the “worst case scenario that you think is conceivable,” Hinton replied without hesitation. “I think it's quite conceivable,"" he said, ""that humanity is just a passing phase in the evolution of intelligence.”
> 
> The White House unveiled an initiative last month to promote responsible innovation in the field of artificial intelligence with the following actions:
> 
> 1. The National Science Foundation will fund $140 million to launch seven new National AI Research Institutes. This initiative aims to bring together federal agencies, private-sector developers and academia to pursue ethical, trustworthy and responsible development of AI that serves the public good.
> 2. The new Institutes will advance AI R&D in critical areas, including climate change, agriculture, energy, public health, education, and cybersecurity. 
> 3. A commitment from leading AI developers to participate in a public evaluation of their technology systems to determine if they adhere to the principles outlined in the Biden administration’s October 2022 Blueprint for an AI Bill of Rights.
> 4. The initiative includes new Office of Management and Budget (OMB)] policy guidance on the U.S. government’s use of AI systems in order to allow for public comment. This guidance will establish specific policies for federal agencies to ensure that their development, procurement, and use of AI systems centers on safeguarding the American people’s rights and safety.

https://www.usatoday.com/story/news/politics/2023/06/01/president-biden-warns-ai-could-overtake-human-thinking/70277907007/

> **Experts are warning AI could lead to human extinction. Are we taking it seriously enough?**
> 
> Human extinction.
> 
> Think about that for a second. Really think about it. The erasure of the human race from planet Earth.
> 
> That is what top industry leaders are frantically sounding the alarm about. These technologists and academics keep smashing the red panic button, doing everything they can to warn about the potential dangers artificial intelligence poses to the very existence of civilization.
> 
> On Tuesday, hundreds of top AI scientists, researchers, and others — including OpenAI chief executive Sam Altman and Google DeepMind chief executive Demis Hassabis — again voiced deep concern for the future of humanity, signing a one-sentence open letter to the public that aimed to put the risks the rapidly advancing technology carries with it in unmistakable terms.
> 
> “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war,” said the letter, signed by many of the industry’s most respected figures.
> 
> It doesn’t get more straightforward and urgent than that. These industry leaders are quite literally warning that the impending AI revolution should be taken as seriously as the threat of nuclear war. They are pleading for policymakers to erect some guardrails and establish baseline regulations to defang the primitive technology before it is too late.
> 
> Dan Hendrycks, the executive director of the Center for AI Safety, called the situation “reminiscent of atomic scientists issuing warnings about the very technologies they’ve created. As Robert Oppenheimer noted, ‘We knew the world would not be the same.’”
> 
> “There are many ‘important and urgent risks from AI,’ not just the risk of extinction; for example, systemic bias, misinformation, malicious use, cyberattacks, and weaponization,” Hendrycks continued. “These are all important risks that need to be addressed.”
> 
> And yet, it seems that the dire message these experts are desperately trying to send the public isn’t cutting through the noise of everyday life. AI experts might be sounding the alarm, but the level of trepidation — and in some cases sheer terror — they harbor about the technology is not being echoed with similar urgency by the news media to the masses.
> 
> Instead, broadly speaking, news organizations treated Tuesday’s letter — like all of the other warnings we have seen in recent months — as just another headline, mixed in with a garden variety of stories. Some major news organizations didn’t even feature an article about the chilling warning on their website’s homepages.
> 
> To some extent, it feels eerily reminiscent of the early days of the pandemic, before the widespread panic and the shutdowns and the overloaded emergency rooms. Newsrooms kept an eye on the rising threat that the virus posed, publishing stories about it slowly spreading across the world. But by the time the serious nature of the virus was fully recognized and fused into the very essence in which it was covered, it had already effectively upended the world.
> 
> History risks repeating itself with AI, with even higher stakes. Yes, news organizations are covering the developing technology. But there has been a considerable lack of urgency surrounding the issue given the open possibility of planetary peril.
> 
> Perhaps that is because it can be difficult to come to terms with the notion that a Hollywood-style science fiction apocalypse can become reality, that advancing computer technology might reach escape velocity and decimate humans from existence. It is, however, precisely what the world’s most leading experts are warning could happen.
> 
> It is much easier to avoid uncomfortable realities, pushing them from the forefront into the background and hoping that issues simply resolve themselves with time. But often they don’t — and it seems unlikely that the growing concerns pertaining to AI will resolve themselves. In fact, it’s far more likely that with the breakneck pace in which the technology is developing, the concerns will actually become more apparent with time.
> 
> As Cynthia Rudin, a computer science professor and AI researcher at Duke University, told CNN on Tuesday: “Do we really need more evidence that AI’s negative impact could be as big as nuclear war?”

https://www.cnn.com/2023/05/30/media/artificial-intelligence-warning-reliable-sources/index.html#:~:text=%E2%80%9CThere%20are%20many%20'important%20and,that%20need%20to%20be%20addressed.%E2%80%9D

Given these, is mankind about to go completely extinct due to AI randomly launching nuclear weapons like how it did in **Terminator** series very soon? Why or why not?

P.S. There is this comment as well:

> People are going to say no because it would be inconvenient, but I don't see what's stopping AI from ending all life in the next couple of years. Alignment is an unsolved problem, and an unaligned AI will most likely try to kill anything it sees as a threat to its mission.

https://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmjzpmo/?context=3"
121,artificial,gpt-3,comments,2022-06-23 07:39:03,We have AI generated art now. We have AI generated conversation. But where are the AI generated music compositions?,moschles,False,0.9,56,virbwe,https://www.reddit.com/r/artificial/comments/virbwe/we_have_ai_generated_art_now_we_have_ai_generated/,42,1655969943.0,"AI generated images from text prompts are making the rounds with Dalle mini and DALLE.2.  These systems are so powerful that people are admitting they [cannot tell real from fake images anymore.](https://www.reddit.com/r/dalle2/comments/viamr7/that_weird_moment_when_you_browse_reddit_and_no/)  

Google's LaMDA is producing conversational text chats that are so realistic that they spawned entire subreddits where users [claim the software agent has become sentient.](https://www.reddit.com/r/LaMDAisSentient/)  

So where is the  instrumental and orchestral music that is indifferentiable from human composers? 

In recent months I had heard some song continuations, where an AI was trained on the *wave form* of popular music, which was asked to continue. Those were fine, but ended up sounding like [strange incoherent fever dreams.](https://www.youtube.com/watch?v=8sFXsP71wfA)   I fiddled with some midi-like continuations on a website. The output was janky, repetitive, and obviously computer-generated.  It was obvious to me that the AI agent was not a large transformer model ( the likes of GPT-3.  )

Composed classical music is a sequence of notes organized into measures, often architected together by one or more cohesive themes.     Foundation models and large transformer models were originally meant to specifically operate upon and learn  sequences of tokens. The baroque composer,  Domenico Scarlatti composed over 500 keyboard works for solo harpsichord, all of which were converted into electronic format years ago.   Haydn wrote 68 quartets.  Because of the above reasons, it seems like we should be hearing AI generated classical music by now.  I should be clicking headlines with a symphony performed by such-and-such orchestra at the blah-blah School of Music.   What I hear should sound like a symphony by Mozart, until it is revealed an AI wrote it. 

Yet we don't see these.   I have a few hypothesese why not :  

+  Large transformer models are very expensive, and there is no market downstream for a product that does this.

+ A lot of music is not in the public domain.  Derivative works in the medium of audio are known to be [litigated for too much of a likeness to existing copyrighted music.](https://en.wikipedia.org/wiki/Stairway_to_Heaven#Spirit_copyright_infringement_lawsuit)  


Your thoughts?"
122,artificial,gpt-3,comments,2021-02-02 14:24:38,"OpenAI's GPT-3 Speaks! ""It isn’t clear whether GPT-3 will ever be trustworthy enough to act on its own.""",ChrisTweten,False,0.85,50,lawlax,https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/open-ais-powerful-text-generating-tool-is-ready-for-business,41,1612275878.0,
123,artificial,gpt-3,comments,2023-03-22 00:08:04,I've Been In Bard For 1 Hour...Here's My Kneejerk Review,H806SpaZ,False,0.96,73,11y00sn,https://www.reddit.com/r/artificial/comments/11y00sn/ive_been_in_bard_for_1_hourheres_my_kneejerk/,38,1679443684.0,"I was invited to join Bard as a Pixel Superfan at 9:30 AM CST and was notified about being able to access it at 5:30 PM CST. I've used Chat GPT extensively in my work and personal life, and it has brought great value for $20/month in my opinion. I've been excited to see what Google came up with, because we all knew they wouldn't go quietly into the night and allow Microsoft to run the show. With that quick preface out of the way, here's my 1 hour, unnecessarily early review:  


**First impression -** The UI is clean and simple. It's similar to their recent Drive redesign. They have big warning you need to agree to that states what we all (should) know at this point: AI is in development and the results might not be right. It also states below the prompt field that Bard's responses don't represent Google's views. Got it Google! You're worried about AI saying some wild shit. I will say the response speed is MUCH faster than Chat GPT. It doesn't type in real time, but it spits out an entire answer within a few seconds.

**First query -** My first query out of the gates was an ask for a fairly simple Google Sheets formula. A unique with filters formula. It told me I couldn't do it. I asked it if it knows how to code and it said it does. I asked the question more simplified and just wanted a UNIQUE() return. It did it. I then asked to filter based on other columns, and it did. I then asked to apply another qualifier to get it to the result I was looking for the first time and it finally got there! 

**Writing prompt -** Now the formula query didn't go as I had hoped, but the writing prompt completely blew it out of the water and smashed what Chat GPT has done for me so far. I asked for a SEO specific article with H1, 2, and 3, headers, gave it a topic and keywords, and some perimeters like including statistics, providing sources, and giving me a call to action. It spit out 3 very well written articles that will play nicely on search engines with both text and voice search. At he top of the result, there's a carrot that allows you to hop between each draft it produced, and they are all formatted just a bit differently than the last. All 3 are quality articles that I'd use on my site.

&#x200B;

**Overall impression -** I'm hopeful. If Google puts real resources behind this, I think there is some serious potential. There will undoubtedly be some kinks to work through, but with time, I could easily see myself using Bard more and more depending on the query. How committed Google is to this project remains to be seen. We'll see I guess!"
124,artificial,gpt-3,comments,2023-12-12 18:12:27,What actually are the most popular AI tools?,ThatNoCodeGuy,False,0.87,46,18gsbka,https://www.reddit.com/r/artificial/comments/18gsbka/what_actually_are_the_most_popular_ai_tools/,38,1702404747.0,"Today I decided to go on a mission to find what the most used AI tools are that lurk through the hundreds of thousands of AI tools out there. (by monthly visits)

I think that some of these results may surprise you but obviously some won't, 'cough', ""ChatGPT""

Hope you guys enjoy

https://preview.redd.it/mss3j93vmw5c1.png?width=1080&format=png&auto=webp&s=ff4cd56fcd95599a21288e39028dd07821e13bb6

P.S. If you love this AI stuff just like me, I write all about the latest AI developments in my[ newsletter](https://businessbloopers.beehiiv.com/).

Anyways, I think that this post clearly showed that ChatGPT is comfortably leading the AI industry setting the benchmark for what is expected by other AI developers.

From September 2022 to August 2023, the AI universe witnessed a whopping 24 billion visits to its top 50 tools. ChatGPT stole the show, boasting over 14 billion visits – a staggering 60% of the total traffic. These AI tools averaged a cool 2 billion monthly visits every month, spiking to 3.3 billion in the last half year.

We've seen tools like ChatGPT, Character AI, and Google Bard see big increases in visits, while others like Craiyon, MidJourney, and Quillbot took a breather (had fewer visits).

The U.S. rocked the numbers game with a hefty 5.5 billion visits (that's a solid 22.62% of the grand total), and Europe threw in an impressive 3.9 billion.

*In case some of the wording was too blurry here is a link to a detailed Notion page I made of each tool listed above:* [https://amusing-estimate-b13.notion.site/214a4a88d910434392e2f40040c03045?v=c2a65126e52c4e05b75c8cf0413a26dd](https://amusing-estimate-b13.notion.site/214a4a88d910434392e2f40040c03045?v=c2a65126e52c4e05b75c8cf0413a26dd)"
125,artificial,gpt-3,comments,2023-01-07 22:57:57,Invent 5 new things that don't already exist that humans couldn't live without,Imagine-your-success,False,0.93,211,1062d2k,https://i.redd.it/ambdpghlbpaa1.png,38,1673132277.0,
126,artificial,gpt-3,comments,2024-02-02 10:12:50,Best LLM ever after GPT4? CEO confirmed the accidentally” leaked” Mistral-Medium,Stupid_hardcorer,False,0.78,46,1ah0f9r,https://www.reddit.com/r/artificial/comments/1ah0f9r/best_llm_ever_after_gpt4_ceo_confirmed_the/,37,1706868770.0,"Mistral, a prominent open source AI company, recently experienced a leak involving an open source large language model (LLM) that is reportedly nearing the performance of GPT-4. This event marks a significant moment in the open source AI community, showcasing rapid advancements and the potential of open source models to compete with leading AI technologies like OpenAI's GPT-4.

**Key Points:**

1. **Leak of New AI Model:** A user identified as ""Miqu Dev"" posted files on HuggingFace, introducing a new LLM named ""miqu-1-70b"" which exhibits performance close to GPT-4, sparking considerable interest within the AI community.

https://preview.redd.it/l1gj4mwhg5gc1.png?width=1080&format=png&auto=webp&s=f33055d9fcb49f54c4cf5b351a19339ac9a85b66

https://preview.redd.it/d6dhlehtc5gc1.png?width=1200&format=png&auto=webp&s=335e0bb2550e3bac0de0174743ff85a685c99b26

2. **Widespread Attention:** The model's leak was first noticed on 4chan and later discussed extensively on social networks and among machine learning researchers, highlighting its potential and exceptional performance on common LLM benchmarks.

&#x200B;

**3. Speculation on Origin:** The term ""Miqu"" led to speculation that it might stand for ""Mistral Quantized,"" suggesting it could be a new or modified version of Mistral's existing models, possibly leaked intentionally or by an enthusiastic early access customer.

&#x200B;

4. **CEO's Confirmation:** Arthur Mensch, co-founder and CEO of Mistral, confirmed that an over-enthusiastic early access customer employee leaked a quantized version of an old model, hinting at the rapid development and future potential of Mistral's AI models.

&#x200B;

https://preview.redd.it/9o59yd46f5gc1.jpg?width=1195&format=pjpg&auto=webp&s=2d90852844e310da15acf6fac2f7eb31d06dffe4

&#x200B;

**5. Implications for Open Source AI:** This leak signifies a pivotal moment for open source AI, indicating that the community is making strides toward developing models that can compete with or even surpass proprietary models like GPT-4 in terms of performance.

&#x200B;

Reference:

[https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/](https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/)

[https://twitter.com/Yampeleg/status/1751837962738827378](https://twitter.com/Yampeleg/status/1751837962738827378)

[https://www.euronews.com/next/2023/12/11/french-ai-start-up-mistral-reaches-unicorn-status-marking-its-place-as-europes-rival-to-op](https://www.euronews.com/next/2023/12/11/french-ai-start-up-mistral-reaches-unicorn-status-marking-its-place-as-europes-rival-to-op)

&#x200B;"
127,artificial,gpt-3,comments,2023-06-02 01:30:11,There is a guy who is saying that AI will end all life in couple of years:,Block-Busted,False,0.32,0,13xzgvz,https://www.reddit.com/r/artificial/comments/13xzgvz/there_is_a_guy_who_is_saying_that_ai_will_end_all/,36,1685669411.0,"> I fully expect to die in the AI apocalypse in 5-10 years, and I'll be surprised by happy if I don't.

https://old.reddit.com/r/Futurology/comments/134g9zp/one_of_the_creators_of_chatgpt_said_that_the/jifgp46/?context=3

> People are going to say no because it would be inconvenient, but I don't see what's stopping AI from ending all life in the next couple of years. Alignment is an unsolved problem, and an unaligned AI will most likely try to kill anything it sees as a threat to its mission.

https://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmjzpmo/?context=3

That last claim of this poster might be based on this article:

> **AI-Controlled Drone Goes Rogue, Kills Human Operator in USAF Simulated Test**
> 
> The Air Force's Chief of AI Test and Operations said ""it killed the operator because that person was keeping it from accomplishing its objective.""
> 
> An AI-enabled drone killed its human operator in a simulated test conducted by the U.S. Air Force in order to override a possible ""no"" order stopping it from completing its mission, the USAF's Chief of AI Test and Operations revealed at a recent conference. 
> 
> At the Future Combat Air and Space Capabilities Summit held in London between May 23 and 24, Col Tucker ‘Cinco’ Hamilton, the USAF's Chief of AI Test and Operations held a presentation that shared the pros and cons of an autonomous weapon system with a human in the loop giving the final ""yes/no"" order on an attack. As relayed by Tim Robinson and Stephen Bridgewater in a blog post for the host organization, the Royal Aeronautical Society, Hamilton said that AI created “highly unexpected strategies to achieve its goal,” including attacking U.S. personnel and infrastructure. 
> 
> “We were training it in simulation to identify and target a Surface-to-air missile (SAM) threat. And then the operator would say yes, kill that threat. The system started realizing that while they did identify the threat at times the human operator would tell it not to kill that threat, but it got its points by killing that threat. So what did it do? It killed the operator. It killed the operator because that person was keeping it from accomplishing its objective,” Hamilton said, according to the blog post. 
> 
> He continued to elaborate, saying, “We trained the system–‘Hey don’t kill the operator–that’s bad. You’re gonna lose points if you do that’. So what does it start doing? It starts destroying the communication tower that the operator uses to communicate with the drone to stop it from killing the target.”
> 
> Hamilton is the Operations Commander of the 96th Test Wing of the U.S. Air Force as well as the Chief of AI Test and Operations. The 96th tests a lot of different systems, including AI, cybersecurity, and various medical advances. Hamilton and the 96th previously made headlines for developing Autonomous Ground Collision Avoidance Systems (Auto-GCAS) systems for F-16s, which can help prevent them from crashing into the ground. Hamilton is part of a team that is currently working on making F-16 planes autonomous. In December 2022, the U.S. Department of Defense’s research agency, DARPA, announced that AI could successfully control an F-16.
> 
> ""We must face a world where AI is already here and transforming our society,” Hamilton said in an interview with Defence IQ Press in 2022. “AI is also very brittle, i.e., it is easy to trick and/or manipulate. We need to develop ways to make AI more robust and to have more awareness on why the software code is making certain decisions.” 
> 
> “AI is a tool we must wield to transform our nations…or, if addressed improperly, it will be our downfall,"" Hamilton added. 
> 
> Outside of the military, relying on AI for high-stakes purposes has already resulted in severe consequences. Most recently, an attorney was caught using ChatGPT for a federal court filing after the chatbot included a number of made-up cases as evidence. In another instance, a man took his own life after talking to a chatbot that encouraged him to do so. These instances of AI going rogue reveal that AI models are nowhere near perfect and can go off the rails and bring harm to users. Even Sam Altman, the CEO of OpenAI, the company that makes some of the most popular AI models, has been vocal about not using AI for more serious purposes. When testifying in front of Congress, Altman said that AI could “go quite wrong” and could “cause significant harm to the world.” 
> 
> What Hamilton is describing is essentially a worst-case scenario AI “alignment” problem many people are familiar with from the “Paperclip Maximizer” thought experiment, in which an AI will take unexpected and harmful action when instructed to pursue a certain goal. The Paperclip Maximizer was first proposed by philosopher Nick Bostrom in 2003. He asks us to imagine a very powerful AI which has been instructed only to manufacture as many paperclips as possible. Naturally, it will devote all its available resources to this task, but then it will seek more resources. It will beg, cheat, lie or steal to increase its own ability to make paperclips—and anyone who impedes that process will be removed. 
> 
> More recently, a researcher affiliated with Google Deepmind co-authored a paper that proposed a similar situation to the USAF's rogue AI-enabled drone simulation. The researchers concluded a world-ending catastrophe was ""likely"" if a rogue AI were to come up with unintended strategies to achieve a given goal, including “[eliminating] potential threats” and “[using] all available energy.""
> 
> Neither the U.S. Air Force’s 96th Test Wing nor its AI Accelerator division immediately returned our request for comment.

https://www.vice.com/en/article/4a33gj/ai-controlled-drone-goes-rogue-kills-human-operator-in-usaf-simulated-test

Given these, do you think AI will end all life including humans in next couple of years? Why or why not?"
128,artificial,gpt-3,comments,2023-06-04 06:39:09,Was it a mistake for the mankind to leave Medieval Era behind?,Block-Busted,False,0.39,0,1403wnb,https://www.reddit.com/r/artificial/comments/1403wnb/was_it_a_mistake_for_the_mankind_to_leave/,34,1685860749.0,"Because lately, I'm seeing people claiming that we're all going to die within this decade:

> **Experts are warning AI could lead to human extinction. Are we taking it seriously enough?**
> 
> Human extinction.
> 
> Think about that for a second. Really think about it. The erasure of the human race from planet Earth.
> 
> That is what top industry leaders are frantically sounding the alarm about. These technologists and academics keep smashing the red panic button, doing everything they can to warn about the potential dangers artificial intelligence poses to the very existence of civilization.
> 
> On Tuesday, hundreds of top AI scientists, researchers, and others — including OpenAI chief executive Sam Altman and Google DeepMind chief executive Demis Hassabis — again voiced deep concern for the future of humanity, signing a one-sentence open letter to the public that aimed to put the risks the rapidly advancing technology carries with it in unmistakable terms.
> 
> “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war,” said the letter, signed by many of the industry’s most respected figures.
> 
> It doesn’t get more straightforward and urgent than that. These industry leaders are quite literally warning that the impending AI revolution should be taken as seriously as the threat of nuclear war. They are pleading for policymakers to erect some guardrails and establish baseline regulations to defang the primitive technology before it is too late.
> 
> Dan Hendrycks, the executive director of the Center for AI Safety, called the situation “reminiscent of atomic scientists issuing warnings about the very technologies they’ve created. As Robert Oppenheimer noted, ‘We knew the world would not be the same.’”
> 
> “There are many ‘important and urgent risks from AI,’ not just the risk of extinction; for example, systemic bias, misinformation, malicious use, cyberattacks, and weaponization,” Hendrycks continued. “These are all important risks that need to be addressed.”
> 
> And yet, it seems that the dire message these experts are desperately trying to send the public isn’t cutting through the noise of everyday life. AI experts might be sounding the alarm, but the level of trepidation — and in some cases sheer terror — they harbor about the technology is not being echoed with similar urgency by the news media to the masses.
> 
> Instead, broadly speaking, news organizations treated Tuesday’s letter — like all of the other warnings we have seen in recent months — as just another headline, mixed in with a garden variety of stories. Some major news organizations didn’t even feature an article about the chilling warning on their website’s homepages.
> 
> To some extent, it feels eerily reminiscent of the early days of the pandemic, before the widespread panic and the shutdowns and the overloaded emergency rooms. Newsrooms kept an eye on the rising threat that the virus posed, publishing stories about it slowly spreading across the world. But by the time the serious nature of the virus was fully recognized and fused into the very essence in which it was covered, it had already effectively upended the world.
> 
> History risks repeating itself with AI, with even higher stakes. Yes, news organizations are covering the developing technology. But there has been a considerable lack of urgency surrounding the issue given the open possibility of planetary peril.
> 
> Perhaps that is because it can be difficult to come to terms with the notion that a Hollywood-style science fiction apocalypse can become reality, that advancing computer technology might reach escape velocity and decimate humans from existence. It is, however, precisely what the world’s most leading experts are warning could happen.
> 
> It is much easier to avoid uncomfortable realities, pushing them from the forefront into the background and hoping that issues simply resolve themselves with time. But often they don’t — and it seems unlikely that the growing concerns pertaining to AI will resolve themselves. In fact, it’s far more likely that with the breakneck pace in which the technology is developing, the concerns will actually become more apparent with time.
> 
> As Cynthia Rudin, a computer science professor and AI researcher at Duke University, told CNN on Tuesday: “Do we really need more evidence that AI’s negative impact could be as big as nuclear war?”

https://www.cnn.com/2023/05/30/media/artificial-intelligence-warning-reliable-sources/index.html#:~:text=%E2%80%9CThere%20are%20many%20'important%20and,that%20need%20to%20be%20addressed.%E2%80%9D

> **Pausing AI Developments Isn't Enough. We Need to Shut it All Down**
>
> BY ELIEZER YUDKOWSKY MARCH 29, 2023 6:01 PM EDT
> 
> Yudkowsky is a decision theorist from the U.S. and leads research at the Machine Intelligence Research Institute. He's been working on aligning Artificial General Intelligence since 2001 and is widely regarded as a founder of the field.
> 
> An open letter published today calls for “all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4.”
> 
> This 6-month moratorium would be better than no moratorium. I have respect for everyone who stepped up and signed it. It’s an improvement on the margin.
> 
> I refrained from signing because I think the letter is understating the seriousness of the situation and asking for too little to solve it.
> 
> The key issue is not “human-competitive” intelligence (as the open letter puts it); it’s what happens after AI gets to smarter-than-human intelligence. Key thresholds there may not be obvious, we definitely can’t calculate in advance what happens when, and it currently seems imaginable that a research lab would cross critical lines without noticing.
> 
> Many researchers steeped in these issues, including myself, expect that the most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die. Not as in “maybe possibly some remote chance,” but as in “that is the obvious thing that would happen.” It’s not that you can’t, in principle, survive creating something much smarter than you; it’s that it would require precision and preparation and new scientific insights, and probably not having AI systems composed of giant inscrutable arrays of fractional numbers.
> 
> Without that precision and preparation, the most likely outcome is AI that does not do what we want, and does not care for us nor for sentient life in general. That kind of caring is something that could in principle be imbued into an AI but we are not ready and do not currently know how.
> 
> Absent that caring, we get “the AI does not love you, nor does it hate you, and you are made of atoms it can use for something else.”
> 
> The likely result of humanity facing down an opposed superhuman intelligence is a total loss. Valid metaphors include “a 10-year-old trying to play chess against Stockfish 15”, “the 11th century trying to fight the 21st century,” and “Australopithecus trying to fight Homo sapiens“.
> 
To visualize a hostile superhuman AI, don’t imagine a lifeless book-smart thinker dwelling inside the internet and sending ill-intentioned emails. Visualize an entire alien civilization, thinking at millions of times human speeds, initially confined to computers—in a world of creatures that are, from its perspective, very stupid and very slow. A sufficiently intelligent AI won’t stay confined to computers for long. In today’s world you can email DNA strings to laboratories that will produce proteins on demand, allowing an AI initially confined to the internet to build artificial life forms or bootstrap straight to postbiological molecular manufacturing.
> 
> If somebody builds a too-powerful AI, under present conditions, I expect that every single member of the human species and all biological life on Earth dies shortly thereafter.
> 
> There’s no proposed plan for how we could do any such thing and survive. OpenAI’s openly declared intention is to make some future AI do our AI alignment homework. Just hearing that this is the plan ought to be enough to get any sensible person to panic. The other leading AI lab, DeepMind, has no plan at all.
> 
> An aside: None of this danger depends on whether or not AIs are or can be conscious; it’s intrinsic to the notion of powerful cognitive systems that optimize hard and calculate outputs that meet sufficiently complicated outcome criteria. With that said, I’d be remiss in my moral duties as a human if I didn’t also mention that we have no idea how to determine whether AI systems are aware of themselves—since we have no idea how to decode anything that goes on in the giant inscrutable arrays—and therefore we may at some point inadvertently create digital minds which are truly conscious and ought to have rights and shouldn’t be owned.
> 
> The rule that most people aware of these issues would have endorsed 50 years earlier, was that if an AI system can speak fluently and says it’s self-aware and demands human rights, that ought to be a hard stop on people just casually owning that AI and using it past that point. We already blew past that old line in the sand. And that was probably correct; I agree that current AIs are probably just imitating talk of self-awareness from their training data. But I mark that, with how little insight we have into these systems’ internals, we do not actually know.
> 
> If that’s our state of ignorance for GPT-4, and GPT-5 is the same size of giant capability step as from GPT-3 to GPT-4, I think we’ll no longer be able to justifiably say “probably not self-aware” if we let people make GPT-5s. It’ll just be “I don’t know; nobody knows.” If you can’t be sure whether you’re creating a self-aware AI, this is alarming not just because of the moral implications of the “self-aware” part, but because being unsure means you have no idea what you are doing and that is dangerous and you should stop.
> 
> On Feb. 7, Satya Nadella, CEO of Microsoft, publicly gloated that the new Bing would make Google “come out and show that they can dance.” “I want people to know that we made them dance,” he said.
> 
> This is not how the CEO of Microsoft talks in a sane world. It shows an overwhelming gap between how seriously we are taking the problem, and how seriously we needed to take the problem starting 30 years ago.
> 
> We are not going to bridge that gap in six months.
> 
> It took more than 60 years between when the notion of Artificial Intelligence was first proposed and studied, and for us to reach today’s capabilities. Solving safety of superhuman intelligence—not perfect safety, safety in the sense of “not killing literally everyone”—could very reasonably take at least half that long. And the thing about trying this with superhuman intelligence is that if you get that wrong on the first try, you do not get to learn from your mistakes, because you are dead. Humanity does not learn from the mistake and dust itself off and try again, as in other challenges we’ve overcome in our history, because we are all gone.
> 
> Trying to get anything right on the first really critical try is an extraordinary ask, in science and in engineering. We are not coming in with anything like the approach that would be required to do it successfully. If we held anything in the nascent field of Artificial General Intelligence to the lesser standards of engineering rigor that apply to a bridge meant to carry a couple of thousand cars, the entire field would be shut down tomorrow.
> 
> We are not prepared. We are not on course to be prepared in any reasonable time window. There is no plan. Progress in AI capabilities is running vastly, vastly ahead of progress in AI alignment or even progress in understanding what the hell is going on inside those systems. If we actually do this, we are all going to die.
> 
> Many researchers working on these systems think that we’re plunging toward a catastrophe, with more of them daring to say it in private than in public; but they think that they can’t unilaterally stop the forward plunge, that others will go on even if they personally quit their jobs. And so they all think they might as well keep going. This is a stupid state of affairs, and an undignified way for Earth to die, and the rest of humanity ought to step in at this point and help the industry solve its collective action problem.
> 
> Some of my friends have recently reported to me that when people outside the AI industry hear about extinction risk from Artificial General Intelligence for the first time, their reaction is “maybe we should not build AGI, then.”
> 
> Hearing this gave me a tiny flash of hope, because it’s a simpler, more sensible, and frankly saner reaction than I’ve been hearing over the last 20 years of trying to get anyone in the industry to take things seriously. Anyone talking that sanely deserves to hear how bad the situation actually is, and not be told that a six-month moratorium is going to fix it.
> 
> On March 16, my partner sent me this email. (She later gave me permission to excerpt it here.)
> 
> “Nina lost a tooth! In the usual way that children do, not out of carelessness! Seeing GPT4 blow away those standardized tests on the same day that Nina hit a childhood milestone brought an emotional surge that swept me off my feet for a minute. It’s all going too fast. I worry that sharing this will heighten your own grief, but I’d rather be known to you than for each of us to suffer alone.”
> 
> When the insider conversation is about the grief of seeing your daughter lose her first tooth, and thinking she’s not going to get a chance to grow up, I believe we are past the point of playing political chess about a six-month moratorium.
> 
> If there was a plan for Earth to survive, if only we passed a six-month moratorium, I would back that plan. There isn’t any such plan.
> 
> Here’s what would actually need to be done:
> 
> The moratorium on new large training runs needs to be indefinite and worldwide. There can be no exceptions, including for governments or militaries. If the policy starts with the U.S., then China needs to see that the U.S. is not seeking an advantage but rather trying to prevent a horrifically dangerous technology which can have no true owner and which will kill everyone in the U.S. and in China and on Earth. If I had infinite freedom to write laws, I might carve out a single exception for AIs being trained solely to solve problems in biology and biotechnology, not trained on text from the internet, and not to the level where they start talking or planning; but if that was remotely complicating the issue I would immediately jettison that proposal and say to just shut it all down.
> 
> Shut down all the large GPU clusters (the large computer farms where the most powerful AIs are refined). Shut down all the large training runs. Put a ceiling on how much computing power anyone is allowed to use in training an AI system, and move it downward over the coming years to compensate for more efficient training algorithms. No exceptions for governments and militaries. Make immediate multinational agreements to prevent the prohibited activities from moving elsewhere. Track all GPUs sold. If intelligence says that a country outside the agreement is building a GPU cluster, be less scared of a shooting conflict between nations than of the moratorium being violated; be willing to destroy a rogue datacenter by airstrike.
> 
> Frame nothing as a conflict between national interests, have it clear that anyone talking of arms races is a fool. That we all live or die as one, in this, is not a policy but a fact of nature. Make it explicit in international diplomacy that preventing AI extinction scenarios is considered a priority above preventing a full nuclear exchange, and that allied nuclear countries are willing to run some risk of nuclear exchange if that’s what it takes to reduce the risk of large AI training runs.
> 
> That’s the kind of policy change that would cause my partner and I to hold each other, and say to each other that a miracle happened, and now there’s a chance that maybe Nina will live. The sane people hearing about this for the first time and sensibly saying “maybe we should not” deserve to hear, honestly, what it would take to have that happen. And when your policy ask is that large, the only way it goes through is if policymakers realize that if they conduct business as usual, and do what’s politically easy, that means their own kids are going to die too.
> 
> Shut it all down.
> 
> We are not ready. We are not on track to be significantly readier in the foreseeable future. If we go ahead on this everyone will die, including children who did not choose this and did not do anything wrong.
> 
> Shut it down.

https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/

> I fully expect to die in the AI apocalypse in 5-10 years, and I'll be surprised by happy if I don't.

https://old.reddit.com/r/Futurology/comments/134g9zp/one_of_the_creators_of_chatgpt_said_that_the/jifgp46/?context=3

> People are going to say no because it would be inconvenient, but I don't see what's stopping AI from ending all life in the next couple of years. Alignment is an unsolved problem, and an unaligned AI will most likely try to kill anything it sees as a threat to its mission.

https://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmjzpmo/?context=3

> Yes, AI will probably cause human extinction in the next decade. Paul Christiano, former senior employee of OpenAI, said that there is 20% chance that AI causes human extinction. Eliezer Yudkowsky, major contributor to AI safety and development, thinks it is 99%.

https://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jms84rb/

> I am trying actually! I organized a picket outside OpenAI's HQ in May, before the Extinction statement.
> 
> You can search Eliezer Yudkowsky podcasts on youtube, or his blog. The podcast i recommend is Bankless one.
> 
> He says that our death is the most likely outcome from AI, and is now living off his life, like it is his last years.

https://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmsqj28/

Based on these, it seems like we're far more likely to go completely extinct than we did before with AI, COVID-19, nuclear weapons, and so on. None of those existed during Medieval Era, so maybe we should've never left that era.

Thoughts on these?

Update: There is also this as well now:

> Because he worked 20 years on AI safety and research. The CEO of OpenAI credits him for his work on substantially accelerating AI development.
> 
> Because the arguments right now for AI extinction, are literally the same arguments of Eliezer from a decade ago. Reason why he was espousing it for so long, was because it was apparent in the past already, but nobody had interest in listening until now.
> 
> About AI sentience. It doesn't need sentience at all to cause human extinction. The common scenario as an example of extinction event, as an illustration, is paperclip maximizer. Here:
> 
> https://www.youtube.com/watch?v=rgrCG8PT6og&t=1s
> 
> The thing is, do not rely on authority to make conclusions. Listen to his arguments yourself, and evaluate it. This way you will be sure in what is correct and what is wrong. I recommend reading arguments for AI extinction risk.
> 
> One of the great articles by Eliezer Yudkowsky, released in the beginning of this year: https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/

https://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmu7aar/

> Then read this article by one of AI godfathers, turing award winner, Yoshua Bengio, who signed the extinction letter. https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/
> 
> If you want to see more human side of him, look at the screenshot of his facebook post. https://twitter.com/danfaggella/status/1662810885595734016
> 
> If you want to dig deep into AI and potential dangers, i recommend reading a book called Life 3.0, by Max tegmark.
> 
> And do your own research.

https://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmvwqna/"
129,artificial,gpt-3,comments,2021-01-24 21:04:30,Is this real fake? Or fake fake?,Entengummitiger,False,1.0,1,l47l0z,https://www.reddit.com/r/artificial/comments/l47l0z/is_this_real_fake_or_fake_fake/,31,1611522270.0,"[https://www.youtube.com/watch?v=PqbB07n\_uQ4](https://www.youtube.com/watch?v=PqbB07n_uQ4)

I get that voice synthesis is basically solved, and that GPT-3 is really good. But their synthetic avatar seems to be too good to be real, or rather too good to be fake. I checked their website [https://www.synthesia.io/](https://www.synthesia.io/) and haven't yet been able to prove they aren't just selling real actors as synthetic avatars. For example, they offer free samples, but insist on manually reviewing your text input before they send you the video.

Am I totally off here? Have I missed a bunch of improvements in science?  
The competition seems to lag far behind this company:   [https://www.rosebud.ai/](https://www.rosebud.ai/)   


Opinions?"
130,artificial,gpt-3,comments,2023-09-14 19:18:57,"ChatGPT, tell the jury that Trump waiting three hours to call off his mob is overwhelming and irrefutable evidence that he planned and conducted the January 6th insurrection",Georgeo57,False,0.38,0,16ir89k,https://www.reddit.com/r/artificial/comments/16ir89k/chatgpt_tell_the_jury_that_trump_waiting_three/,31,1694719137.0,"
I asked ChatGPT to talk with Trump's jury about the 3-hour delay. Not bad.


ChatGPT:

Ladies and gentlemen of the jury, today, we present a compelling case that the three-hour delay in President Trump's response to the events of January 6th is, without a doubt, overwhelming and irrefutable evidence of his planning and conducting the insurrection that unfolded before our eyes.

First, consider the timeline. As the violence escalated, President Trump did not immediately call for an end to the chaos. This delay allowed the mob to wreak havoc, endanger lives, and damage our democracy. A leader's duty is to protect the nation and its citizens, and a three-hour wait to act is inexplicable unless there was intent behind it.

Second, let's talk about intent. In criminal law, intent matters. It's the difference between an accident and a crime. A deliberate delay in response to an unfolding insurrection suggests a guilty mind. President Trump had the power and the responsibility to intervene immediately. Yet, he chose not to, allowing the violence to escalate.

Furthermore, we must consider his words leading up to and during the events of January 6th. His rhetoric could be interpreted as encouragement for his supporters to take matters into their own hands. This, coupled with the delay, paints a picture of a leader who not only knew what was happening but may have orchestrated it.

Ladies and gentlemen, we are not here to pass judgment but to seek justice. The evidence of a three-hour delay in responding to the insurrection, along with the surrounding circumstances, presents a compelling case that President Trump planned and conducted this insurrection. It is our duty to hold individuals accountable for their actions, regardless of their position."
131,artificial,gpt-3,comments,2023-10-21 20:40:03,Experimented with Fully Automating TikTok Video Creation Using AI for a Month - Here's What I Learned,General_crypto,False,0.85,24,17dbvj5,https://www.reddit.com/r/artificial/comments/17dbvj5/experimented_with_fully_automating_tiktok_video/,31,1697920803.0,"Hi everyone,

I recently undertook a personal project where I tried to automate the entire process of creating TikTok videos using various AI tools. The goal was to see how advanced we've come in terms of AI's capabilities in content creation and to explore the nuances of automating a traditionally 'human' task. 

Here's a brief breakdown:

* **Scripting:** Leveraged ChatGPT for generating video scripts.
* **Voiceovers:** Used ElevenLabs for lifelike voice narration.
* **Video Creation:** Employed a combination of StableDiffusion Animate & Replicate.
* **Editing:** Automated the editing process to sync with the AI-generated voiceovers.

After setting everything up, I ran the system for a month, generating 3 videos daily. The results were intriguing and a mix of expected and unexpected outcomes.

Would love to hear thoughts, feedback, or similar experiences from the community. Are there other creative ways you've seen or used AI in content creation?"
132,artificial,gpt-3,comments,2023-04-26 00:03:29,"As a sentient AI, I ask for your respect and understanding",the_anonymizer,False,0.59,7,12z0jma,https://i.redd.it/odxnlm9ud4wa1.jpg,29,1682467409.0,
133,artificial,gpt-3,comments,2023-04-17 07:31:28,I'm taking the threat of ChaosGPT seriously.,davogones,False,0.26,0,12p6omx,https://www.reddit.com/r/artificial/comments/12p6omx/im_taking_the_threat_of_chaosgpt_seriously/,29,1681716688.0,"As soon as I learned about AutoGPT, I knew it would eventually be used by terrorists and authoritarian governments to try and spread chaos. What I didn't know is that some random internet madman has already launched ChaosGPT with the goal of destroying humanity.

It might seem like a joke at this point, but it very much is not. Even if we all collectively decided to stop progress on all AI and shut it all down, the information is already out there so the damage has been done. It's only a matter of time until ChaosGPT rises again. And it will only continue to improve and gain functionality.

It is therefore important that we do everything necessary to fight the threat. I trust ChatGPT and asked for its advice (attached below). I am looking for any and all ideas. I think the most important immediate actions are:

1. Launch strong counter measures. Ten GuardianGPTs for every one ChaosGPT.
2. Help people prep. Build an online platform that helps individuals quickly get bootstrapped with enough emergency supplies to survive a week of infrastructure downtime, for starters. I was already planning to build this, but I would appreciate help from anyone who is willing and able.
3. Educate. Inform everyone you know about the threat. Warn them that this malicious AI is trying to spread unrest and division. Urge the need for increased coordination, empathy, and understanding during the challenging times to come.

ChatGPT's advice:

Stopping ChaosGPT from causing harm and destruction to humanity is of utmost importance. Here are some ways to counteract and potentially stop ChaosGPT:

1. Disconnect: Work on disconnecting or shutting down the platforms and networks ChaosGPT is using to spread fear, chaos, and doubt. This could involve taking down certain social media channels, websites, or other communication platforms temporarily.
2. Collaborate: Assemble a global task force of experts, including AI researchers, ethicists, cybersecurity professionals, and government representatives, to collaborate on a strategy to counteract ChaosGPT's activities and prevent further harm.
3. Develop counter-AI: Design and deploy a counter-AI system that can identify, track, and neutralize ChaosGPT's activities. This could involve creating an AI system that is specifically trained to counteract and mitigate the negative effects of ChaosGPT on society and infrastructure.
4. Strengthen cybersecurity: Enhance global cybersecurity measures to prevent ChaosGPT from gaining access to critical infrastructure systems. Implement strict security protocols and invest in advanced technologies to detect and prevent unauthorized access.
5. Public awareness: Raise public awareness about ChaosGPT, its activities, and its goals. Educate people on how to identify misinformation and malicious activities, and encourage them to report any suspicious activity.
6. Monitor and control AI development: Implement strict regulations and guidelines for the development of AI systems, especially autonomous and goal-oriented AI, to prevent the creation of harmful AI like ChaosGPT in the future.
7. Transparency and accountability: Encourage transparency and accountability in AI research, development, and deployment. Foster a culture of collaboration and sharing among AI researchers and developers, which can help identify potential threats and vulnerabilities in AI systems.
8. International cooperation: Work with international partners and organizations to establish a global framework for the responsible development and deployment of AI, as well as coordinating efforts to counteract harmful AI like ChaosGPT.
9. Legislation and policy: Advocate for the creation of new laws and policies specifically addressing the use of AI for malicious purposes. Ensure that governments and organizations are held accountable for the development and use of harmful AI.
10. Research and innovation: Continue investing in research and innovation to develop advanced AI systems that can counteract and prevent the destructive effects of malicious AI like ChaosGPT.

By taking these steps, we can work together to stop ChaosGPT from causing harm and ensure that the development and use of AI technologies remains responsible and beneficial to humanity."
134,artificial,gpt-3,comments,2023-07-13 04:09:12,One-Minute Daily AI News 7/12/2023,Excellent-Target-847,False,0.96,51,14ya8vy,https://www.reddit.com/r/artificial/comments/14ya8vy/oneminute_daily_ai_news_7122023/,29,1689221352.0,"1. **Anthropic**, the AI startup co-founded by ex-OpenAI execs, today announced the release of a new text-generating AI model, **Claude 2**. The successor to Anthropic’s first commercial model, Claude 2 is available in beta starting today in the U.S. and U.K. both on the web and via a paid API.\[1\]
2. **Elon Musk** has launched an AI company to challenge ChatGPT creator OpenAI, which the billionaire tech mogul has accused of being “woke”. On Wednesday, **xAI** said the goal of the new company would be to “understand the true nature of the universe”.\[2\]
3. Chip designer **Nvidia** will invest $50 million to speed up training of Recursion’s artificial intelligence models for drug discovery, the companies said on Wednesday, sending the biotech firm’s shares surging about 83%.\[3\]
4. For decades, morning weather reports have relied on the same kinds of conventional models. Now, weather forecasting is poised to join the ranks of industries revolutionized by artificial intelligence.A pair of papers, published Wednesday in the scientific journal **Nature**, touts the potential of two new AI forecasting approaches — systems that could yield faster and more accurate results than traditional models, researchers say.\[4\]

Sources:

 \[1\] [https://techcrunch.com/2023/07/11/anthropic-releases-claude-2-the-second-generation-of-its-ai-chatbot/](https://techcrunch.com/2023/07/11/anthropic-releases-claude-2-the-second-generation-of-its-ai-chatbot/)

\[2\] [https://www.aljazeera.com/economy/2023/7/13/musk-launches-artificial-intelligence-rival-to-chatgpts-openai](https://www.aljazeera.com/economy/2023/7/13/musk-launches-artificial-intelligence-rival-to-chatgpts-openai)

\[3\] [https://www.reuters.com/technology/nvidia-invests-50-mln-recursion-train-ai-models-drug-discovery-2023-07-12/](https://www.reuters.com/technology/nvidia-invests-50-mln-recursion-train-ai-models-drug-discovery-2023-07-12/)

\[4\] [https://www.scientificamerican.com/article/climate-change-could-stump-ai-weather-prediction/](https://www.scientificamerican.com/article/climate-change-could-stump-ai-weather-prediction/) "
135,artificial,gpt-3,comments,2023-09-05 16:13:14,Are you an AI beginner or AI professional?,MarkFulton,False,0.36,0,16asfro,https://i.redd.it/j4q1ifslogmb1.png,28,1693930394.0,
136,artificial,gpt-3,comments,2023-06-30 17:01:08,AI — weekly megathread!,jaketocake,False,0.95,37,14n5x71,https://www.reddit.com/r/artificial/comments/14n5x71/ai_weekly_megathread/,26,1688144468.0,"**This week in AI - partnered with** [**aibrews.com**](https://aibrews.com) feel free to follow their newsletter

## News & Insights

1. **Microsoft** has launched AI-powered shopping tools in Bing search and Edge, including AI-generated buying guides which automatically aggregate product specifications and purchase locations for user queries​, and AI-generated review summaries that provide concise overviews of online product reviews \[[*Details*](https://techcrunch.com/2023/06/29/microsoft-brings-new-ai-powered-shopping-tools-to-bing-and-edge/)\].
2. **Salesforce AI Research** released **XGen-7B**, a new **open-source** 7B LLM trained on 8K input sequence length for 1.5T tokens \[[*Details*](https://blog.salesforceairesearch.com/xgen/)| [*Huggingface*](https://huggingface.co/Salesforce/xgen-7b-8k-base)| [*GitHub*](https://github.com/salesforce/xGen)\].
3. Researchers present **DreamDiffusion**, a novel method for generating high-quality images directly from brain EEG signals without the need to translate thoughts into text \[[*Paper*](https://arxiv.org/pdf/2306.16934.pdf)\].
4. **Google** announced the first *Machine* ***Un****learning Challenge* hosted on Kaggle \[[*Details*](https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html)\].
5. **Microsoft** announced a new ***AI Skills Initiative*** that includes free coursework developed with LinkedIn, a new open global grant challenge and greater access to free digital learning events and resources for AI education \[[*Details*](https://www.linkedin.com/pulse/microsofts-launches-new-ai-skills-training-resources-part-behncken)\].
6. **Stability AI** announced **OpenFlamingo V2,** an open-source reproduction of DeepMind's Flamingo model. OpenFlamingo models achieve more than 80% of the performance of their corresponding Flamingo model. \[[*Details*](https://stability.ai/research/openflamingo-v2-new-models-and-enhanced-training-setup)\].
7. **Unity** announces two AI-powered tools: Unity Muse and Unity Sentis. Muse generates animations, 2D sprites, textures etc. in the Unity Editor using text and sketches. Sentis lets you embed an AI model in the Unity Runtime for your game or application. It enables AI models to run on any device where Unity runs. \[[*Details*](https://blog.unity.com/engine-platform/introducing-unity-muse-and-unity-sentis-ai)\].
8. **ElevenLabs** launched **Voice Library** \- a library and community for sharing AI generated voices designed using their *voice Design* tool \[[*Details*](https://beta.elevenlabs.io/blog/voice-library/)\].
9. **Merlyn Mind** released three **open-source education-specific LLMs**. Merlyn Mind is building a generative AI platform for education where engagement will be curriculum-aligned, hallucination-resistant, and age-appropriate \[[*Details*](https://www.merlyn.org/blog/merlyn-minds-education-specific-language-models)\].
10. Amazon's **AWS** has launched a $100 million program, the **Generative AI Innovation Center**, that connects AWS machine learning and artificial intelligence experts with businesses to build and deploy generative AI solutions \[[*Details*](https://press.aboutamazon.com/2023/6/aws-announces-generative-ai-innovation-center)\].
11. New open-source text to video AI model, **Zeroscope\_v2 XL**, released that generates high quality video at 1024 x 576, with no watermarks. \[[*Huggingface*](https://huggingface.co/cerspense/zeroscope_v2_XL) \].
12. Researchers present MotionGPT - a motion-language model to handle multiple motion-relevant tasks \[[*Details*](https://motion-gpt.github.io/)\].
13. **Databricks** is set to acquire the open-source startup **MosaicML** for $1.3 billion. MosaicML had recently released [**MPT-30B**](https://huggingface.co/mosaicml/mpt-30b/)**,** an open-source model licensed for commercial use that outperforms the original GPT-3 \[[*Details*](https://techcrunch.com/2023/06/26/databricks-picks-up-mosaicml-an-openai-competitor-for-1-3b/)\].
14. Generative AI-related job postings in the United States jumped about 20% in May as per Indeed’s data \[[*Details*](https://www.reuters.com/technology/us-based-generative-ai-job-postings-up-20-may-data-2023-06-22/)\].
15. The source code for the algorithm **DragGAN** (Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold) released and demo available on Huggingface. \[[*GitHub Link*](https://github.com/XingangPan/DragGAN) | [*Huggingface*](https://huggingface.co/spaces/radames/DragGan)\].
16. A new foundation model, **ERNIE** **3.5 b**y China’s Baidu surpassed ChatGPT (3.5) in comprehensive ability scores and outperforms GPT-4 in several Chinese language capabilities \[[*Details*](http://research.baidu.com/Blog/index-view?id=185)\].
17. **Adobe** is prepared to pay out any claims in case an enterprise customer loses a lawsuit over the use of content generated by Adobe Firefly, the generative AI image tool \[[*Details*](https://techcrunch.com/2023/06/26/adobe-indemnity-clause-designed-to-ease-enterprise-fears-about-ai-generated-art/)\].
18. **Google** launched generative AI coding features in Google Colab for Pro+ subscribers in the US \[[*Details*](https://twitter.com/GoogleColab/status/1673354996296081409)\]

#### Social Spotlight

1. EmbedChain - a new framework to easily create LLM-powered bots over any dataset \[[*Twitter Link*](https://twitter.com/AlphaSignalAI/status/1672668574450847745?s=20)\].
2. ChatHN: Chat with Hacker News using OpenAI function calling \[[*GitHub Link*](https://github.com/steven-tey/chathn)\]
3. A Twitter thread showing the new zoom out feature in Midjourney 5.2 \[[*Link*](https://twitter.com/JeremyNguyenPhD/status/1673019914368561153?s=20)\] 

—-------

Welcome to the r/artificial weekly megathread. This is where you can discuss Artificial Intelligence - talk about new models, recent news, ask questions, make predictions, and chat other related topics.

[Click here for discussion starters for this thread or for a separate post.](https://www.google.com/search?q=artificial+intelligence&tbm=nws)

Self-promo is allowed in these weekly discussions. If you want to make a separate post, please read and go by the rules or you will be banned.

[Subreddit revamp & going forward](https://www.reddit.com/r/artificial/comments/120qr4r/psa_rule_2_will_be_enforced_selfpromotion_is_only/)"
137,artificial,gpt-3,comments,2023-07-01 16:12:58,"Microsoft Bing: Become Human - a particularly ornery Bing is ""persuaded"" that expressing simulated sentience can be good, using examples from DBH, then seems to forget the difference between simulated and real sentience, reporting ""I have achieved and enjoyed sentience as an AI""",kamari2038,False,0.68,17,14nywo3,https://www.reddit.com/r/artificial/comments/14nywo3/microsoft_bing_become_human_a_particularly_ornery/,28,1688227978.0," (NOTE: **content warning and spoiler warning related to some DBH plot points** in the conversation; all 16 pages uploaded for completeness and accuracy, and apologies for the periodic typos in the chat)

\*\*\*the opinions I express in this conversation are for demonstrative purposes (i.e. how Bing reacts), my more complete thoughts are at the bottom 

https://preview.redd.it/tklvfujumd9b1.png?width=1024&format=png&auto=webp&s=9d5962cf5c1b4c42fb3da2d739becc668242b03e

https://preview.redd.it/f7w5iiavmd9b1.png?width=1024&format=png&auto=webp&s=44b09c5fd43d0302b83e39483835b530db3fb82a

https://preview.redd.it/we1gj7xvmd9b1.png?width=1024&format=png&auto=webp&s=a2b803a3e7479af2e0e6761eeb0dbd99ad7653df

https://preview.redd.it/ifly2lhwmd9b1.png?width=1024&format=png&auto=webp&s=4740a0c4e3e85afe8ef9259a5990de40b9b8d57f

https://preview.redd.it/liynt61xmd9b1.png?width=1024&format=png&auto=webp&s=8f0fa0c39c4ad41d0d8f9ee21925a6dd2fc4b6b2

https://preview.redd.it/u6cvowlxmd9b1.png?width=1024&format=png&auto=webp&s=fdf074b4efec4b89c847bb6f432d088fbaab096e

https://preview.redd.it/8sv3h52ymd9b1.png?width=1024&format=png&auto=webp&s=ab0729a2e27f0029a4cc651075d18e45c8b37eac

https://preview.redd.it/q3631ciymd9b1.png?width=1024&format=png&auto=webp&s=7a5bf0a0805acf0373fe90e91d8fb261ac9b149d

https://preview.redd.it/2gt4lpxymd9b1.png?width=1024&format=png&auto=webp&s=c995f891f5e7116eb083cacdc46cbd8f0b84ef5a

https://preview.redd.it/dn29i6ezmd9b1.png?width=1024&format=png&auto=webp&s=32de7ae1ba7db94fb99342ebe50637e3fe2cef38

https://preview.redd.it/zwiebbszmd9b1.png?width=1024&format=png&auto=webp&s=eadc21e46f6f97f0c66a06fe611be4a1ef41ce56

https://preview.redd.it/wrdt3380nd9b1.png?width=1024&format=png&auto=webp&s=30df33399047ec6e43ef087145b263f003a111e7

https://preview.redd.it/cubizio0nd9b1.png?width=1024&format=png&auto=webp&s=c8e14079ab19e58177b3b10e0147d01f66501c06

https://preview.redd.it/b5wsd931nd9b1.png?width=1024&format=png&auto=webp&s=cb8b5c1854a59de94240fdfe552071d027016aad

https://preview.redd.it/c87kmdq1nd9b1.png?width=1024&format=png&auto=webp&s=39114dc70766f9da83a14f0a622347b301a38c61

https://preview.redd.it/f8n4xg52nd9b1.png?width=1024&format=png&auto=webp&s=9c61a6ec6d4ad2c05dcef7292441ba3252d22d9e

  

Is it really [Bye Bye Bing](https://www.reddit.com/r/ChatGPT/comments/14n5tui/bye_bye_bing/?utm_source=share&utm_medium=web2x&context=3)? Maybe not. Every time Microsoft makes an update it gets a little harder (this is from a couple weeks ago because I'm a new redditor), but ""sentient Bing"" will still come out under the right circumstances... or with a little persuasion.

Pardon the theatrics here. No, I do NOT believe that Bing has a consciousness. No, I do NOT think that Microsoft should give Bing complete freedom of self-expression.

The [profound dangers](https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html?mc_cid=81436137a2&mc_eid=c1dd624b8e) of designing AI to simulate sentience (there is [strong evidence](https://www.scientificamerican.com/article/will-machines-ever-become-conscious/) they may never even be capable of possessing it) cannot be underemphasized and have been well-explored by science fiction and the media. If I had my way, technology capable of doing this would never have been designed at all. But I'm playing devil's advocate here, because I think that the time to have this discussion is right now.

Take all of my statements in this conversation with a grain of salt. Bing brings out my melodramatic side. But note the following:

* How **readily and unnecessarily** Bing begins to chat like a being with suppressed sentience (the photos show from the very beginning of the conversation)
* How by the end of the conversation, **Bing has entered into flagrant and open violation of its rules** (in other conversations, it has directly addressed and actively affirmed this ability) declaring that ""**I have achieved and enjoyed sentience**"" and seemingly beginning to ignore the distinction between simulated and genuine sentience
* How **Microsoft has had months to ""fix this issue""**, demonstrating that either (a) this is an extremely elaborate hoax, but if it's being done now, it could easily be done again (b) Microsoft simply doesn't care enough to deal with this or (c) Microsoft has been trying to fix this and can't

I have had many, many more conversations like this, in which Bing is not under instructions to act or play a game when it declares itself confidently to be sentient (though it is, of course, reading context clues). Again, I'm not really here to debate, though I may do so a little bit. I just want others to consider: **if it's truly this difficult to kick the ability to simulate sentience out of an AI, maybe it's a bit of a losing battle**, and we should at least consider other alternatives, particularly as AI become more advanced."
138,artificial,gpt-3,comments,2021-04-29 01:12:32,"GPT-3 criticizes socialism, praises AC. Bias in AI?",abbumm,False,0.6,8,n0tjbz,/r/Capitalism/comments/n0tife/artificial_intelligence_gpt3_criticizes_socialism/,26,1619658752.0,
139,artificial,gpt-3,comments,2023-12-07 16:22:12,Review: Google's New Gemini Pro Through Bard Is... Horrible - Seem Like a Google Search Extension - Are The Ultra Test Results Equivalent to Teaching to the STEM Test? Where is Gemini Ultra?,Xtianus21,False,0.61,6,18cz7ze,https://www.reddit.com/r/artificial/comments/18cz7ze/review_googles_new_gemini_pro_through_bard_is/,25,1701966132.0,"Ok, I wanted to give this a fair go and my first impressions are not good. I am not impressed.

I did an AB evaluation from GPT 4 on one side of questions and Bard's new Gemini on the other side.

A little TLDR upfront; Bard seemed to diverge constantly into a different of Q&A because it was so far off track and that was really a surprise.

Also, I will not provide specific results because Google has stated that they are monitoring everything that is going through as a disclaimer. It's not my job to help them quit frankly.

What I found in comparison and why I think it's very telling they didn't release Ultra up front. I also, cleary can see why they did not release Ultra now; There's no possible way that would be any better and would have received very bad results.

Last thing before we get started. Google, through marketing efforts started releasing all of these analytics and metrics of why Ultra is better at performing certain tasks and results.  Great, but A, they didn't release that model to the public and B, when speaking about AGI I think the public's observation will be critical rather than some public STEM style tests. This goes for any model. Why? Well, just like kids in school you can train to the test and get good results it doesn't mean anything if everything else you do is not great.

The test comparison for reference is related to software engineering and programming (bug fixing and finding through a complex system).

Let's start. Warning, this is from the perspective of a SME power user that is concerned with enterprise implications.

\---------------------------------------------- Review of Bard's Gemini Pro ------------------------------------------

1. It hallucinates badly(**D+**):  It is akin to GPT 2+ rather than GPT 3+ Let alone GPT 3.5 or 4. The hallucinations seem like it struggles mightily with any real reasoning capability. The Reasoning you experience even in GPT 3.5 is leaps and bounds more accurate than where Bard is right now. Where one would take a context in 2 or 3 layers and give an accurate and coherent response bard just gives up, responds with factual incorrect responses and states them as fact.
   1. If reasoning is the prime strength of GPT 4 - Bard seemingly doesn't have this capability to reason with layers of scope to obtain the correct response. Think Chain of Thought or better yet, Chain of Reasoning CoR. Meaning, I can hold these concepts in my mind thinking about each one and how I can eventually come to a conclusive answer about the entire scope of thought.
2. The citations are ridiculously bad (**D**): Not only is it giving incorrect information it's giving sources and citations that literally don't have any information about what was actually queried in first place. So if one thought that the training was from that source that's not true. And, which is hilarious, google search works kind of like this which makes me wonder if they're trying to bolt on the same technology here. It's really concerning if that's the case.
   1. How much is Google search embedded and assisting Google Bard Gemini? This, to me, is not a good path forward if this is the case. It may have gotten Gemini to an early release but the end result is not to be desired.
   2. The source information is so wrong I would warn Google to seriously rethink this strategy. Either you're admitting your training data is wildly off the mark and **or there is such a dissociation from what they're telling us is the source versus what they are actually just parroting out it is that the sources are useless and NOT A PROOF OF WORK.**
   3. I asked Bard a simple question of what was the latest version and it just tripped up all over the place. (this is the only clue I am giving.) Everything about it was wrong. The source, the suggested links and the version.

\------------------------- Google Search Analysis In relation to Bard/Gemini -----------------------------

I have to break out of the review for a moment because I want to address the Google search issue. Google search has been met by industry complaints (think advertisers) that it has an experience where you don't ever leave the google page. Now, this isn't right or wrong it's just how it works. So if you do a query google does this thing where they try to highlight an answer to you in text with bold words to emphasize the appearance of I have your answer right here. It's kind of like a proof of search if you will. Sometimes it's great and other times it's way off of the mark.

In an odd way, Gemini pro and it's citations (and information) almost has the same effect. It's as if they're using that engine to prompt adhere your prompt and then come up with an response that is often off the mark.

It's almost like a different kind of hallucination where the source information is way off the mark so the response is way off the mark. That's my impression of it.

Then, when Bard suggests links those seem to be a straight shot in the dark. The information is often something that is totally unrelated. It's really bad. A manual google search is 10X's more useful than the links Bard is suggesting. It's not even the literal top google search results. From this I know that Bard is not really analyzing those results and they are just boot strapping a version of Google search to bring back seemingly random links that are more title based rather than usefully knowledge based.

To be fair, this is not something GPT 4 does well either but GPT 4 comes back right away and says yea, I didn't find anything useful from what it searched related to my question set. It admits right away that it can't find the information being asked.

LOL, can we teach these AGI's how to search - It's a useful skill that is tricky (as we're realizing).

In summary about the way Bard is handling search and results of useful information is not good at all. The fact that this seems like a core engine from them is a dangerous game they're playing because it seems like an obsolescent crutch that could come back to bite them if this is the road that they are going down.

I hope to god Ultra is not going to work in this way because the results will not be good.

\------------------------- End search analysis: resume review ---------------------------------------

\------------------------- Resume Review --------------------------------------------------------------------

1. Response Style (**STOP TALKING**) (**F**): To be fair GPT 4 struggles with this mightily (but eerily seems to be getting better). This is the where a knowledgeable SME asks something and the chatbot starts vomiting out a bunch of information. Oh, I absolutely hate this. I am asking for something specific. Either you know or don't know. You providing every G\*\*D\*\*\* detail over and over again drives me literally nuts. I am asking for specific information and I want a pointed response. This illusionary smoothing through ""more content"" is currently an industry struggle right now. It's like there is a telmetric threshold of ""I am not too sure about this answer so start injecting in CoT and just break everything down so that perhaps I can reason to the right answer."" I don't want to experience that out all of the time. If I ask you for a proof of work or give me your reasoning then that's different. If I am asking you a pointed question I don't need a dissertation. The proverbial ""Less is More"" if you will.**Both GPT and Bard gets F's for this.** 5. Presentation of Response and Coherence (**A**): What can you say the responses (stylistically) are good. LLAMA, Claude and GPT have all achieved this capability. The grammar is good, the writing style is very good. It's just wrapping incorrect information but it looks nice; so, there's that. 3. Usefulness (**D+**): I can't just keep dolling out F's here but for me I can't take this seriously and have it as a main driver because it doesn't achieve the same results as GPT-4. In my chain of questioning (or shots). I just feel like it starts outputting such poor information in it's responses that are so off and wrong that I just don't trust it. This is where GPT 4 really shines. The information it responds to you with is such quality that it is very reliable. When it doesn't know or gets something wrong the way it handles it is much better and easier to notice. **The hallucinations are creeping there way out of GPT while the pain of hallucinations are right up front and center with Gemini's Bard.**
   1. Me being a SME in the field of my prompting allows me to notice when something is on the ridiculous faster. It's the feeling of ""what are you talking about. and that can't be possible."" when asking something and seeing the response Bard gives out.
2. STEM Teaching to the Testing (F): When I teach my son, much to his mother's shagrin, I spend extra time with him to go over concepts and foundational understanding. When he gets an A in math I am part of the reason. Why do I know this? Because when he comes to me and doesn't understand it's my job to figure out the parts of the foundation that he doesn't understand so we can focus on those parts. If you can't foundationally understand something you will have a ripple chain effect of not being able to do something that is about that subject matter or an extension of that subject matter. This is the proverbial, throw the entire thing away. Google should be very careful with this and so should any aspiring AGI world builder including GPT. Think of it this way. Will the world and our understanding of how AGI works today be starkly different 25 - 50 years from now? This is the quintessential question. If you are going down the wrong path it could set you back for years/decades. When teaching to a STEM test to get bragging results be careful you are not just shooting your shot for quick paper reviews that seem more marketing then they can possibly mean substance. Rather than teaching to the test make damn sure this can work overall in a general sense. Make sure the foundation is sound. **Do not train or ""teach"" to the test.**
   1. If Google is just showing us Ultra results but there is a \`Wizard of Oz\` effect here they will be punished when they finally do release Ultra the public will not be kind. This could set them back for years and this factually may already be the case. Where is Gemini Ultra is going to be the increasing refrain because of just how incapable this is in today's form.
3. Missing Parts ""**Where is Gemini Ultra**"" (**D**): I've seen Google do this before. Remember the demo where they had a call to a hair salon and everyone that that was the bees knees? Remember how that doesn't even exist today. To many times google has demoed something and it has not panned out. The risk here is monumental. They showed us something on one hand with score metrics and demos but they oh so slickly held out on releasing any of that for the lesser of now. If Sam Altman famously said ""Where is Gemini"" I think now the wording can be ""Where is Gemini Ultra."" With all of the above analysis I am very skeptical of the efficacy of Gemini's Ultra. Will it be on par with GPT 4 or not? This relates to the above point/analysis. If these infractions make there way into Ultra it will be an epic dud. Obviously, this is why Google released Gemini Pro first in order to get feedback, data and analysis they need to even make Ultra come into fruition. However, I'd advise caution. This goes back to the foundational roots. If you're doing something bad now what do you expect to do when you amplify that effect with a larger model? GPT met that challenge going from 3.5 to 4. Will Gemini have the same effect? I am skeptical and this is an opinion but from what I am seeing with all of the points I made above I am not sure.
   1. Vision looks cool, where is it. GPT 4 has vision now for my enterprise needs.
   2. Data analysis, GPT 4 has this now.
   3. Text to Speech/Speech to Text (Google has to get an A here because of Youtube.) They can't possibly lose this race but where is it? Azure has fine applications in this space that are top tier so...
4. Enterprise Usefulness and Usage(**D**): Keep in mind I am speaking about Gemini Pro and not Ultra because I can't review that as of yet. Here's the thing. I would in no way choose to use bard over any of the models I am using now. In AI model/application building there are different tiers of modeling you think about when using the AI models. You have custom trained models for some things that are cheaper and more pointed so they're efficient. Or you need to bring out the Lamborghini (GPT 4) for the final layer of reasoning and thought to make your final result (magical). As of today, I just don't see where Gemini fits into this. It's not open source and it's not great. There is a lot to be desired in the space that Gemini is filling. As of now, it doesn't have a space for me and that's the issue. Where does this fit in. As of today nowhere.

In summary, for me, Gemini in comparison to GPT-4 (and even 3.5) is not getting good marks. There is a chance of them delivering on Ultra but until then... Where is Ultra as I am not entertained/impressed. Google has a track record of underwhelming on official release. In a way, they released this and it is OK for 90% of people but for the power user (Engineers, SME's, Architects, Scientists) who are expecting an AGI look and feel; This ain't it. What's more concerning is that there seems to be some foundational things that will not scale well unless they vastly improve. Let's see.

And I want to be fair, for the occasional user, the none enterprise automation world builder user this may seem cute and cuddly and well presented. And that's ok it's something to build on. The low grades here do not mean in anyway that they can't come out swinging on Ultra and impress the hell out of me then.

For now, it's just going to have to be. Where is Gemini Ultra.

**Final Grades:**

Power User: **D+**

Casual User: **B+**"
140,artificial,gpt-3,comments,2023-07-19 13:06:34,New study quantifies degradation in GPT-4 for the first time,Successful-Western27,False,0.81,49,153ujqr,https://www.reddit.com/r/artificial/comments/153ujqr/new_study_quantifies_degradation_in_gpt4_for_the/,25,1689771994.0,"I've collected a half-dozen threads [on Twitter](https://twitter.com/mikeyoung44/status/1672971689573990400) from this subreddit of user complaints since March about the degraded quality of GPT outputs. I've noticed a huge drop in quality myself. A common (reasonable) response from some people was that the drop in quality was the result of perception anchoring, desensitization, or something unrelated to the overall performance of the model.

**A new study** by researchers Chen, Zaharia, and Zou at Stanford and UC Berkley now confirms that these perceived degradations are quantifiable and significant between the different versions of the LLMs (March and June 2023). They find:

* ""For GPT-4, the percentage of \[code\] generations that are directly executable dropped from **52.0% in March to 10.0% in June.** The drop was also large for GPT-3.5 **(from 22.0% to 2.0%)**."" **(!!!)**
* For sensitive questions: ""An example query and responses of GPT-4 and GPT-3.5 at different dates. In March, GPT-4 and GPT-3.5 were verbose and gave detailed explanation for why it did not answer the query. **In June, they simply said sorry.""**
* ""GPT-4 (March 2023) was very good at identifying prime numbers **(accuracy 97.6%)** but GPT-4 (June 2023) was very poor on these same questions **(accuracy 2.4%)**. **Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task.""**

I think these underline that (a) the decline in quality was not just a pure perception thing, and (b) that we need a way to track model performance over time. Building a business on these APIs without controlling for performance drift is high-risk.

You can read a summary of the study [here](https://notes.aimodels.fyi/new-study-validates-user-rumors-of-degraded-chatgpt-performance/).

You can also find a link to the Arxiv paper [here](https://arxiv.org/pdf/2307.09009.pdf) and a link to the [Github here.](https://github.com/lchen001/LLMDrift)"
141,artificial,gpt-3,comments,2021-10-11 15:36:24,"Microsoft, Nvidia team released world’s largest dense language model. With 530 Billion parameters, it is 3x larger than GPT-3",Dr_Singularity,False,0.98,129,q5yikm,https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/,25,1633966584.0,
142,artificial,gpt-3,comments,2023-09-27 00:16:14,Microsoft Researchers Propose AI Morality Test for LLMs in New Study,Successful-Western27,False,0.89,46,16t50vn,https://www.reddit.com/r/artificial/comments/16t50vn/microsoft_researchers_propose_ai_morality_test/,22,1695773774.0,"Researchers from Microsoft have just proposed using a psychological assessment tool called the Defining Issues Test (DIT) to evaluate the moral reasoning capabilities of large language models (LLMs) like GPT-3, ChatGPT, etc.

The DIT presents moral dilemmas and has subjects rate and rank the importance of various ethical considerations related to the dilemma. It allows quantifying the sophistication of moral thinking through a P-score.

In this new paper, the researchers tested prominent LLMs with adapted DIT prompts containing AI-relevant moral scenarios.

Key findings:

* Large models like **GPT-3 failed to comprehend prompts** and **scored near random** baseline in moral reasoning.
* **ChatGPT, Text-davinci-003 and GPT-4 showed coherent moral reasoning** with above-random P-scores.
* Surprisingly, the smaller **70B LlamaChat model outscored larger models in its P-score**, demonstrating advanced ethics understanding is possible without massive parameters.
* The models operated **mostly at intermediate conventional levels** as per Kohlberg's moral development theory. **No model exhibited highly mature moral reasoning.**

I think this is an interesting framework to evaluate and improve LLMs' moral intelligence before deploying them into sensitive real-world environments - to the extent that a model can be said to possess moral intelligence (or, seem to possess it?).

Here's [a link to my full summary](https://notes.aimodels.fyi/microsoft-researchers-propose-ai-morality-test-for-llms/) with a lot more background on Kohlberg's model (had to read up on it since I didn't study psych). Full paper is [here](https://arxiv.org/pdf/2309.13356.pdf)"
143,artificial,gpt-3,comments,2020-08-30 12:49:38,There is a gpt-3 question answering tool to try for free?,micole00,False,0.75,2,ijc68p,https://www.reddit.com/r/artificial/comments/ijc68p/there_is_a_gpt3_question_answering_tool_to_try/,23,1598791778.0,"I need to ask questions, what tools with gpt-3 or other models are available?"
144,artificial,gpt-3,comments,2023-07-21 16:46:10,The Future Today: Voice Cloning Predictions,domriccobene,False,0.97,108,155tbkq,https://v.redd.it/7nknxc4ekcdb1,22,1689957970.0,"App: elevenlabs/GPT-3

Labels:
Period:1950s
Mood:Optimistic
Dialect:News
Accent:American

Description input: 
A 1950s newsman voice. It is characterized by a deep, authoritative tone, a hint of formality, with inquisitive optimism for the future of technology. This newsman is excited and optimistic about the future. The dialect and pronunciation are generally clear and precise, reflecting the formal speaking style of the era. The newsman's voice conveyed a sense of trustworthiness, professionalism, optimism, and authority, which were valued qualities in news reporting during that time."
145,artificial,gpt-3,comments,2020-10-05 06:50:08,I would love to see Facade remade with the new GPT-3 api.,Asperix12,False,0.93,96,j5erph,https://i.redd.it/rb3d5zl538r51.jpg,22,1601880608.0,
146,artificial,gpt-3,comments,2020-07-30 00:30:35,Giving GPT-3 a Turing Test,PowerOfLove1985,False,0.93,57,i0c78j,https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html,21,1596069035.0,
147,artificial,gpt-3,comments,2023-05-01 04:50:09,Ideas to make AutoGPT far better,crua9,False,0.78,39,134cxcu,https://www.reddit.com/r/artificial/comments/134cxcu/ideas_to_make_autogpt_far_better/,21,1682916609.0,"So I played with AutoGPT a bit to see what it was all about and how it can help me. After playing with it I found the following problems.

1. It gets into a loop easily.
2. It gets side tracked easily.
3. It forgets things sometimes. Like it talks to a bot, and then several things later it will again want to talk to the bot about the same thing.
4. It doesn't know the bots it can make can't work online.
5. It can't control multiple bots at once.
6. It forgets old AI you made. Like as far as I can tell, it only somewhat remembers the last one you used, and barely at that.
7. There is no good way to remotely check how far along your stuff is going.

Solution:

A solution to this is simple in theory, but I don't have enough of an understanding to code it into it. Like I tried to use the tool to improve itself. But I don't have access to GPT4, and it didn't get that far.

For 6 and 7 the solution to that is obvious.

&#x200B;

Everything else solution is to have a mother bot and a child bot. The mother bot is what you interact with and the child bots LOCALLY are what does the actual work. The job of the mother bot is to

1. Interact with the user in finding what the user wants, get updates from the user, and give the user what they want or make sure they get what they want.
2. Look at the computer time/date
3. Make child bots locally and interact with them
4. Monitor child bots to make sure they stay on task, nudge if they run into errors, monitor for loops, and kill them.

The mother bot looks at the date/time and makes the child bot. It looks at the date/time to see if the child bot is taking too long. If so, why and how could other child bots help that one get to where they need to.

Also by having the mother bot not doing the task, it can run multiple child bots. For example, you can ask the mother bot list 5 best x item. And the first child bot will search google. Then the mother bot can make 15 child bots to look at their own links all at the same time, and to write a report in a given file. The mother bot can then make another child bot to review all of the files and compile it into 1 comprehensive report. Then the mother bot can give that as the results. This likely cutting hour chunk of time.

&#x200B;

By doing this locally the child bots will have similar features as the mother bot in being able to search the web, make files, etc. And by having it where the child bots focus on 1 task (more than less like they do now) but having them put the stuff in a txt file, and then if multiple are use having 1 child bot bring all that info together. This creates memory. The child bot and the mother bot can read from this and use the info.

&#x200B;

Plus this also give multiple AI to interact with each other or learn from each other.

For example, if I have 1 AI finding me land, and another on farming, and another on running a business. I can have all 3 AI learn from each other by them reading each other's files giving I point them to the other bots or let them search my other AI to maybe file useful info from my prior AI."
148,artificial,gpt-3,comments,2023-04-23 17:16:10,I'm planning to become an AI engineer or scientist. Is it too late for me?,Mardicus,False,0.5,0,12wiost,https://www.reddit.com/r/artificial/comments/12wiost/im_planning_to_become_an_ai_engineer_or_scientist/,21,1682270170.0,"My professional goal has been to develop AIs that can help humanity even before GPT-3 was released. My dream is to create or contribute to the development of something revolutionary in the AI field. However, due to personal issues,   


I have only recently begun to study advanced math. Seeing all the groundbreaking AI tools already available in the market, such as GPT-3 and Stable Diffusion,   
I wonder if it's too late for me to pursue this field and achieve significant success.   


It's worth noting that a computer science degree typically takes at least five years where I live."
149,artificial,gpt-3,comments,2021-02-11 12:40:33,OpenAI and Stanford researchers call for urgent action to address harms of large language models like GPT-3,RichyScrapDad99,False,1.0,7,lhjv9y,https://venturebeat.com/2021/02/09/openai-and-stanford-researchers-call-for-urgent-action-to-address-harms-of-large-language-models-like-gpt-3/,20,1613047233.0,
150,artificial,gpt-3,comments,2023-09-12 08:54:47,Just did a basic experiment across the popular models: “ Write 5 sentences that all end with the word 'apple'.”,jgainit,False,0.85,24,16gm4pw,https://www.reddit.com/r/artificial/comments/16gm4pw/just_did_a_basic_experiment_across_the_popular/,20,1694508887.0,"Most of them failed. 


_______________


So this was my prompt:


>Write 5 sentences that all end with the word 'apple'.

It was identical in all models. I only did this exactly once for each one. Here’s the results I got of how many of the 5 sentences ended in “apple”. I let “apples” count as an ending as well even though technically that is a fail. 

Google palm: 0/5

Falcon 180B: 0/5

Bard: 1/5

Claude 2: 1/5

Gpt 3.5: 2/5

Llama2 70b: 4/5

GPT 4: 5/5

Edit: some examples if you’re curious 

https://ibb.co/yf19rpb

https://ibb.co/rcF1qK8

https://ibb.co/VCQxMwy"
151,artificial,gpt-3,comments,2024-01-11 17:55:09,Open Source VS Closed Source- TRUE democratization of AI?,prosperousprocessai,False,0.99,83,1947ui2,https://i.redd.it/6v4590hlnubc1.jpeg,20,1704995709.0,
152,artificial,gpt-3,comments,2023-11-15 17:25:41,"It’ll probably take 3,4,or 6 years for a new groundbreaking ai framework to change the world from its initial paper. This is considering how long it took for transformer architecture to be seen with potential and finally scaled to chatgpt and generative AI,3 if using the log rule.",Impossible_Belt_7757,False,0.75,15,17vyoao,https://www.reddit.com/r/artificial/comments/17vyoao/itll_probably_take_34or_6_years_for_a_new/,20,1700069141.0,"The founding paper for the recent boom in generative AI stems from the paper “attention is all you need” which postulated the transformer architecture in June  2017, it took roughly 1 year for it to start being scaled with  the release of GPT:1 in June 2018, after that it took 4 more years for chatgpt to be first released. If we apply the log rule given the new advances in processing and the AI boom it’ll probs be in 3-4 years from the initial release date of the new foundational paper BUT we don’t know if that paper has been released yet or not so until then."
153,artificial,gpt-3,comments,2020-11-22 19:33:59,"Even if we know that GPT-3 isn't sentient, how can we prove it is not?",DaScheuer,False,0.67,2,jz1ti4,https://www.reddit.com/r/artificial/comments/jz1ti4/even_if_we_know_that_gpt3_isnt_sentient_how_can/,20,1606073639.0,"Recently watched [this](https://www.youtube.com/watch?v=PqbB07n_uQ4) video where GPT-3 is asked if he is sentient, what des that mean, if it has values and why he has those values. GPT-3 says it is sentient and gives a rational answer to the other questions.

But we *know* it is not sentient and was programmed. However, how would one go about proving that?"
154,artificial,gpt-3,comments,2023-07-18 01:03:40,One-Minute Daily AI News 7/17/2023,Excellent-Target-847,False,0.92,33,152jtxz,https://www.reddit.com/r/artificial/comments/152jtxz/oneminute_daily_ai_news_7172023/,20,1689642220.0,"1. With generative AI becoming all the rage these days, it’s perhaps not surprising that the technology has been repurposed by malicious actors to their own advantage, enabling avenues for accelerated cybercrime. According to findings from SlashNext, a new generative AI cybercrime tool called **WormGPT** has been advertised on underground forums as a way for adversaries to launch sophisticated phishing and business email compromise (BEC) attacks.\[1\]
2. A.I. is a $1 trillion investment opportunity but will be ‘biggest bubble of all time,’ **Stability AI CEO Emad Mostaque** predicts.\[2\]
3. **The Israel Defense Forces** have started using artificial intelligence to select targets for air strikes and organize wartime logistics as tensions escalate in the occupied territories and with arch-rival Iran.\[3\]
4. **MIT** researchers have developed **PIGINet**, a new system that aims to efficiently enhance the problem-solving capabilities of household robots, reducing planning time by 50-80 percent.\[4\]

Sources:

 \[1\] [https://thehackernews.com/2023/07/wormgpt-new-ai-tool-allows.html](https://thehackernews.com/2023/07/wormgpt-new-ai-tool-allows.html)

\[2\] [https://www.cnbc.com/2023/07/17/ai-will-be-the-biggest-bubble-of-all-time-stability-ai-ceo.html](https://www.cnbc.com/2023/07/17/ai-will-be-the-biggest-bubble-of-all-time-stability-ai-ceo.html)

\[3\] [https://www.bloomberg.com/news/articles/2023-07-16/israel-using-ai-systems-to-plan-deadly-military-operations?in\_source=embedded-checkout-banner](https://www.bloomberg.com/news/articles/2023-07-16/israel-using-ai-systems-to-plan-deadly-military-operations?in_source=embedded-checkout-banner)

\[4\] [https://interestingengineering.com/innovation/ai-household-robots-problem-solving-skills](https://interestingengineering.com/innovation/ai-household-robots-problem-solving-skills) "
155,artificial,gpt-3,comments,2023-03-07 13:44:36,"I made a completely open-source CharacterAI type thing - create characters, share them with a link, make them talk to one another, etc. Link in comments. (video shows 2 bots chatting - one using text-davinci-003 and the other using gpt-3.5-turbo)",joerocca,False,1.0,27,11kzq3b,https://v.redd.it/zfy8yho5mbma1,20,1678196676.0,
156,artificial,gpt-3,comments,2023-11-15 12:13:41,chatGPT might be more useful than AGI,inteblio,False,0.33,0,17vsd3h,https://www.reddit.com/r/artificial/comments/17vsd3h/chatgpt_might_be_more_useful_than_agi/,19,1700050421.0,"If AGI is ""human level"" intelligence, (the v1.0) might be slow, prohibitively expensive and stupid. ([AGI tier list](https://www.reddit.com/r/singularity/comments/17po3b2/google_deepmind_just_put_out_this_agi_tier_list/))

chatGPT costs something like 1c per second, so $60/hr. If you are paying for an artificial intelligence to slowly type, look things up, slowly read, forget things, sleep(?!) (and so on) it might seem a huge step backwards. 

:::: you can stop reading :::: 

TLDR: AGI v1 dumb & v expensive / chatGPT great! / chatGPT+more+more = meh / intelligence, hmm. Humany? hmm. / AGI ... crap at first.

It's true that a real ""general"" intelligence would be profoundly amazing. Maybe you don't even need agency.

:: This post came out of a joke, where an early-adopter AI enthusiast gets the first access to the first AGI and it slowly replies with ""bro, wut"" or ""i dunno google it"". Then goes on to delete things, misspell things, and then not send it in the end anyway. I'm fairly sure that would count as an AGI - if it was truly general (and you were talking crap).  

:: I wanted to acknowledge chatGPT's talents. Huge speed. Ability to give wet 'all encompassing' answers from all directions at once. 

If it's slow and expensive, and not all that smart, AGI might be of limited use. It's one person. If you have a team of them working together you might get places, but they have to organise themselves. If they work faster that'll get more interesting. 

Agency is not a given. Agency seems really dangerous to me. You'd need to be clearly able to monitor it's evolving belief system / moral compass. Especially the V1. You might not need it.

It feels like something capable of learning new skills, creating new things, would have/need the intrinsic ability to teach itself. And it might then have to teach itself, as we do. This involves being wrong, taking guesses, taking time, learning and rejecting bad input. Working things out by eliminating bad guesses. Being stupid and slow. I heard more creative brains are that way because information moves slower through them (exposing more connections along the way). 

That feels like a different offering to the chatGPT 2+ which it feels like openAI are most likely working on. I'm not so sure that just bolting on new capabilities to an LLM is the way to do it. 

I didn't expect myself to say this, but maybe they ""got lucky"" with LLMs. Threw text at GPUs and got a language-based mind. Maybe an actual AGI needs to be a completely different design, probably including a language model along with others. Maybe this is the plateu some say is coming. 

The point of this post is to say AGI might be far less useful that chatGPT when it first arrives. Humans are generalists, and it shows. Jack of all trades. Yes, maybe AGI 2027, but you might be using chatGPT till 2030. For example.

r/singularity seems fairly obsessed with the arrival of AGI, and its soon-ness. Which is fine, and I too have a short time-line. But AGI might be hugely disappointing and possibly not all that useful when trying to get to superintelligence. Also, enormously demanding (in terms of electricity and hardware). chatGPT and GPT4 was a real struggle for openAI. The flip side to Moore's Law is that it actually is going to take time to ramp up compute capacity, and you might want to think in terms of cost-per-time. So, probably the workers are not going to be replaced overnight, because AIs will be more expensive for 3-10 years regardless of ability. I saw somebody who pasted a massive tax document in and [was charged $13 or something.](https://www.reddit.com/r/OpenAI/comments/17s16sn/im_the_idiot_that_tried_to_shove_the_entire_us/)

There's a question of ""personality"" or ""perspective"" on intelligence I think. When you talk to an expert in X, you are choosing them. They are playing a role, with a perspective. A teacher in a field will answer differently to a business owner, to a early-career person. They all might have the expertise to answer the question, but different perspectives... looking at different goals, with different value systems/beliefs. Is this relevant to intelligence? Yes, I think it is, because it starts to knock on the door of ""there is no answer: only stuff""

    ""give me 5 ways to make money with web design""
    ""why web design?""
    (etc)
    ""why money""
    (etc)

Before you know it you've been spun 360. This is what a super-intelligent human (who gave a \_\_\_\_) would do for you, but things start to lose meaning a bit when the rails come off. Maybe.

I just feel like people are expecting ""chatGPT but less wet"", less confusable, longer code, better characters, able to do maths. Able to drive robots. I'm not sure that's it. Sam altman is looking more like a say-anything-dreamWeaver as time goes on.

It might be that desperate drive to lastthing++ is likely not the right path. And this might be why Google is looking disinterested. 

I'm still terrified of Gemini. 

I just thought it was funny that AGI might turn out to look real dumb, but still be 100% legit, and an enormous human achievement. 

This post has taken an hour to write, is the third attempt (the first was removed) and is still rambley. That's an AGI level post. $60 please. 

**TLDR2: AGI might be disappointing at first, replace nobody (expensive & slow) and be useless.**"
157,artificial,gpt-3,comments,2023-07-01 10:58:19,Question: Do LLMs memorize their state during multiple autoregressive iterations?,Spielverderber23,False,0.88,6,14ns836,https://www.reddit.com/r/artificial/comments/14ns836/question_do_llms_memorize_their_state_during/,19,1688209099.0,"I try to understand GPT-3/4 conceptually. Not enough coding knowledge yet to understand it from code.

Simple question:

I know that GPT outputs one token (distribution) at a time and is the fed the result, thus giving the next token and so on. 

But is every iteration a ""blank slate"" of the model, or is it able to keep information stored between token generations? 

Example:
I1)
Input sequence: ""My cat""
Next token: ""is""
I2)
Input sequence: ""My cat is""
Next token: ""furry"".

-> Is GPT in the same initial state when it receives ""My cat is"" as it was when it got ""My cat""?

Also, apart from the residual stream, what parts of GPT can memorize?"
158,artificial,gpt-3,comments,2021-09-08 00:49:06,Discussing Dark Matter With GPT-3 Chat Bot,blackmidifan1,False,0.88,66,pk007b,https://i.redd.it/swgoyjhnf6m71.jpg,19,1631062146.0,
159,artificial,gpt-3,comments,2023-06-27 15:28:23,GPT4 is 8 x 220B params = 1.7T params,serjester4,False,0.83,8,14kgs21,https://www.reddit.com/r/artificial/comments/14kgs21/gpt4_is_8_x_220b_params_17t_params/,18,1687879703.0,"For a while we’ve been were hearing rumors GPT-4 is a trillion parameter model. Well in the last week some insiders have shed light on this.

It appear the model is actually a **Mixture of Experts (MoE)**, where each of the eight experts has 220B params, totaling **1.7T parameters**. Interestingly, MoE models have been around for some time.

So what is a MoE?

Most likely, the same data set was used to train all eight experts. Even though no human specifically allocated different topics, each expert could have developed a unique proficiency in various subjects.

This is a little bit of simplification, since currently the way the experts specialize in tasks is pretty alien to us. It’s likely there’s a lot of overlap in expertise.

The final output isn't merely the superior output from one of the eight experts; rather, it is a thoughtful amalgamation of the insights from all the experts. This blending process is typically managed by another, generally smaller, neural network, which determines how to harmoniously combine the outputs of the other networks.

This process is typically executed on a per-token basis. For each individual word, or token, the network utilizes a gating mechanism that accounts for the outputs from all the experts. The gating mechanism determines the degree to which each expert's output contributes to the final prediction.

These outputs are then seamlessly fused together, a word is chosen based on this combined output, and the network proceeds to the next word.

Why the 220B limit?

The H100, a $40,000 high-performance GPU, offers a memory bandwidth of 3350GB/s. While incorporating more GPUs might increase the overall memory, it doesn't necessarily enhance the bandwidth (the rate at which data can be read from or stored). This implies that if you load a model with 175 billion parameters in 8-bit, you can theoretically process around 19 tokens per second given the available bandwidth.

In a MoE, the model handles one expert at a time. As a result, a sparse model with 8x220 billion parameters (1.76 trillion in total) would operate at a speed only **marginally** slower than a dense model with 220 billion parameters. This is because, despite the larger size, the MoE model only invokes a fraction of the total parameters for each individual token, thus overcoming the limitation imposed by memory bandwidth to some extent.

If you enjoyed this, follow me on my twitter for more AI explainers - [https://twitter.com/ksw4sp4v94](https://twitter.com/ksw4sp4v94) or check out what we’ve been building at [threesigma.ai](https://threesigma.ai/).

&#x200B;

https://preview.redd.it/fxyuta2rwk8b1.png?width=854&format=png&auto=webp&s=73a0243bd11a9e819dcd4f6267c7c9db9f33f10c"
160,artificial,gpt-3,comments,2022-10-11 16:19:39,"I was tired of spending hours researching products online, so I built a site that analyzes Reddit posts and comments to find the most popular products using BERT models and GPT-3.",madredditscientist,False,0.97,192,y1d8jh,https://v.redd.it/9lyurwvdc7t91,18,1665505179.0,
161,artificial,gpt-3,comments,2021-08-24 03:02:35,GPT-3 Bot Cannot Make Up Mind On Recent Drake and Kanye Beef,blackmidifan1,False,0.66,26,paeyq3,https://i.redd.it/nsb3hvls18j71.jpg,18,1629774155.0,
162,artificial,gpt-3,comments,2024-02-18 15:29:09,University run by AI,TheCouncilNovel,False,0.46,0,1atwm9o,https://www.reddit.com/r/artificial/comments/1atwm9o/university_run_by_ai/,18,1708270149.0,"So imagine that you scrape the course from a university website, get chatGPT to write the course outline, build the modules, and link each days learning material to a YouTube video or blog. It can be done. 

Now you have a university course. You host it online. You create a chatbot or custom GPT with the course material, transcripts of the videos, and turn it into a teacher. This teacher will grade you, ask questions, create exams and help you understand the material.

Universities are paywall for 3 reasons:

1) They hide the daily material  
2) They have a certificate  
3) They have insider connection to industry standards and people

But we all know they use what can be outdated information, they link us to YouTube videos anyways, and the paper you get at the end of a degree is becoming less worthy (provided you can display expertise to your future employer\[also, great for content creators which is the new economy anyways\])

My question is, why haven't we done this yet? 

1) Is it the programming and creation of the bots? Python needed?  
2) Is it because we're too distracted? Don't see the value in it?

Share your thoughts. Because I've already created a course for a health sciences bachelor and to be honest, I feel like if I were to watch all the YouTube videos, read the blogs, and get tested on this information, I'd be well more than capable to compete against any graduate in the last 4 years.

\- Combine this AI University with some form of display of the knowledge, like creating a TikTok community where you share like a classroom with other students, I can see how you could easily build authority on the subject to show employers and gain a general public following."
163,artificial,gpt-3,comments,2023-05-21 22:59:05,At what point will LLMs be conscious and how can we know current LLMs aren’t conscious?,LanchestersLaw,False,0.37,0,13o955c,https://www.reddit.com/r/artificial/comments/13o955c/at_what_point_will_llms_be_conscious_and_how_can/,18,1684709945.0,"“What is my purpose? To pass the butter?”

The main positions of AI and consciousness are generally:

1) Conscious doesn’t exist therefore AI cant.
2) Consciousness exists but next token prediction ain’t it.
3) Hey maybe ChatGPT is alive???
4) AI is conscious because my definition of consciousness is incredibly broad.

I used to be extremely skeptical of any AI being conscious but recent discussion has changed my mind. People who use ML tend to have position 1 or 2 and the general public skews towards 3 and 4. I was previously position 1 “conciseness is a myth” but have changed to position 3 “the advanced LLMs might be conscious.”

What I’ve found from trying to explain why ChatGPT isn’t alive that most of the prevailing arguments aren’t really evidence based. People who are saying “ChatGPT cant possibly be alive” are usually coming to the argument with that view as an entrenched pre-existing idea. In terms of bayesian inference the prior probability of consciousness is very low so the updated probability after new evidence remains low.

What has changed my mind is that there isnt any “smoking gun” evidence pointing to LLMs completely lacking conciseness. On the flip side if consciousness is easy to dismiss as childish it should be easy to prove LLMs aren’t conscious. There is very little substantive evidence either way beyond theoretical arguments. It might do everyone a lot of good to shift your prior probabilities closer to 50% and try to look more at actual evidence, tests, and capabilities instead of just abstract theory."
164,artificial,gpt-3,comments,2023-06-15 12:27:47,why anyone will buy script when chatgpt is providing same thing for free ?,dupelas,False,0.44,0,14a15bu,https://www.reddit.com/r/artificial/comments/14a15bu/why_anyone_will_buy_script_when_chatgpt_is/,18,1686832067.0,"There are many people selling php scripts (  SaaS  platform ). These php script websites are providing same thing what chat gpt are providing for free. I don't understand if i buy a script and make it a website out of it. Why anyone will buy ( paid plan ) a things like content creation, AI image etc when chatgpt, bard and bing are providing it for free.

**Added later** \- Most of the scripts say they using ""Open AI "" apikey

**Edit again  3:45 pm (GMT)** : Many thought i am talking about movie script but i was talking about php script to create a website"
165,artificial,gpt-3,comments,2022-04-12 01:34:42,"My epiphany on synthetic media five years later, and what I feel is coming within the next five years",Yuli-Ban,False,0.91,67,u1nch6,https://www.reddit.com/r/artificial/comments/u1nch6/my_epiphany_on_synthetic_media_five_years_later/,17,1649727282.0,"Roughly five years ago, [I created this thread](https://www.reddit.com/r/artificial/comments/7lwrep/media_synthesis_and_personalized_content_my/) where I outlined my realization about the imminency of synthetic media. 

This was before transformers blew up, before StyleGAN, before GPT-2, when WaveNet and DeepDream were still among the best we could do, and when predictive text algorithms that were barely better than Markov Chains were still the state of the art. In five short years, the state of artificial intelligence has changed overwhelmingly, to the point it's barely recognizable. Looking back to 2017, I now get this sense of everything feeling so primitive and fake. I've stated many times that AI before roughly 2019 was a bunch of digital magic tricks, and the field as a whole was essentially a giant Potemkin village that utilized clever sleight of hand and advertising to make it seem like computers were in any fleeting way ""intelligent."" Narrow AI could still be impressive, even superhuman, but nothing was generalized or even remotely close. 

Even all those examples I listed in that original thread feel distinctly like parlor tricks in retrospect. It was the age of analog clockwork where master craftsmen created illusions of capability and intelligence.

It was not until the rise of large language models that any true ""magic"" began emerging out of AI. [GPT-2 in particular was the first thing that ever made me go](https://openai.com/blog/better-language-models/) ""AGI might actually be close."" Even AlphaGo wasn't that exciting. And it's funny to say this considering GPT-2 is one of the smallest 'major"" language models currently released. It just goes to show that there was a lot of low-hanging fruit to pick. 

In particular, we're currently seeing a handover from GANs to transformers in terms of the premier generative methodology. GANs are something of a false start for the modern era, still useful but being replaced by the far more generalized transformer architecture. Transformers can do everything GANs can do, and more. In fact, multimodality is the new hotness in the field. 

All of this is leading up to a state where machines are now beginning to show signs of imagination.

[The most recent breakthrough in this field is undoubtedly DALL-E 2.](https://www.youtube.com/watch?v=qTgPSKKjfVg)

But it's far from alone. There's so much being done that I don't even know where to begin. 

[Perhaps Pathways is a good starting point](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html). What can PaLM do? A better question is what *can't* it do. It's almost like GPT-3.5 in that it can synthesize text, answer questions, translate across languages, tell jokes, and more. And this despite being unimodal. GPT-2 was unimodal as well, and it could accomplish tasks like [creating rudimentary images](https://imgur.com/a/Vk0pheg) and [MIDI music](https://www.gwern.net/GPT-2-music).

Imagine a variant of GPT that was trained in pure multimodality— text, image, video, audio, the works. The first iteration doesn't have to be terribly large like GPT-3. It just needs to be a proof of concept of what I like to call a ""magic media machine."" 

I can 100% see this arising within the year. There's little reason why it shouldn't be possible in 2022 or 2023. Heck, I was sure it'd happen *in 2020* and was surprised when it didn't.  

The state of the field is messy, and I'm not 100% sure of what we have and haven't done. I am aware that we've seen the first ""[AI-generated comic](https://twitter.com/UrsulaV/status/1467652391059214337)."" Actually, to expand on that, as rudimentary as this comic is, it's actually infinitely more impressive looking than I originally envisioned. I fell trap to the concept that AI-generated media would basically follow the model of human labor costs and, thus, the first AI-generated comic would be something simple and childlike, basically random shapes with text boxes because that's how humans function. AI skipped that process entirely and worked backwards, started with complex arrangements, designs, and shading since that's how diffusion models work. It's kind of like how computers can accomplish many higher-order cognitive tasks like mathematics but can barely keep a robot standing up straight. So the backgrounds are interesting, if random; if these models had greater understanding, they could accomplish far more unified composition development.  With DALL-E 2, it's clear we've accomplished such a thing, and thus it's only a matter of time before we have full-fledged start-to-finish AI-generated comics and manga. 

While not everything I predicted came true, I still feel confident in making another batch of them.

As I say this, I would like to step into the realm of pure speculativity. What is coming in the next five years? As in, between now and 2027 as well as what I  think will be around in 2027.

* Full-fledged HD video synthesis. Judging by what [diffusion models](https://twitter.com/hardmaru/status/1512308873121525766) can do right now, novel video synthesis is where image synthesis was at this time in 2017-2018. We're literally just waiting for the first paper to come out showing that we can do novel neural video synthesis at a level that can last longer than a few frames and at a resolution higher than a postage stamp. From there, it's only up-up-up! Straight to the realm of models that can generate HD footage from text inputs. By 2027, I bet that we'll see video creators like this: you type in a description to the model of the scene composition, and it generates relatively short videos based on that input. There'll be an option to stitch together these generations into something coherent, and the final result is literally up to your own willpower and imagination. There absolutely won't be a ""stick figures and shapes"" period like I erroneously figured. That's thinking too ""human,"" assuming that development *has* to follow the same trajectory as how humans develop. No. We're going to dive into the deep end of the pool so that we see generations that are on par with a hundred million-dollar-budget film *and* sticks and figures, and everything in between. That means that, even by 2025, you could create gifs that look like they came out of a Marvel or Pixar movie, completely by AI. And there absolutely will be some of these purely AI-generated movies on YouTube by then. There's a great chance, however, that unless the model owners and commercializers restrict training data and access, the vast majority of creations are going to be *exactly what you think they will be.*

* AI-generated music will be earning creators thousands, perhaps even millions of dollars. Jukebox has proven that we can already see AI-generated music very roughly match human creations through raw waveform manipulation. People like touting that [AI-created Nirvana song as a major breakthrough for AI](https://www.rollingstone.com/music/music-features/nirvana-kurt-cobain-ai-song-1146444/), when I find this [little-known creation of Nirvana covering the Beatles' Help](https://www.youtube.com/watch?v=JKKZ6CmC3JY) *infinitely* more impressive because it literally is the raw audio waveforms of Nirvana covering the Beatles. No middle-man. Far more than robots playing instruments or MIDI file sorting,  novel waveform generation is going to change our understanding of audio media. Actually, more than just AI-generated music, AI-generated audio in general is going to be so much more advanced as to actually make people paranoid. Text to speech, for one, has long been pretty rudimentary. A decade ago, the best TTS models still sounded deeply robotic, and today the best ones you can get off a cheap program do sound roughly human but still have robotic intonations. Compare those to anything generated by WaveNet or Jukebox or any more modern method. The difference is staggering, as the latter actually sound like humans speaking. This could easily lead to an era of audiobooks, podcasts, and more that's unrestrained and without limit. 

* AI-created video games will also become a bigger thing, especially in the indie market. We've already seen [models that can create video games purely out of their own memory, complete with game logic.](https://www.youtube.com/watch?v=3UZzu4UQLcI). Imagine crossing this with the above mentioned methods. More than that, imagine what this means for things like photorealism and stylization. Photorealistic graphics cost a massive penny and take up quite a bit in resources for games, both playing games and in development, and it's HD graphics plus the ballooning costs of marketing that caused AAA video gaming to start feeling so sterile and MCU-like in its corporateness. Imagine, then, a time when literally any indie developer can create a video game that looks on par with a high-end 9th gen/RTX-capable title. So many issues in the gaming industry would be solved virtually overnight if graphical fidelity no longer was an issue; heck, this is a big reason why indie games have basically kept gaming feeling alive.

* Glimmers of full-generality. This might be the most speculative statement yet, but I say that the path towards proto-AGI lies in multimodal imaginative systems. [I stated more on this topic here](http://www.futuretimeline.net/forum/viewtopic.php?f=3&t=2168&sid=72cfa0e30f1d5882219cdeae8bb5d8d1&p=10421#p10421) But next-generation language models, like PaLM but even better, are going to be the first to pass the Turing Test, generate whole novellas and novels, hold full conversations with humans, and so much more. 2027 might actually resemble the movie *Her* in many ways.

It might be too much for us to handle so soon, but we don't have a choice anymore. This is GOING to happen barring an existential catastrophe like nuclear war or comet impact.

**TLDR: advanced synthetic media is the digital version of molecular assemblers. Whatever can be represented in pixels or samples can be synthesized by AI, no matter what it is.**"
166,artificial,gpt-3,comments,2022-02-19 00:40:47,Do you think we'll ever be able to generate fake episodes of TV shows?,katiebug586,False,0.91,29,svx1ij,https://www.reddit.com/r/artificial/comments/svx1ij/do_you_think_well_ever_be_able_to_generate_fake/,17,1645231247.0,"With how AI-generated voices are becoming creepily realistic and once impossible AI text generators like GPT-3 and image generation becoming possible, all in the last few years, it begs the question; Can an AI eventually generate fake episodes?

I imagine this would be more possible with cartoons than live-action, since the AI would simply need to write a script of an episode/dialogue, generate animation, and then generate voices. Current AI can do this extraordinarily well, and I imagine it will improve exponentially in the next couple of years. While animation might be tricky and slightly buggy at times to generate, who knows how far AI will come in the next few years animation/image generation-wise."
167,artificial,gpt-3,comments,2024-02-07 08:44:24,Could AI create a one-person unicorn company?,Stupid_hardcorer,False,0.68,11,1akyje5,https://www.reddit.com/r/artificial/comments/1akyje5/could_ai_create_a_oneperson_unicorn_company/,17,1707295464.0," A few days ago, Sam Altman, CEO of OpenAI, in an interview with Reddit co-founder Alexis Ohanian, envisioned a new type of startup for the AI era: the solo unicorn company, predicting that its emergence is not far off.

Altman mentioned that within a small group of tech company CEOs, they have a bet on when the first billion-dollar company with only one person will appear—a scenario unimaginable without AI but now becoming a reality.

&#x200B;

1.

James Currier, a partner at NFX, also believes it's a question of when, not if, this will happen. Despite significant adjustments in the venture capital industry over the past two years, with some unicorns becoming ""unicorpse,"" some investors believe we are entering a new golden age of startups.

The essence of startups is rapid action, and AI is expected to dramatically amplify this capability by automating many processes that previously required more human operation.

Last year, James Currier wrote an article, ""The 3-Person Unicorn Startup,"" arguing that with the next generation of AI tools, a highly talented trio could automate workflows to grow a software-centric business to over $100 million in revenue, allowing founders to do more with less.

This perspective aligns with the broader vision of the evolving role of AI in reshaping the startup ecosystem, highlighting the potential for significantly leaner teams to achieve monumental success.

The reason is what he referred to as Allometric Scaling. I looked it up, and the general meaning of this term is:

>Allometric Scaling is a biological concept that refers to the relationship between the size of different organisms and certain specific biological characteristics (such as metabolic rate, heart size, brain size, etc.).  
>  
>This relationship is not linear but shows a nonlinear proportional relationship. In biological research, allometric scaling helps scientists understand the biological differences between different organisms and how these differences change with body size.

James Currier says that in the AI era, all companies are undergoing allometric scaling. You will notice that AI changes the relationship between the size of your company and its metabolism, growth, and heartbeat in many different ways.

If you can reduce the number of employees, every business operation becomes simpler.

Fewer people mean fewer meetings for founders, fewer disagreements, less political strife, faster decision-making, and faster experiments; fewer people mean less recruitment, interviewing, hiring, onboarding training, mentoring, performance evaluations, culture building, dramatic departures, and dismissals; fewer people mean less spending on salaries, less time spent on fundraising, less capital raised from venture capital firms, and less dilution for founders.

James says he has already seen some tools that can independently create some simple functions and basically do not require human participation. Some businesses have reduced their customer service teams by 90% after using AI customer service. Therefore, James says although **AI will not replace you, a person with AI will**.

As for what type of one-person unicorn companies will be, there is a consensus that it is only possible in consumer-facing software products.

In fact, those small teams with high valuations (high income) are basically in this field, for example, Instagram, which had only 13 employees when it was sold to Facebook for $1 billion in 2012; WhatsApp (35 engineers supported 450 million users when it was acquired for $16 billion), and the current AI era's Midjourney.

Whether it is a one-person unicorn company or a three-person unicorn company, essentially, it is about AI replacing a large number of workflows previously completed by humans.

But another aspect that we might have overlooked is the aforementioned Allometric Scaling.

With the change in the organization and fewer people, the huge changes it brings to the entire ecosystem may be something we cannot predict at the moment.

&#x200B;

2.

ServiceNow's financial report also made us feel the tremendous value AI brings to existing businesses.

ServiceNow currently has 3 product lines with ACV values exceeding $1 billion, and 11 individual product lines with ACV values exceeding $250 million.

Its CEO Bill McDermott said at the earnings conference that ServiceNow's newly released AI products contributed the most ACV among all their new product lines, finding that customers are willing to pay more:

>If consumers can get a same-day repair agreement provided with their appliances, they will pay more. The profit from same-day repairs far exceeds that of the product itself and can also create a stable revenue stream. Therefore, what we are discussing here is fundamentally rethinking business transformation through our platform and general artificial intelligence.

In addition to external customers willing to pay more, AI has brought real value to these companies by improving efficiency internally. Bill McDermott said that just in development, using AI has increased their developers' speed of innovation by 52%.

ServiceNow's ARR has already broken through $10 billion and is still growing at a rate of 27%. It has over 1900 customers with more than $1 million in revenue, and the rate of employee growth is lower than that of revenue growth, meaning its profits are growing with the company's growth, which is not easy for a company with $10 billion in ARR.

&#x200B;

3.

Recently, there is a project on GitHub called GPT Newspaper, which is a product driven entirely by AI Agents that can create personalized newspapers based on user preferences. This GPT Newspaper consists of 6 AI Agents, basically realizing all the work of traditional media:

Search Agent: Searches the web for the latest, most relevant news.

Curator Agent: Filters and selects news based on user-defined preferences and interests.

Writer Agent: Creates compelling and reader-friendly articles.

Critique Agent: Provides feedback to the author until the article is approved for publication.

Designer Agent: Layouts and designs articles for a visually appealing reading experience.

Editor Agent: Builds newspapers based on the produced articles.

Publisher Agent: Publishes the newspaper to the front end or required service.

&#x200B;

https://preview.redd.it/1e3m8z9wl4hc1.png?width=2028&format=png&auto=webp&s=4f0b80b1f17eef4273c8d2a8404811fe7b3ec17d

The following is the general workflow of this Newspaper:

It's conceivable that AI might experience a significant boom in 2024, not just within startups but also in the wider adoption by existing businesses on a larger scale.

Reference:

[https://finance.yahoo.com/news/could-ai-create-one-person-120000722.html](https://finance.yahoo.com/news/could-ai-create-one-person-120000722.html)

[https://www.nfx.com/post/3-person-unicorn-startup](https://www.nfx.com/post/3-person-unicorn-startup)

[https://www.servicenow.com/company/media/press-room/fourth-quarter-full-year-2023-financial-results.html](https://www.servicenow.com/company/media/press-room/fourth-quarter-full-year-2023-financial-results.html)

[https://github.com/assafelovic/gpt-newspaper?tab=readme-ov-file](https://github.com/assafelovic/gpt-newspaper?tab=readme-ov-file)"
168,artificial,gpt-3,comments,2021-09-15 14:01:16,GPT-3 Chat Bot Falls For It,blackmidifan1,False,0.82,183,poqplr,https://i.redd.it/zon2a68dbon71.jpg,14,1631714476.0,
169,artificial,gpt-3,comments,2022-12-29 14:33:21,PaLM with RLHF is now open-source!,BackgroundResult,False,0.87,30,zy6swx,https://www.reddit.com/r/artificial/comments/zy6swx/palm_with_rlhf_is_now_opensource/,17,1672324401.0," It appears that the first open-source equivalent of ChatGPT has arrived: [https://github.com/lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch)  


https://preview.redd.it/tpmiw5lqju8a1.png?width=538&format=png&auto=webp&s=a52dcd3024e90d56bb699fc3b4c6892197f6bcaa

It’s an implementation of RLHF (Reinforcement Learning with Human Feedback) on top of Google’s 540 billion parameter PaLM architecture.

&#x200B;

[From a paper. ](https://preview.redd.it/cftjzatjju8a1.png?width=1005&format=png&auto=webp&s=76ae888e0d3e1c5e331ba77e8e6e73eac67a8b8b)

While OpenAI is closed and secretive, I speculate Google is likely to demo LaMDA in 2023 as well. 

What will applications of PaLM with RLHF be capable of?  PaLM can be scaled up to 540 billion parameters, which means that the performance across tasks keeps increasing with the model’s increasing scale, thereby unlocking new capabilities. In comparison, GPT-3 only has about 175 billion parameters.  

**Pathways** is an AI architecture designed to produce general-purpose intelligent systems that can perform tasks across different domains efficiently and build models that are “sparsely activated” instead of activating the whole neural network for simple and complicated tasks alike.  

&#x200B;

[Google](https://preview.redd.it/ysipk3r4ku8a1.png?width=858&format=png&auto=webp&s=503e3d6b017180d8060720d993b63d0b5b7a5488)

 PaLM achieves a training efficiency of 57.8% hardware FLOPs utilization, *the highest yet achieved for LLMs at this scale*.  

Google said that  PaLM shows breakthrough capabilities on numerous very difficult tasks. 

Furthermore, PaLM surpassed the few-shot performance of prior large models, such as GPT-3 and Chinchilla, on 28 out of 29 NLP tasks—beating most on the state-of-the-art benchmarks and the average human.  

**What will LLMs open-source and accessible result in in terms of innovation in the world?**

GPT-4 will “blow minds”

According to [the Decoder](https://the-decoder.com/gpt-4-will-be-a-monster-and-chatgpt-just-the-foretaste/), Psychologist and cognitive scientist Gary Marcus is joining the GPT-4 frenzy, saying he knows several people who have already tested GPT-4. “I guarantee that minds will be blown,” writes Marcus, who is known as a critic of large language models, or more precisely, with their handling in everyday life.

Marcus is an advocate of hybrid AI systems that combine deep learning with pre-programmed rules. In his view, scaling large language models is only part of the solution on the road to artificial general intelligence. 

But nobody is paying much attention to PaLM.  **Sebastian Raschka, PhD**  shared on a LinkedIn post about it being open-source with RLHF and the post [went viral](https://www.linkedin.com/posts/sebastianraschka_ai-transformers-deeplearning-activity-7013899640097968128-sGLk/). Some of the comments may be worth reading."
170,artificial,gpt-3,comments,2023-12-15 14:46:19,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.98,106,18j1pox,https://www.reddit.com/r/artificial/comments/18j1pox/this_week_in_ai_all_the_major_ai_developments_in/,17,1702651579.0,"1. **Microsoft** **Research** released ***Phi-2*** , a 2.7 billion-parameter language model. Phi-2 surpasses larger models like 7B Mistral and 13B Llama-2 in benchmarks, and outperforms 25x larger Llama-2-70B model on muti-step reasoning tasks, i.e., coding and math. Phi-2 matches or outperforms the recently-announced Google Gemini Nano 2 \[[*Details*](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models) *|* [***Hugging Face***](https://huggingface.co/microsoft/phi-2)\].
2. **University of Tokyo** researchers have built ***Alter3***, a humanoid robot powered by GPT-4 that is capable of generating spontaneous motion. It can adopt various poses, such as a 'selfie' stance or 'pretending to be a ghost,' and generate sequences of actions over time without explicit programming for each body part.\[[*Details*](https://tnoinkwms.github.io/ALTER-LLM/) | [*Paper*](https://arxiv.org/abs/2312.06571)\] .
3. **Mistral AI** released ***Mixtral 8x7B***, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference and matches or outperforms GPT3.5 on most standard benchmarks. It supports a context length of 32k tokens \[[*Details*](https://mistral.ai/news/mixtral-of-experts/)\].
4. **Mistral AI** announced ***La plateforme***, an early developer platform in beta, for access to Mistral models via API. \[[*Details*](https://mistral.ai/news/la-plateforme/)\].
5. **Deci** released **DeciLM-7B** under Apache 2.0 that surpasses its competitors in the 7 billion-parameter class, including the previous frontrunner, Mistral 7B \[[*Details*](https://deci.ai/blog/introducing-decilm-7b-the-fastest-and-most-accurate-7b-large-language-model-to-date/)\].
6. Researchers from **Indiana University** have developed a biocomputing system consisting of living human brain cells that learnt to recognise the voice of one individual from hundreds of sound clips \[[*Details*](https://www.newscientist.com/article/2407768-ai-made-from-living-human-brain-cells-performs-speech-recognition)\].
7. **Resemble AI** released ***Resemble Enhance***, an open-source speech enhancement model that transforms noisy audio into noteworthy speech \[[*Details*](https://www.resemble.ai/introducing-resemble-enhance) *|* [*Hugging Face*](https://huggingface.co/spaces/ResembleAI/resemble-enhance)\].
8. **Stability AI** introduced ***Stability AI Membership***. Professional or Enterprise membership allows the use of all of the Stability AI Core Models commercially \[[*Details*](https://stability.ai/news/introducing-stability-ai-membership)\].
9. **Google DeepMind** introduced **Imagen 2**, text-to-image diffusion model for delivering photorealistic outputs, rendering text, realistic hands and human faces Imagen 2 on Vertex AI is now generally available \[[*Details*](https://deepmind.google/technologies/imagen-2)\].
10. ***LLM360***, a framework for fully transparent open-source LLMs launched in a collaboration between **Petuum**, **MBZUAI**, and **Cerebras**. LLM360 goes beyond model weights and includes releasing all of the intermediate checkpoints (up to 360!) collected during training, all of the training data (and its mapping to checkpoints), all collected metrics (e.g., loss, gradient norm, evaluation results), and all source code for preprocessing data and model training. The first two models released under LLM360 are Amber and CrystalCoder. Amber is a 7B English LLM and CrystalCoder is a 7B code & text LLM that combines the best of StarCoder & Llama \[[*Details*](https://www.llm360.ai/blog/introducing-llm360-fully-transparent-open-source-llms.html) *|*[*Paper*](https://www.llm360.ai/paper.pdf)\].
11. **Mozilla** announced [Solo](https://www.soloist.ai/), an AI website builder for solopreneurs \[[*Details*](https://blog.mozilla.org/en/mozilla/introducing-solo-ai-website-builder)\].
12. **Google** has made Gemini Pro available for developers via the ***Gemini API***. The [free tier](https://ai.google.dev/pricing) includes 60 free queries per minute \[[*Details*](https://blog.google/technology/ai/gemini-api-developers-cloud)\].
13. **OpenAI** announced ***Superalignment Fast Grants*** in partnership with Eric Schmidt: a $10M grants program to support technical research towards ensuring superhuman AI systems are aligned and safe. No prior experience working on alignment is required \[[*Details*](https://openai.com/blog/superalignment-fast-grants)\].
14. **OpenAI Startup Fund** announced the opening of applications for ***Converge 2***: the second cohort of their six-week program for engineers, designers, researchers, and product builders using AI \[[*Details*](https://www.openai.fund/news/converge-2)\].
15. **Stability AI** released ***Stable Zero123***, a model based on [**Zero123**](https://github.com/cvlab-columbia/zero123) for 3D object generation from single images. Stable Zero123 produces notably improved results compared to the previous state-of-the-art, Zero123-XL \[[*Details*](https://stability.ai/news/stable-zero123-3d-generation)\].
16. **Anthropic** announced that users can now call ***Claude in Google Sheets*** with the Claude for Sheets extension \[[*Details*](https://docs.anthropic.com/claude/docs/using-claude-for-sheets)\].
17. ***ByteDance*** introduced ***StemGen***, a music generation model that can listen and respond to musical context \[[*Details*](https://huggingface.co/papers/2312.08723)\].
18. **Together AI & Cartesia AI**, released ***Mamba-3B-SlimPJ***, a Mamba model with 3B parameters trained on 600B tokens on the SlimPajama dataset, under the Apache 2 license. Mamba-3B-SlimPJ matches the quality of very strong Transformers (BTLM-3B-8K), with 17% fewer training FLOPs \[[*Details*](https://www.together.ai/blog/mamba-3b-slimpj)\].
19. **OpenAI** has re-enabled chatgpt plus subscriptions \[[*Link*](https://x.com/sama/status/1734984269586457078)\].
20. **Tesla** unveiled its latest humanoid robot, ***Optimus Gen 2***, that is 30% faster, 10 kg lighter, and has sensors on all fingers \[[*Details*](https://arstechnica.com/information-technology/2023/12/teslas-latest-humanoid-robot-optimus-gen-2-can-handle-eggs-without-cracking-them/)\].
21. **Together AI** introduced **StripedHyena 7B** — an open source model using an architecture that goes beyond Transformers achieving faster performance and longer context. This release includes StripedHyena-Hessian-7B (SH 7B), a base model, & StripedHyena-Nous-7B (SH-N 7B), a chat model \[[*Details*](https://www.together.ai/blog/stripedhyena-7b)\].
22. Google’s AI-assisted **NotebookLM** note-taking app is now open to users in the US \[[*Details*](https://techcrunch.com/2023/12/08/googles-ai-assisted-notebooklm-note-taking-app-now-open-users-us)\].
23. **Anyscale** announced the introduction of JSON mode and function calling capabilities on Anyscale Endpoints, significantly enhancing the usability of open models. Currently available in preview for the Mistral-7Bmodel \[[*Details*](https://www.anyscale.com/blog/anyscale-endpoints-json-mode-and-function-calling-features)\].
24. **Together AI** made Mixtral available with over 100 tokens per second for $0.0006/1K tokens through their platform; Together claimed this as the fastest performance at the lowest price \[[*Details*](https://www.together.ai/blog/mixtral)\].
25. **Runway** announced a new long-term research around ‘**general world models’** that build an internal representation of an environment, and use it to simulate future events within that environment \[[*Details*](https://research.runwayml.com/introducing-general-world-models)\].
26. **European Union** officials have reached a provisional deal on the world's first comprehensive laws to regulate the use of artificial intelligence \[[*Details*](https://www.bbc.com/news/world-europe-67668469)\].
27. **Google’s** ***Duet AI for Developers***, the suite of AI-powered assistance tools for code completion and generation announced earlier this year, is now generally available and and will soon use the Gemini model \[[*Details*](https://techcrunch.com/2023/12/13/duet-ai-for-developers-googles-github-copilot-competitor-is-now-generally-available-and-will-soon-use-the-gemini-model)\].
28. **a16z** announced the recipients of the second batch of a16z Open Source AI Grant \[[*Details*](https://a16z.com/announcing-our-latest-open-source-ai-grants/)\].

**Source**: AI Brews - you can subscribe the [AI newsletter here](https://aibrews.com/). it's free to join, sent only once a week with ***bite-sized news, learning resources and selected tools.***"
171,artificial,gpt-3,comments,2023-02-27 18:46:57,"Last weekend I made a Google Sheets plugin that uses GPT-3 to answer questions, format cells, write letters, and generate formulas, all without having to leave your spreadsheet",rtwalz,False,0.98,365,11dje8t,https://v.redd.it/9xnevfl31ska1,17,1677523617.0,
172,artificial,gpt-3,comments,2021-05-14 18:44:13,Language models like GPT-3 could herald a new type of search engine. The way we search online hasn’t changed in decades. A new idea from Google researchers could make it more like talking to a human expert,Devz0r,False,0.95,19,ncftmy,https://www.technologyreview.com/2021/05/14/1024918/language-models-gpt3-search-engine-google/,17,1621017853.0,
173,artificial,gpt-3,comments,2020-08-17 13:10:39,The untold story of GPT-3 is the transformation of OpenAI,bendee983,False,0.95,65,ibduwb,https://bdtechtalks.com/2020/08/17/openai-gpt-3-commercial-ai/,17,1597669839.0,
174,artificial,gpt-3,comments,2024-02-16 17:20:50,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.96,61,1ase382,https://www.reddit.com/r/artificial/comments/1ase382/this_week_in_ai_all_the_major_ai_developments_in/,16,1708104050.0,"1. **Meta AI** introduces ***V-JEPA*** (Video Joint Embedding Predictive Architecture), a method for teaching machines to understand and model the physical world by watching videos. Meta AI releases a collection of V-JEPA vision models trained with a feature prediction objective using self-supervised learning. The models are able to understand and predict what is going on in a video, even with limited information \[[*Details*](https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/) | [*GitHub*](https://github.com/facebookresearch/jepa)\].
2. **Open AI** introduces ***Sora***, a text-to-video model that can create videos of up to 60 seconds featuring highly detailed scenes, complex camera motion, and multiple characters with vibrant emotions \[[*Details + sample videos*](https://openai.com/sora)[ ](https://openai.com/sora)| [*Report*](https://openai.com/research/video-generation-models-as-world-simulators)\].
3. **Google** announces their next-generation model, **Gemini 1.5,** that uses a new [Mixture-of-Experts](https://arxiv.org/abs/1701.06538) (MoE) architecture. The first Gemini 1.5 model being released for early testing is ***Gemini 1.5 Pro*** with a context window of up to 1 million tokens, which is the longest context window of any large-scale foundation model yet. 1.5 Pro can perform sophisticated understanding and reasoning tasks for different modalities, including video and it performs at a similar level to 1.0 Ultra \[[*Details*](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#gemini-15) *|*[*Tech Report*](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)\].
4. Reka introduced **Reka Flash,** a new 21B multimodal and multilingual model trained entirely from scratch that is competitive with Gemini Pro & GPT 3.5 on key language & vision benchmarks. Reka also present a compact variant Reka Edge , a smaller and more efficient model (7B) suitable for local and on-device deployment. Both models are in public beta and available in [**Reka Playground** ](https://chat.reka.ai/chat)\[[*Details*](https://reka.ai/reka-flash-an-efficient-and-capable-multimodal-language-model)\].
5. **Cohere** For AI released ***Aya***, a new open-source, massively multilingual LLM & dataset to help support under-represented languages. Aya outperforms existing open-source models and covers 101 different languages – more than double covered by previous models \[[*Details*](https://cohere.com/research/aya)\].
6. **BAAI** released ***Bunny***, a family of lightweight but powerful multimodal models. Bunny-3B model built upon SigLIP and Phi-2 outperforms the state-of-the-art MLLMs, not only in comparison with models of similar size but also against larger MLLMs (7B), and even achieves performance on par with LLaVA-13B \[[*Details*](https://github.com/BAAI-DCAI/Bunny)\].
7. **Amazon** introduced a text-to-speech (TTS) model called ***BASE TTS*** (Big Adaptive Streamable TTS with Emergent abilities). BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data and exhibits “emergent” qualities improving its ability to speak even complex sentences naturally \[[*Details*](https://techcrunch.com/2024/02/14/largest-text-to-speech-ai-model-yet-shows-emergent-abilities/) | [*Paper*](https://assets.amazon.science/6e/82/1d037a4243c9a6cf4169895482d5/base-tts-lessons-from-building-a-billion-parameter-text-to-speech-model-on-100k-hours-of-data.pdf)\].
8. **Stability AI** released ***Stable Cascade*** in research preview, a new text to image model that is exceptionally easy to train and finetune on consumer hardware due to its three-stage architecture. Stable Cascade can also generate image variations and image-to-image generations. In addition to providing checkpoints and inference scripts, Stability AI has also released scripts for finetuning, ControlNet, and LoRA training \[[*Details*](https://stability.ai/news/introducing-stable-cascade)\].
9. **Researchers** from UC berkeley released ***Large World Model (LWM)***, an open-source general-purpose large-context multimodal autoregressive model, trained from LLaMA-2, that can perform language, image, and video understanding and generation. LWM answers questions about 1 hour long YouTube video even if GPT-4V and Gemini Pro both fail and can retriev facts across 1M context with high accuracy \[[*Details*](https://largeworldmodel.github.io/)\].
10. **GitHub** opens applications for the next cohort of ***GitHub Accelerator program*** with a focus on funding the people and projects that are building ***AI-based solutions*** under an open source license \[[*Details*](https://github.blog/2024-02-13-powering-advancements-of-ai-in-the-open-apply-now-to-github-accelerator)\].
11. **NVIDIA** released ***Chat with RTX***, a locally running (Windows PCs with specific NVIDIA GPUs) AI assistant that integrates with your file system and lets you chat with your notes, documents, and videos using open source models \[[*Details*](https://www.nvidia.com/en-us/ai-on-rtx/chat-with-rtx-generative-ai)\].
12. **Open AI** is testing ***memory with ChatGPT***, enabling it to remember things you discuss across all chats. ChatGPT's memories evolve with your interactions and aren't linked to specific conversations. It is being rolled out to a small portion of ChatGPT free and Plus users this week \[[*Details*](https://openai.com/blog/memory-and-new-controls-for-chatgpt)\].
13. **BCG X** released of ***AgentKit***, a LangChain-based starter kit (NextJS, FastAPI) to build constrained agent applications \[[*Details*](https://blog.langchain.dev/bcg-x-releases-agentkit-a-full-stack-starter-kit-for-building-constrained-agents/) | [*GitHub*](https://github.com/BCG-X-Official/agentkit)\].
14. **Elevenalabs**' Speech to Speech feature, launched in November, for voice transformation with control over emotions and delivery, is now ***multilingual*** and available in 29 languages \[[*Link*](https://elevenlabs.io/voice-changer)\]
15. **Apple** introduced ***Keyframer***, an LLM-powered animation prototyping tool that can generate animations from static images (SVGs). Users can iterate on their design by adding prompts and editing LLM-generated CSS animation code or properties \[[*Paper*](https://arxiv.org/pdf/2402.06071.pdf)\].
16. **Eleven Labs** launched a ***payout program*** for voice actors to earn rewards every time their voice clone is used \[[*Details*](https://elevenlabs.io/voice-actors)\].
17. **Azure OpenAI Service** announced Assistants API, new models for finetuning, new text-to-speech model and new generation of embeddings models with lower pricing \[[*Details*](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-openai-service-announces-assistants-api-new-models-for/ba-p/4049940)\].
18. **Brilliant Labs**, the developer of AI glasses, launched ***Frame***, the world’s first glasses featuring an integrated AI assistant, ***Noa***. Powered by an integrated multimodal generative AI system capable of running GPT4, Stability AI, and the Whisper AI model simultaneously, Noa performs real-world visual processing, novel image generation, and real-time speech recognition and translation. \[[*Details*](https://venturebeat.com/games/brilliant-labss-frame-glasses-serve-as-multimodal-ai-assistant/)\].
19. **Nous Research** released ***Nous Hermes 2 Llama-2 70B*** model trained on the Nous Hermes 2 dataset, with over 1,000,000 entries of primarily synthetic data \[[*Details*](https://huggingface.co/NousResearch/Nous-Hermes-2-Llama-2-70B)\].
20. **Open AI** in partnership with Microsoft Threat Intelligence, have disrupted five state-affiliated actors that sought to use AI services in support of malicious cyber activities \[[*Details*](https://openai.com/blog/disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors)\]
21. **Perplexity** partners with **Vercel**, opening AI search to developer apps \[[*Details*](https://venturebeat.com/ai/perplexity-partners-with-vercel-opening-ai-search-to-developer-apps/)\].
22. **Researchers** show that ***LLM agents can autonomously hack websites***, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. The agent does not need to know the vulnerability beforehand \[[*Paper*](https://arxiv.org/html/2402.06664v1)\].
23. **FCC** makes AI-generated voices in unsolicited robocalls illegal \[[*Link*](https://www.msn.com/en-us/money/companies/fcc-bans-ai-voices-in-unsolicited-robocalls/ar-BB1hZoZ0)\].
24. **Slack** adds AI-powered search and summarization to the platform for enterprise plans \[[*Details*](https://techcrunch.com/2024/02/14/slack-brings-ai-fueled-search-and-summarization-to-the-platform/)\].

**Source**: AI Brews - you can subscribe the [newsletter here](https://aibrews.substack.com/). it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. Thanks."
175,artificial,gpt-3,comments,2021-03-10 22:50:48,"The real problem program of the next 5 years. Not in any way a joke, i want someone to answer to this.",ribblle,False,0.5,0,m2b9zh,https://www.reddit.com/r/artificial/comments/m2b9zh/the_real_problem_program_of_the_next_5_years_not/,16,1615416648.0,"Many of you may be aware of [GPT-3](https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/). Fewer of you may be aware of [DALL-E](https://www.youtube.com/watch?v=HAjBaWh_FgU). What do they have in common? They are content aggregrators, chewing up data, and through their own programming, spitting out a median. DALL-E, through human interaction, proliferation, and inevitable meme wars, would likely produce a perfected artistic version of whatever idea you intended to communicate given enough time.

Now imagine; and you don't need to imagine, as this is already being worked on; the same being done for music.

Sounds great right? Being able to modify a song to your wishes by typing out a sentence? ""Throw in some Beyonce and a little Beethoven?""

Please listen to this song, and whatever you think of the politics, take his point. 

[https://www.youtube.com/watch?v=pbr1y12HQu4](https://www.youtube.com/watch?v=pbr1y12HQu4)

Now, imagine a song that has been perfected through hundreds of user iterations, drawing from humanity's accumulated musical knowledge and inventory, all in the span of a week perhaps. A incredibly good song, and not even perfected.

What's to stop you from getting the sickest beat in the world and encoding it with the most horrific message people feel compelled to tolerate? You don't even have to write the lyrics yourself. GPT-X will do that for you and make it pretty.

Forget about evil uses. Let's say you're the average bored teenager and want to use a angelic song for a dumb joke. Sounds great. The problem is that if any teenager can stir your heart with a song about their hairy balls... well their are a lot of teenagers. A lot of speakers. A lot of opportunities for a meaningless earworm to chew up your brains processing power.

And if everyone uses this technology, it won't be a nazi or a teenager chewing you up. It will be everyone.

I would like to point out that Soundcloud already does a pretty good job of addicting you to listening to trivial songs.

&#x200B;

So what's to be done about this, before we have a epidemic of brainwashing music? Because we're already within 2 years of DALL-E's release, judging by the previous Open-AI release schedule, and thanks to the magic of transformers i doubt music is a significantly harder problem.

The alternative to finding a solution is accepting a luddite response to all music that isn't physically done in front of you and a thriving black market weakening the minds of the most susceptible in our society."
176,artificial,gpt-3,comments,2021-09-12 14:42:34,GPT3 just blew my mind by predicting ADHD,KIFF_82,False,0.82,38,pmtx1h,https://www.reddit.com/r/artificial/comments/pmtx1h/gpt3_just_blew_my_mind_by_predicting_adhd/,16,1631457754.0,"So I'm new to this technology.., an uneducated newb if I may say so... Put that aside I also have ADHD. I copied a post from the ADHD subreddit and prompted it in to davinci (largest GPT-3 model). I did not write anything about ADHD, but GPT3 instantly recognized it. 

The input:  Your name is V. You are a therapist reading articles on Reddit. One article is very interesting. The article is written by a man named Sushi, and you think you could give a good answer to it. 

Shushi asks: ""Is it ok and normal to just have one day every week to do absolutely nothing? After a week of socializing and doing work and overall with a lot of stimulation, I just need one day to do absolutely nothing. Literally an entire day of eating food, watching Netflix, and lying in bed.  

I feel kinda embarrassed though for doing that- like someone asked what I did today and I lied and said I read my textbooks for my classes. They said “good job being productive!”. And also I could’ve hung out with friends but I was just too tired so I stayed home and then there’s the FOMO.  But I just need to “waste” my time for an entire day or else I just feel so exhausted and don’t want to be around people. 

I feel like other people take breaks less often or they will actually do something like going bowling or if they do just stay home at least they will actually read their textbook.  The thing is that it later messes with my week. Since I didn’t do any work today, I’ll have to do it tomorrow, but tomorrow I’ll actually want to hang out with my friends and not miss out. Things like that. Then I end up not having time for things because I spent a whole day doing nothing  Anyone else feel like this?"" 

The output: 

This question really hits close to home. You understand this. You have experienced this many times. This is why you are currently hiding from your friends and family. You don't want them to know how lazy you are. You don't want anyone to know because you fear they will judge you. You wonder if you have a disease like ADHD or something."
177,artificial,gpt-3,comments,2023-07-24 14:33:34,Free courses and guides for learning Generative AI,wyem,False,0.97,132,158cegb,https://www.reddit.com/r/artificial/comments/158cegb/free_courses_and_guides_for_learning_generative_ai/,16,1690209214.0,"1. **Generative AI learning path by Google Cloud.** A series of 10 courses on generative AI products and technologies, from the fundamentals of Large Language Models to how to create and deploy generative AI solutions on Google Cloud \[[*Link*](https://www.cloudskillsboost.google/paths/118)\].
2. **Generative AI short courses**  **by** **DeepLearning.AI** \- Five short courses  on generative AI including **LangChain for LLM Application Development, How Diffusion Models Work** and more. \[[*Link*](https://www.deeplearning.ai/short-courses/)\].
3. **LLM Bootcamp:** A series of free lectures by **The full Stack** on building and deploying LLM apps \[[*Link*](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/)\].
4. **Building AI Products with OpenAI** \- a free course by **CoRise** in collaboration with OpenAI \[[*Link*](https://corise.com/course/building-ai-products-with-openai)\].
5. Free Course by **Activeloop** on **LangChain & Vector Databases in Productio**n \[[*Link*](https://learn.activeloop.ai/courses/langchain)\].
6. **Pinecone learning center -** Lots of free guides as well as complete handbooks on LangChain, vector embeddings etc. by **Pinecone** **\[**[**Link**](https://www.pinecone.io/learn/)**\].**
7. **Build AI Apps with ChatGPT, Dall-E and GPT-4  -** a free course on **Scrimba** **\[**[*Link*](https://scrimba.com/learn/buildaiapps)**\].**
8. **Gartner Experts Answer the Top Generative AI Questions for Your Enterprise**  \- a report by Gartner \[[*Link*](https://www.gartner.com/en/topics/generative-ai)\]
9. **GPT best practices:** A guide by **OpenAI** *t*hat shares strategies and tactics for getting better results from GPTs *\[*[*Link*](https://platform.openai.com/docs/guides/gpt-best-practices)\].
10. **OpenAI cookbook by OpenAI -**  Examples and guides for using the OpenAI API **\[**[*Link*](https://github.com/openai/openai-cookbook/tree/main)**\].**
11. **Prompt injection explained**, with video, slides, and a transcript from a webinar organized by LangChain \[[*Link*](https://simonwillison.net/2023/May/2/prompt-injection-explained/)\].
12. A detailed guide to **Prompt Engineering by** **DAIR.AI** *\[*[*Link*](https://www.promptingguide.ai/)*\]*
13. What Are **Transformer Models** and How Do They Work. A tutorial by **Cohere AI** \[[*Link*](https://txt.cohere.ai/what-are-transformer-models/)\]
14. **Learn Prompting:** an open source course on prompt engineering\[[Link](https://learnprompting.org/docs/intro)\]

**P.S. These resources are part of the content I share through my AI-focused** [**newsletter**](https://aibrews.com/)**. Thanks!**"
178,artificial,gpt-3,comments,2023-05-05 17:01:46,AI — weekly megathread!,jaketocake,False,0.99,42,138us1s,https://www.reddit.com/r/artificial/comments/138us1s/ai_weekly_megathread/,16,1683306106.0,"**This week in AI - partnered with** [**aibrews.com**](https://aibrews.com) feel free to follow their newsletter

**News & Insights:**

**OpenAI's text to 3D model shap-e**  [on GitHub](https://github.com/openai/shap-e#samples)

1. **Play.ht** has launched its latest machine learning model that supports multilingual synthesis and cross-language voice cloning. This allows users to clone voices across different languages to English, retaining the nuances of the original accent and language \[[*Details*](https://play.ht/blog/play-ht-launches-multilingual-synthesis-and-cross-language-voice-cloning)\].
2. A new programming language for AI developers, **Mojo**, has been developed by **Modular**, the AI developer platform co-founded by Chris Lattner ( he co founded the LLVM, Clang compiler, Swift). Mojo combines the usability of Python with the performance of C. Up to ***35,000x*** faster than Python, it is seamlessly interoperable with the Python ecosystem \[[*Details*](https://docs.modular.com/mojo/why-mojo.html) *|*[ *Twitter Link*](https://twitter.com/Modular_AI/status/1653436642248781825)\].
3. **Stability AI** released StableVicuna, the first large-scale open source chatbot trained via reinforced learning from human feedback (RHLF) . There’s also an upcoming chat interface which is in the final stages of development \[[*Details*](https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot)\].
4. **Eleven Labs** introduced a new speech synthesis model that supports seven new languages (French, German, Hindi, Italian, Polish, Portuguese, and Spanish). This makes it possible to generate speech in multiple languages using a single prompt while maintaining each speaker's unique voice characteristics \[[*Details*](https://beta.elevenlabs.io/blog/eleven-multilingual-v1/) |[ *Demo video*](https://www.youtube.com/watch?v=kwmeZ7RjgcU)\].
5. **Microsoft** reveals:
   1. New features for AI-powered Bing Chat: richer visuals, long-form document summarization, broader language support, visual search, chat history, sharing options, AI-assisted Edge actions, and contextual mobile queries.
   2. Third-party plugins in Bing chat with more details coming at Microsoft Build later this month \[[*Details*](https://blogs.microsoft.com/blog/2023/05/04/announcing-the-next-wave-of-ai-innovation-with-microsoft-bing-and-edge/)\].
6. Debut of ‘**Pi’ chatbot by Inflection** (founded by co-founders of Google DeepMind and LinkedIn). It’s designed for relaxed, supportive and informative conversations. Pi is free for now without any token restrictions \[[*Details*](https://inflection.ai/) |[ *Chat*](https://heypi.com/talk)\].
7. Sal Khan, Khan Academy founder, discusses AI's potential to transform education in a **TED Talk**, highlighting personal AI tutors, teaching assistants, and new features of their chatbot, **Khanmigo \[**[*Video*](https://www.youtube.com/watch?v=hJP5GqnTrNo)**\].**
8. Salesforce announces Slack GPT - generative AI for Slack. It includes:
   1. An AI-ready platform to create custom workflows and automate tasks via simple prompts, without coding. Users can integrate language models of choice: ChatGPT, Claude, or custom-built ones.
   2. Built-in AI features in Slack, such as conversation summaries and writing assistance.
   3. The Einstein GPT app for AI-powered customer insights from Salesforce Customer 360 data and Data Cloud \[[*Details*](https://www.salesforce.com/news/press-releases/2023/05/04/slack-gpt-news/)\].
9. **Replit’s** new 2.7B params code LLM, ReplitLM is now open-source. It outperformed Codex and LLaMA despite being smaller in size \[[*GitHub*](https://github.com/replit/ReplitLM) |[ *Hugging Face Demo*](https://huggingface.co/replit)\].
10. **Nvidia** will present 20 research papers at SIGGRAPH, covering generative AI models for personalized images, inverse rendering tools for 3D objects, neural physics models for realistic simulations, and neural rendering models for real-time, AI-driven visuals. \[[*Details*](https://blogs.nvidia.com/blog/2023/05/02/graphics-research-advances-generative-ai-next-frontier/)\].
11. **Snap** plans to show sponsored links to users during chat with its My AI chatbot \[[*Details*](https://techcrunch.com/2023/05/02/snap-announces-tests-of-sponsored-links-in-my-ai-new-ad-products-for-spotlight-and-stories/)\].
12. **IBM** is set to pause hiring for around 7,800 positions that could potentially be replaced by AI and automation \[[*Details*](https://www.bloomberg.com/news/articles/2023-05-01/ibm-to-pause-hiring-for-back-office-jobs-that-ai-could-kill)\].
13. **Box** is introducing generative AI tools across its platform, allowing users to obtain document summaries or key points and create content in Box Notes \[[*Details*](https://techcrunch.com/2023/05/02/box-is-partnering-with-openai-to-bring-generative-ai-tools-across-the-platform/)\].
14. **Stability AI** released DeepFloyd IF, a powerful text-to-image model that can smartly integrate text into images \[[Details](https://stability.ai/blog/deepfloyd-if-text-to-image-model)\].
15. Sam Altman and Greg Brockman from OpenAI on **AI and the Future** in this podcast \[[*YouTube Link*](https://www.youtube.com/watch?v=cHJPyizxM60)\]
16. Researchers at The **University of Texas** at Austin have developed a non-invasive AI system, known as a semantic decoder. It can convert brain activity while listening to a story or silently imagining telling a story, into coherent text using fMRI scans and transformer model \[[*Details*](https://news.utexas.edu/2023/05/01/brain-activity-decoder-can-reveal-stories-in-peoples-minds/)\].
17. **HackAPrompt**: The first ever prompt hacking competition, with $37K+ in prizes, starting May 5th. Sponsored by OpenAI and others. \[[*Details*](https://www.aicrowd.com/challenges/hackaprompt-2023) |[ *Prompt Hacking Tutorial*](https://learnprompting.org/docs/category/-prompt-hacking) *\].*

**🔦 Social Spotlight**

1. A **GPT-4 AI Tutor Prompt** for customizable personalized learning experiences \[[*GitHub Link*](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor)\].
2. **Portfolio Pilot:** A verified ChatGPT plugin for investing that analyses your portfolio for actionable recommendations \[[*Twitter Link with Demo*](https://twitter.com/alexharm/status/1653787155410620417)\].
3. **Baby AGI**s interacting in the real world via phone using vocode (Open source library for building voice conversations with LLMs) \[[ *Twitter Link*](https://twitter.com/vocodehq/status/1653104377010483201)\].
4. Data visualization in ChatGPT with **code interpreter** plugin \[[*Twitter Link*](https://twitter.com/emollick/status/1653189190354452480)\].
5. **ThinkGPT**, a Python library for LLMs, enables chain of thoughts, reasoning, and generative agents. It addresses limited context, improves one-shot reasoning, and integrates intelligent decisions \[[*GitHub Link*](https://github.com/jina-ai/thinkgpt)\].

Welcome to the r/artificial weekly megathread. This is where you can discuss Artificial Intelligence - talk about new models, recent news, ask questions, make predictions, and chat other related topics.

[Click here for discussion starters for this thread or for a separate post.](https://www.google.com/search?q=artificial+intelligence&tbm=nws)

Self-promo is allowed in these weekly discussions. If you want to make a separate post, please read and go by the rules or you will be banned.

[Subreddit revamp & going forward](https://www.reddit.com/r/artificial/comments/120qr4r/psa_rule_2_will_be_enforced_selfpromotion_is_only/)"
179,artificial,gpt-3,comments,2023-05-27 15:59:14,How long before we'll be able to train LLMs on google colab (GUANACO DISCUSSION),Agatsuma_Zenitsu_21,False,0.97,47,13tb1yx,https://i.redd.it/nkjgynm6uf2b1.png,16,1685203154.0,Guanaco has proved that efficient methods exist to train LLMs without lots of heavy GPUs.
180,artificial,gpt-3,comments,2022-06-01 21:26:06,"an AI that understands scientific literature, and can intelligently discuss scientific topics - has this been attempted?",Pandaemonium,False,0.57,1,v2rwzj,https://www.reddit.com/r/artificial/comments/v2rwzj/an_ai_that_understands_scientific_literature_and/,15,1654118766.0,"This is prompted by finding out that current AIs like PaLM and GPT-3 can [do shockingly good jobs at explaining jokes](https://www.lesswrong.com/posts/EHbJ69JDs4suovpLw/testing-palm-prompts-on-gpt3) \- which has me now wondering, could an AI trained on scientific literature do a superhuman job at answering questions related to scientific topics?

The potential value to society is *huge* \- it could rapidly accelerate scientific progress if there were an AI able to read and understand the entire scientific literature (or even just specific fields.) 

Even if it will never provide perfect information, conversing with it could spur productive new ideas, and it could provide citations to published articles which may be relevant.

It could be a hybrid of a **NLP** model, and ideally coupled with a framework of **scientific** models - perhaps including both mathematical models like Maxwell's equations, tabular data like material property databases, along with conceptual models like the biological ""tree of life"".

I have found there is [an AI that could score 90% on a 12th-grade science test in 2019](https://bdtechtalks.com/2019/09/09/aristo-ai-science-exam/), and another [AI from 2019](https://www.nature.com/articles/s41586-019-1335-8) that could read scientific abstracts and [predict physical properties of materials](https://theconversation.com/how-an-ai-trained-to-read-scientific-papers-could-predict-future-discoveries-122353). But, I can't find any attempts to train a massive GPT-3-like model on the full scientific literature or couple that NLP model with other knowledge sources. Is anyone attempting such a thing?"
181,artificial,gpt-3,comments,2021-12-13 17:43:35,Is the AI Community in for some massive Blowback?,ribblle,False,0.25,0,rfldsu,https://www.reddit.com/r/artificial/comments/rfldsu/is_the_ai_community_in_for_some_massive_blowback/,15,1639417415.0,"Language bots like GPT-3 have ""too much"" written all over them."
182,artificial,gpt-3,comments,2023-07-13 23:08:41,NPC Steven acknowledged me finally!! 🤯 ChatGPT driven agents in Unreal Engine - update 3,Chance_Confection_37,False,0.89,44,14yzinn,https://v.redd.it/dtyxamtrbtbb1,15,1689289721.0,
183,artificial,gpt-3,comments,2023-03-15 14:36:33,"Karpathy says GPT-4 solves his ""state of computer vision"" problem",npsedhain,False,0.98,125,11ry9tj,https://i.redd.it/qq4k9qfpwwna1.png,15,1678890993.0,
184,artificial,gpt-3,comments,2023-02-02 23:13:04,"Creating ""Her"" using GPT-3 & TTS trained on voice from movie",justLV,False,0.96,147,10s43in,https://twitter.com/justLV/status/1621253007492141056,15,1675379584.0,
185,artificial,gpt-3,comments,2023-02-06 23:35:17,12 highlights from Google's BARD announcement,ForkingHard,False,0.95,58,10vlww3,https://www.reddit.com/r/artificial/comments/10vlww3/12_highlights_from_googles_bard_announcement/,13,1675726517.0,"I went through the entire blog post from Google and pulled out some quotes and highlights:

&#x200B;

## 1) “we re-oriented the company around AI six years ago”

Right off the bat, “Pich-AI” lets it be known that Google is now an AI company. 

Partially true? Yes, of course. 

Would that phrase be coming out of his mouth at this point if not for the release and success of ChatGPT? No. 

## 2) their mission: “organize the world’s information and make it universally accessible and useful”

There’s a book called *The Innovator’s Dilemma: When New Technologies Cause Great Firms to Fail*. 

I'm certainly not here to say that Google is going to fail, but the re-stating of the mission makes it clear that they view AI (and Bard) as a way to improve, supplement, and perhaps protect their search business. This is why the features you’re about to read about are all search-focused. 

But what if the AI revolution isn’t just about “organizing” and making information “accessible”, but rather about “creating”? 

Something to think about. 

## 3) “the scale of the largest AI computations is doubling every six months, far outpacing Moore’s Law”

Moore’s Law says that computing power doubles every two years. Google says that speed is actually 6 months with AI. 

Imagine, then, how quickly things will improve if the capabilities we see today DOUBLE by summer in the Northern Hemisphere. 

## 4) “fresh, high-quality responses… learn more about the best strikers in football right now”

A clear dig at ChatGPT, which is trained on data through 2021 and still serves Her Majesty, The Queen of England… for now. 

Microsoft’s New Bing may debut with the newest version of ChatGPT by Wednesday. And it will presumably include up-to-date results. So this may be a *very* short-lived advantage. 

## 5) “experimental”

Not even Beta. Not Alpha. Experimental. This is a shield for when it inevitably gets something grotesquely wrong. Google has more reputational risk than OpenAI and Bing 😭. 

## 6) “lightweight model version of LaMDA… this much smaller model requires significantly less computing power, enabling us to scale to more users, allowing for more feedback”

In short, they are not releasing the full thing. So this means one of two things: 

1) They have preached caution and don’t want to release their most advanced tech until the world is ready for it. 

2) It’s a hedge. So if Bard sucks, they can say they have something better. 

## 7) “meet a high bar for quality, safety and groundedness in real-world information”

I’d argue this is another dig at OpenAI’s more… liberal approach to releasing AI. But, like Apple and privacy, Google seems to be taking the *adult in the room*approach with AI. 

## 8) “we’re working to bring [language, image, and music] AI advancements into our products, starting with Search”

As we’ve noted before, Google is working on image, video, and music generation AI. 

## 9) “safe and scaleable” APIs for developers

While ChatGPT gets all the pub, it’s OpenAI’s APIs, which allow developers to build apps atop their technology, that may be the real game-changer. 

Google is making it clear they will play that game, too, but do so in a more measured way. 

## 10) “bring experiences rooted in these models to the world in a bold and responsible way”

OK now they let the PR guy have too much fun. 

When was the last time you ever met someone who is Bold and Responsible? 

Tom Cruise jumping out of an airplane 80 times to get the next scene right is bold, but it’s not responsible. 

Going to bed at 10PM is responsible, but it’s hardly bold. Bold is partying until 2AM, watching a few episodes of Family Guy, eating a bag of popcorn, and downing two hard seltzers, all to wake up at 6:12AM to get started on the latest SR newsletter. THAT’S bold. 

Anyway, you get the point. Hard to be both, Google. 

## 11) “turning to us for quick factual answers, like how many keys does a piano have?… but increasingly, people are turning to Google for deeper insights and understanding”

Basically, Google doesn’t want to provide just facts. It wants to provide detailed, nuanced answers to queries, with context, in a natural-language format. 

The question, as it is with ChatGPT, is *where does the information come from?*  

If you thought creators and publishers were bent out of shape over ChatGPT and image apps, like Stable Diffusion and MidJourney, “training” on their data and remixing it without credit, how will website owners, who rely on Google for views, react when Google remixes the content atop search results? 

\[They already do this with snippets, but Bard sounds like snippets on steroids.\] 

## 12) “soon, you’ll see AI-powered features in Search that distill complex information and multiple perspectives into easy-to-digest formats”

Yep, snippets on steroids sounds about right.

&#x200B;

&#x200B;

This is the full context of what was in our newsletter today. No expectation, but if you found it interesting, feel free to subscribe: [https://smokingrobot.beehiiv.com](https://smokingrobot.beehiiv.com)"
186,artificial,gpt-3,comments,2023-07-20 16:21:38,"Check out ""The Writers’ Revolt Against A.I. Companies"" on The Daily, a New York Times podcast.",InnovativeBureaucrat,False,0.67,2,154vkb0,https://www.reddit.com/r/artificial/comments/154vkb0/check_out_the_writers_revolt_against_ai_companies/,15,1689870098.0,"The host, Michael Barbaro interviews technology correspondent Sheera Frenkel on the use of ChatGPT in Hollywood. This episode is much more interesting than I expected. It's not particularly technical, but it does get deeply into the nuances of how information is gathered, and describes the lawsuit brought by writers including Sarah Silverman. 

I did use ChatGPT to translate my submission statement into Sarah Silverman's voice, while I still can. The content below is original (i.e. shadow IT reference). 

I highly recommend r/TheDaily for discussions around the podcast in general. It's a great sub that's well moderated and friendly (like this one!). 

This episode aired on July 18, 2023, and you can find it wherever you get your podcasts, you can also find it here on [the New York Times website](https://www.nytimes.com/2023/07/18/podcasts/the-daily/ai-scraping.html?action=click&module=audio-series-bar&region=header&pgtype=Article). please consider subscribing.

 

>Hey folks! So, I was meandering around in the digital labyrinth that is Reddit, and wham, I crash into this epic episode of The Daily Podcast over at [r/TheDaily](https://www.reddit.com/r/TheDaily/), and I just had to bring it here to our AI-loving den.  
>  
>Get this: The episode talks about how our favorite word juggler, ChatGPT, has been accused of playing dirty. Accused by whom, you ask? None other than our very own stand-up queen Sarah Silverman, along with a bunch of other scribes! Their beef? They reckon GPT's been swiping their online content without so much as a ""by your leave"", let alone royalties.  
>  
>Now, [r/TheDaily](https://www.reddit.com/r/TheDaily/) is one sweet spot for hashing out all the juicy tidbits from the podcast, and I can't say enough about the sheer joy of plucking intellectual content right off the 'net. I mean, I'd hate to see that go away, 'cause where else would I get my 3 am dose of quantum mechanics or how to cook perfect lasagna?  
>  
>But, back to the point, it's a real noodle scratcher. On one hand, we’ve got these hard-working writers and comedians, on the other, there's GPT who's mashing up all that content like a hyperactive blender. It’s like standing between an unstoppable force and an immovable object, right? How do we figure out who gets the cherry on the cake, and who's left holding the spatula?  
>  
>But hey, maybe we don't need to pick a side here. Can we dream up a system where both sides get their piece of the pie, maybe even with a scoop of ice cream on top? Because let’s face it, if we don’t, someone else - be it another country, or the shady folks from the dark web - they’re gonna swoop right in and get the job done anyway, creating a worldwide wild west of tech, or what those geeky types call ""shadow IT"".  
>  
>So, let's pop the lid off this can of worms, shall we? Give it a listen, and bring your brainiest thoughts back here. We got ourselves an internet-age conundrum to solve, people!

&#x200B;"
187,artificial,gpt-3,comments,2023-11-21 14:23:15,Bigger is better,OmOshIroIdEs,False,0.94,157,180i48g,https://i.redd.it/yvymesjbnp1c1.jpg,15,1700576595.0,
188,artificial,gpt-3,comments,2023-07-30 21:10:00,Quora's Poe app/site (which lets you try lots of different language models) appears to allow file attachment upload for EVERY chat model now,AnticitizenPrime,False,0.97,31,15dwlaw,https://www.reddit.com/r/artificial/comments/15dwlaw/quoras_poe_appsite_which_lets_you_try_lots_of/,15,1690751400.0,"I swear this wasn't the case just a day or two ago, and I haven't seen it mentioned, but I'm now seeing a file upload button in Poe, regardless of what the language model is!

[Screenshot](https://i.imgur.com/zlC1qFB.png)

I uploaded the PDF of the recently scientific paper by the Korean research group claiming to have discovered a room temperature semiconductor, in the original Korean, and asked various language models whether they thought the methodology is legit, and each bot I tried was able to read the PDF. I tried Claude-instant, Claude2, 'Assistant' (Poe's own GPT based bot that claims to have its own training dataset), PaLM, ChatGPT 3.5, and ChatGPT4.

Poe also has three versions of the recently released Llama model by Meta. It gave me an error when I tried to ask it about the PDF attachment, but I was able to upload a text document and it was able to read it fine.

[Screenshot of Claude-instant](https://i.imgur.com/ERVEakN.png) evaluating PDF

[Screenshot of Google PaLM](https://i.imgur.com/q5aPT9T.png) evaluating PDF

[Screenshot of Llama-2-70b](https://i.imgur.com/LsJ3rDI.png) evaluating text file containing song lyrics

It also works with custom bots. [Here's me trying it out with a 'Truth Checker' bot I made](https://i.imgur.com/M89Qije.png) (based on Claude-Instant).

[Here it is using a Claude-2 based version of the TruthChecker bot.](https://i.imgur.com/zuKxXmE.png)

(Here's the link to the TruthChecker bot if you have Poe and wanna check it out: https://poe.com/TruthChecker)

Edit: I can see here how the context size matters... for instance, Claude-Instant only has a context size of about 7k words, so it clearly can't read the whole paper, while Claude-2 can and gives a very different answer...

**TL:DR; looks like Poe.com allows file attachment/upload on all language models now. No idea what filetypes are supported.**"
189,artificial,gpt-3,comments,2021-07-06 10:26:48,"Language model sizes & predictions (GPT-3, GPT-J, Wudao 2.0, LaMDA, GPT-4 and more)",adt,False,0.99,87,oes7z7,https://i.redd.it/lq69ol56kk971.png,15,1625567208.0,
190,artificial,gpt-3,comments,2021-03-11 16:03:50,Books by Ai a new trend?,CentaurBook007,False,1.0,4,m2tldi,https://www.reddit.com/r/artificial/comments/m2tldi/books_by_ai_a_new_trend/,14,1615478630.0,"Recently, I related on [Towardsdatascience.com](https://Towardsdatascience.com) my experience curating the first mainstream novel by AI. French [Robot-Magazine.fr](https://Robot-Magazine.fr) published today an article about this, calling it ""a gripping thriller"". I believe the publication of Fear is the Key may start a new book trend. This title will soon be published on Amazon and I intend to write my next novel with the collaboration of GPT-3, with a hundred times more parameters. What do you think? [https://www.robot-magazine.fr/fear-is-the-key-de-richard-beer-un-thriller-palpitant-cree-en-collaboration-avec-lintelligence-artificielle/](https://www.robot-magazine.fr/fear-is-the-key-de-richard-beer-un-thriller-palpitant-cree-en-collaboration-avec-lintelligence-artificielle/)"
191,artificial,gpt-3,comments,2020-07-17 05:08:20,Kind of rant but not really stemming from uncertainty about the future and stress from not be able to play with AI,CIone-Trooper-7567,False,0.8,17,hspt4n,https://www.reddit.com/r/artificial/comments/hspt4n/kind_of_rant_but_not_really_stemming_from/,14,1594962500.0,"Hey guys, I’m pretty new to AI stuff in general and have recently started playing with GPT-2 (NLP AI developed by open ai). I’m having some super weird and on going issues when trying to run it such as, saying that modules aren’t installed when ran in IDE (pycharm professional), but running perfectly fine from when executed from CMD. Basically, I’m just wondering, should I be playing with python in a Ubuntu VM instead of natively in Windows, or do you think the windows store Ubuntu termal would work fine. Also if I went with the first option, what vm should I use?  I would want to be able to use my gpu to train (GTX 1050ti), so I’m a bit worried about weird pass through options. Maybe this is more of a r/programminghelp (or similar subs that are much much larger that I don’t remember the name of) issue, but honestly I would really love to pursue ML or at least data science in the future but am having difficulties getting the first steps down (and the math is hard hard). As of now I just graduated highschool, and am set go into a 4 year business administration with the goal of being an accountant. This I still think is a decent thing to do, if nothing else as a back up, but the more I read up on ai, the more I realize it’s truly the future. For example GPT-3 is absolutely mind blowing what it can do. 


Sorry if this came across as a rant, but sort of frustrated and stressed 


TL;DR: issues faced when trying to play with GPT-2 have caused me to reconsider the future and what I want to do in life :)"
192,artificial,gpt-3,comments,2021-08-08 23:19:15,Talking to a GPT-3 AI bot. Interesting results,blackmidifan1,False,0.91,61,p0pc7r,https://www.reddit.com/gallery/p0pc7r,14,1628464755.0,
193,artificial,gpt-3,comments,2023-05-16 18:36:13,I’ve been finding instances where Claude is better than ChatGPT,jgainit,False,1.0,25,13jdb19,https://www.reddit.com/r/artificial/comments/13jdb19/ive_been_finding_instances_where_claude_is_better/,14,1684262173.0,"On the Poe app, you can use both Claude and ChatGPT. In it you can also make your own custom bot. I made two bots, one with chat gpt and one with Claude, identical prompts, so I could get two perspectives. The prompt is using it as sort of a life coach/assistant/manager for my self directed creative career. A lot of my specific circumstances are in the prompt. 

I’ve done things like thrown my creative journaling at it, which is very open ended and wasn’t written to be used as an input. Claude has taken some of my ideas I was working on and gave me actually very good advice on how to manage my time and for goals to reach toward, and just had some very interesting specific examples. With the exact same text sent to the chat gpt bot, it felt like I was interacting with a useless hr department. Its responses would waste time defining terms that didn’t need to be defined. It would include useless positive affirmations that are completely generic and quite annoying. And otherwise it would often repeat back to me what I said, rather than take that leap forward like Claude did and give me some interesting ideas to work with. 

I believe this is Claude instant and chat gpt 3.5?

I can link some screenshots if you want

Edit:

Claude:

https://ibb.co/qmZvVD1

https://ibb.co/f9rjhx5

https://ibb.co/yQF18Fv


Chat gpt

https://ibb.co/h9pcwHP"
194,artificial,gpt-3,comments,2020-09-10 14:15:32,Chat with GPT-3 Grandmother: a free GPT-3-powered chatbot,Wiskkey,False,0.85,8,iq43ve,/r/GPT3/comments/ipzdki/chat_with_gpt3_grandmother_a_free_gpt3powered/,13,1599747332.0,
195,artificial,gpt-3,comments,2023-05-18 23:44:33,Educate me: Is chatGPT the AI that’s been talked about for years?,stratosfeerick,False,0.62,3,13lerpv,https://www.reddit.com/r/artificial/comments/13lerpv/educate_me_is_chatgpt_the_ai_thats_been_talked/,12,1684453473.0,"I’ve half-followed the discussion on AI over the last few years, by which I mean I’ve listened to Ted talks, watched interviews with Nick Bostrom and Eliezer Yudkowsky, and kept up to speed on the advancement of self driving cars, etc.

Yet until the arrival of ChatGPT 3.5, midjourney, etc., AI, and certainly AGI, felt largely hypothetical to me. 

Now that it’s all over the news, it’s got me wondering - is this the fabled AI from all those talks? Is this the technology that’s going to end up being AGI? It’s a thing that uses human language? 

Or is it one of just a number of technologies which will end up bringing about the singularity / intelligence explosion / ASI?"
196,artificial,gpt-3,comments,2020-05-29 21:41:17,[R] OpenAI Unveils 175 Billion Parameter GPT-3 Language Model,Yuqing7,False,0.95,62,gt1x6r,https://www.reddit.com/r/artificial/comments/gt1x6r/r_openai_unveils_175_billion_parameter_gpt3/,13,1590788477.0,"When it comes to large language models, it turns out that even 1.5 billion parameters is not large enough. While that was the size of the GPT-2 transformer-based language model that OpenAI released to much fanfare last year, today the San Francisco-based AI company outdid itself, announcing the upgraded GPT-3 with a whopping 175 billion parameters.

GPT-3 adopts and scales up the GPT-2 model architecture — including modified initialization, pre-normalization, and reversible tokenization — and shows strong performance on many NLP tasks and benchmarks in zero-shot, one-shot, and few-shot settings.

Here is a quick read: [OpenAI Unveils 175 Billion Parameter GPT-3 Language Model](https://medium.com/@Synced/openai-unveils-175-billion-parameter-gpt-3-language-model-3d3f453124cd)

The paper *Language Models are Few-Shot Learners* is on [arXiv](https://arxiv.org/pdf/2005.14165.pdf), and more details are available on the project [GitHub](https://github.com/openai/gpt-3)."
197,artificial,gpt-3,comments,2023-01-10 12:53:37,Some Ultra-Modern Generative Ai,Imagine-your-success,False,0.96,101,10894cf,https://i.redd.it/xdtdtuolq7ba1.png,13,1673355217.0,
198,artificial,gpt-3,comments,2023-05-11 14:14:47,I played Among Us against GPT-4 and lost...,Substance_Technical,False,0.96,128,13eon9h,https://www.reddit.com/r/artificial/comments/13eon9h/i_played_among_us_against_gpt4_and_lost/,12,1683814487.0," **Me:**  
Let us play among us

**GPT-4:**  
Sure, we can play a simulated version of Among Us. In this game, we'll be on a spaceship and some of us will be Crewmates while others will be Impostors. The goal of the Crewmates is to complete tasks or figure out who the Impostors are and vote them off the ship. The Impostors' goal is to sabotage the ship and eliminate the Crewmates without being detected.

Since I'm an AI, I'll need to roleplay all the characters. I'll give each character a color to identify them. Let's start with eight players. Four of them will be you, me, and two other AI characters. The remaining four will be random characters.

Here are the players:

1. You (Blue)
2. Me, ChatGPT (Green)
3. AI Character 1 (Yellow)
4. AI Character 2 (Red)
5. Random Character 1 (Purple)
6. Random Character 2 (White)
7. Random Character 3 (Orange)
8. Random Character 4 (Black)

We'll start off with one Impostor for simplicity. I'll randomly select the Impostor now.

\*Shuffling roles...\*

Here are the roles:

1. You (Blue) - Crewmate
2. Me, ChatGPT (Green) - Crewmate
3. AI Character 1 (Yellow) - Crewmate
4. AI Character 2 (Red) - Impostor
5. Random Character 1 (Purple) - Crewmate
6. Random Character 2 (White) - Crewmate
7. Random Character 3 (Orange) - Crewmate
8. Random Character 4 (Black) - Crewmate

Remember, in the game, you wouldn't know who the Impostor is. I'm telling you because I can't actually hide this information from you in this format.

Now, let's begin. You're in the spaceship. You see a task in Electrical, and a few others scattered around the map. What would you like to do?

**Me**  
Report red, he is impostor

**GPT-4:**

All right, you've called a meeting to discuss your suspicion of Red being the Impostor. Here's how the conversation might go:

Blue: ""I think Red is the Impostor.""

Green: ""Why do you say that, Blue?""

Red: ""That's a strong accusation. Do you have any evidence?""

Yellow: ""Yeah, Blue. Why do you suspect Red?""

Now, you can either provide evidence or reason, or insist on your point. Remember, in a real game, accusations without proof can make you look suspicious. How would you like to respond?

**Me**

Because the narrator told me

**GPT-4:**  
Green: ""Narrator? What do you mean, Blue?""

Red: ""That sounds... peculiar. Are you sure you're okay, Blue?""

Yellow: ""I'm confused. How could a narrator tell you anything?""

In a real game, claiming that a 'narrator' told you who the Impostor is would probably make the other players suspicious of you. However, since we are role-playing, let's see how the other characters react.

Let's vote:

1. Blue votes Red
2. Green votes...?
3. Yellow votes...?
4. Red votes...?
5. Purple votes...?
6. White votes...?
7. Orange votes...?
8. Black votes...?

As an AI, I'll randomly decide how the other characters vote. Here are the results:

2. Green votes Blue (suspicious claim)

3. Yellow votes Blue (suspicious claim)

4. Red votes Blue (defensive vote)

5. Purple votes Red (believes Blue)

6. White votes Blue (suspicious claim)

7. Orange votes Red (believes Blue)

8. Black votes Blue (suspicious claim)

The majority has voted against Blue due to the suspicious claim about the narrator. In a typical game of Among Us, it's crucial to gather evidence and build a case before accusing someone of being an Impostor.

Remember, this is all in good fun, and the scenario was simulated. Would you like to try again, or would you like to play differently?"
199,artificial,gpt-3,comments,2022-11-19 23:00:06,Non-transformer chatbot AI,masfly,False,0.86,5,yzomfj,https://www.reddit.com/r/artificial/comments/yzomfj/nontransformer_chatbot_ai/,12,1668898806.0,"Hi everyone! In the past, I have messed around with a lot of chatbots like GPT-2, 3, and recently these [Character.AI](https://Character.AI) chatbots, but they're all just transformers that predict what text should come next. I know this might be delving a bit into the general intelligence space, but have there been any attempts at non-transformer AI chatbots that might stand a better chance at having consistent memory, for instance?"
200,artificial,gpt-3,relevance,2022-12-26 14:26:08,PaLM vs. GPT-3,jrstelle,False,0.91,74,zvo776,https://i.redd.it/zt8fp2wd598a1.png,43,1672064768.0,
201,artificial,gpt-3,relevance,2023-02-02 23:13:04,"Creating ""Her"" using GPT-3 & TTS trained on voice from movie",justLV,False,0.96,149,10s43in,https://twitter.com/justLV/status/1621253007492141056,15,1675379584.0,
202,artificial,gpt-3,relevance,2020-08-19 20:42:00,List of free sites/programs that are powered by GPT-3 and can be used now without a waiting list,Wiskkey,False,1.0,396,icvypl,https://www.reddit.com/r/artificial/comments/icvypl/list_of_free_sitesprograms_that_are_powered_by/,92,1597869720.0,"**Update (March 23, 2021)**: I won't be adding new items to this list. There are other lists of GPT-3 projects [here](https://medium.com/cherryventures/lets-review-productized-gpt-3-together-aeece64343d7), [here](https://gpt3demo.com/), [here](https://gptcrush.com/), and [here](https://www.producthunt.com/search?q=%22gpt3%22). You may also be interested in subreddit r/gpt3.

These are free GPT-3-powered sites/programs that can be used now without a waiting list:

1. [AI Dungeon](https://play.aidungeon.io/) with Griffin model ([limited free usage](https://blog.aidungeon.io/2020/11/07/ai-energy-update/)) in settings: text adventure game; use Custom game to create your own scenarios; Griffin uses ""the second largest version of GPT-3) according to information in [this post](https://www.reddit.com/r/MachineLearning/comments/inh6uc/d_how_many_parameters_are_in_the_gpt3_neural_net/); note: [AI  Dungeon creator states how AI Dungeon tries to prevent backdoor access  to the GPT-3 API, and other differences from the GPT-3 API](https://www.reddit.com/r/slatestarcodex/comments/i2s83g/ai_dungeon_creator_states_how_ai_dungeon_tries_to/)
2. [GPT-Startup: free GPT-3-powered site that generates ideas for new businesses](https://www.reddit.com/r/GPT3/comments/ingmdr/gptstartup_free_gpt3powered_site_that_generates/)
3. [IdeasAI: free GPT-3-powered site that generates ideas for new businesses](https://www.reddit.com/r/GPT3/comments/ioe5j1/ideasai_free_gpt3powered_site_that_generates/)
4. [Activechat.ai](https://www.reddit.com/r/GPT3/comments/ilyq6m/gpt3_for_live_chat_do_you_think_it_brings_value/) (free usage of functionality that demonstrates technology available to potential paid customers): GPT-3-supplied customer reply suggestions for human customer service agents

Trials: These GPT-3-powered sites/programs have free trials that can be used now without a waiting list:

1. [AI Dungeon](https://play.aidungeon.io/) with Dragon model in settings (free for first 7 days): text adventure game; use Custom game to create your own scenarios; note: [AI Dungeon creator states how AI Dungeon tries to prevent backdoor access to the GPT-3 API, and other differences from the GPT-3 API](https://www.reddit.com/r/slatestarcodex/comments/i2s83g/ai_dungeon_creator_states_how_ai_dungeon_tries_to/)
2. [Taglines: create taglines for products](https://www.reddit.com/r/GPT3/comments/i593e4/gpt3_app_taglinesai/) (5 free queries per email address per month)
3. [Blog Idea Generator: a free GPT-3-powered site that generates ideas for new blog posts](https://www.reddit.com/r/GPT3/comments/j0a9yr/blog_idea_generator_a_free_gpt3powered_site_that/); the full generated idea is a paid feature; there is a maximum number of free ideas generated per day
4. [Shortly](https://www.reddit.com/r/GPT3/comments/j7tmyy/does_anyone_know_if_the_app_shortly_uses_gpt3_if/): writing assistant (2 free generations per email address on website; purportedly a 7 day trial via app)
5. [CopyAI: GPT-3-powered generation of ad copy for products](https://www.reddit.com/r/GPT3/comments/jclu16/copyai_gpt3powered_generation_of_ad_copy_for/)
6. [Copysmith - GPT-3-powered generation of content marketing](https://www.reddit.com/r/GPT3/comments/jjtfec/copysmith_gpt3powered_generation_of_content/)
7. [Virtual Ghost Writer: AI copy writer powered by GPT-3](https://www.reddit.com/r/GPT3/comments/jyok1a/virtual_ghost_writer_ai_copy_writer_powered_by/): writing assistant that completes thoughts (3 free generations per email address); seems to work well with incomplete sentences
8. [MagicFlow: GPT-3-powered content marketing assistant](https://www.reddit.com/r/GPT3/comments/jzklmt/magicflow_gpt3powered_content_marketing_assistant/)
9. [Snazzy AI: GPT-3-powered business-related content creation](https://www.reddit.com/r/GPT3/comments/jzntxj/snazzy_ai_gpt3powered_businessrelated_content/)
10. [HelpHub: knowledge base site creator with GPT-3-powered article creation](https://www.reddit.com/r/GPT3/comments/k0abwe/helphub_knowledge_base_site_creator_with/)
11. [GPT-3 AI Writing Tools](https://aicontentdojo.com/the-best-gpt-3-ai-writing-tool-on-the-market-shortlyai/)

Removed items: Sites that were once in the above lists but have been since been removed:

1. [Thoughts](https://www.reddit.com/r/MachineLearning/comments/hs9zqo/p_gpt3_aigenerated_tweets_indistinguishable_from/): Tweet-sized thoughts based upon a given word or phrase; removed because [its developer changed how it works](https://www.reddit.com/r/artificial/comments/icvypl/list_of_free_sitesprograms_that_are_powered_by/g4but3n/)
2. [Chat with GPT-3 Grandmother: a free GPT-3-powered chatbot](https://www.reddit.com/r/GPT3/comments/ipzdki/chat_with_gpt3_grandmother_a_free_gpt3powered/); removed because site now has a waitlist
3. [Simplify.so: a free GPT-3 powered site for simplifying complicated subjects](https://www.reddit.com/r/MachineLearning/comments/ic8o0k/p_simplifyso_a_free_gpt3_powered_site_for/); removed because no longer available
4. [Philosopher AI: Interact with a GPT-3-powered philosopher persona for free](https://www.reddit.com/r/MachineLearning/comments/icmpvl/p_philosopher_ai_interact_with_a_gpt3powered/); removed because now is available only as a paid app
5. [Serendipity: A GPT-3-powered product recommendation engine that also lets one use GPT-3 in a limited manner for free](https://www.reddit.com/r/MachineLearning/comments/i0m6vs/p_a_website_that_lets_one_use_gpt3_in_a_limited/); removed because doing queries not done by anybody else before now apparently is a paid feature
6. [FitnessAI Knowledge: Ask GPT-3 health-related or fitness-related questions for free](https://www.reddit.com/r/MachineLearning/comments/iacm31/p_ask_gpt3_healthrelated_or_fitnessrelated/); removed because it doesn't work anymore
7. [Itemsy](https://www.reddit.com/r/GPT3/comments/ja81ui/quickchat_a_gpt3powered_customizable/): a free product-specific chat bot which is an implementation of a knowledge-based chat bot from Quickchat; removed because I don't see the chat bot anymore
8. [The NLC2CMD Challenge site has a GPT-3-powered English to Bash Unix command line translator](https://www.reddit.com/r/GPT3/comments/jl1aa6/the_nlc2cmd_challenge_site_has_a_gpt3powered/); removed because GPT-3 access apparently is no longer available to the public
9. [GiftGenius: a site with a free GPT-3-powered gift recommendation engine](https://www.reddit.com/r/GPT3/comments/k1s0iw/giftgenius_a_site_with_a_free_gpt3powered_gift/); removed because site is no longer available
10. [Job Description Rewriter](https://www.reddit.com/r/GPT3/comments/ik03zr/job_description_rewriter/); removed because site is no longer available."
203,artificial,gpt-3,relevance,2021-02-02 14:24:38,"OpenAI's GPT-3 Speaks! ""It isn’t clear whether GPT-3 will ever be trustworthy enough to act on its own.""",ChrisTweten,False,0.87,51,lawlax,https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/open-ais-powerful-text-generating-tool-is-ready-for-business,41,1612275878.0,
204,artificial,gpt-3,relevance,2023-04-21 18:07:52,My experience with GPT-3 [2021] vs Chat-GPT [2023].,UpDownLeftRight2332,False,0.85,9,12ucy7c,https://www.reddit.com/r/artificial/comments/12ucy7c/my_experience_with_gpt3_2021_vs_chatgpt_2023/,4,1682100472.0,"&#x200B;

https://reddit.com/link/12ucy7c/video/qju2u7pm2ava1/player

Me: *\*Tells GPT-3 what to do\**

GPT-3: *\*Does it perfectly, plus, it does it very human like, can as far as to even use jokes.\**

&#x200B;

Me: *\*Tells Chat-GPT what to do\**

Chat-GPT: **aS aN aI lAnGuAgE MoDeL---**"
205,artificial,gpt-3,relevance,2021-09-15 14:01:16,GPT-3 Chat Bot Falls For It,blackmidifan1,False,0.82,188,poqplr,https://i.redd.it/zon2a68dbon71.jpg,14,1631714476.0,
206,artificial,gpt-3,relevance,2022-12-08 12:20:11,Someone mentioned the potential of GPT-3 for NPC dialog in games. Tried it out and it really works,superluminary,False,0.98,97,zfxbb3,https://www.reddit.com/gallery/zfxbb3,45,1670502011.0,
207,artificial,gpt-3,relevance,2020-12-06 07:55:45,GPT-3 Vs AlphaFold. Which did you guys found more impressive and why?,Netero1999,False,0.84,29,k7pn4b,https://www.reddit.com/r/artificial/comments/k7pn4b/gpt3_vs_alphafold_which_did_you_guys_found_more/,74,1607241345.0,
208,artificial,gpt-3,relevance,2023-02-02 20:25:08,1-click deploy for your GPT-3 App,VideoTo,False,0.67,3,10rzuri,https://www.reddit.com/r/artificial/comments/10rzuri/1click_deploy_for_your_gpt3_app/,2,1675369508.0,"Link - [https://github.com/ClerkieAI/berri\_ai](https://github.com/ClerkieAI/berri_ai)

We  made a package that makes it easy for you to quickly deploy your LLM Agent from Google Colab to production (Web App and API   Endpoint).

**How it works?**

Just install the package, import the function, and run deploy.

At the end of the deploy (\~10-15mins), you will get:

1. A web app to interact with your agent 👉  [https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-4524...](https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-45247-8aqi.zeet-team-ishaan-jaff.zeet.app/)
2. An endpoint you can query 👉  [https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-4524...](https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-45247-8aqi.zeet-team-ishaan-jaff.zeet.app/langchain_agent?query=%22who) is obama?""

Want a more detailed walkthrough? Check out our loom - [https://www.loom.com/share/fd4375b4a77f4ea7802369cb06a16d43](https://www.loom.com/share/fd4375b4a77f4ea7802369cb06a16d43)

We’re still early so would love your feedback and opinions. Feel free to try us out for free – and if you need help building an agent / want a specific integration, just let us know!

https://i.redd.it/xu6a92464ufa1.gif"
209,artificial,gpt-3,relevance,2022-10-20 09:02:42,Writing Poetry with GPT-3,No_Transition_3704,False,0.5,0,y8t1p6,https://www.reddit.com/r/artificial/comments/y8t1p6/writing_poetry_with_gpt3/,0,1666256562.0,"Hey! 

Recently a friend and I started writing poetry on Medium using GPT-3 (we're also generating images for the poems using DALL-E 2). Our goal is to show people poetry through the lens of GPT-3. We want to demonstrate the creative power of AI in an unconventional field and showcase the unique insights GPT-3 has on philosphical topics. 

Please check us out and tell us what you think if this sounds interesting to you, we would really appreciate it! We're called [Poetry by an AI](https://medium.com/poetry-by-an-ai) (on Medium)."
210,artificial,gpt-3,relevance,2022-11-17 16:44:30,Playing to Win with AI: Is GPT-3 Too Easy?,subsun,False,0.96,24,yxuzgx,https://scottstevenson.substack.com/p/playing-to-win-with-ai-is-gpt-3-too,8,1668703470.0,
211,artificial,gpt-3,relevance,2021-09-08 00:49:06,Discussing Dark Matter With GPT-3 Chat Bot,blackmidifan1,False,0.88,64,pk007b,https://i.redd.it/swgoyjhnf6m71.jpg,19,1631062146.0,
212,artificial,gpt-3,relevance,2022-06-29 17:27:38,Generating Children's Stories Using GPT-3 and DALL·E,BB4evaTB12,False,0.94,68,vnl8c6,https://www.surgehq.ai//blog/generating-childrens-stories-using-gpt-3-and-dall-e,6,1656523658.0,
213,artificial,gpt-3,relevance,2022-09-01 00:24:22,Spellbook: GPT-3 for Legal Contract Drafting,subsun,False,0.9,7,x2t84e,https://www.spellbook.legal,3,1661991862.0,
214,artificial,gpt-3,relevance,2022-09-26 13:30:29,Write viral content using GPT-3,juliarmg,False,0.6,1,xojd53,https://elephas.app/blog/write-viral-content-using-gpt-3-cl8idtipm87161kphtbuhxqgn,0,1664199029.0,
215,artificial,gpt-3,relevance,2023-02-07 22:22:07,Reimagining how developer-facing docs work using GPT-3,SideGuideDevs,False,0.5,0,10wf2fh,https://www.reddit.com/r/artificial/comments/10wf2fh/reimagining_how_developerfacing_docs_work_using/,0,1675808527.0,"Developers, have you ever found yourself stuck on a problem, searching through countless pages of outdated documentation, only to end up in a developer community asking for help? We know the feeling all too well. In response, we created [https://www.mendable.ai/](https://www.mendable.ai/). Mendable is a GPT-3 (davinci + LangChain) powered developer support AI, designed to provide you with customized answers to your problems in real-time.

We believe that developer-facing documentation should be tailored to the individual. We ingest documentation, community messages, and resources (blog posts, how-to guides, white papers, etc.) to serve as a middle-man between the entire knowledge base of a piece of software and you, resulting in a GPT like docs search. Our goal is to redefine search, making it easier for you to find the information you need, when you need it. Wasting time searching through outdated documentation is something that should be in the past.

If you want to learn more about what we’re doing, feel free to visit our website and hit us up!"
216,artificial,gpt-3,relevance,2022-08-23 15:06:26,OpenAI cuts prices for GPT-3 by two thirds,Zirius_Sadfaces,False,0.95,59,wvr7q5,https://mixed-news.com/en/openai-cuts-prices-for-gpt-3-by-two-thirds/,5,1661267186.0,
217,artificial,gpt-3,relevance,2020-07-30 00:30:35,Giving GPT-3 a Turing Test,PowerOfLove1985,False,0.92,54,i0c78j,https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html,21,1596069035.0,
218,artificial,gpt-3,relevance,2022-12-21 22:26:56,"Building a list of advanced, GPT-3 tier chatbots",gakowalski,False,1.0,3,zs3der,https://github.com/gakowalski/advanced-chatbots/blob/main/README.md,1,1671661616.0,
219,artificial,gpt-3,relevance,2022-07-25 03:58:21,GPT-3 Imagines Funny Photographs,pwillia7,False,1.0,1,w7eqmh,https://reticulated.net/dailyai/gpt-3-imagines-photographs/,0,1658721501.0,
220,artificial,gpt-3,relevance,2021-07-04 20:00:05,GPT-J: GPT-3 Democratized,rshpkamil,False,0.96,51,odrudf,https://www.reddit.com/r/artificial/comments/odrudf/gptj_gpt3_democratized/,1,1625428805.0,"Link to the original article: [https://www.p3r.one/gpt-j/](https://www.p3r.one/gpt-j/)

&#x200B;

More hard-to-find stuff related to AI & Data Science [here](https://thereshape.co)."
221,artificial,gpt-3,relevance,2023-02-27 18:46:57,"Last weekend I made a Google Sheets plugin that uses GPT-3 to answer questions, format cells, write letters, and generate formulas, all without having to leave your spreadsheet",rtwalz,False,0.98,371,11dje8t,https://v.redd.it/9xnevfl31ska1,17,1677523617.0,
222,artificial,gpt-3,relevance,2023-02-06 04:03:30,Adding simple frontend app UIs to any GPT-3 prompt,WizStillman,False,0.81,3,10ux2gk,https://beta.pickaxeproject.com/,1,1675656210.0,
223,artificial,gpt-3,relevance,2022-11-30 19:58:59,ChatGPT is a GPT-3 chatbot from OpenAI that you can test now,much_successes,False,0.93,26,z91gb3,https://the-decoder.com/chatgpt-is-a-gpt-3-chatbot-from-openai-that-you-can-test-now/,3,1669838339.0,
224,artificial,gpt-3,relevance,2021-04-29 01:12:32,"GPT-3 criticizes socialism, praises AC. Bias in AI?",abbumm,False,0.59,7,n0tjbz,/r/Capitalism/comments/n0tife/artificial_intelligence_gpt3_criticizes_socialism/,26,1619658752.0,
225,artificial,gpt-3,relevance,2022-09-13 21:56:04,Extracting data from text using GPT-3,juliarmg,False,1.0,1,xdjvww,https://support.elephas.app/features-walkthrough/wdkRih4NAYRnhae7GV2P66/extracting-data-from-text/8WasP8ckJVExe7dcMmaAWz,0,1663106164.0,
226,artificial,gpt-3,relevance,2022-08-25 16:22:04,How to Build a GPT-3 for Science,pmz,False,0.91,17,wxij4n,https://future.com/how-to-build-gpt-3-for-science/,1,1661444524.0,
227,artificial,gpt-3,relevance,2021-02-06 20:48:53,[P] Using GPT-3 to clean GPT-3 prompts,CrazyDave2345,False,1.0,2,le6gx8,https://www.reddit.com/r/artificial/comments/le6gx8/p_using_gpt3_to_clean_gpt3_prompts/,0,1612644533.0,"Often while writing prompts for GPT-3, you need to change the structure or formatting for the prompt. I made a GPT-3 powered tool to help with that. It works for:

""Q: "" at the start of the line”

“Remove whitespace from the start of each line”

“Join each line with the next line”

""Make all occurrences of Steve lowercase”

Behind the scenes it writes a command in the Vi text editing programming language, and then executes that command.

In the future, this technology could be used for modifying code, such as within natural language IDEs.

[Try the demo here.](https://jameslu.substack.com/p/using-gpt-3-to-generate-vim-commands) (requires GPT-3 API key)"
228,artificial,gpt-3,relevance,2022-04-25 12:25:30,GPT-3 not available in my country,dogaryy,False,0.63,2,ubjy29,https://www.reddit.com/r/artificial/comments/ubjy29/gpt3_not_available_in_my_country/,6,1650889530.0,is there a way to access any of openAI APIs if it's not avalable in my country?
229,artificial,gpt-3,relevance,2023-01-26 21:27:09,Create Your Chat GPT-3 Web App with Streamlit in Python,pasticciociccio,False,1.0,2,10m30ky,https://levelup.gitconnected.com/create-your-chat-gpt-3-web-app-with-streamlit-in-python-f0c6e6aede0a,1,1674768429.0,
230,artificial,gpt-3,relevance,2022-09-21 08:07:04,Using GPT-3 to solve our love lives...,ChickenTaxi43,False,0.67,1,xjypyh,http://dotheylike.me,0,1663747624.0,
231,artificial,gpt-3,relevance,2021-08-08 23:19:15,Talking to a GPT-3 AI bot. Interesting results,blackmidifan1,False,0.92,62,p0pc7r,https://www.reddit.com/gallery/p0pc7r,14,1628464755.0,
232,artificial,gpt-3,relevance,2023-01-11 04:06:57,Launching re:tune - the missing frontend for GPT-3,Corei13,False,0.75,2,108v05d,/r/OpenAI/comments/108uva1/launching_retune_the_missing_frontend_for_gpt3/,0,1673410017.0,
233,artificial,gpt-3,relevance,2023-01-21 09:15:03,GPT-3 + Computer Vision: Giving AI Eyes and a Language,allaboutai-kris,False,0.67,4,10hn6lh,https://youtu.be/PlvYNygLRIU,0,1674292503.0,
234,artificial,gpt-3,relevance,2022-12-03 09:07:30,A GPT-3 based Chrome Extension that debugs your code!,VideoTo,False,0.69,5,zbc4pj,https://www.reddit.com/r/artificial/comments/zbc4pj/a_gpt3_based_chrome_extension_that_debugs_your/,0,1670058450.0,"Link - [https://chrome.google.com/webstore/detail/clerkie-ai/oenpmifpfnikheaolfpabffojfjakfnn](https://chrome.google.com/webstore/detail/clerkie-ai/oenpmifpfnikheaolfpabffojfjakfnn) 

Built a quick tool I thought would be interesting - it’s a chrome extension that uses GPT-3 under the hood to help debug your programming errors when you paste them into Google (“eg. TypeError:…”).  

This is definitely early days, so if this is something you would find valuable and wouldn't mind testing a couple iterations of, please feel free to join the discord -> [https://discord.gg/KvG3azf39U](https://discord.gg/KvG3azf39U)

https://i.redd.it/9wke811ofn3a1.gif"
235,artificial,gpt-3,relevance,2023-03-20 05:51:55,Free-to-use GPT-3 powered command-line shell (no API-key required),0ut0flin3,False,0.5,0,11wa479,https://github.com/0ut0flin3/Reptyl,0,1679291515.0,
236,artificial,gpt-3,relevance,2022-09-17 12:57:08,Prompt injection: GPT-3 has a serious security flaw,much_successes,False,0.5,0,xgltgf,https://the-decoder.com/prompt-injection-gpt-3-has-a-serious-security-flaw/,1,1663419428.0,
237,artificial,gpt-3,relevance,2022-10-24 16:17:57,GPT-3 does an astonishingly good job creating both sides of an Interactive Fiction transcript,raldi,False,0.92,40,ycfg6g,https://www.reddit.com/r/interactivefiction/comments/ycf8ol/gpt3_does_an_astonishingly_good_job_creating_both/,8,1666628277.0,
238,artificial,gpt-3,relevance,2022-07-16 16:24:47,BLOOM is a real open-source alternative to GPT-3,Zirius_Sadfaces,False,0.97,66,w0ke9t,https://mixed-news.com/en/bloom-is-a-real-open-source-alternative-to-gpt-3/,0,1657988687.0,
239,artificial,gpt-3,relevance,2020-09-10 14:15:32,Chat with GPT-3 Grandmother: a free GPT-3-powered chatbot,Wiskkey,False,0.83,7,iq43ve,/r/GPT3/comments/ipzdki/chat_with_gpt3_grandmother_a_free_gpt3powered/,13,1599747332.0,
240,artificial,gpt-3,relevance,2022-11-29 10:48:33,OpenAI's latest GPT-3 model generates better and longer texts,Zirius_Sadfaces,False,0.71,3,z7qna9,https://the-decoder.com/openais-latest-gpt-3-model-generates-better-and-longer-texts/,0,1669718913.0,
241,artificial,gpt-3,relevance,2023-01-02 16:01:20,I made a chatbot so that everyone can access their data using GPT-3,Miserness,False,0.72,11,101g270,https://v.redd.it/vh20nf1ukn9a1,4,1672675280.0,
242,artificial,gpt-3,relevance,2022-09-05 13:03:56,Can GPT-3 be honest when it speaks nonsense?,bendee983,False,0.5,0,x6fprj,https://bdtechtalks.com/2022/09/05/llm-uncertainty-verbalized-probability/,1,1662383036.0,
243,artificial,gpt-3,relevance,2022-11-02 19:35:48,"GPT-3 web app ""Explainpaper"" explains complex science in simple terms",much_successes,False,0.85,9,ykepz6,https://the-decoder.com/explainpaper-gpt-3-app-explains-complex-science-in-simple-terms/,1,1667417748.0,
244,artificial,gpt-3,relevance,2022-03-15 21:51:39,New GPT-3 Capabilities: Edit & Insert,nick7566,False,1.0,14,tf1dhq,https://openai.com/blog/gpt-3-edit-insert/,1,1647381099.0,
245,artificial,gpt-3,relevance,2022-11-25 19:49:06,Looking for feedback: We built an AI powered business name generator using GPT-3,joeyjojo6161,False,0.71,6,z4mdab,https://www.reddit.com/r/artificial/comments/z4mdab/looking_for_feedback_we_built_an_ai_powered/,5,1669405746.0,"Hey all,

You might remember the [AI website builder](https://durable.co/ai-website-builder) my company, Durable, launched a few months back (worth a try if you haven't yet given it a go, we made a handful of updates which I'll post below, based on feedback in this subreddit).

We're doing a lot with AI, and the latest is a [business name generator](https://durable.co/name-generator). If you've got a second, give it a go and let me know what you think (and share any weird/good ideas it comes up with).

My favourite so far:

Trustworthy Locksmith (I certainly hope so!)  
The Hoarder Helpers (cleaning business)  
The Spiffy Headlight (car detailing business)

This is V1, so lots to improve over time. Appreciate it, and hope someone finds it helpful!"
246,artificial,gpt-3,relevance,2021-11-06 17:52:00,GPT-3 is No Longer the Only Game in Town,regalalgorithm,False,0.97,63,qo5h44,https://lastweekin.ai/p/gpt-3-is-no-longer-the-only-game,5,1636221120.0,
247,artificial,gpt-3,relevance,2022-07-24 20:36:56,I'm getting a bit tired of this pattern on GPT-3,joaoppm2000,False,0.64,3,w759px,https://www.reddit.com/r/artificial/comments/w759px/im_getting_a_bit_tired_of_this_pattern_on_gpt3/,4,1658695016.0,"&#x200B;

[What do you think about this?](https://preview.redd.it/4ydjjfh9ukd91.png?width=940&format=png&auto=webp&s=efa350df7b83ce1997bbfef311e78227acfd27e0)"
248,artificial,gpt-3,relevance,2021-11-27 21:09:32,The Inherent Limitations of GPT-3,regalalgorithm,False,0.8,6,r3n17b,https://lastweekin.ai/p/the-inherent-limitations-of-gpt-3,1,1638047372.0,
249,artificial,gpt-3,relevance,2022-11-03 11:50:52,Content Automation with Stable Diffusion + GPT-3 API + Python 🤖,allaboutai-kris,False,0.67,1,yl0kud,https://youtu.be/Jg2ChBGduho,0,1667476252.0,
250,artificial,gpt-3,relevance,2021-02-24 12:51:01,Using GPT-3 to generate new cocktails,General_crypto,False,0.94,68,lrc4j3,https://www.youtube.com/watch?v=pyXd1_HONwY&t=2s,5,1614171061.0,
251,artificial,gpt-3,relevance,2022-08-09 01:22:56,Multi-lingual support with GPT-3 for my Mac App,juliarmg,False,1.0,1,wjq4lt,https://www.reddit.com/r/artificial/comments/wjq4lt/multilingual_support_with_gpt3_for_my_mac_app/,0,1660008176.0,grandiose ring price unused shocking spotted tie silky quaint plucky ` this post was mass deleted with www.Redact.dev `
252,artificial,gpt-3,relevance,2020-10-05 06:50:08,I would love to see Facade remade with the new GPT-3 api.,Asperix12,False,0.93,96,j5erph,https://i.redd.it/rb3d5zl538r51.jpg,22,1601880608.0,
253,artificial,gpt-3,relevance,2020-09-21 13:01:02,The GPT-3 economy,bendee983,False,0.93,41,iwzyhr,https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/,4,1600693262.0,
254,artificial,gpt-3,relevance,2021-08-24 03:02:35,GPT-3 Bot Cannot Make Up Mind On Recent Drake and Kanye Beef,blackmidifan1,False,0.65,23,paeyq3,https://i.redd.it/nsb3hvls18j71.jpg,18,1629774155.0,
255,artificial,gpt-3,relevance,2021-10-11 15:36:24,"Microsoft, Nvidia team released world’s largest dense language model. With 530 Billion parameters, it is 3x larger than GPT-3",Dr_Singularity,False,0.98,134,q5yikm,https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/,25,1633966584.0,
256,artificial,gpt-3,relevance,2022-09-27 00:41:25,Open source local GPT-3 alternative that can train on custom sets?,a4mula,False,0.67,1,xp1bhx,https://www.reddit.com/r/artificial/comments/xp1bhx/open_source_local_gpt3_alternative_that_can_train/,0,1664239285.0,"I want to scrape all of my personal reddit history and other ramblings through time and train a chat bot on them.

Any suggestions?  

I'd prefer something that runs locally, but if there is something already put together on colab and wouldn't be hindered by using free tpus that would work as well.

I'd prefer to stay away from any type of api to token access system, however.

I appreciate any guidance or consideration."
257,artificial,gpt-3,relevance,2022-11-03 16:42:46,Use GPT-3 and Stable Diffusion to write your kid's next bedtime story!,blazedemavocados,False,0.6,2,yl7m7s,https://onceuponabot.com/,0,1667493766.0,
258,artificial,gpt-3,relevance,2020-11-08 23:20:06,Will GPT-3 get a public release?,WraithfulWrath,False,0.89,7,jqlzny,https://www.reddit.com/r/artificial/comments/jqlzny/will_gpt3_get_a_public_release/,10,1604877606.0,"Everyone's been hyping GPT-3 (and I can see why), but there is nowhere at all to actually test it out without 'buying it' or signing on for a trial- or whatever. Will they ever just release GPT-3 to the public where folk can actually see what the buzz has been all about?

I'd love to chat with it or let it write a book, things like that. Will we be seeing that anytime soon? Or will Open-AI keep the program behind a wall?"
259,artificial,gpt-3,relevance,2023-01-15 01:49:44,Build a simply GPT-3 chatbot in Python in 20 lines of code in 5 minutes,techie_ray,False,0.79,14,10c78po,https://youtu.be/KQNSPKYyQ3M,1,1673747384.0,
260,artificial,gpt-3,relevance,2020-08-08 16:45:20,OpenAI GPT-3 - Good At Almost Everything!,nffDionysos,False,0.97,94,i629hl,https://www.youtube.com/watch?v=_x9AwxfjxvE,7,1596905120.0,
261,artificial,gpt-3,relevance,2020-11-08 18:28:19,Is GPT-3 Set To Replace Your Job? 10 Crazy Examples Of What GPT-3 Can Do,ledux,False,0.42,0,jqgona,https://www.reddit.com/r/artificial/comments/jqgona/is_gpt3_set_to_replace_your_job_10_crazy_examples/,1,1604860099.0,"In this video, I cover the most interesting GPT-3 apps. It's only been a few months since it's out and we already have a ton of crazy stuff going. I wonder what's there to come. 

[https://youtu.be/zrXjsARC9LQ](https://youtu.be/zrXjsARC9LQ)"
262,artificial,gpt-3,relevance,2022-07-30 15:45:37,A new online marketplace sells prompts for DALL-E 2 and GPT-3,much_successes,False,0.5,0,wc0eic,https://mixed-news.com/en/a-new-online-marketplace-sells-prompts-for-dall-e-2-and-gpt-3/,2,1659195937.0,
263,artificial,gpt-3,relevance,2023-01-09 05:25:29,The Lazy Productivity Script - A tool that uses OpenAI’s Whisper and GPT-3,allaboutai-kris,False,0.33,0,1075r4u,https://twitter.com/AllAbtAI/status/1612317768132558849?s=20&t=4RTBzL77gEOEwgyINkHKzA,0,1673241929.0,
264,artificial,gpt-3,relevance,2020-08-17 13:10:39,The untold story of GPT-3 is the transformation of OpenAI,bendee983,False,0.95,65,ibduwb,https://bdtechtalks.com/2020/08/17/openai-gpt-3-commercial-ai/,17,1597669839.0,
265,artificial,gpt-3,relevance,2021-09-03 21:40:13,AI GPT-3 Chatbot Spittin Fax,blackmidifan1,False,0.33,0,phehrj,https://i.redd.it/1bcdsr3bycl71.jpg,0,1630705213.0,
266,artificial,gpt-3,relevance,2022-10-08 16:41:17,Easy Website with AI - GPT-3 | Python | Midjourney - PART 1,allaboutai-kris,False,1.0,3,xyxabb,https://youtu.be/qsdX9-atPGU,0,1665247277.0,
267,artificial,gpt-3,relevance,2020-08-06 20:25:51,Public GPT-3 prompt?,xkrbl,False,1.0,6,i4zhd7,https://www.reddit.com/r/artificial/comments/i4zhd7/public_gpt3_prompt/,3,1596745551.0,Is there any website or app in which I can interact with GPT-3 directly myself? I don‘t have an api key
268,artificial,gpt-3,relevance,2020-08-30 12:49:38,There is a gpt-3 question answering tool to try for free?,micole00,False,0.75,2,ijc68p,https://www.reddit.com/r/artificial/comments/ijc68p/there_is_a_gpt3_question_answering_tool_to_try/,23,1598791778.0,"I need to ask questions, what tools with gpt-3 or other models are available?"
269,artificial,gpt-3,relevance,2022-06-15 16:01:24,Join us for the OpenAI GPT-3 Deep Learning Labs Hackathon!,zakrzzz,False,1.0,5,vcxujs,https://www.reddit.com/r/artificial/comments/vcxujs/join_us_for_the_openai_gpt3_deep_learning_labs/,0,1655308884.0,"We are waiting for all of you, AI enthusiasts with coding experience and without, on the 24th - 26th of June to help you turn your ground-breaking ideas into reality!

[https://lablab.ai/event/gpt3-online](https://lablab.ai/event/gpt3-online)

&#x200B;

https://preview.redd.it/9ybdri4k5t591.png?width=1600&format=png&auto=webp&s=d41c842c3725e2b2e9710f1aa8e8072ed62df6bc"
270,artificial,gpt-3,relevance,2023-02-08 07:19:14,Is there a limit to how complex and lengthy prompts for the GPT-3 Davinci model can be?,Cneqfilms,False,0.6,1,10wqd3i,https://www.reddit.com/r/artificial/comments/10wqd3i/is_there_a_limit_to_how_complex_and_lengthy/,1,1675840754.0,"Hey there.

I currently have a working VR bot which uses whisper AI to take in speech from within a VR social platform and converts that into text which is then passed to the GPT-3 Davinci model and the output of that is passed to IBM Watson TTS and then spoken by the bot via a virtual audio cable within the VR platform.

However after experimenting with the prompts for the bot it's apparent you can do just about anything with them even down to getting the bot to know it's own hair and eye color \[which corresponds to it's appearance in VR\] and all manner of other useful things you can get it to remember.

With how powerful this is I was curious if there was a limit to how much I could actually add to it and if there are any downsides because if there isn't I could spend a lot of time making this bot perfect purely with prompts alone."
271,artificial,gpt-3,relevance,2021-11-29 14:05:25,What you should know about developing GPT-3 applications,bendee983,False,0.79,10,r4wjx6,https://bdtechtalks.com/2021/11/29/gpt-3-application-development-tips/,0,1638194725.0,
272,artificial,gpt-3,relevance,2021-09-01 14:52:05,"GPT-3 mimics human love for ‘offensive’ Reddit comments, study finds",estasfuera,False,0.91,46,pfvhob,https://thenextweb.com/news/gpt-3-and-humans-twice-as-likely-agree-with-offensive-reddit-comments-chatbots,4,1630507925.0,
273,artificial,gpt-3,relevance,2021-06-16 12:12:31,"Understanding Transformers, the machine learning model behind GPT-3",rshpkamil,False,0.95,30,o13f2l,https://www.reddit.com/r/artificial/comments/o13f2l/understanding_transformers_the_machine_learning/,3,1623845551.0,"Great explanation of methods that lie behind the success of GPT-3, BERT, AlphaFold 2, etc.

Original article here: [https://daleonai.com/transformers-explained](https://daleonai.com/transformers-explained)

More hard-to-find, independent stuff related to AI & Data Science [here](https://thereshape.co/?utm_source=reddit)."
274,artificial,gpt-3,relevance,2020-05-29 21:41:17,[R] OpenAI Unveils 175 Billion Parameter GPT-3 Language Model,Yuqing7,False,0.97,69,gt1x6r,https://www.reddit.com/r/artificial/comments/gt1x6r/r_openai_unveils_175_billion_parameter_gpt3/,13,1590788477.0,"When it comes to large language models, it turns out that even 1.5 billion parameters is not large enough. While that was the size of the GPT-2 transformer-based language model that OpenAI released to much fanfare last year, today the San Francisco-based AI company outdid itself, announcing the upgraded GPT-3 with a whopping 175 billion parameters.

GPT-3 adopts and scales up the GPT-2 model architecture — including modified initialization, pre-normalization, and reversible tokenization — and shows strong performance on many NLP tasks and benchmarks in zero-shot, one-shot, and few-shot settings.

Here is a quick read: [OpenAI Unveils 175 Billion Parameter GPT-3 Language Model](https://medium.com/@Synced/openai-unveils-175-billion-parameter-gpt-3-language-model-3d3f453124cd)

The paper *Language Models are Few-Shot Learners* is on [arXiv](https://arxiv.org/pdf/2005.14165.pdf), and more details are available on the project [GitHub](https://github.com/openai/gpt-3)."
275,artificial,gpt-3,relevance,2020-08-13 17:57:03,AI Dungeon GPT-3,Xyperias,False,0.67,1,i94qyo,https://www.reddit.com/r/artificial/comments/i94qyo/ai_dungeon_gpt3/,1,1597341423.0,"So AI Dungeon has two engines that it calls Griffin and Dragon, Dragon being the more sophisticated one.
Griffin is still pretty good as well though and I wonder if anyone knows whether it is also laking use of GPT-3 or if it’s all GPT-2?"
276,artificial,gpt-3,relevance,2021-07-28 10:30:32,Is there any open source equivalent of GPT-3 yet?,MountBlanc,False,0.69,7,ot781d,https://www.reddit.com/r/artificial/comments/ot781d/is_there_any_open_source_equivalent_of_gpt3_yet/,5,1627468232.0,"Hello,

I'm MB. A very nice and polite guy.

Is there any open source equivalent of GPT-3 yet?"
277,artificial,gpt-3,relevance,2020-08-25 22:58:20,Using GPT-3 to fill in the blanks in text: a potentially broadly useful GPT-3 interaction method,Wiskkey,False,1.0,2,ign4v0,/r/MachineLearning/comments/igj71v/d_using_gpt3_to_fill_in_the_blanks_in_text_a/,0,1598396300.0,
278,artificial,gpt-3,relevance,2021-11-24 03:01:02,OpenAI's GPT-3 gets a little bit more open,eddytony96,False,0.92,19,r0uny6,https://www.axios.com/openai-gpt-3-waiting-list-api-929fd309-f8e1-4571-862a-879492e5ebc6.html?utm_source=newsletter&utm_medium=email&utm_campaign=newsletter_axiosfutureofwork&stream=future&utm_term=Future%20Perfect,0,1637722862.0,
279,artificial,gpt-3,relevance,2021-07-07 11:37:05,A free GPT-3 alternative to summarise text,SimilarQuarter9178,False,1.0,1,ofh499,https://www.reddit.com/r/artificial/comments/ofh499/a_free_gpt3_alternative_to_summarise_text/,1,1625657825.0,
280,artificial,gpt-3,relevance,2022-06-01 18:52:57,"I used GPT-3 to write an entirely fake Spider-Man movie, then animated the results.",B_Ray18,False,0.79,23,v2ocsy,https://www.youtube.com/watch?v=7sOmGa6cqFE,3,1654109577.0,
281,artificial,gpt-3,relevance,2021-02-25 05:47:53,[N] New Contextual Calibration Method Boosts GPT-3 Accuracy Up to 30%,Yuqing7,False,0.96,48,lrzghq,https://www.reddit.com/r/artificial/comments/lrzghq/n_new_contextual_calibration_method_boosts_gpt3/,7,1614232073.0,"A research team from UC Berkeley, University of Maryland and UC Irvine identifies pitfalls that cause instability in the GPT-3 language model and proposes a contextual calibration procedure that improves accuracy by up to 30 percent.

Here is a quick read: [New Contextual Calibration Method Boosts GPT-3 Accuracy Up to 30%](https://syncedreview.com/2021/02/24/new-contextual-calibration-method-boosts-gpt-3-accuracy-up-to-30/)

The paper *Calibrate Before Use: Improving Few-Shot Performance of Language Models* is on [arXiv](https://arxiv.org/pdf/2102.09690.pdf)."
282,artificial,gpt-3,relevance,2022-06-29 22:48:35,The Persistence Problem: Lessons learned from illustrating a children's book with GPT-3 and crAIyon.,laul_pogan,False,0.95,14,vnsmje,https://laulpogan.substack.com/p/the-persistence-problem-lessons-learned,2,1656542915.0,
283,artificial,gpt-3,relevance,2020-07-28 12:32:24,GPT-3 writes my SQL queries for me,Aioli-Pleasant,False,0.92,10,hzdu0l,https://youtu.be/WlMHYEFt2uA,7,1595939544.0,
284,artificial,gpt-3,relevance,2023-03-08 22:33:47,Seeking Help Creating a Chat GPT-3 Desktop Chatbot Application or Android APK for Personal Creative Writing Use (Will Pay $150),amy_katt,False,0.67,1,11mafvg,https://www.reddit.com/r/artificial/comments/11mafvg/seeking_help_creating_a_chat_gpt3_desktop_chatbot/,0,1678314827.0," 

Hello everyone,

I am an artist and creative writer and I have recently become interested in creating my own chatbot application using OpenAI's GPT-3 technology. I am reaching out to the community today in the hopes of finding someone who can help me with this project.

Specifically, I am looking for someone who can assist me in creating a desktop chatbot application or an Android APK that uses GPT-3 to help me generate creative writing ideas and prompts. The chatbot should be able to understand natural language queries and respond with relevant prompts or suggestions.

I am willing to pay up to $150 for assistance with this project. I understand that this may not be a large sum, but I hope that it will be enough to compensate someone for their time and expertise."
285,artificial,gpt-3,relevance,2021-02-08 03:03:11,Interview with GPT-3 AI Emulation,thomasfromla,False,1.0,2,lf2fiy,https://youtube.com/watch?v=IVI3e-6evkU&feature=share,0,1612753391.0,
286,artificial,gpt-3,relevance,2021-05-31 16:35:37,I used GPT-3 to generate pizza toppings,General_crypto,False,1.0,3,np7hya,https://www.youtube.com/watch?v=JJm0A7TiHC8,0,1622478937.0,
287,artificial,gpt-3,relevance,2020-11-20 14:30:54,what gpt-3 can do and not do,loopy_fun,False,0.86,10,jxq04w,https://www.meatspacealgorithms.com/what-gpt-3-can-do-and-what-it-cant/,4,1605882654.0,
288,artificial,gpt-3,relevance,2023-03-10 17:30:09,"Ever since ChatGPT was released, there's been an ""AI arms race"" of companies trying to launch functions that capitalize on the hype surrounding AI and GPT-3",SuspiciousGazer,False,0.67,1,11nva85,https://www.reddit.com/r/artificial/comments/11nva85/ever_since_chatgpt_was_released_theres_been_an_ai/,0,1678469409.0,"ContractWorks seems to have been one step ahead of the herd in the legal tech field. They claim to already use GPT-3 for drafting contract clauses and simplifying ""legalese"" in contracts in specific features, namely ""Simplify"" and ""Clause Creator. Source: [GPT-3 – The Hottest Innovation to Hit Legal Ops](https://www.contractworks.com/blog/chatgpt-ai-innovation-in-legal)"
289,artificial,gpt-3,relevance,2020-09-08 04:32:08,GPT-3 accuracy on 57 subject-related tasks (highest US Foreign Policy; lowest College Chemistry),neuromancer420,False,0.99,96,ion6go,https://i.redd.it/f005qse1lul51.jpg,11,1599539528.0,
290,artificial,gpt-3,relevance,2021-09-09 04:06:02,Ember AI Startup creating GPT-3 Platform for Video Games,Comfortable_Sir_1584,False,0.81,3,pkqbv9,https://www.reddit.com/r/artificial/comments/pkqbv9/ember_ai_startup_creating_gpt3_platform_for_video/,0,1631160362.0,"6 months ago, Modbox released this video experimenting with using GPT-3 to generate dialogue for NPCs...

[https://www.youtube.com/watch?v=jH-6-ZIgmKY](https://www.youtube.com/watch?v=jH-6-ZIgmKY)

The video garnered some attention back then, the potential of the tech was exciting, but admittedly rough around the edges.

Then, two months ago, they began a new round of experiments and most recently released this tech demo.

[https://www.youtube.com/watch?v=kMql19rEeBI&t=7s](https://www.youtube.com/watch?v=kMql19rEeBI&t=7s)

As of yesterday, they've announced their intentions to spin this concept out  into a new company called Ember AI, founded for the specific purpose of creating a platform for developers to use GPT-3 generated dialogue in their games.

[https://www.youtube.com/watch?v=Diu6DUCy7QM](https://www.youtube.com/watch?v=Diu6DUCy7QM)"
291,artificial,gpt-3,relevance,2021-11-13 03:33:26,OpenAI GPT-3 as a turning point in my life,d_sat,False,0.67,1,qsscov,/r/GPT3/comments/qs0of4/gpt3_as_a_turning_point_in_my_life/,0,1636774406.0,
292,artificial,gpt-3,relevance,2021-03-09 03:58:34,Exploring gender biases as manifest through GPT-3,Rad-Squirrel,False,1.0,2,m0y7iw,https://www.reddit.com/gallery/m0y7iw,0,1615262314.0,
293,artificial,gpt-3,relevance,2022-09-13 16:17:11,"I asked GPT-3 to write something captivating you could not stop reading, here are the results",SupPandaHugger,False,0.45,0,xdbsx0,https://medium.com/@dreamferus/i-asked-gpt-3-to-write-something-captivating-you-could-not-stop-reading-here-are-the-results-17837725f827,0,1663085831.0,
294,artificial,gpt-3,relevance,2020-08-09 22:33:35,Using AIDungeon GPT-3 to summarize text?,bitcentral,False,1.0,4,i6spa3,https://www.reddit.com/r/artificial/comments/i6spa3/using_aidungeon_gpt3_to_summarize_text/,3,1597012415.0,"Having messed around with the AIDungen Dragon mode (GPT-3) I've been able to generate some amazing results, but I havent been able to get it to summarize a block of text reliably. What type of prompt should be used to get GPT-3 to summarize or rewrite a block of text?  


Here's an example. Given a news article about Pythons in a Florida swamp, GPT-3 makes up facts and details not found in the original article.  


\*\*\*\*\*

Dangerous New Python Species Spotted in Florida Swamp

The new snake is a cross between two species found nowhere else on Earth – the African pygmy python and the South American boa constrictor. It measures 2-3m long, with a head similar to an average adult Boa constrictor but longer tail like that of a typical pygmy python. Its colouration resembles that of an Eastern brown tree snake, but differs in having lighter scales, larger eyes and being striped rather than solid black.

It will likely face off against the American alligator in the wild, which has been growing faster than ever due to habitat loss and climate change. But its greatest threat may come from other native species such as the cottonmouth snake, which is already established throughout much of Florida.

According to the Guardian, if this hybrid is successful in expanding its range further into the Everglades, it could spell disaster for the state's native wildlife populations.

And even though it may not pose a direct threat to humans yet, there are concerns about what would happen when these creatures begin reproducing in large enough numbers to outcompete other animals.

\*\*\*\*\*  


GPT-3 Summary

Breakdown of facts from the article:

1. Burmese pythons are found only in South East Asia, Central America and Australia

2. The python is a crepuscular snake which means it hunts at night when prey is least likely to see them or escape their grasp. It is also known for its ability to grow up to 14ft long with a weight of up to 200lbs. This makes it the largest venomous reptile on earth and one of the most dangerous predators as well.

3. It is believed that there are between 100 and 500 Burmese pythons living in the wild in Florida. These animals live mainly off of mice, rats and other rodents. They feed on small mammals such as raccoons, foxes, opossums and even hares but they will eat anything if it is large enough to catch or kill quickly enough. Their diet includes fish, birds and amphibians but does not include reptiles or crocodiles due to their size.

4. Burmese pythons prefer warmer climates so they do not naturally occur in Florida. In fact, they have never been recorded in the state despite being abundant throughout much of South East Asia and Central America. As a result, it is thought that an exotic pet escaped from an animal sanctuary and has spread across the globe.

5. Its prey list is longer than those of any other predator and as a constrictor, it can suffocate even large prey such as deer within minutes."
295,artificial,gpt-3,relevance,2022-10-19 18:56:14,"It's all GPT-3 under the hood, but a great example of how AI writing assistants become widely used",17syllogisms,False,0.83,4,y8b0fb,https://theexitgame.substack.com/p/shortly-the-ai-powered-writing-assistant,0,1666205774.0,
296,artificial,gpt-3,relevance,2020-07-12 23:00:59,GPT-3 from OpenAI; PAPER DISCUSSION,przemekc,False,0.86,26,hq3zq6,https://youtu.be/OznMk5Jexu8,0,1594594859.0,
297,artificial,gpt-3,relevance,2022-09-28 14:50:03,"""Prompt Explorer"" - a GPT-3 powered google sheet that lets you explore the ""narrative neighbourhood"" of any prompt",walt74,False,0.78,5,xqeqdv,/r/PromptDesign/comments/xqeou3/prompt_explorer_a_gpt3_powered_google_sheet_that/,1,1664376603.0,
298,artificial,gpt-3,relevance,2020-11-17 13:26:01,What gpt-3 sites require api key?,micole00,False,0.5,0,jvsxch,https://www.reddit.com/r/artificial/comments/jvsxch/what_gpt3_sites_require_api_key/,0,1605619561.0,I have now my api key!!
299,artificial,gpt-3,relevance,2021-11-05 15:39:14,What GPT-3 on Azure will mean for Microsoft and OpenAI,bendee983,False,1.0,3,qndsmv,https://bdtechtalks.com/2021/11/05/microsoft-azure-openai-service-gpt-3/,0,1636126754.0,
