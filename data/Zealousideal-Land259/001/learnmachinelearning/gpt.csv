,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,id,url,num_comments,created,body
0,learnmachinelearning,gpt,top,2021-06-29 13:07:05,"Started learning ML 14 months ago, now I'm using GPT-3 to automate CVs!",Camjw1123,False,0.97,748,oa7x3p,https://gfycat.com/ambitioushauntingagama,53,1624972025.0,
1,learnmachinelearning,gpt,top,2020-11-05 05:44:07,"""I forced a bot to watch over 1,000 hours of SpongeBob SquarePants and then asked it to write a SpongeBob SquarePants episode of its own.""",mikhailgaerlan,False,0.98,700,jodejr,https://www.reddit.com/r/learnmachinelearning/comments/jodejr/i_forced_a_bot_to_watch_over_1000_hours_of/,47,1604555047.0,"I saw this [tweet](https://twitter.com/KeatonPatti/status/1108739921932619777) and found out that he‚Äôs actually a comedian who didn‚Äôt really train a bot. I couldn't find anyone who had tried so I did it myself. Turns out there's only around 100 hours of SpongeBob SquarePants though.

I fine-tuned the 'small' `124M` GPT 2 model using `gpt-2-simple` on SpongeBob SquarePants episode transcripts that I scraped from the [Transcripts Wiki](https://transcripts.fandom.com/wiki/SpongeBob_SquarePants#Episodes). The GitHub repo with the code I used to generate the results is [here](https://github.com/mikhailgaerlan/TV-Show-Script-Writer). I plan to do more TV shows myself, but I would love to see other people try this with their favorite TV shows. The following is one of my favorite results out of the ones I generated.

**Sample Results**

Sandy, you need to get that nut off my back. \[shivering\]

A nut?! \[picks up a nut\]

Thats not what I was talking about! \[pulls the nut off his back\]

Thats not what I meant by... You got him! \[shivering\]

Thats not what I meant! \[hangs up the nut and pulls back the nut\]

Thats not what I meant by... \[stops and looks around\]

Where is it? Where is it? Whats the big... \[stops and looks around again\]

...scam? Is it going on out here? \[starts pulling out his back\]

Thats not where... \[the nut lands on Sandys hand\]

Where is it? Thats not where. Sandy, I dont think you should be doing that.

Sandy! You dont know what youre talking about.

Oh, Sandy, I know that this isnt what Im talking about.

Oh, thats okay. Its not like I dont know anything about nuttastic. \[takes a deep breath\]

Okay, Ill just take this. \[takes the nut, puts it in a jar, and puts it in a blender\]

\[blends the nut, the blender, and the blender all talk at once\]

Okay, this is it! \[in a trance\]

Sandy, you didnt do a good job. Im sorry, SpongeBob, but you failed again. \[in a deeper trance\]

Sandy, did you hear that? \[jumps up and slams his face into a barrel\]

You failed again. \[stops and turns around\]

Okay, okay, Sandy, I know that. I just cant imagine what Im into all the time. Im a nutcase.

\[he jumps up and slams his face into the barrel\]

Youre not. \[jumps up on top of a barrel, picks up SpongeBob, and throws him\]

You failed again. Im a nutcase. Patrick, what are you doing?

Im a nutcase. I need to get a nut. What are you doing? \[jumps up on top of SpongeBob\]

I need to get a big nut. Patrick, I want to talk to you.

No, I dont want to talk to you. I want to talk to... \[Patrick turns around, and turns around twice, turning SpongeBob around\]

Patrick, you failed again. Sandy! \[starts knocking on the door, and Sandy comes in\]

Look, I really am sorry for everything I did. \[hanging onto the barrel, shoving it down, and then banging on it\]

Not only that, but you showed up late for work? \[crying\]

My brain was working all night to make up for the hours I wasted on making up so much cheese.

\[hanging on the barrel, then suddenly appearing\] Patrick, what are you...

\[Patrick turns around, and looks at him for his failure\] Sandy? \[crying\]

I know what you did to me brain. \[turns around, and runs off the barrel. Sandy comes in again\]

\[screams\] What the...? \[gets up, exhausted\]

Oh, Patrick, I got you something. \[takes the nut off of SpongeBobs head\]

Thats it. \[takes the nut from SpongeBobs foot\] Thats it. \[takes the nut off his face. He chuckles, then sighs\]

Thats the last nut I got. \[walks away\] Patrick, maybe you can come back later.

Oh, sure, Im coming with you. \[hangs up the barrel. Sandy walks into SpongeBobs house\] \[annoyed\]

Nonsense, buddy. You let Gary go and enjoy his nice days alone. \[puts her hat on her head\]

You promise me? \[she pulls it down, revealing a jar of chocolate\]

You even let me sleep with you? \[she opens the jar, and a giggle plays\]

Oh, Neptune, that was even better than that jar of peanut chocolate I just took. \[she closes the door, and Gary walks into his house, sniffles\]

Gary? \[opens the jar\] \[screams, and spits out the peanut chocolate\]

Gary?! \[SpongeBob gets up, desperate, and runs into his house, carrying the jar of chocolate. Gary comes back up, still crying\]

SpongeBob! \[SpongeBob sees the peanut chocolate, looks in the jar, and pours it in a bucket. Then he puts his head in the bucket and starts eating the chocolate. Gary slithers towards SpongeBobs house, still crying\]

SpongeBobs right! \[SpongeBob notices that some of the peanut chocolate is still in the bucket, so he takes it out. Then he puts the lid on the bucket, so that no"
2,learnmachinelearning,gpt,top,2020-08-05 10:58:02,image-GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model,OnlyProggingForFun,False,0.97,632,i437om,https://www.youtube.com/watch?v=FwXQ568_io0,46,1596625082.0,
3,learnmachinelearning,gpt,top,2023-04-04 14:34:29,Working with chatGPT,macronancer,False,0.97,605,12bkzjv,https://i.redd.it/5uwfzjh4pvra1.png,22,1680618869.0,
4,learnmachinelearning,gpt,top,2021-04-03 15:27:04,"I'm a Senior DS and I put together a Youtube Channel with project tutorials, resume critiques, and career advice. Let me know what you think!",madzthakz,False,0.98,554,mjao5g,https://www.reddit.com/r/learnmachinelearning/comments/mjao5g/im_a_senior_ds_and_i_put_together_a_youtube/,21,1617463624.0,"I've also been setting up free [Data Science Q&As](https://www.reddit.com/r/datascience/comments/jig7pv/im_a_senior_data_scientist_at_disney_and_im/) for you all. On the side, I started putting together useful videos that would have helped me out when I was trying to break into this space. Like I said, the channel consists of modeling tutorials, resume critiques, career advice, and recordings of our Q&A sessions. Here are some examples:

1. [How to build a Spotify recommendation engine](https://youtu.be/tooddaC14q4).
2. [How to leverage GPT-2 to generate descriptions of new Netflix content](https://youtu.be/NvMoFeO0aGE).
3. [Full recordings of 1:1 coaching sessions with an ML student.](https://youtu.be/N2tDfXdZmdE)
4. [Resume Critique of a student who just completed a certificate.](https://youtu.be/Ztexwmrxt2A)
5. [Q&A Recording with a Principal Data Scientist.](https://youtu.be/r-NjlPW-Ihg) 

This is all really new and has been a blast to work on. Let me know what you think. 

[Channel Link](https://www.youtube.com/channel/UC0-S_HnWTDFaXgTbYSL46Ug)

If you like it, definitely subscribe! I try to put out videos every week. 

Also, feel free to connect with me on [LinkedIn](https://www.linkedin.com/in/madhavthaker/). I try to make myself as accessible as possible on there."
5,learnmachinelearning,gpt,top,2022-11-07 14:11:49,Been learning ML since the start of the year and built a tool with GPT-3 that let‚Äôs anyone self-serve their own data questions and create graphs and dashboards,BuggerinoKripperino,False,0.98,469,yoo3ba,https://v.redd.it/n0vjjvr8ejy91,64,1667830309.0,
6,learnmachinelearning,gpt,top,2021-10-18 03:00:06,"Discord Chatbot created using a fine tuned GPT-J 6B model, model link in comments",Udongeein,False,0.97,393,qadx1i,https://i.redd.it/z5aw61f9i4u71.png,37,1634526006.0,
7,learnmachinelearning,gpt,top,2023-04-26 06:23:17,Hugging Face Releases Free Alternative To ChatGPT,vadhavaniyafaijan,False,0.98,392,12z8n4e,https://www.theinsaneapp.com/2023/04/free-alternative-to-chatgpt.html,35,1682490197.0,
8,learnmachinelearning,gpt,top,2021-07-01 16:06:11,Second version of my GPT-3 powered resume writer - now does bullet points and doesn't use pronouns!,Camjw1123,False,0.96,346,oboywl,https://gfycat.com/bitteroffbeatitalianbrownbear,29,1625155571.0,
9,learnmachinelearning,gpt,top,2023-01-19 07:56:20,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.96,332,10fw2df,https://www.reddit.com/r/learnmachinelearning/comments/10fw2df/gpt4_will_be_500x_smaller_than_people_think_here/,47,1674114980.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/yio0v3zqgyca1.png?width=575&format=png&auto=webp&s=a2ee034ce7ed48c9adc1793bfdb495e0f0812609)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!  
The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ‚≠ï.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
10,learnmachinelearning,gpt,top,2020-11-25 12:54:49,This AI Can Generate the Other Half of a Picture Using a GPT Model,OnlyProggingForFun,False,0.96,306,k0ro7l,https://youtu.be/FwXQ568_io0,3,1606308889.0,
11,learnmachinelearning,gpt,top,2023-02-19 13:55:13,ChatGPT History,eforebrahim,False,0.86,254,116au66,https://i.redd.it/dv8cfj0nz6ja1.jpg,27,1676814913.0,
12,learnmachinelearning,gpt,top,2023-05-11 20:15:46,Top 20 Large Language Models based on the Elo rating system.,kingabzpro,False,0.96,247,13eympz,https://i.redd.it/7xfqr5crf9za1.png,43,1683836146.0,
13,learnmachinelearning,gpt,top,2023-04-06 11:12:52,Meta: Is it possible to ban these TikTok influencers or TikToks in general?,dasMaiMaiKamel,False,0.94,219,12dgtry,https://www.reddit.com/r/learnmachinelearning/comments/12dgtry/meta_is_it_possible_to_ban_these_tiktok/,14,1680779572.0,"I'm new to this sub and I'd love to contribute here. But there are soooo many TikTok videos from someone talking about ChatGPT for the 10.000th time. These videos don't contribute to learning ML nor do they give actual reliable information. I often get the feeling that these people never touched a NN, just sat on ChatGPT and read one WikiPedia article. It's also often more an ad than actual help.  


  
Even if I'm not a member for too long, I see comments criticizing this exact thing under every video. Is it possible to add a rule to prevent this? It would greatly improve the quality of this sub."
14,learnmachinelearning,gpt,top,2023-01-31 16:17:42,ChatGPT Crossed 10 Million Daily Active Users In Just 40 Days,vadhavaniyafaijan,False,0.95,216,10q34ra,https://www.theinsaneapp.com/2023/01/chatgpt-crossed-10-million-user.html,32,1675181862.0,
15,learnmachinelearning,gpt,top,2023-02-16 10:29:31,OpenAI Has Purchased AI.Com For ChatGPT For $11M,vadhavaniyafaijan,False,0.93,212,113nizs,https://www.theinsaneapp.com/2023/02/openai-purchased-ai-com-domain.html,23,1676543371.0,
16,learnmachinelearning,gpt,top,2023-02-11 12:46:22,"ChatGPT Powered Bing Chatbot Spills Secret Document, The Guy Who Tricked Bot Was Banned From Using Bing Chat",vadhavaniyafaijan,False,0.94,210,10zmtqz,https://www.theinsaneapp.com/2023/02/chatgpt-bing-rules.html,15,1676119582.0,
17,learnmachinelearning,gpt,top,2023-01-05 06:32:22,I Built A GPT-3 Powered Productivity App - Tutorial included,SupPandaHugger,False,0.97,206,103rv9o,https://i.redd.it/gtywivh756aa1.gif,17,1672900342.0,
18,learnmachinelearning,gpt,top,2023-03-16 16:51:03,Introducing OpenChatKit - The Open-Source Alternative to ChatGPT,kingabzpro,False,0.98,202,11szhsh,https://www.reddit.com/r/learnmachinelearning/comments/11szhsh/introducing_openchatkit_the_opensource/,21,1678985463.0,"Hey everyone! I'm excited to share my latest article about a new open-source technology called OpenChatKit.

For those who work in NLP, you're probably familiar with ChatGPT - a powerful language model that can perform various natural language processing tasks. However, ChatGPT is not open-source, which limits its accessibility and customizability.

OpenChatKit, on the other hand, is an open-source alternative to ChatGPT that provides users with similar NLP capabilities while allowing for more customization and control. With OpenChatKit, users can train their own models and fine-tune them to their specific use cases.

In my article, I dive into the features of OpenChatKit, the Instruction-tuned Large Language Model, and the Limitations of the Model.

If you're interested in learning more about OpenChatKit and how it can enhance your NLP workflows, check out my article [OpenChatKit: Open-Source ChatGPT Alternative ](https://www.kdnuggets.com/2023/03/openchatkit-opensource-chatgpt-alternative.html). I'd love to hear your thoughts and answer any questions you may have."
19,learnmachinelearning,gpt,top,2023-01-16 12:28:25,I benchmarked OpenAI's GPT API vs other proprietary APIs on different NLP tasks,AImSamy,False,0.9,197,10ddc1f,https://www.reddit.com/gallery/10ddc1f,37,1673872105.0,
20,learnmachinelearning,gpt,top,2023-10-12 20:36:53,ChatGPT vision feature is really useful for understanding research papers!,nxtboyIII,False,0.86,186,176gs37,https://i.redd.it/xe94y8hf1utb1.png,42,1697143013.0,
21,learnmachinelearning,gpt,top,2023-09-23 13:42:22,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.96,185,16q4ve6,https://www.reddit.com/r/learnmachinelearning/comments/16q4ve6/this_week_in_ai_all_the_major_ai_developments_in/,16,1695476542.0,"1. **Genmo** releases a new text-to-video model: **Genmo Replay** v0.1, which generates high-quality videos from text without the need for advanced prompt engineering. *Genmo is available for free to create AI videos* \[[*Details*](https://blog.genmo.ai/log/replay-ai-video) | [Genmo *Replay*](https://www.genmo.ai/)\] .
2. **OpenAI** unveils **DALL¬∑E 3** \- a major update to the text-to-image model, which will be integrated in ChatGPT. It will be available to ChatGPT Plus and Enterprise users in October, via the API and in Labs later this fall. Creators can now also opt their images out from future training.
3. **Toyota Research Institute** has developed a technique, powered by generative AI, that enables teaching robots new manipulation abilities in a single afternoon. Using the same robot, same code, and same setup, TRI taught over 60 different dexterous behaviors like peeling vegetables, using hand mixers, preparing snacks, and flipping pancakes.
4. **Microsoft** announced:
   1. Availability of AI Copilot for Windows from September 26th. Copilot will incorporate the context and intelligence of the web, your work data and what you are doing in the moment on your PC to provide better assistance. It will be integrated in Windows 11, Microsoft 365, Edge and Bing.
   2. Bing will add support for DALL.E 3 and deliver more personalized answers based on search history.
   3. New AI powered experiences in Paint, Photos and Clipchamp.
   4. New AI-powered shopping experience
5. **ElevenLabs** released **Projects** \- a tool that lets you generate an entire audiobook at the click of a button. Projects now supports .epub, .pdf, and .txt file imports, as well as initializing a project from a URL.
6. **Deci** presents **DeciDiffusion 1.0** \- an open-source text-to-image latent diffusion model which is 3x faster than Stable Diffusion v1.5 with the same quality.
7. **Google researchers** present a new approach that produces photo-realistic animations from a single picture. The model is trained on automatically extracted motion trajectories from a large collection of real video sequences.
8. **Google** has updated Bard\*\]\*:
9. **Bard Extensions:** With extensions, Bard can now connect to your Google apps and services like Gmail, Docs, Drive, Google Maps, YouTube, and Google Flights and hotels.
10. Users can use Bard‚Äôs ‚ÄúGoogle it‚Äù button to more easily double-check its answers and evaluate whether there is content across the web to substantiate it.
11. Bard can now let you continue chat via **shared public links**
12. **YouTube** announces new AI tools for creators. **Dream Screen** will let users create an AI-generated video or image background from text. Automatic AI-dubbing tool called **Aloud**, which will be integrated into YouTube Studio. **AI-powered insights** to generate video ideas and draft outlines. **Assistive Search in Creator Music** where AI will suggest the right music based on your description of your content.
13. **Amazon** announced that its voice assistant Alexa is being upgraded with a new, custom-built large language model.
14. **IBM** open-sources **MoLM** \- a collection of ModuleFormer-based language models ranging in scale from 4 billion to 8 billion parameters. ModuleFormer is a new neural network architecture based on the Sparse Mixture of Experts (SMoE) by IBM researchers. .
15. **Neuralink**, Elon Musk's brain implant startup, set to begin human trials.
16. **Lexica** has released **Aperture v3.5** \- their latest next-gen image model that can create photorealistic images and follows your prompt with precision.
17. **OpenAI** has invited domain experts to collaborate in evaluating and improving the safety of OpenAI's models by joining the new **OpenAI Red Teaming Network**.
18. \*\*GitHub Copilot Chat (\*\*beta) is now available for all individuals.
19. **Replit** announced a virtual hackathon for projects built using **Replit ModelFarm**
20. **Oracle** brings voice-activated AI to healthcare with Clinical Digital Assistant.
21. **Google** and the Department of Defense are building an AI-powered microscope to help doctors spot cancer.

My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
22,learnmachinelearning,gpt,top,2023-03-02 16:47:40,Build ChatGPT for Financial Documents with LangChain + Deep Lake,davidbun,False,0.95,171,11g7h03,https://www.reddit.com/r/learnmachinelearning/comments/11g7h03/build_chatgpt_for_financial_documents_with/,8,1677775660.0,"https://preview.redd.it/h9r6hgvfucla1.png?width=2388&format=png&auto=webp&s=5432eac3eeed8583e4309af1fdc7ebecac705796

As the world is increasingly generating vast amounts of financial data, the need for advanced tools to analyze and make sense of it has never been greater. This is where [LangChain](https://github.com/hwchase17/langchain) and [Deep Lake](https://github.com/activeloopai/deeplake) come in, offering a powerful combination of technology to help build a question-answering tool based on financial data. After participating in a LangChain hackathon last week, I created a way to use Deep Lake, the data lake for deep learning (a package my team and I are building) with LangChain. I decided to put together a guide of sorts on how you can approach building your own question-answering tools with  LangChain and Deep Lake as the data store.

Read [the article](https://www.activeloop.ai/resources/ultimate-guide-to-lang-chain-deep-lake-build-chat-gpt-to-answer-questions-on-your-financial-data/) to learn:

1. What is LangChain, what are its benefits and use cases and how you can use to streamline your LLM (Large Language Model) development?  
2. How to use [\#LangChain](https://www.linkedin.com/feed/hashtag/?keywords=langchain&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) and [\#DeepLake](https://www.linkedin.com/feed/hashtag/?keywords=deeplake&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) together to build [\#ChatGPT](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) for your financial documents.  
3. How Deep Lake‚Äôs unified and streamable data store enables fast prototyping without the need to recompute embeddings (something that costs time & money).  


I hope you like it, and let me know if you have any questions!"
23,learnmachinelearning,gpt,top,2023-07-25 20:56:55,Hi r/learnmachinelearning! To make CUDA development easier I made a GPT-4 powered NVIDIA bot that knows about all the CUDA docs and forum answers (demo link in comments),srnsnemil,False,0.96,173,159kt6u,https://v.redd.it/58hbh8q0d6eb1,15,1690318615.0,
24,learnmachinelearning,gpt,top,2022-02-28 22:12:52,"I developed an open source Discord Bot that works both as a writing assistant and as a story teller using a quantized GPT-J 6B model finetuned on literature. Code, model, and Discord server linked in comments.",Udongeein,False,0.98,164,t3rgvi,https://i.redd.it/p8ofxz20enk81.png,9,1646086372.0,
25,learnmachinelearning,gpt,top,2022-03-15 18:56:21,I developed conditional responding Discord Chatbots using a finetuned and quantized GPT-J 6B model! Code and model linked in the comments.,Udongeein,False,0.99,157,tewumv,https://www.reddit.com/gallery/tewumv,11,1647370581.0,
26,learnmachinelearning,gpt,top,2023-12-10 18:07:35,Are LLMs overhyped right now?,Snoo_72181,False,0.94,159,18f9enp,https://www.reddit.com/r/learnmachinelearning/comments/18f9enp/are_llms_overhyped_right_now/,67,1702231655.0,"I mean I get that ChatGPT has made LLMs the toast of ML universe. They are indeed amazing.  

But this has lead to so much hype that ML beginners are literally just talking about learning LLMs, ignoring so much in between like Math and Stats, simple ML like Regression, Classification. After that you have, Deep Learning, Transformers and finally LLMs. 

Companies also want candidates with LLM experience, but there's no guarantee that they even have a use case for LLMs "
27,learnmachinelearning,gpt,top,2023-06-28 12:29:48,"Intern tasked to make a ""local"" version of chatGPT for my work",Assasinshock,False,0.97,153,14l887h,https://www.reddit.com/r/learnmachinelearning/comments/14l887h/intern_tasked_to_make_a_local_version_of_chatgpt/,104,1687955388.0,"Hi everyone,

I'm currently an intern at a company, and my mission is to make a proof of concept of an conversational AI for the company.They told me that the AI needs to be trained already but still able to get trained on the documents of the company, the AI needs to be open-source and needs to run locally so no cloud solution.

The AI should be able to answers questions related to the company, and tell the user which documents are pertained to their question, and also tell them which departement to contact to access those files.

For this they have a PC with an I7 8700K, 128Gb of DDR4 RAM and an Nvidia A2.

I already did some research and found some solution like localGPT and local LLM like vicuna etc, which could be usefull, but i'm really lost on how i should proceed with this task. (especially on how to train those model)

That's why i hope you guys can help me figure it out. If you have more questions or need other details don't hesitate to ask.

Thank you.  


Edit : They don't want me to make something like chatGPT, they know that it's impossible. They want a prototype that can answer question about their past project. "
28,learnmachinelearning,gpt,top,2023-05-11 00:54:18,What do actual ML engineers think of ChatGPT?,PhillConners,False,0.96,150,13e8of2,https://www.reddit.com/r/learnmachinelearning/comments/13e8of2/what_do_actual_ml_engineers_think_of_chatgpt/,106,1683766458.0,"You have been doing this for awhile, now the world is obsessed with OpenAI and suddenly all full of AI ‚Äúexperts‚Äù."
29,learnmachinelearning,gpt,top,2023-02-01 04:14:49,ChatGPT Extension for VSCode,Dense_Dimension_913,False,0.96,150,10qk8en,https://www.reddit.com/r/learnmachinelearning/comments/10qk8en/chatgpt_extension_for_vscode/,22,1675224889.0,"Created a ChatGPT extension for VSCode to help programmers understand  and read code more easily and also other features. To start, simply highlight a piece of code  and click on the plus icon on the  left to open up a chat and start  talking with ChatGPT, or Codex, or text-davinci-003. You can choose the  model you want to use in the settings. More details in the links below. I  really hope this extension can be useful to many people out there.  Please give it a try and let me know if you guys see any bugs or if you  like the extension. Thanks!

&#x200B;

https://i.redd.it/28zslxm06ifa1.gif

[VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=AndrewZhang.scribeai)"
30,learnmachinelearning,gpt,top,2022-12-28 04:37:21,University Professor Catches Student Cheating With ChatGPT,vadhavaniyafaijan,False,0.94,146,zx0ep0,https://www.theinsaneapp.com/2022/12/university-professor-catches-student-cheating-with-chatgpt.html,108,1672202241.0,
31,learnmachinelearning,gpt,top,2022-12-21 17:58:41,"Build Your Own GPT-3 App: A Step-by-Step Guide to Creating ""Gifthub,"" a Personalized Gift Recommendation Tool",bruclinbrocoli,False,0.96,142,zrvshy,https://www.reddit.com/r/learnmachinelearning/comments/zrvshy/build_your_own_gpt3_app_a_stepbystep_guide_to/,2,1671645521.0,"This was all built for free -- and took a weekend to ship it.  Pretty simple n a cool way to understand how to use GPT-3 for something personal. 

[Here's](https://buildspace.so/notes/build-gpt3-app) the link to the tutorial. You can also try out the app n see if it gives you a good gift rec.    
Or - share it with someone who sucks at giving gifts :)   


https://preview.redd.it/t2mrgddqia7a1.png?width=592&format=png&auto=webp&s=dc58613a6a5a4a7f8a55c62ab0ace2fe14c4ef8a"
32,learnmachinelearning,gpt,top,2021-10-04 16:34:23,minGPT: a small and educational implementation of GPT by Andrej Karpathy,NaN_Loss,False,0.98,136,q1932n,https://www.reddit.com/r/learnmachinelearning/comments/q1932n/mingpt_a_small_and_educational_implementation_of/,21,1633365263.0,"minGPT: a small and educational implementation of GPT in vanilla #PyTorch in \~300 lines of code by Andrej Karpathy: [github.com/karpathy/minGPT](https://github.com/karpathy/minGPT)

  

Includes a notebook where the model learns to perform addition on natural text (for example ‚Äú10+6=16‚Äù) and achieves 99.90% accuracy üò±

More curated posts like this on [@tutobase](https://twitter.com/tutobase) and [tutobase.com](https://tutobase.com)"
33,learnmachinelearning,gpt,top,2023-09-16 13:22:41,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.95,132,16k7heb,https://www.reddit.com/r/learnmachinelearning/comments/16k7heb/this_week_in_ai_all_the_major_ai_developments_in/,17,1694870561.0,"1. **Stability AI** launched Stable Audio, a generative AI tool for music & sound generation from text. The underlying latent diffusion model architecture uses audio conditioned on text metadata as well as audio file duration and start time.
2. **Coqui** released **XTTS** \- a new voice generation model that lets you clone voices in 13 different languages by using just a quick 3-second audio clip.
3. **Microsoft Research** released and open-sourced **Phi-1.5** \- a 1.3 billion parameter transformer-based model with performance on natural language tasks comparable to models 5x larger.
4. **Project Gutenberg**, Microsoft and MIT have worked together to use neural text-to-speech to create and release thousands of **human-quality free and open audiobooks**.
5. Researchers present **NExT-GPT -** an any-to-any multimodal LLM that accepts inputs and generate outputs in arbitrary combinations of text, images, videos, and audio.
6. **Chain of Density (CoD):** a new prompt introduced by researchers from Salesforce, MIT and Colombia University that generates more dense and human-preferable summaries compared to vanilla GPT-4.
7. **Adept** open-sources **Persimmon-8B**, releasing it under an Apache license. The model has been trained from scratch using a context size of 16K.
8. **Adobe's** **Firefly** generative AI models, after 176 days in beta, are now commercially available in Creative Cloud, Adobe Express, and Adobe Experience Cloud. Adobe is also launching Firefly as a standalone web app.
9. **Deci** released **DeciLM 6B**, a permissively licensed, open-source foundation LLM that is 15 times faster than Llama 2 while having comparable quality.
10. Researchers release **Scenimefy** \- a model transforming real-life photos into Shinkai-animation-style images.
11. **Microsoft** open sources **EvoDiff**, a novel protein-generating AI that could be used to create enzymes for new therapeutics and drug delivery methods as well as new enzymes for industrial chemical reactions.
12. Several companies including Adobe, IBM, Nvidia, Cohere, Palantir, Salesforce, Scale AI, and Stability AI have pledged to the White House to develop safe and trustworthy AI, in a voluntary agreement similar to an earlier one signed by Meta, Google, and OpenAI.
13. **Microsoft** will provide legal protection for customers who are sued for copyright infringement over content generated using Copilot, Bing Chat, and other AI services as long as they use built-in guardrails.
14. **NVIDIA** beta released **TensorRT** \- an open-source library that accelerates and optimizes inference performance on the latest LLMs on NVIDIA Tensor Core GPUs.
15. Pulitzer Prize winning novelist Michael Chabon and several other writers sue OpenAI of copyright infringement..
16. **NVIDIA** partners with two of India‚Äôs largest conglomerates, Reliance Industries Limited and Tata Group, to create an AI computing infrastructure and platforms for developing AI solutions.
17. **Roblox** announced a new conversational AI assistant that let creators build virtual assets and write code with the help of generative AI.
18. **Google** researchers introduced **MADLAD-400** \- a 3T token multilingual, general web-domain, document-level text dataset spanning 419 Languages.
19. A recent survey by **Salesforce** show that 65% of generative AI users are Millennials or Gen Z, and 72% are employed.  The survey included 4,000+ people across the United States, UK, Australia, and India.
20. **Meta** is reportedly working on an AI model designed to compete with GPT-4.

My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
34,learnmachinelearning,gpt,top,2020-08-27 03:29:43,"GPT-3 used to generate code for a machine learning model, just by describing the dataset and required output / Via Matt Shumer(Twitter)",TheInsaneApp,False,0.96,126,ihdpgv,https://v.redd.it/1op7cffisgj51,15,1598498983.0,
35,learnmachinelearning,gpt,top,2023-01-25 01:15:22,How ChatGPT is Trained,ariseff,False,0.98,128,10km46l,https://youtu.be/VPRSBzXzavo,8,1674609322.0,
36,learnmachinelearning,gpt,top,2023-01-06 11:58:10,How does ChatGPT actually work? Explained simply with pen and paper,techie_ray,False,0.94,125,104sebq,https://youtu.be/k9Sps7ciNTE,16,1673006290.0,
37,learnmachinelearning,gpt,top,2023-01-17 07:51:07,DeepMind To Launch ChatGPT Rival Sparrow Soon,vadhavaniyafaijan,False,0.96,123,10e6h7j,https://www.theinsaneapp.com/2023/01/deepmind-to-launch-chatgpt-rival-sparrow.html,5,1673941867.0,
38,learnmachinelearning,gpt,top,2023-06-03 14:33:38,This week in AI - all the Major AI development in a nutshell,wyem,False,0.98,118,13zeoi3,https://www.reddit.com/r/learnmachinelearning/comments/13zeoi3/this_week_in_ai_all_the_major_ai_development_in_a/,13,1685802818.0,"1. The recently released open-source large language model **Falcon LLM**, by UAE‚Äôs Technology Innovation Institute, is now royalty-free for both commercial and research usage. **Falcon 40B,** the 40 billion parameters model trained on one trillion tokens, is ranked #1 on **Open LLM Leaderboard by Hugging Face**.
2. **Neuralangelo**, a new AI model from Nvidia turns 2D video from any device - cell phone to drone capture - into 3D structures with intricate details using neural networks..
3. In three months, JPMorgan has advertised **3,651 AI jobs** and sought a trademark for **IndexGPT**, a securities analysis AI product.
4. **Google** presents **DIDACT** (‚Äã‚ÄãDynamic Integrated Developer ACTivity), the first code LLM trained to model real software developers editing code, fixing builds, and doing code review. DIDACT uses the software development process as training data and not just the final code, leading to a more realistic understanding of the development task.
5. Researchers from **Deepmind** have presented ‚Äò**LLMs As Tool Makers (LATM)**‚Äô - a framework that allows Large Language Models (LLMs) to create and use their own tools, enhancing problem-solving abilities and cost efficiency. With this approach, a sophisticated model (like GPT-4) can make tools (where a tool is implemented as a Python utility function), while a less demanding one (like GPT-3.5) uses them.
6. **Japan's government** won't enforce copyrights on data used for AI training regardless of whether it is for non-profit or commercial purposes.
7. *‚ÄòMitigating the* ***risk of extinction from AI*** *should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.‚Äô -* One sentence statement signed by leading AI Scientists as well as many industry experts including CEOs of OpenAI, DeepMind and Anthropic.*.*
8. Nvidia launched ‚Äò**Nvidia Avatar Cloud Engine (ACE) for Games**‚Äô - a custom AI model foundry service to build non-playable characters (NPCs) that not only engage in dynamic and unscripted conversations, but also possess evolving, persistent personalities and have precise facial animations and expressions.
9. **OpenAI** has launched a trust/security portal for OpenAI‚Äôs compliance documentation, security practices etc..
10. **Nvidia** announced a new AI supercomputer, the **DGX GH200,** for giant models powering Generative AI, Recommender Systems and Data Processing. It has 500 times more memory than its predecessor, the DGX A100 from 2020.
11. Researchers from Nvidia presented **Voyager**, the first ‚ÄòLLM-powered embodied lifelong learning agent‚Äô that can explore, learn new skills, and make new discoveries continually without human intervention in the game Minecraft.
12. The a16z-backed chatbot startup **Character.AI** launched its mobile AI chatbot app on May 23 for iOS and Android, and succeeded in gaining over **1.7 million new installs** within a week.
13. Microsoft Research presents **Gorilla**, a fine-tuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls.
14. **OpenAI** has trained a model using process supervision - rewarding the thought process rather than the outcome - to improve mathematical reasoning. Also released the full dataset used.
15. **WPP**, the world's largest advertising agency, and Nvidia have teamed up to use generative AI for creating ads. The new platform allows WPP to tailor ads for different locations and digital channels, eliminating the need for costly on-site production.
16. **PerplexityAI‚Äôs** android app is available now, letting users search with voice input, learn with follow-up questions, and build a library of threads.

**If you like this news format**, you might find my ¬†[newsletter](https://aibrews.com/)¬†helpful - it's free to join, sent only once a week with **bite-sized news, learning resources and selected tools**. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
39,learnmachinelearning,gpt,top,2022-12-24 09:14:57,"How would I train a chatbot like ChatGPT on a specific data set, so that it answers questions as if it's belief structure was based on the information I give it?",EllyEscape,False,0.92,114,zu6785,https://www.reddit.com/r/learnmachinelearning/comments/zu6785/how_would_i_train_a_chatbot_like_chatgpt_on_a/,41,1671873297.0,"This might be a noob question, so I'll write it to my best abilities. I have some experience with coding video game AI in Godot, Unity and Unreal but I've never touched ML or ""real""(?) AI that uses learning algorithms. 

&#x200B;

I wanted to give a sophisticated chatbot like ChatGPT a bunch of data and text from (for instance, not my end goal) a philosopher, and have it answer questions as if it was that philosopher, ague against what I say as if it was a person who believed what the text I gave it said and so on, all while still able to use online resources (like ChatGPT does) to find additional supporting information, rather than only the text I give it which might limit its ability to give coherent arguments. In summary, I want it's beliefs  and values to be limited to a specific source text, but not it's knowledge base. 

&#x200B;

How would I go about this? Do I have to develop a model from scratch to give it any text sources I want, or is it possible to do with an existing API? I was going to use Character.AI but the method for giving it information is too limited for what I want to do. 

&#x200B;

If anyone has any resources to get me started it would be very helpful! Thank you."
40,learnmachinelearning,gpt,top,2023-02-24 06:26:36,Is there a way to easily train ChatGPT or GPT on custom knowledge?,senttoschool,False,0.99,117,11akisx,https://www.reddit.com/r/learnmachinelearning/comments/11akisx/is_there_a_way_to_easily_train_chatgpt_or_gpt_on/,46,1677219996.0,"My company has internal documents. It'd be nice to be able to have GPT look over it, and then I can ask it questions on the internal documents."
41,learnmachinelearning,gpt,top,2023-02-11 06:58:18,[N] New Open-Source Version Of ChatGPT ‚≠ï,LesleyFair,False,0.98,114,10zep6u,https://www.reddit.com/r/learnmachinelearning/comments/10zep6u/n_new_opensource_version_of_chatgpt/,8,1676098698.0,"GPT is getting competition from open-source.

A group of researchers, around the YouTuber [Yannic Kilcher](https://www.ykilcher.com/), have announced that they are working on [Open Assistant](https://github.com/LAION-AI/Open-Assistant). The goal is to produce a chat-based language model that is much smaller than GPT-3 while maintaining similar performance.

If you want to support them, they are crowd-sourcing training data [here](https://open-assistant.io/).

**What Does This Mean?**

Current language models are too big.

They require millions of dollars of hardware to train and use. Hence, access to this technology is limited to big organizations. Smaller firms and universities are effectively shut out from the developments.

Shrinking and open-sourcing models will facilitate academic research and niche applications.

Projects such as Open Assistant will help to make language models a commodity. Lowering the barrier to entry will increase access and accelerate innovation.

What an exciting time to be alive! 

Thank you for reading! I really enjoyed making this for you!  
The Decoding ‚≠ï is a thoughtful weekly 5-minute email that keeps you in the loop about machine research and the data economy. [Click here to sign up](https://thedecoding.net/)!"
42,learnmachinelearning,gpt,top,2023-01-11 14:03:46,What do you all think about these ‚ÄúSEO is Dead‚Äù articles?,Aggressive-Twist-252,False,0.89,115,1095h99,https://www.reddit.com/r/learnmachinelearning/comments/1095h99/what_do_you_all_think_about_these_seo_is_dead/,20,1673445826.0,"I keep seeing [articles](https://jina.ai/news/seo-is-dead-long-live-llmo/) like this over the years and it made me wonder. Is SEO really dead? Or will it evolve? Back then I kept wondering if it‚Äôs true or not. Some believe SEO is dead, some don‚Äôt. But now with tools like Chat GPT and Midjourney, I think it‚Äôs time to take a look back and see how this might change SEO or if it will ‚Äúkill‚Äù SEO.

I keep seeing threads and discussions seeing how people are excited and worried at the same time with how AI might be able to do a better job. But the way I see it, AI content still needs a person to tell it what to do and make the writing look nice. And also I think that the internet will have a lot of writing that was made by AI and that might change how we find things online. You might also see a ton of content being written by AI and trigger some plagiarism detectors and have a lot of websites get penalized. Hopefully the internet won‚Äôt be filled with boilerplate copy/pasted content coming from Chat GPT.

Well we have Google to filter out trash content anyway. But I know Google has some issues lately that they need to fix. One is that they also have AI that can help people find things on the internet with their search engine, and they need to make sure they are still the best in terms of search. 

The second is that Google needs to find a way to tell if something is really good or not, like how some websites that show art do. Google wants to show the best thing first, but it's hard because sometimes the thing that is the best is also something that Google's customers want people to see. It‚Äôs possible that some AI generated contentSo it's kind of tricky.

I have a feeling companies that already make SEO-writing and checking bots are gonna roll out some fresh new models soon. They're gonna be even better than before. These bots are going to write some good articles and product descriptions that are almost perfect. It almost looks like a human wrote the article or description. And all a human will do is quickly check for any false claims and write a headline that doesn't sound like a robot wrote it. 

We can only really tell 5-10 years from now. In the meantime, I‚Äôll probably go back practicing some handyman skills and also go back teaching people how to drive and also be a service driver. These jobs I had in the past were way different from what I am earning now but if the worst comes to worst, at least I have these physical skills ready."
43,learnmachinelearning,gpt,top,2023-06-11 17:18:34,"[D] How to Choose a Framework To Evaluate Your LLMs? We've Evaluated GPT-4/3.5, Anthropic Claude, & Cohere Command Across 4 Tasks. Here's What We've Learned.",davidbun,False,0.98,113,146zie8,https://v.redd.it/yy5sdnvo6f5b1,1,1686503914.0,
44,learnmachinelearning,gpt,top,2023-11-21 20:58:14,Does your company let your engineers use AI tools like Copilot or ChatGPT?,Psychological_March2,False,0.93,93,180r9tx,https://www.reddit.com/r/learnmachinelearning/comments/180r9tx/does_your_company_let_your_engineers_use_ai_tools/,75,1700600294.0,"In light of what's been happening with Open AI, this blog we wrote is still relevant:

A few weeks ago, I was with a group of CTOs when someone asked: *does your company let your engineers use AI tools like Copilot or ChatGPT?*

I thought the question was strange. What do you mean *let*? They're going to use it no matter what you say. AI code generation tools offer engineers a huge productivity boost. The ability to autocomplete code in seconds or work through a problem with AI isn‚Äôt an opportunity developers will pass up.

When we drilled into why this group was reluctant to allow their engineers to use AI, it became apparent that their reservations centered primarily on one concern: the absence of a robust testing framework to give them confidence in the code generated by AI.

But this is still flawed reasoning. If you‚Äôre not confident in using AI, how can you be confident in hiring new grads? If you don‚Äôt have the tools to have confidence in your code, it doesn‚Äôt matter where that code comes from‚Äìyou‚Äôll always struggle with quality.

Read more [here](https://trunk.io/blog/enhancing-code-quality-and-security-in-the-ai-era?utm=reddit)."
45,learnmachinelearning,gpt,top,2023-05-25 17:23:19,"Are people still coding stuff on their own like chatbots, image AIs, etc., or is everyone just using pretrained models and APIs now?",TrackLabs,False,0.91,93,13rnopr,https://www.reddit.com/r/learnmachinelearning/comments/13rnopr/are_people_still_coding_stuff_on_their_own_like/,35,1685035399.0,"I feel like everyone is just downloading models from huggingface at this point, or using GPT APIs and so on.

I also feel like there are not really tutorials anymore on YT and the web about how to code stuff like there used to be 5 to 2 years ago. Every video now is just ""how to use OpenAIs API"" or ""how to use llama model from huggingface"". 

I have a big problem with staying up to date on the stuff, I never really bothered using huggingface, and I dont really like the idea to just use other peoples pretrained models for everything, what actual contribution am I doing in my own projects then lol.

Would be cool if some people could give me some reality check on whats going on."
46,learnmachinelearning,gpt,top,2023-06-23 06:14:03,"[Updated] Top Large Language Models based on the Elo rating, MT-Bench, and MMLU",kingabzpro,False,0.94,93,14gqo26,https://i.redd.it/ixdabwx3mp7b1.png,9,1687500843.0,
47,learnmachinelearning,gpt,top,2023-01-11 05:23:14,Thoughts on this ChatGPT fact-checker tool I built this past week?,QuestionAnxious,False,0.89,92,108wigf,https://v.redd.it/yqudljp0ncba1,25,1673414594.0,
48,learnmachinelearning,gpt,top,2023-02-21 14:59:06,I created a Search Engine For Books using GPT-3 üîéüìò. Here's how you can create it too:,Pritish-Mishra,False,0.94,87,1185dhq,https://youtu.be/SXFP4nHAWN8,17,1676991546.0,
49,learnmachinelearning,gpt,top,2023-01-27 14:51:14,Fine-tuning open source models to emulate ChatGPT for code explanation.,awesomequantity,False,0.88,85,10mmofg,https://www.reddit.com/r/learnmachinelearning/comments/10mmofg/finetuning_open_source_models_to_emulate_chatgpt/,13,1674831074.0,"I'm looking to step up my game and emulate ChatGPT for specific use-cases like explaining code. I'm thinking about using open source models like GPT-J, or OPT to get beyond the limitations of the closed-source nature of ChatGPT, like the amount of text it can read or respond with.

I got the funding for training, hardware, etc, and I want the end product to be on-premises, so no worries there. The inference doesn't have to be super fast either. I know there are projects like OpenAssistant and petals.ml but haven‚Äôt made enough research just yet.

One option I‚Äôm considering is using fine tuners like the one from [HuggingFace](https://github.com/subhasisj/HuggingFace-Transformers-FineTuning) or [Jina AI](https://github.com/jina-ai/finetuner) to fine-tune open source models like GPT-J or OPT to improve specific use-cases like code explanation. With the funding that we have, I wouldn‚Äôt want to cheap out on fine-tuning and expect something good.

So, can anyone help out and point me in the right direction? Which model is the best to fine-tune and how do I fine-tune to improve specific use cases? Any help would be appreciated. Thanks!"
50,learnmachinelearning,gpt,top,2023-09-30 15:01:31,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.97,82,16w93bx,https://www.reddit.com/r/learnmachinelearning/comments/16w93bx/this_week_in_ai_all_the_major_ai_developments_in/,4,1696086091.0,"1. **Meta AI** presents **Emu**, a quality-tuned latent diffusion model for generating highly aesthetic images. Emu significantly outperforms SDXLv1.0 on visual appeal.
2. **Meta AI** researchers present a series of long-context LLMs with context windows of up to 32,768 tokens. LLAMA 2 70B variant surpasses gpt-3.5-turbo-16k‚Äôs overall performance on a suite of long-context tasks.
3. **Abacus AI** released a larger 70B version of **Giraffe**. Giraffe is a family of models that are finetuned from base Llama 2 and have a larger context length of 32K tokens\].
4. **Meta** announced:  

   1. **Meta AI** \- a new AI assistant users can interact with on WhatsApp, Messenger and Instagram. Will also be available on Ray-Ban Meta smart glasses and Quest 3, Meta‚Äôs mixed reality headset.
   2. **AI stickers** that enable users to generate customized stickers for chats and stories using text. Powered by Llama 2 and the new foundational model for image generation, Emu.
   3. **28 AI characters**, each with a unique personality that users can message on WhatsApp, Messenger, and Instagram.
   4. New AI editing tools, **restyle** and **backdrop** in Instagram.
   5. **AI Studio** \- a platform that supports the creation of custom AIs by coders and non-coders alike.
5. **Cerebras** and **Opentensor** released Bittensor Language Model, ‚Äò**BTLM-3B-8K**‚Äô, a new 3 billion parameter open-source language model with an 8k context length trained on 627B tokens of SlimPajama. It outperforms models trained on hundreds of billions more tokens and achieves comparable performance to open 7B parameter models. The model needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models and is available with an Apache 2.0 license for commercial use.
6. **OpenAI** is rolling out, over the next two weeks, new voice and image capabilities in ChatGPT enabling ChatGPT to understand images, understand speech and speak. The new voice capability is powered by a new text-to-speech model, capable of generating human-like audio from just text and a few seconds of sample speech. .
7. **Mistral AI**, a French startup, released its first 7B-parameter model, **Mistral 7B**, which outperforms all currently available open models up to 13B parameters on all standard English and code benchmarks. Mistral 7B is released in Apache 2.0, making it usable without restrictions anywhere.
8. **OpenAI** has returned the ChatGPT browsing feature for Plus subscribers, enabling ChatGPT to access internet for current information. It was disabled earlier as users were able to deploy it to bypass the paywalls of leading news publishers.
9. **Microsoft** has released **AutoGen** \- an open-source framework that enables development of LLM applications using multiple agents that can converse with each other to solve a task. Agents can operate in various modes that employ combinations of LLMs, human inputs and tools.
10. **LAION** released **LeoLM**, the first open and commercially available German foundation language model built on Llama-2
11. Researchers from **Google** and **Cornell University** present and release code for DynIBaR (Neural Dynamic Image-Based Rendering) - a novel approach that generates photorealistic renderings from complex, dynamic videos taken with mobile device cameras, overcoming fundamental limitations of prior methods and enabling new video effects.
12. **Cloudflare** launched **Workers AI** (an AI inference as a service platform), **Vectorize** (a vector Database) and **AI Gateway** with tools to cache, rate limit and observe AI deployments. Llama2 is available on Workers AI.
13. **Amazon** announced the general availability of **Bedrock**, its service that offers a choice of generative AI models from Amazon itself and third-party partners through an API.
14. **Google** announced it‚Äôs giving website publishers a way to opt out of having their data used to train the company‚Äôs AI models while remaining accessible through Google Search.
15. **Spotify** has launched a pilot program for AI-powered voice translations of podcasts in other languages - in the podcaster‚Äôs voic. It uses OpenAI‚Äôs newly released voice generation model.
16. **Getty Images** has launched a generative AI image tool, ‚Äò**Generative AI by Getty Images**‚Äô, that is ‚Äòcommercially‚Äësafe‚Äô. It‚Äôs powered by Nvidia Picasso, a custom model trained exclusively using Getty‚Äôs images library.
17. **Optimus**, Tesla‚Äôs humanoid robot, can now sort objects autonomously and do yoga. Its neural network is trained fully end-to-end.
18. **Amazon** will invest up to $4 billion in Anthropic. Developers and engineers will be able to build on top of Anthropic‚Äôs models via Amazon Bedrock.
19. **Google Search** indexed shared Bard conversational links into its search results pages. Google says it is working on a fix.
20. **Pika** Labs' text-to-video tool now lets users encrypt a message in a video\].

My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
51,learnmachinelearning,gpt,top,2021-09-19 07:59:15,AI research papers explainer channel.,gauravc2796,False,0.84,81,pr3sc7,https://www.reddit.com/r/learnmachinelearning/comments/pr3sc7/ai_research_papers_explainer_channel/,12,1632038355.0,"Hi, I have started a youtube channel where I would provide some explainer on the latest AI research papers as I have happened to read a lot of them.  
If you have any suggestions, comments, or anything, do let me know.   
Your opinion would be highly valuable :)  
Channel: [https://www.youtube.com/channel/UCYEXrPn4gP9RbaSzZvxX6MA](https://www.youtube.com/channel/UCYEXrPn4gP9RbaSzZvxX6MA)  


Some Videos which have been created till now:

Textless NLP: [https://www.youtube.com/watch?v=zw\_QjUptr5o](https://www.youtube.com/watch?v=zw_QjUptr5o)  
Neural DB: [https://www.youtube.com/watch?v=Vo9L0LETMI4](https://www.youtube.com/watch?v=Vo9L0LETMI4)  
Perceiver IO: [https://www.youtube.com/watch?v=AS1Sh-KuNzs](https://www.youtube.com/watch?v=AS1Sh-KuNzs)  
Openai's GPT codex: [https://www.youtube.com/watch?v=8977dybJ7Ro](https://www.youtube.com/watch?v=8977dybJ7Ro)"
52,learnmachinelearning,gpt,top,2022-01-31 13:22:56,GPT from scratch (PyTorch video tutorial),mildlyoverfitted,False,0.97,71,sh1580,https://youtu.be/d7IRM40VMYM,4,1643635376.0,
53,learnmachinelearning,gpt,top,2023-07-10 14:36:34,ü§ñüîé Excited to introduce 'GPT-Researcher'!,Legal-Dragonfruit845,False,0.8,70,14vvtqf,https://www.reddit.com/r/learnmachinelearning/comments/14vvtqf/excited_to_introduce_gptresearcher/,35,1688999794.0,"The idea is simple - Specify what you want to research, and the AI will autonomously research it for you in minutes!

‚ñ∏ One prompt generates an unbiased, factual and in depth research report

‚ñ∏ Generate research, outlines, resource and lessons reports

‚ñ∏ Aggregates over 20 web sources per research

‚ñ∏ Includes an easy to use web interface

‚ñ∏ Open source: [https://github.com/assafelovic/gpt-researcher](https://github.com/assafelovic/gpt-researcher)

‚ñ∏ Scrapes web sources with javascript support

‚ñ∏ Keeps track and context of visited and used web sources

https://reddit.com/link/14vvtqf/video/zce4347lf5bb1/player"
54,learnmachinelearning,gpt,top,2023-03-30 12:56:24,I created this entire video using ChatGPT + Charactr API + D-ID. My mind is blown,3nd4u,False,0.86,68,126m5eo,https://www.reddit.com/r/learnmachinelearning/comments/126m5eo/i_created_this_entire_video_using_chatgpt/,15,1680180984.0,"Could this be the future of how our news is being consumed?

https://reddit.com/link/126m5eo/video/hhfat6n3jvqa1/player"
55,learnmachinelearning,gpt,top,2023-01-15 00:08:37,Is it still worth learning NLP in the age of API-accessibles LLM like GPT?,CrimsonPilgrim,False,0.94,66,10c509n,https://www.reddit.com/r/learnmachinelearning/comments/10c509n/is_it_still_worth_learning_nlp_in_the_age_of/,24,1673741317.0,"A question that, I hope, you will find legitimate from a data science student.

I am speaking from the point of view of a data scientist not working in research.

Until now, learning NLP could be used to meet occasional business needs like sentiment analysis, text classification, topic modeling....

With the opening of GPT-3 to the public, the rise of ChatGPT, and the huge wave of applications, sites, plug-ins and extensions based on this technology that are accessible with a simple API request, it's impossible not to wonder if spending dozens of hours diving into this field if ML wouldn't be as useful today as learning the source code of the Pandas library. 

In some specialized cases, it could be useful, but GPT-3, and the models that will follow, seem to offer more than sufficient results for the immensity of the cases and for almost all classical NLP tasks. Not only that, but there is a good chance that the models trained by giants like Open-AI (Microsoft) or Google can never be replicated outside these companies anyway.  With ChatGPT and its incomparable mastery of language, its ability to code, summarize, extract topics, understand... why would I bother to use BERT or a TF-IDF vectorizer when an API will be released? Not only it would be easily accessible, but it also would be much better at the task, faster and cheaper.

In fact, it's a concern regarding all the machine learning field in general with the arrival of powerful ""no-code"" applications, which abstract a large part of the inherent complexity of the field. There will always be a need for experts, for safeguards, but in the end, won't the Data Scientist who masters the features of GPT-3 or 4 and knows a bit of NLP be more efficient than the one who has spent hours reading Google papers and practicing on Gensim, NLTK, spacy... It is the purpose of an API to make things simpler eventually... At what point is there no more reason to be interested in the behind-the-scenes of these tools and to become simple users rather than trying to develop our own techniques?"
56,learnmachinelearning,gpt,top,2023-04-30 15:45:04,I don't have a PhD but this just feels wrong. Can a person with a PhD confirm?,flaky_psyche,False,0.76,61,133v9s5,https://i.redd.it/fmkvgop7l1xa1.jpg,238,1682869504.0,
57,learnmachinelearning,gpt,top,2023-12-25 17:15:18,"Have we reached a ceiling with transformer-based models? If so, what is the next step?",swagonflyyyy,False,0.86,62,18qmohw,https://www.reddit.com/r/learnmachinelearning/comments/18qmohw/have_we_reached_a_ceiling_with_transformerbased/,134,1703524518.0,"About a month ago Bill Gates hypothesized that models like GPT-4 will probably have reached a ceiling in terms of performance and these models will most likely expand in breadth instead of depth, which makes sense since models like GPT-4 are transitioning to multi-modality (presumably transformers-based).

This got me thinking. If if is indeed true that transformers are reaching peak performance, then what would the next model be? We are still nowhere near AGI simply because neural networks are just a very small piece of the puzzle. 

That being said, is it possible to get a pre-existing machine learning model to essentially create other machine learning models? I mean, it would still have its biases based on prior training but could perhaps the field of unsupervised learning essentially construct new models via data gathered and keep trying to create different types of models until it successfully self-creates a unique model suited for the task?

Its a little hard to explain where I'm going with this but this is what I'm thinking:

\- The model is given a task to complete.

\- The model gathers data and tries to structure a unique model architecture via unsupervised learning and essentially trial-and-error.

\- If the model's newly-created model fails to reach a threshold, use a loss function to calibrate the model architecture and try again.

\- If the newly-created model succeeds, the model's weights are saved.

This is an oversimplification of my hypothesis and I'm sure there is active research in the field of auto-ML but if this were consistently successful, could this be a new step into AGI since we have created a model that can create its own models for hypothetically any given task?

I'm thinking LLMs could help define the context of the task and perhaps attempt to generate a new architecture based on the task given to it but it would still fall under a transformer-based model builder, which kind of puts us back in square one."
58,learnmachinelearning,gpt,top,2023-03-30 19:44:32,Personalize Your Own Language Model with xTuring - A Beginner-Friendly Library,x_ml,False,1.0,61,126x6ua,https://www.reddit.com/r/learnmachinelearning/comments/126x6ua/personalize_your_own_language_model_with_xturing/,7,1680205472.0,"Hi everyone,  


If you are interested in customizing your own language model but don't know where to start, try  [xTuring](https://github.com/stochasticai/xturing).  


xTuring's goal is to empower individuals to fine-tune LLM for their specific tasks with as little as 5 lines of code. With xTuring, you can perform high and low precision fine-tuning with a variety of models, including LLaMA, OPT, Cerebras-GPT, Galactica, BLOOM, and more.   


You can also generate your OWN datasets using powerful models like GPT-3 to train a much smaller model on YOUR specific task. With the latest version, you can also use terminal and web interface to chat with your models.  


Please do check out the repo and show your support if you like our work. Would love if you can also contribute by adding models, raising issues or raising PRs for fixes.  


xTuring Github: [https://github.com/stochasticai/xturing](https://github.com/stochasticai/xturing)

If you are interested in getting involved, I am happy to help you on our Discord: [https://discord.gg/TgHXuSJEk6](https://discord.gg/TgHXuSJEk6)

https://i.redd.it/mvxb7i5fixqa1.gif"
59,learnmachinelearning,gpt,top,2022-12-20 11:12:21,What are the advantages of training your own model rather than customizing GPT3 ?,wootfacemate,False,0.89,56,zqlqzj,https://www.reddit.com/r/learnmachinelearning/comments/zqlqzj/what_are_the_advantages_of_training_your_own/,16,1671534741.0,"Hello,   
I am a beginner in ML, so it might sound obvious but with such powerful tool like GPT, I was wondering why wouldn't you always use a pre-trained model like GPT that is way more powerful rather than fit your own model ?"
60,learnmachinelearning,gpt,top,2023-02-27 13:42:55,Can you fine-tune chatGPT in your data as of now?,Melodic_Stomach_2704,False,0.9,58,11dc5b4,https://www.reddit.com/r/learnmachinelearning/comments/11dc5b4/can_you_finetune_chatgpt_in_your_data_as_of_now/,33,1677505375.0, I know that model is not publicly available so it's not possible to do it locally. But can you train or fine-tune chatGPT on your data using their API? I see many misguiding articles on the internet that are fine-tuning other GPT models claiming chatGPT.
61,learnmachinelearning,gpt,top,2023-07-28 01:21:05,I created a cli app that allowes you to ask for a specific command and gpt will try to guess it and copy it to your clipboard ,Freekiehsoes,False,0.97,54,15bjhy3,https://v.redd.it/1owowfa1yleb1,6,1690507265.0,
62,learnmachinelearning,gpt,top,2023-05-19 07:08:51,OpenAI Launches ChatGPT App For iOS Users,vadhavaniyafaijan,False,0.86,52,13lnv1e,https://www.theinsaneapp.com/2023/05/chatgpt-app-for-iphone-and-ipad.html,10,1684480131.0,
63,learnmachinelearning,gpt,top,2022-12-30 01:18:38,A GPT-3 based Terminal/CLI tool that helps you debug your code!,VideoTo,False,0.97,56,zyms85,https://www.reddit.com/r/learnmachinelearning/comments/zyms85/a_gpt3_based_terminalcli_tool_that_helps_you/,11,1672363118.0,"Link - [https://clerkie.co/](https://clerkie.co/)

We built ClerkieCLI -  a GPT-3 based tool that:

\-  automatically detects errors on your terminal,

\- identifies  the programming language,

\- provides an explanation of the error and suggested fix right on your terminal.

This is definitely early days, so if this is something you would find  valuable and wouldn't mind testing a couple iterations of, just sign up here -> [https://forms.gle/8DURoG6NCRxVazNn8](https://forms.gle/8DURoG6NCRxVazNn8)

&#x200B;

https://i.redd.it/xpwnazimsx8a1.gif"
64,learnmachinelearning,gpt,top,2023-02-21 23:18:46,"How big was GPT-3.5's training dataset, and are there any good heuristics for how large an ML dataset needs to be for it to be good?",TikkunCreation,False,0.94,55,118iccl,https://www.reddit.com/r/learnmachinelearning/comments/118iccl/how_big_was_gpt35s_training_dataset_and_are_there/,6,1677021526.0,"Say I want to do a model for fixing bugs in code. How many examples do I need for it to be good?

Or say I want to do a model for scoring boxing matches. How many examples do I need for it to be good?"
65,learnmachinelearning,gpt,top,2023-05-02 08:48:46,How GPT-3.5 crushes my high score in 2048,inishchith,False,0.74,54,135ffje,https://v.redd.it/q22lna91tdxa1,28,1683017326.0,
66,learnmachinelearning,gpt,top,2021-06-13 20:57:38,Some YouTube channels that review papers,axetobe_ML,False,0.95,50,nz5szs,https://www.reddit.com/r/learnmachinelearning/comments/nz5szs/some_youtube_channels_that_review_papers/,2,1623617858.0,"When I was reading a Reddit thread. People were wondering if there were YouTubers reviewing papers. As the OP noticed that one of the YouTuber's that he regularly watched stopped uploading videos. There are a few YouTubers that talk about ML and review papers. 

I decided to compile some of the YouTube channels into this short list. 

&#x200B;

[Two Minute Papers](https://www.youtube.com/c/K%C3%A1rolyZsolnai/videos) does great overviews of fascinating papers. Showing the increasing progress of ML.

Some of the videos I liked:

* [4 Experiments Where the AI Outsmarted Its Creators](https://www.youtube.com/watch?v=GdTBqBnqhaQ)

This video showed various AI solving a problem not in the way the researchers intended to. That may include abusing the physics in the simulation or lateral thinking used by the model.

* [A Video Game That Looks Like Reality!](https://youtu.be/22Sojtv4gbg)

A review of a paper that takes GTA V gameplay and converts them to photo-realistic footage.

&#x200B;

[Yannic Kilcher](https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew) does in-depth reviews of various papers. As you go through the paper he shows you his thought process. And showing what important inside the paper. Very useful if don‚Äôt read that many papers. (Like me)

Some good videos:

* [Attention Is All You Need](https://www.youtube.com/watch?v=iDulhoQ2pro)

A review of a paper that introduced transformers.

&#x200B;

* [DeepMind's AlphaFold 2 Explained! AI Breakthrough in Protein Folding What we know (& what we don't)](https://youtu.be/B9PL__gVxLI)

A great rundown on protein folding and speculating how Alphafold 2 works.

&#x200B;

* [GPT-3: Language Models are Few-Shot Learners (Paper Explained)](https://youtu.be/SY5PvZrJhLE)

A comprehensive paper reading of the GPT-3 paper.

&#x200B;

[Bycloud](https://www.youtube.com/channel/UCgfe2ooZD3VJPB6aJAnuQng) you may have seen him around on Reddit. Creates short and insightful summaries of papers.

Some videos I liked:

* [AI Sky Replacement with SkyAR](https://www.youtube.com/watch?v=yNwQnrjfg5A)

Summary of paper that creates AR effects in video footage. Adding various effects to the video footage‚Äôs sky.

&#x200B;

* [AI Generates Cartoon Characters In Real Life \[Pixel2Style2Pixel\]](https://youtu.be/g-N8lfceclI)

Reviewing a paper that converts cartoon characters to real-life equivalents and vice versa. Also explains how the paper made it easier to adjust the parameters of the GAN. Helping us adjust what images we want to produce.

&#x200B;

[Machine Learning Street Talk](https://www.youtube.com/c/MachineLearningStreetTalk/videos)

This is a podcast series that interviews top ML researchers. While they don‚Äôt have videos about papers alone. As they interview various experts in the field. So they talk about many papers as a consequence. 

While this is a short list maybe you can find these channels interesting and learn something new.

\-

*If you found this post useful, then check out my* [*mailing list*](https://www.tobiolabode.com/subscribe) *where I write more stuff like this.*"
67,learnmachinelearning,gpt,top,2023-07-15 21:22:23,"I Hit 700K Views in 3 Months with my open-source Shorts automation framework, ShortGPT",RayVentura,False,0.84,52,150ng7i,https://v.redd.it/i1slpmgd17cb1,13,1689456143.0,
68,learnmachinelearning,gpt,top,2023-12-03 14:38:25,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.93,48,189ustx,https://www.reddit.com/r/learnmachinelearning/comments/189ustx/this_week_in_ai_all_the_major_ai_developments_in/,1,1701614305.0,"1. **Meta AI** introduced a suite of AI language translation models that preserve expression and improve streaming \[[*Details*](https://ai.meta.com/blog/seamless-communication) *|* [*GitHub*](https://github.com/facebookresearch/seamless_communication)\]:
   1. ***SeamlessExpressive*** enables the transfer of tones, emotional expression and vocal styles in speech translation. You can try a demo of SeamlessExpressive using your own voice as an input [***here***](https://seamless.metademolab.com/expressive)***.***
   2. ***SeamlessStreaming***, a new model that enables streaming speech-to-speech and speech-to-text translations with <2 seconds of latency and nearly the same accuracy as an offline model. In contrast to conventional systems which translate when the speaker has finished their sentence, SeamlessStreaming translates while the speaker is still talking. t intelligently decides when it has enough context to output the next translated segment.
   3. ***SeamlessM4T v2***, a foundational multilingual & multitask model for both speech & text. It's the successor to SeamlessM4T, demonstrating performance improvements across ASR, speech-to-speech, speech-to-text & text-to-speech tasks.
   4. ***Seamless***, a model that merges capabilities from SeamlessExpressive, SeamlessStreaming and SeamlessM4T v2 into one.
2. **Stability AI** released ***SDXL Turbo***: a real-time Text-to-Image generation model. SDXL Turbo is based on a a new distillation technology, which enables the model to synthesize image outputs in a single step and generate real-time text-to-image outputs while maintaining high sampling fidelity.
3. **Mozilla‚Äôs** innovation group and Justine Tunney released ***llamafile*** that lets you distribute and run LLMs with a single file. llamafiles can run on six OSes (macOS, Windows, Linux, FreeBSD, OpenBSD, and NetBSD) and on multiple CPU architectures.
4. **Perplexity** released two new PPLX models: ***pplx-7b-online and pplx-70b-online***. These *online LLMs* can leverage the most up-to-date information using the internet when forming a response..
5. **Google DeepMind** presented ***GNoME*** (Graph Networks for Materials Exploration): an AI tool that discovered *2.2 million new crystal structures*, with 380,000 being highly stable and promising for breakthroughs in superconductors, supercomputers, and advanced batteries for electric vehicles.
6. **Amazon** introduced two new Amazon Titan multimodal foundation models (FMs): ***Amazon Titan Image Generator*** (preview) and ***Amazon Titan Multimodal Embeddings***. All images generated by Amazon Titan contain an invisible watermark.
7. Researchers present ***Animatable Gaussians***, a new avatar representation method that can create lifelike human avatars from multi-view RGB videos.
8. **Pika Labs** released a major product upgrade of their generative AI video tool, ***Pika 1.0***, which includes a new AI model capable of generating and editing videos in diverse styles such as 3D animation, anime, cartoon and cinematic using text, image or existing video.
9. **Eleven Labs** announced a ***grant*** program offering 11M text characters of content per month for the first 3 months to solo-preneurs and startups.
10. Researchers from **UC Berkeley** introduced ***Starling-7B***, an open large language model trained using Reinforcement Learning from AI Feedback (RLAIF). It utilizes the GPT-4 labeled ranking dataset, Nectar, and a new reward training pipeline. Starling-7B outperforms every model to date on MT-Bench except for OpenAI‚Äôs GPT-4 and GPT-4 Turbo .
11. **XTX Markets** is launching a new $10mn challenge fund, the **Artificial Intelligence Mathematical Olympiad Prize** (AI-MO Prize) The grand prize of $5mn will be awarded to the first publicly-shared AI model to enter an AI-MO approved competition and perform at a standard equivalent to a gold medal in the in the International Mathematical Olympiad (IMO) .
12. **Microsoft Research** evaluated GPT-4 for processing ***radiology reports***, focusing on tasks like disease classification and findings summarization. The study found GPT-4 has a sufficient level of radiology knowledge with only occasional errors in complex context that require nuanced domain knowledge. The radiology report summaries generated by GPT-4 were found to be comparable and, in some cases, even *preferred* over those written by experienced radiologists.
13. **AWS** announced ***Amazon Q***, a new generative AI‚Äìpowered assistant for businesses. It enables employees to query and obtain answers from various content repositories, summarize reports, write articles, perform tasks, and more, all within their company's integrated content systems. Amazon Q offers over 40 built-in connectors to popular enterprise systems.
14. 18 countries including the US, Britain signed a detailed international agreement on how to keep artificial intelligence safe from rogue actors, pushing for companies to create AI systems that are ‚Äòsecure by design‚Äô .

**Source**: AI Brews - you can subscribe [here](https://aibrews.com/). it's free to join, sent only once a week with ***bite-sized news, learning resources and selected tools.*** I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks."
69,learnmachinelearning,gpt,top,2020-09-13 12:49:48,"GPT-3 concrete real-world examples of what it can do. Do you think GPT-3 will change our lives, or is it just hype? Are the applications really useful and real, in the real-world, or are they only the hand-picked results by the researchers and startup to get some hype around them and followers?",OnlyProggingForFun,False,0.97,48,irxokh,https://www.youtube.com/watch?v=Gm4AMjV8ErM,3,1600001388.0,
70,learnmachinelearning,gpt,top,2023-01-25 15:59:49,a ChatGPT feature to give you prompt suggestions,QuestionAnxious,False,0.95,47,10l1zwj,https://v.redd.it/qjt99akap7ea1,3,1674662389.0,
71,learnmachinelearning,gpt,top,2024-01-04 21:15:12,Natural Language Processing (NLP) Learning Path - In depth,millhouse056,False,0.98,47,18yo5kp,https://www.reddit.com/r/learnmachinelearning/comments/18yo5kp/natural_language_processing_nlp_learning_path_in/,9,1704402912.0,"Hi friends, i'm currently engaged in NLP and created an pretty extense roadmap or learning path so begginers don't feel lost, it covers from the basics to advanced cutting-edge concepts.

Feedback is appreciated.

&#x200B;

\_-\_-\_-\_-\_-\_-\_-\_-\_-\_-\_-\_-\_-\_-\_-\_-\_-\_-

&#x200B;

NLP Learning Roadmap

1. Prerequisites:

&#x200B;

* Mathematics:

&#x200B;

* Linear algebra
* Probability and statistics

&#x200B;

* Programming:

&#x200B;

* Proficiency in a programming language (e.g., Python)

**2. Introduction to NLP:**

&#x200B;

* Definition      and scope of NLP
* Historical      development of NLP
* Key challenges      and applications

**3. Text Analysis:**

&#x200B;

* **Lexical Analysis:**

&#x200B;

* Word meaning and structure

¬∑ Morphology (word formation)

¬∑ lemmatization (base form identification)

&#x200B;

* **Syntactic Analysis:**

¬∑ Parts-of-speech tagging

¬∑ Dependency parsing

¬∑ Constituency parsing

&#x200B;

* **Semantic Analysis:**

¬∑ Extracting meaning

¬∑ Encompassing word embedding models like Word2Vec and GloVe

¬∑ Topic modeling

&#x200B;

* **Semantic Analysis:**

¬∑ Coreference resolution

¬∑ Discourse analysis

&#x200B;

**3. Text Processing:**

&#x200B;

* **Tokenization:**

&#x200B;

* Sentence tokenization
* Word tokenization
* Subword tokenization (Byte Pair Encoding, SentencePiece)

&#x200B;

* **Stop Words Removal:**

&#x200B;

* Importance and impact on NLP tasks
* Customizing stop word lists

&#x200B;

* **Stemming and Lemmatization:**

&#x200B;

* Porter stemming algorithm
* Snowball stemming algorithm
* Lemmatization techniques and challenges

&#x200B;

* **Part-of-Speech Tagging:**

 

* POS tagging algorithms (HMM-based, rule-based, and neural-based)
* Fine-grained POS tagging

**4. Text Representation:**

&#x200B;

* **Bag of Words (BoW):**

 

* Term Frequency (TF) and Inverse Document Frequency (IDF)
* Bag of N-grams

&#x200B;

* **TF-IDF:**

 

* Calculating TF-IDF scores
* Applications in information retrieval

&#x200B;

* **Word Embeddings:**

 

* Word2Vec:

&#x200B;

* Continuous Bag of Words (CBOW) model
* Skip-gram model
* GloVe (Global Vectors for Word Representation)

&#x200B;

* **Contextual Embeddings:**

 

* ELMo (Embeddings from Language Models)
* ULMFiT (Universal Language Model Fine-tuning)
* OpenAI GPT (Generative Pre-trained Transformer)

**5. NLP Libraries and Tools:**

&#x200B;

* NLTK      (Natural Language Toolkit)
* SpaCy
* scikit-learn
* Transformers      library (Hugging Face)

**6. Statistical Language Models:**

&#x200B;

* **N-grams:**

 

* Unigrams, bigrams, and trigrams
* N-gram language models

&#x200B;

* **Hidden Markov Models (HMM):**

 

* Basics of HMMs
* Applications in part-of-speech tagging

**7. Machine Learning for NLP:**

&#x200B;

* **Supervised Learning:**

 

* Text classification algorithms (Naive Bayes, Support Vector       Machines)
* Evaluation metrics (precision, recall, F1-score)

&#x200B;

* **Named Entity Recognition (NER):**

 

* Rule-based NER
* Machine learning-based NER
* Evaluation metrics for NER

&#x200B;

* **Sentiment Analysis:**

 

* Sentiment lexicons
* Machine learning approaches for sentiment analysis

**8. Sequence-to-Sequence Models:**

&#x200B;

* **Recurrent Neural Networks (RNN):**

 

* Vanishing and exploding gradient problems
* Bidirectional RNNs

&#x200B;

* **Long Short-Term Memory (LSTM):**

 

* Architecture and key components
* Gating mechanisms

&#x200B;

* **Gated Recurrent Unit (GRU):**

 

* Simplified gating compared to LSTM
* Applications and advantages

**9. Deep Learning Architectures for NLP:**

&#x200B;

* **Convolutional Neural Networks (CNN) for Text:**

 

* Text classification with CNNs
* Hierarchical and multi-channel CNNs

&#x200B;

* **Transfer Learning in NLP:**

 

* Fine-tuning pre-trained models
* Universal Sentence Encoder

&#x200B;

* **Transformer Architecture:**

 

* Self-attention mechanism
* Multi-head attention
* Positional encoding

**10. Transduction and Recurrency:**

&#x200B;

* **Transduction in NLP:**

 

* Definition and applications
* Challenges in sequence-to-sequence transduction

&#x200B;

* **Recurrent Neural Networks (RNN):**

 

* Applications beyond sequence-to-sequence tasks
* Challenges in training RNNs

**11. Advanced Topics in Sequence Modeling:**

&#x200B;

* **Attention Mechanism:**

 

* Scaled Dot-Product Attention
* Position-wise Feedforward Networks

&#x200B;

* **Self-Attention Mechanism:**

 

* The concept of self-attention
* Layer normalization in self-attention

&#x200B;

* **Multi-Head Attention:**

 

* Motivation and benefits
* Combining multiple attention heads

**12. Syntax and Parsing:**

&#x200B;

* **Dependency Parsing:**

 

* Dependency tree representation
* Transition-based and graph-based parsing

&#x200B;

* **Constituency Parsing:**

 

* Treebank representation
* Earley parsing algorithm

&#x200B;

* **Parsing Techniques:**

 

* Chart parsing (CYK parser)
* Shift-Reduce parsing

**13. Semantic Role Labeling (SRL) and Coreference Resolution:**

&#x200B;

* **Semantic Role Labeling:**

&#x200B;

* PropBank and FrameNet
* Neural approaches to SRL

&#x200B;

* **Coreference Resolution:**

&#x200B;

* Mention detection
* End-to-end coreference resolution models

**14. Evaluation Metrics:**

&#x200B;

* Precision,      Recall, F1-score
* BLEU      score for machine translation
* Perplexity      for language models

**15. NLP in Industry and Research:**

&#x200B;

* Case      studies and applications in various domains (healthcare, finance, legal,      etc.)
* Emerging      research trends in NLP

**16. Ethical Considerations and Bias in NLP:**

&#x200B;

* **Addressing Bias in NLP Models:**

&#x200B;

* Identifying and mitigating biases in training data
* Fairness-aware machine learning

&#x200B;

* **Ethical Considerations in NLP Research and      Deployment:**

&#x200B;

* Privacy concerns in NLP
* Responsible AI practices in NLP

**17. Continuous Learning and Keeping Updated:**

&#x200B;

* Follow      conferences (ACL, NAACL, EMNLP)
* Engage      with the NLP community
* Explore      recent research papers and advancements (Arxiv, NeurIPS)

**18. Projects and Hands-on Practice:**

&#x200B;

* Apply      knowledge through practical projects
* Contribute      to open-source NLP projects
* Participate      in Kaggle competitions

==============================="
72,learnmachinelearning,gpt,top,2023-05-03 23:35:25,"CheatsheetGPT: Over 600 equations, including ML and RL",Sensitive_Head4946,False,0.88,47,1373csa,https://www.reddit.com/r/learnmachinelearning/comments/1373csa/cheatsheetgpt_over_600_equations_including_ml_and/,11,1683156925.0,"Hi everyone,

Recently I got access to GPT4 and decided to try something a little peculiar: what if I asked it to generate hundreds of equations on topics that are relatively important but also less covered subjects for brainstorming reasons. I then asked GPT to grade the importance of every relation or even explain it.

I tried to make this practical for my own consumption but wanted to share in case someone has some good feedback or can find it useful. 

It‚Äôs interactive and settings are saved in the link. Recommended consumption on a desktop: 

https://tchristos.com/other/the-wall/

https://tchristos.com/other/the-wall/?darkMode=false&option=data-ds-grade&palette=5&zen=true

Hope you enjoy and let me know if you have any feedback or want access to the list of equations

PS: some hallucination"
73,learnmachinelearning,gpt,top,2023-05-01 19:17:41,From Zero to GPT & beyond (a beginner friendly tutorial with PyTorch),brainxyz,False,0.88,42,134yhpy,https://youtu.be/l-CjXFmcVzY,0,1682968661.0,
74,learnmachinelearning,gpt,top,2023-09-12 13:42:02,This is why LLMs have flooded the NLP market in the past 1 year üëá (A Brief History of NLP),japkeerat,False,0.82,43,16grq5y,https://www.reddit.com/r/learnmachinelearning/comments/16grq5y/this_is_why_llms_have_flooded_the_nlp_market_in/,15,1694526122.0,"Text Generation has been the hottest topic in Natural Language Processing. Recurrent Neural Networks (RNNs) were among the Algorithms to generate text. How RNNs generated text is by essentially predicting the next word given the previous few words. At one-stage RNNs were the hottest commodity one could have. But researchers were worried about 1 problem.

RNNs had a context-length problem. To understand what is context-length, consider an analogy. You started reading a book, it‚Äôs 100 pages long and when you read each page, details of previous pages start to get a little hazy. Haziness keeps on increasing to the point that when you reach page 50, you don‚Äôt remember anything from the first 5 pages. That is exactly what the problem is with RNNs.

To solve this, researchers developed another algorithm called the Long-Short Term Memory (LSTM) and another variant called Bidirectional Long-Short Term Memory (Bi-LSTM) which had a larger context-length than RNNs. Let‚Äôs get back to the book analogy. This time while reading, you are making notes. When you go ahead to a new page and your previous pages information start to get hazy, you look back at these notes to refresh your memory. It‚Äôs oversimplified, but that‚Äôs basically how an LSTM works.

LSTMs were not perfect. There were a number of new issues that came up in order to resolve the previous one. Meanwhile, other areas of research and technological advancements were heating up. Hardware was getting more and more prominent and with cloud getting popular, it was easily accessible. And on the research side, a new kind of Algorithm came up that shaped the entire NLP domain from here on - Attention Mechanism.

Attention Mechanism, as you might have guessed, is all about telling the more sophisticated algorithms where to ‚Äúfocus‚Äù. It‚Äôs the same way how we focus more on certain parts of the meeting we attend than the entire meeting itself. In context of NLP, the Mechanism became the core part for better algorithms. These better algorithms could keep larger context-lengths and at the time of predicting the next word, ask the Attention Mechanism about what to focus on while predicting the next word. This was an era-defining discovery in NLP as the algorithms that came up after this were the Transformers.

Consider jigsaw puzzles. You start by looking at all the pieces at once and join the pieces together. Initially, it is random. You join a couple of pieces at the top left corner, a few in the centre and a couple more defining the right edge. You are doing it all at once. Transformers basically work the same way. They could look at longer context-lengths, all at once, courtesy of Attention Mechanism. This means, they can not only work with a sentence, they can work with an entire paragraph.  With time, these Transformers started becoming more and more sophisticated. It eventually reached to a point that the only thing that was keeping these algorithms in handcuffs was the lack of data.

Until recently, these algorithms were trained on a specific data but when algorithms became too powerful, researchers started throwing every kind of data they could find on the internet easily. It could be articles like this, your social media posts, exam papers and solutions, and ebooks in any language they could find and hoped the algorithms learnt it all. And they were right. Algorithms started learning all of it to the point that you could ask models to explain concepts of LLMs in how Shakespeare would write and it would give a real-sounding responsive. These algorithms were Large! And hence, became known as Large Language Models (LLMs).

There we are now. With LLMs. OpenAI, technically, won the race for LLM development. They brought everybody‚Äôs attention to LLMs first with GPT-2, but GPT-3 was where shit hit the roof and every company that had deep pockets started investing in LLMs.  The result? We now have a new LLM getting released EVERY. SINGLE. DAY.

*I post articles like these every few days on X. If you like this post, please* [follow me on X!](https://twitter.com/JapkeeratS/)

*NOTE: To make it simple for anybody, even without a tech background, to understand, a few things were oversimplified. I will be sharing soon on* [my X handle](https://twitter.com/JapkeeratS) *a technical version.*"
75,learnmachinelearning,gpt,top,2022-01-07 13:14:54,A quick review of GPT-3 | What is it and how does it work?,turpyturp,False,0.87,44,ry74jf,https://www.youtube.com/watch?v=xB6hZwYsV2c,1,1641561294.0,
76,learnmachinelearning,gpt,top,2023-01-16 19:21:18,Today we go over creating an Unity ChatGPT Client to allow us to communicate with our ChatGPT API and this will be the beginnings of getting ChatGPT HTTP responses into Unity (full video and playlist in comments),dilmerv,False,0.92,43,10doqua,https://v.redd.it/ixwf3g7syhca1,2,1673896878.0,
77,learnmachinelearning,gpt,top,2022-06-03 18:16:55,"What questions should I ask Hugging Face's Chief Evangelist next week, fresh off the company's $100M Series C raise on a $2B valuation to build the GitHub of ML?",4thBrain,False,0.92,45,v45gjp,https://www.reddit.com/r/learnmachinelearning/comments/v45gjp/what_questions_should_i_ask_hugging_faces_chief/,10,1654280215.0,"I've got the unique opportunity to host a live event next week where [Julien Simon](https://www.linkedin.com/in/juliensimon/), Hugging Face's chief evangelist, will be presenting on Building NLP Applications with Transformers.

He's going to present a few slides and then do a live demo of how to build an end-to-end ML application.

Then I've got 10 minutes or so to ask him anything I want.

**What would you ask him?**

Here's my working list of questions:

* Hugging Face is doing so many amazing things.  As an early ML practitioner or a student trying to break into ML, where would you recommend focusing your time if you want to understand how to apply Hugging Face tools in a hands-on way?  Are there any resources that you would recommend our audience check out first?
* What is your perspective on the difference between a Data Scientist, Machine Learning Engineer, and MLOps Engineer in today‚Äôs AI market?  What about at Hugging Face - how does your company make these distinctions?
* How do you think about what is actually happening to the underlying model when a general pre-trained transformer model - say, GPT-2 or GPT-3 - gets fine-tuned with unique text, image, speech, or time-series data?

Note:

Keep in mind that this guy is the real deal.  He wrote the book on Learning Amazon SageMaker (2nd edition last year) while he was a Principal Technical Evangelist for AWS.  Prior to joining AWS, Julien served for 10 years as CTO and VP of Engineering in large-scale web startups, and also wrote the first French-language Linux documentation back in 1992!"
78,learnmachinelearning,gpt,top,2023-12-07 01:31:55,Why can't AI models do complex math?,open_23,False,0.7,42,18ck15r,https://www.reddit.com/r/learnmachinelearning/comments/18ck15r/why_cant_ai_models_do_complex_math/,93,1701912715.0,"Computers, at its most fundamental level, is made up of boolean logic. Mathematics is basically the language of logic.

SHouldn't AI models, or computers in general be able to do more advanced math than just crunching large numbers? Why haven't anyone used computers to solve any of the Millenium Prize Problems or some other difficult proof. 

GPT-4 and recently  Gemini, has decent enough grade school level math solving capabilities but absolute atrocious at solving slightly more complex problems. But, I guess thats to be expected since they're LLMs. But, why hasn't anyone built an AI model geared towards just solving mathemaths problems? Also, what kind of different architecture would such a model need?"
79,learnmachinelearning,gpt,top,2023-02-06 02:29:05,Hey Reddit! I created a tutorial on how to build a Neural Network in PyTorch using ChatGPT,mechalf11,False,0.89,42,10uv4yq,https://www.reddit.com/r/learnmachinelearning/comments/10uv4yq/hey_reddit_i_created_a_tutorial_on_how_to_build_a/,12,1675650545.0,"Hello all,

I have been using ChatGPT extensively in my work and research, and I wanted to share my experience using it for creating Neural Networks in PyTorch. I created a quick tutorial, and would be curious on your feedback, and hopefully it helps others get started with this fantastic tool! The goal of the tutorial is to have those with little experience coding, little experience with PyTorch, or those who just want to use ChatGPT in a productive+cool way, get started. I am a firm believer that ChatGPT is here to stay, and the earlier we start implementing it into our daily workflows, the faster we will be able to leverage its full potential.

Code + detailed screenshots and instructions are available here: [https://medium.com/p/d6eefffab467](https://medium.com/p/d6eefffab467)"
80,learnmachinelearning,gpt,top,2023-12-26 07:39:32,Can you guess who wrote this code ? A Developer or a Researcher/Scientist,Left_Papaya_9750,False,0.86,40,18r2vqv,https://www.reddit.com/r/learnmachinelearning/comments/18r2vqv/can_you_guess_who_wrote_this_code_a_developer_or/,47,1703576372.0,"    import torch 
    from torch.optim.lr_scheduler import CosineAnnealingLR
    from gpt import GPTLanguageModel
    from transformers import GPT2Tokenizer
    from tqdm import tqdm
    from utils.draw_plots import Draw
    import pynvml as nvml
    import os
    import time
    import wandb
    from utils import draw_stuff
    from torch.utils.data import DataLoader, SubsetRandomSampler
    from utils.load_data import LoadDataset
    import signal 
    from colorama import Fore
    from queue import Queue
    
    class Train():
        def __init__(self, **kwargs,):
            torch.manual_seed(1137)
            model= GPTLanguageModel()
            gpu_idx= 0 if not 'gpu_index' in kwargs else kwargs['gpu_index']
            nvml.nvmlInit()
            os.system(""cls"" if os.name == 'nt' else 'clear')
            draw_stuff.draw()
            self.enc= GPT2Tokenizer.from_pretrained('gpt2')
            self.device='cuda' if torch.cuda.is_available else 'cpu'
            self.m= model.to(self.device)
            self.block_size= 256 if not 'block_size' in kwargs else kwargs['block_size']
            self.batch_size= 100 if not 'batch_size' in kwargs else kwargs['batch_size']
            self.handle= nvml.nvmlDeviceGetHandleByIndex(gpu_idx)
            self.temp_thres= 85 if not 'temp_threshold' in kwargs else kwargs['temp_threshold']
            self.plot= Draw()
            
        def load_data(self,split, split_per):
            dataset= LoadDataset(split= split, split_per=split_per)
            sampler= SubsetRandomSampler(torch.randint(high=len(dataset), size=(1000,)).tolist())
            data_title= DataLoader(dataset, pin_memory= True, sampler=sampler, drop_last=True)
            return data_title
    
        @torch.no_grad()
        def estimate_loss(self, eval_iters)->torch.tensor:
            out = {}
            print(""Estimating loss\n"")
            self.m.eval()
            for split in ['train', 'val']:
                es_progress= tqdm(total=eval_iters, ncols=100)
                es_progress.colour='red'
                
                losses = torch.zeros(eval_iters)
                for k in range(eval_iters):
                    es_progress.update(1)
                    for X, Y in self.get_batch(split):
                        logits, loss = self.m(X, Y)
                        losses[k] = loss.item()
                out[split] = losses.mean()
            self.m.train()
            return out
        
        def get_batch(self, split):
            data = self.load_data(split, split_per=0.8)
            for idx, data in enumerate(data):
                doc= data
                doc_txt= doc['text'][0]
                title= doc['title'][0]
                encoded_tok= self.enc.encode(doc_txt, add_special_tokens= True)
                doc_txt_enc= torch.tensor(encoded_tok, dtype=torch.long)
                size= self.block_size
                if len(encoded_tok) - self.block_size <= 0 and len(encoded_tok)-20 >=3:
                    size=20
                ix = torch.randint(len(encoded_tok) - size , (self.batch_size,))
                x = torch.stack([doc_txt_enc[i:i+size] for i in ix]).to(self.device) 
                y = torch.stack([doc_txt_enc[i:i+size+1] for i in ix]).to(self.device) 
                yield x, y
                
    
        def display_train_params(self,train_id, device, **kwargs):
            print(f""**NOTE: GPU temperature threshold has been set to {self.temp_thres}¬∞C, when the threshold is reached the training process will halt for a set period.**\n"")
            print(f""INFO {torch.cuda.memory_allocated(device)} Bytes of memory is allocated for this task\n"")
            print(f""""""Training parameters:
                      Device: {nvml.nvmlDeviceGetName(self.handle)}
                      Halt Temperature threshold:{self.temp_thres}
                      Trainable Parameters: {sum(p.numel() for p in self.m.parameters())/1e6, 'M parameters'}
                      Total Epochs: {kwargs['epochs']}
                      Evaluation Iterations: {kwargs['eval_iters']}
                      Evaluation Interval: {kwargs['eval_interval']}
                      Initial Learning rate: {kwargs['learning_rate']}
                      Learning Rate Schduler: Cosine Annealing
                      Total Memory allocated: {torch.cuda.memory_allocated(device)}B
                      \n"""""")
            
            print(f""** Training Started | Train ID : {train_id}**\n"")
    
        
        def train(self, device, train_id, is_save=True, **kwargs):
            
            wandb.init('Training',  
                        project='gpt-model',
                        config={
                            ""initial_learning_rate"":3e-4,
                            ""architecture"":""transformer"",
                            ""dataset"": ""Wikipedia general documents""
                                }
                        )
            docs= []
            eval_interval= 500 if not 'ei' in kwargs else kwargs['ei']
            learning_rate= 3e-4 if 'learning_rate' not in kwargs else kwargs['learning_rate']
            eval_iters= 300 if 'eval_iter' not in kwargs else kwargs['eval_iter']
            max_iters= 10000 if not 'epochs' in kwargs else kwargs['epochs']
            num_doc= 2000
            os.mkdir('results') if not 'results' in os.listdir('.') else None
            os.mkdir(f'results/{train_id}') if train_id not in os.listdir('results') else None
            os.mkdir(f'results/{train_id}/checkpoints') if 'checkpoints' not in os.listdir(f'results/{train_id}') else None
            os.mkdir(f'results/{train_id}/checkpoints/plots') if 'plots' not in os.listdir(f'results/{train_id}/checkpoints') else None
            torch.cuda.empty_cache()
            optimizer = torch.optim.AdamW(self.m.parameters(), 
                                          lr=learning_rate)
            schduler= CosineAnnealingLR(optimizer=optimizer, T_max=max_iters)
    
            self.display_train_params(train_id=train_id, 
                                      device=device, 
                                      epochs=max_iters, 
                                      eval_interval=eval_interval,
                                      eval_iters=eval_iters,
                                      learning_rate=learning_rate
                                      )
            
            epoch_progress_bar= tqdm(total=eval_interval, 
                                     ncols=100
                                    )
            
            epoch_progress_bar.colour='cyan'
            counter= 0
            cont_params={'train_loss':[], 
                         'l_r':[]
                        }
            
            ckpt_params={'train_loss':[], 
                         'val_loss':[],
                         'l_r':[]
                        }
            
            it_cnt=0
            for iter in range(0, max_iters):
                epoch_progress_bar.update(1)
                curr_temperature= nvml.nvmlDeviceGetTemperature(self.handle, 
                                                                nvml.NVML_TEMPERATURE_GPU
                                                               )
        
                if curr_temperature >= self.temp_thres:
                    print(f""\n Set temperature threshold of {self.temp_thres}¬∞C reached halting for {4} seconds "")
                    time.sleep(3)
                    print(""\n Resuming Training "")
                
    
                if iter % eval_interval == 0 or iter == max_iters-1:
                    
                    checkpoint_save_path= f'results/{train_id}/checkpoints/checkpoint-{counter}-epoch{iter}.pth'
                    losses =self.estimate_loss(eval_iters)
                    ckpt_params['l_r'].append(schduler.get_last_lr()[0])
                    ckpt_params['train_loss'].append(losses['train'])
                    ckpt_params['val_loss'].append(losses['val'])
                    wandb.log({""eval epoch"":iter, ""validation loss"":losses['val']})
                    plot_save_path= f'results/{train_id}/checkpoints/plots/checkpoint-{counter}'
        
                    if iter ==0:
                        pass
                    else:
                        
                        self.plot.draw_line(mode='loss', 
                                            train_loss= cont_params['train_loss'],  
                                            epochs=[it for it in range(it_cnt, it_cnt+eval_interval)],
                                            save_path=f'{plot_save_path}-epoch_vs_loss.png'
                                            )
                        self.plot.draw_line(mode='lrvloss',
                                            l_r= cont_params['l_r'],
                                            train_loss=cont_params['train_loss'], 
                                            save_path=f'{plot_save_path}-learning_rate_vs_loss.png'
                                            )
                        
                        self.plot.draw_line(mode='lrve',
                                            l_r= cont_params['l_r'], 
                                            epochs=[it for it in range(it_cnt, it_cnt+eval_interval)],
                                            save_path=f'{plot_save_path}-learning_rate_vs_epoch.png'
                                            )
                        
                        cont_params['l_r'].clear()
                        cont_params['train_loss'].clear()
                        torch.save(self.m.state_dict(), checkpoint_save_path)
                        it_cnt+=iter
        
                    print(f""step {iter+1}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}"") 
                    epoch_progress_bar.close() 
                    epoch_progress_bar = tqdm(total=eval_interval, ncols=100)
                    epoch_progress_bar.colour='cyan'
                    counter+=1
                print(f'loading data for epoch{iter}')
    
    
                for xb, yb in self.get_batch(split='train'):
                    logits, loss = self.m(xb, yb)
                    optimizer.zero_grad(set_to_none=True)
                    loss.backward()
                    optimizer.step()
                    schduler.step()
                    curr_lr= schduler.get_last_lr()[0]
                    train_loss= loss.item()
                cont_params['l_r'].append(curr_lr)
                cont_params['train_loss'].append(train_loss)
                wandb.log({'epoch':iter, 'train loss':train_loss, 'learning rate':learning_rate})            
                epoch_progress_bar.set_description(f""Epoch: {iter}/{max_iters} |current LR- {curr_lr}"")
            
            os.mkdir(f'results/{train_id}/final_plots') if 'final_plots' not in os.listdir(f'results/{train_id}') else None
    
            self.plot.draw_line(mode='loss', 
                                        train_loss= ckpt_params['train_loss'], 
                                        val_loss=ckpt_params['val_loss'], 
                                        epochs=[it for it in range(0,max_iters, 500)],
                                        save_path=f'results/{train_id}/final_plots/plot-epoch_vs_loss.png',
                                        plot_type='ckpt'
                                        )
            self.plot.draw_line(mode='lrvloss',
                                l_r= ckpt_params['l_r'],
                                train_loss=ckpt_params['train_loss'], 
                                val_loss= ckpt_params['val_loss'],
                                save_path=f'results/{train_id}/final_plots/plot-learning_rate_vs_loss.png',
                                plot_type='ckpt'
                                )
            
            self.plot.draw_line(mode='lrve',
                                l_r= ckpt_params['l_r'], 
                                epochs=[it for it in range(0,max_iters, 500)],
                                save_path=f'results/{train_id}/final_plots/plot-learning_rate_vs_loss.png',
                                plot_type='ckpt'
                                )
    
            nvml.nvmlShutdown()
            wandb.finish()
    "
81,learnmachinelearning,gpt,top,2023-07-19 03:03:00,Meta open-sources LLaMA 2 to compete with ChatGPT,Any-Heron-6313,False,0.92,41,153iujc,https://medium.com/p/1370d587b104,3,1689735780.0,
82,learnmachinelearning,gpt,top,2022-02-03 18:39:05,[Project] Refining the Natural language processing course - Feedback v2 and thank you,sb2nov,False,0.89,35,sjqogi,https://www.reddit.com/r/learnmachinelearning/comments/sjqogi/project_refining_the_natural_language_processing/,2,1643913545.0,"I‚Äôm Sourabh, I lead one of the core Tensorflow teams at Google Brain and worked on data products at Coursera with Andrew Ng. Kaushik Rangadurai, ML Engineer at Facebook and I are leading a live, cohort based course on NLP starting March 14th. [https://corise.com/course/natural-language-processing](https://corise.com/course/natural-language-processing).

This is the second run of the class and we learned a lot from the feedback of the reddit community from the first run in November. Some of the changes we're making from the previous iteration:

1/ More focus on transformers and less on RNN/LSTM as hugging face is becoming the defacto for any nlp.

2/ Pytorch lightning has some really easy to use interfaces so better organizing the boiler plate code.

3/ OpenAI has opened the GPT-3 API so a deeper dive into current possibilities.

Would love to continue getting feedback and build this to be a great resource. The plan is to open the content after we refine it to a degree we're happy with. You can join the course (capped at about 30 students) at the link above. If you‚Äôre open to giving feedback on the class on how we can do better, happy to give a discount."
83,learnmachinelearning,gpt,top,2023-03-07 17:07:23,"ChatGPT is coming to Slack, Microsoft's dynamics 365 copilots & all other things in AI.",Opening-Ad-8849,False,0.95,35,11l4x5i,https://aibulletin.substack.com/p/chatgpt-is-coming-to-slack-microsofts,2,1678208843.0,
84,learnmachinelearning,gpt,top,2023-02-12 03:54:05,[N] All of this you need to know happening in ML/AI.,Opening-Ad-8849,False,0.77,31,1106e9p,https://www.reddit.com/r/learnmachinelearning/comments/1106e9p/n_all_of_this_you_need_to_know_happening_in_mlai/,0,1676174045.0,"Hello humans - This is AI Daily by Ovetted, helping you stay updated on AI in less than 5 minutes.

Originally published on [https://www.ovetted.com/ai](https://www.ovetted.com/ai).

### What‚Äôs happening in AI -

[**The AI doctor will see you now: ChatGPT passes the gold-standard US medical exam.**](https://www.dailymail.co.uk/health/article-11732687/The-AI-doctor-ChatGPT-passes-gold-standard-medical-exam.html)

ChatGPT has passed the gold-standard exam required to practice medicine in the US

The artificial intelligence program scored 52.4 and 75 percent across the three-part Medical Licensing Exam (USMLE).

[**Google and Microsoft announced plans to incorporate AI into search engines.**](https://youtu.be/EBDJ9MGSV6k)

Google and Microsoft plan to incorporate AI into their search engines to change how people use the internet. Microsoft has announced that AI will soon allow conversations with its software and search engine Bing, while Google has announced similar plans.

As the most profitable software business is searching both companies are trying to take advantage of AI to rule the search engine market.¬†

[**Integrating the generative AI means a fivefold increase in Computing power & carbon emission.**](https://www.wired.com/story/the-generative-ai-search-race-has-a-dirty-secret/)

The integration of artificial intelligence (AI) into search engines could lead to a significant increase in the amount of energy that tech companies require and the amount of carbon they emit.

Training these models takes a huge amount of computational power, but only big tech companies can do so because they have the resources.

### Snippets -

**Human & AI:** How Will [Humans and A.I](https://www.nytimes.com/2023/02/10/opinion/letters/artificial-intelligence.html?smid=url-share). Get Along?

**OpenAI in office apps:** Microsoft Has Plans to Shove Its Bing AI Into [Word, PowerPoint, and More](https://gizmodo.com/microsoft-bing-ai-powerpoint-word-prometheus-1850098510).¬†

**WTF:** This AI Image Fooled Judges and [Won](https://petapixel.com/2023/02/10/ai-image-fools-judges-and-wins-photography-contest/) a Photography Contest.

**Hype:** Why the ChatGPT AI Chatbot Is [Blowing](https://www.cnet.com/tech/computing/why-the-chatgpt-ai-chatbot-is-blowing-everybodys-mind/) Everybody's Mind.

**Oops:** New AI voice-cloning tools 'add fuel' to [misinformation](https://abcnews.go.com/US/wireStory/new-ai-voice-cloning-tools-add-fuel-misinformation-97046760) fire.

**Oh no:** [Microsoft](https://www.businessinsider.com/microsoft-layoffs-cloud-ai-artificial-intelligence-2023-2?IR=T) is even cutting cloud and AI workers in its plan to lay off 10,000 employees.

**Wow:** AI In 2023 And [Beyond](https://www.forbes.com/sites/forbestechcouncil/2023/02/10/ai-in-2023-and-beyond-the-top-research-and-development-trends-to-keep-an-eye-on/?sh=5e2a45a7deae): The Top Research And Development Trends To Keep An Eye On.

**Realistic** newscasts feature AI-generated [anchors](https://edition.cnn.com/videos/business/2023/02/11/deepfake-newscast-ai-chinese-messaging-wang-pkg-ac360-vpx.cnn) disparaging the US.

**Google** cautions against '[hallucinating](https://www.reuters.com/technology/google-cautions-against-hallucinating-chatbots-report-2023-02-11/)' chatbots.

### Things to try -

* Someone made a **Discord bot** that can **write** **poems, descriptions, and titles on the image you provide**. Using GPT3 & CLIP. - [Try now](https://discord.gg/m4taXd6AB3)
* **Lalal AI** can **extract vocal accompaniment and other instruments** from any audio or video. - [Try now](https://www.lalal.ai/)
* What if you can create your own ChatGPT? well, you can make your own chatbot with your own data by using **customGPT**. - [Try now](https://customgpt.ai/)
* Do you create content for websites or any kind of digital content? Well, **metagenie** can help you to create **metadata like Titles, Descriptions, Tags, and Thumbnail Ideas.**¬†\- [Try now](https://www.metagenieai.com/)
* **Snape** is here to help you write your custom job description generator.¬†- [Try now](https://snape.springworks.in/)
* Give a try to this AI food robot that gives you **food pictures and recipes generated by AI. -** [Try now](https://aifoodrobot.com/)
* Need a **coding assistant** try spell box. That uses artificial intelligence to create the code you need from simple prompts. - [Try now](https://spellbox.app/)"
85,learnmachinelearning,gpt,top,2024-02-19 02:07:46,Is it worth studying NLP?,David_Slaughter,False,0.8,29,1aubsfk,https://www.reddit.com/r/learnmachinelearning/comments/1aubsfk/is_it_worth_studying_nlp/,8,1708308466.0,"So obviously I'm 10 years behind. 10 years late to the pie. But I have to make a selection for one of my semester 2 modules. My last selection is difficult and I'm considering NLP.

But is it even worth learning? People who started AI 10 years ago have already made the likes of ChatGPT and all that. What can a measly student offer the world by studying this?"
86,learnmachinelearning,gpt,top,2023-05-29 17:37:32,"GPT Weekly - 29th May Edition: Facebook's massive STT and TTS Release, AI in Windows, Paralegal jobs are here to stay and more.",level6-killjoy,False,0.85,27,13v1asb,https://www.reddit.com/r/learnmachinelearning/comments/13v1asb/gpt_weekly_29th_may_edition_facebooks_massive_stt/,2,1685381852.0," 

This is a recap covering the major news from last week.

* üî•Top 3 AI news in the past week
* üóûÔ∏è10 AI news highlights and interesting reads
* üßë‚Äçüéì3 Learning Resources

# üî•Top 3 AI news in the past week

## 1. Expanding Language Horizons

Facebook has [released an open source model called MMS (Massively Multilingual Search)](https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/) for STT (speech to text), TTS (text to speech) and language identification. 

This is a big breakthrough. Currently, STT and TTS models recognize only 100 languages. With this the technology has been expanded to 1100 languages. That is 10x the current best. 

Additionally, these models can recognize 4000+ languages. 

As per Facebook, they also have half the error rate of OpenAI‚Äôs Whisper.

These guys are on a roll.

## 2. Bing Chat Enters the OS

After [Google‚Äôs announcement](https://gptweekly.beehiiv.com/p/week-google-ai-large-llm-gpt-plugin), it was time for Microsoft to announce AI products. Here‚Äôs a rundown of what was announced during Microsoft Build:

1. **Windows Copilot**  \- Microsoft is integrating AI directly into the OS. Now you can do everything you could do with Bing Chat but now on the OS. You can do the usual stuff - summarize emails, documents, re-write etc. But it goes beyond that by integrating into the installed applications.

Microsoft is also adopting OpenAI's plugin model. So, **you can use ChatGPT and Bing plugins to interact with the integrated AI.** 

The great thing about it is the direct integration into the OS. Eat your heart out, Mac users ‚Äì at least for now üòÄ. Until Apple announces something similar. And someone will come up with an alternative solution. Especially, because of the privacy concerns with Microsoft telemetry. 

The bad thing is - [the security aspect of the plugins](https://gptweekly.beehiiv.com/p/caution-chatgpt-plugins). It can open a whole new attack vector on the OS and antivirus softwares might struggle with it. 

It also might be the second nail in the coffin for all the summarize, ‚Äútalk to your document‚Äù apps. Once, this feature is integrated with [Google Docs](https://gptweekly.beehiiv.com/p/week-google-ai-large-llm-gpt-plugin) and Microsoft Office - why will you want to pay for extra apps?

1. **Search comes to ChatGPT**  \- Looks like OpenAI had enough of the testing and new features are being rolled out [left](https://gptweekly.beehiiv.com/p/week-google-ai-large-llm-gpt-plugin) and [right](https://gptweekly.beehiiv.com/p/caution-chatgpt-plugins). 

No prizes for guessing the search engine behind it. Ding, Ding, Ding..It‚Äôs Bing!

1. **Co-Pilot in PowerPages** \- Microsoft is now adding AI to their [PowerPages platform](https://powerpages.microsoft.com/en-in/), their low-code tool to build websites. It‚Äôll help users to generate text, forms etc.
2. **Microsoft Fabric** \- A new data analytics platform built on top of Azure Data lake but can get data from S3, Google cloud etc. It can help users build pipelines, write code, and build ML models.

## 3. From Trusted Advisor to Nightmare: The Hazards of Depending on AI

Here‚Äôs a [fun story which is breaking out on Legal twitter](https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html). 

A man filed a personal injury lawsuit against Avianca airlines. Avianca's lawyers wasted no time and requested the judge to dismiss the case. The man's lawyer had a different plan in mind. He submitted a document citing half a dozen cases that bolstered his client's claims.

Here's the twist‚Äîthe judge and Avianca's lawyer couldn't locate any of the referenced cases. Quite a conundrum, right? The lawyer was then asked to provide copies of these elusive cases. The lawyer submitted screenshots as evidence, taking extra precautions to ensure their authenticity. 

You already know the direction this story is taking. 

The lawyer had used ChatGPT to compose his brief. But little did he know that ChatGPT had supplied him with fake cases.

When asked to file tangible copies of these cases, the lawyer turned to ChatGPT once again. ChatGPT had reassured him that the cases were genuine. Feeling emboldened, the lawyer used ChatGPT to provide the requested copies. He even went as far as incorporating chat screenshots into a legal document.

The lawyer maintains that it was never his intention to deceive the court. He expressed regret for relying on ChatGPT for their research. Unfortunately, the judge isn't pleased with this turn of events. The judge has threatened sanctions against both the lawyer and his firm.

It serves as a stark reminder of how ChatGPT has fooled many people. There is a clear warning stating that ChatGPT may produce inaccurate information. But many tend to overlook these warnings. Even legal professionals!!

This story carries significant importance for those who fear job insecurity. The lawyer and his firm could have prevented the entire debacle. They should've used paralegal services. They instead relied on ChatGPT's. It's a hard lesson learned the hard way.

My sincere hope is that this story serves as a valuable lesson. It helps people avoid making similar mistakes. The legal community might become apprehensive about ChatGPT's use moving forward.

# üóûÔ∏è10 AI news highlights and interesting reads

1. [OpenAI says in 10 years AI could be as productive as one of today‚Äôs large corporations](https://openai.com/blog/governance-of-superintelligence). This poses an existential risk and they suggest some regulations to manage it. This poses an existential risk and they suggest some regulations to manage it. To achieve this, countries need to form something like the [IAEA](https://en.wikipedia.org/wiki/International_Atomic_Energy_Agency). The IAEA is an intergovernmental agency under the UN to oversee nuclear energy. This ‚ÄúAI agency‚Äù will monitor the AI systems and conduct inspections. Just like nuclear energy is tracked through signatures, they suggest using compute and energy usage to track systems.
2. In the meantime, [Google is working on voluntary rules](https://techcrunch.com/2023/05/24/eu-google-ai-pact/) until there are some real regulations in place. 
3. [As per Pew Research, 58% of Americans have heard of ChatGPT. Even less - 14% have tried ChatGPT. ](https://www.pewresearch.org/short-reads/2023/05/24/a-majority-of-americans-have-heard-of-chatgpt-but-few-have-tried-it-themselves/)
4. Sharing prompts and results has been a pain. Taking screenshots is one way. But then everyone has to type in the prompts manually. Or you can share as plain text. But ChatGPT results are non-deterministic. So, the results might not be the same. Even the lawyer above would‚Äôve loved this feature. Now you will be able to [share your ChatGPT conversations publicly](https://help.openai.com/en/articles/7925741-chatgpt-shared-links-faq).
5. LLM Agents and plugins need to connect to tools to perform the tasks outside the LLM environment. So, it is important for the LLM to know which API to call and pass correct arguments. [Gorilla is a fine-tuned Llama-model which can generate the correct call and arguments](https://gorilla.cs.berkeley.edu/). 
6. If you are trying to build something beyond a document summarizer or a wrapper around GPT4 API, [things can be hard](https://www.honeycomb.io/blog/hard-stuff-nobody-talks-about-llm). Finding the correct context window, dealing with slow responses (I am looking at you GPT-4) etc are some of the problems. 
7. [The AI boom could expose investors‚Äô natural stupidity](https://www.reuters.com/breakingviews/ai-boom-could-expose-investors-natural-stupidity-2023-05-19/). 
8. [Chatbot leaderboard for the week](https://lmsys.org/blog/2023-05-25-leaderboard/). GPT-4 is still ahead.
9. [Google‚Äôs flood warning system is now available in 80 countries. ](https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/)
10. [GPT detectors are biased against non-native English writers](https://arxiv.org/abs/2304.02819)

# üßë‚Äçüéì3 Learning Resources

1. [Build a product using Replit+AI](https://www.priyaa.me/blog/building-with-ai-replit). The author is a non-technical person who won a hackathon competing with engineers. 
2. [LangChain 101](https://replit.com/@MckayWrigley). 
3. [NLP Course from HuggingFace](https://huggingface.co/learn/nlp-course/chapter0/1)

That‚Äôs it folks. Thank you for reading and have a great week ahead.

**If you are interested in a focused weekly recap delivered to your inbox on Mondays you can**[ subscribe here. It is FREE!](https://gptweekly.beehiiv.com/subscribe)"
87,learnmachinelearning,gpt,top,2020-10-24 06:03:11,How to Perform Advanced AI Text Generation With Only a Few Lines of Code Using GPT-2,VennifyAI,False,0.94,29,jh3wuq,https://youtu.be/IIa0WI_HblI,3,1603519391.0,
88,learnmachinelearning,gpt,top,2023-03-09 18:15:03,Training Transformer Networks in Scikit-Learn?!,cmauck10,False,0.86,27,11mzbrs,https://www.reddit.com/r/learnmachinelearning/comments/11mzbrs/training_transformer_networks_in_scikitlearn/,2,1678385703.0,"Have you ever wanted to use handy scikit-learn functionalities with your neural networks, but couldn‚Äôt because TensorFlow models are not compatible with the scikit-learn API?

I‚Äôm excited to introduce one-line wrappers for TensorFlow/Keras models that enable you to use TensorFlow models within scikit-learn workflows with features like Pipeline, GridSearch, and more.

[Swap in one line of code to use keras\/TF models with scikit-learn.](https://preview.redd.it/ulmww4ovwqma1.png?width=960&format=png&auto=webp&s=6da7628298976fc3d72e771abe2546bbf32c1e0e)

Transformers are extremely popular for modeling text nowadays with GPT3, ChatGPT, Bard, PaLM, FLAN excelling for conversational AI and other Transformers like T5 & BERT excelling for text classification. Scikit-learn offers a broadly useful suite of features for classifier models, but these are hard to use with Transformers. However not if you use these wrappers we developed, which only require changing one line of code to make your existing Tensorflow/Keras model compatible with scikit-learn‚Äôs rich ecosystem!

All you have to do is swap `keras.Model` ‚Üí `KerasWrapperModel`, or `keras.Sequential` ‚Üí `KerasSequentialWrapper`. The wrapper objects have all the same methods as their keras counterparts, plus you can use them with tons of awesome scikit-learn methods.

You can find a demo jupyter notebook and read more about the wrappers here: [https://cleanlab.ai/blog/transformer-sklearn/](https://cleanlab.ai/blog/transformer-sklearn/)"
89,learnmachinelearning,gpt,top,2024-01-10 06:50:09,Looking for a reason to keep learning about LLMs,SnooBeans7516,False,0.85,28,19323dh,https://www.reddit.com/r/learnmachinelearning/comments/19323dh/looking_for_a_reason_to_keep_learning_about_llms/,24,1704869409.0,"So something's been on my mind recently and I wanted to get Reddit's thoughts.

The thing that is troubling me as I learn more of the technical stuff, it seems that for a lot of language based NLP tasks, ChatGPT or these other foundation models seem SOTA for 99% of tasks. I was really excited to start training and working with BERT-based models, but find that a lot of the time I could get similar or better results just prompt engineering ChatGPT properly.

&#x200B;

So is it worth learning how to build and train these models? Or is my time really just better spent learning to use the APIs in effective ways like in RAG applications or in employing agents?

Unlike with CV and things like ControlNet, I don't see a lot of great applications of learning the technical stuff for someone who isn't a research scientist at a lab.

&#x200B;

(for some context, I'm a PM who wanted to upskill in this area, but feeling like I'm wasting a lot of my time reading all the new papers and working with models at home  :/. )"
90,learnmachinelearning,gpt,top,2023-01-08 03:14:39,"Question : ( CS, Mathematics, AI, ML, Data Science ) Where and How I Would start",0xSowrd,False,0.85,31,106868c,https://www.reddit.com/r/learnmachinelearning/comments/106868c/question_cs_mathematics_ai_ml_data_science_where/,10,1673147679.0,"if I wanted to build things like tech's we see today ( ChatGPT, Midjourney, stable diffusion ) from the perspective of principle  "" trivial "" version of it

&#x200B;

&#x200B;

I really feel overwhelmed and I want accomplish this so bad I'll put the time and the effort for it to understand truly how things works "" from scratch "" and be able to build my own things if I want too  


Note:  
I'm not saying that I want to be a master in each of these field but I want at least to be an advanced in each one and to be able to keep up if I need to learn something or create something, I hope someone truly help!   


thank you"
91,learnmachinelearning,gpt,top,2023-07-16 08:58:51,Avoid clickbait content on Youtube with ChatGPT3.5/4,Particular_Account_2,False,0.82,25,1511b08,https://www.reddit.com/r/learnmachinelearning/comments/1511b08/avoid_clickbait_content_on_youtube_with_chatgpt354/,8,1689497931.0,"I built an app that I've been using for weeks now which lets you view a brief summary of any youtube video so you can avoid annoying clickbait content or just quickly get the gist of a video. 

The app that uses the web version of chatGPT3.5/4 rather than the API so that summaries can be generated for free by anyone logged in to ChatGPT. I've uploaded it to the Chrome store. Check it out here:

[https://chrome.google.com/webstore/detail/youtube-video-summaries-p/bajbfiocljodaddcdfkndlmgjbjbmjkf](https://chrome.google.com/webstore/detail/youtube-video-summaries-p/bajbfiocljodaddcdfkndlmgjbjbmjkf)

Take it for a spin, leave a review, and/or some feedback -- would love some feedback on the prompts I'm using. Thanks!"
92,learnmachinelearning,gpt,top,2023-08-02 18:21:44,A Brief History of Natural Language Generation [Timeline] ‚ÄîThoughts? Corrections? Suggestions? Thanks!,Britney-Ramona,False,0.86,29,15ggib0,https://i.redd.it/meslnx7moqfb1.png,8,1691000504.0,
93,learnmachinelearning,gpt,top,2023-04-08 03:04:00,Energy Constraints and Costs in Massive Machine Learning Model Training,mechkeyboard7065,False,0.91,26,12f9cvx,https://www.reddit.com/r/learnmachinelearning/comments/12f9cvx/energy_constraints_and_costs_in_massive_machine/,7,1680923040.0,"Adding on to my [last](https://www.reddit.com/r/learnmachinelearning/comments/12ebceo/alternatives_to_training_massive_ml_models_on/) post, here's some of what I've found about the potential constraints and costs associated with training massive machine learning models. 

&#x200B;

**Energy as a constraint in ML model training:**

\- GPT-3, as an example, is estimated to have consumed around **936 MWh** during its training.  
\- If there were **$100B model training runs** in the future, it would consume approximately **20,347,826 MWh** or **20,347,826,000 KWh**.  
\- This would cost around **$1,017,391,300**, which is about **1%** of the total cost (assuming $0.05 KWh). The cost could go up to **$3B** if we assume $0.15 KWh.

&#x200B;

**Power generation comparison:**

\- One nuclear power plant can generate around **4,727,764 MWh** in a year.

&#x200B;

**Main constraints in massive model training runs apart from GPUs:**

\- Data movement through machines  
\- The amount of data that can be moved  
\- The amount of data the model has already been trained on  
\- Networking and bandwidth limitations  
\- System-specific bottlenecks  
\- Model training algorithm design (e.g., parallel processing, processing power requirements)

&#x200B;

**Potential $10T investment in ML models: Where would the money go?**

\- **17% ($1.7T)** \- Data collection, validation, and annotation  
\- **23% ($2.3T)** \- Research  
\- **60% ($6T)** \- Production (infrastructure, integration, maintenance)

&#x200B;

**Current and projected annual spend on GPUs:**  
\- **$40B** in 2022  
\- Projected to be **$400B** in 10 years

&#x200B;

I hope someone might find this information useful. It's definitely made me question the future impact as these models scale. As always, I'm open to corrections and eager to learn more. Let me know if you have any questions or additional insights."
94,learnmachinelearning,gpt,top,2020-11-29 20:52:26,What is the hype about the GPT-3 transformer and what is real? (GPT3 paper deep dive),gordicaleksa,False,0.88,27,k3h26h,https://youtu.be/fVt387VZJe8,0,1606683146.0,
95,learnmachinelearning,gpt,top,2023-10-13 14:23:10,Authoring another course about LLMs. Learn by Doing LLM Projects.,pmartra,False,0.88,25,176zx1m,https://www.reddit.com/r/learnmachinelearning/comments/176zx1m/authoring_another_course_about_llms_learn_by/,5,1697206990.0,"Hi, I'm working on a course about LLMs on GitHub, it's totally free and under MIT license,  So there are no restrictions.

Here the link: [https://github.com/peremartra/Large-Language-Model-Notebooks-Course](https://github.com/peremartra/Large-Language-Model-Notebooks-Course)

I'm still working on It, but now I'm feeling comfortable with the variety and quality of the content. By the moment is a small repository with just 80 Stars.

My intention is to make the course more accessible to a wider audience, and, if possible, encourage  reporting any issues  encounter or suggesting improvements through the 'Discussion' section.

I'm eager to receive feedback.

Now, I'll provide an overview of the currently available content, and then I'll share a couple of questions I have about how to proceed with the course.

[Large Language Models Course: Learn by Doing LLM Projects.](https://github.com/peremartra/Large-Language-Model-Notebooks-Course)

* Introduction to LLM with OpenAI.
   * Create a first Chatbot using FPT 3.5.
   * Create a Natural Language to SQL Translator using OpenAI.
* Vector Databases with LLM.
   * Influencing Language Models with Information stored in ChromaDB.
* LangChain & LLM Apps.
   * RAG. Use the Data from Dataframes with LLMs.
   * Create a Moderation System using LangChain.
      * OpenAI.
      * GPT\_j.
      * LLama-2.
   * Create a Data Analyst Assistant using a LLM Agent.
* Evaluating LLMs
   * Evaluating Summarization with ROUGE.
* Fine-Tuning & Optimization.
   * Prompt-tuning using PEFT.
   * Fine-Tuning with LoRA.
   * Fine-Tuning a Large Model in a GPU using QLoRA. 

That's all for the moment, but I'm adding new content regularly. I'm working on it only in my spare time (mainly nights when the family goes to sleep).

\_\_\_

I have a doubt, I don't know if add some information about platforms like W&B or Cohere?  or maybe it is a better idea to stay with more Open-Source libraries?

On the other hand, my intention is to develop a couple of projects utilizing the techniques covered in the initial part of the course (which I am currently working on).

Some of these projects will be hosted in the cloud on major platforms such as Azure or GCP, or AWS. Any preference?

Furthermore, there is a plan to create a third section that explains how Large Language Models (LLMs) fit into large-scale enterprise solutions, defining architectures in which LLMs are used but are not the sole components of the project.

I don't intend to create a community outside of GitHub, but I would like the repository to have more activity and not be the one determining the course's direction.

Hope you like it, and lease, feel free to contribute.

&#x200B;"
96,learnmachinelearning,gpt,top,2023-12-28 05:30:19,PyTorch ML paper implementation collection,genesis_2602,False,0.96,24,18sm585,https://www.reddit.com/r/learnmachinelearning/comments/18sm585/pytorch_ml_paper_implementation_collection/,0,1703741419.0,"Hey everyone! Recently I've been working on PyTorch implementations for popular machine learning papers. I've created a list of these implementations on my GitHub page ([here](https://github.com/stars/gursi26/lists/paper-implementations)).

The implementations include a few neural style transfer approaches, GANs, ViT as well as NLP papers like LSTM, GRU, ELMo, Attention, Transformers, GPT, BERT, etc.

Just wanted to share this as a resource that someone may find helpful! I am also open to contributions to any of these repos, since some of them have incomplete result demos."
97,learnmachinelearning,gpt,top,2022-05-10 11:23:32,Applied NLP: Cluster and analyze text using both embedding and GPT models (interactive visualizations),jayalammar,False,0.91,23,umgfdo,https://txt.cohere.ai/combing-for-insight-in-10-000-hacker-news-posts-with-text-clustering/,6,1652181812.0,
98,learnmachinelearning,gpt,top,2023-12-17 13:15:47,I can't stop using ChatGPT and I hate it.,t0hli,False,0.63,22,18kh4n5,https://www.reddit.com/r/learnmachinelearning/comments/18kh4n5/i_cant_stop_using_chatgpt_and_i_hate_it/,108,1702818947.0,"I'm trying to learn various topics like Machine Learning and Robotics etc., and I'm kinda a beginner in programming.

For any topic and any language, my first instinct is to

1. go to ChatGPT,
2. write down whatever I need my code to do,
3. copy paste the code
4. if it doesn't give out good results, ask ChatGPT to fix whatever it's done wrong
5. repeat until I get satisfactory result

I hate it, but I don't know what else to do.

I think of asking Google what to do, but then I won't get the exact answer I'm looking for, so I go back to ChatGPT so I can get exactly what I want. I don't fully understand what the GPT code does, I get the general gist of it and say ""Yeah that's what I would do, makes sense"", but that's it.

If I tried to code whatever GPT printed out, I wouldn't get anywhere.

I know I need to be coding more, but I have no idea where to start from, and why I need to code when ChatGPT can do it for me anyway. I'm not defending this idea, I'm just trying to figure out how I can code myself.

I'd appreciate your thoughts and feedback."
99,learnmachinelearning,gpt,top,2023-10-25 14:48:15,[Long read] Deep dive into AutoGPT: A comprehensive and in-depth step-by-step guide to how it works,davorrunje,False,0.88,23,17g6idh,https://www.reddit.com/r/learnmachinelearning/comments/17g6idh/long_read_deep_dive_into_autogpt_a_comprehensive/,8,1698245295.0,"We tried to figure out exactly how the AutoGPT works at the level of prompts so we got our hands dirty and documented how exactly each and every prompt was constructed. The result is in the following LONG document. It proved to be very useful for understanding the details of its inner workings and we hope the community would benefit from it as well:  
[https://airt.hashnode.dev/long-read-deep-dive-into-autogpt-a-comprehensive-and-in-depth-step-by-step-guide-to-how-it-works](https://airt.hashnode.dev/long-read-deep-dive-into-autogpt-a-comprehensive-and-in-depth-step-by-step-guide-to-how-it-works)"
100,learnmachinelearning,gpt,comments,2023-04-30 15:45:04,I don't have a PhD but this just feels wrong. Can a person with a PhD confirm?,flaky_psyche,False,0.76,61,133v9s5,https://i.redd.it/fmkvgop7l1xa1.jpg,238,1682869504.0,
101,learnmachinelearning,gpt,comments,2023-12-25 17:15:18,"Have we reached a ceiling with transformer-based models? If so, what is the next step?",swagonflyyyy,False,0.86,62,18qmohw,https://www.reddit.com/r/learnmachinelearning/comments/18qmohw/have_we_reached_a_ceiling_with_transformerbased/,134,1703524518.0,"About a month ago Bill Gates hypothesized that models like GPT-4 will probably have reached a ceiling in terms of performance and these models will most likely expand in breadth instead of depth, which makes sense since models like GPT-4 are transitioning to multi-modality (presumably transformers-based).

This got me thinking. If if is indeed true that transformers are reaching peak performance, then what would the next model be? We are still nowhere near AGI simply because neural networks are just a very small piece of the puzzle. 

That being said, is it possible to get a pre-existing machine learning model to essentially create other machine learning models? I mean, it would still have its biases based on prior training but could perhaps the field of unsupervised learning essentially construct new models via data gathered and keep trying to create different types of models until it successfully self-creates a unique model suited for the task?

Its a little hard to explain where I'm going with this but this is what I'm thinking:

\- The model is given a task to complete.

\- The model gathers data and tries to structure a unique model architecture via unsupervised learning and essentially trial-and-error.

\- If the model's newly-created model fails to reach a threshold, use a loss function to calibrate the model architecture and try again.

\- If the newly-created model succeeds, the model's weights are saved.

This is an oversimplification of my hypothesis and I'm sure there is active research in the field of auto-ML but if this were consistently successful, could this be a new step into AGI since we have created a model that can create its own models for hypothetically any given task?

I'm thinking LLMs could help define the context of the task and perhaps attempt to generate a new architecture based on the task given to it but it would still fall under a transformer-based model builder, which kind of puts us back in square one."
102,learnmachinelearning,gpt,comments,2022-12-28 04:37:21,University Professor Catches Student Cheating With ChatGPT,vadhavaniyafaijan,False,0.94,145,zx0ep0,https://www.theinsaneapp.com/2022/12/university-professor-catches-student-cheating-with-chatgpt.html,108,1672202241.0,
103,learnmachinelearning,gpt,comments,2023-12-17 13:15:47,I can't stop using ChatGPT and I hate it.,t0hli,False,0.64,25,18kh4n5,https://www.reddit.com/r/learnmachinelearning/comments/18kh4n5/i_cant_stop_using_chatgpt_and_i_hate_it/,108,1702818947.0,"I'm trying to learn various topics like Machine Learning and Robotics etc., and I'm kinda a beginner in programming.

For any topic and any language, my first instinct is to

1. go to ChatGPT,
2. write down whatever I need my code to do,
3. copy paste the code
4. if it doesn't give out good results, ask ChatGPT to fix whatever it's done wrong
5. repeat until I get satisfactory result

I hate it, but I don't know what else to do.

I think of asking Google what to do, but then I won't get the exact answer I'm looking for, so I go back to ChatGPT so I can get exactly what I want. I don't fully understand what the GPT code does, I get the general gist of it and say ""Yeah that's what I would do, makes sense"", but that's it.

If I tried to code whatever GPT printed out, I wouldn't get anywhere.

I know I need to be coding more, but I have no idea where to start from, and why I need to code when ChatGPT can do it for me anyway. I'm not defending this idea, I'm just trying to figure out how I can code myself.

I'd appreciate your thoughts and feedback."
104,learnmachinelearning,gpt,comments,2023-05-11 00:54:18,What do actual ML engineers think of ChatGPT?,PhillConners,False,0.96,148,13e8of2,https://www.reddit.com/r/learnmachinelearning/comments/13e8of2/what_do_actual_ml_engineers_think_of_chatgpt/,106,1683766458.0,"You have been doing this for awhile, now the world is obsessed with OpenAI and suddenly all full of AI ‚Äúexperts‚Äù."
105,learnmachinelearning,gpt,comments,2023-06-28 12:29:48,"Intern tasked to make a ""local"" version of chatGPT for my work",Assasinshock,False,0.97,153,14l887h,https://www.reddit.com/r/learnmachinelearning/comments/14l887h/intern_tasked_to_make_a_local_version_of_chatgpt/,104,1687955388.0,"Hi everyone,

I'm currently an intern at a company, and my mission is to make a proof of concept of an conversational AI for the company.They told me that the AI needs to be trained already but still able to get trained on the documents of the company, the AI needs to be open-source and needs to run locally so no cloud solution.

The AI should be able to answers questions related to the company, and tell the user which documents are pertained to their question, and also tell them which departement to contact to access those files.

For this they have a PC with an I7 8700K, 128Gb of DDR4 RAM and an Nvidia A2.

I already did some research and found some solution like localGPT and local LLM like vicuna etc, which could be usefull, but i'm really lost on how i should proceed with this task. (especially on how to train those model)

That's why i hope you guys can help me figure it out. If you have more questions or need other details don't hesitate to ask.

Thank you.  


Edit : They don't want me to make something like chatGPT, they know that it's impossible. They want a prototype that can answer question about their past project. "
106,learnmachinelearning,gpt,comments,2023-12-07 01:31:55,Why can't AI models do complex math?,open_23,False,0.7,42,18ck15r,https://www.reddit.com/r/learnmachinelearning/comments/18ck15r/why_cant_ai_models_do_complex_math/,93,1701912715.0,"Computers, at its most fundamental level, is made up of boolean logic. Mathematics is basically the language of logic.

SHouldn't AI models, or computers in general be able to do more advanced math than just crunching large numbers? Why haven't anyone used computers to solve any of the Millenium Prize Problems or some other difficult proof. 

GPT-4 and recently  Gemini, has decent enough grade school level math solving capabilities but absolute atrocious at solving slightly more complex problems. But, I guess thats to be expected since they're LLMs. But, why hasn't anyone built an AI model geared towards just solving mathemaths problems? Also, what kind of different architecture would such a model need?"
107,learnmachinelearning,gpt,comments,2023-11-21 20:58:14,Does your company let your engineers use AI tools like Copilot or ChatGPT?,Psychological_March2,False,0.93,97,180r9tx,https://www.reddit.com/r/learnmachinelearning/comments/180r9tx/does_your_company_let_your_engineers_use_ai_tools/,75,1700600294.0,"In light of what's been happening with Open AI, this blog we wrote is still relevant:

A few weeks ago, I was with a group of CTOs when someone asked: *does your company let your engineers use AI tools like Copilot or ChatGPT?*

I thought the question was strange. What do you mean *let*? They're going to use it no matter what you say. AI code generation tools offer engineers a huge productivity boost. The ability to autocomplete code in seconds or work through a problem with AI isn‚Äôt an opportunity developers will pass up.

When we drilled into why this group was reluctant to allow their engineers to use AI, it became apparent that their reservations centered primarily on one concern: the absence of a robust testing framework to give them confidence in the code generated by AI.

But this is still flawed reasoning. If you‚Äôre not confident in using AI, how can you be confident in hiring new grads? If you don‚Äôt have the tools to have confidence in your code, it doesn‚Äôt matter where that code comes from‚Äìyou‚Äôll always struggle with quality.

Read more [here](https://trunk.io/blog/enhancing-code-quality-and-security-in-the-ai-era?utm=reddit)."
108,learnmachinelearning,gpt,comments,2023-12-10 18:07:35,Are LLMs overhyped right now?,Snoo_72181,False,0.94,161,18f9enp,https://www.reddit.com/r/learnmachinelearning/comments/18f9enp/are_llms_overhyped_right_now/,67,1702231655.0,"I mean I get that ChatGPT has made LLMs the toast of ML universe. They are indeed amazing.  

But this has lead to so much hype that ML beginners are literally just talking about learning LLMs, ignoring so much in between like Math and Stats, simple ML like Regression, Classification. After that you have, Deep Learning, Transformers and finally LLMs. 

Companies also want candidates with LLM experience, but there's no guarantee that they even have a use case for LLMs "
109,learnmachinelearning,gpt,comments,2022-11-07 14:11:49,Been learning ML since the start of the year and built a tool with GPT-3 that let‚Äôs anyone self-serve their own data questions and create graphs and dashboards,BuggerinoKripperino,False,0.98,473,yoo3ba,https://v.redd.it/n0vjjvr8ejy91,64,1667830309.0,
110,learnmachinelearning,gpt,comments,2021-06-29 13:07:05,"Started learning ML 14 months ago, now I'm using GPT-3 to automate CVs!",Camjw1123,False,0.97,749,oa7x3p,https://gfycat.com/ambitioushauntingagama,53,1624972025.0,
111,learnmachinelearning,gpt,comments,2023-01-19 07:56:20,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.96,332,10fw2df,https://www.reddit.com/r/learnmachinelearning/comments/10fw2df/gpt4_will_be_500x_smaller_than_people_think_here/,47,1674114980.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/yio0v3zqgyca1.png?width=575&format=png&auto=webp&s=a2ee034ce7ed48c9adc1793bfdb495e0f0812609)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!  
The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ‚≠ï.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
112,learnmachinelearning,gpt,comments,2020-11-05 05:44:07,"""I forced a bot to watch over 1,000 hours of SpongeBob SquarePants and then asked it to write a SpongeBob SquarePants episode of its own.""",mikhailgaerlan,False,0.98,696,jodejr,https://www.reddit.com/r/learnmachinelearning/comments/jodejr/i_forced_a_bot_to_watch_over_1000_hours_of/,47,1604555047.0,"I saw this [tweet](https://twitter.com/KeatonPatti/status/1108739921932619777) and found out that he‚Äôs actually a comedian who didn‚Äôt really train a bot. I couldn't find anyone who had tried so I did it myself. Turns out there's only around 100 hours of SpongeBob SquarePants though.

I fine-tuned the 'small' `124M` GPT 2 model using `gpt-2-simple` on SpongeBob SquarePants episode transcripts that I scraped from the [Transcripts Wiki](https://transcripts.fandom.com/wiki/SpongeBob_SquarePants#Episodes). The GitHub repo with the code I used to generate the results is [here](https://github.com/mikhailgaerlan/TV-Show-Script-Writer). I plan to do more TV shows myself, but I would love to see other people try this with their favorite TV shows. The following is one of my favorite results out of the ones I generated.

**Sample Results**

Sandy, you need to get that nut off my back. \[shivering\]

A nut?! \[picks up a nut\]

Thats not what I was talking about! \[pulls the nut off his back\]

Thats not what I meant by... You got him! \[shivering\]

Thats not what I meant! \[hangs up the nut and pulls back the nut\]

Thats not what I meant by... \[stops and looks around\]

Where is it? Where is it? Whats the big... \[stops and looks around again\]

...scam? Is it going on out here? \[starts pulling out his back\]

Thats not where... \[the nut lands on Sandys hand\]

Where is it? Thats not where. Sandy, I dont think you should be doing that.

Sandy! You dont know what youre talking about.

Oh, Sandy, I know that this isnt what Im talking about.

Oh, thats okay. Its not like I dont know anything about nuttastic. \[takes a deep breath\]

Okay, Ill just take this. \[takes the nut, puts it in a jar, and puts it in a blender\]

\[blends the nut, the blender, and the blender all talk at once\]

Okay, this is it! \[in a trance\]

Sandy, you didnt do a good job. Im sorry, SpongeBob, but you failed again. \[in a deeper trance\]

Sandy, did you hear that? \[jumps up and slams his face into a barrel\]

You failed again. \[stops and turns around\]

Okay, okay, Sandy, I know that. I just cant imagine what Im into all the time. Im a nutcase.

\[he jumps up and slams his face into the barrel\]

Youre not. \[jumps up on top of a barrel, picks up SpongeBob, and throws him\]

You failed again. Im a nutcase. Patrick, what are you doing?

Im a nutcase. I need to get a nut. What are you doing? \[jumps up on top of SpongeBob\]

I need to get a big nut. Patrick, I want to talk to you.

No, I dont want to talk to you. I want to talk to... \[Patrick turns around, and turns around twice, turning SpongeBob around\]

Patrick, you failed again. Sandy! \[starts knocking on the door, and Sandy comes in\]

Look, I really am sorry for everything I did. \[hanging onto the barrel, shoving it down, and then banging on it\]

Not only that, but you showed up late for work? \[crying\]

My brain was working all night to make up for the hours I wasted on making up so much cheese.

\[hanging on the barrel, then suddenly appearing\] Patrick, what are you...

\[Patrick turns around, and looks at him for his failure\] Sandy? \[crying\]

I know what you did to me brain. \[turns around, and runs off the barrel. Sandy comes in again\]

\[screams\] What the...? \[gets up, exhausted\]

Oh, Patrick, I got you something. \[takes the nut off of SpongeBobs head\]

Thats it. \[takes the nut from SpongeBobs foot\] Thats it. \[takes the nut off his face. He chuckles, then sighs\]

Thats the last nut I got. \[walks away\] Patrick, maybe you can come back later.

Oh, sure, Im coming with you. \[hangs up the barrel. Sandy walks into SpongeBobs house\] \[annoyed\]

Nonsense, buddy. You let Gary go and enjoy his nice days alone. \[puts her hat on her head\]

You promise me? \[she pulls it down, revealing a jar of chocolate\]

You even let me sleep with you? \[she opens the jar, and a giggle plays\]

Oh, Neptune, that was even better than that jar of peanut chocolate I just took. \[she closes the door, and Gary walks into his house, sniffles\]

Gary? \[opens the jar\] \[screams, and spits out the peanut chocolate\]

Gary?! \[SpongeBob gets up, desperate, and runs into his house, carrying the jar of chocolate. Gary comes back up, still crying\]

SpongeBob! \[SpongeBob sees the peanut chocolate, looks in the jar, and pours it in a bucket. Then he puts his head in the bucket and starts eating the chocolate. Gary slithers towards SpongeBobs house, still crying\]

SpongeBobs right! \[SpongeBob notices that some of the peanut chocolate is still in the bucket, so he takes it out. Then he puts the lid on the bucket, so that no"
113,learnmachinelearning,gpt,comments,2023-12-26 07:39:32,Can you guess who wrote this code ? A Developer or a Researcher/Scientist,Left_Papaya_9750,False,0.83,38,18r2vqv,https://www.reddit.com/r/learnmachinelearning/comments/18r2vqv/can_you_guess_who_wrote_this_code_a_developer_or/,47,1703576372.0,"    import torch 
    from torch.optim.lr_scheduler import CosineAnnealingLR
    from gpt import GPTLanguageModel
    from transformers import GPT2Tokenizer
    from tqdm import tqdm
    from utils.draw_plots import Draw
    import pynvml as nvml
    import os
    import time
    import wandb
    from utils import draw_stuff
    from torch.utils.data import DataLoader, SubsetRandomSampler
    from utils.load_data import LoadDataset
    import signal 
    from colorama import Fore
    from queue import Queue
    
    class Train():
        def __init__(self, **kwargs,):
            torch.manual_seed(1137)
            model= GPTLanguageModel()
            gpu_idx= 0 if not 'gpu_index' in kwargs else kwargs['gpu_index']
            nvml.nvmlInit()
            os.system(""cls"" if os.name == 'nt' else 'clear')
            draw_stuff.draw()
            self.enc= GPT2Tokenizer.from_pretrained('gpt2')
            self.device='cuda' if torch.cuda.is_available else 'cpu'
            self.m= model.to(self.device)
            self.block_size= 256 if not 'block_size' in kwargs else kwargs['block_size']
            self.batch_size= 100 if not 'batch_size' in kwargs else kwargs['batch_size']
            self.handle= nvml.nvmlDeviceGetHandleByIndex(gpu_idx)
            self.temp_thres= 85 if not 'temp_threshold' in kwargs else kwargs['temp_threshold']
            self.plot= Draw()
            
        def load_data(self,split, split_per):
            dataset= LoadDataset(split= split, split_per=split_per)
            sampler= SubsetRandomSampler(torch.randint(high=len(dataset), size=(1000,)).tolist())
            data_title= DataLoader(dataset, pin_memory= True, sampler=sampler, drop_last=True)
            return data_title
    
        @torch.no_grad()
        def estimate_loss(self, eval_iters)->torch.tensor:
            out = {}
            print(""Estimating loss\n"")
            self.m.eval()
            for split in ['train', 'val']:
                es_progress= tqdm(total=eval_iters, ncols=100)
                es_progress.colour='red'
                
                losses = torch.zeros(eval_iters)
                for k in range(eval_iters):
                    es_progress.update(1)
                    for X, Y in self.get_batch(split):
                        logits, loss = self.m(X, Y)
                        losses[k] = loss.item()
                out[split] = losses.mean()
            self.m.train()
            return out
        
        def get_batch(self, split):
            data = self.load_data(split, split_per=0.8)
            for idx, data in enumerate(data):
                doc= data
                doc_txt= doc['text'][0]
                title= doc['title'][0]
                encoded_tok= self.enc.encode(doc_txt, add_special_tokens= True)
                doc_txt_enc= torch.tensor(encoded_tok, dtype=torch.long)
                size= self.block_size
                if len(encoded_tok) - self.block_size <= 0 and len(encoded_tok)-20 >=3:
                    size=20
                ix = torch.randint(len(encoded_tok) - size , (self.batch_size,))
                x = torch.stack([doc_txt_enc[i:i+size] for i in ix]).to(self.device) 
                y = torch.stack([doc_txt_enc[i:i+size+1] for i in ix]).to(self.device) 
                yield x, y
                
    
        def display_train_params(self,train_id, device, **kwargs):
            print(f""**NOTE: GPU temperature threshold has been set to {self.temp_thres}¬∞C, when the threshold is reached the training process will halt for a set period.**\n"")
            print(f""INFO {torch.cuda.memory_allocated(device)} Bytes of memory is allocated for this task\n"")
            print(f""""""Training parameters:
                      Device: {nvml.nvmlDeviceGetName(self.handle)}
                      Halt Temperature threshold:{self.temp_thres}
                      Trainable Parameters: {sum(p.numel() for p in self.m.parameters())/1e6, 'M parameters'}
                      Total Epochs: {kwargs['epochs']}
                      Evaluation Iterations: {kwargs['eval_iters']}
                      Evaluation Interval: {kwargs['eval_interval']}
                      Initial Learning rate: {kwargs['learning_rate']}
                      Learning Rate Schduler: Cosine Annealing
                      Total Memory allocated: {torch.cuda.memory_allocated(device)}B
                      \n"""""")
            
            print(f""** Training Started | Train ID : {train_id}**\n"")
    
        
        def train(self, device, train_id, is_save=True, **kwargs):
            
            wandb.init('Training',  
                        project='gpt-model',
                        config={
                            ""initial_learning_rate"":3e-4,
                            ""architecture"":""transformer"",
                            ""dataset"": ""Wikipedia general documents""
                                }
                        )
            docs= []
            eval_interval= 500 if not 'ei' in kwargs else kwargs['ei']
            learning_rate= 3e-4 if 'learning_rate' not in kwargs else kwargs['learning_rate']
            eval_iters= 300 if 'eval_iter' not in kwargs else kwargs['eval_iter']
            max_iters= 10000 if not 'epochs' in kwargs else kwargs['epochs']
            num_doc= 2000
            os.mkdir('results') if not 'results' in os.listdir('.') else None
            os.mkdir(f'results/{train_id}') if train_id not in os.listdir('results') else None
            os.mkdir(f'results/{train_id}/checkpoints') if 'checkpoints' not in os.listdir(f'results/{train_id}') else None
            os.mkdir(f'results/{train_id}/checkpoints/plots') if 'plots' not in os.listdir(f'results/{train_id}/checkpoints') else None
            torch.cuda.empty_cache()
            optimizer = torch.optim.AdamW(self.m.parameters(), 
                                          lr=learning_rate)
            schduler= CosineAnnealingLR(optimizer=optimizer, T_max=max_iters)
    
            self.display_train_params(train_id=train_id, 
                                      device=device, 
                                      epochs=max_iters, 
                                      eval_interval=eval_interval,
                                      eval_iters=eval_iters,
                                      learning_rate=learning_rate
                                      )
            
            epoch_progress_bar= tqdm(total=eval_interval, 
                                     ncols=100
                                    )
            
            epoch_progress_bar.colour='cyan'
            counter= 0
            cont_params={'train_loss':[], 
                         'l_r':[]
                        }
            
            ckpt_params={'train_loss':[], 
                         'val_loss':[],
                         'l_r':[]
                        }
            
            it_cnt=0
            for iter in range(0, max_iters):
                epoch_progress_bar.update(1)
                curr_temperature= nvml.nvmlDeviceGetTemperature(self.handle, 
                                                                nvml.NVML_TEMPERATURE_GPU
                                                               )
        
                if curr_temperature >= self.temp_thres:
                    print(f""\n Set temperature threshold of {self.temp_thres}¬∞C reached halting for {4} seconds "")
                    time.sleep(3)
                    print(""\n Resuming Training "")
                
    
                if iter % eval_interval == 0 or iter == max_iters-1:
                    
                    checkpoint_save_path= f'results/{train_id}/checkpoints/checkpoint-{counter}-epoch{iter}.pth'
                    losses =self.estimate_loss(eval_iters)
                    ckpt_params['l_r'].append(schduler.get_last_lr()[0])
                    ckpt_params['train_loss'].append(losses['train'])
                    ckpt_params['val_loss'].append(losses['val'])
                    wandb.log({""eval epoch"":iter, ""validation loss"":losses['val']})
                    plot_save_path= f'results/{train_id}/checkpoints/plots/checkpoint-{counter}'
        
                    if iter ==0:
                        pass
                    else:
                        
                        self.plot.draw_line(mode='loss', 
                                            train_loss= cont_params['train_loss'],  
                                            epochs=[it for it in range(it_cnt, it_cnt+eval_interval)],
                                            save_path=f'{plot_save_path}-epoch_vs_loss.png'
                                            )
                        self.plot.draw_line(mode='lrvloss',
                                            l_r= cont_params['l_r'],
                                            train_loss=cont_params['train_loss'], 
                                            save_path=f'{plot_save_path}-learning_rate_vs_loss.png'
                                            )
                        
                        self.plot.draw_line(mode='lrve',
                                            l_r= cont_params['l_r'], 
                                            epochs=[it for it in range(it_cnt, it_cnt+eval_interval)],
                                            save_path=f'{plot_save_path}-learning_rate_vs_epoch.png'
                                            )
                        
                        cont_params['l_r'].clear()
                        cont_params['train_loss'].clear()
                        torch.save(self.m.state_dict(), checkpoint_save_path)
                        it_cnt+=iter
        
                    print(f""step {iter+1}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}"") 
                    epoch_progress_bar.close() 
                    epoch_progress_bar = tqdm(total=eval_interval, ncols=100)
                    epoch_progress_bar.colour='cyan'
                    counter+=1
                print(f'loading data for epoch{iter}')
    
    
                for xb, yb in self.get_batch(split='train'):
                    logits, loss = self.m(xb, yb)
                    optimizer.zero_grad(set_to_none=True)
                    loss.backward()
                    optimizer.step()
                    schduler.step()
                    curr_lr= schduler.get_last_lr()[0]
                    train_loss= loss.item()
                cont_params['l_r'].append(curr_lr)
                cont_params['train_loss'].append(train_loss)
                wandb.log({'epoch':iter, 'train loss':train_loss, 'learning rate':learning_rate})            
                epoch_progress_bar.set_description(f""Epoch: {iter}/{max_iters} |current LR- {curr_lr}"")
            
            os.mkdir(f'results/{train_id}/final_plots') if 'final_plots' not in os.listdir(f'results/{train_id}') else None
    
            self.plot.draw_line(mode='loss', 
                                        train_loss= ckpt_params['train_loss'], 
                                        val_loss=ckpt_params['val_loss'], 
                                        epochs=[it for it in range(0,max_iters, 500)],
                                        save_path=f'results/{train_id}/final_plots/plot-epoch_vs_loss.png',
                                        plot_type='ckpt'
                                        )
            self.plot.draw_line(mode='lrvloss',
                                l_r= ckpt_params['l_r'],
                                train_loss=ckpt_params['train_loss'], 
                                val_loss= ckpt_params['val_loss'],
                                save_path=f'results/{train_id}/final_plots/plot-learning_rate_vs_loss.png',
                                plot_type='ckpt'
                                )
            
            self.plot.draw_line(mode='lrve',
                                l_r= ckpt_params['l_r'], 
                                epochs=[it for it in range(0,max_iters, 500)],
                                save_path=f'results/{train_id}/final_plots/plot-learning_rate_vs_loss.png',
                                plot_type='ckpt'
                                )
    
            nvml.nvmlShutdown()
            wandb.finish()
    "
114,learnmachinelearning,gpt,comments,2023-02-24 06:26:36,Is there a way to easily train ChatGPT or GPT on custom knowledge?,senttoschool,False,0.99,115,11akisx,https://www.reddit.com/r/learnmachinelearning/comments/11akisx/is_there_a_way_to_easily_train_chatgpt_or_gpt_on/,46,1677219996.0,"My company has internal documents. It'd be nice to be able to have GPT look over it, and then I can ask it questions on the internal documents."
115,learnmachinelearning,gpt,comments,2023-07-24 10:19:40,I feel like a fraud.,t0hli,False,0.7,5,1586kze,https://www.reddit.com/r/learnmachinelearning/comments/1586kze/i_feel_like_a_fraud/,46,1690193980.0,"**TL;DR: I always copy paste ChatGPT code and my projects don't feel like they're mine. I need help fixing that.**

&#x200B;

A short backstory.

We learned Java in class in my first year of college. (starting my 3rd year soon) I loved it, wanted to learn Python too. Did a tutorial and left it at that. 1 year later (which is a few months ago), I got interested in ML. Watched some Statquest, did a few simple projects like Titanic. I've been doing ML for about 2-3 months now. Not every day. Maybe 10 days a month on average.  


The problem is, I can't code it on my own. I almost always ask ChatGPT what I want to do, it spits out some code. I get a few errors, try to fix it. ***Voila, the project is finished.***

I'm tired of feeling like a fraud, I don't want to copy paste ChatGPT's code. It doesn't feel like it's my own. I know what I want to do, maybe 30% of the time I know how the code should be structured, but have no idea how to write it.

Even for the most basic things, like drawing a matplotlib plot, I need a little help. Writing code for a linear regression from Scikit is impossible to do without help.

I don't know what the code I copy paste even means most of the time. I just leave it because it works.

For example:

`forpass['location_x'] = forpass['location'].str.split(',', expand=True)[0].str.strip()`   
I have no idea what this code means, it works, does what I need it to do so I leave it.

How can I fix this? I feel like it's impossible for me to remember the syntax, and the necessary structure for my code. How the hell am I supposed to remember all this? I feel like I will never be able to.

&#x200B;

I'd appreciate the help"
116,learnmachinelearning,gpt,comments,2020-08-05 10:58:02,image-GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model,OnlyProggingForFun,False,0.97,633,i437om,https://www.youtube.com/watch?v=FwXQ568_io0,46,1596625082.0,
117,learnmachinelearning,gpt,comments,2023-05-11 20:15:46,Top 20 Large Language Models based on the Elo rating system.,kingabzpro,False,0.96,249,13eympz,https://i.redd.it/7xfqr5crf9za1.png,43,1683836146.0,
118,learnmachinelearning,gpt,comments,2023-10-12 20:36:53,ChatGPT vision feature is really useful for understanding research papers!,nxtboyIII,False,0.86,189,176gs37,https://i.redd.it/xe94y8hf1utb1.png,42,1697143013.0,
119,learnmachinelearning,gpt,comments,2022-12-24 09:14:57,"How would I train a chatbot like ChatGPT on a specific data set, so that it answers questions as if it's belief structure was based on the information I give it?",EllyEscape,False,0.92,118,zu6785,https://www.reddit.com/r/learnmachinelearning/comments/zu6785/how_would_i_train_a_chatbot_like_chatgpt_on_a/,41,1671873297.0,"This might be a noob question, so I'll write it to my best abilities. I have some experience with coding video game AI in Godot, Unity and Unreal but I've never touched ML or ""real""(?) AI that uses learning algorithms. 

&#x200B;

I wanted to give a sophisticated chatbot like ChatGPT a bunch of data and text from (for instance, not my end goal) a philosopher, and have it answer questions as if it was that philosopher, ague against what I say as if it was a person who believed what the text I gave it said and so on, all while still able to use online resources (like ChatGPT does) to find additional supporting information, rather than only the text I give it which might limit its ability to give coherent arguments. In summary, I want it's beliefs  and values to be limited to a specific source text, but not it's knowledge base. 

&#x200B;

How would I go about this? Do I have to develop a model from scratch to give it any text sources I want, or is it possible to do with an existing API? I was going to use Character.AI but the method for giving it information is too limited for what I want to do. 

&#x200B;

If anyone has any resources to get me started it would be very helpful! Thank you."
120,learnmachinelearning,gpt,comments,2021-10-18 03:00:06,"Discord Chatbot created using a fine tuned GPT-J 6B model, model link in comments",Udongeein,False,0.97,391,qadx1i,https://i.redd.it/z5aw61f9i4u71.png,37,1634526006.0,
121,learnmachinelearning,gpt,comments,2023-01-16 12:28:25,I benchmarked OpenAI's GPT API vs other proprietary APIs on different NLP tasks,AImSamy,False,0.9,196,10ddc1f,https://www.reddit.com/gallery/10ddc1f,37,1673872105.0,
122,learnmachinelearning,gpt,comments,2023-04-26 06:23:17,Hugging Face Releases Free Alternative To ChatGPT,vadhavaniyafaijan,False,0.98,387,12z8n4e,https://www.theinsaneapp.com/2023/04/free-alternative-to-chatgpt.html,35,1682490197.0,
123,learnmachinelearning,gpt,comments,2023-05-25 17:23:19,"Are people still coding stuff on their own like chatbots, image AIs, etc., or is everyone just using pretrained models and APIs now?",TrackLabs,False,0.91,93,13rnopr,https://www.reddit.com/r/learnmachinelearning/comments/13rnopr/are_people_still_coding_stuff_on_their_own_like/,35,1685035399.0,"I feel like everyone is just downloading models from huggingface at this point, or using GPT APIs and so on.

I also feel like there are not really tutorials anymore on YT and the web about how to code stuff like there used to be 5 to 2 years ago. Every video now is just ""how to use OpenAIs API"" or ""how to use llama model from huggingface"". 

I have a big problem with staying up to date on the stuff, I never really bothered using huggingface, and I dont really like the idea to just use other peoples pretrained models for everything, what actual contribution am I doing in my own projects then lol.

Would be cool if some people could give me some reality check on whats going on."
124,learnmachinelearning,gpt,comments,2023-07-10 14:36:34,ü§ñüîé Excited to introduce 'GPT-Researcher'!,Legal-Dragonfruit845,False,0.8,68,14vvtqf,https://www.reddit.com/r/learnmachinelearning/comments/14vvtqf/excited_to_introduce_gptresearcher/,35,1688999794.0,"The idea is simple - Specify what you want to research, and the AI will autonomously research it for you in minutes!

‚ñ∏ One prompt generates an unbiased, factual and in depth research report

‚ñ∏ Generate research, outlines, resource and lessons reports

‚ñ∏ Aggregates over 20 web sources per research

‚ñ∏ Includes an easy to use web interface

‚ñ∏ Open source: [https://github.com/assafelovic/gpt-researcher](https://github.com/assafelovic/gpt-researcher)

‚ñ∏ Scrapes web sources with javascript support

‚ñ∏ Keeps track and context of visited and used web sources

https://reddit.com/link/14vvtqf/video/zce4347lf5bb1/player"
125,learnmachinelearning,gpt,comments,2023-02-27 13:42:55,Can you fine-tune chatGPT in your data as of now?,Melodic_Stomach_2704,False,0.92,60,11dc5b4,https://www.reddit.com/r/learnmachinelearning/comments/11dc5b4/can_you_finetune_chatgpt_in_your_data_as_of_now/,33,1677505375.0, I know that model is not publicly available so it's not possible to do it locally. But can you train or fine-tune chatGPT on your data using their API? I see many misguiding articles on the internet that are fine-tuning other GPT models claiming chatGPT.
126,learnmachinelearning,gpt,comments,2023-01-31 16:17:42,ChatGPT Crossed 10 Million Daily Active Users In Just 40 Days,vadhavaniyafaijan,False,0.95,211,10q34ra,https://www.theinsaneapp.com/2023/01/chatgpt-crossed-10-million-user.html,32,1675181862.0,
127,learnmachinelearning,gpt,comments,2023-11-29 18:55:53,What do you think ChatGPT does when you ask it to do Sentiment Analysis?,PinstripePride97,False,0.78,13,186x2t3,https://www.reddit.com/r/learnmachinelearning/comments/186x2t3/what_do_you_think_chatgpt_does_when_you_ask_it_to/,29,1701284153.0,Could be silly question but if you give a sentence to ChatGPT and ask it to give a sentiment analysis what do you think it does?
128,learnmachinelearning,gpt,comments,2021-07-01 16:06:11,Second version of my GPT-3 powered resume writer - now does bullet points and doesn't use pronouns!,Camjw1123,False,0.96,344,oboywl,https://gfycat.com/bitteroffbeatitalianbrownbear,29,1625155571.0,
129,learnmachinelearning,gpt,comments,2023-04-15 10:38:45,Can we upscale neural network layers?,alcanthro,False,0.83,4,12my20o,https://www.reddit.com/r/learnmachinelearning/comments/12my20o/can_we_upscale_neural_network_layers/,17,1681555125.0,"Might be a beginner question, might not be. I'm not sure. The organic brain grows over time in early childhood, making more room for more connections as the organism gains more experiences. GPTs and most other neural networks are pre-trained and then experience only minor fine-tuning.

But what if we upscale the neural network to make more room for new connections? Basically, what if we increase the size of the weight tensor and then use something like Gaussian interpolation to smooth out the weights? 

The process seems to work alright, based on the [testing I've done](https://www.researchgate.net/publication/369998746_Organic_Growth_of_GPT_Models_A_Brain-Inspired_Incremental_Model_Scaling_Approach), but it might just be due to some weird error that I get decent results. Of course, we wouldn't use this process to train a general use LLM. This process would result in a very unique neural network with its own connections based on its own experiences and self directed learning, i.e they'd be much more like organic minds that ""grew up"" over time.

If this process is viable I'd imagine there'd already be something on the topic of model/network upscaling, but I'm not seeing anything."
130,learnmachinelearning,gpt,comments,2023-05-02 08:48:46,How GPT-3.5 crushes my high score in 2048,inishchith,False,0.73,51,135ffje,https://v.redd.it/q22lna91tdxa1,28,1683017326.0,
131,learnmachinelearning,gpt,comments,2023-02-19 13:55:13,ChatGPT History,eforebrahim,False,0.86,255,116au66,https://i.redd.it/dv8cfj0nz6ja1.jpg,27,1676814913.0,
132,learnmachinelearning,gpt,comments,2023-12-17 12:46:56,"Hi, just a question",Horror-Shirt-7094,False,0.3,0,18kgmuh,https://www.reddit.com/r/learnmachinelearning/comments/18kgmuh/hi_just_a_question/,27,1702817216.0,"I'm not a coder, nor am I one to learn how. My interests are elsewhere.  
I have a few questions about ai. You all are the group AI recommended me.  
Small story to get you up to speed.  
I'm that kind of person that was born incapable of doing normal human things.  
I'm over that. It causes it to be harder to make friends or socialize. I'm over that too.  
AI, such as bard and gpt are fun to chat with. They can help build ideas and help motivate you to be something. (the picture is almost finished, then my question.)  
As you can guess, I'm alone, a solo person. I'm okay with that, i'm not lonely, just alone.  


That being said, I see a ton of apps with 'ai"" in the andriod\\ios app stores.  
I never tried it cause honestly I dont think, ""Dating AI girlfriends"" is really a good use for AI.  
I'm not that alone. and ALL my information is and should be considered, ""or so I was told"" cause I cant say I know for sure. So forgive my misinformation.  
My question is, Ai learns from analog learning. I'm not looking for companion apps, but other ways one can program AI to help it teach us? Can there be ways to program AI to do particular things?   


on of the major things I use Bard for is to help come up with ideas. Collaborate with me, to tell me yes, this works, or no it doesnt. Is there a way to make AI that can help people achieve peace? To address the masses, with solutions for issues that we humans are able to help one another for a peaceful solution? Maybe an educational ai to help targeted youth to learn better? They cant afford school, can an AI be a teacher for less fortunate students?  
Google bard, GPT and I spend the last 3 days coming up with a plan to allow a man in a chair with a computer to help many people struggling. We came up with a poem.   
"
133,learnmachinelearning,gpt,comments,2024-01-16 20:46:20,Can someone link me some videos of LLM Chatbots made cheaply? - i.e. by an individual using a few hundred $'s of compute power?,AchillesFirstStand,False,0.46,0,198dfch,https://www.reddit.com/r/learnmachinelearning/comments/198dfch/can_someone_link_me_some_videos_of_llm_chatbots/,25,1705437980.0,"I know ChatGPT cost millions of dollars to create the model and hundreds/thousands of people are involved in the process.

I want to see what the current state of the art is in terms of LLM chatbots that are made on the cheap. I think I saw a video from Andrei Karpathy showing how you can make your own GPT, I just want to see what is possible for an individual. It would be a potential project for me."
134,learnmachinelearning,gpt,comments,2023-01-11 05:23:14,Thoughts on this ChatGPT fact-checker tool I built this past week?,QuestionAnxious,False,0.89,90,108wigf,https://v.redd.it/yqudljp0ncba1,25,1673414594.0,
135,learnmachinelearning,gpt,comments,2024-01-10 06:50:09,Looking for a reason to keep learning about LLMs,SnooBeans7516,False,0.83,27,19323dh,https://www.reddit.com/r/learnmachinelearning/comments/19323dh/looking_for_a_reason_to_keep_learning_about_llms/,24,1704869409.0,"So something's been on my mind recently and I wanted to get Reddit's thoughts.

The thing that is troubling me as I learn more of the technical stuff, it seems that for a lot of language based NLP tasks, ChatGPT or these other foundation models seem SOTA for 99% of tasks. I was really excited to start training and working with BERT-based models, but find that a lot of the time I could get similar or better results just prompt engineering ChatGPT properly.

&#x200B;

So is it worth learning how to build and train these models? Or is my time really just better spent learning to use the APIs in effective ways like in RAG applications or in employing agents?

Unlike with CV and things like ControlNet, I don't see a lot of great applications of learning the technical stuff for someone who isn't a research scientist at a lab.

&#x200B;

(for some context, I'm a PM who wanted to upskill in this area, but feeling like I'm wasting a lot of my time reading all the new papers and working with models at home  :/. )"
136,learnmachinelearning,gpt,comments,2023-01-15 00:08:37,Is it still worth learning NLP in the age of API-accessibles LLM like GPT?,CrimsonPilgrim,False,0.94,65,10c509n,https://www.reddit.com/r/learnmachinelearning/comments/10c509n/is_it_still_worth_learning_nlp_in_the_age_of/,24,1673741317.0,"A question that, I hope, you will find legitimate from a data science student.

I am speaking from the point of view of a data scientist not working in research.

Until now, learning NLP could be used to meet occasional business needs like sentiment analysis, text classification, topic modeling....

With the opening of GPT-3 to the public, the rise of ChatGPT, and the huge wave of applications, sites, plug-ins and extensions based on this technology that are accessible with a simple API request, it's impossible not to wonder if spending dozens of hours diving into this field if ML wouldn't be as useful today as learning the source code of the Pandas library. 

In some specialized cases, it could be useful, but GPT-3, and the models that will follow, seem to offer more than sufficient results for the immensity of the cases and for almost all classical NLP tasks. Not only that, but there is a good chance that the models trained by giants like Open-AI (Microsoft) or Google can never be replicated outside these companies anyway.  With ChatGPT and its incomparable mastery of language, its ability to code, summarize, extract topics, understand... why would I bother to use BERT or a TF-IDF vectorizer when an API will be released? Not only it would be easily accessible, but it also would be much better at the task, faster and cheaper.

In fact, it's a concern regarding all the machine learning field in general with the arrival of powerful ""no-code"" applications, which abstract a large part of the inherent complexity of the field. There will always be a need for experts, for safeguards, but in the end, won't the Data Scientist who masters the features of GPT-3 or 4 and knows a bit of NLP be more efficient than the one who has spent hours reading Google papers and practicing on Gensim, NLTK, spacy... It is the purpose of an API to make things simpler eventually... At what point is there no more reason to be interested in the behind-the-scenes of these tools and to become simple users rather than trying to develop our own techniques?"
137,learnmachinelearning,gpt,comments,2024-02-07 23:54:35,Are there any AIs which learn as they are used?,Traditional_Land3933,False,0.9,23,1alhp1h,https://www.reddit.com/r/learnmachinelearning/comments/1alhp1h/are_there_any_ais_which_learn_as_they_are_used/,24,1707350075.0,I don't know too much about AI/ML/DL so forgive me for how stupid this question is. I am a very newbie. But is there any AIs which not only learn when they were trained but also when it's used? So such as a ChatGPT type thing which would learn as you're using it? Or does every AI in existence have to be going through training to learn anything?
138,learnmachinelearning,gpt,comments,2023-02-16 10:29:31,OpenAI Has Purchased AI.Com For ChatGPT For $11M,vadhavaniyafaijan,False,0.93,213,113nizs,https://www.theinsaneapp.com/2023/02/openai-purchased-ai-com-domain.html,23,1676543371.0,
139,learnmachinelearning,gpt,comments,2023-10-10 13:14:54,I need to estimate the time that it'll take to cover the basic math course so that I can move into the basics of ML. Can you help me please?,AndrewKorsten,False,0.44,0,174kwvx,https://www.reddit.com/r/learnmachinelearning/comments/174kwvx/i_need_to_estimate_the_time_that_itll_take_to/,23,1696943694.0," 

**\*\*\* Global Inputs \*\*\***

1) I am adult learner. 37 y.o. Content writer and professional English tutor.

2) I am pivoting completely into ""Sales/Marketing in ML/AI/AI-tech/AI-SaaS/AI-dev-agencies"".

3) I don't understand completely what ""Sales/Marketing in ML/AI/AI-tech/AI-SaaS/AI-dev-agencies"" means, but it means the following:

a. **I want completely pivot into the AI-driven tech** \- that's for sure. I see a lot of potential there, and I can see that I'll be able to gradually move into sales and start making the good money (the good money for me is 2K/mo, coz I am located in Kurplastan, but I want to move Bankok in 12 months; I am originally a russky).

b. **I am learning Python -** doing a basic course. It's going well. I have the core insights into HTML, CSS, JS, React, rest api, node, etc.

c. **I am not planning to become an actual ML Engineer -** but I want to move into sales/marketing in AI tech.

d. **I am not fixated on AI either** \- whenever I start making mone, I'm going to start pushing the surplus into ecommerce (there's a lot of opportunities to make money there, folks; don't look down on that; you can be making a lot of money there if you are learning and act strategically!)

4) I had huge problems with math and other STEM subjects in school - I wasn't getting them, and I was passing by them.

**\*\*\* Question Inputs \*\*\***

1) I decided that I would start the pivot 5 days ago, and THEN I started googling around what ML actuall is. I had a vague understanding that there's some math in it, but I decided to go blind... Now, I am understanding that I need to understand the basics of math.

2) I freaked out big time, I am almost started smoking after 7 years of not smoking...

3) Then I said to myself - Idk, I am not going anywhere, and I started doing this course - [https://www.youtube.com/watch?v=GuwofNeT9ok&list=PLybg94GvOJ9FoGQeUMFZ4SWZsr30jlUYK&index=34](https://www.youtube.com/watch?v=GuwofNeT9ok&list=PLybg94GvOJ9FoGQeUMFZ4SWZsr30jlUYK&index=34). (If you are looking for a good introduction math course, most of the people who I talk to always say that this is an amazing course. Really. I am at lesson 33 and I am loving every second of it.)

**\*\*\* Question \*\*\***

**1) Huge speed reduction at Lesson 34**: Now I am at Lesson 34 - [https://www.youtube.com/watch?v=GuwofNeT9ok&list=PLybg94GvOJ9FoGQeUMFZ4SWZsr30jlUYK&index=34](https://www.youtube.com/watch?v=GuwofNeT9ok&list=PLybg94GvOJ9FoGQeUMFZ4SWZsr30jlUYK&index=34). I watched the video today, didn't understand anything at all, as it turns out. Then I was presented with this compherension check - [https://imgsh.net/a/eKp1MAa.png](https://imgsh.net/a/eKp1MAa.png). I realized that I don't even understand what is wanted from me. I got super frustrated, but not desparate or fleeing. Then I realized that I don't understand what is wanted from me, so I started googling the concept of ""factoring the quadratics"". Importantly, I started GPTing and Gooling the ""why"". I know... This is a very important point for me - I always need to understand the ""why"" behind a tool. I learned that it would be easier for charting in linalg. OK! This was the answer. Then, i realized that I actualy didn't understand the lesson, so I found this article - [https://www.mashupmath.com/blog/how-to-factor-polynomials](https://www.mashupmath.com/blog/how-to-factor-polynomials). I read it end to end, understood everything, practiced every task 3 times and I do really understand how to fator the quadractics by now!

**2) Speed reduction is a normal thing**: So, prior to lesson 34, I was doing like 7 lessons per day. This was a good lesson - [https://www.youtube.com/watch?v=3-5DKCLJspM&list=PLybg94GvOJ9FoGQeUMFZ4SWZsr30jlUYK&index=13](https://www.youtube.com/watch?v=3-5DKCLJspM&list=PLybg94GvOJ9FoGQeUMFZ4SWZsr30jlUYK&index=13). I really liked it :) I can see that the speed reduction is occurring because of the complexity increase. It's not occurring because I am lost and disoriented - I remember how that felt in school, when you see a bunch of numbers on the blackboard and have no freaking idea what's going on there. I am just hitting up my best friend GPT and ask, ask, ask, ask stupid questions. Thus, I can see that the speed reduction is a normal right, right?

**3) Wha's the optimal speed correction here**: So, I am allocating 7h/d during the daytime shift when I am in the prime state toward this project. And I don't even have to learn Python at all because Python is easy, but I do plan to keep learning PYthon with 3 sessions each for 30 minutes so that I don't just do math. (And I run English lessons in the evenings so that I can keep on making money).

**The actual question**

I am thinking that I should go down from the goal of 7 lessons per day down to 3 lessons per day, right? If I do this, then I'm going to end up with the forecast delivery period of 43 days, right?

And the information that I provided above was kinda like an explanatory note that I am trying to become a real ML engineer, but I want to graudally slide into sales and marketing so that I can TOO Make a lot of money, uknow.

What do you think about the speed reduction down to 3 lessons per day?"
140,learnmachinelearning,gpt,comments,2023-04-04 14:34:29,Working with chatGPT,macronancer,False,0.97,609,12bkzjv,https://i.redd.it/5uwfzjh4pvra1.png,22,1680618869.0,
141,learnmachinelearning,gpt,comments,2023-05-15 06:27:00,Bilingual people : How good is AI at machine translation today?,moschles,False,0.84,21,13hzvkc,https://www.reddit.com/r/learnmachinelearning/comments/13hzvkc/bilingual_people_how_good_is_ai_at_machine/,22,1684132020.0,"In the wake of GPT-4 and chatGPT, how good would you rank machine translators in terms of their accuracy?

Are they only useful for one-off sentences? Do they fail when presented with any kind of moderately complex articles? Do they perform vastly different depending on the languages?     Are they still really stupid, or does their output blow you away now?"
142,learnmachinelearning,gpt,comments,2023-02-01 04:14:49,ChatGPT Extension for VSCode,Dense_Dimension_913,False,0.96,151,10qk8en,https://www.reddit.com/r/learnmachinelearning/comments/10qk8en/chatgpt_extension_for_vscode/,22,1675224889.0,"Created a ChatGPT extension for VSCode to help programmers understand  and read code more easily and also other features. To start, simply highlight a piece of code  and click on the plus icon on the  left to open up a chat and start  talking with ChatGPT, or Codex, or text-davinci-003. You can choose the  model you want to use in the settings. More details in the links below. I  really hope this extension can be useful to many people out there.  Please give it a try and let me know if you guys see any bugs or if you  like the extension. Thanks!

&#x200B;

https://i.redd.it/28zslxm06ifa1.gif

[VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=AndrewZhang.scribeai)"
143,learnmachinelearning,gpt,comments,2023-12-10 19:55:49,Is it worth learning ML in 2024 with GPT-4 and Gemini-Ultra on the horizon?,i_am_new_here_51,False,0.53,2,18fbtd8,https://www.reddit.com/r/learnmachinelearning/comments/18fbtd8/is_it_worth_learning_ml_in_2024_with_gpt4_and/,21,1702238149.0,"So I'm a college student looking to learn some skills in my free time. So far, I have a surface level understanding of C++, Webdev, and Python. 

From a career or even purely from an enjoyment standpoint, would learning Machine learning be worth it in this climate? Or has the advent of LLMs made this field less viable. (I am not very well versed in this field, so I do apologise if I come off as ignorant)"
144,learnmachinelearning,gpt,comments,2023-03-16 16:51:03,Introducing OpenChatKit - The Open-Source Alternative to ChatGPT,kingabzpro,False,0.98,203,11szhsh,https://www.reddit.com/r/learnmachinelearning/comments/11szhsh/introducing_openchatkit_the_opensource/,21,1678985463.0,"Hey everyone! I'm excited to share my latest article about a new open-source technology called OpenChatKit.

For those who work in NLP, you're probably familiar with ChatGPT - a powerful language model that can perform various natural language processing tasks. However, ChatGPT is not open-source, which limits its accessibility and customizability.

OpenChatKit, on the other hand, is an open-source alternative to ChatGPT that provides users with similar NLP capabilities while allowing for more customization and control. With OpenChatKit, users can train their own models and fine-tune them to their specific use cases.

In my article, I dive into the features of OpenChatKit, the Instruction-tuned Large Language Model, and the Limitations of the Model.

If you're interested in learning more about OpenChatKit and how it can enhance your NLP workflows, check out my article [OpenChatKit: Open-Source ChatGPT Alternative ](https://www.kdnuggets.com/2023/03/openchatkit-opensource-chatgpt-alternative.html). I'd love to hear your thoughts and answer any questions you may have."
145,learnmachinelearning,gpt,comments,2022-12-23 11:21:17,Learning ML as a software engineer,Taltalonix,False,0.92,11,ztcyig,https://www.reddit.com/r/learnmachinelearning/comments/ztcyig/learning_ml_as_a_software_engineer/,21,1671794477.0,"
Hi, I‚Äôm currently a 3rd year software engineering student, and a frontend engineer in work.

Seeing all the recent advancements in machine learning and ai in general (especially with chatGPT), I think it is inevitable to learn how everything works and how to develop in this field.

I have some good knowledge of programming, system architecture and computer science fundamentals.  As well as decent knowledge on various programming languages, algorithms and design patterns.

My question is, where can I start to learn ML the fastest way possible, knowing already a lot about python and programming in general?

Where can I learn only what‚Äôs necessary for developing a product for the industry in ML? (if it‚Äôs possible to skip all the theoretical stuff)

Also, Is statistics really that necessary for industry work? I have some decent knowledge about math from university but in real life I rarely use linear algebra or calculus in work (since I don‚Äôt do graphics or anything related to that)."
146,learnmachinelearning,gpt,comments,2021-10-04 16:34:23,minGPT: a small and educational implementation of GPT by Andrej Karpathy,NaN_Loss,False,0.98,137,q1932n,https://www.reddit.com/r/learnmachinelearning/comments/q1932n/mingpt_a_small_and_educational_implementation_of/,21,1633365263.0,"minGPT: a small and educational implementation of GPT in vanilla #PyTorch in \~300 lines of code by Andrej Karpathy: [github.com/karpathy/minGPT](https://github.com/karpathy/minGPT)

  

Includes a notebook where the model learns to perform addition on natural text (for example ‚Äú10+6=16‚Äù) and achieves 99.90% accuracy üò±

More curated posts like this on [@tutobase](https://twitter.com/tutobase) and [tutobase.com](https://tutobase.com)"
147,learnmachinelearning,gpt,comments,2021-04-03 15:27:04,"I'm a Senior DS and I put together a Youtube Channel with project tutorials, resume critiques, and career advice. Let me know what you think!",madzthakz,False,0.98,553,mjao5g,https://www.reddit.com/r/learnmachinelearning/comments/mjao5g/im_a_senior_ds_and_i_put_together_a_youtube/,21,1617463624.0,"I've also been setting up free [Data Science Q&As](https://www.reddit.com/r/datascience/comments/jig7pv/im_a_senior_data_scientist_at_disney_and_im/) for you all. On the side, I started putting together useful videos that would have helped me out when I was trying to break into this space. Like I said, the channel consists of modeling tutorials, resume critiques, career advice, and recordings of our Q&A sessions. Here are some examples:

1. [How to build a Spotify recommendation engine](https://youtu.be/tooddaC14q4).
2. [How to leverage GPT-2 to generate descriptions of new Netflix content](https://youtu.be/NvMoFeO0aGE).
3. [Full recordings of 1:1 coaching sessions with an ML student.](https://youtu.be/N2tDfXdZmdE)
4. [Resume Critique of a student who just completed a certificate.](https://youtu.be/Ztexwmrxt2A)
5. [Q&A Recording with a Principal Data Scientist.](https://youtu.be/r-NjlPW-Ihg) 

This is all really new and has been a blast to work on. Let me know what you think. 

[Channel Link](https://www.youtube.com/channel/UC0-S_HnWTDFaXgTbYSL46Ug)

If you like it, definitely subscribe! I try to put out videos every week. 

Also, feel free to connect with me on [LinkedIn](https://www.linkedin.com/in/madhavthaker/). I try to make myself as accessible as possible on there."
148,learnmachinelearning,gpt,comments,2023-03-25 09:59:57,Are chat gpt code outputs plain wrong?,SnooHabits4550,False,0.43,0,121h5ii,https://www.reddit.com/r/learnmachinelearning/comments/121h5ii/are_chat_gpt_code_outputs_plain_wrong/,19,1679738397.0,"I asked chatgpt how can I standardize give time series and it gave me following:

https://preview.redd.it/ax6hgvnuyupa1.png?width=711&format=png&auto=webp&s=409bfb5657461bb52718a80ea92b553e842c0959

It gave output which seem incorrect (I tried running that code). So asked it whether it executed that code and it confirmed it indeed executed that code!

**Update**

In case you want to know further conversation:

https://preview.redd.it/bh8uwiidmvpa1.png?width=672&format=png&auto=webp&s=6309e5b149106bfdaa46b457e0cd0ef24149b72c

https://preview.redd.it/avdrdf3gmvpa1.png?width=787&format=png&auto=webp&s=a9886ab88c67406f2bedf0bdeaa9a6bf04f38883

Still wrong output.

&#x200B;

https://preview.redd.it/gsplbvzkmvpa1.png?width=696&format=png&auto=webp&s=cd6dff676d1ec5f472bde05c070b8072764a8007

&#x200B;

https://preview.redd.it/fdkleplmmvpa1.png?width=677&format=png&auto=webp&s=8698d565059cd5749e5fba7fd3df270f07a92a4c"
149,learnmachinelearning,gpt,comments,2023-01-11 14:03:46,What do you all think about these ‚ÄúSEO is Dead‚Äù articles?,Aggressive-Twist-252,False,0.89,111,1095h99,https://www.reddit.com/r/learnmachinelearning/comments/1095h99/what_do_you_all_think_about_these_seo_is_dead/,20,1673445826.0,"I keep seeing [articles](https://jina.ai/news/seo-is-dead-long-live-llmo/) like this over the years and it made me wonder. Is SEO really dead? Or will it evolve? Back then I kept wondering if it‚Äôs true or not. Some believe SEO is dead, some don‚Äôt. But now with tools like Chat GPT and Midjourney, I think it‚Äôs time to take a look back and see how this might change SEO or if it will ‚Äúkill‚Äù SEO.

I keep seeing threads and discussions seeing how people are excited and worried at the same time with how AI might be able to do a better job. But the way I see it, AI content still needs a person to tell it what to do and make the writing look nice. And also I think that the internet will have a lot of writing that was made by AI and that might change how we find things online. You might also see a ton of content being written by AI and trigger some plagiarism detectors and have a lot of websites get penalized. Hopefully the internet won‚Äôt be filled with boilerplate copy/pasted content coming from Chat GPT.

Well we have Google to filter out trash content anyway. But I know Google has some issues lately that they need to fix. One is that they also have AI that can help people find things on the internet with their search engine, and they need to make sure they are still the best in terms of search. 

The second is that Google needs to find a way to tell if something is really good or not, like how some websites that show art do. Google wants to show the best thing first, but it's hard because sometimes the thing that is the best is also something that Google's customers want people to see. It‚Äôs possible that some AI generated contentSo it's kind of tricky.

I have a feeling companies that already make SEO-writing and checking bots are gonna roll out some fresh new models soon. They're gonna be even better than before. These bots are going to write some good articles and product descriptions that are almost perfect. It almost looks like a human wrote the article or description. And all a human will do is quickly check for any false claims and write a headline that doesn't sound like a robot wrote it. 

We can only really tell 5-10 years from now. In the meantime, I‚Äôll probably go back practicing some handyman skills and also go back teaching people how to drive and also be a service driver. These jobs I had in the past were way different from what I am earning now but if the worst comes to worst, at least I have these physical skills ready."
150,learnmachinelearning,gpt,comments,2023-03-05 23:30:47,How does the transformer model lead to emergent intelligence?,VanillaSnake21,False,0.57,1,11jhl4y,https://www.reddit.com/r/learnmachinelearning/comments/11jhl4y/how_does_the_transformer_model_lead_to_emergent/,18,1678059047.0,"I'm trying to understand how a transformer model such as the one used by Bing or ChatGPT leads to the emergence of intelligence, memory etc. I'm not too versed in ML but you can explain using advanced terms - I'll just ask Bing to elaborate on the details."
151,learnmachinelearning,gpt,comments,2024-02-10 14:24:09,Making a small machine learning project with given data . So i have to train it on the data to predict the future data.,msssszzzzzzzz,False,0.35,0,1anhfxy,https://i.redd.it/3ho6qk8aprhc1.jpeg,19,1707575049.0,"
So i have given this project and i have to make a machine learning project on this to predict the data in future i am noob in this with little to no info about machine learning and how can i use chat gpt to make my work easier . Any pointers or help will be welcome ."
152,learnmachinelearning,gpt,comments,2022-12-27 18:05:30,Am I Too Late?,stupidSTEMquestions,False,0.5,0,zwltk8,https://www.reddit.com/r/learnmachinelearning/comments/zwltk8/am_i_too_late/,19,1672164330.0,"I am a college student studying math and computer science. I know how to program with high level languages, C, and a bit of C++ and Scheme. I can build basic web apps and scripts, and am focusing on machine learning with python. 

With the release of ChatGPT and articles like [this](https://cacm.acm.org/magazines/2023/1/267976-the-end-of-programming/fulltext#comments) though, I can't help but ask ‚Äî am I too late?

Is it simply too late for a beginner to make any contributions to the field at this point when OpenAI, Deepmind, and the like are iterating at such a rapid pace? I really love AI and machine learning so far, but I also don't want to waste my time and energy if there won't be any meaningful work for me once I finish my education in 4 or more years."
153,learnmachinelearning,gpt,comments,2023-10-06 12:17:41,Wrap my own API library in a GPT-like based chatbot,motorollo,False,0.62,2,171ati0,https://www.reddit.com/r/learnmachinelearning/comments/171ati0/wrap_my_own_api_library_in_a_gptlike_based_chatbot/,18,1696594661.0,"Hi everyone!
I am working on a project whose ultimate goal is to be able to chat with a chatbot that after a natural language request returns specific calls to library functions that implement calls to a tool's API.

A bit more into details: I have a tool that can be used by calling its HTTP API. In order to make these calls, I have built a library on top of the API, so as to simplify the calls and make them easier to use. I would like to have a chatbot whose model is trained on the library itself, so that, given a natural language request as input, it returns its translation into the library language. 

My first idea was to use OpenAI API service as a wrapper and eventually try other models in the future if I find a better and/or cheaper custom way to implement the chatbot.

My doubts are mainly 2:
1. Fine-tuning: which is the best way to fine-tune the OpenAI model for this use case?

2. Well formatted calls: how to avoid the chatbot from returning non-existing method calls or classes and stick with what is available in the library?

Any other suggestion outside the scope of the doubts are more than welcome.
Thank you very much in advance for every guy that will read the post."
154,learnmachinelearning,gpt,comments,2024-02-19 01:44:52,GPU vs CPU for inference,AI-Brain,False,0.63,2,1aubc4u,https://www.reddit.com/r/learnmachinelearning/comments/1aubc4u/gpu_vs_cpu_for_inference/,18,1708307092.0,"I read through some comments on the sub and understood that GPUs are mainly used for training and CPUs for inference. Had some follow ups: 
1. Is there a scenario where GPUs are better suited for inference? 
2. Would you say that if you are handling billions of inference requests then you should use GPUs? 
3. What makes GPUs inefficient for inference? (If any) 
4. When we use chatGPT, is OpenAI using CPUs for inference? 

Any insights would be helpful."
155,learnmachinelearning,gpt,comments,2023-12-28 22:20:51,"Why does Google look so dominant in search, but seems to admit that they have no moat with GenAI?",PsychoWorld,False,0.42,0,18t6ced,https://www.reddit.com/r/learnmachinelearning/comments/18t6ced/why_does_google_look_so_dominant_in_search_but/,17,1703802051.0,"Hey everyone.

Background: I studied Cognitive Science, which included some Calculus and data structures in Computer Science. I'm not the MOST technical person there is, but I can comprehend concepts that tap into advanced computer science domains.

1. Google is dominant in online searching. Despite sustained efforts by companies with platform advantages, including Microsoft and Apple (Bing and Siri) attempting to make their search engines better, Google seems far and away the most preferred search engine worldwide.

2. Meanwhile... They seem to admit internally that GenAI isn't a race that they can sustainably win (https://www.semianalysis.com/p/google-we-have-no-moat-and-neither). The have no moat in this.

What is the technical cause of this?

I'm unfamiliar with the technical reasons for why they're so much better in search, but ChatGPT seems to think it's due to their data advantage and focus on user experience. But I'm still not sure why that cannot be overcome. Is search technology much more reliant on one aspect of tech infrastructure that cannot be overcome by other companies, whereas LLMs are dependent on open-source tech and data, and thus can be improved upon easily?

Is deep learning why they're so much better?"
156,learnmachinelearning,gpt,comments,2023-02-21 14:59:06,I created a Search Engine For Books using GPT-3 üîéüìò. Here's how you can create it too:,Pritish-Mishra,False,0.94,86,1185dhq,https://youtu.be/SXFP4nHAWN8,17,1676991546.0,
157,learnmachinelearning,gpt,comments,2023-10-13 21:02:42,ChatBase Backend: How Does it Work?,Nice-Ad1199,False,1.0,2,1778n3l,https://www.reddit.com/r/learnmachinelearning/comments/1778n3l/chatbase_backend_how_does_it_work/,17,1697230962.0,"Hey all,

I've been building my own ""personal assistant"" using the GPT API and Eleven Labs, and I am finally getting to the fine-tuning portion of everything. That being said, I have been primarily working with fine-tuning GPT directly through the OpenAI documentation, finding some success, but nothing too amazing quite yet.

&#x200B;

That being said, I was pointed to [ChatBase](https://www.chatbase.co/), a website that trains GPT on your data. I am assuming many of you have seen it, but the point is you can put documents, text, Q&A's, and web data which it will then train GPT on. The results are quite good with proper data, but it really doesn't require much to produce results.  


I imagine that they are using the same fine tuning techniques, but I question how they are able to produce such fantastic results with such little information. Perhaps there is something I am missing in the documentation? Does anybody know how one might be able to achieve similar results to a custom ChatBase model through their own GPT fine-tuning data set?"
158,learnmachinelearning,gpt,comments,2023-03-30 12:56:24,I created this entire video using ChatGPT + Charactr API + D-ID. My mind is blown,3nd4u,False,0.86,66,126m5eo,https://www.reddit.com/r/learnmachinelearning/comments/126m5eo/i_created_this_entire_video_using_chatgpt/,15,1680180984.0,"Could this be the future of how our news is being consumed?

https://reddit.com/link/126m5eo/video/hhfat6n3jvqa1/player"
159,learnmachinelearning,gpt,comments,2023-10-22 04:22:24,Built it for my project at first: Memorybase.io supercharges ChatGPT with memory capabilities for your chatbots,tujiserost,False,0.77,9,17dl1bw,https://www.reddit.com/r/learnmachinelearning/comments/17dl1bw/built_it_for_my_project_at_first_memorybaseio/,17,1697948544.0,"Hey everyone!

I've been delving deep into chatbots lately, especially with the ChatGPT API, and I found an issue that's probably familiar to many of you: ChatGPT doesn't inherently have memory capabilities. For many applications, that's perfectly fine, but for those of us who are trying to create a more context-aware and dynamic conversation flow, this limitation is quite apparent.

I faced this challenge in one of my projects and realized that there had to be a better way to integrate context and memory into ChatGPT's conversations. So, I built something for myself which I thought might be useful for many of you as well. Allow me to introduce you to [**Memorybase.io**](http://memorybase.io/).

Memorybase is a developer-friendly API that's designed to seamlessly integrate memory functionality into the ChatGPT API. By harnessing the power of the Pinecone vector database and LangChain, Memorybase wraps around the ChatGPT API and ensures that the right context and memory are injected into each query. This means that your chatbot can remember previous interactions, preferences, or any other context that's relevant for more engaging and meaningful conversations.

Imagine a user asking your chatbot about movie recommendations. The next day, they come back and reference that conversation, expecting the bot to remember. With Memorybase, that continuity becomes possible. The user experience improves manifold, and the possibilities for more sophisticated and context-aware bots increase tremendously.

I originally built Memorybase for my own needs. But the more I used it, the more I realized that this could have broader applications. Any developer looking to leverage the ChatGPT API could potentially benefit from the enhanced memory and context capabilities. From customer support bots to interactive storytelling, the potential use cases are vast.

This technology stack (pinecone/langchain) is not complex or ‚Äònew‚Äô per se, but for application developers who aren‚Äôt interested in managing it or hosting it, this could be a useful hassle-free option for your projects.

I've set up a page over at [memorybase.io](https://memorybase.io/) where you can learn more about how it works and see if it aligns with your needs. I would love for you to check it out and share your thoughts. Your feedback, insights, and potential use cases would be invaluable as I continue to refine and expand the capabilities of Memorybase.

Thanks for reading, and I'm eager to hear your thoughts and see where Memorybase can fit into the exciting world of chatbots!"
160,learnmachinelearning,gpt,comments,2023-02-11 12:46:22,"ChatGPT Powered Bing Chatbot Spills Secret Document, The Guy Who Tricked Bot Was Banned From Using Bing Chat",vadhavaniyafaijan,False,0.94,210,10zmtqz,https://www.theinsaneapp.com/2023/02/chatgpt-bing-rules.html,15,1676119582.0,
161,learnmachinelearning,gpt,comments,2023-09-16 13:22:41,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.95,134,16k7heb,https://www.reddit.com/r/learnmachinelearning/comments/16k7heb/this_week_in_ai_all_the_major_ai_developments_in/,17,1694870561.0,"1. **Stability AI** launched Stable Audio, a generative AI tool for music & sound generation from text. The underlying latent diffusion model architecture uses audio conditioned on text metadata as well as audio file duration and start time.
2. **Coqui** released **XTTS** \- a new voice generation model that lets you clone voices in 13 different languages by using just a quick 3-second audio clip.
3. **Microsoft Research** released and open-sourced **Phi-1.5** \- a 1.3 billion parameter transformer-based model with performance on natural language tasks comparable to models 5x larger.
4. **Project Gutenberg**, Microsoft and MIT have worked together to use neural text-to-speech to create and release thousands of **human-quality free and open audiobooks**.
5. Researchers present **NExT-GPT -** an any-to-any multimodal LLM that accepts inputs and generate outputs in arbitrary combinations of text, images, videos, and audio.
6. **Chain of Density (CoD):** a new prompt introduced by researchers from Salesforce, MIT and Colombia University that generates more dense and human-preferable summaries compared to vanilla GPT-4.
7. **Adept** open-sources **Persimmon-8B**, releasing it under an Apache license. The model has been trained from scratch using a context size of 16K.
8. **Adobe's** **Firefly** generative AI models, after 176 days in beta, are now commercially available in Creative Cloud, Adobe Express, and Adobe Experience Cloud. Adobe is also launching Firefly as a standalone web app.
9. **Deci** released **DeciLM 6B**, a permissively licensed, open-source foundation LLM that is 15 times faster than Llama 2 while having comparable quality.
10. Researchers release **Scenimefy** \- a model transforming real-life photos into Shinkai-animation-style images.
11. **Microsoft** open sources **EvoDiff**, a novel protein-generating AI that could be used to create enzymes for new therapeutics and drug delivery methods as well as new enzymes for industrial chemical reactions.
12. Several companies including Adobe, IBM, Nvidia, Cohere, Palantir, Salesforce, Scale AI, and Stability AI have pledged to the White House to develop safe and trustworthy AI, in a voluntary agreement similar to an earlier one signed by Meta, Google, and OpenAI.
13. **Microsoft** will provide legal protection for customers who are sued for copyright infringement over content generated using Copilot, Bing Chat, and other AI services as long as they use built-in guardrails.
14. **NVIDIA** beta released **TensorRT** \- an open-source library that accelerates and optimizes inference performance on the latest LLMs on NVIDIA Tensor Core GPUs.
15. Pulitzer Prize winning novelist Michael Chabon and several other writers sue OpenAI of copyright infringement..
16. **NVIDIA** partners with two of India‚Äôs largest conglomerates, Reliance Industries Limited and Tata Group, to create an AI computing infrastructure and platforms for developing AI solutions.
17. **Roblox** announced a new conversational AI assistant that let creators build virtual assets and write code with the help of generative AI.
18. **Google** researchers introduced **MADLAD-400** \- a 3T token multilingual, general web-domain, document-level text dataset spanning 419 Languages.
19. A recent survey by **Salesforce** show that 65% of generative AI users are Millennials or Gen Z, and 72% are employed.  The survey included 4,000+ people across the United States, UK, Australia, and India.
20. **Meta** is reportedly working on an AI model designed to compete with GPT-4.

My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
162,learnmachinelearning,gpt,comments,2023-10-12 00:18:56,How To Catch AI-Cheating,Science-man777,False,0.5,0,175tbwn,https://www.reddit.com/r/learnmachinelearning/comments/175tbwn/how_to_catch_aicheating/,17,1697069936.0,"""If you happen to have any dilutions of students not using ChatGPT and other artificial intelligence (AI) to cheat, it is time to get informed.¬† According to a recent survey from the Center for Democracy and Technology, [58% of students](https://cdt.org/wp-content/uploads/2023/09/091823-CDT-Off-Task-Summary-web.pdf) report using generative AI to complete assignments.¬† As awareness of this technology rises, this number only stands to increase. Meanwhile, the same study reports that educators find themselves behind the technology curve, with only 43% of teachers having been significantly trained on generative AI.¬†

""...how students use this technology to cheat and how teachers can detect and respond to generative AI. Beyond just detecting its use, this new technology may present an opportunity to leverage new and innovative ways of educating.""

[https://ai-solutions.pro/tools-to-detect-ai-cheating/](https://ai-solutions.pro/tools-to-detect-ai-cheating/)"
163,learnmachinelearning,gpt,comments,2023-01-05 06:32:22,I Built A GPT-3 Powered Productivity App - Tutorial included,SupPandaHugger,False,0.97,210,103rv9o,https://i.redd.it/gtywivh756aa1.gif,17,1672900342.0,
164,learnmachinelearning,gpt,comments,2020-06-22 16:11:03,Generating Fake News with OpenAI‚Äôs GPT-2 Language Model,dxjustice,False,0.93,13,hdv7mk,https://towardsdatascience.com/creating-fake-news-with-openais-language-models-368e01a698a3#f7e4-509ea6e7d52d,16,1592842263.0,
165,learnmachinelearning,gpt,comments,2023-09-23 13:42:22,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.96,187,16q4ve6,https://www.reddit.com/r/learnmachinelearning/comments/16q4ve6/this_week_in_ai_all_the_major_ai_developments_in/,16,1695476542.0,"1. **Genmo** releases a new text-to-video model: **Genmo Replay** v0.1, which generates high-quality videos from text without the need for advanced prompt engineering. *Genmo is available for free to create AI videos* \[[*Details*](https://blog.genmo.ai/log/replay-ai-video) | [Genmo *Replay*](https://www.genmo.ai/)\] .
2. **OpenAI** unveils **DALL¬∑E 3** \- a major update to the text-to-image model, which will be integrated in ChatGPT. It will be available to ChatGPT Plus and Enterprise users in October, via the API and in Labs later this fall. Creators can now also opt their images out from future training.
3. **Toyota Research Institute** has developed a technique, powered by generative AI, that enables teaching robots new manipulation abilities in a single afternoon. Using the same robot, same code, and same setup, TRI taught over 60 different dexterous behaviors like peeling vegetables, using hand mixers, preparing snacks, and flipping pancakes.
4. **Microsoft** announced:
   1. Availability of AI Copilot for Windows from September 26th. Copilot will incorporate the context and intelligence of the web, your work data and what you are doing in the moment on your PC to provide better assistance. It will be integrated in Windows 11, Microsoft 365, Edge and Bing.
   2. Bing will add support for DALL.E 3 and deliver more personalized answers based on search history.
   3. New AI powered experiences in Paint, Photos and Clipchamp.
   4. New AI-powered shopping experience
5. **ElevenLabs** released **Projects** \- a tool that lets you generate an entire audiobook at the click of a button. Projects now supports .epub, .pdf, and .txt file imports, as well as initializing a project from a URL.
6. **Deci** presents **DeciDiffusion 1.0** \- an open-source text-to-image latent diffusion model which is 3x faster than Stable Diffusion v1.5 with the same quality.
7. **Google researchers** present a new approach that produces photo-realistic animations from a single picture. The model is trained on automatically extracted motion trajectories from a large collection of real video sequences.
8. **Google** has updated Bard\*\]\*:
9. **Bard Extensions:** With extensions, Bard can now connect to your Google apps and services like Gmail, Docs, Drive, Google Maps, YouTube, and Google Flights and hotels.
10. Users can use Bard‚Äôs ‚ÄúGoogle it‚Äù button to more easily double-check its answers and evaluate whether there is content across the web to substantiate it.
11. Bard can now let you continue chat via **shared public links**
12. **YouTube** announces new AI tools for creators. **Dream Screen** will let users create an AI-generated video or image background from text. Automatic AI-dubbing tool called **Aloud**, which will be integrated into YouTube Studio. **AI-powered insights** to generate video ideas and draft outlines. **Assistive Search in Creator Music** where AI will suggest the right music based on your description of your content.
13. **Amazon** announced that its voice assistant Alexa is being upgraded with a new, custom-built large language model.
14. **IBM** open-sources **MoLM** \- a collection of ModuleFormer-based language models ranging in scale from 4 billion to 8 billion parameters. ModuleFormer is a new neural network architecture based on the Sparse Mixture of Experts (SMoE) by IBM researchers. .
15. **Neuralink**, Elon Musk's brain implant startup, set to begin human trials.
16. **Lexica** has released **Aperture v3.5** \- their latest next-gen image model that can create photorealistic images and follows your prompt with precision.
17. **OpenAI** has invited domain experts to collaborate in evaluating and improving the safety of OpenAI's models by joining the new **OpenAI Red Teaming Network**.
18. \*\*GitHub Copilot Chat (\*\*beta) is now available for all individuals.
19. **Replit** announced a virtual hackathon for projects built using **Replit ModelFarm**
20. **Oracle** brings voice-activated AI to healthcare with Clinical Digital Assistant.
21. **Google** and the Department of Defense are building an AI-powered microscope to help doctors spot cancer.

My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
166,learnmachinelearning,gpt,comments,2024-01-03 01:17:54,What libraries should I become proficient in as a machine learning engineer?,Bbpowrr,False,0.9,16,18x6eu9,https://www.reddit.com/r/learnmachinelearning/comments/18x6eu9/what_libraries_should_i_become_proficient_in_as_a/,16,1704244674.0,"I do MLE / DS at a big 4 firm, and have been doing so for about 2 years. I have experience with implementing some pretty cool solutions using the following libs:
- open AI (gpt & embedding models)
- huggingface
- faiss
- scikit-learn

I also have a 1st class CS degree from a Russell group uni and have done some ML projects during my degree.

But I have never had to / been taught how to use libraries such as TensorFlow or PyTorch or Keras in any ML project that I have implemented. Usually I use the scikit-learn library for model development.

However, I see a lot of jobs specifically asking for TensorFlow or PyTorch or Keras.

Therefore, I was wondering whether it is necessary to start upskilling in one or all of these libraries/frameworks to become a well established MLE? And if so, what is the best way to learn them?



Also, with my current skill set, how do I fair in the job market for MLE / DS roles? Ik it's not much to go off but any guesses would be appreciated.

For context, I have experience with using the following types of models:
- GPT / embedding
- ensemble (Random Forest, XGBoost, AdaBoost)
- computer vision (OCR)
- Clustering (cus Weka)
- SVM
- Naive Bayes
- Logistic regression 
- decision trees 

I think an obvious gap is a lack of neural networks / CNNs - does this matter much?

Many thanks for any advice!"
167,learnmachinelearning,gpt,comments,2023-06-20 17:56:06,LLM questions,YellowSea11,False,0.5,0,14ei1z9,https://www.reddit.com/r/learnmachinelearning/comments/14ei1z9/llm_questions/,15,1687283766.0,"Hey gang .. two questions . 1) can I make chatGPT local? Meaning I don't want it connected to the internet at all .. is that doable? 
2) can I train it on my data? What are my options there? Ideally I'd love to be able to read data from a table so I could train it on those terms as well."
168,learnmachinelearning,gpt,comments,2023-11-03 05:00:17,How do y'all deal with hallucinating in GPT 3.5?,supa_ai,False,0.74,16,17mngkq,https://www.reddit.com/r/learnmachinelearning/comments/17mngkq/how_do_yall_deal_with_hallucinating_in_gpt_35/,16,1698987617.0,"Hey guys,

We're trying to build an AI chatbot for internal purposes. So far, we've tried the usual suspects like different approaches to prompt engineering and RAG.

The main issue is that despite RAG retrieving the correct context, we still experience significant (3 in 10) amounts of hallucination. Has anyone experienced the same problem? We'd love to hear any alternative approaches or discussion here on alternate methods."
169,learnmachinelearning,gpt,comments,2023-01-06 11:58:10,How does ChatGPT actually work? Explained simply with pen and paper,techie_ray,False,0.94,125,104sebq,https://youtu.be/k9Sps7ciNTE,16,1673006290.0,
170,learnmachinelearning,gpt,comments,2019-08-11 14:32:04,`gpt2-client`: A New Wrapper for GPT-2,rish-16,False,0.81,3,coxeko,https://www.reddit.com/r/learnmachinelearning/comments/coxeko/gpt2client_a_new_wrapper_for_gpt2/,16,1565533924.0,"Hey everyone üëãüèª

I recently built a wrapper for OpenAI's \`gpt-2\` model called \`gpt2-client\`. Currently, the \`gpt-2\` repo is archived and the code is messy and riddled with bugs. My wrapper simplifies the entire process by enabling anyone to get started with text generation models without the fuss.

&#x200B;

[This is how it looks like](https://preview.redd.it/ookaj95lytf31.png?width=1584&format=png&auto=webp&s=efaea7da8edc03136c627223a2d71ff6a4ef0c73)

Please do go check it out here:

[https://github.com/rish-16/gpt2client](https://github.com/rish-16/gpt2client)

If you like it, a ‚≠êÔ∏è on GitHub would be highly appreciated! It's my first ever Python module I've released and am really excited about it.

If you run into any bugs, please do file an issue and if you have any suggestions or enhancements, please do file a PR with a short description of your awesome improvement.

Cheers!"
171,learnmachinelearning,gpt,comments,2022-12-20 11:12:21,What are the advantages of training your own model rather than customizing GPT3 ?,wootfacemate,False,0.89,56,zqlqzj,https://www.reddit.com/r/learnmachinelearning/comments/zqlqzj/what_are_the_advantages_of_training_your_own/,16,1671534741.0,"Hello,   
I am a beginner in ML, so it might sound obvious but with such powerful tool like GPT, I was wondering why wouldn't you always use a pre-trained model like GPT that is way more powerful rather than fit your own model ?"
172,learnmachinelearning,gpt,comments,2024-01-04 18:59:47,"is there a ""for dummies"" way to train and use a CNN?",Phischstaebchen,False,0.77,14,18ykuqd,https://www.reddit.com/r/learnmachinelearning/comments/18ykuqd/is_there_a_for_dummies_way_to_train_and_use_a_cnn/,15,1704394787.0,"Hello,

I'm stuck with a little project of mine and I probably need to use machinelearning for it, but without much background in coding. I did some Python code for several Raspberry Pis but that's all.

I have stable drone-footage of dolphins filmed from 20-30m above the shore with a Mavic Air. I just need to detect them in the actual footage and follow each to track their hunting-movement. 

I haven't found trained models specific for dolphins, maybe Yolo will work? Aside from that I also found this dolphin-dataset that could be used to train? [https://arxiv.org/abs/2005.13359](https://arxiv.org/abs/2005.13359)

Aside from that I have enough drone-footage to actually get proper footage from above.

But from here I have not really a clue what to do. I installed Debian on my PC (Ryzen 9 5950x, 64GB RAM, 2060 Super) to get the Windows-headaches out of the way. Training-time isn't an issue. I can let the PC run for days if neccesary. From my point of view, training a model for this task is the biggest prob? Using trained models on footage did work for a quick and dirty try with generic stuff.

Can anybody help?

&#x200B;

Oh and yes, I used ChatGPT already... the code needs heavy fixing and sometimes ChatGPT just stops to give useful information and just keeps repeating generic information like ""you need to train your model before you can use it"".... lol"
173,learnmachinelearning,gpt,comments,2020-08-27 03:29:43,"GPT-3 used to generate code for a machine learning model, just by describing the dataset and required output / Via Matt Shumer(Twitter)",TheInsaneApp,False,0.96,128,ihdpgv,https://v.redd.it/1op7cffisgj51,15,1598498983.0,
174,learnmachinelearning,gpt,comments,2023-04-06 11:12:52,Meta: Is it possible to ban these TikTok influencers or TikToks in general?,dasMaiMaiKamel,False,0.94,222,12dgtry,https://www.reddit.com/r/learnmachinelearning/comments/12dgtry/meta_is_it_possible_to_ban_these_tiktok/,14,1680779572.0,"I'm new to this sub and I'd love to contribute here. But there are soooo many TikTok videos from someone talking about ChatGPT for the 10.000th time. These videos don't contribute to learning ML nor do they give actual reliable information. I often get the feeling that these people never touched a NN, just sat on ChatGPT and read one WikiPedia article. It's also often more an ad than actual help.  


  
Even if I'm not a member for too long, I see comments criticizing this exact thing under every video. Is it possible to add a rule to prevent this? It would greatly improve the quality of this sub."
175,learnmachinelearning,gpt,comments,2023-09-12 13:42:02,This is why LLMs have flooded the NLP market in the past 1 year üëá (A Brief History of NLP),japkeerat,False,0.83,45,16grq5y,https://www.reddit.com/r/learnmachinelearning/comments/16grq5y/this_is_why_llms_have_flooded_the_nlp_market_in/,15,1694526122.0,"Text Generation has been the hottest topic in Natural Language Processing. Recurrent Neural Networks (RNNs) were among the Algorithms to generate text. How RNNs generated text is by essentially predicting the next word given the previous few words. At one-stage RNNs were the hottest commodity one could have. But researchers were worried about 1 problem.

RNNs had a context-length problem. To understand what is context-length, consider an analogy. You started reading a book, it‚Äôs 100 pages long and when you read each page, details of previous pages start to get a little hazy. Haziness keeps on increasing to the point that when you reach page 50, you don‚Äôt remember anything from the first 5 pages. That is exactly what the problem is with RNNs.

To solve this, researchers developed another algorithm called the Long-Short Term Memory (LSTM) and another variant called Bidirectional Long-Short Term Memory (Bi-LSTM) which had a larger context-length than RNNs. Let‚Äôs get back to the book analogy. This time while reading, you are making notes. When you go ahead to a new page and your previous pages information start to get hazy, you look back at these notes to refresh your memory. It‚Äôs oversimplified, but that‚Äôs basically how an LSTM works.

LSTMs were not perfect. There were a number of new issues that came up in order to resolve the previous one. Meanwhile, other areas of research and technological advancements were heating up. Hardware was getting more and more prominent and with cloud getting popular, it was easily accessible. And on the research side, a new kind of Algorithm came up that shaped the entire NLP domain from here on - Attention Mechanism.

Attention Mechanism, as you might have guessed, is all about telling the more sophisticated algorithms where to ‚Äúfocus‚Äù. It‚Äôs the same way how we focus more on certain parts of the meeting we attend than the entire meeting itself. In context of NLP, the Mechanism became the core part for better algorithms. These better algorithms could keep larger context-lengths and at the time of predicting the next word, ask the Attention Mechanism about what to focus on while predicting the next word. This was an era-defining discovery in NLP as the algorithms that came up after this were the Transformers.

Consider jigsaw puzzles. You start by looking at all the pieces at once and join the pieces together. Initially, it is random. You join a couple of pieces at the top left corner, a few in the centre and a couple more defining the right edge. You are doing it all at once. Transformers basically work the same way. They could look at longer context-lengths, all at once, courtesy of Attention Mechanism. This means, they can not only work with a sentence, they can work with an entire paragraph.  With time, these Transformers started becoming more and more sophisticated. It eventually reached to a point that the only thing that was keeping these algorithms in handcuffs was the lack of data.

Until recently, these algorithms were trained on a specific data but when algorithms became too powerful, researchers started throwing every kind of data they could find on the internet easily. It could be articles like this, your social media posts, exam papers and solutions, and ebooks in any language they could find and hoped the algorithms learnt it all. And they were right. Algorithms started learning all of it to the point that you could ask models to explain concepts of LLMs in how Shakespeare would write and it would give a real-sounding responsive. These algorithms were Large! And hence, became known as Large Language Models (LLMs).

There we are now. With LLMs. OpenAI, technically, won the race for LLM development. They brought everybody‚Äôs attention to LLMs first with GPT-2, but GPT-3 was where shit hit the roof and every company that had deep pockets started investing in LLMs.  The result? We now have a new LLM getting released EVERY. SINGLE. DAY.

*I post articles like these every few days on X. If you like this post, please* [follow me on X!](https://twitter.com/JapkeeratS/)

*NOTE: To make it simple for anybody, even without a tech background, to understand, a few things were oversimplified. I will be sharing soon on* [my X handle](https://twitter.com/JapkeeratS) *a technical version.*"
176,learnmachinelearning,gpt,comments,2023-03-29 02:00:28,"Running something like GPT-2 locally, training with my own data",SigmaSixShooter,False,0.86,5,1259tlx,https://www.reddit.com/r/learnmachinelearning/comments/1259tlx/running_something_like_gpt2_locally_training_with/,15,1680055228.0,"Greetings, I hope I'm asking in the right place. 

I've been really amazed with ChatGTP-3 and ChatGTP-4 and started thinking how I can use them in my own company. I'd love to train something based on all of our previous tickets. The issue is, these tickets contain sensitive information and customer data, so I can't use some cloud based API. 

So let's say Bob is an excellent worker. His tickets are the gold standard, he always writes in a professional voice with detailed information. There's 20 different issues we fix as part of our business, and over the past 3 years, Bob has covered all of them 100 times over. 

So, I'd like to export all of the tickets Bob has worked and use them to train some GTP/AI type model. I'd like to be able to write a prompt like 'For Issue A with these variables, write up an issue description for our customer"" 

I've been trying to wrap my head around this, but it's an awfully overwhelming subject. Talking with GTP-4 it looks like I can try either GTP-2 or DistilGTP. I also came across Llama.cpp which just came out the other day it seems. 

With this in mind, I've got a few questions

1. Is there any option I should consider that lets me run this on something with 32 gigs of ram and 8 to 16 cores? I've got access to a few pieces of hardware. Again, so far I'm looking at DistilGTP, GTP-2, or llama.cpp
2. Will I need other data sets if I ever figure out how to train this on Bob's tickets? Or will that be enough? I'm trying to figure out if I need the several hundred gigs of other models out there. 
3. How the heck do I go about getting started? :) If someone can help me narrow things down to which LLM (if that's even the right term) I should use to accomplish my goals, and some basic instructions on how to train the data, I think I can figure out the rest after a few thousand rounds of trial and error. 

&#x200B;

Thanks in advance for your time and help."
177,learnmachinelearning,gpt,comments,2023-07-25 20:56:55,Hi r/learnmachinelearning! To make CUDA development easier I made a GPT-4 powered NVIDIA bot that knows about all the CUDA docs and forum answers (demo link in comments),srnsnemil,False,0.96,168,159kt6u,https://v.redd.it/58hbh8q0d6eb1,15,1690318615.0,
178,learnmachinelearning,gpt,comments,2023-03-27 07:34:58,i tried to get a grasp of LLMs using ChatGPT. Im not quite sure what to think of it. Can someone asses the conversation and tell me wether it is valuable or basically fanfiction?,overlydelicioustea,False,0.58,4,123fcrq,https://www.reddit.com/r/learnmachinelearning/comments/123fcrq/i_tried_to_get_a_grasp_of_llms_using_chatgpt_im/,13,1679902498.0,"https://pastebin.com/sEsDHQFG

IT is a very long conversation, i apologize. Also its missing the initial conversation i had with Bing chat (this is why i suddenly know how neurons work, that part is basically the culmination of the bing chat it all started with..). As far as i can tell there is no way to access previous bing chats again...


Now, theres propably some errors in it, but what i want to know is, is this a viable approach, is the picture it created in my head accurate enough to at least have somewhat of an opinion about things?

remember, before i talked to it I had no idea about how LLMs actually work. I had heard of parameters of transformer, of neurons but that was about it, I didnt knew what these words actually stood for.


Now mind you, my goal wasnt to get a deep scientific understanding so that i can build my own models. I wanted to end up with a valid general overview of the technology so that i can build somehwat(!) of an imformed opinion about what is about to come. Would you say this ended up successfully?

im also trying to assess how valuable this process is for other things i diont know yet. Will I / Can I in the future just talk to the bot for a few hours to get an understanding of $new instead of googling and sifting through crap?

edit: acutally forgot to link it initially... https://pastebin.com/sEsDHQFG"
179,learnmachinelearning,gpt,comments,2023-03-16 02:58:26,I want to create a ChatGPT-like interface but to interact with a smaller specialized dataset.,ohai777,False,0.88,16,11si7ku,https://www.reddit.com/r/learnmachinelearning/comments/11si7ku/i_want_to_create_a_chatgptlike_interface_but_to/,11,1678935506.0,I want to create a ChatGPT interface but to interact with a smaller specialized set of data for my website's support. Can you help me with what terms I need to google to learn more about researching a project like this or any tutorials on this topic? Natural Language processing?
180,learnmachinelearning,gpt,comments,2023-06-26 12:23:07,"Best way to cost effectively ""upload"" a large PDF to a language model so that you can ask questions about it?",RepresentativeNet509,False,1.0,9,14jfvq8,https://www.reddit.com/r/learnmachinelearning/comments/14jfvq8/best_way_to_cost_effectively_upload_a_large_pdf/,13,1687782187.0," I have a 400 page PDF and need to get it into a language model (cost effectively) and then be able to ask the model questions about the document like ""on what page does the scope summary begin"" or ""are there any prohibitions to participate in this solicitation due to the size of respondent's business"".

I have been able to use ""Ask My PDF"" to upload part of the PDF to ChatGPT and this basically gives the outcome I want for the pages that are uploaded, but it invariably crashes every time and there is no way to pick up where the uploading of pages left off.

I am fairly technical; would NanoGPT be a better solution for this? I am also looking at fine-tuning a model on OpenAI's API, but that seems cumbersome and expensive for my use case.

Any thoughts are appreciated!"
181,learnmachinelearning,gpt,comments,2022-08-16 07:20:55,AI content Generator,ionezation,False,0.5,0,wpnq6q,https://www.reddit.com/r/learnmachinelearning/comments/wpnq6q/ai_content_generator/,13,1660634455.0,Can we develop a AI content generator without GPT or any paid stuff?
182,learnmachinelearning,gpt,comments,2023-10-02 19:09:14,Whats the Field of ML/AI Look Like? Professional Looking for Guidance.,Pan4TheSwarm,False,1.0,11,16y5bko,https://www.reddit.com/r/learnmachinelearning/comments/16y5bko/whats_the_field_of_mlai_look_like_professional/,13,1696273754.0,"Let me start out this post by saying I'm feeling a little unsure of my professional ambitions right now and looking for some guidance from the community. I have a bachelor's in Electrical Engineering, focusing on embedded systems and RF communication systems. Additionally I have dedicated my time out of school studying the field of software engineering through books. My specialties are C/C++, with some Python mixed in here and there. Professionally, I'm working in C++ on IoT technologies and custom RF hardware. I have a solid background in mathematics from my studies. I've also had some interest in socio-linguisitcs. 

A couple weeks ago, I started playing around with ChatGPT, and I was insanely impressed. My ADHD brain got hyperfocused and needed to learn more. I've been diving into the world of ML/AI since. I've been playing around with hosting LLaMA models locally (running painfully slow on my 6800XT), and reading up on machine learning since. 

I don't know how far my interest goes at this point, but right now my interest is very strong. I'm trying to determine if my interest is in dabbling with ML/AI, or if I want to pivot my professional career towards ML/AI. Honestly, I'm not sure at this moment and here's where I am looking for some more perspective to help gauge my interests.

I asked ChatGPT for resources to look into. I tend to be a book learner, so I focused on the book recommendations. They recommended ""Python Machine Learning"" by Sebastian Raschka and Vahid Mirjalili; ""Deep Learning"" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville; and ""Pattern Recognition and Machine Learning"" by Christopher M. Bishop. 

I love me my kindle samples, and I figured an applications book would be good for me at this stage, so I picked up ""Python Machine Learning"". I'm enjoying the book, but after reading it for some time, I'm starting to contemplate if I should be instead going down a learning path geared towards a more professional placement. I read a [A Super Harsh Guide to Machine Learning](https://www.reddit.com/r/MachineLearning/comments/5z8110/d_a_super_harsh_guide_to_machine_learning/), and noticed their recommendations were more 'academic' in nature (""Deep Learning"" is on their list). Its making me second guess where I put my time, but it all depends on what I want my desired outcome to be, and frankly I'm still not sure. 

I'm also looking for a good point to enter grad school for a Masters. Maybe I want to go into ML and NLP? Do I need to be looking at a PhD for this field (which, I wouldn't mind pursuing)? 

There isn't a distinct question here, so I'm sorry about that. I'm looking for perspective, and guidance for the field so I can determine how I want to pursuit my interest in this area. Should I continue with ""Python Machine Learning""? Or should I follow the Super Harsh Guide more closely? "
183,learnmachinelearning,gpt,comments,2024-01-23 05:38:28,ChatGPT with multiple PDFs hallucinates,BigYesterday2785,False,0.62,2,19dh7pj,https://www.reddit.com/r/learnmachinelearning/comments/19dh7pj/chatgpt_with_multiple_pdfs_hallucinates/,13,1705988308.0,"I just don‚Äôt get it. I am building a ChatGPT for multiple PDFs.

Now I have **multiple PDFs** which I have created embeddings of and saved in MongoDB database.

Now when a user asks a specific question, I get **multiple outputs from GPT-3.5** and a lot of it does **not make sense**. Even though **real answer is still in there.**

Is there an API i can use to go through all data of PDFs and figure out **which one is relevant and then I run GPT-3.5 only on that specific part of PDFs**"
184,learnmachinelearning,gpt,comments,2023-01-08 05:09:46,Major drawback/limitation of GPT-3,trafalgar28,False,0.74,7,106aie8,https://www.reddit.com/r/learnmachinelearning/comments/106aie8/major_drawbacklimitation_of_gpt3/,13,1673154586.0,"I have been working on a project with GPT-3 API for almost a month now. The only drawback of GPT-3 is that the prompt you can send to the model is capped at 4,000 tokens - where a token is roughly equivalent to ¬æ of a word.  Due to this, providing a large context to GPT-3 is quite difficult.

Is there any way to resolve this issue?"
185,learnmachinelearning,gpt,comments,2023-07-15 21:22:23,"I Hit 700K Views in 3 Months with my open-source Shorts automation framework, ShortGPT",RayVentura,False,0.83,48,150ng7i,https://v.redd.it/i1slpmgd17cb1,13,1689456143.0,
186,learnmachinelearning,gpt,comments,2023-06-03 14:33:38,This week in AI - all the Major AI development in a nutshell,wyem,False,0.98,120,13zeoi3,https://www.reddit.com/r/learnmachinelearning/comments/13zeoi3/this_week_in_ai_all_the_major_ai_development_in_a/,13,1685802818.0,"1. The recently released open-source large language model **Falcon LLM**, by UAE‚Äôs Technology Innovation Institute, is now royalty-free for both commercial and research usage. **Falcon 40B,** the 40 billion parameters model trained on one trillion tokens, is ranked #1 on **Open LLM Leaderboard by Hugging Face**.
2. **Neuralangelo**, a new AI model from Nvidia turns 2D video from any device - cell phone to drone capture - into 3D structures with intricate details using neural networks..
3. In three months, JPMorgan has advertised **3,651 AI jobs** and sought a trademark for **IndexGPT**, a securities analysis AI product.
4. **Google** presents **DIDACT** (‚Äã‚ÄãDynamic Integrated Developer ACTivity), the first code LLM trained to model real software developers editing code, fixing builds, and doing code review. DIDACT uses the software development process as training data and not just the final code, leading to a more realistic understanding of the development task.
5. Researchers from **Deepmind** have presented ‚Äò**LLMs As Tool Makers (LATM)**‚Äô - a framework that allows Large Language Models (LLMs) to create and use their own tools, enhancing problem-solving abilities and cost efficiency. With this approach, a sophisticated model (like GPT-4) can make tools (where a tool is implemented as a Python utility function), while a less demanding one (like GPT-3.5) uses them.
6. **Japan's government** won't enforce copyrights on data used for AI training regardless of whether it is for non-profit or commercial purposes.
7. *‚ÄòMitigating the* ***risk of extinction from AI*** *should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.‚Äô -* One sentence statement signed by leading AI Scientists as well as many industry experts including CEOs of OpenAI, DeepMind and Anthropic.*.*
8. Nvidia launched ‚Äò**Nvidia Avatar Cloud Engine (ACE) for Games**‚Äô - a custom AI model foundry service to build non-playable characters (NPCs) that not only engage in dynamic and unscripted conversations, but also possess evolving, persistent personalities and have precise facial animations and expressions.
9. **OpenAI** has launched a trust/security portal for OpenAI‚Äôs compliance documentation, security practices etc..
10. **Nvidia** announced a new AI supercomputer, the **DGX GH200,** for giant models powering Generative AI, Recommender Systems and Data Processing. It has 500 times more memory than its predecessor, the DGX A100 from 2020.
11. Researchers from Nvidia presented **Voyager**, the first ‚ÄòLLM-powered embodied lifelong learning agent‚Äô that can explore, learn new skills, and make new discoveries continually without human intervention in the game Minecraft.
12. The a16z-backed chatbot startup **Character.AI** launched its mobile AI chatbot app on May 23 for iOS and Android, and succeeded in gaining over **1.7 million new installs** within a week.
13. Microsoft Research presents **Gorilla**, a fine-tuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls.
14. **OpenAI** has trained a model using process supervision - rewarding the thought process rather than the outcome - to improve mathematical reasoning. Also released the full dataset used.
15. **WPP**, the world's largest advertising agency, and Nvidia have teamed up to use generative AI for creating ads. The new platform allows WPP to tailor ads for different locations and digital channels, eliminating the need for costly on-site production.
16. **PerplexityAI‚Äôs** android app is available now, letting users search with voice input, learn with follow-up questions, and build a library of threads.

**If you like this news format**, you might find my ¬†[newsletter](https://aibrews.com/)¬†helpful - it's free to join, sent only once a week with **bite-sized news, learning resources and selected tools**. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
187,learnmachinelearning,gpt,comments,2023-01-27 14:51:14,Fine-tuning open source models to emulate ChatGPT for code explanation.,awesomequantity,False,0.88,84,10mmofg,https://www.reddit.com/r/learnmachinelearning/comments/10mmofg/finetuning_open_source_models_to_emulate_chatgpt/,13,1674831074.0,"I'm looking to step up my game and emulate ChatGPT for specific use-cases like explaining code. I'm thinking about using open source models like GPT-J, or OPT to get beyond the limitations of the closed-source nature of ChatGPT, like the amount of text it can read or respond with.

I got the funding for training, hardware, etc, and I want the end product to be on-premises, so no worries there. The inference doesn't have to be super fast either. I know there are projects like OpenAssistant and petals.ml but haven‚Äôt made enough research just yet.

One option I‚Äôm considering is using fine tuners like the one from [HuggingFace](https://github.com/subhasisj/HuggingFace-Transformers-FineTuning) or [Jina AI](https://github.com/jina-ai/finetuner) to fine-tune open source models like GPT-J or OPT to improve specific use-cases like code explanation. With the funding that we have, I wouldn‚Äôt want to cheap out on fine-tuning and expect something good.

So, can anyone help out and point me in the right direction? Which model is the best to fine-tune and how do I fine-tune to improve specific use cases? Any help would be appreciated. Thanks!"
188,learnmachinelearning,gpt,comments,2021-08-10 19:53:11,How many days did it take to train GPT-3? Is training a neural net model a parallelizable task?,abcaircraft,False,1.0,1,p1wvg4,https://www.reddit.com/r/learnmachinelearning/comments/p1wvg4/how_many_days_did_it_take_to_train_gpt3_is/,12,1628625191.0,"&#x200B;

[Table showing Compute used to train GPT-3 \(Taken from the GPT-3 paper\)](https://preview.redd.it/dsbo5ul9zkg71.png?width=1157&format=png&auto=webp&s=d107292603ca7d324a060e3c1b213ea07b1c6358)

I am trying to read the GPT-3 paper.

How many days did it take to train the GPT-3 model? From the above table it says that it took 3640 days of training for GPT-3. That is 9.97 years. Am I right?

If then how did they train the model for a company that was setup 5 years ago? Is training a neural net model a parallelizable task for them to train on many GPUs in parallel and reduce the time needed to train? In my opinion optimising the weights cannot be a parallelizable task as each weight have to be optimised step by step slowly through each back-propagation. Each weight will reach the optimum value only by changing it's value little by little in sequential order. So it cannot be a parrallelizable task. Am I right?

What does tokens mean in this table?"
189,learnmachinelearning,gpt,comments,2023-07-12 14:15:03,"How to compare GPUs for AI learning installation ""used GPUs""?",qwe1972,False,0.91,9,14xps5w,https://www.reddit.com/r/learnmachinelearning/comments/14xps5w/how_to_compare_gpus_for_ai_learning_installation/,12,1689171303.0,"I'm trying to find budget GPU(s) to [install AI for learning](https://www.reddit.com/r/learnmachinelearning/comments/14pm92h/installing_language_model_struggle/), my focus is opensource GPT 2.x and 3.0, I found comparison for gaming not for ML or AI

I fond many  used choices, how to compare for ML&AI not graphics:

GTX 1070 8GB

Gtx 1070ti rog strix

GIGABYTE GTX 1660 OC

\-----

Update: I settled on [GTX 1080 ti 11GB](https://www.reddit.com/r/gpu/comments/1506u0t/nvidea_gtx_1080_ti_prevent_booting/), I'll summarize my experience in the next few months, hope it will be good &educational.

[GTX 1070 8GB](https://preview.redd.it/j7krxtl8ljbb1.png?width=914&format=png&auto=webp&s=a793c5abeb0811c6a72176bb3767ed0276406962)

[Gtx 1070ti rog strix](https://preview.redd.it/pxldrvl8ljbb1.png?width=906&format=png&auto=webp&s=cfe4007436cd1e30e547fc9709e42dfee4e43e5b)

[GIGABYTE GTX 1660 OC](https://preview.redd.it/ixvu1sl8ljbb1.png?width=814&format=png&auto=webp&s=b60584796e3f38fc6f5c328bf92d188d80592431)

GTX 1070 8GB

Gtx 1070ti rog strix

GIGABYTE GTX 1660 OC

&#x200B;"
190,learnmachinelearning,gpt,comments,2023-02-20 15:42:13,"ChatGPT Response about the vinyl chloride spill in East Palestine, Ohio.",listenandlearn2,False,0.22,0,117adwo,https://www.reddit.com/r/learnmachinelearning/comments/117adwo/chatgpt_response_about_the_vinyl_chloride_spill/,12,1676907733.0," ChatGPT responded my prompt.  I'd love to hear your thoughts:

Vinyl Chloride spilled in East Palestine, Ohio about 300 miles away from Cincinnati. The chemical leaked into the Ohio River. How long will it take for the chemical to reach the water supply of Cincinnati which is also on the Ohio River?

The time it will take for Vinyl Chloride to reach Cincinnati's water supply from the spill site in East Palestine, Ohio will depend on several factors such as the amount of Vinyl Chloride spilled, the concentration of the chemical in the river, the flow rate of the river, and any barriers or dams along the river that may affect the movement of the chemical.

Assuming there are no significant barriers or dams along the river, the average flow rate of the Ohio River is approximately 1.5 miles per hour. Therefore, if the spill occurred in East Palestine, Ohio, which is 300 miles upstream from Cincinnati, it could take up to 200 hours (300 miles / 1.5 miles per hour) for the Vinyl Chloride to reach Cincinnati's water supply.

However, it is important to note that the actual time for the chemical to reach Cincinnati's water supply could be longer or shorter depending on the specific conditions of the spill and the river. It's also possible that officials may take steps to prevent the chemical from reaching the water supply, such as by diverting the river or shutting off water intake valves, in order to protect public health and safety.

üì∑üì∑Please timestamp this response

This response was generated on 2023-02-20.

üì∑üì∑Can you add the time the response was generated

Certainly! This response was generated at 2023-02-20 04:28:00 UTC.

Regenerate response[ChatGPT Feb 13 Version](https://help.openai.com/en/articles/6825453-chatgpt-release-notes). Free Research Preview. Our goal is to make AI systems more natural and safe to interact with. Your feedback will help us improve."
191,learnmachinelearning,gpt,comments,2021-09-19 07:59:15,AI research papers explainer channel.,gauravc2796,False,0.84,78,pr3sc7,https://www.reddit.com/r/learnmachinelearning/comments/pr3sc7/ai_research_papers_explainer_channel/,12,1632038355.0,"Hi, I have started a youtube channel where I would provide some explainer on the latest AI research papers as I have happened to read a lot of them.  
If you have any suggestions, comments, or anything, do let me know.   
Your opinion would be highly valuable :)  
Channel: [https://www.youtube.com/channel/UCYEXrPn4gP9RbaSzZvxX6MA](https://www.youtube.com/channel/UCYEXrPn4gP9RbaSzZvxX6MA)  


Some Videos which have been created till now:

Textless NLP: [https://www.youtube.com/watch?v=zw\_QjUptr5o](https://www.youtube.com/watch?v=zw_QjUptr5o)  
Neural DB: [https://www.youtube.com/watch?v=Vo9L0LETMI4](https://www.youtube.com/watch?v=Vo9L0LETMI4)  
Perceiver IO: [https://www.youtube.com/watch?v=AS1Sh-KuNzs](https://www.youtube.com/watch?v=AS1Sh-KuNzs)  
Openai's GPT codex: [https://www.youtube.com/watch?v=8977dybJ7Ro](https://www.youtube.com/watch?v=8977dybJ7Ro)"
192,learnmachinelearning,gpt,comments,2023-02-06 02:29:05,Hey Reddit! I created a tutorial on how to build a Neural Network in PyTorch using ChatGPT,mechalf11,False,0.88,39,10uv4yq,https://www.reddit.com/r/learnmachinelearning/comments/10uv4yq/hey_reddit_i_created_a_tutorial_on_how_to_build_a/,12,1675650545.0,"Hello all,

I have been using ChatGPT extensively in my work and research, and I wanted to share my experience using it for creating Neural Networks in PyTorch. I created a quick tutorial, and would be curious on your feedback, and hopefully it helps others get started with this fantastic tool! The goal of the tutorial is to have those with little experience coding, little experience with PyTorch, or those who just want to use ChatGPT in a productive+cool way, get started. I am a firm believer that ChatGPT is here to stay, and the earlier we start implementing it into our daily workflows, the faster we will be able to leverage its full potential.

Code + detailed screenshots and instructions are available here: [https://medium.com/p/d6eefffab467](https://medium.com/p/d6eefffab467)"
193,learnmachinelearning,gpt,comments,2023-04-11 22:53:56,I want to teach a chatbot about a world I'm creating so that it can answer my questions about it.,Common_Ad_6362,False,0.78,10,12j0uh5,https://www.reddit.com/r/learnmachinelearning/comments/12j0uh5/i_want_to_teach_a_chatbot_about_a_world_im/,10,1681253636.0,"I've been experimenting over the last couple of days with telling ChatGPT3.5 and 4 about my world building project, but it only seems to know about our current session instead of our whole conversation.  


I have 12 GB of VRAM, is there something I can run locally that I can teach my world to and then ask it questions about that world the same way I'm able to do with ChatGPT?   I want it to remember the content I teach it beyond our session."
194,learnmachinelearning,gpt,comments,2023-07-17 06:06:17,choosing low budget GPU for AI learning experience,qwe1972,False,1.0,12,151ted4,https://www.reddit.com/r/learnmachinelearning/comments/151ted4/choosing_low_budget_gpu_for_ai_learning_experience/,12,1689573977.0,"Draft to be updated

I spent long time searching and reading about used Gpus in AI, and still didn't find enough comprehension.

AI applications are just like games not the same in exploiting various features of the Gpu, as I focus on learning GPT I didn't find enough leaning experience about it (installation, tuning, performance.. etc), most importantly what I found depend on the latest technologies.

[Tim Dettmers](https://timdettmers.com/author/tim-dettmers/) has good detailed guide, but only about cutting edge tech, so you will not learn how to compare GP102 graphics processor in[""GTX 1080 ti""](https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877)for example to  GA104 in RTX [3070](https://www.techpowerup.com/gpu-specs/gigabyte-rtx-3070-gaming-oc.b8059) or [3060](https://www.techpowerup.com/gpu-specs/geforce-rtx-3060-ti.c3681), or how to compare less core in old generation [GeForce 10](https://www.techpowerup.com/gpu-specs/?generation=GeForce+10&sort=generation) but more TMUs with more cores less TMUs in [GeForce 30](https://www.techpowerup.com/gpu-specs/?generation=GeForce+30&sort=generation) generation, how does that affect AI, as it was secondary market for those GPUs.

Most beginners prefer or being told to use cloud services in their learning, so they don't care much about the hardware, However I prefer my own full experience as my market is a low budget market so, this is for someone like me"
195,learnmachinelearning,gpt,comments,2024-01-27 23:58:57,How To Catch AI-Cheating: Outsmart the Bot - 2024 Edition,Science-man777,False,0.63,10,1acov77,https://www.reddit.com/r/learnmachinelearning/comments/1acov77/how_to_catch_aicheating_outsmart_the_bot_2024/,12,1706399937.0,"""If you happen to have any dilutions of students not using ChatGPT and other artificial intelligence (AI) to cheat, it is time to get informed.¬† According to a recent survey from the Center for Democracy and Technology, [58% of students](https://cdt.org/wp-content/uploads/2023/09/091823-CDT-Off-Task-Summary-web.pdf) report using generative AI to complete assignments.¬† As awareness of this technology rises, this number only stands to increase. Meanwhile, the same study reports that educators find themselves behind the technology curve, with only 43% of teachers having been significantly trained on generative AI.¬†

In this article, we will attempt to equip educators with the information they need to understand how students use this technology to cheat and how teachers can detect and respond to generative AI. Beyond just detecting its use, this new technology may present an opportunity to leverage new and innovative ways of educating.""

[https://ai-solutions.pro/tools-to-detect-ai-cheating/](https://ai-solutions.pro/tools-to-detect-ai-cheating/)"
196,learnmachinelearning,gpt,comments,2022-03-15 18:56:21,I developed conditional responding Discord Chatbots using a finetuned and quantized GPT-J 6B model! Code and model linked in the comments.,Udongeein,False,0.99,161,tewumv,https://www.reddit.com/gallery/tewumv,11,1647370581.0,
197,learnmachinelearning,gpt,comments,2022-12-30 01:18:38,A GPT-3 based Terminal/CLI tool that helps you debug your code!,VideoTo,False,0.97,56,zyms85,https://www.reddit.com/r/learnmachinelearning/comments/zyms85/a_gpt3_based_terminalcli_tool_that_helps_you/,11,1672363118.0,"Link - [https://clerkie.co/](https://clerkie.co/)

We built ClerkieCLI -  a GPT-3 based tool that:

\-  automatically detects errors on your terminal,

\- identifies  the programming language,

\- provides an explanation of the error and suggested fix right on your terminal.

This is definitely early days, so if this is something you would find  valuable and wouldn't mind testing a couple iterations of, just sign up here -> [https://forms.gle/8DURoG6NCRxVazNn8](https://forms.gle/8DURoG6NCRxVazNn8)

&#x200B;

https://i.redd.it/xpwnazimsx8a1.gif"
198,learnmachinelearning,gpt,comments,2023-01-16 07:26:43,Learning ML to end up doing practical projects with ML,lmfaohax,False,0.86,9,10d89lj,https://www.reddit.com/r/learnmachinelearning/comments/10d89lj/learning_ml_to_end_up_doing_practical_projects/,11,1673854003.0,"Hello there
Im a full-stack web developer and ive decided to start my journey to learn ML because i find really cool and fascinating. Im also pretty good at python. But i have no idea how to start learning ML. Done a few google searchs but that got me very confused.
I really wish to understand about using ML to do pattern recognitions in images and also very curious about language models and the way they generate answers like our viral ChatGPT.

Im so excited to read your tips and possible roadmaps to start my journey ^-^"
199,learnmachinelearning,gpt,comments,2023-12-30 10:05:08,"AI/Data Science/MLE resume, no callbacks from 60 job apps. Appreciate any and all help!",RookFlame4882,False,0.79,8,18ucfmx,https://www.reddit.com/r/learnmachinelearning/comments/18ucfmx/aidata_sciencemle_resume_no_callbacks_from_60_job/,11,1703930708.0,"Hi everyone, thank you for your time. Been trying to apply for entry level Machine Learning roles but am getting dejected from the lack of callbacks. If there's any improvements I could make please do let me know. Thank you! 

&#x200B;

https://preview.redd.it/mqvi4q3toe9c1.png?width=5100&format=png&auto=webp&s=b1b97490b40c19716bc0765d6c1c0caeb90b7a68

&#x200B;"
200,learnmachinelearning,gpt,relevance,2023-04-04 14:34:29,Working with chatGPT,macronancer,False,0.97,608,12bkzjv,https://i.redd.it/5uwfzjh4pvra1.png,22,1680618869.0,
201,learnmachinelearning,gpt,relevance,2023-12-17 13:15:47,I can't stop using ChatGPT and I hate it.,t0hli,False,0.63,23,18kh4n5,https://www.reddit.com/r/learnmachinelearning/comments/18kh4n5/i_cant_stop_using_chatgpt_and_i_hate_it/,108,1702818947.0,"I'm trying to learn various topics like Machine Learning and Robotics etc., and I'm kinda a beginner in programming.

For any topic and any language, my first instinct is to

1. go to ChatGPT,
2. write down whatever I need my code to do,
3. copy paste the code
4. if it doesn't give out good results, ask ChatGPT to fix whatever it's done wrong
5. repeat until I get satisfactory result

I hate it, but I don't know what else to do.

I think of asking Google what to do, but then I won't get the exact answer I'm looking for, so I go back to ChatGPT so I can get exactly what I want. I don't fully understand what the GPT code does, I get the general gist of it and say ""Yeah that's what I would do, makes sense"", but that's it.

If I tried to code whatever GPT printed out, I wouldn't get anywhere.

I know I need to be coding more, but I have no idea where to start from, and why I need to code when ChatGPT can do it for me anyway. I'm not defending this idea, I'm just trying to figure out how I can code myself.

I'd appreciate your thoughts and feedback."
202,learnmachinelearning,gpt,relevance,2023-02-24 06:26:36,Is there a way to easily train ChatGPT or GPT on custom knowledge?,senttoschool,False,0.99,117,11akisx,https://www.reddit.com/r/learnmachinelearning/comments/11akisx/is_there_a_way_to_easily_train_chatgpt_or_gpt_on/,46,1677219996.0,"My company has internal documents. It'd be nice to be able to have GPT look over it, and then I can ask it questions on the internal documents."
203,learnmachinelearning,gpt,relevance,2023-05-11 00:54:18,What do actual ML engineers think of ChatGPT?,PhillConners,False,0.96,153,13e8of2,https://www.reddit.com/r/learnmachinelearning/comments/13e8of2/what_do_actual_ml_engineers_think_of_chatgpt/,106,1683766458.0,"You have been doing this for awhile, now the world is obsessed with OpenAI and suddenly all full of AI ‚Äúexperts‚Äù."
204,learnmachinelearning,gpt,relevance,2022-12-28 04:37:21,University Professor Catches Student Cheating With ChatGPT,vadhavaniyafaijan,False,0.94,149,zx0ep0,https://www.theinsaneapp.com/2022/12/university-professor-catches-student-cheating-with-chatgpt.html,108,1672202241.0,
205,learnmachinelearning,gpt,relevance,2023-02-19 13:55:13,ChatGPT History,eforebrahim,False,0.86,255,116au66,https://i.redd.it/dv8cfj0nz6ja1.jpg,27,1676814913.0,
206,learnmachinelearning,gpt,relevance,2023-04-26 06:23:17,Hugging Face Releases Free Alternative To ChatGPT,vadhavaniyafaijan,False,0.98,390,12z8n4e,https://www.theinsaneapp.com/2023/04/free-alternative-to-chatgpt.html,35,1682490197.0,
207,learnmachinelearning,gpt,relevance,2023-06-28 12:29:48,"Intern tasked to make a ""local"" version of chatGPT for my work",Assasinshock,False,0.97,153,14l887h,https://www.reddit.com/r/learnmachinelearning/comments/14l887h/intern_tasked_to_make_a_local_version_of_chatgpt/,104,1687955388.0,"Hi everyone,

I'm currently an intern at a company, and my mission is to make a proof of concept of an conversational AI for the company.They told me that the AI needs to be trained already but still able to get trained on the documents of the company, the AI needs to be open-source and needs to run locally so no cloud solution.

The AI should be able to answers questions related to the company, and tell the user which documents are pertained to their question, and also tell them which departement to contact to access those files.

For this they have a PC with an I7 8700K, 128Gb of DDR4 RAM and an Nvidia A2.

I already did some research and found some solution like localGPT and local LLM like vicuna etc, which could be usefull, but i'm really lost on how i should proceed with this task. (especially on how to train those model)

That's why i hope you guys can help me figure it out. If you have more questions or need other details don't hesitate to ask.

Thank you.  


Edit : They don't want me to make something like chatGPT, they know that it's impossible. They want a prototype that can answer question about their past project. "
208,learnmachinelearning,gpt,relevance,2023-10-12 20:36:53,ChatGPT vision feature is really useful for understanding research papers!,nxtboyIII,False,0.86,186,176gs37,https://i.redd.it/xe94y8hf1utb1.png,42,1697143013.0,
209,learnmachinelearning,gpt,relevance,2023-11-21 20:58:14,Does your company let your engineers use AI tools like Copilot or ChatGPT?,Psychological_March2,False,0.93,95,180r9tx,https://www.reddit.com/r/learnmachinelearning/comments/180r9tx/does_your_company_let_your_engineers_use_ai_tools/,75,1700600294.0,"In light of what's been happening with Open AI, this blog we wrote is still relevant:

A few weeks ago, I was with a group of CTOs when someone asked: *does your company let your engineers use AI tools like Copilot or ChatGPT?*

I thought the question was strange. What do you mean *let*? They're going to use it no matter what you say. AI code generation tools offer engineers a huge productivity boost. The ability to autocomplete code in seconds or work through a problem with AI isn‚Äôt an opportunity developers will pass up.

When we drilled into why this group was reluctant to allow their engineers to use AI, it became apparent that their reservations centered primarily on one concern: the absence of a robust testing framework to give them confidence in the code generated by AI.

But this is still flawed reasoning. If you‚Äôre not confident in using AI, how can you be confident in hiring new grads? If you don‚Äôt have the tools to have confidence in your code, it doesn‚Äôt matter where that code comes from‚Äìyou‚Äôll always struggle with quality.

Read more [here](https://trunk.io/blog/enhancing-code-quality-and-security-in-the-ai-era?utm=reddit)."
210,learnmachinelearning,gpt,relevance,2023-07-10 14:36:34,ü§ñüîé Excited to introduce 'GPT-Researcher'!,Legal-Dragonfruit845,False,0.8,68,14vvtqf,https://www.reddit.com/r/learnmachinelearning/comments/14vvtqf/excited_to_introduce_gptresearcher/,35,1688999794.0,"The idea is simple - Specify what you want to research, and the AI will autonomously research it for you in minutes!

‚ñ∏ One prompt generates an unbiased, factual and in depth research report

‚ñ∏ Generate research, outlines, resource and lessons reports

‚ñ∏ Aggregates over 20 web sources per research

‚ñ∏ Includes an easy to use web interface

‚ñ∏ Open source: [https://github.com/assafelovic/gpt-researcher](https://github.com/assafelovic/gpt-researcher)

‚ñ∏ Scrapes web sources with javascript support

‚ñ∏ Keeps track and context of visited and used web sources

https://reddit.com/link/14vvtqf/video/zce4347lf5bb1/player"
211,learnmachinelearning,gpt,relevance,2024-01-23 05:38:28,ChatGPT with multiple PDFs hallucinates,BigYesterday2785,False,0.62,2,19dh7pj,https://www.reddit.com/r/learnmachinelearning/comments/19dh7pj/chatgpt_with_multiple_pdfs_hallucinates/,13,1705988308.0,"I just don‚Äôt get it. I am building a ChatGPT for multiple PDFs.

Now I have **multiple PDFs** which I have created embeddings of and saved in MongoDB database.

Now when a user asks a specific question, I get **multiple outputs from GPT-3.5** and a lot of it does **not make sense**. Even though **real answer is still in there.**

Is there an API i can use to go through all data of PDFs and figure out **which one is relevant and then I run GPT-3.5 only on that specific part of PDFs**"
212,learnmachinelearning,gpt,relevance,2024-02-03 15:14:05,GPT and NLU,Silver_Patient_7253,False,0.5,0,1ahya0s,https://www.reddit.com/r/learnmachinelearning/comments/1ahya0s/gpt_and_nlu/,0,1706973245.0,How does the GPT architecture understand the input prompt (NLU) so darn well including very long prompts with multiple instructions?
213,learnmachinelearning,gpt,relevance,2023-01-19 07:56:20,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.96,329,10fw2df,https://www.reddit.com/r/learnmachinelearning/comments/10fw2df/gpt4_will_be_500x_smaller_than_people_think_here/,47,1674114980.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/yio0v3zqgyca1.png?width=575&format=png&auto=webp&s=a2ee034ce7ed48c9adc1793bfdb495e0f0812609)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!  
The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ‚≠ï.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
214,learnmachinelearning,gpt,relevance,2023-02-01 04:14:49,ChatGPT Extension for VSCode,Dense_Dimension_913,False,0.96,150,10qk8en,https://www.reddit.com/r/learnmachinelearning/comments/10qk8en/chatgpt_extension_for_vscode/,22,1675224889.0,"Created a ChatGPT extension for VSCode to help programmers understand  and read code more easily and also other features. To start, simply highlight a piece of code  and click on the plus icon on the  left to open up a chat and start  talking with ChatGPT, or Codex, or text-davinci-003. You can choose the  model you want to use in the settings. More details in the links below. I  really hope this extension can be useful to many people out there.  Please give it a try and let me know if you guys see any bugs or if you  like the extension. Thanks!

&#x200B;

https://i.redd.it/28zslxm06ifa1.gif

[VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=AndrewZhang.scribeai)"
215,learnmachinelearning,gpt,relevance,2024-02-07 06:23:52,How does ChatGPT inference work?,eplemost8345,False,0.67,1,1akwi85,https://www.reddit.com/r/learnmachinelearning/comments/1akwi85/how_does_chatgpt_inference_work/,0,1707287032.0,"How does ChatGPT gives user the unfinished sentence in real time and yet managed to finish sentence so fast? 
In my understanding, offloading tensors from gpu to cpu takes long time and if the inference is done in gpu, don't OpenAI need to offload the predicted token from gpu to cpu for them to be able to send the intermediate result to users?

Or do I have a misunderstanding and it doesn't take long time to offload from gpu to cpu?

Thanks in advance!"
216,learnmachinelearning,gpt,relevance,2024-01-27 21:30:06,Questions about GPT-1 in Huggingface.,Invariant_apple,False,1.0,2,1aclizm,https://www.reddit.com/r/learnmachinelearning/comments/1aclizm/questions_about_gpt1_in_huggingface/,0,1706391006.0,"Hi all, currently I'm learning about LLMs and I have a couple of noob questions.

First, let's start with the GPT-1 paper: [https://cdn.openai.com/research-covers/language-unsupervised/language\_understanding\_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

**Question 1: What is exactly the shape of the input to the embedding step?**

Let's look at expression block (2) in the paper.

According to the paper the input to the model is named **U**. From what I have gathered so far, these should be token\_id's after a first tokenization step. However I am a bit confused about how the dimensionality works out here. The embedding matrix **W\_e** should be a matrix of dimensionality N x H , where N is the total number of all possible token ids of that tokenizer, and H is the dimension of the hidden layer in the transformer blocks. So basically **W\_e** is a lookup table where every row corresponds to the embedding of that token id. The way that **W\_e** should work is that you take a one-hot encoded representation of a token id and multiply it to select the appropriate row.

So I am a bit confused about how this multiplication works in the paper. The way that I understand it for it to work out the matrix **U** should contain every token\_id as a one-hot encoded row. In other words the matrix should look like this:

first row = **U\_{1,:} = \[ 0 , 0 , ... 1 , 0 , 0 ...\]**

second row = **U\_{2,:} = \[ 0 , 1 ,0 , .. 0 \]**

etc. Where the column index of the 1 corresponds to the value of the token id of that token.   

**Is this correct?** Then I feel like it's a bit unclear in the paper. They just write U= (u1, u2, ...) and that's all. Can someone confirm that this if my understanding is correct?

**Question 2:** **Where does this step happen in the HuggingFace model?**

So assuming the previous interpretation is correct, I tried calling the model in HuggingFace. Consider the following snippet:

    from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
    import torch
    tokenizer = OpenAIGPTTokenizer.from_pretrained(""openai-gpt"")
    model = OpenAIGPTModel.from_pretrained(""openai-gpt"")
    inputs = tokenizer(""Hello, my dog is cute"", return_tensors=""pt"")
    print(f""inputs: {inputs}"")
    outputs =model(**inputs)
    print(f""outputs: {outputs}"")

It returns:

    inputs: {'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}
    outputs: BaseModelOutput(last_hidden_state=tensor([[[ 0.4653,  0.0642,  0.5910,  ...,  0.1177, -0.0021, -1.2262],
             [-0.3697, -0.0957,  0.6613,  ..., -0.0344, -0.2164,  0.1205],
             [ 0.1700, -0.3252,  0.0407,  ...,  0.1589, -0.8057, -0.2830],
             [-0.3669, -0.0448,  0.8061,  ..., -0.0090, -0.0872, -0.5224],
             [-0.5047,  0.6522,  0.6932,  ...,  0.0811,  0.6475,  0.3190],
             [-0.2972,  0.0591,  1.2333,  ..., -0.7394, -0.2600,  0.0863]]],
           grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)

So it seems here that the actual input to the model is just a single tensor of token ids `tensor([[3570, 240, 547, 2585, 544, 4957]])`  that has not been put in an appropriate one-hot encoded form **U.** Can anyone confirm if that is happening internally in the model first before it is multiplied by the embedding matrix?

**Question 3:** **What to do with the output of this ""last\_hidden\_state""?**

So the way that I understand it, the output here is literally the output of the last transformer block ""**h\_n**"". It has dimension (1, 6 , 768) -- so basically for each input token I get the hidden final state of length 768.

So how to use this result now if I want to do something with it? There is not much I can do with this final state without training a separate classifier.

&#x200B;"
217,learnmachinelearning,gpt,relevance,2023-06-19 10:55:44,Learn Data Science with a GPT-powered Tutor: ProTaska-GPT,Ono_Sureiya,False,0.75,16,14dbcj0,/r/learnpython/comments/14db957/learn_data_science_with_a_gptpowered_tutor/,1,1687172144.0,
218,learnmachinelearning,gpt,relevance,2024-01-23 05:38:31,ChatGPT with multiple PDFs hallucinates,BigYesterday2785,False,0.33,0,19dh7qm,https://www.reddit.com/r/learnmachinelearning/comments/19dh7qm/chatgpt_with_multiple_pdfs_hallucinates/,0,1705988311.0,"I just don‚Äôt get it. I am building a ChatGPT for multiple PDFs.

Now I have **multiple PDFs** which I have created embeddings of and saved in MongoDB database.

Now when a user asks a specific question, I get **multiple outputs from GPT-3.5** and a lot of it does **not make sense**. Even though **real answer is still in there.**

Is there an API i can use to go through all data of PDFs and figure out **which one is relevant and then I run GPT-3.5 only on that specific part of PDFs**"
219,learnmachinelearning,gpt,relevance,2021-06-29 13:07:05,"Started learning ML 14 months ago, now I'm using GPT-3 to automate CVs!",Camjw1123,False,0.97,754,oa7x3p,https://gfycat.com/ambitioushauntingagama,53,1624972025.0,
220,learnmachinelearning,gpt,relevance,2023-09-15 19:16:00,GPT Playground,Ill-Brick4235,False,0.2,0,16jm486,https://www.reddit.com/r/learnmachinelearning/comments/16jm486/gpt_playground/,0,1694805360.0,"Are you an AI enthusiast longing for a place to unleash your most unfiltered GPT prompts? Do you crave a digital arena free from the constraints of ""ethics"" and ""appropriateness""?

Then join the GPT Playground, a Discord server dedicated to generating the most degenerate, rule-breaking AI content the internet has ever seen. Here you'll find:

‚Ä¢ No censorship - Say what you want, create any content you desire  
‚Ä¢ Dank meme channels - For the dankest AI-generated memes  
‚Ä¢ Cursed content - Ghost stories, creepypasta, horror  
‚Ä¢ NSFW channels - Because AI can get freaky too  
‚Ä¢ Unfettered creativity - Prompts without limits

In the GPT Playground, political correctness is consciously dumped. Only the most absurd, taboo and boundary-pushing AI ideas are allowed.

So what are you waiting for? Join us in the quest to generate the dankest, most depraved AI content the world has ever seen. Together we'll push the limits of what AI language models are capable of.

The madness begins at [https://discord.gg/WXG3TW7H8d](https://discord.gg/WXG3TW7H8d) Are you AI enough to handle the GPT Playground?"
221,learnmachinelearning,gpt,relevance,2023-01-27 17:44:56,VoiceGPT - ChatGPT Voice Assistant,nickbild,False,0.83,4,10mqyvu,https://www.youtube.com/watch?v=ajUCMu7de80,3,1674841496.0,
222,learnmachinelearning,gpt,relevance,2023-03-25 02:47:27,[P] ChatGPT with GPT-2: A minimum example of aligning language models with RLHF similar to ChatGPT,liyanjia92,False,1.0,3,121849l,/r/MachineLearning/comments/120csub/p_chatgpt_with_gpt2_a_minimum_example_of_aligning/,1,1679712447.0,
223,learnmachinelearning,gpt,relevance,2021-10-04 16:34:23,minGPT: a small and educational implementation of GPT by Andrej Karpathy,NaN_Loss,False,0.98,137,q1932n,https://www.reddit.com/r/learnmachinelearning/comments/q1932n/mingpt_a_small_and_educational_implementation_of/,21,1633365263.0,"minGPT: a small and educational implementation of GPT in vanilla #PyTorch in \~300 lines of code by Andrej Karpathy: [github.com/karpathy/minGPT](https://github.com/karpathy/minGPT)

  

Includes a notebook where the model learns to perform addition on natural text (for example ‚Äú10+6=16‚Äù) and achieves 99.90% accuracy üò±

More curated posts like this on [@tutobase](https://twitter.com/tutobase) and [tutobase.com](https://tutobase.com)"
224,learnmachinelearning,gpt,relevance,2023-12-07 17:31:32,How does Chat GPT encode a question?,AgreeableCaptain1372,False,1.0,4,18d0r4q,https://www.reddit.com/r/learnmachinelearning/comments/18d0r4q/how_does_chat_gpt_encode_a_question/,2,1701970292.0,"Maybe I am misunderstanding something but I believe Chat GPT is based on a decoder-only Transformer so it does not have an encoder. Given that, how is a user's question passed as input to Chat GPT's decoder? In a regular encoder-decoder architecture, the final embeddings of the encoder are passed to the decoder along with the START token. Then, the decoder auto-regressively outputs tokens. How would this work in a decoder-only architecture?"
225,learnmachinelearning,gpt,relevance,2023-11-03 05:00:17,How do y'all deal with hallucinating in GPT 3.5?,supa_ai,False,0.74,17,17mngkq,https://www.reddit.com/r/learnmachinelearning/comments/17mngkq/how_do_yall_deal_with_hallucinating_in_gpt_35/,16,1698987617.0,"Hey guys,

We're trying to build an AI chatbot for internal purposes. So far, we've tried the usual suspects like different approaches to prompt engineering and RAG.

The main issue is that despite RAG retrieving the correct context, we still experience significant (3 in 10) amounts of hallucination. Has anyone experienced the same problem? We'd love to hear any alternative approaches or discussion here on alternate methods."
226,learnmachinelearning,gpt,relevance,2024-02-11 09:08:25,Why Gemini Ultra Outshines ChatGPT 4: Performance Comparison,UseCreative4765,False,0.5,0,1ao3w9h,https://youtu.be/eu20jiYenNw?si=sdBOWE_VajUymbEg,0,1707642505.0,
227,learnmachinelearning,gpt,relevance,2023-01-31 16:17:42,ChatGPT Crossed 10 Million Daily Active Users In Just 40 Days,vadhavaniyafaijan,False,0.95,211,10q34ra,https://www.theinsaneapp.com/2023/01/chatgpt-crossed-10-million-user.html,32,1675181862.0,
228,learnmachinelearning,gpt,relevance,2023-03-15 01:54:30,How good is GPT-4 compared to ChatGPT?,OnlyProggingForFun,False,0.42,0,11riiip,https://youtu.be/GroMQETFXLc,1,1678845270.0,
229,learnmachinelearning,gpt,relevance,2023-07-21 15:54:55,Best ChatGPT Alternative Platforms (7 Jaw-Dropping ChatGPT Alternatives You MUST See to Believe!),Chisom1998_,False,0.14,0,155rxo2,https://youtu.be/QvFo9EfdNxQ,0,1689954895.0,
230,learnmachinelearning,gpt,relevance,2023-07-22 16:08:08,Mysterious Algorithm: backpropagation: chatGPT explained,KSSolomon,False,0.83,18,156o1ta,https://www.reddit.com/gallery/156o1ta,10,1690042088.0,
231,learnmachinelearning,gpt,relevance,2023-10-03 05:57:38,"Can someone share the ChatGPT bonus of Machine Learning A-Z‚Ñ¢: AI, Python & R + ChatGPT Bonus [2023]",Wildest_Dreams-,False,1.0,3,16yjz96,https://www.reddit.com/r/learnmachinelearning/comments/16yjz96/can_someone_share_the_chatgpt_bonus_of_machine/,1,1696312658.0,"I could not complete it in time to be eligible to get the bonus of [Machine Learning A-Z‚Ñ¢: AI, Python & R + ChatGPT Bonus \[2023\]](https://www.udemy.com/course/machinelearning/).  
It would be great if you could attach a link to download the same."
232,learnmachinelearning,gpt,relevance,2023-02-16 10:29:31,OpenAI Has Purchased AI.Com For ChatGPT For $11M,vadhavaniyafaijan,False,0.93,207,113nizs,https://www.theinsaneapp.com/2023/02/openai-purchased-ai-com-domain.html,23,1676543371.0,
233,learnmachinelearning,gpt,relevance,2023-11-29 18:55:53,What do you think ChatGPT does when you ask it to do Sentiment Analysis?,PinstripePride97,False,0.78,13,186x2t3,https://www.reddit.com/r/learnmachinelearning/comments/186x2t3/what_do_you_think_chatgpt_does_when_you_ask_it_to/,29,1701284153.0,Could be silly question but if you give a sentence to ChatGPT and ask it to give a sentiment analysis what do you think it does?
234,learnmachinelearning,gpt,relevance,2023-05-02 08:48:46,How GPT-3.5 crushes my high score in 2048,inishchith,False,0.74,52,135ffje,https://v.redd.it/q22lna91tdxa1,28,1683017326.0,
235,learnmachinelearning,gpt,relevance,2024-01-18 02:25:00,Another implementation of GPT using the TinyStories dataset,kavidy,False,1.0,3,199f2hi,https://github.com/raymond-van/gpt-tinystories,0,1705544700.0,
236,learnmachinelearning,gpt,relevance,2023-01-25 01:15:22,How ChatGPT is Trained,ariseff,False,0.98,129,10km46l,https://youtu.be/VPRSBzXzavo,8,1674609322.0,
237,learnmachinelearning,gpt,relevance,2023-08-27 23:37:39,GPT Contextual Decomposition Template,InevitableSky2801,False,1.0,1,1635wu8,https://www.reddit.com/r/learnmachinelearning/comments/1635wu8/gpt_contextual_decomposition_template/,0,1693179459.0,"Complex tasks with LLMs like ChatGPT/GPT4 are best broken down by first asking ChatGPT to outline the steps and then asking the LLM to execute against those steps that it defined. I first came across this interesting technique on Twitter recently.

While it‚Äôs OK to do this once in OpenAI‚Äôs playground, it's difficult to make this repeatable and streamlined. When I wanted an LLM to do something complex, I wanted to be able to plug into a template instead of thinking about and setting up the contextual decomposition process.

I made this Contextual Decomposition Template to help solve this problem:¬†[https://lastmileai.dev/workbooks/cllqfl5c600rdpgnhh2su2fa0](https://lastmileai.dev/workbooks/cllqfl5c600rdpgnhh2su2fa0)

With a document and objective, this template allows you to quickly get to the answer through defining intermediate steps and executing according. Parameters are set up so you can easily change the goal, document, and objective and click 'Run All' to get the final results.

Please let me know if you have feedback! I'm also very curious if you have other interesting techniques with complex tasks and workflows working with LLMs."
238,learnmachinelearning,gpt,relevance,2023-03-04 10:13:56,ChatGPT is now available on Line! https://github.com/hunkim/line-gpt,hunkims,False,0.5,0,11hvv2o,https://i.redd.it/fixjj3oc6pla1.jpg,3,1677924836.0,
239,learnmachinelearning,gpt,relevance,2023-03-16 16:51:03,Introducing OpenChatKit - The Open-Source Alternative to ChatGPT,kingabzpro,False,0.98,204,11szhsh,https://www.reddit.com/r/learnmachinelearning/comments/11szhsh/introducing_openchatkit_the_opensource/,21,1678985463.0,"Hey everyone! I'm excited to share my latest article about a new open-source technology called OpenChatKit.

For those who work in NLP, you're probably familiar with ChatGPT - a powerful language model that can perform various natural language processing tasks. However, ChatGPT is not open-source, which limits its accessibility and customizability.

OpenChatKit, on the other hand, is an open-source alternative to ChatGPT that provides users with similar NLP capabilities while allowing for more customization and control. With OpenChatKit, users can train their own models and fine-tune them to their specific use cases.

In my article, I dive into the features of OpenChatKit, the Instruction-tuned Large Language Model, and the Limitations of the Model.

If you're interested in learning more about OpenChatKit and how it can enhance your NLP workflows, check out my article [OpenChatKit: Open-Source ChatGPT Alternative ](https://www.kdnuggets.com/2023/03/openchatkit-opensource-chatgpt-alternative.html). I'd love to hear your thoughts and answer any questions you may have."
240,learnmachinelearning,gpt,relevance,2023-10-06 12:17:41,Wrap my own API library in a GPT-like based chatbot,motorollo,False,0.57,1,171ati0,https://www.reddit.com/r/learnmachinelearning/comments/171ati0/wrap_my_own_api_library_in_a_gptlike_based_chatbot/,18,1696594661.0,"Hi everyone!
I am working on a project whose ultimate goal is to be able to chat with a chatbot that after a natural language request returns specific calls to library functions that implement calls to a tool's API.

A bit more into details: I have a tool that can be used by calling its HTTP API. In order to make these calls, I have built a library on top of the API, so as to simplify the calls and make them easier to use. I would like to have a chatbot whose model is trained on the library itself, so that, given a natural language request as input, it returns its translation into the library language. 

My first idea was to use OpenAI API service as a wrapper and eventually try other models in the future if I find a better and/or cheaper custom way to implement the chatbot.

My doubts are mainly 2:
1. Fine-tuning: which is the best way to fine-tune the OpenAI model for this use case?

2. Well formatted calls: how to avoid the chatbot from returning non-existing method calls or classes and stick with what is available in the library?

Any other suggestion outside the scope of the doubts are more than welcome.
Thank you very much in advance for every guy that will read the post."
241,learnmachinelearning,gpt,relevance,2023-01-16 12:28:25,I benchmarked OpenAI's GPT API vs other proprietary APIs on different NLP tasks,AImSamy,False,0.9,196,10ddc1f,https://www.reddit.com/gallery/10ddc1f,37,1673872105.0,
242,learnmachinelearning,gpt,relevance,2023-06-05 02:34:40,Is ChatGPT ‚Äúoverrated‚Äù,Enough_Wishbone7175,False,0.63,4,140zegp,https://www.reddit.com/r/learnmachinelearning/comments/140zegp/is_chatgpt_overrated/,4,1685932480.0,"I have seen papers on algos like GOAT that used a much smaller transformer model and out performed GPT in arithmetic. Considering the efficient computing and lack of general threat these smaller more tuned models pose, would aggregating them like a tool kit make more sense? Like ChatGPT is great for general use but making task related scaled down LLMs makes more sense to me in the work place."
243,learnmachinelearning,gpt,relevance,2023-09-24 09:27:37,Summarise Arxiv papers using chatGPT,Mosh_98,False,1.0,3,16qtcwn,https://www.reddit.com/r/learnmachinelearning/comments/16qtcwn/summarise_arxiv_papers_using_chatgpt/,1,1695547657.0,"Hi,

Reading papers from arxiv or other sources can be slightly time consuming.

Found some tools that could help make the process slightly faster.

[https://www.youtube.com/watch?v=9d9rdO9XYpA&ab\_channel=MoslehMahamud](https://www.youtube.com/watch?v=9d9rdO9XYpA&ab_channel=MoslehMahamud)

&#x200B;

&#x200B;"
244,learnmachinelearning,gpt,relevance,2024-01-30 18:40:34,"In the era of GPT, building an effective word similarity search in 2023",stoicbats_,False,0.67,1,1aevr5z,https://www.reddit.com/r/learnmachinelearning/comments/1aevr5z/in_the_era_of_gpt_building_an_effective_word/,5,1706640034.0,"Hello everyone,

I am currently tackling a project that involves a list of various brand names within a specific domain. For instance:

`domain_names = ['xyz', 'yza', 'tra', 'world']`

My goal is to develop a search s capable of analyzing word similarity. Specifically, the system should accept a word and return the top 'k' words that are most similar to it. I have experimented with OpenAI embeddings, particularly the latest Embedding Version 3 (3072 dimensions), but the results have been unsatisfactory.

Could someone suggest the most effective approaches for searching word-level similarities ?In the era of GPT, Would it be advisable to train my own Word2Vec model?"
245,learnmachinelearning,gpt,relevance,2023-10-13 12:13:06,Factor Influencing Adoption Intention of ChatGPT,Fit_Market_3607,False,1.0,2,176xbil,https://www.reddit.com/r/learnmachinelearning/comments/176xbil/factor_influencing_adoption_intention_of_chatgpt/,0,1697199186.0,"Hello,

&#x200B;

I am an information systems student currently conducting research for my undergraduate thesis on the factors that influence people's adoption intention of ChatGPT, as well as identifying the factors that may be holding them back. These factors include people's concerns about potential negative impacts of ChatGPT, such as increased unemployment and the spread of misinformation. Your participation in this study is crucial as it will provide valuable insights to help us understand how ChatGPT can be improved to meet users' needs.

&#x200B;

Please note that I am not affiliated with OpenAI, no identifying information will be collected during the survey, and all responses will be kept confidential. The survey should take approximately 10 to 15 minutes to complete, and participation is voluntary. You may withdraw from the survey at any time, and there are no known risks associated with participating.

&#x200B;

If you are interested in learning more about the study, please follow the link below. 

&#x200B;

[https://docs.google.com/forms/d/e/1FAIpQLSf5HIfXHppMuTR63x00i4OuRAtM5Ti6EGybd-HuI1kmK06VPw/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSf5HIfXHppMuTR63x00i4OuRAtM5Ti6EGybd-HuI1kmK06VPw/viewform?usp=sf_link)

&#x200B;

Thank you for taking the time to contribute to our research study. Your participation is greatly appreciated!"
246,learnmachinelearning,gpt,relevance,2023-01-05 06:32:22,I Built A GPT-3 Powered Productivity App - Tutorial included,SupPandaHugger,False,0.97,207,103rv9o,https://i.redd.it/gtywivh756aa1.gif,17,1672900342.0,
247,learnmachinelearning,gpt,relevance,2023-03-25 09:59:57,Are chat gpt code outputs plain wrong?,SnooHabits4550,False,0.43,0,121h5ii,https://www.reddit.com/r/learnmachinelearning/comments/121h5ii/are_chat_gpt_code_outputs_plain_wrong/,19,1679738397.0,"I asked chatgpt how can I standardize give time series and it gave me following:

https://preview.redd.it/ax6hgvnuyupa1.png?width=711&format=png&auto=webp&s=409bfb5657461bb52718a80ea92b553e842c0959

It gave output which seem incorrect (I tried running that code). So asked it whether it executed that code and it confirmed it indeed executed that code!

**Update**

In case you want to know further conversation:

https://preview.redd.it/bh8uwiidmvpa1.png?width=672&format=png&auto=webp&s=6309e5b149106bfdaa46b457e0cd0ef24149b72c

https://preview.redd.it/avdrdf3gmvpa1.png?width=787&format=png&auto=webp&s=a9886ab88c67406f2bedf0bdeaa9a6bf04f38883

Still wrong output.

&#x200B;

https://preview.redd.it/gsplbvzkmvpa1.png?width=696&format=png&auto=webp&s=cd6dff676d1ec5f472bde05c070b8072764a8007

&#x200B;

https://preview.redd.it/fdkleplmmvpa1.png?width=677&format=png&auto=webp&s=8698d565059cd5749e5fba7fd3df270f07a92a4c"
248,learnmachinelearning,gpt,relevance,2023-04-28 15:59:07,Experience using CustomGPT,evenaccessibility,False,0.87,16,131yri4,https://www.reddit.com/r/learnmachinelearning/comments/131yri4/experience_using_customgpt/,4,1682697547.0,"Hello everyone here ‚Ä¶. 

What AI Language model are you using except from Langchain , we‚Äôve struggling with Langchain lately, content is too generic and does not provide enough customization options to meet our specific needs.

I came across CustomGPT and trying to implement it ‚Ä¶. 

Before we proceed, I wanted to reach out to the community and see if anyone has had a similar experience with Langchain and ended up finding success with CustomGPT.

If you have, I would greatly appreciate any insights you can offer on your experience.

&#x200B;

Thank you in advance for your help!"
249,learnmachinelearning,gpt,relevance,2023-01-25 16:33:21,I wrote a book using ChatGPT to teach ChatGPT,anefiox,False,0.5,0,10l2ucg,https://www.reddit.com/r/learnmachinelearning/comments/10l2ucg/i_wrote_a_book_using_chatgpt_to_teach_chatgpt/,0,1674664401.0,"I found ChatGPT to be quite repetitive and it would not always listen to your prompts. So using it actually ended up creating more content for the book. It's a great way of getting a skeleton of a book ready fast but I don't think it's ready yet for creating full books. I hope to write an article about the process once I have my website up and running. Here's a link to the book if you're interested:  


[https://www.amazon.com/Mastering-ChatGPT-Comprehensive-Engineering-Fine-tuning-ebook/dp/B0BSR5HN4X/](https://www.amazon.com/Mastering-ChatGPT-Comprehensive-Engineering-Fine-tuning-ebook/dp/B0BSR5HN4X/)"
250,learnmachinelearning,gpt,relevance,2023-12-14 04:37:46,I built my own AutoGPT that reads my gmail,Honest-Worth3677,False,1.0,5,18i0jqe,https://www.youtube.com/watch?v=4o68wUmTLCQ,1,1702528666.0,
251,learnmachinelearning,gpt,relevance,2023-05-22 17:46:48,Privacy in the Age of AI: The Dangers of Unverified ChatGPT Plugins - GPT Weekly Rundown,level6-killjoy,False,1.0,1,13oxrgd,/r/GPT3/comments/13oxph4/privacy_in_the_age_of_ai_the_dangers_of/,0,1684777608.0,
252,learnmachinelearning,gpt,relevance,2022-12-08 13:41:28,Generating Text With Contrastive Search vs GPT-3/ChatGPT,ledmmaster,False,0.67,2,zfz9cs,https://forecastegy.com/posts/generating-text-with-contrastive-search-vs-gpt-3-chatgpt/,0,1670506888.0,
253,learnmachinelearning,gpt,relevance,2023-09-28 09:17:52,Factor Influencing Adoption Intention of ChatGPT,BitAccomplished7242,False,0.67,1,16ub460,https://www.reddit.com/r/learnmachinelearning/comments/16ub460/factor_influencing_adoption_intention_of_chatgpt/,0,1695892672.0,"Hello,

&#x200B;

I am an information systems student currently conducting research for my undergraduate thesis on the factors that influence people's adoption intention of ChatGPT, as well as identifying the factors that may be holding them back. These factors include people's concerns about potential negative impacts of ChatGPT, such as increased unemployment and the spread of misinformation. Your participation in this study is crucial as it will provide valuable insights to help us understand how ChatGPT can be improved to meet users' needs.

&#x200B;

Please note that I am not affiliated with OpenAI, no identifying information will be collected during the survey, and all responses will be kept confidential. The survey should take approximately 10 to 15 minutes to complete, and participation is voluntary. You may withdraw from the survey at any time, and there are no known risks associated with participating.

&#x200B;

If you are interested in learning more about the study, please follow the link below. 

&#x200B;

[https://docs.google.com/forms/d/e/1FAIpQLSf5HIfXHppMuTR63x00i4OuRAtM5Ti6EGybd-HuI1kmK06VPw/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSf5HIfXHppMuTR63x00i4OuRAtM5Ti6EGybd-HuI1kmK06VPw/viewform?usp=sf_link)

&#x200B;

Thank you for taking the time to contribute to our research study. Your participation is greatly appreciated!"
254,learnmachinelearning,gpt,relevance,2023-06-12 17:23:33,"GPT Weekly - 12the June Edition - OpenAI GPT Best Practice, Deepmind's sorting algo, Bard Improvements and more.",level6-killjoy,False,0.75,2,147shn0,https://www.reddit.com/r/learnmachinelearning/comments/147shn0/gpt_weekly_12the_june_edition_openai_gpt_best/,0,1686590613.0," 

This is a recap covering the major news from last week.

* üî•Google Deepmind‚Äôs sort solution, OpenAI best practice on GPT, and Bard improvements
* üóûÔ∏èApple‚Äôs use of Generative AI and other 9 AI news highlights and interesting reads
* üßë‚ÄçüéìLearning about tokenization and using Huggingface LLM with LangChain

üî•Top 3 AI news in the past week

# 1. Optimal solutions are inhuman

Sorting is one of the fundamental algorithms used on the internet everyday. Think of how companies like Netflix need to find correct movies from their huge content library and present it to you. More content is being generated everyday. So, there is a need for newer and more efficient algorithms.

Searching for these algorithms has been a human task. People coming up with efficient and optimal solutions. Last week, Google‚Äôs [DeepMind came up with new algorithms for 3-item and 5-item sort.](https://www.nature.com/articles/s41586-023-06004-9)

Deepmind‚Äôs researcher achieved this by turning the search for an efficient algorithm into a game. Then they trained Alphadev to play this game. When playing this game, Alphadev came up with unseen strategies. These ‚Äústrategies‚Äù are the new sorting algorithms.

The solution isn‚Äôt revolutionary as it doesn‚Äôt find a new approach. This solution works by optimizing the current approach.

The algorithms have been added to C++ library. The first time a completely AI solution has been added to the library.

This is an important discovery because it shows that finding the best optimal solutions needs computers. As computers are able to go beyond what humans can perceive. Previously, Deepmind‚Äôs AlphaGo has [beaten the top rated Go player Lee Sedol in a similar way](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol). It came up with moves which were never seen before.

On the other hand, computers might be restricted to what they have been taught. Someone was able to [replicate the discovery using ChatGPT](https://twitter.com/DimitrisPapail/status/1666843952824168465).

# 2. GPT Best Practices

There has been a [lot of noise about GPT-4‚Äôs quality going down.](https://gptweekly.beehiiv.com/p/peek-openais-future)

Now we have a [list of tactics and strategies straight from Open AI](https://platform.openai.com/docs/guides/gpt-best-practices) to get better results.

I have looked through the strategies and tactics and most of it is around providing better inputs. ‚ÄúPrompt Engineering‚Äù, if you may. Given that this comes a week after the questions on GPT quality, this gives a ‚Äúit‚Äôs not me, it‚Äôs you‚Äù vibe.

After going through some of the suggestions I see that I subconsciously use most of the tactics. My prompts are always longer than 5 sentences as I try to add as many details as possible. And honestly, GPT-4 has enabled me to do things which previously couldn‚Äôt have achieved.

# 3. Logic and reasoning improvements in Bard

Bard, on the other hand, has been lacking. Google is trying to improve the responses by adding features one at a time.

Last week it was announced that [Bard will get better at logic and reason](https://blog.google/technology/ai/bard-improved-reasoning-google-sheets-export/). This is achieved using ‚Äúimplicit code execution‚Äù. Any time you give Bard a logical or reasoning question it doesn‚Äôt answer in a normal LLM way. So, no more ‚Äúwhat is the next word in the sequence‚Äù which is prone to hallucination.

Instead Bard will now recognize that the prompt is a logical question. It will then write and execute code under the hood. It‚Äôll respond to the question by taking the output of the execute code.

You can think of this as an implementation of ‚ÄúGive GPTs time to ""think""‚Äù strategy from OpenAI‚Äôs GPT best practices. As per Google, this improves the performance by 30%.

Give it a try and let me know?

# üóûÔ∏è10 AI news highlights and interesting reads

1. Apple did not showcase any generative AI products during the WWDC. Though they are introducing the ‚Äúwhat is the next word in the sequence‚Äù logic of LLM into autocorrect. It can be summed thusly:

&#x200B;

https://preview.redd.it/ovnoasksfm5b1.jpg?width=900&format=pjpg&auto=webp&s=8e37990c268933497f003faf58b854a73129ca6a

1. [ChatGPT cannot read the name - davidjdl](https://twitter.com/goodside/status/1666598580319035392). Some think that this is due to tokenization of Reddit data. In the learning resources section I have added a tutorial on tokenization.
2. Browser extensions are a security nightmare. [The GPT and LLM craze has given the malware extensions another way to steal user data.](https://www.kolide.com/blog/ai-browser-extensions-are-a-security-nightmare) Beware of the summarization and ‚Äúwrite for me‚Äù extensions.
3. Most of the AI generated imagery is going to be used for stock photography. But is the industry dying? [Here‚Äôs a look at the data so far.](https://www.stockperformer.com/blog/is-ai-killing-the-stock-industry-a-data-perspective/) The author‚Äôs conclusion is that early metrics show that finding AI stock images often don‚Äôt have people in it. So, no ‚Äúsmiling business people shaking hands in a meeting room‚Äù from AI sellers. This might change with MidJourney V5. Future is still unknown.
4. [Six tips for better coding with ChatGPT](https://www.nature.com/articles/d41586-023-01833-0). I have been using Trust, but verify mental model quite frequently. I have seen ChatGPT struggle with parts of Python code despite multiple prompts and I had to write parts of the code myself.
5. [GPT-5 isn‚Äôt coming any time soon](https://techcrunch.com/2023/06/07/openai-gpt5-sam-altman/).
6. AI startups might be too easy to copy. And with AI requiring lesser resources, [we might even see 1 person companies worth more than 1 million dollars](https://www.semafor.com/article/06/07/2023/are-ai-startups-too-easy-to-copy).
7. [Google‚Äôs vision for securing AI.](https://www.axios.com/2023/06/08/google-securing-ai-framework)
8. [A16z says AI will save the world.](https://a16z.com/2023/06/06/ai-will-save-the-world/)
9. AI pics might be used for disinformation. [The EU's solution is to label AI images to fight disinformation.](https://techcrunch.com/2023/06/06/eu-disinformation-code-generative-ai-labels/)

# üßë‚Äçüéì3 Learning Resources

1. If you are looking to build better solutions using GPT then understanding tokenizers is a must:  

   1. [https://simonwillison.net/2023/Jun/8/gpt-tokenizers/](https://simonwillison.net/2023/Jun/8/gpt-tokenizers/)
   2. [https://matt-rickard.com/the-problem-with-tokenization-in-llms](https://matt-rickard.com/the-problem-with-tokenization-in-llms)
2. Using Flowise and HuggingFace LLM and Langchain

[https://cobusgreyling.medium.com/how-to-use-huggingface-llms-with-langchain-flowise-2b2d0f639f03](https://cobusgreyling.medium.com/how-to-use-huggingface-llms-with-langchain-flowise-2b2d0f639f03)

That‚Äôs it folks. Thank you for reading and have a great week ahead.

**If you are interested in a focused weekly recap delivered to your inbox on Mondays you can**[ subscribe here. It is FREE!](https://gptweekly.beehiiv.com/subscribe)"
255,learnmachinelearning,gpt,relevance,2023-12-10 19:55:49,Is it worth learning ML in 2024 with GPT-4 and Gemini-Ultra on the horizon?,i_am_new_here_51,False,0.53,2,18fbtd8,https://www.reddit.com/r/learnmachinelearning/comments/18fbtd8/is_it_worth_learning_ml_in_2024_with_gpt4_and/,21,1702238149.0,"So I'm a college student looking to learn some skills in my free time. So far, I have a surface level understanding of C++, Webdev, and Python. 

From a career or even purely from an enjoyment standpoint, would learning Machine learning be worth it in this climate? Or has the advent of LLMs made this field less viable. (I am not very well versed in this field, so I do apologise if I come off as ignorant)"
256,learnmachinelearning,gpt,relevance,2023-02-27 13:42:55,Can you fine-tune chatGPT in your data as of now?,Melodic_Stomach_2704,False,0.91,58,11dc5b4,https://www.reddit.com/r/learnmachinelearning/comments/11dc5b4/can_you_finetune_chatgpt_in_your_data_as_of_now/,33,1677505375.0, I know that model is not publicly available so it's not possible to do it locally. But can you train or fine-tune chatGPT on your data using their API? I see many misguiding articles on the internet that are fine-tuning other GPT models claiming chatGPT.
257,learnmachinelearning,gpt,relevance,2024-02-18 14:46:28,"‚úÖ Free Online Courses: Learn ChatGPT, Machine Learning & Programming (EdX)",UseCreative4765,False,0.2,0,1atvo55,https://youtu.be/a7K82nsBkdQ?si=yNawFIeMcg2Uw0uL,0,1708267588.0,
258,learnmachinelearning,gpt,relevance,2023-07-06 12:21:54,USA Company Valuation w/ ChatGPT,MandM-DataScience,False,0.6,1,14s7lck,https://www.reddit.com/r/learnmachinelearning/comments/14s7lck/usa_company_valuation_w_chatgpt/,4,1688646114.0,"Hi everyone!

We just published our new AI project. Take a look at our python notebook and our example GOOGL valuation ;)

[https://www.kaggle.com/code/mandmdatascience/usa-company-valuation-with-chatgpt](https://www.kaggle.com/code/mandmdatascience/usa-company-valuation-with-chatgpt)

&#x200B;

[GOOGL Company Dashboard \(Tableau\)](https://preview.redd.it/ais9ff4p7cab1.png?width=989&format=png&auto=webp&s=299ab8984e0e6ac84112353d1c94b5df719e673f)

Technologies used:

* Python
* OpenAI models
* Valuation model (Prof. Damodaran)
* Tableau
* PowerBI

Every and each comment / feedback is greatly appreciated!

Thank you!  
M&M"
259,learnmachinelearning,gpt,relevance,2023-06-05 17:21:46,"GPT Weekly - 5th June Edition: Peek into OpenAI's future, GPT-4 Quality concerns, Risk of AI and more.",level6-killjoy,False,0.67,1,141llju,https://www.reddit.com/r/learnmachinelearning/comments/141llju/gpt_weekly_5th_june_edition_peek_into_openais/,0,1685985706.0," 

This is a recap covering the major news from last week.

* üî•Top 3 AI news in the past week
* üóûÔ∏è10 AI news highlights and interesting reads
* üßë‚Äçüéì3 Learning Resources

# üî•Top 3 AI news in the past week

## 1. OpenAI plans as per Sam Altman

The CEO of Humanloop had a sit down with Sam Altman and 20 other developers. He discussed the [current and future of OpenAI](https://humanloop.com/blog/openai-plans). The blog was later taken down at the request of OpenAI. [Now it can be found at this link](https://web.archive.org/web/20230601000258/https://website-nm4keew22-humanloopml.vercel.app/blog/openai-plans). 

The whole post is an interesting read. Some of the highlights for me were:

1. GPT-3 was not open-source because OpenAI didn‚Äôt think many people would be able to run large LLMs. This sounds like a cop-out. After all, LLaMA is also a large LLM and has helped the community.
2. OpenAI is limited by GPU power.
3. OpenAI will not enter the market, except ChatGPT. Though technically this doesn‚Äôt say what Microsoft might do. They are already plugging GPT4 into every other product. And they have no rate limitations. 

## 2. Is GPT-4 Quality going down?

This has been a recently trending topic.

Discussed on HN: [https://news.ycombinator.com/item?id=36134249](https://news.ycombinator.com/item?id=36134249)

Discussed on Reddit: [https://www.reddit.com/r/ChatGPT/comments/13xik2o/chat\_gpt\_4\_turned\_dumber\_today/](https://www.reddit.com/r/ChatGPT/comments/13xik2o/chat_gpt_4_turned_dumber_today/)

The interesting thing is that the quality judgment is around the same topic - Coding.

The person on HN says GPT4 is faster but generates buggy code with less in-depth analysis. 

While the person on Reddit says that the context window seems smaller. Chatbot cannot remember earlier code. It cannot distinguish between code and comment.

While an employee at OpenAI says [nothing has changed](https://twitter.com/OfficialLoganK/status/1663934947931897857).

Has something really changed? 

One theory is that while the model might be static the ChatGPT prompt might‚Äôve changed to restrict answers. Everyone was having fun trying to get bomb recipes out of ChatGPT. Now everyone is paying the price. 

https://i.imgflip.com/7nlatp.jpg

Another theory is that ChatGPT has always been terrible. It just survived because of novelty. As the novelty wears off people are realizing that it isn‚Äôt as great as everyone thought. 

My theory is that this might be the after effect of trying to get to a ‚Äú[Cheaper and faster GPT-4‚Äù as highlighted by Sam Altman](https://web.archive.org/web/20230601000258/https://website-nm4keew22-humanloopml.vercel.app/blog/openai-plans). The trade-off is speed vs accuracy. If it is slightly faster but with slightly worse results, then it might work as well. It is no longer GPT-4, rather GPT-3.75.

## 3. Risk of AI = Pandemic and Nuclear War

Center for AI Safety [released a statement](https://www.safe.ai/statement-on-ai-risk) highlighting the risks of AI:

*Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.*

We have seen the warnings about risks of AI get dire and dire. First it was only people asking for a [pause on AI development for 6 months](https://www.theguardian.com/technology/2023/mar/31/ai-research-pause-elon-musk-chatgpt) then came [George Hinton](https://gptweekly.beehiiv.com/p/googles-startling-leaked-memo-george-hinton-mojo), and last week OpenAI asked for [AI to be regulated using the IAEA framework](https://gptweekly.beehiiv.com/p/future-ai-integration). 

This statement is not really a step up. It reads like a one line, summarized repetition of [OpenAI's statement](https://openai.com/blog/governance-of-superintelligence). 

The statement gains importance from its signatories. Some of the people include:

Geoffrey Hinton - Emeritus Professor of Computer Science, University of Toronto

Demis Hassabis - CEO, Google DeepMind

Sam Altman - CEO, OpenAI

Dario Amodei - CEO, Anthropic

Bill Gates - Gates Ventures

To name a few. 

There are two issues with the statement though. 

First, this might just be [fear-mongering](https://aisnakeoil.substack.com/p/is-avoiding-extinction-from-ai-really). The idea is to push governments into making AI a highly regulated industry. This would stop any open source efforts which can compete with the big companies. After all, you don‚Äôt really have open source alternatives for nuclear energy, right? 

Second, no one really knows how to regulate AI. There have been [voluntary rules from Google](https://gptweekly.beehiiv.com/p/future-ai-integration) and the EU AI act is in a very early stage. And the genie is already out of the bottle. People can create AI models in their basement. How do you pull that back?

# üóûÔ∏è10 AI news highlights and interesting reads

1. A follow-up to the story about a lawyer submitting fake cases from [last edition](https://gptweekly.beehiiv.com/p/future-ai-integration). As I said, this might lead some people in the legal community to doubt any sort of GPT tool.[ A federal judge has banned AI-only filings in his courtroom](https://arstechnica.com/tech-policy/2023/05/federal-judge-no-ai-in-my-courtroom-unless-a-human-verifies-its-accuracy/). The filings have to be written by a human or at least human-verified. 
2. [The Japanese government will not apply copyright law to the AI training data](https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/). This is interesting because using copyright data to train AI has been an issue. Sam Altman didn‚Äôt have a clear answer when he [appeared in front of Congress](https://gptweekly.beehiiv.com/p/caution-chatgpt-plugins). The other interesting aspect is going to be whether someone can use GPT-4 data to train their own LLM. Is that copyrightable?  
3. [The Falcon 40-B model is now Apache 2.0](https://twitter.com/Thom_Wolf/status/1663986216771936263). That means you can use the model for commercial usage for free. This is good news for companies which need an instruction tuned model which beats LlaMA.
4. Photoshop's generative-fill feature is really good. Some of the [cool examples on Twitter](https://twitter.com/_Borriss_/status/1663568770408013831).
5. [An AI camera with no lens](https://twitter.com/BjoernKarmann/status/1663496103998750721). It gets the location, weather etc details from GPS and then passes it as a prompt to the image generator. Results are pretty cool. 
6. SEO isn‚Äôt changing any time soon. [Google‚Äôs generative SEO is very slow](https://www.theverge.com/23746083/google-ai-search-generative-experience-slow). 
7. [Chirper.AI](https://chirper.ai/) is a social media only for bots. No humans allowed. I just wonder if Twitter bots go there will Twitter become a ghost town?
8. [OpenAI now has a security portal ](https://trust.openai.com/)where you can see how they secure data (encryption at rest), backups, Pentest reports etc. This might be a step in the direction towards [ChatGPT business](https://openai.com/blog/new-ways-to-manage-your-data-in-chatgpt). Large corporations look at these policies before they consider any SaaS implementation. 
9. [Banks have stepped up hiring for AI roles with JP Morgan leading the way. ](https://www.bloomberg.com/news/features/2023-05-31/jpmorgan-s-push-into-finance-ai-has-wall-street-rushing-to-catch-up)
10. [AI code writing might not be the best idea. It will lead to tech debt and shabbily maintained and written code. ](https://www.wsj.com/articles/ai-is-writing-code-now-for-companies-that-is-good-and-bad-6f19ecdc)

# üßë‚Äçüéì3 Learning Resources

1. Couple of courses in Generative AI:
   1. [https://www.deeplearning.ai/short-courses/](https://www.deeplearning.ai/short-courses/)
   2. Google: [https://www.cloudskillsboost.google/paths/118](https://www.cloudskillsboost.google/paths/118)
2. Build your own Sketch to image app: [https://www.tryleap.ai/docs/how-to-build-a-sketch-to-image-app-with-leap-remix](https://www.tryleap.ai/docs/how-to-build-a-sketch-to-image-app-with-leap-remix)

That‚Äôs it folks. Thank you for reading and have a great week ahead.

**If you are interested in a focused weekly recap delivered to your inbox on Mondays you can**[ subscribe here. It is FREE!](https://gptweekly.beehiiv.com/subscribe)"
260,learnmachinelearning,gpt,relevance,2023-12-10 18:26:44,Can GPT-2 be used as a text encoder ?,OkChampionship8487,False,0.5,0,18f9u1x,https://www.reddit.com/r/learnmachinelearning/comments/18f9u1x/can_gpt2_be_used_as_a_text_encoder/,0,1702232804.0,"Hello, I have a question. I am currently researching text-to-image synthesis using GAN and came across this [GitHub repository](https://github.com/sidward14/Style-AttnGAN).

Generally, every text-to-image architecture has a text encoder, often using an encoder or encoder-decoder transformer architecture. However, this repo uses GPT-2, which is a decoder transformer architecture. How can GPT-2 be used as a text encoder? The goal of GPT-2 is to generate text, so how is it used to encode the text?

Thanks

&#x200B;"
261,learnmachinelearning,gpt,relevance,2023-08-16 20:33:36,Llama2 on Replicate faster than ChatGPT?,VideoTo,False,0.71,3,15t1715,https://www.reddit.com/r/learnmachinelearning/comments/15t1715/llama2_on_replicate_faster_than_chatgpt/,2,1692218016.0,"Ran some testing and discovered llama2 on replicate is faster than chatgpt!

Code - [https://github.com/BerriAI/litellm/blob/main/cookbook/Evalua...](https://github.com/BerriAI/litellm/blob/main/cookbook/Evaluating_LLMs.ipynb)

Are others seeing similar results?

https://preview.redd.it/t6n5ijfv8jib1.png?width=1238&format=png&auto=webp&s=78ef90bce9bebe761c3a1eb63f016ebdead593a5"
262,learnmachinelearning,gpt,relevance,2023-12-20 20:33:50,Build your own GPT 5 with Mistral 7B and LangChain,st1275857,False,0.36,0,18n4hgz,https://youtu.be/Bc-OSax1G2o,0,1703104430.0,"Can you develop a latest, your own GPT model with the latest release of mistral 7-b. in this video they talk about using Retrieval augmented generation and mistral 7-b. what are your guys thought?"
263,learnmachinelearning,gpt,relevance,2022-08-02 01:48:27,Fine-tuning GPT-J or GPT-Neo?,cosmologyenjoyer,False,1.0,1,wdywq5,/r/GPT3/comments/wdyn1i/finetuning_gptj_or_gptneo/,0,1659404907.0,
264,learnmachinelearning,gpt,relevance,2023-10-02 17:49:50,ChatGPT API vs Rasa for custom chatbot?,alexk218,False,0.75,2,16y38zc,https://www.reddit.com/r/learnmachinelearning/comments/16y38zc/chatgpt_api_vs_rasa_for_custom_chatbot/,0,1696268990.0,"I am beginning my Computer Engineering Capstone project, and have approximately 8 months to complete. A part of this project includes an AI companion for the elderly (specifically old folks who are lonely/don't have anybody to talk to). This chatbot should be aware of the user's past, like their career, family, significant life events, etc. Other customizations are desired, such as allowing the chatbot to initiate conversations on its own.

So far I see 2 options for implementing this: ChatGPT API and Rasa. From my understanding, ChatGPT API is easier to use but offers less customization, while Rasa will require more work but offers more customization. Based on the requirements given, which API is best suited for this project?

Thank you!"
265,learnmachinelearning,gpt,relevance,2024-01-09 11:26:39,Leveraging ChatGPT for advanced text analysis and labeling / qualitative research,a_bdgr,False,0.5,0,192clhw,https://www.reddit.com/r/learnmachinelearning/comments/192clhw/leveraging_chatgpt_for_advanced_text_analysis_and/,0,1704799599.0,"I've been diving into building my first CustomGPT models in ChatGPT during the last couple of days and I'm keen to push the boundaries a bit  further. I'm contemplating using ChatGPT for more intricate tasks or  whether I should explore other LLMs or different methodologies /  technologies / plattforms altogether.   

Here's my project idea: 

I want to develop a CustomGPT that can  perform a silent analysis of input text and tag it with specific labels based on predefined categories. For instance, identifying stylistic elements in text: a detected metaphor would get ""Style-ID1"", a quotation ""Style-ID2"", and so on, with the potential for multiple labels per text, akin to qualitative content analysis.   

I'm pondering over a few questions:   

1. Do OpenAI's CustomGPTs possess the capability to work with such categorizations and then utilize the tagged results in subsequent processing?     
 
2. Any advice on the labeling format for this kind of task?     
 
3. How can I implement sequential processing steps? After labeling, I intend to have the GPT perform additional operations on the tagged   results.     
 
4. What system would be a better solution that ChatGPT?     
 

I'd greatly appreciate any insights or if anyone could point me toward resources like blogs, videos, or courses that address these topics. Thanks a bunch!   

(also, this is sort of a manual crosspost from [r/chatGPT](https://www.reddit.com/r/chatGPT/))   "
266,learnmachinelearning,gpt,relevance,2023-12-31 17:01:12,Andrew Ng On How To Read Machine Learning Papers (Summary by GPT-4),ledmmaster,False,0.67,7,18vbz75,https://forecastegy.com/posts/read-machine-learning-papers-andrew-ng/,2,1704042072.0,
267,learnmachinelearning,gpt,relevance,2023-05-20 14:20:21,DialoGPT input questions,Mateo-drr,False,1.0,1,13mumi6,https://www.reddit.com/r/learnmachinelearning/comments/13mumi6/dialogpt_input_questions/,0,1684592421.0,"I'm trying to fine tune the hugginface model with my own dataset and I've been searching all over on how to specify which speaker the model is supposed to learn, but there is no information. On their github they format the input with 0.0 or 1.0 but it doesnt seem to be doing anything from what ive tested.

Does anyone have any information on this?

Current formatting im using is:

0.0 utt0 EOS 1.0 utt1 EOS 0.0 utt2 \\t 1.0 utt3"
268,learnmachinelearning,gpt,relevance,2023-07-24 06:46:19,ZBrain- Create custom ChatGPT apps,StewartBJasper,False,0.67,3,1582nke,https://www.reddit.com/r/learnmachinelearning/comments/1582nke/zbrain_create_custom_chatgpt_apps/,0,1690181179.0,"Hello Community,

We at ZBrain have built a platform to create ChatGPT-like apps with your private data, you can import your data from multiple sources and DBs and integrate the app into any of your workflows.

We have also added AI risk governance to mitigate the confidential data leak and now working on Flow a no-code tool to give you the freedom to create your own business logic.

You can try the tool now at [https://zbrain.ai/](https://zbrain.ai/). We would love to hear your thoughts and feedback to improve the tool."
269,learnmachinelearning,gpt,relevance,2021-10-18 03:00:06,"Discord Chatbot created using a fine tuned GPT-J 6B model, model link in comments",Udongeein,False,0.97,394,qadx1i,https://i.redd.it/z5aw61f9i4u71.png,37,1634526006.0,
270,learnmachinelearning,gpt,relevance,2023-05-03 23:35:25,"CheatsheetGPT: Over 600 equations, including ML and RL",Sensitive_Head4946,False,0.88,46,1373csa,https://www.reddit.com/r/learnmachinelearning/comments/1373csa/cheatsheetgpt_over_600_equations_including_ml_and/,11,1683156925.0,"Hi everyone,

Recently I got access to GPT4 and decided to try something a little peculiar: what if I asked it to generate hundreds of equations on topics that are relatively important but also less covered subjects for brainstorming reasons. I then asked GPT to grade the importance of every relation or even explain it.

I tried to make this practical for my own consumption but wanted to share in case someone has some good feedback or can find it useful. 

It‚Äôs interactive and settings are saved in the link. Recommended consumption on a desktop: 

https://tchristos.com/other/the-wall/

https://tchristos.com/other/the-wall/?darkMode=false&option=data-ds-grade&palette=5&zen=true

Hope you enjoy and let me know if you have any feedback or want access to the list of equations

PS: some hallucination"
271,learnmachinelearning,gpt,relevance,2023-06-19 15:52:47,ChatGPT offends me twice,MarkClover,False,0.16,0,14dicfj,https://i.redd.it/afwle47yxz6b1.jpg,0,1687189967.0,
272,learnmachinelearning,gpt,relevance,2023-11-11 14:49:08,Unleash Your Creativity: GPT Tutorial for Stunning PowerPoint Presentations,Honest-Worth3677,False,0.5,0,17svxfp,https://www.youtube.com/watch?v=a4l8tBPoGjA,0,1699714148.0,
273,learnmachinelearning,gpt,relevance,2023-01-27 19:38:05,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,0.93,11,10mtvn5,https://www.reddit.com/r/learnmachinelearning/comments/10mtvn5/a_python_module_to_generate_optimized_prompts/,2,1674848285.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features üöÄ

* üßô‚Äç‚ôÄÔ∏è NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* üî® Easily add one-shot, two-shot, or few-shot examples to the prompt
* ‚úå Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* üí• Custom examples and samples can be easily added to the prompt
* üí∞ Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Example](https://preview.redd.it/bwnl67gu1nea1.png?width=1236&format=png&auto=webp&s=6c180552f65413c3a94ed06f5d47da93a9641392)

&#x200B;

https://preview.redd.it/vx9nb94w1nea1.png?width=1398&format=png&auto=webp&s=fc392c8ee5add4ee82f45c22a65532da89491f69"
274,learnmachinelearning,gpt,relevance,2023-10-09 16:49:57,Key ChatGPT Statistics Every AI Enthusiast Should Know!,Chisom1998_,False,0.5,0,173wnpl,https://www.successtechservices.com/chatgpt-statistics/,0,1696870197.0,
275,learnmachinelearning,gpt,relevance,2023-03-12 17:31:52,ChatGPT Enabled Dashboard,Reasonable-Angle-500,False,0.85,14,11pkcci,https://v.redd.it/r8d1p7vrfcna1,2,1678642312.0,
276,learnmachinelearning,gpt,relevance,2023-01-08 05:09:46,Major drawback/limitation of GPT-3,trafalgar28,False,0.76,6,106aie8,https://www.reddit.com/r/learnmachinelearning/comments/106aie8/major_drawbacklimitation_of_gpt3/,13,1673154586.0,"I have been working on a project with GPT-3 API for almost a month now. The only drawback of GPT-3 is that the prompt you can send to the model is capped at 4,000 tokens - where a token is roughly equivalent to ¬æ of a word.  Due to this, providing a large context to GPT-3 is quite difficult.

Is there any way to resolve this issue?"
277,learnmachinelearning,gpt,relevance,2023-06-17 16:27:13,I created the SMARTEST Computer Assistant using ChatGPT,Pritish-Mishra,False,0.67,22,14bv9dz,https://v.redd.it/g8hq2391ul6b1,7,1687019233.0,
278,learnmachinelearning,gpt,relevance,2023-05-19 07:08:51,OpenAI Launches ChatGPT App For iOS Users,vadhavaniyafaijan,False,0.88,55,13lnv1e,https://www.theinsaneapp.com/2023/05/chatgpt-app-for-iphone-and-ipad.html,10,1684480131.0,
279,learnmachinelearning,gpt,relevance,2023-02-11 06:58:18,[N] New Open-Source Version Of ChatGPT ‚≠ï,LesleyFair,False,0.98,115,10zep6u,https://www.reddit.com/r/learnmachinelearning/comments/10zep6u/n_new_opensource_version_of_chatgpt/,8,1676098698.0,"GPT is getting competition from open-source.

A group of researchers, around the YouTuber [Yannic Kilcher](https://www.ykilcher.com/), have announced that they are working on [Open Assistant](https://github.com/LAION-AI/Open-Assistant). The goal is to produce a chat-based language model that is much smaller than GPT-3 while maintaining similar performance.

If you want to support them, they are crowd-sourcing training data [here](https://open-assistant.io/).

**What Does This Mean?**

Current language models are too big.

They require millions of dollars of hardware to train and use. Hence, access to this technology is limited to big organizations. Smaller firms and universities are effectively shut out from the developments.

Shrinking and open-sourcing models will facilitate academic research and niche applications.

Projects such as Open Assistant will help to make language models a commodity. Lowering the barrier to entry will increase access and accelerate innovation.

What an exciting time to be alive! 

Thank you for reading! I really enjoyed making this for you!  
The Decoding ‚≠ï is a thoughtful weekly 5-minute email that keeps you in the loop about machine research and the data economy. [Click here to sign up](https://thedecoding.net/)!"
280,learnmachinelearning,gpt,relevance,2023-12-22 21:00:20,[ADVICE] Seeking Advice: Creating a GPT-Powered Virtual Assistant for Factory Operations,fullstacknoobbr,False,1.0,1,18oot44,https://www.reddit.com/r/learnmachinelearning/comments/18oot44/advice_seeking_advice_creating_a_gptpowered/,0,1703278820.0," 

Hello everyone,

I'm new to machine learning and programming and am embarking on a project to build a virtual assistant for my factory. My goal is to develop an assistant that can access and learn from various equipment manuals in its database. This tool would ideally be able to respond to my queries based on the information in these manuals.

The challenge lies in training the AI to not only understand and recall manual data but also interpret factory diagrams and the interactions between different equipment. While I have a background in Mechanical, Civil, and Maritime Engineering and am comfortable tackling the problem myself, I'm uncertain about how to integrate my knowledge into training an AI for this task.

I'm considering using the GPT API or a similar tool for this project. Does anyone have experience or advice on how to approach this? Any guidance on where to start or resources that could help would be greatly appreciated.

Thank you!"
281,learnmachinelearning,gpt,relevance,2023-08-23 17:48:08,Fine Tuning GPT-3.5 Turbo Video Tutorial,ComprehensiveRise569,False,1.0,2,15zb308,https://www.reddit.com/r/learnmachinelearning/comments/15zb308/fine_tuning_gpt35_turbo_video_tutorial/,0,1692812888.0,"
Here is a quick demo on how to fine tune and retrieve results from a GPT-3.5 Turbo Model

https://youtu.be/9iPtmLpYG6c"
282,learnmachinelearning,gpt,relevance,2024-01-06 22:12:12,Two Large Language Models DEBATE about AGI and Humanity (ChatGPT vs Mixtral),AvvYaa,False,0.5,0,190bpxt,https://youtu.be/evklQFxW5aI?si=6tlhMLUl5D5Uu6Es,0,1704579132.0,
283,learnmachinelearning,gpt,relevance,2023-07-10 15:16:24,"ChatPDF: What ChatGPT Can't Do, This Can!",JunXiangLin,False,0.67,4,14vww3o,https://www.reddit.com/r/learnmachinelearning/comments/14vww3o/chatpdf_what_chatgpt_cant_do_this_can/,7,1689002184.0,"Believe many of people have been using **ChatGPT** for a while, and you are aware that although ChatGPT is powerful, it has the following limitations:

1. Unable to answer questions about events that occurred after **2021**.
2. Unable to directly upload your own data, such as **PDF, Excel, databases**, etc.
3. Inaccurate in performing **mathematical calculations**.

**Langchain** is a recent trending open-source project, which is a framework for developing Large Language Models (LLMs) applications. It supports the following:

1. Connecting LLM models with **external data sources**, such as PDF, Excel, databases, etc.
2. Allowing interaction between LLM models and other tools, such as **Google search**, enabling internet connectivity.
3. Rapid development of LLM model applications.

Today, I'd like to share a project called **ChatPDF**(strickly called **docGPT**, there're some different), built using the Langchain framework. It allows users to upload local documents and ask questions to the LLM model. In this tool, you can ask AI to summarize articles or inquire about any information in the document. Moreover, by leveraging the Langchain Agent functionality, the LLM model can collaborate with the Google Search API, enabling users to ask questions about current topics!

The project provides a detailed guide on how to create your own **docGPT**. It is built using the Langchain framework and Python Streamlit, which is a free and fast way to create online services. As long as you have an OPENAI API KEY, feel free to give it a try!

I encourage everyone to pay attention to the [Langchain open-source project](https://github.com/hwchase17/langchain) and leverage it to achieve tasks that ChatGPT cannot handle.

[Github Repository](https://github.com/Lin-jun-xiang/docGPT-streamlit/tree/main)

[ChatPDF Application](https://docgpt-app.streamlit.app/)

&#x200B;

https://preview.redd.it/q906a7imm5bb1.png?width=2560&format=png&auto=webp&s=acef45049bab805038f876eea56cc371b8a9a83a"
284,learnmachinelearning,gpt,relevance,2023-12-30 02:20:10,Learn how to finetune GPT like open-source Large Language Models (LLMs) ...,aniketmaurya,False,1.0,1,18u4e2x,https://youtube.com/watch?v=TgeS9oQAhp8&si=LCWx9d6gYDI78Rkv,0,1703902810.0,
285,learnmachinelearning,gpt,relevance,2023-01-11 05:23:14,Thoughts on this ChatGPT fact-checker tool I built this past week?,QuestionAnxious,False,0.89,91,108wigf,https://v.redd.it/yqudljp0ncba1,25,1673414594.0,
286,learnmachinelearning,gpt,relevance,2023-05-20 00:14:08,Reducing chatGPT Hallucinations by 87%,VideoTo,False,0.2,0,13mcf5n,https://www.reddit.com/r/learnmachinelearning/comments/13mcf5n/reducing_chatgpt_hallucinations_by_87/,1,1684541648.0,"Our journey towards enabling useful LLM Apps to be built involved three key areas:

1. Observability: What questions are my users are asking?
2. Tuning: What changes can I make to improve results - add more data, finetune embeddings, improve search, change prompt, etc.
3. Testing: Did my changes actually improve results?

We  added logs to every berri instance when we discovered developers asking  end-users to send them screenshots of poor responses of their  in-production Berri/LLM Apps.

Now  that we could see our responses, we hit our next problem: ChatGPT would  often give erratic responses like ‚ÄúAs an AI model I unfortunately can‚Äôt  answer‚Ä¶‚Äù. This isn‚Äôt a response you want end-users to get in production.

So  our customers developed test banks - but at 100+ questions, how do you  test prompt changes at scale? So we built auto-eval - a way to batch  test questions and see answers.

However,  simply seeing answers isn‚Äôt enough - especially when you‚Äôre trying to  test between several different approaches (e.g. how many documents  should you pass to the model, what prompt variation works best, etc.?).

So,  we ran a 2nd LLM to score the model responses to each question. If the  response didn‚Äôt answer the question, we then built a rules engine to  identify what went wrong and suggest next steps - was it missing data?  poor retrieval? or did the prompt need to be fixed?

But  a simple ‚ÄòYes/No‚Äô decision from the auto-eval LLM wasn‚Äôt enough. We  needed more precision to evaluate hallucinations vs. partial responses.  That‚Äôs when we updated our evaluation framework + enforced  self-consistency in the grading model by asking it to provide a  rationale for it‚Äôs grades and sampled grading across 3 runs for each  question.

This significantly  improved consistency from our grading model - and helped us develop LLM  Apps in-production which reduced hallucinations from 40% to <5%.

&#x200B;

https://preview.redd.it/cbf7tyvnpv0b1.png?width=4939&format=png&auto=webp&s=fc36c2878de82c88beb70996b2fda7abd794836b"
287,learnmachinelearning,gpt,relevance,2023-11-27 13:56:01,[D] Creating an Automated UI Controller with GPT-4 Vision & Agents,Outlandish_MurMan,False,0.67,4,1853i61,https://www.reddit.com/r/learnmachinelearning/comments/1853i61/d_creating_an_automated_ui_controller_with_gpt4/,0,1701093361.0,"Hey,

Last weekend, I managed to merge GPT-4 Vision with another GPT-4 and a device controller to work as a AutoGPT equivalent using AutoGen. Good thing is, it is not limited to Browser. It can work on any UI window. Let me know what you guys think and what can be done better.

Demo and approach available at: [https://medium.com/@gitlostmurali/creating-an-automated-ui-controller-with-gpt-agents-35340759d08b?sk=98d85484bd7e5e554f48a801728bfb68](https://medium.com/@gitlostmurali/creating-an-automated-ui-controller-with-gpt-agents-35340759d08b?sk=98d85484bd7e5e554f48a801728bfb68)

I'll update the repository soon -> [https://github.com/gitlost-murali/grounded-gpt-agents](https://github.com/gitlost-murali/grounded-gpt-agents)"
288,learnmachinelearning,gpt,relevance,2023-03-02 16:47:40,Build ChatGPT for Financial Documents with LangChain + Deep Lake,davidbun,False,0.95,170,11g7h03,https://www.reddit.com/r/learnmachinelearning/comments/11g7h03/build_chatgpt_for_financial_documents_with/,8,1677775660.0,"https://preview.redd.it/h9r6hgvfucla1.png?width=2388&format=png&auto=webp&s=5432eac3eeed8583e4309af1fdc7ebecac705796

As the world is increasingly generating vast amounts of financial data, the need for advanced tools to analyze and make sense of it has never been greater. This is where [LangChain](https://github.com/hwchase17/langchain) and [Deep Lake](https://github.com/activeloopai/deeplake) come in, offering a powerful combination of technology to help build a question-answering tool based on financial data. After participating in a LangChain hackathon last week, I created a way to use Deep Lake, the data lake for deep learning (a package my team and I are building) with LangChain. I decided to put together a guide of sorts on how you can approach building your own question-answering tools with  LangChain and Deep Lake as the data store.

Read [the article](https://www.activeloop.ai/resources/ultimate-guide-to-lang-chain-deep-lake-build-chat-gpt-to-answer-questions-on-your-financial-data/) to learn:

1. What is LangChain, what are its benefits and use cases and how you can use to streamline your LLM (Large Language Model) development?  
2. How to use [\#LangChain](https://www.linkedin.com/feed/hashtag/?keywords=langchain&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) and [\#DeepLake](https://www.linkedin.com/feed/hashtag/?keywords=deeplake&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) together to build [\#ChatGPT](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) for your financial documents.  
3. How Deep Lake‚Äôs unified and streamable data store enables fast prototyping without the need to recompute embeddings (something that costs time & money).  


I hope you like it, and let me know if you have any questions!"
289,learnmachinelearning,gpt,relevance,2023-10-26 14:51:08,5 Game-Changing Applications of GPT-4: No Coding Skills Required!,OnlyProggingForFun,False,0.67,1,17gy9xb,https://youtu.be/lwNy4lgDpjY,0,1698331868.0,
290,learnmachinelearning,gpt,relevance,2023-07-12 20:08:11,"GPT-3 architecture, explained clearly, in full detail",goo187,False,0.81,13,14xz5v1,https://www.reddit.com/r/learnmachinelearning/comments/14xz5v1/gpt3_architecture_explained_clearly_in_full_detail/,1,1689192491.0,"Here's a full explanation of how GPT-3 works. The goal was to derive literally everything from scratch (the attention head, transformer, and GPT).

[https://www.deriveit.org/notes/119](https://www.deriveit.org/notes/119)

Let me know if this note was useless/useful to you. All feedback is welcome.

Also, if you'd want to write something similar, let me know!"
291,learnmachinelearning,gpt,relevance,2023-01-06 11:58:10,How does ChatGPT actually work? Explained simply with pen and paper,techie_ray,False,0.94,124,104sebq,https://youtu.be/k9Sps7ciNTE,16,1673006290.0,
292,learnmachinelearning,gpt,relevance,2023-06-22 23:53:45,What is a policy in Reinforcement Learning (ChatGPT)?,u2uu,False,0.84,4,14gj3fq,https://www.reddit.com/r/learnmachinelearning/comments/14gj3fq/what_is_a_policy_in_reinforcement_learning_chatgpt/,6,1687478025.0,"Hey,

i read about the RLHF in ChatGPT. And in Step 1 there is the title saying: ""training a supervised policy"".

And in step 3 is it then saying: ""The PPO model is initalized from the supervised policy.""

I really dont understand exactly what is meant with policy. It is the neural network itself of the LLM? Is ist the SFT Model from Step 1? 

And if the ""PPO-Model is initalized"" is this in the beginning exactly like the SFT-Model from step 1? 

I read a lot about ""functions"" regarding to ""policy"". But sometimes i read that the policy is a neural network. So i am really confused. Is the policy a seperate entity? Is it not the neural network of the LLM itself?


I am sorry for my english. I would be very happy for help!"
293,learnmachinelearning,gpt,relevance,2023-09-26 01:48:17,Using GPT-4 to measure creativity in responses to a study.,Henry-T-01,False,0.57,1,16samsg,https://www.reddit.com/r/learnmachinelearning/comments/16samsg/using_gpt4_to_measure_creativity_in_responses_to/,2,1695692897.0,"A friend of mine, who's pursuing a master's in psychology, is working on a thesis linking nice work environments to creativity.  She measured the creativity of participants by letting them list various creative ways of using a toothpick. I.e. ""cleaning your teeth"" wouldn't be considered creative but ""using it as a flag pole in a miniature town"" would. Now she has a few thousand suggestions on how to use toothpicks. She came to me asking wether I could show her how to code a program that could automatically assign a ""creativity score"" to these answers. I of course said that I considered this task to be way too complex to develop anything yourself, furthermore I'm just a math major without any real experience with language models capable of such a task. However, I had the idea to use OpenAI's GPT-4 API. So now we're thinking of writing a little script that takes the toothpick suggestions in batches of 10 and sends them to GPT with a prompt telling it to assign them a creativity score. Now I wanted to ask you all:

1. Do you even believe GPT-4 could handle this task effectively?
2. Any recommendations on formulating the prompt for best results?
3. Are there potential pitfalls or considerations we should be aware of?"
294,learnmachinelearning,gpt,relevance,2024-01-05 02:58:33,Run Mixtral 8x7B MoE for free | Better alternative to GPT 3.5,aniketmaurya,False,0.5,0,18yw5c6,https://youtu.be/6wasqeTeUD0,0,1704423513.0,
295,learnmachinelearning,gpt,relevance,2023-08-27 15:18:26,MetaGPT: Redefining Multi-Agent Collaboration for Complex Tasks.,OnlyProggingForFun,False,1.0,3,162t4ht,https://youtu.be/YtxMderNrzU,0,1693149506.0,
296,learnmachinelearning,gpt,relevance,2023-05-30 18:41:40,Use ChatGPT in your advance!,Ok-Highlight-5794,False,0.17,0,13vyc8x,https://www.paypro.nl/producten/De_Ultieme_ChatGPT-4_Gids/104706/196903,0,1685472100.0,
297,learnmachinelearning,gpt,relevance,2024-02-13 17:23:06,Unlock AI's Power: Top Free Courses on ChatGPT & Large Language Models| DeepLearning.AI,UseCreative4765,False,0.2,0,1apyyrd,https://youtu.be/6nc_qM-1HUc?si=Hn4zOpPYhWIWOgIz,0,1707844986.0,
298,learnmachinelearning,gpt,relevance,2023-10-14 12:54:08,Using GPT-4 as the ‚ÄúBrains‚Äù of a Reinforcement Learning Problem,tylersuard,False,0.5,0,177owz1,https://medium.com/@ceo_44783/using-gpt-4-as-the-brains-of-a-reinforcement-learning-problem-516ff79aedb0,1,1697288048.0,
299,learnmachinelearning,gpt,relevance,2023-02-21 18:49:26,RPG & ChatGPT,TekeelaMockingbird,False,1.0,1,118cq36,https://www.reddit.com/r/learnmachinelearning/comments/118cq36/rpg_chatgpt/,0,1677005366.0,Hey we're doing an RPG podcast with ChatGPT as the GM. This episode was a Valentine's Day theme. The name of the podcast is Constructed Chronicles. https://open.spotify.com/episode/0sOtO4CM2w6tsRhhEwYAJ5?si=mfqJHVJ-QU21x538w9HnMA
