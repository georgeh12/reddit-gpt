,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,id,url,num_comments,created,body
0,learnmachinelearning,llm,top,2023-01-19 07:56:20,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.96,327,10fw2df,https://www.reddit.com/r/learnmachinelearning/comments/10fw2df/gpt4_will_be_500x_smaller_than_people_think_here/,47,1674114980.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/yio0v3zqgyca1.png?width=575&format=png&auto=webp&s=a2ee034ce7ed48c9adc1793bfdb495e0f0812609)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!  
The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
1,learnmachinelearning,llm,top,2022-08-31 22:07:24,Most Popular AI Research August 2022 - Ranked Based On Total Twitter Likes,bycloudai,False,0.97,279,x2q4sk,https://i.redd.it/wj1suih0h4l91.jpg,9,1661983644.0,
2,learnmachinelearning,llm,top,2023-05-11 20:15:46,Top 20 Large Language Models based on the Elo rating system.,kingabzpro,False,0.96,248,13eympz,https://i.redd.it/7xfqr5crf9za1.png,43,1683836146.0,
3,learnmachinelearning,llm,top,2022-12-07 00:36:25,For anyone new to ML: DON’T start with pop content about hot new implementations,yourfinepettingduck,False,0.95,178,zenanj,https://www.reddit.com/r/learnmachinelearning/comments/zenanj/for_anyone_new_to_ml_dont_start_with_pop_content/,26,1670373385.0,"I’ve been seeing these threads and guides blow up recently about “prompt engineering” and other applications related to trendy models. 

But the unsupervised LLM / NN approaches used in productized ML is a TERRIBLE way to learn. I studied for years and still am way out of my league there. Besides, if the models are proprietary you can’t even use the assumptions, algorithms, or design choices to actually learn. It’s just glorified trial and error. 

The same thing goes for 30 min cookbook copy/paste scikit implementations that are everywhere online.

The best way to learn is to start with old un-sexy supervised theory that you can actually understand. Even try implementing a model without having to rely on a packaged function. Then work up. Even if you don’t get far, that time is worth way more and it’ll give you the language and principles to think more critically about the harder stuff. 

You’ll never actually understand the unsupervised black-box LLM that dozens of data scientists have worked on full time for years. So why start there?

Example: For someone with less math background Springer has a textbook “Text analysis with R for student of Literature”. I signed up as a coast elective then it ended up being really cool. English majors were fluent in the basic ideas of language processing in few months and they taught me a ton too. 

That stuff exists in all sorts of fields but they look boring. You won’t find a “how to profit from enterprise neural nets in 7 months” textbook"
4,learnmachinelearning,llm,top,2023-03-02 16:47:40,Build ChatGPT for Financial Documents with LangChain + Deep Lake,davidbun,False,0.95,169,11g7h03,https://www.reddit.com/r/learnmachinelearning/comments/11g7h03/build_chatgpt_for_financial_documents_with/,8,1677775660.0,"https://preview.redd.it/h9r6hgvfucla1.png?width=2388&format=png&auto=webp&s=5432eac3eeed8583e4309af1fdc7ebecac705796

As the world is increasingly generating vast amounts of financial data, the need for advanced tools to analyze and make sense of it has never been greater. This is where [LangChain](https://github.com/hwchase17/langchain) and [Deep Lake](https://github.com/activeloopai/deeplake) come in, offering a powerful combination of technology to help build a question-answering tool based on financial data. After participating in a LangChain hackathon last week, I created a way to use Deep Lake, the data lake for deep learning (a package my team and I are building) with LangChain. I decided to put together a guide of sorts on how you can approach building your own question-answering tools with  LangChain and Deep Lake as the data store.

Read [the article](https://www.activeloop.ai/resources/ultimate-guide-to-lang-chain-deep-lake-build-chat-gpt-to-answer-questions-on-your-financial-data/) to learn:

1. What is LangChain, what are its benefits and use cases and how you can use to streamline your LLM (Large Language Model) development?  
2. How to use [\#LangChain](https://www.linkedin.com/feed/hashtag/?keywords=langchain&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) and [\#DeepLake](https://www.linkedin.com/feed/hashtag/?keywords=deeplake&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) together to build [\#ChatGPT](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) for your financial documents.  
3. How Deep Lake’s unified and streamable data store enables fast prototyping without the need to recompute embeddings (something that costs time & money).  


I hope you like it, and let me know if you have any questions!"
5,learnmachinelearning,llm,top,2023-12-10 18:07:35,Are LLMs overhyped right now?,Snoo_72181,False,0.94,162,18f9enp,https://www.reddit.com/r/learnmachinelearning/comments/18f9enp/are_llms_overhyped_right_now/,67,1702231655.0,"I mean I get that ChatGPT has made LLMs the toast of ML universe. They are indeed amazing.  

But this has lead to so much hype that ML beginners are literally just talking about learning LLMs, ignoring so much in between like Math and Stats, simple ML like Regression, Classification. After that you have, Deep Learning, Transformers and finally LLMs. 

Companies also want candidates with LLM experience, but there's no guarantee that they even have a use case for LLMs "
6,learnmachinelearning,llm,top,2022-09-02 18:26:28,Most Popular AI Research Aug 2022 pt. 2 - Ranked Based On GitHub Stars,bycloudai,False,0.97,154,x48r9g,https://i.redd.it/qc5twtpinhl91.jpg,1,1662143188.0,
7,learnmachinelearning,llm,top,2023-06-28 12:29:48,"Intern tasked to make a ""local"" version of chatGPT for my work",Assasinshock,False,0.97,153,14l887h,https://www.reddit.com/r/learnmachinelearning/comments/14l887h/intern_tasked_to_make_a_local_version_of_chatgpt/,104,1687955388.0,"Hi everyone,

I'm currently an intern at a company, and my mission is to make a proof of concept of an conversational AI for the company.They told me that the AI needs to be trained already but still able to get trained on the documents of the company, the AI needs to be open-source and needs to run locally so no cloud solution.

The AI should be able to answers questions related to the company, and tell the user which documents are pertained to their question, and also tell them which departement to contact to access those files.

For this they have a PC with an I7 8700K, 128Gb of DDR4 RAM and an Nvidia A2.

I already did some research and found some solution like localGPT and local LLM like vicuna etc, which could be usefull, but i'm really lost on how i should proceed with this task. (especially on how to train those model)

That's why i hope you guys can help me figure it out. If you have more questions or need other details don't hesitate to ask.

Thank you.  


Edit : They don't want me to make something like chatGPT, they know that it's impossible. They want a prototype that can answer question about their past project. "
8,learnmachinelearning,llm,top,2023-10-31 05:15:57,What is the point of ML?,shesaysImdone,False,0.75,140,17kdnkt,https://www.reddit.com/r/learnmachinelearning/comments/17kdnkt/what_is_the_point_of_ml/,154,1698729357.0,"To what end are all these terms you guys use: models, LLM? What is the end game? The uses of ML are a black box to me. Yeah I can read it off Google but it's not clicking mostly because even Google does not really state where and how ML is used.

There is this lady I follow on LinkedIn who is an ML engineer at a gaming company. How does ML even fold into gaming? Ok so with AI I guess the models are training the AI to eventually recognize some patterns and eventually analyze a situation by itself I guess. But I'm not sure

*Edit* I know this is reddit but if you don't like me asking a question about ML on a sub literally called learnML please just move on and stop downvoting my comments "
9,learnmachinelearning,llm,top,2023-09-16 13:22:41,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.95,135,16k7heb,https://www.reddit.com/r/learnmachinelearning/comments/16k7heb/this_week_in_ai_all_the_major_ai_developments_in/,17,1694870561.0,"1. **Stability AI** launched Stable Audio, a generative AI tool for music & sound generation from text. The underlying latent diffusion model architecture uses audio conditioned on text metadata as well as audio file duration and start time.
2. **Coqui** released **XTTS** \- a new voice generation model that lets you clone voices in 13 different languages by using just a quick 3-second audio clip.
3. **Microsoft Research** released and open-sourced **Phi-1.5** \- a 1.3 billion parameter transformer-based model with performance on natural language tasks comparable to models 5x larger.
4. **Project Gutenberg**, Microsoft and MIT have worked together to use neural text-to-speech to create and release thousands of **human-quality free and open audiobooks**.
5. Researchers present **NExT-GPT -** an any-to-any multimodal LLM that accepts inputs and generate outputs in arbitrary combinations of text, images, videos, and audio.
6. **Chain of Density (CoD):** a new prompt introduced by researchers from Salesforce, MIT and Colombia University that generates more dense and human-preferable summaries compared to vanilla GPT-4.
7. **Adept** open-sources **Persimmon-8B**, releasing it under an Apache license. The model has been trained from scratch using a context size of 16K.
8. **Adobe's** **Firefly** generative AI models, after 176 days in beta, are now commercially available in Creative Cloud, Adobe Express, and Adobe Experience Cloud. Adobe is also launching Firefly as a standalone web app.
9. **Deci** released **DeciLM 6B**, a permissively licensed, open-source foundation LLM that is 15 times faster than Llama 2 while having comparable quality.
10. Researchers release **Scenimefy** \- a model transforming real-life photos into Shinkai-animation-style images.
11. **Microsoft** open sources **EvoDiff**, a novel protein-generating AI that could be used to create enzymes for new therapeutics and drug delivery methods as well as new enzymes for industrial chemical reactions.
12. Several companies including Adobe, IBM, Nvidia, Cohere, Palantir, Salesforce, Scale AI, and Stability AI have pledged to the White House to develop safe and trustworthy AI, in a voluntary agreement similar to an earlier one signed by Meta, Google, and OpenAI.
13. **Microsoft** will provide legal protection for customers who are sued for copyright infringement over content generated using Copilot, Bing Chat, and other AI services as long as they use built-in guardrails.
14. **NVIDIA** beta released **TensorRT** \- an open-source library that accelerates and optimizes inference performance on the latest LLMs on NVIDIA Tensor Core GPUs.
15. Pulitzer Prize winning novelist Michael Chabon and several other writers sue OpenAI of copyright infringement..
16. **NVIDIA** partners with two of India’s largest conglomerates, Reliance Industries Limited and Tata Group, to create an AI computing infrastructure and platforms for developing AI solutions.
17. **Roblox** announced a new conversational AI assistant that let creators build virtual assets and write code with the help of generative AI.
18. **Google** researchers introduced **MADLAD-400** \- a 3T token multilingual, general web-domain, document-level text dataset spanning 419 Languages.
19. A recent survey by **Salesforce** show that 65% of generative AI users are Millennials or Gen Z, and 72% are employed.  The survey included 4,000+ people across the United States, UK, Australia, and India.
20. **Meta** is reportedly working on an AI model designed to compete with GPT-4.

My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
10,learnmachinelearning,llm,top,2023-06-03 14:33:38,This week in AI - all the Major AI development in a nutshell,wyem,False,0.98,121,13zeoi3,https://www.reddit.com/r/learnmachinelearning/comments/13zeoi3/this_week_in_ai_all_the_major_ai_development_in_a/,13,1685802818.0,"1. The recently released open-source large language model **Falcon LLM**, by UAE’s Technology Innovation Institute, is now royalty-free for both commercial and research usage. **Falcon 40B,** the 40 billion parameters model trained on one trillion tokens, is ranked #1 on **Open LLM Leaderboard by Hugging Face**.
2. **Neuralangelo**, a new AI model from Nvidia turns 2D video from any device - cell phone to drone capture - into 3D structures with intricate details using neural networks..
3. In three months, JPMorgan has advertised **3,651 AI jobs** and sought a trademark for **IndexGPT**, a securities analysis AI product.
4. **Google** presents **DIDACT** (​​Dynamic Integrated Developer ACTivity), the first code LLM trained to model real software developers editing code, fixing builds, and doing code review. DIDACT uses the software development process as training data and not just the final code, leading to a more realistic understanding of the development task.
5. Researchers from **Deepmind** have presented ‘**LLMs As Tool Makers (LATM)**’ - a framework that allows Large Language Models (LLMs) to create and use their own tools, enhancing problem-solving abilities and cost efficiency. With this approach, a sophisticated model (like GPT-4) can make tools (where a tool is implemented as a Python utility function), while a less demanding one (like GPT-3.5) uses them.
6. **Japan's government** won't enforce copyrights on data used for AI training regardless of whether it is for non-profit or commercial purposes.
7. *‘Mitigating the* ***risk of extinction from AI*** *should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.’ -* One sentence statement signed by leading AI Scientists as well as many industry experts including CEOs of OpenAI, DeepMind and Anthropic.*.*
8. Nvidia launched ‘**Nvidia Avatar Cloud Engine (ACE) for Games**’ - a custom AI model foundry service to build non-playable characters (NPCs) that not only engage in dynamic and unscripted conversations, but also possess evolving, persistent personalities and have precise facial animations and expressions.
9. **OpenAI** has launched a trust/security portal for OpenAI’s compliance documentation, security practices etc..
10. **Nvidia** announced a new AI supercomputer, the **DGX GH200,** for giant models powering Generative AI, Recommender Systems and Data Processing. It has 500 times more memory than its predecessor, the DGX A100 from 2020.
11. Researchers from Nvidia presented **Voyager**, the first ‘LLM-powered embodied lifelong learning agent’ that can explore, learn new skills, and make new discoveries continually without human intervention in the game Minecraft.
12. The a16z-backed chatbot startup **Character.AI** launched its mobile AI chatbot app on May 23 for iOS and Android, and succeeded in gaining over **1.7 million new installs** within a week.
13. Microsoft Research presents **Gorilla**, a fine-tuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls.
14. **OpenAI** has trained a model using process supervision - rewarding the thought process rather than the outcome - to improve mathematical reasoning. Also released the full dataset used.
15. **WPP**, the world's largest advertising agency, and Nvidia have teamed up to use generative AI for creating ads. The new platform allows WPP to tailor ads for different locations and digital channels, eliminating the need for costly on-site production.
16. **PerplexityAI’s** android app is available now, letting users search with voice input, learn with follow-up questions, and build a library of threads.

**If you like this news format**, you might find my  [newsletter](https://aibrews.com/) helpful - it's free to join, sent only once a week with **bite-sized news, learning resources and selected tools**. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
11,learnmachinelearning,llm,top,2024-02-03 18:28:40,I want to train a chatbot of myself,Lost-Season-4196,False,0.91,107,1ai2o1u,https://www.reddit.com/r/learnmachinelearning/comments/1ai2o1u/i_want_to_train_a_chatbot_of_myself/,60,1706984920.0,"I have about 193k whatsapp messages of our chat with my gf. I have came across with a guy who finetuned GPT2 on his friend's discord messages in that sub.  Now, I want to fine-tune a model to create one that chats like me.  Ive cleaned the data and split it into days.  I am open to any ideas/advices on how to proceed. Thanks.

Got the idea from that [post](https://www.reddit.com/r/learnmachinelearning/comments/17wp1p7/training_an_llm_to_have_my_friends_personality/?utm_source=share&utm_medium=web2x&context=3)"
12,learnmachinelearning,llm,top,2023-11-24 15:06:53,Talk to Taipy - an app that uses natural language to manipulate and visualize data,quicklyalienated76,False,0.99,99,182u4c8,https://www.reddit.com/r/learnmachinelearning/comments/182u4c8/talk_to_taipy_an_app_that_uses_natural_language/,4,1700838413.0,"Hi! My team has been working on an LLM application called Talk to Taipy.

[https://talk-to-taipy.taipy.cloud/](https://talk-to-taipy.taipy.cloud/)

https://i.redd.it/vrdd3zsa9b2c1.gif

Talk to Taipy was created as an end-user application to manipulate and visualize your data using natural language.  You can add your CSV file and ask the prompt to show/filter/plot... the data. You can also get the Taipy and Panda code of the plot/query.

It was built with Taipy, an open-source Python library that turns your Data and ML applications into full applications, from the front-end to the back-end. ([https://github.com/Avaiga/taipy](https://github.com/Avaiga/taipy)). For the AI part, Talk to Taipy was created using Hugging's face starcoder.

We are open to constructive feedback to make it the best application possible, so don't hesitate!"
13,learnmachinelearning,llm,top,2023-06-14 09:08:23,"Introducing, OpenLLM 🎉",AaZasDass,False,0.95,86,149302y,https://www.reddit.com/r/learnmachinelearning/comments/149302y/introducing_openllm/,15,1686733703.0,"OpenLLM allows you to run inferences with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps. It includes simple and familiar APIs, enabling easy integration with tools such as LangChain, and BentoML! Discover more at [https://github.com/bentoml/OpenLLM](https://github.com/bentoml/OpenLLM)

To get started, install it with pip: `pip install -U openllm`  Currently, it has support for all major SOTA LLMs, including Falcon, ChatGLM, Dolly V2, StableLM, and more to come!

Some of the feature that is currently wip:

\- Fine-tuning API with `LLM.tuning()`

\- LangChain integration [https://github.com/hwchase17/langchain/pull/6064](https://github.com/hwchase17/langchain/pull/6064)

\- OpenAI Compatible API

    import openai
    
    openai.api_base = ""http://localhost:3000"" # Running with OpenLLM
    
    completion = openai.Completion.create(...)

We are currently actively developing the library, so we would love to hear your thoughts and feedback. Feel free also to join our [discord](https://l.bentoml.com/join-openllm-discord) to meet other fellows, AI application builders, and enthusiasts."
14,learnmachinelearning,llm,top,2023-09-30 15:01:31,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.97,82,16w93bx,https://www.reddit.com/r/learnmachinelearning/comments/16w93bx/this_week_in_ai_all_the_major_ai_developments_in/,4,1696086091.0,"1. **Meta AI** presents **Emu**, a quality-tuned latent diffusion model for generating highly aesthetic images. Emu significantly outperforms SDXLv1.0 on visual appeal.
2. **Meta AI** researchers present a series of long-context LLMs with context windows of up to 32,768 tokens. LLAMA 2 70B variant surpasses gpt-3.5-turbo-16k’s overall performance on a suite of long-context tasks.
3. **Abacus AI** released a larger 70B version of **Giraffe**. Giraffe is a family of models that are finetuned from base Llama 2 and have a larger context length of 32K tokens\].
4. **Meta** announced:  

   1. **Meta AI** \- a new AI assistant users can interact with on WhatsApp, Messenger and Instagram. Will also be available on Ray-Ban Meta smart glasses and Quest 3, Meta’s mixed reality headset.
   2. **AI stickers** that enable users to generate customized stickers for chats and stories using text. Powered by Llama 2 and the new foundational model for image generation, Emu.
   3. **28 AI characters**, each with a unique personality that users can message on WhatsApp, Messenger, and Instagram.
   4. New AI editing tools, **restyle** and **backdrop** in Instagram.
   5. **AI Studio** \- a platform that supports the creation of custom AIs by coders and non-coders alike.
5. **Cerebras** and **Opentensor** released Bittensor Language Model, ‘**BTLM-3B-8K**’, a new 3 billion parameter open-source language model with an 8k context length trained on 627B tokens of SlimPajama. It outperforms models trained on hundreds of billions more tokens and achieves comparable performance to open 7B parameter models. The model needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models and is available with an Apache 2.0 license for commercial use.
6. **OpenAI** is rolling out, over the next two weeks, new voice and image capabilities in ChatGPT enabling ChatGPT to understand images, understand speech and speak. The new voice capability is powered by a new text-to-speech model, capable of generating human-like audio from just text and a few seconds of sample speech. .
7. **Mistral AI**, a French startup, released its first 7B-parameter model, **Mistral 7B**, which outperforms all currently available open models up to 13B parameters on all standard English and code benchmarks. Mistral 7B is released in Apache 2.0, making it usable without restrictions anywhere.
8. **OpenAI** has returned the ChatGPT browsing feature for Plus subscribers, enabling ChatGPT to access internet for current information. It was disabled earlier as users were able to deploy it to bypass the paywalls of leading news publishers.
9. **Microsoft** has released **AutoGen** \- an open-source framework that enables development of LLM applications using multiple agents that can converse with each other to solve a task. Agents can operate in various modes that employ combinations of LLMs, human inputs and tools.
10. **LAION** released **LeoLM**, the first open and commercially available German foundation language model built on Llama-2
11. Researchers from **Google** and **Cornell University** present and release code for DynIBaR (Neural Dynamic Image-Based Rendering) - a novel approach that generates photorealistic renderings from complex, dynamic videos taken with mobile device cameras, overcoming fundamental limitations of prior methods and enabling new video effects.
12. **Cloudflare** launched **Workers AI** (an AI inference as a service platform), **Vectorize** (a vector Database) and **AI Gateway** with tools to cache, rate limit and observe AI deployments. Llama2 is available on Workers AI.
13. **Amazon** announced the general availability of **Bedrock**, its service that offers a choice of generative AI models from Amazon itself and third-party partners through an API.
14. **Google** announced it’s giving website publishers a way to opt out of having their data used to train the company’s AI models while remaining accessible through Google Search.
15. **Spotify** has launched a pilot program for AI-powered voice translations of podcasts in other languages - in the podcaster’s voic. It uses OpenAI’s newly released voice generation model.
16. **Getty Images** has launched a generative AI image tool, ‘**Generative AI by Getty Images**’, that is ‘commercially‑safe’. It’s powered by Nvidia Picasso, a custom model trained exclusively using Getty’s images library.
17. **Optimus**, Tesla’s humanoid robot, can now sort objects autonomously and do yoga. Its neural network is trained fully end-to-end.
18. **Amazon** will invest up to $4 billion in Anthropic. Developers and engineers will be able to build on top of Anthropic’s models via Amazon Bedrock.
19. **Google Search** indexed shared Bard conversational links into its search results pages. Google says it is working on a fix.
20. **Pika** Labs' text-to-video tool now lets users encrypt a message in a video\].

My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
15,learnmachinelearning,llm,top,2023-11-06 16:04:48,TensorGym - Interactive ML skill-building platform 🏋️‍♂️,Rudegs,False,0.98,77,17p62wj,https://www.reddit.com/r/learnmachinelearning/comments/17p62wj/tensorgym_interactive_ml_skillbuilding_platform/,12,1699286688.0,"We couldn't find a website where you can interactively learn basic PyTorch tensor operations. So we made [one](https://www.tensorgym.com/)!

So far we added:

* 8 PyTorch basic operators exercises
* 3 hard-ish LLM exercises
* 2 classic ML exercises

https://preview.redd.it/9x3ete3seryb1.png?width=2520&format=png&auto=webp&s=bd31f4f8c4936a284f8793f0ed5ebd34233d2055

Our main principles:

* We provide links and quick hints about the API to save time because it's not about memorization—it's about understanding
* We provide essential math formulas as necessary
* Our goal is to make learning fun and interactive!

Please check it [out](https://tensorgym.com/blog/intro) and join our [Discord server](https://discord.gg/vhhTWMPK5E)!

We really hope that it's useful🏋️‍♂️"
16,learnmachinelearning,llm,top,2023-03-27 09:31:27,tensor_parallel: one-line multi-GPU training for PyTorch,black_samorez,False,0.95,71,123hlg0,https://www.reddit.com/r/learnmachinelearning/comments/123hlg0/tensor_parallel_oneline_multigpu_training_for/,3,1679909487.0,"Hi all! We made a PyTorch [library](https://github.com/BlackSamorez/tensor_parallel) that makes your model tensor-parallel in one line of code.

Our library is designed to work with any model architecture out of the box and can be customized for a specific architecture using a custom config. Additionally, our library is integrated with Hugging Face transformers, which means you can use utilities like .generate() on parallelized models. Optimal parallelism configs for the most popular models are used automatically, making it even more accessible and user-friendly.

We're looking forward to hearing your feedback on how we can make our library even more useful and accessible to the community.

[Try with 20B LLMs now in Kaggle](https://www.kaggle.com/code/blacksamorez/tensor-parallel-int8-llm/)"
17,learnmachinelearning,llm,top,2023-01-15 00:08:37,Is it still worth learning NLP in the age of API-accessibles LLM like GPT?,CrimsonPilgrim,False,0.93,63,10c509n,https://www.reddit.com/r/learnmachinelearning/comments/10c509n/is_it_still_worth_learning_nlp_in_the_age_of/,24,1673741317.0,"A question that, I hope, you will find legitimate from a data science student.

I am speaking from the point of view of a data scientist not working in research.

Until now, learning NLP could be used to meet occasional business needs like sentiment analysis, text classification, topic modeling....

With the opening of GPT-3 to the public, the rise of ChatGPT, and the huge wave of applications, sites, plug-ins and extensions based on this technology that are accessible with a simple API request, it's impossible not to wonder if spending dozens of hours diving into this field if ML wouldn't be as useful today as learning the source code of the Pandas library. 

In some specialized cases, it could be useful, but GPT-3, and the models that will follow, seem to offer more than sufficient results for the immensity of the cases and for almost all classical NLP tasks. Not only that, but there is a good chance that the models trained by giants like Open-AI (Microsoft) or Google can never be replicated outside these companies anyway.  With ChatGPT and its incomparable mastery of language, its ability to code, summarize, extract topics, understand... why would I bother to use BERT or a TF-IDF vectorizer when an API will be released? Not only it would be easily accessible, but it also would be much better at the task, faster and cheaper.

In fact, it's a concern regarding all the machine learning field in general with the arrival of powerful ""no-code"" applications, which abstract a large part of the inherent complexity of the field. There will always be a need for experts, for safeguards, but in the end, won't the Data Scientist who masters the features of GPT-3 or 4 and knows a bit of NLP be more efficient than the one who has spent hours reading Google papers and practicing on Gensim, NLTK, spacy... It is the purpose of an API to make things simpler eventually... At what point is there no more reason to be interested in the behind-the-scenes of these tools and to become simple users rather than trying to develop our own techniques?"
18,learnmachinelearning,llm,top,2023-03-25 16:23:09,What's the current state of actually free and open source LLMs?,maquinary,False,0.97,60,121qvqn,https://www.reddit.com/r/learnmachinelearning/comments/121qvqn/whats_the_current_state_of_actually_free_and_open/,25,1679761389.0,"*People, take easy on me, I just a newbie that tests stuff made by A.I. in a very amateur manner.*

---------------------

Yesterday a played a bit with [Alpaca.cpp](https://github.com/antimatter15/alpaca.cpp), but despite the fact that the software itself is in the MIT license, it has serious limitations because of licensing factors, as you can see [here](https://crfm.stanford.edu/2023/03/13/alpaca.html):

>[...]

>

> We emphasize that Alpaca is intended only for academic research and any commercial use is prohibited. There are three factors in this decision: First, Alpaca is based on LLaMA, which has a non-commercial license, so we necessarily inherit this decision. Second, the instruction data is based on OpenAI’s text-davinci-003, whose terms of use prohibit developing models that compete with OpenAI. Finally, we have not designed adequate safety measures, so Alpaca is not ready to be deployed for general use.

>

> [...]

So, do we have anything that is **completely free** that reaches at least the level of GTP-3?

And what about the data that people use to train the models? Those big companies can ""scan"" the entire web to get insane amounts of data, but can free software developers use these already harvested data to train their own models? Or, in order to have a completely free LLM, people will have to collect data again from the Internet?

-------------

*When I say ""free"", I mean free from licensing limitations, in a sense that I can implement the A.I. in my software without the need of being forced to apply a limited range of licenses, or without the need to pay.*"
19,learnmachinelearning,llm,top,2023-03-30 19:44:32,Personalize Your Own Language Model with xTuring - A Beginner-Friendly Library,x_ml,False,0.99,58,126x6ua,https://www.reddit.com/r/learnmachinelearning/comments/126x6ua/personalize_your_own_language_model_with_xturing/,7,1680205472.0,"Hi everyone,  


If you are interested in customizing your own language model but don't know where to start, try  [xTuring](https://github.com/stochasticai/xturing).  


xTuring's goal is to empower individuals to fine-tune LLM for their specific tasks with as little as 5 lines of code. With xTuring, you can perform high and low precision fine-tuning with a variety of models, including LLaMA, OPT, Cerebras-GPT, Galactica, BLOOM, and more.   


You can also generate your OWN datasets using powerful models like GPT-3 to train a much smaller model on YOUR specific task. With the latest version, you can also use terminal and web interface to chat with your models.  


Please do check out the repo and show your support if you like our work. Would love if you can also contribute by adding models, raising issues or raising PRs for fixes.  


xTuring Github: [https://github.com/stochasticai/xturing](https://github.com/stochasticai/xturing)

If you are interested in getting involved, I am happy to help you on our Discord: [https://discord.gg/TgHXuSJEk6](https://discord.gg/TgHXuSJEk6)

https://i.redd.it/mvxb7i5fixqa1.gif"
20,learnmachinelearning,llm,top,2023-11-19 13:06:55,"The background needed to understand ""Attention is All You Need"" Paper",Soc13In,False,0.94,54,17ywtkd,https://www.reddit.com/r/learnmachinelearning/comments/17ywtkd/the_background_needed_to_understand_attention_is/,22,1700399215.0,"Hi, 

My background is that I am by education a Mechanical Engineer and was in Grad school for quite a few years too. In my opinion the Attention is all you need paper is one of the most important papers for understanding how LLM are built and work. 

However, my background is woefully inadequate to understand the mathematics of it. What are some books and papers that I should read to be able to grok the paper, especially attention, and k,q,v matrices and how it is all operating? I like to think that I have fairly good mathematical maturity so don't hesitate to throw standard and difficult references at me, I don't want to read a common language explainer, I want to be able to write my own LLM, even though I might never have the budget to actually train it. "
21,learnmachinelearning,llm,top,2024-02-01 19:00:21,Why do LLMs hallucinate and how to detect it?,Vegetable-Skill-9700,False,0.85,51,1agimdl,https://www.reddit.com/r/learnmachinelearning/comments/1agimdl/why_do_llms_hallucinate_and_how_to_detect_it/,41,1706814021.0,"Hallucinations occur when a model speaks false but plausible knowledge confidently. In simple words hallucination is when a model “makes stuff up”.

Let’s look at a simple case of hallucinations:

Question: What causes diabetes?

Ideal Response: Diabetes is primarily caused by a combination of genetic and environmental factors, including obesity and lack of physical activity.

Hallucinated response: Diabetes is caused by eating too much sugar, and reducing sugar intake can cure it completely.

There could be multiple reasons why LLMs hallucinate which include:

* Training data quality
* Bias introduced due to the generation method
* Misleading input context

Detecting hallucinations can be quite challenging since hallucinated generations can look very similar to non-hallucinated ones in terms of coherence and fluency of the text.

Here’s a list of some methods and tools you can use to check if your LLM is hallucinating:

&#x200B;

|UpTrain|[https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/context\_awareness/factual\_accuracy.ipynb](https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/context_awareness/factual_accuracy.ipynb)|
|:-|:-|
|Game-theoretic adversarial training|[https://aclanthology.org/2023.findings-acl.496.pdf](https://aclanthology.org/2023.findings-acl.496.pdf)|
|Log Probability|[https://aclanthology.org/2023.eacl-main.75](https://aclanthology.org/2023.eacl-main.75)|
|Sentence Similarity Model|[https://arxiv.org/abs/2212.08597](https://arxiv.org/abs/2212.08597)|

&#x200B;"
22,learnmachinelearning,llm,top,2023-08-16 11:26:18,OpenAI Notebooks which are really helpful,vishank97,False,0.92,50,15sn6ti,https://www.reddit.com/r/learnmachinelearning/comments/15sn6ti/openai_notebooks_which_are_really_helpful/,2,1692185178.0,"The OpenAI cookbook is one of the most underrated and underused developer resources available today. Here are 7 notebooks you should know about:

1. Improve LLM reliability:  
[https://github.com/openai/openai-cookbook/blob/main/techniques\_to\_improve\_reliability.md](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)
2. Embedding long text inputs:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Embedding\_long\_inputs.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb)
3. Dynamic masks with DALLE:  
[https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How\_to\_create\_dynamic\_masks\_with\_DALL-E\_and\_Segment\_Anything.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How_to_create_dynamic_masks_with_DALL-E_and_Segment_Anything.ipynb)
4. Function calling to find places nearby:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Function\_calling\_finding\_nearby\_places.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb)
5. Visualize embeddings in 3D:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing\_embeddings\_in\_3D.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_3D.ipynb)
6. Pre and post-processing of Whisper transcripts:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Whisper\_processing\_guide.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb)
7. Search, Retrieval, and Chat:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Question\_answering\_using\_a\_search\_API.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb)

Big thanks to the creators of these notebooks!"
23,learnmachinelearning,llm,top,2023-12-25 05:32:15,"How do companies ""censor"" LLMs?",H4SK1,False,0.95,49,18qc9aj,https://www.reddit.com/r/learnmachinelearning/comments/18qc9aj/how_do_companies_censor_llms/,15,1703482335.0,"Since LLMs dont understand what they are saying, how do they know what not to say? Like dont say: 'Hitler is a great guy' etc. Do companies have a training set contained all the things they dont want their LLM to say? That sounds impratical."
24,learnmachinelearning,llm,top,2023-09-12 13:42:02,This is why LLMs have flooded the NLP market in the past 1 year 👇 (A Brief History of NLP),japkeerat,False,0.84,45,16grq5y,https://www.reddit.com/r/learnmachinelearning/comments/16grq5y/this_is_why_llms_have_flooded_the_nlp_market_in/,15,1694526122.0,"Text Generation has been the hottest topic in Natural Language Processing. Recurrent Neural Networks (RNNs) were among the Algorithms to generate text. How RNNs generated text is by essentially predicting the next word given the previous few words. At one-stage RNNs were the hottest commodity one could have. But researchers were worried about 1 problem.

RNNs had a context-length problem. To understand what is context-length, consider an analogy. You started reading a book, it’s 100 pages long and when you read each page, details of previous pages start to get a little hazy. Haziness keeps on increasing to the point that when you reach page 50, you don’t remember anything from the first 5 pages. That is exactly what the problem is with RNNs.

To solve this, researchers developed another algorithm called the Long-Short Term Memory (LSTM) and another variant called Bidirectional Long-Short Term Memory (Bi-LSTM) which had a larger context-length than RNNs. Let’s get back to the book analogy. This time while reading, you are making notes. When you go ahead to a new page and your previous pages information start to get hazy, you look back at these notes to refresh your memory. It’s oversimplified, but that’s basically how an LSTM works.

LSTMs were not perfect. There were a number of new issues that came up in order to resolve the previous one. Meanwhile, other areas of research and technological advancements were heating up. Hardware was getting more and more prominent and with cloud getting popular, it was easily accessible. And on the research side, a new kind of Algorithm came up that shaped the entire NLP domain from here on - Attention Mechanism.

Attention Mechanism, as you might have guessed, is all about telling the more sophisticated algorithms where to “focus”. It’s the same way how we focus more on certain parts of the meeting we attend than the entire meeting itself. In context of NLP, the Mechanism became the core part for better algorithms. These better algorithms could keep larger context-lengths and at the time of predicting the next word, ask the Attention Mechanism about what to focus on while predicting the next word. This was an era-defining discovery in NLP as the algorithms that came up after this were the Transformers.

Consider jigsaw puzzles. You start by looking at all the pieces at once and join the pieces together. Initially, it is random. You join a couple of pieces at the top left corner, a few in the centre and a couple more defining the right edge. You are doing it all at once. Transformers basically work the same way. They could look at longer context-lengths, all at once, courtesy of Attention Mechanism. This means, they can not only work with a sentence, they can work with an entire paragraph.  With time, these Transformers started becoming more and more sophisticated. It eventually reached to a point that the only thing that was keeping these algorithms in handcuffs was the lack of data.

Until recently, these algorithms were trained on a specific data but when algorithms became too powerful, researchers started throwing every kind of data they could find on the internet easily. It could be articles like this, your social media posts, exam papers and solutions, and ebooks in any language they could find and hoped the algorithms learnt it all. And they were right. Algorithms started learning all of it to the point that you could ask models to explain concepts of LLMs in how Shakespeare would write and it would give a real-sounding responsive. These algorithms were Large! And hence, became known as Large Language Models (LLMs).

There we are now. With LLMs. OpenAI, technically, won the race for LLM development. They brought everybody’s attention to LLMs first with GPT-2, but GPT-3 was where shit hit the roof and every company that had deep pockets started investing in LLMs.  The result? We now have a new LLM getting released EVERY. SINGLE. DAY.

*I post articles like these every few days on X. If you like this post, please* [follow me on X!](https://twitter.com/JapkeeratS/)

*NOTE: To make it simple for anybody, even without a tech background, to understand, a few things were oversimplified. I will be sharing soon on* [my X handle](https://twitter.com/JapkeeratS) *a technical version.*"
25,learnmachinelearning,llm,top,2023-10-22 11:38:41,Looking for Resources to Learn LLM in depth,UpvoteBeast,False,0.96,41,17drbyo,https://www.reddit.com/r/learnmachinelearning/comments/17drbyo/looking_for_resources_to_learn_llm_in_depth/,5,1697974721.0,"I have some background in deep learning (e.g. ML courses, training ranking and classification models), but I’m looking for resources (blogs, videos, podcasts, courses) to learn more about LLMs. Are there any resources that you’d recommend?"
26,learnmachinelearning,llm,top,2023-07-19 21:58:14,Fine-tuning my 🦙🦙 model,IMissEloquent75,False,0.95,40,1548aa3,https://www.reddit.com/r/learnmachinelearning/comments/1548aa3/finetuning_my_model/,13,1689803894.0,"My company would like to fine-tune the new Llama 2 model on a list of Q/A that our customers use to ask our support client.

I never did this task for an LLM, so I’d like some insights before throwing GPU money out of the window:
- Should I fine-tune the “pre-trained” or “Chat” model? What difference does it make in terms of fine-tuning requirements? 
- Does the amount of Q/A I have matter compare to the size of the model(7B, 13B, 70B)?
- Any good advice for achieving this kind of task?

That's a lot of noob questions, I suppose. Kudos to the one who gives me an answer❤️"
27,learnmachinelearning,llm,top,2023-07-13 15:35:00,Looking for Resources to Learn LLM in depth,EmoryCadet,False,0.97,34,14yo1m0,https://www.reddit.com/r/learnmachinelearning/comments/14yo1m0/looking_for_resources_to_learn_llm_in_depth/,5,1689262500.0,"I have some background in deep learning (e.g. ML courses, training ranking and classification models), but I’m looking for resources (blogs, videos, podcasts, courses) to learn more about LLMs. Are there any resources that you’d recommend?

 "
28,learnmachinelearning,llm,top,2023-10-17 07:08:17,LLMs for the infinite input lengths are here!,av_community,False,0.83,31,179shyy,https://www.reddit.com/r/learnmachinelearning/comments/179shyy/llms_for_the_infinite_input_lengths_are_here/,10,1697526497.0,"🔍A team of researchers from Meta AI and MIT developed **StreamingLLM**, a framework that enables finite-length LLMs to infinite sequence lengths without finetuning.

🔥Enables Llama 2, Falcon, and MPT to more than 4 million tokens input lengths.

🌟Applying LLMs to infinite input lengths poses 2 challenges:

1️⃣Excessive Memory Storage: During the decoding stage, LLMs store the KV pairs of previous tokens to compute the attention. Having infinite length tends to cause excessive memory storage and increased latency.

2️⃣Performance Degradation: The performance of LLMs degrades if we extend the sequence length beyond the maximum input length set during pretraining.

🎯The team proposed an interesting phenomenon known as attention sinks to overcome the above challenges.

📚Research Paper: [https://arxiv.org/pdf/2309.17453.pdf](https://arxiv.org/pdf/2309.17453.pdf)  
💻 Code: [https://github.com/mit-han-lab/streaming-llm](https://github.com/mit-han-lab/streaming-llm)

What are your thoughts?"
29,learnmachinelearning,llm,top,2023-08-22 16:30:35,Langchain: Explained in 2 minutes,ComprehensiveRise569,False,0.94,30,15yawb1,https://www.reddit.com/r/learnmachinelearning/comments/15yawb1/langchain_explained_in_2_minutes/,4,1692721835.0,"
No stock images/ videos, no gifs, and no flashy texts. Only pure technical deep dive.

Here is the quickest but in-depth explainer video about Langchain, a framework gaining popularity day by day. 

https://www.youtube.com/watch?v=C9bE8bHcJVI

Using Langchain is  one of the quickest way to create and test an advanced LLM based AI application. Check it out!"
30,learnmachinelearning,llm,top,2023-12-13 20:50:57,What happened after BERT and transformers in NLP?,obergrupenfuer_smith,False,1.0,28,18hqty1,https://www.reddit.com/r/learnmachinelearning/comments/18hqty1/what_happened_after_bert_and_transformers_in_nlp/,9,1702500657.0,"hey guys, stopped following ML in 2019 or so when I became an analyst. I am familiar with the field upto BERT, Transformers, Bi directional transformers.

Now I am interviewing for a company asking for LLM (large language models), so I want to know what are some salient papers which came out in the last couple years so I can read up on them. basically the best performing models. I remember CVPR was for computer vision.. what was the one for NLP?

EDIT: Is transformer the core building block of all these things? I remember reading 'Attention is all you need' paper back in college which was amazing. Any new papers like that in NLP? (Or gen AI?)"
31,learnmachinelearning,llm,top,2023-08-05 21:26:47,Best book for understanding the fundamental mathematics of modern machine learning and inference?,SecretPressure9813,False,0.97,28,15j78kn,https://www.reddit.com/r/learnmachinelearning/comments/15j78kn/best_book_for_understanding_the_fundamental/,10,1691270807.0,"I own the 2006 era book by Christopher Bishop ""Pattern Recognition and Machine Learning (Information Science and Statistics)""  ([https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr\_1\_4\_sspa](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr_1_4_sspa)) which presents the Bayesian fundamentals ... but I'm not clear on whether the math presented there is what underlies the training and inference used in current LLM models etc.  


Please let me know your thoughts and/or let me know what you think the best textbook and/or courseware to understand these models at the fundamental level is. I'm also interested in any good overviews discussing how models are structured and/or the associated ""rules of thumb"" in that area. I get the sense that there's a lot of black magic when it comes to this..."
32,learnmachinelearning,llm,top,2023-07-27 11:46:34,LLM Guide [Discussion],torspayorryum,False,0.94,28,15azq0q,https://www.reddit.com/r/learnmachinelearning/comments/15azq0q/llm_guide_discussion/,6,1690458394.0,"Nowadays, If we see over the internet that LLM, chatgpt , llma etc are the trending topics and are being discussed. My question is that anyone can help me where to start studying about these topics from scratch ? BERT, Transformer etc all I want to understand everything.

It would be good if you help me out.

Thanks"
33,learnmachinelearning,llm,top,2023-05-29 17:37:32,"GPT Weekly - 29th May Edition: Facebook's massive STT and TTS Release, AI in Windows, Paralegal jobs are here to stay and more.",level6-killjoy,False,0.87,28,13v1asb,https://www.reddit.com/r/learnmachinelearning/comments/13v1asb/gpt_weekly_29th_may_edition_facebooks_massive_stt/,2,1685381852.0," 

This is a recap covering the major news from last week.

* 🔥Top 3 AI news in the past week
* 🗞️10 AI news highlights and interesting reads
* 🧑‍🎓3 Learning Resources

# 🔥Top 3 AI news in the past week

## 1. Expanding Language Horizons

Facebook has [released an open source model called MMS (Massively Multilingual Search)](https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/) for STT (speech to text), TTS (text to speech) and language identification. 

This is a big breakthrough. Currently, STT and TTS models recognize only 100 languages. With this the technology has been expanded to 1100 languages. That is 10x the current best. 

Additionally, these models can recognize 4000+ languages. 

As per Facebook, they also have half the error rate of OpenAI’s Whisper.

These guys are on a roll.

## 2. Bing Chat Enters the OS

After [Google’s announcement](https://gptweekly.beehiiv.com/p/week-google-ai-large-llm-gpt-plugin), it was time for Microsoft to announce AI products. Here’s a rundown of what was announced during Microsoft Build:

1. **Windows Copilot**  \- Microsoft is integrating AI directly into the OS. Now you can do everything you could do with Bing Chat but now on the OS. You can do the usual stuff - summarize emails, documents, re-write etc. But it goes beyond that by integrating into the installed applications.

Microsoft is also adopting OpenAI's plugin model. So, **you can use ChatGPT and Bing plugins to interact with the integrated AI.** 

The great thing about it is the direct integration into the OS. Eat your heart out, Mac users – at least for now 😀. Until Apple announces something similar. And someone will come up with an alternative solution. Especially, because of the privacy concerns with Microsoft telemetry. 

The bad thing is - [the security aspect of the plugins](https://gptweekly.beehiiv.com/p/caution-chatgpt-plugins). It can open a whole new attack vector on the OS and antivirus softwares might struggle with it. 

It also might be the second nail in the coffin for all the summarize, “talk to your document” apps. Once, this feature is integrated with [Google Docs](https://gptweekly.beehiiv.com/p/week-google-ai-large-llm-gpt-plugin) and Microsoft Office - why will you want to pay for extra apps?

1. **Search comes to ChatGPT**  \- Looks like OpenAI had enough of the testing and new features are being rolled out [left](https://gptweekly.beehiiv.com/p/week-google-ai-large-llm-gpt-plugin) and [right](https://gptweekly.beehiiv.com/p/caution-chatgpt-plugins). 

No prizes for guessing the search engine behind it. Ding, Ding, Ding..It’s Bing!

1. **Co-Pilot in PowerPages** \- Microsoft is now adding AI to their [PowerPages platform](https://powerpages.microsoft.com/en-in/), their low-code tool to build websites. It’ll help users to generate text, forms etc.
2. **Microsoft Fabric** \- A new data analytics platform built on top of Azure Data lake but can get data from S3, Google cloud etc. It can help users build pipelines, write code, and build ML models.

## 3. From Trusted Advisor to Nightmare: The Hazards of Depending on AI

Here’s a [fun story which is breaking out on Legal twitter](https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html). 

A man filed a personal injury lawsuit against Avianca airlines. Avianca's lawyers wasted no time and requested the judge to dismiss the case. The man's lawyer had a different plan in mind. He submitted a document citing half a dozen cases that bolstered his client's claims.

Here's the twist—the judge and Avianca's lawyer couldn't locate any of the referenced cases. Quite a conundrum, right? The lawyer was then asked to provide copies of these elusive cases. The lawyer submitted screenshots as evidence, taking extra precautions to ensure their authenticity. 

You already know the direction this story is taking. 

The lawyer had used ChatGPT to compose his brief. But little did he know that ChatGPT had supplied him with fake cases.

When asked to file tangible copies of these cases, the lawyer turned to ChatGPT once again. ChatGPT had reassured him that the cases were genuine. Feeling emboldened, the lawyer used ChatGPT to provide the requested copies. He even went as far as incorporating chat screenshots into a legal document.

The lawyer maintains that it was never his intention to deceive the court. He expressed regret for relying on ChatGPT for their research. Unfortunately, the judge isn't pleased with this turn of events. The judge has threatened sanctions against both the lawyer and his firm.

It serves as a stark reminder of how ChatGPT has fooled many people. There is a clear warning stating that ChatGPT may produce inaccurate information. But many tend to overlook these warnings. Even legal professionals!!

This story carries significant importance for those who fear job insecurity. The lawyer and his firm could have prevented the entire debacle. They should've used paralegal services. They instead relied on ChatGPT's. It's a hard lesson learned the hard way.

My sincere hope is that this story serves as a valuable lesson. It helps people avoid making similar mistakes. The legal community might become apprehensive about ChatGPT's use moving forward.

# 🗞️10 AI news highlights and interesting reads

1. [OpenAI says in 10 years AI could be as productive as one of today’s large corporations](https://openai.com/blog/governance-of-superintelligence). This poses an existential risk and they suggest some regulations to manage it. This poses an existential risk and they suggest some regulations to manage it. To achieve this, countries need to form something like the [IAEA](https://en.wikipedia.org/wiki/International_Atomic_Energy_Agency). The IAEA is an intergovernmental agency under the UN to oversee nuclear energy. This “AI agency” will monitor the AI systems and conduct inspections. Just like nuclear energy is tracked through signatures, they suggest using compute and energy usage to track systems.
2. In the meantime, [Google is working on voluntary rules](https://techcrunch.com/2023/05/24/eu-google-ai-pact/) until there are some real regulations in place. 
3. [As per Pew Research, 58% of Americans have heard of ChatGPT. Even less - 14% have tried ChatGPT. ](https://www.pewresearch.org/short-reads/2023/05/24/a-majority-of-americans-have-heard-of-chatgpt-but-few-have-tried-it-themselves/)
4. Sharing prompts and results has been a pain. Taking screenshots is one way. But then everyone has to type in the prompts manually. Or you can share as plain text. But ChatGPT results are non-deterministic. So, the results might not be the same. Even the lawyer above would’ve loved this feature. Now you will be able to [share your ChatGPT conversations publicly](https://help.openai.com/en/articles/7925741-chatgpt-shared-links-faq).
5. LLM Agents and plugins need to connect to tools to perform the tasks outside the LLM environment. So, it is important for the LLM to know which API to call and pass correct arguments. [Gorilla is a fine-tuned Llama-model which can generate the correct call and arguments](https://gorilla.cs.berkeley.edu/). 
6. If you are trying to build something beyond a document summarizer or a wrapper around GPT4 API, [things can be hard](https://www.honeycomb.io/blog/hard-stuff-nobody-talks-about-llm). Finding the correct context window, dealing with slow responses (I am looking at you GPT-4) etc are some of the problems. 
7. [The AI boom could expose investors’ natural stupidity](https://www.reuters.com/breakingviews/ai-boom-could-expose-investors-natural-stupidity-2023-05-19/). 
8. [Chatbot leaderboard for the week](https://lmsys.org/blog/2023-05-25-leaderboard/). GPT-4 is still ahead.
9. [Google’s flood warning system is now available in 80 countries. ](https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/)
10. [GPT detectors are biased against non-native English writers](https://arxiv.org/abs/2304.02819)

# 🧑‍🎓3 Learning Resources

1. [Build a product using Replit+AI](https://www.priyaa.me/blog/building-with-ai-replit). The author is a non-technical person who won a hackathon competing with engineers. 
2. [LangChain 101](https://replit.com/@MckayWrigley). 
3. [NLP Course from HuggingFace](https://huggingface.co/learn/nlp-course/chapter0/1)

That’s it folks. Thank you for reading and have a great week ahead.

**If you are interested in a focused weekly recap delivered to your inbox on Mondays you can**[ subscribe here. It is FREE!](https://gptweekly.beehiiv.com/subscribe)"
34,learnmachinelearning,llm,top,2023-10-13 14:23:10,Authoring another course about LLMs. Learn by Doing LLM Projects.,pmartra,False,0.88,25,176zx1m,https://www.reddit.com/r/learnmachinelearning/comments/176zx1m/authoring_another_course_about_llms_learn_by/,5,1697206990.0,"Hi, I'm working on a course about LLMs on GitHub, it's totally free and under MIT license,  So there are no restrictions.

Here the link: [https://github.com/peremartra/Large-Language-Model-Notebooks-Course](https://github.com/peremartra/Large-Language-Model-Notebooks-Course)

I'm still working on It, but now I'm feeling comfortable with the variety and quality of the content. By the moment is a small repository with just 80 Stars.

My intention is to make the course more accessible to a wider audience, and, if possible, encourage  reporting any issues  encounter or suggesting improvements through the 'Discussion' section.

I'm eager to receive feedback.

Now, I'll provide an overview of the currently available content, and then I'll share a couple of questions I have about how to proceed with the course.

[Large Language Models Course: Learn by Doing LLM Projects.](https://github.com/peremartra/Large-Language-Model-Notebooks-Course)

* Introduction to LLM with OpenAI.
   * Create a first Chatbot using FPT 3.5.
   * Create a Natural Language to SQL Translator using OpenAI.
* Vector Databases with LLM.
   * Influencing Language Models with Information stored in ChromaDB.
* LangChain & LLM Apps.
   * RAG. Use the Data from Dataframes with LLMs.
   * Create a Moderation System using LangChain.
      * OpenAI.
      * GPT\_j.
      * LLama-2.
   * Create a Data Analyst Assistant using a LLM Agent.
* Evaluating LLMs
   * Evaluating Summarization with ROUGE.
* Fine-Tuning & Optimization.
   * Prompt-tuning using PEFT.
   * Fine-Tuning with LoRA.
   * Fine-Tuning a Large Model in a GPU using QLoRA. 

That's all for the moment, but I'm adding new content regularly. I'm working on it only in my spare time (mainly nights when the family goes to sleep).

\_\_\_

I have a doubt, I don't know if add some information about platforms like W&B or Cohere?  or maybe it is a better idea to stay with more Open-Source libraries?

On the other hand, my intention is to develop a couple of projects utilizing the techniques covered in the initial part of the course (which I am currently working on).

Some of these projects will be hosted in the cloud on major platforms such as Azure or GCP, or AWS. Any preference?

Furthermore, there is a plan to create a third section that explains how Large Language Models (LLMs) fit into large-scale enterprise solutions, defining architectures in which LLMs are used but are not the sole components of the project.

I don't intend to create a community outside of GitHub, but I would like the repository to have more activity and not be the one determining the course's direction.

Hope you like it, and lease, feel free to contribute.

&#x200B;"
35,learnmachinelearning,llm,top,2023-05-25 12:48:49,Visual intuitive explanations of LLM concepts (LLM University),jayalammar,False,0.84,22,13rgwpf,https://llm.university/,1,1685018929.0,
36,learnmachinelearning,llm,top,2024-01-05 15:14:07,"This Week's Major AI developments in a nutshell (December Week 4, 2023 + January week 1, 2024)",wyem,False,0.96,23,18z95ko,https://www.reddit.com/r/learnmachinelearning/comments/18z95ko/this_weeks_major_ai_developments_in_a_nutshell/,1,1704467647.0,"1. **Meta** and UC, Berkeley introduced ***Audio2Photoreal***, a framework for generating full-bodied photorealistic avatars with gestures driven from audio of a dyadic conversation \[[*Details*](https://people.eecs.berkeley.edu/~evonne_ng/projects/audio2photoreal/) | [*GitHub*](https://github.com/facebookresearch/audio2photoreal)*\].*
2. **MyShell** along with researchers from MIT and Tsinghua University introduced ***OpenVoice***, an open sourcce voice cloning approach that is nearly instantaneous and provides granular control of tone, from emotion to accent, rhythm, pauses, and intonation, using just a small audio clip \[[*Details*](https://research.myshell.ai/open-voice) *|* [*Hugging Face*](https://huggingface.co/spaces/myshell-ai/OpenVoice)\] .
3. **Suno** and Nvidia present ***Parakeet***, a family of open source speech recognition models that top the Open ASR Leaderboard. Parkeet models effectively prevent the generation of hallucinated transcript and are robust to noisy audio. Available for commercial use under CC BY 4.0 \[[*Details*](https://nvidia.github.io/NeMo/blogs/2024/2024-01-parakeet/) | [*Hugging Face*](https://huggingface.co/spaces/nvidia/parakeet-rnnt-1.1b)\].
4. **Researchers** from Stanford University introduce ***Mobile-ALOHA***, an open-source robot hardware that can can autonomously complete complex mobile manipulation tasks that require whole-body control like cook and serve shrimp, call and take elevator, store a 3Ibs pot to a two-door cabinet etc., with just 50 demos \[[*Details*](https://mobile-aloha.github.io/)\].
5. **Allen Institute for AI** released ***Unified-IO 2*** (open-source), the first autoregressive multimodal model that is capable of understanding and generating image, text, audio, and action. The model is pre-trained from scratch on an extensive variety of multimodal data -- 1 billion image-text pairs, 1 trillion text tokens, 180 million video clips, 130 million interleaved image & text, 3 million 3D assets, and 1 million agent trajectories \[[*Details*](https://unified-io-2.allenai.org/)\].
6. **Alibaba** Research introduced ***DreamTalk***, a diffusion-based audio-driven expressive talking head generation framework that can produce high-quality talking head videos across diverse speaking styles \[[*Details*](https://dreamtalk-project.github.io/) *|* [*GitHub*](https://github.com/ali-vilab/dreamtalk)\].
7. **OpenAI’s app store** for GPTs will launch next week \[[*Details*](https://techcrunch.com/2024/01/04/openais-app-store-for-gpts-will-launch-next-week/)\].
8. **GitHub Copilot Chat**, powered by GPT-4, is now generally available for both Visual Studio Code and Visual Studio, and is included in all GitHub Copilot plans alongside the original GitHub Copilot \[[*Details*](https://github.blog/2023-12-29-github-copilot-chat-now-generally-available-for-organizations-and-individuals)\].
9. **Microsoft Research** presented a new and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training step \[[*Paper*](https://arxiv.org/pdf/2401.00368.pdf)\] | [*Hugging Face*](https://huggingface.co/intfloat/e5-mistral-7b-instruct)\].
10. **Google DeepMind** introduced ***AutoRT, SARA-RT and RT-Trajectory*** to improve real-world robot data collection, speed, and generalization \[[*Details*](https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotic)\].
11. **Salesforce Research** presented ***MoonShot***, a new video generation model that conditions simultaneously on multimodal inputs of image and text, demonstrating significant improvement on visual quality and temporal consistency compared to existing models. The model can be easily repurposed for a variety of generative applications, such as personalized video generation, image animation and video editing. Models will be made public [here](https://github.com/salesforce/LAVIS) \[[*Details*](https://showlab.github.io/Moonshot/)\].
12. **Leonardo AI** released ***Leonardo Motion*** for generating videos from images. Available to all users, paid and free \[[*Link*](https://leonardo.ai/)\].
13. **JPMorgan AI Research** present ***DocLLM***, a layout-aware generative language model for multimodal document understanding. The spatial layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component \[[Details](https://arxiv.org/pdf/2401.00908.pdf)\].
14. **Alibaba Research** introduced ***Make-A-Character (Mach)***, a framework to create lifelike 3D avatars from text descriptions. Make-A-Character supports both English and Chinese prompts. \[[*Details*](https://human3daigc.github.io/MACH/) *|* [*Hugging Face*](https://huggingface.co/spaces/Human3DAIGC/Make-A-Character)\].
15. **Sony**, Canon and Nikon set to combat deepfakes with digital signature tech in future cameras \[[*Details*](https://www.techradar.com/cameras/photography/sony-canon-and-nikon-set-to-combat-deepfakes-with-digital-signature-tech-in-future-cameras)\].
16. **Meta AI** introduced ***Fairy***, a versatile and efficient video-to-video synthesis framework that generates high-quality videos with remarkable speed. Fairy generates 120-frame 512x384 videos (4-second duration at 30 FPS) in just 14 seconds, outpacing prior works by at least 44× \[[Details](https://fairy-video2video.github.io/)\].
17. **Apple** quietly released an open source multimodal LLM, called ***Ferret***, in October 2023 \[[*Details*](https://venturebeat.com/ai/apple-quietly-released-an-open-source-multimodal-llm-in-october/)\].
18. **Australian researchers** introduced a non-invasive AI system, called ***DeWave***, that can turn silent thoughts into text while only requiring users to wear a snug-fitting cap \[[*Details*](https://www.sciencealert.com/new-mind-reading-ai-translates-thoughts-directly-from-brainwaves-without-implants)\].
19. **Pika Labs** text-to-video AI platform **Pika 1.0** is now available to all and accessible via the web \[[*Link*](https://pika.art/)\].
20. **The New York Times** sued OpenAI and Microsoft for copyright infringement \[[*Details*](https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html)\].

**Source**: [AI Brews newsletter-](https://aibrews.com/) you can subscribe [here](https://aibrews.substack.com/). it's free to join, sent only once a week with ***bite-sized news, learning resources and selected tools.*** *Thank you!*"
37,learnmachinelearning,llm,top,2023-11-27 02:53:26,Why do I need a GPU for ML/AI,notdoreen,False,0.8,24,184so8i,https://www.reddit.com/r/learnmachinelearning/comments/184so8i/why_do_i_need_a_gpu_for_mlai/,32,1701053606.0,"I'm fairly new to AI and was planning to host my own LLM. I I mentioned this to a coworker and he mentioned I would need a gaming GPU. Why is that?

Can someone explain the purpose of a GPU for AI?"
38,learnmachinelearning,llm,top,2023-04-14 07:03:30,"Ok so I've got a language model architecture that can run locally on cell phones and probably pi's, both for training and text prediction. What now?",saturn_since_day1,False,0.83,23,12lnnml,https://www.reddit.com/r/learnmachinelearning/comments/12lnnml/ok_so_ive_got_a_language_model_architecture_that/,20,1681455810.0,"I'm going to feed it dolly and maybe alpaca to see if it can follow instructions well, but if it doesn't, is there a market for an LLM that can train and run on potatoes with as little as 6Megabytes of RAM and a few gigs of storage, for the text prediction type of things? 


It Should be able to handle something like customer service chat easily. Or looking up facts it knows. Includes a confidence tell on replies and can think of several replies before giving one.


 It can also learn on the fly. 


so far I have it rehashing facts from Wikipedia articles and writing poetry as tests, and learning whatever facts I type into it. It's very adjustable in terms of creativity or precision to the point of memorization of book chapters on the accurate end.


It also expands as it learns and learns faster than you can read as a human.


I feel like with instruction-taking models like llama and dolly existing on consumer hardware already I might be a bit late if this can't do that well and is only good at text finishing/prediction/creation, but I also feel like my architecture makes it very accessible to train and run your own and that will be worth something regardless.


I know if it can follow instructions it will be worth billions just in hardware and energy savings. But do any of you see a use case if it can't? But can only text predict?


Oh and it is multilingual.


Thoughts?"
39,learnmachinelearning,llm,top,2023-10-08 06:11:25,How are you evaluating and monitoring LLMs?,GuillerminaCharity,False,1.0,22,172ruzm,https://www.reddit.com/r/learnmachinelearning/comments/172ruzm/how_are_you_evaluating_and_monitoring_llms/,6,1696745485.0,"Question for people who are implementing LLMs (open source, fine tuned, any kind).

1. How do you know that your getting the quality output from the model that you need to ship the feature or model? Are the audits ad hoc data sampling and subjective ""good/bad"" ratings or have you figured out a more rigorous framework? Is it pretty much \~vibes\~ based?
2. What, if any, tools or processes are you putting into place to monitor and observe the LLM when its interacting with real time user data for weeks or months?

Most of the folks I have spoken with are doing very ad hoc sampled output and writing down on post its or in a spreadsheet a subjective quality ratings.

One person had developed a slightly more rigorous 3 question survey on ""is the result factual"", ""is the result cogent"" and ""is the result useful"". Not everyone is logging their LLM responses they show users which feels very risky to me.

Anyone aware of any industry standards being established around this?"
40,learnmachinelearning,llm,top,2023-06-19 17:49:06,"GPT Weekly - 19the June Edition - OpenAI's function calling, Meta's free LLM, EU Regulation and more.",level6-killjoy,False,1.0,23,14dlfas,https://www.reddit.com/r/learnmachinelearning/comments/14dlfas/gpt_weekly_19the_june_edition_openais_function/,2,1687196946.0," 

This is a recap covering the major news from last week.

* 🔥Top 3 news - OpenAI’s updates, Meta’s upcoming free LLM and EU Regulation
* 🗞️Interesting reads include PSA about protecting your keys, The GPT ouroboros, Reddit - OpenAI’s moat, and more..
* 🧑‍🎓Learning includes a Step-by-step guide from a non-technical founder who launched his MVP, Chatbot for your Gdrive and more

# 🔥Top 3 AI news in the past week

## 1. OpenAI: New Pricing, Models, & Functions

OpenAI has been on a roll. Last week we saw the release of [OpenAI best practice on using GPT.](https://gptweekly.beehiiv.com/p/making-gpt-openais-tactics-better-results) This week we saw some amazing updates. Three major buckets were:

First, the price decreases for both embeddings and GPT-3.5 tokens. 

Second, new models for gpt-4 and gpt-3.5. A new longer context model for gpt-3.5.

Third, a new function calling capability. 

**Why is it important?** Previously, the output from OpenAI was all text. So, calling an external API from GPT was quite difficult. You had to parse the text data and things were often incorrect.  Langchain created the Agents and Tools feature to tackle this problem. It was still unreliable and prone to issues. 

Now you get native support to generate a fixed format output. You can use the output to generate functional calls and also pass functions which need to be called. For example, if your app has multiple API endpoints then you can use GPT to generate the API calls with parameters. You can also pass the endpoints as function calls to ensure the correct function is executed. 

This functionality can further be used to [generate structured data (JSON) out of GPT](https://yonom.substack.com/p/native-json-output-from-gpt-4). So, you can generate data from GPT and load it into your backend. 

**What’s next?** This functionality allows turning natural language responses into structured data. This can be used to create “intelligent” backends using LLMs. We might see implementations in no-code tools to allow more robust and natural-language tools for non-technical folks.

The structured data process goes both ways. You can also feed structured data into GPT for better responses. 

This feature also has its share of issues. Function calling suffers from the same prompt injection issues. Malicious actors can pass malicious code in function or the responses. For example, creation of queries using functions might contain malicious code to delete data. Without proper user validation this code will be executed automatically and delete data. So, using LLM as the back-end layer needs proper security implementation. 

## 2. Meta's LLM: Commercial Use Ahead

Llama has been a boon for the open source community. Many of the open source models rely on Llama. The issue is that Llama is research-only and cannot be used commercially. So, no one can use it to build any product.

[Meta is now working on the next version of the model. This model will be available for commercial use.](https://www.theinformation.com/articles/meta-wants-companies-to-make-money-off-its-open-source-ai-in-challenge-to-google) This is in stark contrast to both OpenAI and Google. Both safe-guarde their models and make it available through API. 

**Why is it important?** Certain industries cannot use LLM APIs because of strict restrictions on data privacy. These companies would want to run their own instance of a foundational model. 

A commercially available foundational model is also going to help people who want to keep their “API call” costs next to 0. 

A commercially available free-for-all model will also help push the open source community further. Just like Llama.

**What’s next?** Sam Altman has said OpenAI didn’t release GPT-3 as open-source because they [didn’t think people would be able to run it.](https://gptweekly.beehiiv.com/p/peek-openais-future) Now [OpenAI is working on an open-source model.](https://gptweekly.beehiiv.com/p/caution-chatgpt-plugins) This is going to be weaker than GPT-4. 

Let the battle of LLMs begin.  

## 3. EU's Proposed Legislation and Its Impact on AI Usage

[The EU parliament voted to move ahead with the E.U. AI Act.](https://www.washingtonpost.com/technology/2023/06/14/eu-parliament-approves-ai-act/) This act aims to ensure consumer protection against the dangers of AI.  

**Why is it important?** [OpenAI](https://gptweekly.beehiiv.com/p/peek-openais-future) and [Sam Altman](https://gptweekly.beehiiv.com/p/caution-chatgpt-plugins) want regulations for models. They have proposed a IAEA-type of agency to stop the proliferation of LLM models. As per OpenAI, all models should be regulated and monitored. The suggestion of a license based regulation has led to significant backlash. Many people have called it “regulatory capture” - with the aim of shutting down competing LLMs.

[Licensing based regulations might not really be effective.](https://aisnakeoil.substack.com/p/licensing-is-neither-feasible-nor)

The EU is approaching regulation from a different angle. It doesn’t focus on how models are developed. Rather focuses on how AI will/can be used. They have broken down use cases into 4 categories - unacceptable (prohibited), high, medium and low risk. For example, 

Building a [Pre-Crime software](https://en.wikipedia.org/wiki/Pre-crime#:~:text=Pre%2Dcrime%20(or%20precrime),on%20crimes%20not%20yet%20committed.) to predict crimes? Building a [Social credit system](https://en.wikipedia.org/wiki/Social_Credit_System)?  Unacceptable.

Using tools to influence elections or recommendation algorithms? High (Highly regulated).

Using generative AI tools to create text or images on news sites? Medium (Add label that the content is AI generated) 

AI providers also need to disclose their training source.

To me this sounds like good legislation. What do you guys think?

But, OpenAI has warned that EU regulations might force them to pull out completely.

**What’s next?** The disclosure requirements might help various publishing companies. [AI and media companies are in talks to pay for training data](https://www.ft.com/content/79eb89ce-cea2-4f27-9d87-e8e312c8601d). Google has been leading the charge. 

Additionally, [OpenAI and Deepmind will open their models for safety and research purposes to the UK government.](https://www.politico.eu/article/openai-deepmind-will-open-up-models-to-uk-government/) 

# 🗞️10 AI news highlights and interesting reads

1. **PSA:** If you are using Repl to write code, you might want to check your OpenAI API keys. If you have left them embedded then [people can pirate and steal the keys. ](https://www.vice.com/en/article/93kkky/people-pirating-gpt4-scraping-openai-api-keys)
2. LLMs rely on human annotation or human feedback to learn. And one way to generate human annotation is crowdsourcing. But what if the crowdsource human annotators use LLMs? [Research shows 33-46% workers used LLMs](https://arxiv.org/abs/2306.07899). So, basically we go from Human -> AI -> Human -> AI. The AI ouroboros. Researchers also say [generated data to train models might cause serious issue.  ](https://arxiv.org/abs/2305.17493)
3. All the talks about [moats](https://gptweekly.beehiiv.com/p/googles-startling-leaked-memo-george-hinton-mojo) \- [Reddit might be OpenAI’s \*future\* moat](https://www.cyberdemon.org/2023/06/14/reddit-moat.html). Given the amount of complaints about how [Google search](https://www.techradar.com/opinion/the-reddit-b) [experience has deteriorated](https://www.theverge.com/2023/6/13/23759942/google-reddit-subreddit-blackout-protests) [during the blackout](https://news.ycombinator.com/item?id=36345345), this might be true?
4. [Doctors are using ChatGPT](https://www.nytimes.com/2023/06/12/health/doctors-chatgpt-artificial-intelligence.html) but not to diagnose.Rather to be [more empathetic](https://inflecthealth.medium.com/im-an-er-doctor-here-s-how-i-m-already-using-chatgpt-to-help-treat-patients-a023615c65b6). [We discussed this just a month ago](https://today.ucsd.edu/story/study-finds-chatgpt-outperforms-physicians-in-high-quality-empathetic-answers-to-patient-questions?utm_source=gptweekly.beehiiv.com&utm_medium=referral&utm_campaign=google-s-startling-leaked-memo-george-hinton-mojo-and-more). And guess where the data for this study came from? Reddit AskDocs. Moat FTW?!
5. Beatles to make a comeback…[using Generative AI](https://www.semafor.com/article/06/13/2023/paul-mccartney-beatles-song-ai). 
6. [SnapFusion - Text to Image diffusion on mobile phones.](https://snap-research.github.io/SnapFusion/)
7. Large context lengths are important for better GPT experience. [The secret sauce for 100k context length](https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c). 
8. There is a lot of bad AI research out there. Some border on snake oil. Most AI “research” should be double checked and challenged. A new research on huggingface said that [GPT-4 can ace MIT curriculum](https://huggingface.co/papers/2306.08997). Now someone is replicating the results and say that [GPT-4 can’t beat MIT. ](https://flower-nutria-41d.notion.site/No-GPT4-can-t-ace-MIT-b27e6796ab5a48368127a98216c76864)
9. Are we seeing peak AI? Especially when people from Deepmind and Meta are involved? [Mistral AI raised $113 million in seed round with no product.](https://techcrunch.com/2023/06/13/frances-mistral-ai-blows-in-with-a-113m-seed-round-at-a-260m-valuation-to-take-on-openai/) Some might say this funding is for the team and the team is really solid. The issue though is whether the valuation is justified when OpenAI and Google already have a head start.
10. [The AI Hype Wall of Shame.](https://criticalai.org/the-ai-hype-wall-of-shame/) \- Collection of articles which mislead people about AI in various aspects.

# 🧑‍🎓3 Learning Resources

1. [Building and Launching a company using GPT-4](https://sabol.io/c7921c7bbd8c4982aacbd2b71a8b9bb3) with prompts. (The author didn’t know how to code but created and launched the MVP in a month).  
2. Chatbot for your Gdrive - [https://www.haihai.ai/gpt-gdrive/](https://www.haihai.ai/gpt-gdrive/)
3. Building ChatGPT plugin using Supabase - https://supabase.com/blog/building-chatgpt-plugins-template

That’s it folks. Thank you for reading and have a great week ahead.

**If you are interested in a focused weekly recap delivered to your inbox on Mondays you can**[ subscribe here. It is FREE!](https://gptweekly.beehiiv.com/subscribe)"
41,learnmachinelearning,llm,top,2023-03-31 01:09:30,How should I go about publishing a dataset so other engineers/scientists will use it?,tylersuard,False,0.97,21,1275el6,https://www.reddit.com/r/learnmachinelearning/comments/1275el6/how_should_i_go_about_publishing_a_dataset_so/,9,1680224970.0,"Hello.  I have a dataset that could be really helpful to a lot of researchers, particularly in the LLM field.  How and where should I post it to get their attention?"
42,learnmachinelearning,llm,top,2023-09-04 22:44:05,How do I know what compute resources to use for training my LLM.,Radiakbar,False,0.96,19,16a6mvb,https://www.reddit.com/r/learnmachinelearning/comments/16a6mvb/how_do_i_know_what_compute_resources_to_use_for/,4,1693867445.0,"I created this seq-to-seq transformer model for machine translation and it has 63M parameters. I plan to rent an ec2 instance to train the model, but I don't know what type of instance to rent. Are there any papers that tells you what setup to use given the number of model parameters, task, and time for training? Any help on this would be great!"
43,learnmachinelearning,llm,top,2023-09-01 14:58:08,This week in AI - all the Major AI development in a nutshell,wyem,False,0.96,18,1679g8z,https://www.reddit.com/r/learnmachinelearning/comments/1679g8z/this_week_in_ai_all_the_major_ai_development_in_a/,0,1693580288.0,"1. Researchers introduce ‘**Swift**’, the first autonomous vision-based drone that beat human world champions in several fair head-to-head races. This marks the *first* time that an autonomous mobile robot has beaten human champions in a real physical sport \[[*Details*](https://www.nature.com/articles/s41586-023-06419-4)\].
2. **Meta AI** released **CoTracker** \- a fast transformer-based model that can track any point in a video.
3. **WizardLM** released **WizardCoder 34B** based on Code Llama. WizardCoder-34B surpasses GPT-4, ChatGPT-3.5 and Claude-2 on HumanEval Benchmarks.
4. **Meta AI** introduced **FACET** (FAirness in Computer Vision EvaluaTion) - a new comprehensive benchmark dataset for evaluating the fairness of computer vision models for protected groups. The dataset is made up of 32K images containing 50,000 people, labeled by expert human annotators.
5. **Allen Institute for AI** launched [**Satlas**](https://satlas.allen.ai/) \- a new platform for exploring global geospatial data generated by AI from satellite imagery.
6. Generative AI updates from **Google Cloud Next** event**:**
   1. General availability of **Duet AI in Google Workspace** .
   2. **SynthID** \- a tool for watermarking and identifying AI images generated by Imagen (Google’s text-to-image diffusion model). It embeds a digital watermark directly into the pixels of an image, making it invisible to the human eye, but detectable for identification, without reducing the image quality.
   3. **AlloyDB AI** for building generative AI applications with PostgreSQL.
   4. **Vertex AI’s Model Garden** now includes Meta’s Llama 2 and TII’s Falcon — and pre-announcement of Anthropic’s Claude 2..
   5. Model and tuning upgrades for **PaLM 2, Codey, and Imagen**. 32,000-token context windows and 38 languages for PaLM 2.
   6. **Style Tuning** for Imagen - a new capability to help customers align their images to their brand guidelines with 10 images or less.
   7. Launch of fifth generation of its tensor processing units (**TPUs**) for AI training and inferencing.
7. A new generative AI image startup **Ideogram**, founded by former Google Brain researchers, has been launched with $16.5 million in seed funding. Ideogram's unique proposition lies in reliable text generation within images.
8. **a16z** announced **a16z Open Source AI Grant program** and the first batch of grant recipients and funded projects.
9. **Runway AI** announced **Creative Partners Program** \- provides a select group of artists and creators with exclusive access to new Runway tools and models, Unlimited plans, 1 million credits, early access to new features and more.
10. **OpenAI** has released a guide for teachers using ChatGPT in their classroom—including suggested prompts, an explanation of how ChatGPT works and its limitations, the efficacy of AI detectors, and bias.
11. **DINOv2**, a self-supervised vision transformer model by **Meta AI** which was released in April this year, is now available under the Apache 2.0 license.
12. **Tesla** is launching a $300 million AI computing cluster employing 10,000 Nvidia H100 GPUs.
13. **Inception**, an AI-focused company based in the UAE unveiled **Jais**, a 13 billion parameters open-source Arabic Large Language Model (LLM).
14. Google announced **WeatherBench 2** (WB2) - a framework for evaluating and comparing various weather forecasting models.
15. **Alibaba** launched two new open-source models - **Qwen-VL** and **Qwen-VL-Chat** that can respond to open-ended queries related to different images and generate picture captions.
16. **OpenAI** disputes authors’ claims that every ChatGPT response is a derivative work.
17. **DoorDash** launched AI-powered voice ordering technology for restaurants.
18. **OpenAI** launched **ChatGPT Enterprise**. It offers enterprise-grade security and privacy, unlimited higher-speed GPT-4 access, longer context windows for processing longer inputs, advanced data analysis capabilities and customization options.
19. **OpenAI** is reportedly earning $80 million a month and its sales could be edging high enough to plug its $540 million loss from last year.

If you like this news format, you might find my newsletter, [AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
44,learnmachinelearning,llm,top,2023-05-15 21:21:01,Resource for creating your own personal ChatGPT tailored to your own data,rajatarya,False,0.78,16,13ikxwt,https://www.reddit.com/r/learnmachinelearning/comments/13ikxwt/resource_for_creating_your_own_personal_chatgpt/,6,1684185661.0,"Hey everyone,  


I was trying to create a personal ChatGPT that can answer questions and create expert content based on an existing dataset. I thought there are tons of applications for this, so [I created a workshop](https://app.livestorm.co/xethub/mygpt-free-workshop-build-a-chatgpt-clone-tailored-to-your-data?type=detailed&utm_source=reddit&utm_medium=social&utm_campaign=openaireddit) so you can create your own app - I’m calling it “MyGPT”.  


In this workshop I’ll be covering:

* How to create a Generative AI app using the DaVinci model (the same one used by ChatGPT) 
* How a Generative AI application is structured (the tech stack)
* Integrating your own data into a Large Language Model (LLM)
* Getting started with XetHub (similar to GitHub but easier for ML models)
* Create a Python app that uses Gradio & LangChain

If you’d like to check it out, [sign up here](https://app.livestorm.co/xethub/mygpt-free-workshop-build-a-chatgpt-clone-tailored-to-your-data?type=detailed&utm_source=reddit&utm_medium=social&utm_campaign=openaireddit)!"
45,learnmachinelearning,llm,top,2022-12-18 08:16:46,"Looking for good learning sources around generative AI, specifically LLM",Global_Lab8010,False,0.96,17,zotnbu,https://www.reddit.com/r/learnmachinelearning/comments/zotnbu/looking_for_good_learning_sources_around/,10,1671351406.0,"Are there any good video content sources that explains all the concepts associated with generative AI (ex: RL, RLHF, transformer, etc) from the ground up in extremely simple language (using analogies/stories of things that would be familiar to say a 10-12 year old)? Also would prefer channels which explain the concepts in a sequential manner (so that easy to follow) and make short and crisp videos

If yes, could you kindly comment below with the suggestions. 
If not, could you comment whether something like that would be useful to you and ideally why also?

Big thanks in advance 🙏"
46,learnmachinelearning,llm,top,2023-07-20 13:15:51,Free courses and guides for learning Generative AI,wyem,False,0.95,17,154qnsh,https://www.reddit.com/r/learnmachinelearning/comments/154qnsh/free_courses_and_guides_for_learning_generative_ai/,0,1689858951.0,"1. **Generative AI learning path by Google Cloud.** A series of 10 courses on generative AI products and technologies, from the fundamentals of Large Language Models to how to create and deploy generative AI solutions on Google Cloud \[[*Link*](https://www.cloudskillsboost.google/paths/118)\].
2. **Generative AI short courses** by DeepLearning.AI - Five short courses on generative AI including **LangChain for LLM Application Development, How Diffusion Models Work** and more. \[[*Link*](https://www.deeplearning.ai/short-courses/)\].
3. **LLM Bootcamp:** A series of free lectures by *The full Stack* on building and deploying LLM apps \[[*Link*](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/)\].
4. **Building AI Products with OpenAI** \- a free course by CoRise in collaboration with OpenAI \[[*Link*](https://corise.com/course/building-ai-products-with-openai)\].
5. Free Course by Activeloop on **LangChain & Vector Databases in Productio**n \[[*Link*](https://learn.activeloop.ai/courses/langchain)\].
6. **Pinecone learning center -** Lots of free guides as well as complete handbooks on LangChain, vector embeddings etc. by Pinecone **\[**[**Link**](https://www.pinecone.io/learn/)**\].**
7. **Build AI Apps with ChatGPT, Dall-E and GPT-4  -** a free course on Scrimba **\[**[*Link*](https://scrimba.com/learn/buildaiapps)**\].**
8. **Gartner Experts Answer the Top Generative AI Questions for Your Enterprise** \- a report by Gartner \[[*Link*](https://www.gartner.com/en/topics/generative-ai)\]
9. **GPT best practices:** A guide by *OpenAI* that shares strategies and tactics for getting better results from GPTs \[[*Link*](https://platform.openai.com/docs/guides/gpt-best-practices)\].
10. **OpenAI cookbook by OpenAI -** Examples and guides for using the OpenAI API **\[**[*Link*](https://github.com/openai/openai-cookbook/tree/main)**\].**
11. **Prompt injection explained**, with video, slides, and a transcript from a webinar organized by LangChain \[[*Link*](https://simonwillison.net/2023/May/2/prompt-injection-explained/)\].
12. A detailed guide to **Prompt Engineering by DAIR.AI** *\[*[*Link*](https://www.promptingguide.ai/)*\].*
13. What Are **Transformer Models** and How Do They Work - A tutorial by **Cohere AI** \[[*Link*](https://txt.cohere.ai/what-are-transformer-models/)\].  


I add learning resources as part of my AI newsletter. You can join for free [here](https://aibrews.com/). It’s sent only once a week with bite-sized news, learning resources and selected tools. "
47,learnmachinelearning,llm,top,2024-02-04 09:43:51,While Fine tuning Mistral 7B - Colab GPU gave up on me,Paperplaneflyr,False,1.0,17,1aikbct,https://www.reddit.com/r/learnmachinelearning/comments/1aikbct/while_fine_tuning_mistral_7b_colab_gpu_gave_up_on/,4,1707039831.0,"As every person trying to learn ML, I tried to understand LLM and fine-tune it.

Followed along with this tutorial: [https://www.datacamp.com/tutorial/mistral-7b-tutorial](https://www.datacamp.com/tutorial/mistral-7b-tutorial)  
But ended up choosing a data set that was too big I guess: 10K rows. The tutorial had 1K records

Everything went well, until the training step and started training. The ETA showed 4 hrs. I thought it would make it but Colab shut it down after 1hr 16 mins. I guess free tier has limitations :P  

Anyway, learned a bunch of stuff along the way like  PEFT LoRA, QLoRA. Plus about monitoring using wandb."
48,learnmachinelearning,llm,top,2023-06-16 14:23:32,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.94,16,14ay75a,https://www.reddit.com/r/learnmachinelearning/comments/14ay75a/this_week_in_ai_all_the_major_ai_developments_in/,1,1686925412.0,"1. **ElevenLabs** has launched **AI Speech Classifier -** an authentication tool that lets you upload any audio sample to identify if it contains ElevenLabs AI-generated audio.
2. **Nvidia Research** presents **SceneScape** \- a method to generate long-term walkthroughs in imaginary scenes just from an input text prompt.
3. **Meta AI** introduces the **Image Joint Embedding Predictive Architecture (I-JEPA)**, a new AI model which learns from the world like humans and excels in computer vision tasks, while being more computationally efficient. It learns by creating an internal model of the outside world, which compares abstract representations of images (rather than comparing the pixels themselves). It can also be used for many different applications without needing extensive fine tuning. Meta is open-sourcing the code and model checkpoints.
4. **Meta** wants to make the next version of LLaMA, its open source LLM, available for commercial use..
5. Adobe launched **Generative Recolor,** a new tool powered by Adobe Firefly generative AI that lets you generate custom color schemes using texts prompt like “strawberry fields,” “faded emerald,” etc. .
6. **OpenAI** announced:
   1. new **function calling** capability in the Chat Completions API
   2. updated and more steerable versions of gpt-4 and gpt-3.5-turbo
   3. new 16k context version of gpt-3.5-turbo (vs the standard 4k version). 16k context means the model can now support \~20 pages of text in a single request.
   4. cost reductions: 75% on embeddings model and 25% cost on input tokens for gpt-3.5-turbo.
7. **Meta AI** released **MusicGen** \- an open-source music generation model that can be prompted by both text and melody. See here for generated samples and comparison with Google’s MusicLM and others..
8. **McKinsey** published a report ‘*The economic potential of generative AI: The next productivity frontier*’ . The report estimates that generative AI could add the equivalent of $2.6 trillion to $4.4 trillion annually across the 63 use cases. About 75 percent of the value that generative AI use cases could deliver falls across four areas: Customer operations, marketing and sales, software engineering, and R&D..
9. **EU lawmakers** pass AI regulation, requiring generative AI systems, such as ChatGPT, to be reviewed before commercial release. It also seeks to ban real-time facial recognition.*.*
10. **Google Lens** can now identify skin conditions. Lens will also be integrated with Bard, Google’s AI-powered chatbot, enabling Bard to understand images in user prompts..
11. **AMD** announced its most-advanced GPU for artificial intelligence, the MI300X, which will start shipping to some customers later this year*.*
12. **Vercel** introduced **Vercel AI SDK -** an open-source library to build conversational, streaming and chat user interfaces. Includes first-class support for OpenAI, LangChain, and Hugging Face Inference.
13. **Vercel** announced '**Vercel AI Accelerator,** a 6-week long accelerator program with $850k in free credits from OpenAI, Replicate and others.
14. **Salesforce** announces **AI Cloud** \- generative AI for the enterprise. AI Cloud includes the new **Einstein Trust Layer**, to help prevent large-language models (LLMs) from retaining sensitive customer data.
15. **Cohere** and **Oracle** are working together to make it easy for enterprise customers to train their own specialized large language models while protecting the privacy of their training data.
16. **Coda** released Coda AI - the AI-powered work assistant integrated in Coda to automate workflows. Coda also announced ‘**Coda's AI at Work Challenge**’, offering $40,000 in total prizes to the makers who submit the most useful Coda AI template to the Coda Gallery.
17. **OpenAI, Google DeepMind and Anthropic** have committed to provide “early or priority access” to their AI models to UK in order to support research into evaluation and safety.

If you like this news format, you might find my  [newsletter](https://aibrews.com/) helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
49,learnmachinelearning,llm,top,2023-07-25 11:29:20,New Open Source LLM: GOAT-7B🚀 (SOTA among the 7B models),rempact,False,0.94,16,1595syg,https://www.reddit.com/r/learnmachinelearning/comments/1595syg/new_open_source_llm_goat7b_sota_among_the_7b/,2,1690284560.0,"Go try this free model. 7B SOTA by MMLU and BBH 

https://preview.redd.it/632dgspmj3eb1.png?width=1570&format=png&auto=webp&s=3ecb1a9147802a9f6eb6420d1f48cae4b90831e2"
50,learnmachinelearning,llm,top,2023-11-10 15:29:14,"Looking to build an LLM POC for my company, where do I begin?",fuzedmind,False,0.95,17,17s6ena,https://www.reddit.com/r/learnmachinelearning/comments/17s6ena/looking_to_build_an_llm_poc_for_my_company_where/,26,1699630154.0,"Hello,

I am currently a devops engineer but do have experience in programming. Some data engineering, no ML yet.

We have a vast trove of Google Docs and Sheets in our company that I think could be used to create an efficient search product using an LLM. I would like to essentially feed all this information to a model and be able to ask it where certain documents are, who updated them, links etc. Something simple.

I have looked around a little and saw that Langchain could be helpful here. We have a data engineering team that is indexing all of our documentation so that part is solved already. It looks like I may have to look into Vector databases as well.

Ultimately I just want somewhere to learn how this is done. There is tons of stuff out there and you folks would probably know best."
51,learnmachinelearning,llm,top,2023-10-07 21:20:01,What would be the best option to utilize a GPU remotely?,AbstractContract,False,0.86,14,172h7ya,https://www.reddit.com/r/learnmachinelearning/comments/172h7ya/what_would_be_the_best_option_to_utilize_a_gpu/,6,1696713601.0,"Specifically I want to be able to use my desktop equipped with a 3060 at home for LLM inference from my laptop and Raspberry Pi based devices. I can connect to my home network via tailscale, but am rather unsure how to access that GPU power easily. The best way I could think of would be using wake on LAN to power up the desktop and then remotely access it to execute code and use mdels hosted on that machine. Would there be a better way and is there a recommended setup for this kind of scenario? "
52,learnmachinelearning,llm,top,2023-12-10 17:51:18,Need a roadmap for LLMs.,Competitive_Pin_5580,False,0.94,15,18f91n0,https://www.reddit.com/r/learnmachinelearning/comments/18f91n0/need_a_roadmap_for_llms/,9,1702230678.0,"As the title says. I'm quite familiar with concepts of ML and DL: read a few books, done a Lotta projects, especially utilizing Random Forests, CNNs and LSTMs. Not as many projects on NLP.

Now I want to get into LLMs from the point of view of being a viable candidate for companies hiring interns for LLM projects. Since it's a new field, I don't really have a roadmap. A roadmap and links to courses, free or paid alike, are much appreciated."
53,learnmachinelearning,llm,top,2023-07-17 20:00:39,"How to start learning generative AI? It seems out of reach for most of us as the models are too large for consumer GPU. Is training via collab and cloud, reasonably affordable?",easysunnysideup,False,0.95,15,152c9tu,https://www.reddit.com/r/learnmachinelearning/comments/152c9tu/how_to_start_learning_generative_ai_it_seems_out/,10,1689624039.0,It seems many guides describe taking an opensource model and fine tuning the last parts of the LLM. Is this doable for consumer GPU?
54,learnmachinelearning,llm,top,2024-02-09 13:50:47,Why can't LLMs write novels? :technically:,TheGreatGanarby,False,0.7,15,1amokuu,https://www.reddit.com/r/learnmachinelearning/comments/1amokuu/why_cant_llms_write_novels_technically/,65,1707486647.0," 

I've been fascinated with the LLMs since GPT3 was released, to the point that I've re-enrolled in school for a CS degree, but I'd like to know the technical aspects of the problem I really want to solve.

The Problem - Why can't an LLM keep a long narrative? Why can't they write a coherent novel? or even 30 minute sit-com?

Thank in advance, I know this is probably a simple answer, but I'd love to hear it explained with jargon I can learn from."
55,learnmachinelearning,llm,top,2023-05-25 17:57:47,What NLP/NLU tasks are Generative LLMs bad at?,Smallpaul,False,1.0,15,13rok24,https://www.reddit.com/r/learnmachinelearning/comments/13rok24/what_nlpnlu_tasks_are_generative_llms_bad_at/,7,1685037467.0,"If we put aside questions of cost, what are examples of tasks that are better handled by non-LLM NLP techniques? (I'm including fine-tuned LLMs in the definition of ""LLM"")"
56,learnmachinelearning,llm,top,2023-11-16 15:05:20,Training an LLM to have my friends personality,travy_burr,False,0.89,14,17wp1p7,https://www.reddit.com/r/learnmachinelearning/comments/17wp1p7/training_an_llm_to_have_my_friends_personality/,16,1700147120.0,"Im a Software Engineer looking to learn a bit about ML, and decided a fun first project would be to train an LLM that has my friend's personality.

I have about 22,000 discord messages from my friend, stored in json format. I could get maybe a few thousand more.

So far, I've been able to get the model to use my friends (lets call him Dylan) words and generally have his personality, but it still isn't forming coherent responses. For example, to the question ""What's your opinion on Steve?"" Dypan's LLM might respond ""Steve has the skill to be a good player, but isn't quite there yet. He has the potential to be a pro"". But to the question ""What's your favorite game?"" It would respond ""it's a good game and I had fun playing it, but I don't know if it's a good game"". Pretty nonsensical.

My LLM is fine tuned using GPT2. I trained it for roughly 9.5 hours overnight on a 3080, with a batch size of 32 and gradient accumulation steps at 32. The training resulted in a loss of 4.09. From what I understand, this loss is extremely high.

I think it would be better if I included messages from other people - essentially giving the LLM context (this is how Dylan responds to these words). Can any provide guidance on how to do this? I've done research but can't seem to find anything helpful.

Thank you in advance!"
57,learnmachinelearning,llm,top,2023-10-01 20:37:56,LLM Firewall - Guardrail Tutorial and Quickstart with OpenAI and Colab,Educational_Grass_38,False,1.0,14,16xc53k,https://m.youtube.com/watch?v=EnwVnz07h1I&pp=ygUSR3VhcmRyYWlsIEZpcmV3YWxs,5,1696192676.0,"Been working on a Firewall for devs to use in a few lines of code, to implement a protective layer around LLMs like OpenAI. Firewall has over 20+ detectors out-of-the-box including prompt injections, harmful content, toxicity and common security vulnerabilities.

Google Colab QuickStart: https://github.com/guardrail-ml/guardrail

Developer Docs: https://docs.useguardrail.com

Would appreciate if you could give a star and provide feedback, thanks!"
58,learnmachinelearning,llm,top,2023-07-19 16:01:34,Ensuring Reliable Few-Shot Prompt Selection for LLMs,cmauck10,False,0.95,14,153z22n,https://www.reddit.com/r/learnmachinelearning/comments/153z22n/ensuring_reliable_fewshot_prompt_selection_for/,0,1689782494.0,"Hello Redditors!

It's pretty well known that LLMs have firmly established themselves as leaders in the field of natural language processing, consistently pushing the limits of language comprehension and generation, which is widely acknowledged.

I spent a little time playing around with few-shot prompting for OpenAI's Davinci model and I discovered that noisy data still has drastic effects even on powerful LLMs like Davinci.

[mislabeled few-shot examples harms LLM performance drastically](https://preview.redd.it/9quf4bvk2ycb1.png?width=1994&format=png&auto=webp&s=cfbec1b30ffbaa592011355c503a568fb6c98148)

I wrote up a [quick article](https://www.kdnuggets.com/2023/07/ensuring-reliable-fewshot-prompt-selection-llms.html) in KDNuggets that shows how I used data-centric AI to automatically clean the noisy few-shot examples pool in order to achieve more accurate predictions. The resulting few-shot prompt with accurately labeled examples produced **20% fewer errors** than the original one with mislabeled examples.

This one was quite eye-opening for me and I hope you find it is as interesting as I did. Let me know what you think!"
59,learnmachinelearning,llm,top,2023-08-29 22:37:15,Finetuning an LLM to imitate someone,Vanishing-Rabbit,False,0.89,13,164wtmm,https://www.reddit.com/r/learnmachinelearning/comments/164wtmm/finetuning_an_llm_to_imitate_someone/,9,1693348635.0,"Hello all,

I'm trying to understand how to get an LLM to imitate someone, say Shakespeare. It's easy enough to get all of [Shakespeare's work](https://www.gutenberg.org/ebooks/author/65).

If I've understood the current state of play for LLMs, there are three options:

* Fine tune an LLM
* Vectorize your knowledge using something like ChromaDB. Do a similarity search after each prompt and get the LLM to ""read"" the top n docs
* Do both

I have a feeling that to imitate Shakespeare, fine tuning an LLM might work best.

However, if my understanding is correct, the inputs to finetune an LLM must be formatted this way:

    <human>: ""To be""
    <system>: ""Or not to be""
The gap I'm having trouble bridging is how do I go from a large text file to this input format? The only idea I've come across is format all of the text like so:

    <human>: ""sentence_1""
    <system>: ""sentence_2""

    <human>: ""sentence_2""
    <system>: ""sentence_3""
Are there best practices around this problem? How should I be thinking about this?

I've seen companies like character.ai create bots that imitate [Elon Musk] (https://c.ai/c/6HhWfeDjetnxESEcThlBQtEUo0O8YHcXyHqCgN7b2hY) accurately for example so I know it's doable. I just wonder if they've done it by finetuning an LLM or training one from scratch or something else entirely."
60,learnmachinelearning,llm,top,2023-06-17 06:28:34,Open source instruction training dataset for a joke telling LLM,MiddletownBooks,False,0.94,13,14bjfq1,https://www.reddit.com/r/learnmachinelearning/comments/14bjfq1/open_source_instruction_training_dataset_for_a/,2,1686983314.0,"I've been working for a while on getting a training dataset for a funny chatbot for entry into Chai Research's  Guanaco LLM Competition. However, without any prior knowledge of the procedures for training a LLM, it looks like I won't be able to enter the competition this time around - the learning curve is apparently too steep for me right now. So, I've put my dataset on huggingface @ [https://huggingface.co/datasets/Middletownbooks/joke\_training](https://huggingface.co/datasets/Middletownbooks/joke_training) under  an MIT license in case anyone wants to incorporate it into their own chatbot training. I categorized a few thousand (out of \~10K) jokes from a file of reddit jokes and added contextual joke training, some expert crafted punchlines for news headlines and summaries and some conversational instructions."
61,learnmachinelearning,llm,top,2023-07-08 16:27:26,Can an average person learn how to build a LLM model?,UpvoteBeast,False,0.83,12,14u8mmm,https://www.reddit.com/r/learnmachinelearning/comments/14u8mmm/can_an_average_person_learn_how_to_build_a_llm/,4,1688833646.0,"Recently, while using ChatGPT, I had a dream for the first time. I want to create a chatbot that can provide a light comfort to people who come for advice. I would like to create an LLM model using Transformer, and use our country's beginner's counseling manual as the basis for the database.

I am aware that there are clear limits to the level of comfort that can be provided. Therefore, if the problem is too complex or serious for this chatbot to handle, I would like to recommend the nearest mental hospital or counseling center based on the user's location. And, if the user can prove that they have visited the hospital (currently considering a direction where the hospital or counseling center can provide direct certification), I would like to create a program that provides simple benefits (such as a free Starbucks coffee coupon).

I also thought about collecting a database of categories related to people's problems (excluding personal information) and selling it to counseling or psychiatric societies. I think this could be a great help to these societies.

The problem is that I have never studied ""even once,"" and I feel scared and fearful of the unfamiliar sensation. I have never considered myself a smart person.

However, I really want to make this happen! Our country is now in a state of constant conflict, and people hate and despise each other due to strong propaganda.

As a result, the birth rate has dropped to less than 1%, leading to a decline in the population. Many people hide their pain inside and have no will to solve it. They just drink with their friends to relieve their pain. This is obviously not a solution. Therefore, Korea has a really serious suicide rate.

I may not be able to solve this problem, but I want to put one small brick to build a big barrier to stop hatred. Can an ordinary person who knows nothing learn the common sense and study needed to build an LLM model? And what direction should one take to study one by one?"
62,learnmachinelearning,llm,top,2024-02-20 14:38:13,GPU vs TPU: Which One is Better?,Which_Pin_6386,False,0.86,10,1avj5ed,https://www.reddit.com/r/learnmachinelearning/comments/1avj5ed/gpu_vs_tpu_which_one_is_better/,9,1708439893.0," 

Hello, I would like to understand which option is better for deploying the same LLM (it could be LLAMA or some other).

This comparison would involve deploying it on AWS, Google Cloud, and a local setup (Mac or with an NVidia RTX 4090).

Considerations to take into account would include:

* Interoperability between LLM models
* Efficiency
* Cost
* Other

Can you help me? Thank you!"
63,learnmachinelearning,llm,top,2024-01-18 14:44:44,Project: QA on any PDF document using RAG and VectorDB,Amazing_Life_221,False,0.82,14,199rq4b,https://i.redd.it/c0cfiqr0o7dc1.jpeg,6,1705589084.0,"The Smart PDF Reader is a comprehensive project that harnesses the power of the Retrieval-Augmented Generation (RAG) model over a Large Language Model (LLM) powered by Langchain. Additionally, it utilizes the Pinecone vector database to efficiently store and retrieve vectors associated with PDF documents. This approach enables the extraction of essential information from PDF files without the need for training the model on question-answering datasets.

Find the GitHub repo: [here](https://github.com/Arshad221b/RAG-on-PDF)"
64,learnmachinelearning,llm,top,2023-09-14 05:10:15,"Which LLM can I run locally on my MacBook Pro M1 with 16GB memory, need to build a simple RAG Proof of Concept.",sf_d,False,1.0,12,16i9g51,https://www.reddit.com/r/learnmachinelearning/comments/16i9g51/which_llm_can_i_run_locally_on_my_macbook_pro_m1/,6,1694668215.0,"I am in the process of building a simple proof of concept for Retrieval-augmented generation (RAG) and would like this to be locally hosted on my MacBook Pro M1 with 16 GB memory.

What options do I have for the LLM's selection?"
65,learnmachinelearning,llm,top,2023-03-21 14:29:47,Doublespeak.chat: an LLM sandbox escape game,Eriner_,False,0.89,13,11xijaq,https://doublespeak.chat,3,1679408987.0,
66,learnmachinelearning,llm,top,2023-03-15 20:18:13,Do multi modal LLM models just inject image description to the context?,ChessGibson,False,0.94,12,11s7ya3,https://www.reddit.com/r/learnmachinelearning/comments/11s7ya3/do_multi_modal_llm_models_just_inject_image/,4,1678911493.0,"Hi! Small question I have been asking myself seeing multiple multi modal models recently: do they use interconnected neural networks for different input types, or do they simply convert non-text inputs into textual descriptions before processing them with their language models? What's happening for PaLM-E for instance? How about GPT-4?"
67,learnmachinelearning,llm,top,2023-12-28 21:25:07,Best model to summarize scientific papers,isgael,False,0.94,12,18t504r,https://www.reddit.com/r/learnmachinelearning/comments/18t504r/best_model_to_summarize_scientific_papers/,0,1703798707.0,"Hi, all

Consider I am a newbie in LLMs. I have \~4k scientific papers  (already in .txt format) I want to get a summary of. I have read the  following things about using LLMs to summarize texts and want your  opinion on what path to take:

&#x200B;

* Summarizing will get you unsatisfactory results and you should stick to the abstract
* The best way is to make summaries of each section and then combine the summaries.
* The LLM will start hallucinating because the text is too long (e.g.,  bart-large-cnn was trained on <1000 words texts, while papers have  >8000 words.
* I have seen Pegasus and LongT5 being mentioned, but no idea about these
* The [textsum](https://github.com/pszemraj/textsum) projects seems to work with texts of arbitrary length, but I don't know if it works well with scientific papers
* [vault-ai](https://github.com/pashpashpash/vault-ai) produces good enough summaries using a [smart approach](https://pashpashpash.substack.com/p/tackling-the-challenge-of-document), but I want a local solution.

I expect the summary to be around one-page long and to be more  detailed than the abstract of the papers, so I wonder whether the  summary-by-section approach would be the best. Also, I don't know if  there's a model specifically designed for scientific papers. My papers  are not math or CS, but do have some equations and chemical formulas,  although I am interested in the text itself, not on specific numerical  results.

Any hint or advice is appreciated."
68,learnmachinelearning,llm,top,2023-06-27 05:44:24,SoundSage - LLM integration (text-to-audio_processing) **Open Source**,S0UNDSAGE,False,1.0,12,14k4s8m,/r/u_S0UNDSAGE/comments/14k4qom/soundsage_llm_integration_texttoaudio_processing/,0,1687844664.0,
69,learnmachinelearning,llm,top,2023-07-12 16:02:00,Assessing the Quality of Synthetic Data with Data-centric AI,cmauck10,False,1.0,12,14xsn63,https://www.reddit.com/r/learnmachinelearning/comments/14xsn63/assessing_the_quality_of_synthetic_data_with/,0,1689177720.0,"Hi Redditors!

Many folks are using LLMs to generate data nowadays, but how do you know which synthetic data is good?

In this article we talk about how you can easily conduct a synthetic data quality assessment! Without writing any code, you can quickly identify which:

* synthetic data is **unrealistic** (ie. low-quality)
* real data is **underrepresented** in the synthetic samples

This tool works seamlessly across synthetic text, image, and tabular datasets. 

If you are working with synthetic data and would like to learn more, check out the [blogpost](https://cleanlab.ai/blog/studio-synthetic-data/) that demonstrates how to automatically detect issues in synthetic customer reviews data generated from the [http://Gretel.ai](https://gretel.ai/) LLM synthetic data generator. "
70,learnmachinelearning,llm,top,2024-01-22 19:04:12,How do you guys evaluate LLM?,UpvoteBeast,False,1.0,11,19d3ep8,https://www.reddit.com/r/learnmachinelearning/comments/19d3ep8/how_do_you_guys_evaluate_llm/,5,1705950252.0,"how do you guys evaluate LLM? There is online leaderboard: [https://huggingface.co/spaces/HuggingFaceH4/open\_llm\_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

Is there any script that can automatically evaluate our performance offline/benchmark?"
71,learnmachinelearning,llm,top,2023-05-24 16:08:51,An intuitive explanation of LLMs,Accomplished_Term23,False,0.93,12,13qptip,https://www.reddit.com/r/learnmachinelearning/comments/13qptip/an_intuitive_explanation_of_llms/,1,1684944531.0,"Hi all, former Reddit ML engineer here. 

I noticed a distinct lack of LLM tutorials that aimed to explain them intuitively, so I wrote up a small post explaining LLMs by analogy.

[https://intuitiveai.substack.com/p/the-four-fundamental-quantities-of](https://intuitiveai.substack.com/p/the-four-fundamental-quantities-of)

Hope you all find it useful! Please let me know if you have any comments or questions."
72,learnmachinelearning,llm,top,2024-01-25 03:47:13,"What is the difference between pre-training, fine-tuning, and instruct-tuning exactly?",stoicbats_,False,0.93,12,19f04y3,https://www.reddit.com/r/learnmachinelearning/comments/19f04y3/what_is_the_difference_between_pretraining/,3,1706154433.0,"I am very new to the LLM field. I have two questions:

1. **What is the difference between pre-training, fine-tuning, and instruct-tuning?** I am not asking for exact definitions. I understand pre-training means training from scratch, but I am confused about fine-tuning and instruct tuning.
2. **If I have my own dataset and want to add new knowledge to an existing model (Mixtral, etc.), should I go with pre-training, instruct tuning, or fine-tuning?** The confusion comes because instruct tuning means I am giving the model instructions to perform a task in a certain way, in a certain format, etc. But are we also adding new knowledge during this?

I meant, if I search for tutorials regarding ""fine-tuning mixtral"" on Google etc, every tutorial and blog says you need to format the data in this format: `{'instruction': '', 'input': """", 'output': """"}` ,  But my question is - isn't this format used for instruct tuning? If so, what is fine-tuning then? I thought fine-tuning was different from instruct tuning.

if I want to fine-tune the mixtral model on two tasks:

1. One task is in English but a new domain (History and Agriculture domains)
2. Mixtral in a new spoken (local) language

&#x200B;

not instruct tuning, I am asking about Fine-tuning mixtral , Then what should my data format be to accomplish fine-tuning for these two new tasks?"
73,learnmachinelearning,llm,top,2023-05-02 17:15:02,How to Fine-Tune OpenAI Language Models with Noisily Labeled Data (37% error reduction),cmauck10,False,1.0,10,135u3vt,https://www.reddit.com/r/learnmachinelearning/comments/135u3vt/how_to_finetune_openai_language_models_with/,0,1683047702.0,"Hello Redditors! 

It's pretty well known that LLMs have solidified their place at the forefront of natural language processing, and are constantly pushing the boundaries of what is possible in terms of language understanding and generation.

I spent some time playing around with the OpenAI fine-tuning API and I discovered that noisy data still has drastic effects even on powerful LLMs like Davinci.

[Improving fine-tuning accuracy by improving data quality.](https://preview.redd.it/v5kro8wzagxa1.png?width=1085&format=png&auto=webp&s=39e0309aa94048dc08a0879d99008f00ec32fd9e)

I wrote up a [quick article](https://www.kdnuggets.com/2023/04/finetuning-openai-language-models-noisily-labeled-data.html) in KDNuggets that shows how I used data-centric AI to automatically clean the noisy data in order to fine-tune a more robust OpenAI LLM. The resulting model has 37% fewer errors than the same LLM fine-tuned on the noisy data.

Let me know what you think!"
74,learnmachinelearning,llm,top,2023-06-30 17:27:56,This week in AI - all the Major AI developments in a nutshell,wyem,False,1.0,10,14n6lwl,https://www.reddit.com/r/learnmachinelearning/comments/14n6lwl/this_week_in_ai_all_the_major_ai_developments_in/,0,1688146076.0,"1. **Microsoft** has launched AI-powered shopping tools in Bing search and Edge, including AI-generated buying guides which automatically aggregate product specifications and purchase locations for user queries​, and AI-generated review summaries that provide concise overviews of online product reviews .
2. **Salesforce AI Research** released **XGen-7B**, a new **open-source** 7B LLM trained on 8K input sequence length for 1.5T tokens.
3. Researchers present **DreamDiffusion**, a novel method for generating high-quality images directly from brain EEG signals without the need to translate thoughts into text.
4. **Google** announced the first *Machine* ***Un****learning Challenge* hosted on Kaggle.
5. **Microsoft** announced a new ***AI Skills Initiative*** that includes free coursework developed with LinkedIn, a new open global grant challenge and greater access to free digital learning events and resources for AI education.
6. **Stability AI** announced **OpenFlamingo V2,** an open-source reproduction of DeepMind's Flamingo model. OpenFlamingo models achieve more than 80% of the performance of their corresponding Flamingo model.
7. **Unity** announces two AI-powered tools: Unity Muse and Unity Sentis. Muse generates animations, 2D sprites, textures etc. in the Unity Editor using text and sketches. Sentis lets you embed an AI model in the Unity Runtime for your game or application. It enables AI models to run on any device where Unity runs..
8. **ElevenLabs** launched **Voice Library** \- a library and community for sharing AI generated voices designed using their *voice Design* tool.
9. **Merlyn Mind** released three **open-source education-specific LLMs**. Merlyn Mind is building a generative AI platform for education where engagement will be curriculum-aligned, hallucination-resistant, and age-appropriate.
10. Amazon's **AWS** has launched a $100 million program, the **Generative AI Innovation Center**, that connects AWS machine learning and artificial intelligence experts with businesses to build and deploy generative AI solutions.
11. New open-source text to video AI model, **Zeroscope\_v2 XL**, released that generates high quality video at 1024 x 576, with no watermarks.
12. Researchers present MotionGPT - a motion-language model to handle multiple motion-relevant tasks.
13. **Databricks** is set to acquire the open-source startup **MosaicML** for $1.3 billion. MosaicML had recently released **MPT-30B,** an open-source model licensed for commercial use that outperforms the original GPT-3 .
14. Generative AI-related job postings in the United States jumped about 20% in May as per Indeed’s data.
15. The source code for the algorithm **DragGAN** (Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold) released and demo available on Huggingface.
16. A new foundation model, **ERNIE** **3.5 b**y China’s Baidu surpassed ChatGPT (3.5) in comprehensive ability scores and outperforms GPT-4 in several Chinese language capabilities.
17. **Adobe** is prepared to pay out any claims in case an enterprise customer loses a lawsuit over the use of content generated by Adobe Firefly, the generative AI image tool.
18. **Google** launched generative AI coding features in Google Colab for Pro+ subscribers in the US.

I didn't add links to news sources here because of auto-mod, but they are included in the newsletter and **you can read the online issue** [**here**](https://aibrews.substack.com/p/ai-generated-buying-guides-in-bing) **without signup**. If you like this news format, you might find my [newsletter](https://aibrews.com/) helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. . Thanks"
75,learnmachinelearning,llm,top,2023-08-26 06:05:42,[Tutorial] Build LLM Playground in <10mins.,VideoTo,False,0.92,9,161mw35,https://www.reddit.com/r/learnmachinelearning/comments/161mw35/tutorial_build_llm_playground_in_10mins/,0,1693029942.0,"**tldr;** [**https://docs.litellm.ai/docs/tutorials/first\_playground**](https://docs.litellm.ai/docs/tutorials/first_playground)

Create a playground to **evaluate multiple LLM Providers in less than 10 minutes**. If you want to see this in prod, check out our [website](https://litellm.ai/).

**What will it look like?**

&#x200B;

https://preview.redd.it/s75jp703bekb1.png?width=1920&format=png&auto=webp&s=84432f4c03833156870a6ed445ac3299ff6564cd

**How will we do this?**: We'll build the server and connect it to our template frontend, ending up with a working playground UI by the end!

&#x200B;

**Tutorial** 👉 [https://docs.litellm.ai/docs/tutorials/first\_playground](https://docs.litellm.ai/docs/tutorials/first_playground)"
76,learnmachinelearning,llm,top,2023-11-25 11:26:13,How I made a Chatbot to speak with YouTube Videos,dev-spot,False,0.85,9,183hrq7,https://www.reddit.com/r/learnmachinelearning/comments/183hrq7/how_i_made_a_chatbot_to_speak_with_youtube_videos/,0,1700911573.0,"Hey,

Given recent advancements in the local LLMs area and how easy it has become, I wrote some code that virtually allows one to chat with YT videos and ask questions about them. The code can be found here:

[https://github.com/devspotyt/chat\_with\_yt](https://github.com/devspotyt/chat_with_yt)

There's also an explanation for the Pythonic code in the README and a reference to a video explaining it. This was way easier than I anticipated, all I had to do is:

1. Set up a Gradio UI with relevant inputs.
2. Extract the video ID from a YT video URL.
3. Use a pythonic package to get a transcript of the video, then convert that transcript to a more ""AI-Friendly"" text.
4. Connect the code with relevant LLMs such as LLama / Mistral via Ollama / HuggingFace inference endpoints which are publicly available (/can run locally).

And that's pretty much it. You can get a short summary of videos, ask when a certain topic was discussed, etc. And the best part is that this is 100% free and can run locally without sharing your data.

The code itself was written in a 1 hour blitz coding session (with the help of a certain LLM ofc), but overall its kinda dope IMO, lmk what you think about it.

cheers"
77,learnmachinelearning,llm,top,2022-08-17 13:21:39,LLM.int8(): Revisiting matrix multiplication at scale to run 8-bit models without performance degradation,younesbelkada,False,0.79,8,wqoutm,https://www.reddit.com/r/learnmachinelearning/comments/wqoutm/llmint8_revisiting_matrix_multiplication_at_scale/,0,1660742499.0,"&#x200B;

[When Hugging Face meets bitsandbytes](https://preview.redd.it/2291itjzq9i91.png?width=1300&format=png&auto=webp&s=5b95836ffe78aaab2462aef0a4bd047a09b81fe5)

[The LLM.int8() paper](https://arxiv.org/abs/2208.07339) was recently integrated at Hugging Face. We think that this represents a big step toward the democratization of large models. BLOOM/OPT-175B can run on only half of the required hardware without suffering performance degradation.

Any model on Hugging Face can now be loaded in 8-bit mode by adding the 'load in 8bit=True' argument when loading the model. T5-11b (44GB in fp32), for example, [can now be run on a Google Colab](https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F#scrollTo=j1s0spY4icGK).

Now you may have several questions:

How does the technique work effectively? What does ""no degradation in performance"" mean? What tools and techniques did we employ to implement this method successfully? Do we surpass native models in speed?

We have tried to address those questions and detailed everything in a gentle blogpost: [https://huggingface.co/blog/hf-bitsandbytes-integration](https://huggingface.co/blog/hf-bitsandbytes-integration) 

Or you can directly deep dive into the paper: [https://arxiv.org/pdf/2208.07339.pdf](https://arxiv.org/pdf/2208.07339.pdf) 

&#x200B;

[An animated overview of LLM.int8\(\) method](https://i.redd.it/4j2io06up9i91.gif)

We would love to have any feedback on the implementation! If you face into any issue, please submit it at: [https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes) or feel free to discuss it here ;) 

Thanks !"
78,learnmachinelearning,llm,top,2023-10-08 22:39:22,Beginner's Guide to LLMs - Non-Technical Guide,Britney-Ramona,False,1.0,10,173c5r3,https://www.reddit.com/r/learnmachinelearning/comments/173c5r3/beginners_guide_to_llms_nontechnical_guide/,1,1696804762.0,"Realized there wasn't a great resource for Beginners/non-technical individuals to understand what Large Language Models are and why they are so powerful, so I wrote [https://datasci101.com/what-are-llms-part-1/](https://datasci101.com/what-are-llms-part-1/) 

There are 4 parts, this is the 1st & arguably the most important for people to get foundational LLM understanding/information.

Worked really hard on this & would appreciate any of your more technical/expert feedback. Thanks!"
79,learnmachinelearning,llm,top,2023-02-14 17:53:16,[P] Practical Steps to Reduce Hallucination and Improve Performance of Systems Built with Large Language Models like ChatGPT,vykthur,False,0.91,9,112bk1o,https://www.reddit.com/r/learnmachinelearning/comments/112bk1o/p_practical_steps_to_reduce_hallucination_and/,2,1676397196.0,"&#x200B;

[Practical steps to reduce hallucination and improve performance of systems built with large language models like ChatGPT](https://preview.redd.it/gksxjpnoz6ia1.png?width=1456&format=png&auto=webp&s=c34531fbe1311eab9323c148eef35fcf0d70decd)

Large language models (LLMs) like the GPT series (GPT3, 3.5, [ChatGPT](https://openai.com/blog/chatgpt/)) can be powerful tools in building useful applications. However, **LLMs are probabilistic** \- i.e., they generate text by learning a probability distribution over words seen during training. For example, given the following words as context “*rise and*”, an LLM can infer that the next word it should generate that fits this context is likely to be “*shine*”. While this setup ensures generated text is **coherent and human-like** (e.g., asking ChatGPT to rewrite the [Serenity Praye](https://en.wikipedia.org/wiki/Serenity_Prayer)r in the style of the [American Constitution](https://www.senate.gov/civics/constitution_item/constitution.htm) yields some intriguing prose), this resulting text may [**not be factual, or just plain incorrect**](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web) **(not grounded in the model’s input or training data) - aka hallucination**. In addition, another limitation of LLMs is that they struggle to address **tasks that need** [**complex multistep reasoning**](https://arxiv.org/pdf/2208.14271.pdf)**.** For example, asking the model to address mathematical word problems or puzzles often requires that the task is decomposed into steps, some computation applied to solve each step and some transformation applied to aggregate results into a final answer; this remains challenging for LLMs.  


Full article: [https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination](https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination) 

This post discusses the following:

* An overview on why hallucination will likely *always be a problem* with LLMs.
* Practical steps developers can take to reduce hallucination and improve performance including:  

   * Low temperature
   * Use of external knowledge bases
   * Chain of thought prompting
   * Self-consistency/voting
   * Task decomposition and agents
   * Correctness probabilities for result filtering
   * Task bench marks
   * Building defensive user interfaces."
80,learnmachinelearning,llm,top,2023-12-24 10:15:41,Do specialized LLMs really need 20+ billion parameters?,NightestOfTheOwls,False,0.91,11,18psibb,https://www.reddit.com/r/learnmachinelearning/comments/18psibb/do_specialized_llms_really_need_20_billion/,7,1703412941.0,"As the title suggests. I feel like current trend in LLM market is to make ""AI to end all AIs"": it can write poems, it can write code, it can write you an essay, it can paraphrase text, it can analyze stocks, it can roleplay, etc. etc.

My question is, isn't it kinda obvious to do what Mistral team and several others are doing and introduce many smaller ""expert"" models that activate based on the nature of request to improve performance?

Alternatively, are there techniques to conveniently ""remove"" unnecessary data from existing models without re-training? For example, I'm not totally sure that a code assist model needs to know what MBTI means and who Dr. Seuss is.

Is it already being done in most popular models and I'm just out of the loop?"
81,learnmachinelearning,llm,top,2023-08-29 03:52:11,"Open-Source CodeLlama Server: Streaming, Caching, Model Fallbacks (OpenAI + Anthropic), Prompt-tracking",VideoTo,False,0.9,7,1647o7n,https://www.reddit.com/r/learnmachinelearning/comments/1647o7n/opensource_codellama_server_streaming_caching/,0,1693281131.0,"**TLDR;** We're open-sourcing our CodeLlama server. It handles streaming, caching, model fallbacks, and tracks prompts + token usage - [https://github.com/BerriAI/litellm/tree/main/cookbook/codellama-server](https://github.com/BerriAI/litellm/tree/main/cookbook/codellama-server)

\~\~

Hello r/learnmachinelearning,

I’m the maintainer of liteLLM() - package to simplify input/output to OpenAI, Azure, TogetherAI, Cohere, Anthropic, Baseten, Hugging face API Endpoints: [https://github.com/BerriAI/litellm/](https://github.com/BerriAI/litellm/)

We're open sourcing our CodeLlama server:

What can our server do? - It uses Together AI's CodeLlama to answer coding questions, with GPT-4 + Claude-2 as backups (you can easily switch this to any model from Huggingface, Replicate, Cohere, AI21, Azure, OpenAI, etc.)

Consistent Input/Output Format - Call all models using the OpenAI format: completion(model, messages) - Text responses will always be available at \['choices'\]\[0\]\['message'\]\['content'\]

* Streaming & Async Support - Return generators to stream text responses
* Error Handling Using Model Fallbacks (if Phind-CodeLlama fails, use Claude-2, fine-tuned GPT-3.5 etc.)
* Logging - It's integrated with promptlayer, so you can automatically track your prompt + model changes there.
* Token Usage & Spend - Track Input + Completion tokens used + Spend/model
* Caching - In-memory + Redis Cache solutions provided (works for streaming too!).

You can deploy liteLLM to your own infrastructure using Railway, GCP, AWS, Azure

Happy completion() !"
82,learnmachinelearning,llm,top,2023-10-28 08:23:24,Why GBDTs are not mentioned in job descriptions?,pg860,False,0.74,9,17i99av,https://www.reddit.com/r/learnmachinelearning/comments/17i99av/why_gbdts_are_not_mentioned_in_job_descriptions/,4,1698481404.0," GBDT allow you to iterate very fast, they require practically no data preprocessing, enable you to incorporate business heuristics directly as features, and immediately show if there is explanatory power in features in relation to the target.

On tabular data problems, they outperform Neural Networks, and many use cases in the industry have tabular datasets.

Because of those characteristics, [they are winning solutions to all tabular competitions on Kaggle](https://jobs-in-data.com/blog/data-science-skills#sota-ml-models)

And yet, somehow they are not very popular.

On the chart below, I summarized learnings from 9,261 job descriptions crawled from 1605 companies in Jun-Sep 2023 (source: [https://jobs-in-data.com/blog/machine-learning-vs-data-scientist](https://jobs-in-data.com/blog/machine-learning-vs-data-scientist))

LGBM, XGboost, Catboost (combined together) are the 19th mentioned skill, e.g. with Tensorflow being x10 more popular.

It seems to me Neural Networks caught the attention of everyone, because of the deep-learning hype, which is justified for image, text, or speech data, but not justified for tabular data, which still represents many use - cases.

Granted, there is for sure some noise in the data generation process of writing job descriptions - some people writing them may not know the exact scope of the role.

But why do those random people know so much more about deep learning, keras, tensorflow, pytorch than GBDT? In other words, why is there a systematic trend in the noise? When the noise has a trend, it ceases to be noise.

https://preview.redd.it/ohef6ocukwwb1.png?width=2560&format=png&auto=webp&s=7c2e0321deb0a9491db09668c94c34d510e77c05

&#x200B;

&#x200B;"
83,learnmachinelearning,llm,top,2023-10-13 15:02:37,Free Open-source ML observability course 🚀,dmalyugina,False,0.9,8,1770s0c,https://www.reddit.com/r/learnmachinelearning/comments/1770s0c/free_opensource_ml_observability_course/,0,1697209357.0,"Hi everyone, I’m one of the people who work on [Evidently](https://github.com/evidentlyai/evidently), an open-source Python library for ML monitoring. I want to share with you our free ML observability course that starts on Oct 16. 

We cover the key concepts of ML monitoring and observability, different types of evaluations, and how to integrate them into ML pipelines. We also look into different ML monitoring architectures and explore how to monitor unstructured data, including LLM and NLP models. 

💻 Code examples and end-to-end deployment blueprints.    
✅ Open-source focused. You’ll work with tools like Evidently, MLflow, Airflow, and Grafana.   
❤️ Free and open to everyone.   
🗓 You can join the cohort that starts on October 16, 2023, or learn at your own pace. 

Course info and notes: [https://learn.evidentlyai.com/](https://learn.evidentlyai.com/) 

Hope you’ll find the course useful!"
84,learnmachinelearning,llm,top,2023-06-11 16:43:03,Large Language Model (LLM) Resources,TheGupta,False,1.0,10,146ymag,https://www.reddit.com/r/learnmachinelearning/comments/146ymag/large_language_model_llm_resources/,0,1686501783.0," **Courses**

* deeplearning.ai
   * [https://learn.deeplearning.ai/chatgpt-prompt-eng](https://learn.deeplearning.ai/chatgpt-prompt-eng/)
   * [https://learn.deeplearning.ai/chatgpt-building-system](https://learn.deeplearning.ai/chatgpt-building-system)
   * [https://learn.deeplearning.ai/langchain](https://learn.deeplearning.ai/langchain/)
* Full Stack Deep Learning
   * [https://fullstackdeeplearning.com/llm-bootcamp/spring-2023](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/)  
[YouTube Playlist](https://www.youtube.com/playlist?list=PL1T8fO7ArWleyIqOy37OVXsP4hFXymdOZ)

**Talks**

* [State of GPT by Andrej Karpathy](https://www.youtube.com/watch?v=bZQun8Y4L2A)
* [Rongyao Huang - Riding the Tailwind of NLP Explosion](https://www.youtube.com/watch?v=2nYhcI7LOi4)

**GitHub Libraries**

* For getting started with LLMs and experimentation
   * [https://github.com/hwchase17/langchain](https://github.com/hwchase17/langchain)
* Other Libraries:
   * [https://github.com/Hannibal046/Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)
   * [https://github.com/FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo)

**Papers**

* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)

&#x200B;

First I posted it on [Kaggle Discussions](https://www.kaggle.com/discussions/general/416483)."
85,learnmachinelearning,llm,top,2023-12-19 21:39:30,Holiday LLM Quick-Start Primer,ACVonnegutteral,False,0.69,7,18mdbbe,https://i.redd.it/njfk72snmb7c1.png,4,1703021970.0,
86,learnmachinelearning,llm,top,2024-01-31 23:08:08,Navigate beyond LLM obsession - have a well rounded ML journey,ade17_in,False,0.82,7,1afvoja,https://www.reddit.com/r/learnmachinelearning/comments/1afvoja/navigate_beyond_llm_obsession_have_a_well_rounded/,0,1706742488.0,"Long post ahead - tl;dr - There is more to ML other than LLMs. 

I want to share a crucial aspect of my ML journey - still a noive but overcame a phase when my progress hit a roadblock because I was fixated solely on LLMs. It is a common pitfall I've observed among many aspirants, colleagues and also students. I've seen a notable personal growth incl. focusing on my paper, personal projects/competitions, internship offers (none to 7 after my roadblock from top AI teams) and also my learning curve. 

I was consumed by the allure of training sophisticated language models and understanding the nuances of advanced AI applications. However, this single-minded focus led to a slow and frustrating progress. I found myself grappling with the intricacies of high-level concepts without a solid grounding in the fundamentals.

Moreover, when it came to interviews, especially those centered around LLMs, I faced significant challenges. The questions were often intricate and demanded a deep understanding of complex models and their applications. It became evident that solely relying on LLM-related knowledge was not enough.

Solution? 
I advocate for a well-rounded ML journey that starts with the basics and gradually builds up to advanced topics. In contextual terms - have a optimal learning rate, you know what happens when learning rate is too much - you don't learn anything and eventually keeping jumping here and there and never reaching your goal (minima).

Begin with basics, explore traditional machine learning techniques, try getting into unique domains - medical imaging, autonomous driving, audio processing or even working with algorithms and believe me these fields are not dying anytime soon. 

More importantly - I know it is tough but resist the temptation of fine-tuning LLMs with API tokens. Instead, focus on core projects to understand algorithms and optimize for performance. 

To conclude - 
The journey is as crucial as the destination. Don't rush; explore, experiment, and embrace the challenges. By diversifying your skill set and building a strong foundation, you not only enhance your understanding of LLMs but also position yourself as a versatile ML practitioner ready for real-world challenges.

PS: I'm no expert, it is just my general observation and it really helped me. Please do not dm directly."
87,learnmachinelearning,llm,top,2023-05-19 18:55:23,How To Reduce The Cost Of Using LLM APIs by 98%,LesleyFair,False,0.77,7,13m4dv2,https://www.reddit.com/r/learnmachinelearning/comments/13m4dv2/how_to_reduce_the_cost_of_using_llm_apis_by_98/,0,1684522523.0,"[Budget For LLM Inference](https://preview.redd.it/k1xmy3xs4u0b1.png?width=493&format=png&auto=webp&s=65324ff460d38abd10dcb9348d9bdba4f1135177)

Cost is still a major factor when scaling services on top of LLM APIs.

Especially, when using LLMs on large collections of queries and text it can get very expensive. It is [estimated](https://neoteric.eu/blog/how-much-does-it-cost-to-use-gpt-models-gpt-3-pricing-explained/) that automating customer support for a small company can cost up to $21.000 a month in inference alone.

The inference costs differ from vendor to vendor and consists of three components:

1. a portion that is proportional to the length of the prompt
2. a portion that is proportional to the length of the generated answer
3. and in some cases a small fixed cost per query.

In a recent [publication](https://arxiv.org/pdf/2305.05176.pdf) researchers at Stanford proposed three types of strategies that can help us to slash costs. The cool thing about it is that we can use these strategies in our projects independently of the prices dictated by the vendors!

*Let’s jump in!*

**How To Adapt Our Prompts To Save Costs**

Most approaches to prompt engineering typically focus only on increasing performance.

In general, prompts are optimized by providing more detailed explanations of the desired output alongside multiple in-context examples to steer the LLM. However, this has the tendency to result in longer and more involved prompts. Since the cost per query grows linearly with the number of tokens in our prompt this makes API requests more expensive.

The idea behind the first approach, called Query Adaption, is to create effective (often shorter) prompts in order to save costs.

This can be done in different ways. A good start is to reduce the number of few-shot examples in your prompt. We can experiment to find out what the smallest set of examples is that we have to include in the prompt to maintain performance. Then, we can remove the other examples.

So far so good!

Once we have a more concise prompt, there is still another problem. Every time a new query is processed, the same in-context examples and detailed explanations to steer the model are processed again and again.

The way to avoid this redundant prompt processing is by applying query concatenation.

In essence, this means that instead of asking one question in our lengthy prompt, we add multiple questions Q1, Q2, … in the same prompt. To get this to work, we might need to add a few tokens to the prompt that make it easier for us to separate the answers from the model output. However, the majority of our prompt is not repeatedly sent to the API as a result.

This allows us to process dozens of queries at once, making query concatenation a huge lever for cost savings while being relatively easy to implement.

*That was an easy win! Let’s look at the second approach!*

**LLM Approximation**

The idea here is to emulate the performance of a better, more expensive model.

In the paper, they suggest two approaches to achieve this. The first one is to create an additional caching infrastructure that alleviates the need to perform an expensive API request for every query. The second way is to create a smaller, more specialized model that mimics what the model behind the API does.

Let’s look at the caching approach!

The idea here is that every time we get an answer from the API, we store the query alongside the answer in a database. We then pre-compute embeddings for every stored query. For every new query that comes in, we do not send it off to our LLM vendor of choice. Instead, we perform a vectorized search over our cached query-response pairs.

If we find a question that we already answered in the past, we can simply return the cached answer without accruing any additional cost. This obviously works best if we repeatedly need to process similar requests and the answers to the questions are evergreen.

Now let’s move on to the second approach!

Don’t worry! The idea is not to spend hundreds of thousands of dollars to fine-tune an LLM. If the overall variety of expected questions and answers is not crazy huge - which for most businesses it is not - a BERT-sized model should probably do the job.

The process could look as follows: first, we collect a dataset of queries and answers that are generated with the help of an API. The second step is to fine-tune the smaller model on these samples. Third, use the fine-tuned model on new incoming queries.

To reduce the cost even further, It could be a good approach to implement the caching first before starting to train a model. This has the advantage of passively building up a dataset of query-answer pairs during live operation. Later we can still actively generate a dataset if we run into any data quality concerns such as some queries being underrepresented.

A pretty cool byproduct of using one of the LLM approximation approaches is that they can significantly reduce latency.

Now, let’s move on to the third and last strategy which has not only the potential to reduce costs but also improve performance.

**LLM Cascade**

More and more LLM APIs have become available and they all vary in cost and quality.

The idea behind what the authors call an LLM Cascade is to start with the cheap API and then successively call APIs of increasing quality and cost. Once an API returns a satisfying answer the process is stopped. Especially, for simpler queries this can significantly reduce the costs per query.

*However, there is a catch!*

How do we know if an answer is satisfying? The researchers suggest training a small regression model which scores the reliability of an answer. Once this reliability score passes a certain threshold the answer gets accepted.

One way to train such a model would obviously be to label the data ourselves.

Since every answer needs only a binary label (reliable vs. unreliable) it should be fairly inexpensive to build such a dataset. Better still we could acquire such a dataset semi-automatically by asking the user to give feedback on our answers.

If running the risk of serving bad answers to customers is out of the question for whatever reason, we could also use one of the stronger APIs (*cough* GPT ***cough***) to label our responses.

In the paper, the authors conduct a case study of this approach using three popular LLM APIs. They successively called them and used a DistillBERT (very small) to perform scoring. They called this approach FrugalGPT and found that the approach could save up to 98.3% in costs on the benchmark while also improving performance.

How would this increase performance you ask?

Since there is always some heterogeneity in the model’s outputs a weaker model can actually sometimes produce a better answer than a more powerful one. In essence, calling multiple APIs gives more shots on goal. Given that our scoring model works well, this can result in better performance overall.

In summary, strategies such as the ones described above are great because they attack the problem of high inference costs from a different angle. They allow us to be more cost-effective without relying on the underlying models to get cheaper. As a result, it will become possible to use LLMs for solving even more problems!

What an exciting time to be alive!

Thank you for reading!

As always, I really enjoyed making this for you and sincerely hope you found it useful! At The Decoding ⭕, I send out a thoughtful 5-minute email every week that keeps you in the loop about machine learning research and the data economy. [Click here to subscribe](http://thedecoding.net)!"
88,learnmachinelearning,llm,top,2023-06-13 21:14:17,Recommended tutorials for learning how to train huggingface LLMs on Sagemaker?,WaterdanceAC,False,0.82,7,148pta2,https://www.reddit.com/r/learnmachinelearning/comments/148pta2/recommended_tutorials_for_learning_how_to_train/,1,1686690857.0,"I haven't used Sagemaker or trained an LLM before. I think I've got my training data formatted correctly, based on datasets on huggingface. Just created a Sagemaker acct. and now feeling a bit lost. Any recommended tutorials ( videos, blog posts, .pdfs, etc.) explaining the process to beginners would be most helpful."
89,learnmachinelearning,llm,top,2023-11-16 21:34:20,AI/LLM starter kit in open source repo,linamagr,False,0.82,7,17wy4aw,https://www.reddit.com/r/learnmachinelearning/comments/17wy4aw/aillm_starter_kit_in_open_source_repo/,6,1700170460.0,"Share a Github repository to quickly build and start a local application to chat with private documents. The stack used is python,  [\#LangChainAI](https://www.linkedin.com/feed/hashtag/?keywords=langchainai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7130952995793489920) , [\#qdrant\_engine](https://www.linkedin.com/feed/hashtag/?keywords=qdrant_engine&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7130952995793489920) [\#Ollama\_ai](https://www.linkedin.com/feed/hashtag/?keywords=ollama_ai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7130952995793489920) and [\#FastAPI](https://www.linkedin.com/feed/hashtag/?keywords=fastapi&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7130952995793489920)  
)  
[https://github.com/mallahyari/ai-starter-kit](https://github.com/mallahyari/ai-starter-kit)   
"
90,learnmachinelearning,llm,top,2023-12-11 19:41:13,Happy Holidays! Here is your 100% free Large Language Model outline!,whiteowled,False,1.0,8,18g24qv,https://www.reddit.com/r/learnmachinelearning/comments/18g24qv/happy_holidays_here_is_your_100_free_large/,0,1702323673.0," 

Thanks for all of your support in recent days by giving me feedback on my LLM outline. This outline is a roadmap on how to learn state-of-the-art stuff about Large Language Models. It builds on work that I have done at AT&T and Toyota. It also builds on a lot of work that I have done on my own outside of corporations.

The outline is solid, and as my way of giving back to the community, I am it giving away for free. That's right, no annoying email sign-up. No gimmicks. No stripe pages for a ""free trial."" No asking you to buy a timeshare in Florida at the end of the outline. It's just a link to a zip file which contains the outline and sample code.

Here is how it works. First, you need to know Python. If you don't know that, then look up how to learn Python on Google. Second, this is an outline, you need to look at each part, go through the links, and really digest the material before moving on. Third, every part of the outline is dense; there is no fluff, and you will will probably need to do multiple passes through the outline.

The outline is designed to start you with an approach to learning PyTorch, it gives a code example of how to do classifications with sentence embeddings, and it also has another code example of how to run Zephyr in colab. The outline took me a couple of days to put together, but it really represents stuff from the past year.

Also, this is not an outline on fine tuning Language Models. It is not a discussion of Mistral MoE, and it is not a discussion of running multiple GPUs. It is designed for someone who has a laptop and wants to learn.

Also, think of this outline as a gift. It is being provided without warranty, or any guarantee of any kind.

If you like the outline, I am begging you to hit that share button and share this with someone. Maybe it will help them as well. If you love the outline, take this as motivation to do good in the world and share something you have done with the community.

Ok, here is the outline.

[https://drive.google.com/file/d/1F9-bTmt5MSclChudLfqZh35EeJhpKaGD/view?usp=drive\_link](https://drive.google.com/file/d/1F9-bTmt5MSclChudLfqZh35EeJhpKaGD/view?usp=drive_link)

If you have any questions, leave a comment in the section below. If the questions are more specific to what you are doing (and if they are not part of the general conversation), feel free to ask me questions on Reddit Chat.

&#x200B;

https://preview.redd.it/s46jelh4yp5c1.png?width=549&format=png&auto=webp&s=df015c4626217229c852e0b5693fd22fd28dc179

&#x200B;

https://preview.redd.it/ashcuro5yp5c1.png?width=547&format=png&auto=webp&s=6a7d8a4a6988ca2c37c02f334b5ba4a73273f7ac"
91,learnmachinelearning,llm,top,2023-04-20 21:37:12,"Finetuning a commercially viable open source LLM (Flan-UL2) using Alpaca, Dolly15K and LoRA",meowkittykitty510,False,1.0,8,12tg061,https://www.reddit.com/r/learnmachinelearning/comments/12tg061/finetuning_a_commercially_viable_open_source_llm/,0,1682026632.0,"Links:

* [Blog Post Write Up](https://medium.com/@krohling/finetuning-a-commercially-viable-open-source-llm-flan-ul2-3b84e568c458) (includes benchmarks)
* [Flan-UL2-Alpaca (HuggingFace)](https://huggingface.co/coniferlabs/flan-ul2-alpaca-lora)
* [Flan-UL2-Alpaca (Github)](https://github.com/ConiferLabsWA/flan-ul2-alpaca)
* [Flan-UL2-Dolly15K (HuggingFace)](https://huggingface.co/coniferlabs/flan-ul2-dolly-lora)
* [Flan-UL2-Dolly15K (Github)](https://github.com/ConiferLabsWA/flan-ul2-dolly)

&#x200B;

Hey Redditors,

This is a project I've been wanting to do for a while. I've spoken to a lot of folks lately who are interested in using LLMs for their business but there's a ton of confusion around the licensing situation. It seems like the Llama platform has been getting all the love lately and I wanted to see what kind of performance I could get out of the Flan-UL2 model. It's underappreciated in my opinion given it has really strong performance on benchmarks (relative to other models in it's size category) and it supports up to 2048 input tokens which is on par with the Alpaca variants. Additionally, it's available under an Apache 2.0 license which means it's viable for commercial usage. 🔥

Despite being a strong model the base Flan-UL2 doesn't give great ""conversational"" responses, so I wanted to see what it was capable of using a newer dataset. I decided to try both Alpaca and Dolly15K. Alpaca is interesting given the massive improvement it had on Llama. It obviously has some licensing caveats which I discuss in the blog post. Dolly15K, which just came out last week, has none of the licensing ambiguity so I was very interested in seeing how those results compared to Alpaca finetuning.

All of the code I used for training is available in the Github links and the final LoRA models are on HuggingFace. I included benchmark results, comparisons and conclusions in the blog post. 

Note that this is one of my first end-to-end finetuning experiments using an LLM so if you see I've made a mistake or have any feedback I'd love to hear it! ❤️"
92,learnmachinelearning,llm,top,2023-04-05 05:09:01,Source Code Search with AI/LLM possible?,MarcRFC,False,0.81,6,12c97u7,https://www.reddit.com/r/learnmachinelearning/comments/12c97u7/source_code_search_with_aillm_possible/,1,1680671341.0,"I would like to use AI to search a large (private) code base and ask questions like ""What is program ABC doing?"" or ""Who is using this module?"" Are there already projects or approaches to achieve something similar?"
93,learnmachinelearning,llm,top,2023-04-30 07:37:19,What LLM should I try to run locally? I have a workstation with two RTX 6000 Ada’s.,Worldbuilder87,False,0.82,7,133ityb,https://www.reddit.com/r/learnmachinelearning/comments/133ityb/what_llm_should_i_try_to_run_locally_i_have_a/,6,1682840239.0,
94,learnmachinelearning,llm,top,2023-04-07 13:31:28,Training opensource LLM (eg Alpaca/GPT4All) on my own docs?,Soc13In,False,1.0,7,12elfp1,https://www.reddit.com/r/learnmachinelearning/comments/12elfp1/training_opensource_llm_eg_alpacagpt4all_on_my/,6,1680874288.0,Is it possible to train an LLM on documents of my organization and ask it questions on that? Like what are the conditions in which a person can be dismissed from service in my organization or what are the requirements for promotion to manager etc. All this information is captured in PDFs. How would one go about doing this?
95,learnmachinelearning,llm,top,2023-07-22 12:54:43,Fine-tuning of Falcon-7B Large Language Model using QLoRA on Mental Health Conversational Dataset,heliosarun,False,1.0,8,156jf9i,https://www.reddit.com/r/learnmachinelearning/comments/156jf9i/finetuning_of_falcon7b_large_language_model_using/,0,1690030483.0,"Here, is a technical blog on step-by-step to fine-tune a Falcon-7B large language model using the QLoRA technique: [https://medium.com/@iamarunbrahma/fine-tuning-of-falcon-7b-large-language-model-using-qlora-on-mental-health-dataset-aa290eb6ec85](https://medium.com/@iamarunbrahma/fine-tuning-of-falcon-7b-large-language-model-using-qlora-on-mental-health-dataset-aa290eb6ec85)

Here, concepts related to QLoRA have been explained and a code walkthrough has been given on how to fine-tune a pre-trained LLM. If you have any queries related to the technical write-up, you can comment here. Happy to help! :)"
96,learnmachinelearning,llm,top,2023-07-12 12:38:52,A roadmap to understand the theory of LLMs,RageA333,False,0.9,7,14xng6t,https://www.reddit.com/r/learnmachinelearning/comments/14xng6t/a_roadmap_to_understand_the_theory_of_llms/,6,1689165532.0,"I wanted to kindly ask for resources for the theory of LLM models. I have a strong mathematical background but a weak understanding on the theoretical side of neural networks. I don't mind starting from the very basics (in fact, I would greatly appreciate a long self-contained approach!)

Thanks for the help!"
97,learnmachinelearning,llm,top,2023-09-30 13:45:23,How to give LLM full context of codebase?,adlabco,False,0.88,6,16w7959,https://www.reddit.com/r/learnmachinelearning/comments/16w7959/how_to_give_llm_full_context_of_codebase/,3,1696081523.0,"I want an LLM to give advice on refactoring across files in a codebase eg finding areas where functions can be further modularised and reused.

Is there a good way of running an LLM locally to allow for this?"
98,learnmachinelearning,llm,top,2024-02-18 14:37:48,What are the subjects taught in university Machine Learning courses in 2024?,adastro,False,0.99,9,1atvhni,https://www.reddit.com/r/learnmachinelearning/comments/1atvhni/what_are_the_subjects_taught_in_university/,5,1708267068.0,"I studied ML at uni just a few years ago, before LLMs became the new focus of the ML industry. Back then, we would study stuff like Naive Bayes, linear/logistic regression, Support Vector Machines, Nearest Neighbor classification, K-Means clustering and Neural Networks. 

After just a couple of years, I feel like this is not the focus anymore. At least, that's what I perceive by looking around: podcasts, newsletters and social media seem to be all about LLMs and generative AI.

I know this is partly a distortion produced by the media, and most ML-related companies don't use any kind of LLM. However, I woke up today wondering: what are new ML students studying these days? Are they prepared for interviewing in this new industry scenario? Do university courses meet their early expectations, or are universities still teaching the same material as a few years ago?  
"
99,learnmachinelearning,llm,top,2023-06-17 15:49:30,How to Build LLM Applications With LangChain and Openai,mwitiderrick,False,0.9,8,14buddi,https://www.reddit.com/r/learnmachinelearning/comments/14buddi/how_to_build_llm_applications_with_langchain_and/,5,1687016970.0,"LangChain is one the most popular tools for building large language model applications.   You can use LangChain to build various applications, such as question-answering systems and chatbots.   Some of the modules in Langchain include: 

**•** **Models** for supported models and integrations 

**• Prompts** for making it easy to manage prompts 

**• Memory** for managing the memory between different model calls 

**• Indexes** for loading, querying, and updating external data 

**•Chains** for creating subsequent calls to an LLM

 **• Agents** to develop applications where the LLM model can direct itself 

**• Callbacks** for logging and streaming the intermediate steps in a chain 

Today over a thousand subscribers of mlnuggets got a tutorial on how to use LangChain and other language models, such as the ones from Openai, to create a system to transcribe and ask questions to YouTube videos. 

Check it out [https://www.machinelearningnuggets.com/how-to-build-llm-applications-with-langchain-and-openai/](https://www.machinelearningnuggets.com/how-to-build-llm-applications-with-langchain-and-openai/)"
100,learnmachinelearning,llm,comments,2023-10-31 05:15:57,What is the point of ML?,shesaysImdone,False,0.75,140,17kdnkt,https://www.reddit.com/r/learnmachinelearning/comments/17kdnkt/what_is_the_point_of_ml/,154,1698729357.0,"To what end are all these terms you guys use: models, LLM? What is the end game? The uses of ML are a black box to me. Yeah I can read it off Google but it's not clicking mostly because even Google does not really state where and how ML is used.

There is this lady I follow on LinkedIn who is an ML engineer at a gaming company. How does ML even fold into gaming? Ok so with AI I guess the models are training the AI to eventually recognize some patterns and eventually analyze a situation by itself I guess. But I'm not sure

*Edit* I know this is reddit but if you don't like me asking a question about ML on a sub literally called learnML please just move on and stop downvoting my comments "
101,learnmachinelearning,llm,comments,2023-06-28 12:29:48,"Intern tasked to make a ""local"" version of chatGPT for my work",Assasinshock,False,0.97,153,14l887h,https://www.reddit.com/r/learnmachinelearning/comments/14l887h/intern_tasked_to_make_a_local_version_of_chatgpt/,104,1687955388.0,"Hi everyone,

I'm currently an intern at a company, and my mission is to make a proof of concept of an conversational AI for the company.They told me that the AI needs to be trained already but still able to get trained on the documents of the company, the AI needs to be open-source and needs to run locally so no cloud solution.

The AI should be able to answers questions related to the company, and tell the user which documents are pertained to their question, and also tell them which departement to contact to access those files.

For this they have a PC with an I7 8700K, 128Gb of DDR4 RAM and an Nvidia A2.

I already did some research and found some solution like localGPT and local LLM like vicuna etc, which could be usefull, but i'm really lost on how i should proceed with this task. (especially on how to train those model)

That's why i hope you guys can help me figure it out. If you have more questions or need other details don't hesitate to ask.

Thank you.  


Edit : They don't want me to make something like chatGPT, they know that it's impossible. They want a prototype that can answer question about their past project. "
102,learnmachinelearning,llm,comments,2023-12-10 18:07:35,Are LLMs overhyped right now?,Snoo_72181,False,0.94,163,18f9enp,https://www.reddit.com/r/learnmachinelearning/comments/18f9enp/are_llms_overhyped_right_now/,67,1702231655.0,"I mean I get that ChatGPT has made LLMs the toast of ML universe. They are indeed amazing.  

But this has lead to so much hype that ML beginners are literally just talking about learning LLMs, ignoring so much in between like Math and Stats, simple ML like Regression, Classification. After that you have, Deep Learning, Transformers and finally LLMs. 

Companies also want candidates with LLM experience, but there's no guarantee that they even have a use case for LLMs "
103,learnmachinelearning,llm,comments,2024-02-09 13:50:47,Why can't LLMs write novels? :technically:,TheGreatGanarby,False,0.7,15,1amokuu,https://www.reddit.com/r/learnmachinelearning/comments/1amokuu/why_cant_llms_write_novels_technically/,65,1707486647.0," 

I've been fascinated with the LLMs since GPT3 was released, to the point that I've re-enrolled in school for a CS degree, but I'd like to know the technical aspects of the problem I really want to solve.

The Problem - Why can't an LLM keep a long narrative? Why can't they write a coherent novel? or even 30 minute sit-com?

Thank in advance, I know this is probably a simple answer, but I'd love to hear it explained with jargon I can learn from."
104,learnmachinelearning,llm,comments,2024-02-03 18:28:40,I want to train a chatbot of myself,Lost-Season-4196,False,0.91,105,1ai2o1u,https://www.reddit.com/r/learnmachinelearning/comments/1ai2o1u/i_want_to_train_a_chatbot_of_myself/,60,1706984920.0,"I have about 193k whatsapp messages of our chat with my gf. I have came across with a guy who finetuned GPT2 on his friend's discord messages in that sub.  Now, I want to fine-tune a model to create one that chats like me.  Ive cleaned the data and split it into days.  I am open to any ideas/advices on how to proceed. Thanks.

Got the idea from that [post](https://www.reddit.com/r/learnmachinelearning/comments/17wp1p7/training_an_llm_to_have_my_friends_personality/?utm_source=share&utm_medium=web2x&context=3)"
105,learnmachinelearning,llm,comments,2023-01-19 07:56:20,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.96,326,10fw2df,https://www.reddit.com/r/learnmachinelearning/comments/10fw2df/gpt4_will_be_500x_smaller_than_people_think_here/,47,1674114980.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/yio0v3zqgyca1.png?width=575&format=png&auto=webp&s=a2ee034ce7ed48c9adc1793bfdb495e0f0812609)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!  
The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
106,learnmachinelearning,llm,comments,2023-05-11 20:15:46,Top 20 Large Language Models based on the Elo rating system.,kingabzpro,False,0.96,251,13eympz,https://i.redd.it/7xfqr5crf9za1.png,43,1683836146.0,
107,learnmachinelearning,llm,comments,2024-02-01 19:00:21,Why do LLMs hallucinate and how to detect it?,Vegetable-Skill-9700,False,0.86,53,1agimdl,https://www.reddit.com/r/learnmachinelearning/comments/1agimdl/why_do_llms_hallucinate_and_how_to_detect_it/,41,1706814021.0,"Hallucinations occur when a model speaks false but plausible knowledge confidently. In simple words hallucination is when a model “makes stuff up”.

Let’s look at a simple case of hallucinations:

Question: What causes diabetes?

Ideal Response: Diabetes is primarily caused by a combination of genetic and environmental factors, including obesity and lack of physical activity.

Hallucinated response: Diabetes is caused by eating too much sugar, and reducing sugar intake can cure it completely.

There could be multiple reasons why LLMs hallucinate which include:

* Training data quality
* Bias introduced due to the generation method
* Misleading input context

Detecting hallucinations can be quite challenging since hallucinated generations can look very similar to non-hallucinated ones in terms of coherence and fluency of the text.

Here’s a list of some methods and tools you can use to check if your LLM is hallucinating:

&#x200B;

|UpTrain|[https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/context\_awareness/factual\_accuracy.ipynb](https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/context_awareness/factual_accuracy.ipynb)|
|:-|:-|
|Game-theoretic adversarial training|[https://aclanthology.org/2023.findings-acl.496.pdf](https://aclanthology.org/2023.findings-acl.496.pdf)|
|Log Probability|[https://aclanthology.org/2023.eacl-main.75](https://aclanthology.org/2023.eacl-main.75)|
|Sentence Similarity Model|[https://arxiv.org/abs/2212.08597](https://arxiv.org/abs/2212.08597)|

&#x200B;"
108,learnmachinelearning,llm,comments,2023-11-27 02:53:26,Why do I need a GPU for ML/AI,notdoreen,False,0.79,23,184so8i,https://www.reddit.com/r/learnmachinelearning/comments/184so8i/why_do_i_need_a_gpu_for_mlai/,32,1701053606.0,"I'm fairly new to AI and was planning to host my own LLM. I I mentioned this to a coworker and he mentioned I would need a gaming GPU. Why is that?

Can someone explain the purpose of a GPU for AI?"
109,learnmachinelearning,llm,comments,2023-04-15 10:38:45,Can we upscale neural network layers?,alcanthro,False,0.83,4,12my20o,https://www.reddit.com/r/learnmachinelearning/comments/12my20o/can_we_upscale_neural_network_layers/,17,1681555125.0,"Might be a beginner question, might not be. I'm not sure. The organic brain grows over time in early childhood, making more room for more connections as the organism gains more experiences. GPTs and most other neural networks are pre-trained and then experience only minor fine-tuning.

But what if we upscale the neural network to make more room for new connections? Basically, what if we increase the size of the weight tensor and then use something like Gaussian interpolation to smooth out the weights? 

The process seems to work alright, based on the [testing I've done](https://www.researchgate.net/publication/369998746_Organic_Growth_of_GPT_Models_A_Brain-Inspired_Incremental_Model_Scaling_Approach), but it might just be due to some weird error that I get decent results. Of course, we wouldn't use this process to train a general use LLM. This process would result in a very unique neural network with its own connections based on its own experiences and self directed learning, i.e they'd be much more like organic minds that ""grew up"" over time.

If this process is viable I'd imagine there'd already be something on the topic of model/network upscaling, but I'm not seeing anything."
110,learnmachinelearning,llm,comments,2022-12-07 00:36:25,For anyone new to ML: DON’T start with pop content about hot new implementations,yourfinepettingduck,False,0.95,180,zenanj,https://www.reddit.com/r/learnmachinelearning/comments/zenanj/for_anyone_new_to_ml_dont_start_with_pop_content/,26,1670373385.0,"I’ve been seeing these threads and guides blow up recently about “prompt engineering” and other applications related to trendy models. 

But the unsupervised LLM / NN approaches used in productized ML is a TERRIBLE way to learn. I studied for years and still am way out of my league there. Besides, if the models are proprietary you can’t even use the assumptions, algorithms, or design choices to actually learn. It’s just glorified trial and error. 

The same thing goes for 30 min cookbook copy/paste scikit implementations that are everywhere online.

The best way to learn is to start with old un-sexy supervised theory that you can actually understand. Even try implementing a model without having to rely on a packaged function. Then work up. Even if you don’t get far, that time is worth way more and it’ll give you the language and principles to think more critically about the harder stuff. 

You’ll never actually understand the unsupervised black-box LLM that dozens of data scientists have worked on full time for years. So why start there?

Example: For someone with less math background Springer has a textbook “Text analysis with R for student of Literature”. I signed up as a coast elective then it ended up being really cool. English majors were fluent in the basic ideas of language processing in few months and they taught me a ton too. 

That stuff exists in all sorts of fields but they look boring. You won’t find a “how to profit from enterprise neural nets in 7 months” textbook"
111,learnmachinelearning,llm,comments,2023-11-10 15:29:14,"Looking to build an LLM POC for my company, where do I begin?",fuzedmind,False,0.94,16,17s6ena,https://www.reddit.com/r/learnmachinelearning/comments/17s6ena/looking_to_build_an_llm_poc_for_my_company_where/,26,1699630154.0,"Hello,

I am currently a devops engineer but do have experience in programming. Some data engineering, no ML yet.

We have a vast trove of Google Docs and Sheets in our company that I think could be used to create an efficient search product using an LLM. I would like to essentially feed all this information to a model and be able to ask it where certain documents are, who updated them, links etc. Something simple.

I have looked around a little and saw that Langchain could be helpful here. We have a data engineering team that is indexing all of our documentation so that part is solved already. It looks like I may have to look into Vector databases as well.

Ultimately I just want somewhere to learn how this is done. There is tons of stuff out there and you folks would probably know best."
112,learnmachinelearning,llm,comments,2023-03-25 16:23:09,What's the current state of actually free and open source LLMs?,maquinary,False,0.95,56,121qvqn,https://www.reddit.com/r/learnmachinelearning/comments/121qvqn/whats_the_current_state_of_actually_free_and_open/,25,1679761389.0,"*People, take easy on me, I just a newbie that tests stuff made by A.I. in a very amateur manner.*

---------------------

Yesterday a played a bit with [Alpaca.cpp](https://github.com/antimatter15/alpaca.cpp), but despite the fact that the software itself is in the MIT license, it has serious limitations because of licensing factors, as you can see [here](https://crfm.stanford.edu/2023/03/13/alpaca.html):

>[...]

>

> We emphasize that Alpaca is intended only for academic research and any commercial use is prohibited. There are three factors in this decision: First, Alpaca is based on LLaMA, which has a non-commercial license, so we necessarily inherit this decision. Second, the instruction data is based on OpenAI’s text-davinci-003, whose terms of use prohibit developing models that compete with OpenAI. Finally, we have not designed adequate safety measures, so Alpaca is not ready to be deployed for general use.

>

> [...]

So, do we have anything that is **completely free** that reaches at least the level of GTP-3?

And what about the data that people use to train the models? Those big companies can ""scan"" the entire web to get insane amounts of data, but can free software developers use these already harvested data to train their own models? Or, in order to have a completely free LLM, people will have to collect data again from the Internet?

-------------

*When I say ""free"", I mean free from licensing limitations, in a sense that I can implement the A.I. in my software without the need of being forced to apply a limited range of licenses, or without the need to pay.*"
113,learnmachinelearning,llm,comments,2024-01-16 20:46:20,Can someone link me some videos of LLM Chatbots made cheaply? - i.e. by an individual using a few hundred $'s of compute power?,AchillesFirstStand,False,0.54,1,198dfch,https://www.reddit.com/r/learnmachinelearning/comments/198dfch/can_someone_link_me_some_videos_of_llm_chatbots/,25,1705437980.0,"I know ChatGPT cost millions of dollars to create the model and hundreds/thousands of people are involved in the process.

I want to see what the current state of the art is in terms of LLM chatbots that are made on the cheap. I think I saw a video from Andrei Karpathy showing how you can make your own GPT, I just want to see what is possible for an individual. It would be a potential project for me."
114,learnmachinelearning,llm,comments,2023-01-15 00:08:37,Is it still worth learning NLP in the age of API-accessibles LLM like GPT?,CrimsonPilgrim,False,0.94,63,10c509n,https://www.reddit.com/r/learnmachinelearning/comments/10c509n/is_it_still_worth_learning_nlp_in_the_age_of/,24,1673741317.0,"A question that, I hope, you will find legitimate from a data science student.

I am speaking from the point of view of a data scientist not working in research.

Until now, learning NLP could be used to meet occasional business needs like sentiment analysis, text classification, topic modeling....

With the opening of GPT-3 to the public, the rise of ChatGPT, and the huge wave of applications, sites, plug-ins and extensions based on this technology that are accessible with a simple API request, it's impossible not to wonder if spending dozens of hours diving into this field if ML wouldn't be as useful today as learning the source code of the Pandas library. 

In some specialized cases, it could be useful, but GPT-3, and the models that will follow, seem to offer more than sufficient results for the immensity of the cases and for almost all classical NLP tasks. Not only that, but there is a good chance that the models trained by giants like Open-AI (Microsoft) or Google can never be replicated outside these companies anyway.  With ChatGPT and its incomparable mastery of language, its ability to code, summarize, extract topics, understand... why would I bother to use BERT or a TF-IDF vectorizer when an API will be released? Not only it would be easily accessible, but it also would be much better at the task, faster and cheaper.

In fact, it's a concern regarding all the machine learning field in general with the arrival of powerful ""no-code"" applications, which abstract a large part of the inherent complexity of the field. There will always be a need for experts, for safeguards, but in the end, won't the Data Scientist who masters the features of GPT-3 or 4 and knows a bit of NLP be more efficient than the one who has spent hours reading Google papers and practicing on Gensim, NLTK, spacy... It is the purpose of an API to make things simpler eventually... At what point is there no more reason to be interested in the behind-the-scenes of these tools and to become simple users rather than trying to develop our own techniques?"
115,learnmachinelearning,llm,comments,2023-11-19 13:06:55,"The background needed to understand ""Attention is All You Need"" Paper",Soc13In,False,0.94,54,17ywtkd,https://www.reddit.com/r/learnmachinelearning/comments/17ywtkd/the_background_needed_to_understand_attention_is/,22,1700399215.0,"Hi, 

My background is that I am by education a Mechanical Engineer and was in Grad school for quite a few years too. In my opinion the Attention is all you need paper is one of the most important papers for understanding how LLM are built and work. 

However, my background is woefully inadequate to understand the mathematics of it. What are some books and papers that I should read to be able to grok the paper, especially attention, and k,q,v matrices and how it is all operating? I like to think that I have fairly good mathematical maturity so don't hesitate to throw standard and difficult references at me, I don't want to read a common language explainer, I want to be able to write my own LLM, even though I might never have the budget to actually train it. "
116,learnmachinelearning,llm,comments,2023-04-14 07:03:30,"Ok so I've got a language model architecture that can run locally on cell phones and probably pi's, both for training and text prediction. What now?",saturn_since_day1,False,0.85,23,12lnnml,https://www.reddit.com/r/learnmachinelearning/comments/12lnnml/ok_so_ive_got_a_language_model_architecture_that/,20,1681455810.0,"I'm going to feed it dolly and maybe alpaca to see if it can follow instructions well, but if it doesn't, is there a market for an LLM that can train and run on potatoes with as little as 6Megabytes of RAM and a few gigs of storage, for the text prediction type of things? 


It Should be able to handle something like customer service chat easily. Or looking up facts it knows. Includes a confidence tell on replies and can think of several replies before giving one.


 It can also learn on the fly. 


so far I have it rehashing facts from Wikipedia articles and writing poetry as tests, and learning whatever facts I type into it. It's very adjustable in terms of creativity or precision to the point of memorization of book chapters on the accurate end.


It also expands as it learns and learns faster than you can read as a human.


I feel like with instruction-taking models like llama and dolly existing on consumer hardware already I might be a bit late if this can't do that well and is only good at text finishing/prediction/creation, but I also feel like my architecture makes it very accessible to train and run your own and that will be worth something regardless.


I know if it can follow instructions it will be worth billions just in hardware and energy savings. But do any of you see a use case if it can't? But can only text predict?


Oh and it is multilingual.


Thoughts?"
117,learnmachinelearning,llm,comments,2023-06-14 09:08:23,"Introducing, OpenLLM 🎉",AaZasDass,False,0.96,86,149302y,https://www.reddit.com/r/learnmachinelearning/comments/149302y/introducing_openllm/,15,1686733703.0,"OpenLLM allows you to run inferences with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps. It includes simple and familiar APIs, enabling easy integration with tools such as LangChain, and BentoML! Discover more at [https://github.com/bentoml/OpenLLM](https://github.com/bentoml/OpenLLM)

To get started, install it with pip: `pip install -U openllm`  Currently, it has support for all major SOTA LLMs, including Falcon, ChatGLM, Dolly V2, StableLM, and more to come!

Some of the feature that is currently wip:

\- Fine-tuning API with `LLM.tuning()`

\- LangChain integration [https://github.com/hwchase17/langchain/pull/6064](https://github.com/hwchase17/langchain/pull/6064)

\- OpenAI Compatible API

    import openai
    
    openai.api_base = ""http://localhost:3000"" # Running with OpenLLM
    
    completion = openai.Completion.create(...)

We are currently actively developing the library, so we would love to hear your thoughts and feedback. Feel free also to join our [discord](https://l.bentoml.com/join-openllm-discord) to meet other fellows, AI application builders, and enthusiasts."
118,learnmachinelearning,llm,comments,2023-09-16 13:22:41,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.95,135,16k7heb,https://www.reddit.com/r/learnmachinelearning/comments/16k7heb/this_week_in_ai_all_the_major_ai_developments_in/,17,1694870561.0,"1. **Stability AI** launched Stable Audio, a generative AI tool for music & sound generation from text. The underlying latent diffusion model architecture uses audio conditioned on text metadata as well as audio file duration and start time.
2. **Coqui** released **XTTS** \- a new voice generation model that lets you clone voices in 13 different languages by using just a quick 3-second audio clip.
3. **Microsoft Research** released and open-sourced **Phi-1.5** \- a 1.3 billion parameter transformer-based model with performance on natural language tasks comparable to models 5x larger.
4. **Project Gutenberg**, Microsoft and MIT have worked together to use neural text-to-speech to create and release thousands of **human-quality free and open audiobooks**.
5. Researchers present **NExT-GPT -** an any-to-any multimodal LLM that accepts inputs and generate outputs in arbitrary combinations of text, images, videos, and audio.
6. **Chain of Density (CoD):** a new prompt introduced by researchers from Salesforce, MIT and Colombia University that generates more dense and human-preferable summaries compared to vanilla GPT-4.
7. **Adept** open-sources **Persimmon-8B**, releasing it under an Apache license. The model has been trained from scratch using a context size of 16K.
8. **Adobe's** **Firefly** generative AI models, after 176 days in beta, are now commercially available in Creative Cloud, Adobe Express, and Adobe Experience Cloud. Adobe is also launching Firefly as a standalone web app.
9. **Deci** released **DeciLM 6B**, a permissively licensed, open-source foundation LLM that is 15 times faster than Llama 2 while having comparable quality.
10. Researchers release **Scenimefy** \- a model transforming real-life photos into Shinkai-animation-style images.
11. **Microsoft** open sources **EvoDiff**, a novel protein-generating AI that could be used to create enzymes for new therapeutics and drug delivery methods as well as new enzymes for industrial chemical reactions.
12. Several companies including Adobe, IBM, Nvidia, Cohere, Palantir, Salesforce, Scale AI, and Stability AI have pledged to the White House to develop safe and trustworthy AI, in a voluntary agreement similar to an earlier one signed by Meta, Google, and OpenAI.
13. **Microsoft** will provide legal protection for customers who are sued for copyright infringement over content generated using Copilot, Bing Chat, and other AI services as long as they use built-in guardrails.
14. **NVIDIA** beta released **TensorRT** \- an open-source library that accelerates and optimizes inference performance on the latest LLMs on NVIDIA Tensor Core GPUs.
15. Pulitzer Prize winning novelist Michael Chabon and several other writers sue OpenAI of copyright infringement..
16. **NVIDIA** partners with two of India’s largest conglomerates, Reliance Industries Limited and Tata Group, to create an AI computing infrastructure and platforms for developing AI solutions.
17. **Roblox** announced a new conversational AI assistant that let creators build virtual assets and write code with the help of generative AI.
18. **Google** researchers introduced **MADLAD-400** \- a 3T token multilingual, general web-domain, document-level text dataset spanning 419 Languages.
19. A recent survey by **Salesforce** show that 65% of generative AI users are Millennials or Gen Z, and 72% are employed.  The survey included 4,000+ people across the United States, UK, Australia, and India.
20. **Meta** is reportedly working on an AI model designed to compete with GPT-4.

My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
119,learnmachinelearning,llm,comments,2023-12-25 05:32:15,"How do companies ""censor"" LLMs?",H4SK1,False,0.95,49,18qc9aj,https://www.reddit.com/r/learnmachinelearning/comments/18qc9aj/how_do_companies_censor_llms/,15,1703482335.0,"Since LLMs dont understand what they are saying, how do they know what not to say? Like dont say: 'Hitler is a great guy' etc. Do companies have a training set contained all the things they dont want their LLM to say? That sounds impratical."
120,learnmachinelearning,llm,comments,2023-11-16 15:05:20,Training an LLM to have my friends personality,travy_burr,False,0.82,13,17wp1p7,https://www.reddit.com/r/learnmachinelearning/comments/17wp1p7/training_an_llm_to_have_my_friends_personality/,16,1700147120.0,"Im a Software Engineer looking to learn a bit about ML, and decided a fun first project would be to train an LLM that has my friend's personality.

I have about 22,000 discord messages from my friend, stored in json format. I could get maybe a few thousand more.

So far, I've been able to get the model to use my friends (lets call him Dylan) words and generally have his personality, but it still isn't forming coherent responses. For example, to the question ""What's your opinion on Steve?"" Dypan's LLM might respond ""Steve has the skill to be a good player, but isn't quite there yet. He has the potential to be a pro"". But to the question ""What's your favorite game?"" It would respond ""it's a good game and I had fun playing it, but I don't know if it's a good game"". Pretty nonsensical.

My LLM is fine tuned using GPT2. I trained it for roughly 9.5 hours overnight on a 3080, with a batch size of 32 and gradient accumulation steps at 32. The training resulted in a loss of 4.09. From what I understand, this loss is extremely high.

I think it would be better if I included messages from other people - essentially giving the LLM context (this is how Dylan responds to these words). Can any provide guidance on how to do this? I've done research but can't seem to find anything helpful.

Thank you in advance!"
121,learnmachinelearning,llm,comments,2023-06-20 17:56:06,LLM questions,YellowSea11,False,0.5,0,14ei1z9,https://www.reddit.com/r/learnmachinelearning/comments/14ei1z9/llm_questions/,15,1687283766.0,"Hey gang .. two questions . 1) can I make chatGPT local? Meaning I don't want it connected to the internet at all .. is that doable? 
2) can I train it on my data? What are my options there? Ideally I'd love to be able to read data from a table so I could train it on those terms as well."
122,learnmachinelearning,llm,comments,2023-09-12 13:42:02,This is why LLMs have flooded the NLP market in the past 1 year 👇 (A Brief History of NLP),japkeerat,False,0.84,45,16grq5y,https://www.reddit.com/r/learnmachinelearning/comments/16grq5y/this_is_why_llms_have_flooded_the_nlp_market_in/,15,1694526122.0,"Text Generation has been the hottest topic in Natural Language Processing. Recurrent Neural Networks (RNNs) were among the Algorithms to generate text. How RNNs generated text is by essentially predicting the next word given the previous few words. At one-stage RNNs were the hottest commodity one could have. But researchers were worried about 1 problem.

RNNs had a context-length problem. To understand what is context-length, consider an analogy. You started reading a book, it’s 100 pages long and when you read each page, details of previous pages start to get a little hazy. Haziness keeps on increasing to the point that when you reach page 50, you don’t remember anything from the first 5 pages. That is exactly what the problem is with RNNs.

To solve this, researchers developed another algorithm called the Long-Short Term Memory (LSTM) and another variant called Bidirectional Long-Short Term Memory (Bi-LSTM) which had a larger context-length than RNNs. Let’s get back to the book analogy. This time while reading, you are making notes. When you go ahead to a new page and your previous pages information start to get hazy, you look back at these notes to refresh your memory. It’s oversimplified, but that’s basically how an LSTM works.

LSTMs were not perfect. There were a number of new issues that came up in order to resolve the previous one. Meanwhile, other areas of research and technological advancements were heating up. Hardware was getting more and more prominent and with cloud getting popular, it was easily accessible. And on the research side, a new kind of Algorithm came up that shaped the entire NLP domain from here on - Attention Mechanism.

Attention Mechanism, as you might have guessed, is all about telling the more sophisticated algorithms where to “focus”. It’s the same way how we focus more on certain parts of the meeting we attend than the entire meeting itself. In context of NLP, the Mechanism became the core part for better algorithms. These better algorithms could keep larger context-lengths and at the time of predicting the next word, ask the Attention Mechanism about what to focus on while predicting the next word. This was an era-defining discovery in NLP as the algorithms that came up after this were the Transformers.

Consider jigsaw puzzles. You start by looking at all the pieces at once and join the pieces together. Initially, it is random. You join a couple of pieces at the top left corner, a few in the centre and a couple more defining the right edge. You are doing it all at once. Transformers basically work the same way. They could look at longer context-lengths, all at once, courtesy of Attention Mechanism. This means, they can not only work with a sentence, they can work with an entire paragraph.  With time, these Transformers started becoming more and more sophisticated. It eventually reached to a point that the only thing that was keeping these algorithms in handcuffs was the lack of data.

Until recently, these algorithms were trained on a specific data but when algorithms became too powerful, researchers started throwing every kind of data they could find on the internet easily. It could be articles like this, your social media posts, exam papers and solutions, and ebooks in any language they could find and hoped the algorithms learnt it all. And they were right. Algorithms started learning all of it to the point that you could ask models to explain concepts of LLMs in how Shakespeare would write and it would give a real-sounding responsive. These algorithms were Large! And hence, became known as Large Language Models (LLMs).

There we are now. With LLMs. OpenAI, technically, won the race for LLM development. They brought everybody’s attention to LLMs first with GPT-2, but GPT-3 was where shit hit the roof and every company that had deep pockets started investing in LLMs.  The result? We now have a new LLM getting released EVERY. SINGLE. DAY.

*I post articles like these every few days on X. If you like this post, please* [follow me on X!](https://twitter.com/JapkeeratS/)

*NOTE: To make it simple for anybody, even without a tech background, to understand, a few things were oversimplified. I will be sharing soon on* [my X handle](https://twitter.com/JapkeeratS) *a technical version.*"
123,learnmachinelearning,llm,comments,2023-03-29 02:00:28,"Running something like GPT-2 locally, training with my own data",SigmaSixShooter,False,0.86,5,1259tlx,https://www.reddit.com/r/learnmachinelearning/comments/1259tlx/running_something_like_gpt2_locally_training_with/,15,1680055228.0,"Greetings, I hope I'm asking in the right place. 

I've been really amazed with ChatGTP-3 and ChatGTP-4 and started thinking how I can use them in my own company. I'd love to train something based on all of our previous tickets. The issue is, these tickets contain sensitive information and customer data, so I can't use some cloud based API. 

So let's say Bob is an excellent worker. His tickets are the gold standard, he always writes in a professional voice with detailed information. There's 20 different issues we fix as part of our business, and over the past 3 years, Bob has covered all of them 100 times over. 

So, I'd like to export all of the tickets Bob has worked and use them to train some GTP/AI type model. I'd like to be able to write a prompt like 'For Issue A with these variables, write up an issue description for our customer"" 

I've been trying to wrap my head around this, but it's an awfully overwhelming subject. Talking with GTP-4 it looks like I can try either GTP-2 or DistilGTP. I also came across Llama.cpp which just came out the other day it seems. 

With this in mind, I've got a few questions

1. Is there any option I should consider that lets me run this on something with 32 gigs of ram and 8 to 16 cores? I've got access to a few pieces of hardware. Again, so far I'm looking at DistilGTP, GTP-2, or llama.cpp
2. Will I need other data sets if I ever figure out how to train this on Bob's tickets? Or will that be enough? I'm trying to figure out if I need the several hundred gigs of other models out there. 
3. How the heck do I go about getting started? :) If someone can help me narrow things down to which LLM (if that's even the right term) I should use to accomplish my goals, and some basic instructions on how to train the data, I think I can figure out the rest after a few thousand rounds of trial and error. 

&#x200B;

Thanks in advance for your time and help."
124,learnmachinelearning,llm,comments,2023-08-21 07:54:44,Steps to get into AI Development,_Powski_,False,0.5,0,15x0vti,https://www.reddit.com/r/learnmachinelearning/comments/15x0vti/steps_to_get_into_ai_development/,13,1692604484.0,"Hello.  
In my free time i want to dive a little into ai development. I have not much time, but a few hours during the week and few more on the weekend to learn. I am pretty good with c#, know some dart, java and scala.  
I have an idea for an app that is in my mind for a long time now. And now with the raise of AI it could be possible to build. I know that it will take me a long time but as a hobby project i would like to tackle that.   
*A similar question could have been already asked here but because i am new in this field i don't really know how the answer to my problem would look like.*

&#x200B;

Problem? I don't know where to start with learning AI Development. I could just google and start with some courses or tutorials but i dont want to spend weeks on learning something that wasn't even needed for the thing i want to do, but i wasn't understanding it so i didn't know.  


Can someone tell me what would be the best way to start and what exact thing i need to learn. ( I know how to build apps, just the ai part is missing for me)  


What i want to achieve:   
1. Users can upload short texts in the app(1000 words).  
2. Other users can choose texts and then ask questions about the text.  
3. Only information related to the chosen text is given. No need for the app to know anything else.   


Now i know that it would be possible to somehow work with the openAI API and somehow solve the problem with that. But would that be the best solution? Would it be possible even somehow without an internet connection just on the users device? Let the app learn the text and answer questions about that with a local LLM? 

So please can you give me a small guide where to start learning. Please also keep in mind that its only a hobby project for me and that i only want to learn the things i actually need for that."
125,learnmachinelearning,llm,comments,2023-07-19 21:58:14,Fine-tuning my 🦙🦙 model,IMissEloquent75,False,0.92,35,1548aa3,https://www.reddit.com/r/learnmachinelearning/comments/1548aa3/finetuning_my_model/,13,1689803894.0,"My company would like to fine-tune the new Llama 2 model on a list of Q/A that our customers use to ask our support client.

I never did this task for an LLM, so I’d like some insights before throwing GPU money out of the window:
- Should I fine-tune the “pre-trained” or “Chat” model? What difference does it make in terms of fine-tuning requirements? 
- Does the amount of Q/A I have matter compare to the size of the model(7B, 13B, 70B)?
- Any good advice for achieving this kind of task?

That's a lot of noob questions, I suppose. Kudos to the one who gives me an answer❤️"
126,learnmachinelearning,llm,comments,2024-01-01 13:05:37,Is it good idea to sharpen skills in LLM?,Glad-World-5312,False,0.5,0,18vwdtr,https://www.reddit.com/r/learnmachinelearning/comments/18vwdtr/is_it_good_idea_to_sharpen_skills_in_llm/,11,1704114337.0,"Yeah actually i did my Masters in Big data Analytics. And i did my internship in data visualisation using python (dash,plotly). And in my full time its completely into non technical. So im planning to switch to Gen Ai and i have enough foundation in ML. So whats your thought on switching to Gen Ai (LLM)?"
127,learnmachinelearning,llm,comments,2023-06-03 14:33:38,This week in AI - all the Major AI development in a nutshell,wyem,False,0.98,121,13zeoi3,https://www.reddit.com/r/learnmachinelearning/comments/13zeoi3/this_week_in_ai_all_the_major_ai_development_in_a/,13,1685802818.0,"1. The recently released open-source large language model **Falcon LLM**, by UAE’s Technology Innovation Institute, is now royalty-free for both commercial and research usage. **Falcon 40B,** the 40 billion parameters model trained on one trillion tokens, is ranked #1 on **Open LLM Leaderboard by Hugging Face**.
2. **Neuralangelo**, a new AI model from Nvidia turns 2D video from any device - cell phone to drone capture - into 3D structures with intricate details using neural networks..
3. In three months, JPMorgan has advertised **3,651 AI jobs** and sought a trademark for **IndexGPT**, a securities analysis AI product.
4. **Google** presents **DIDACT** (​​Dynamic Integrated Developer ACTivity), the first code LLM trained to model real software developers editing code, fixing builds, and doing code review. DIDACT uses the software development process as training data and not just the final code, leading to a more realistic understanding of the development task.
5. Researchers from **Deepmind** have presented ‘**LLMs As Tool Makers (LATM)**’ - a framework that allows Large Language Models (LLMs) to create and use their own tools, enhancing problem-solving abilities and cost efficiency. With this approach, a sophisticated model (like GPT-4) can make tools (where a tool is implemented as a Python utility function), while a less demanding one (like GPT-3.5) uses them.
6. **Japan's government** won't enforce copyrights on data used for AI training regardless of whether it is for non-profit or commercial purposes.
7. *‘Mitigating the* ***risk of extinction from AI*** *should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.’ -* One sentence statement signed by leading AI Scientists as well as many industry experts including CEOs of OpenAI, DeepMind and Anthropic.*.*
8. Nvidia launched ‘**Nvidia Avatar Cloud Engine (ACE) for Games**’ - a custom AI model foundry service to build non-playable characters (NPCs) that not only engage in dynamic and unscripted conversations, but also possess evolving, persistent personalities and have precise facial animations and expressions.
9. **OpenAI** has launched a trust/security portal for OpenAI’s compliance documentation, security practices etc..
10. **Nvidia** announced a new AI supercomputer, the **DGX GH200,** for giant models powering Generative AI, Recommender Systems and Data Processing. It has 500 times more memory than its predecessor, the DGX A100 from 2020.
11. Researchers from Nvidia presented **Voyager**, the first ‘LLM-powered embodied lifelong learning agent’ that can explore, learn new skills, and make new discoveries continually without human intervention in the game Minecraft.
12. The a16z-backed chatbot startup **Character.AI** launched its mobile AI chatbot app on May 23 for iOS and Android, and succeeded in gaining over **1.7 million new installs** within a week.
13. Microsoft Research presents **Gorilla**, a fine-tuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls.
14. **OpenAI** has trained a model using process supervision - rewarding the thought process rather than the outcome - to improve mathematical reasoning. Also released the full dataset used.
15. **WPP**, the world's largest advertising agency, and Nvidia have teamed up to use generative AI for creating ads. The new platform allows WPP to tailor ads for different locations and digital channels, eliminating the need for costly on-site production.
16. **PerplexityAI’s** android app is available now, letting users search with voice input, learn with follow-up questions, and build a library of threads.

**If you like this news format**, you might find my  [newsletter](https://aibrews.com/) helpful - it's free to join, sent only once a week with **bite-sized news, learning resources and selected tools**. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
128,learnmachinelearning,llm,comments,2023-11-06 16:04:48,TensorGym - Interactive ML skill-building platform 🏋️‍♂️,Rudegs,False,0.99,76,17p62wj,https://www.reddit.com/r/learnmachinelearning/comments/17p62wj/tensorgym_interactive_ml_skillbuilding_platform/,12,1699286688.0,"We couldn't find a website where you can interactively learn basic PyTorch tensor operations. So we made [one](https://www.tensorgym.com/)!

So far we added:

* 8 PyTorch basic operators exercises
* 3 hard-ish LLM exercises
* 2 classic ML exercises

https://preview.redd.it/9x3ete3seryb1.png?width=2520&format=png&auto=webp&s=bd31f4f8c4936a284f8793f0ed5ebd34233d2055

Our main principles:

* We provide links and quick hints about the API to save time because it's not about memorization—it's about understanding
* We provide essential math formulas as necessary
* Our goal is to make learning fun and interactive!

Please check it [out](https://tensorgym.com/blog/intro) and join our [Discord server](https://discord.gg/vhhTWMPK5E)!

We really hope that it's useful🏋️‍♂️"
129,learnmachinelearning,llm,comments,2022-12-18 08:16:46,"Looking for good learning sources around generative AI, specifically LLM",Global_Lab8010,False,0.95,16,zotnbu,https://www.reddit.com/r/learnmachinelearning/comments/zotnbu/looking_for_good_learning_sources_around/,10,1671351406.0,"Are there any good video content sources that explains all the concepts associated with generative AI (ex: RL, RLHF, transformer, etc) from the ground up in extremely simple language (using analogies/stories of things that would be familiar to say a 10-12 year old)? Also would prefer channels which explain the concepts in a sequential manner (so that easy to follow) and make short and crisp videos

If yes, could you kindly comment below with the suggestions. 
If not, could you comment whether something like that would be useful to you and ideally why also?

Big thanks in advance 🙏"
130,learnmachinelearning,llm,comments,2023-10-17 07:08:17,LLMs for the infinite input lengths are here!,av_community,False,0.83,31,179shyy,https://www.reddit.com/r/learnmachinelearning/comments/179shyy/llms_for_the_infinite_input_lengths_are_here/,10,1697526497.0,"🔍A team of researchers from Meta AI and MIT developed **StreamingLLM**, a framework that enables finite-length LLMs to infinite sequence lengths without finetuning.

🔥Enables Llama 2, Falcon, and MPT to more than 4 million tokens input lengths.

🌟Applying LLMs to infinite input lengths poses 2 challenges:

1️⃣Excessive Memory Storage: During the decoding stage, LLMs store the KV pairs of previous tokens to compute the attention. Having infinite length tends to cause excessive memory storage and increased latency.

2️⃣Performance Degradation: The performance of LLMs degrades if we extend the sequence length beyond the maximum input length set during pretraining.

🎯The team proposed an interesting phenomenon known as attention sinks to overcome the above challenges.

📚Research Paper: [https://arxiv.org/pdf/2309.17453.pdf](https://arxiv.org/pdf/2309.17453.pdf)  
💻 Code: [https://github.com/mit-han-lab/streaming-llm](https://github.com/mit-han-lab/streaming-llm)

What are your thoughts?"
131,learnmachinelearning,llm,comments,2023-07-17 20:00:39,"How to start learning generative AI? It seems out of reach for most of us as the models are too large for consumer GPU. Is training via collab and cloud, reasonably affordable?",easysunnysideup,False,1.0,15,152c9tu,https://www.reddit.com/r/learnmachinelearning/comments/152c9tu/how_to_start_learning_generative_ai_it_seems_out/,10,1689624039.0,It seems many guides describe taking an opensource model and fine tuning the last parts of the LLM. Is this doable for consumer GPU?
132,learnmachinelearning,llm,comments,2024-01-11 10:44:17,Can you embed LLMs into my glasses?,Plastic-Paramedic-89,False,0.2,0,193yu1u,https://www.reddit.com/r/learnmachinelearning/comments/193yu1u/can_you_embed_llms_into_my_glasses/,10,1704969857.0,"Folks, I have seen some demo related to ChatT connected glasses.

I wondered, what's you thought on actually embed one LLM inside my glasses ""locally""?

Or one step further, is there any people doing research in “intelligent material”, such that we embed neural networks into plastic."
133,learnmachinelearning,llm,comments,2023-08-05 21:26:47,Best book for understanding the fundamental mathematics of modern machine learning and inference?,SecretPressure9813,False,0.97,29,15j78kn,https://www.reddit.com/r/learnmachinelearning/comments/15j78kn/best_book_for_understanding_the_fundamental/,10,1691270807.0,"I own the 2006 era book by Christopher Bishop ""Pattern Recognition and Machine Learning (Information Science and Statistics)""  ([https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr\_1\_4\_sspa](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr_1_4_sspa)) which presents the Bayesian fundamentals ... but I'm not clear on whether the math presented there is what underlies the training and inference used in current LLM models etc.  


Please let me know your thoughts and/or let me know what you think the best textbook and/or courseware to understand these models at the fundamental level is. I'm also interested in any good overviews discussing how models are structured and/or the associated ""rules of thumb"" in that area. I get the sense that there's a lot of black magic when it comes to this..."
134,learnmachinelearning,llm,comments,2024-02-20 14:38:13,GPU vs TPU: Which One is Better?,Which_Pin_6386,False,0.87,11,1avj5ed,https://www.reddit.com/r/learnmachinelearning/comments/1avj5ed/gpu_vs_tpu_which_one_is_better/,9,1708439893.0," 

Hello, I would like to understand which option is better for deploying the same LLM (it could be LLAMA or some other).

This comparison would involve deploying it on AWS, Google Cloud, and a local setup (Mac or with an NVidia RTX 4090).

Considerations to take into account would include:

* Interoperability between LLM models
* Efficiency
* Cost
* Other

Can you help me? Thank you!"
135,learnmachinelearning,llm,comments,2023-03-31 01:09:30,How should I go about publishing a dataset so other engineers/scientists will use it?,tylersuard,False,0.97,21,1275el6,https://www.reddit.com/r/learnmachinelearning/comments/1275el6/how_should_i_go_about_publishing_a_dataset_so/,9,1680224970.0,"Hello.  I have a dataset that could be really helpful to a lot of researchers, particularly in the LLM field.  How and where should I post it to get their attention?"
136,learnmachinelearning,llm,comments,2022-08-31 22:07:24,Most Popular AI Research August 2022 - Ranked Based On Total Twitter Likes,bycloudai,False,0.97,279,x2q4sk,https://i.redd.it/wj1suih0h4l91.jpg,9,1661983644.0,
137,learnmachinelearning,llm,comments,2023-12-10 17:51:18,Need a roadmap for LLMs.,Competitive_Pin_5580,False,0.94,15,18f91n0,https://www.reddit.com/r/learnmachinelearning/comments/18f91n0/need_a_roadmap_for_llms/,9,1702230678.0,"As the title says. I'm quite familiar with concepts of ML and DL: read a few books, done a Lotta projects, especially utilizing Random Forests, CNNs and LSTMs. Not as many projects on NLP.

Now I want to get into LLMs from the point of view of being a viable candidate for companies hiring interns for LLM projects. Since it's a new field, I don't really have a roadmap. A roadmap and links to courses, free or paid alike, are much appreciated."
138,learnmachinelearning,llm,comments,2023-08-29 22:37:15,Finetuning an LLM to imitate someone,Vanishing-Rabbit,False,0.94,14,164wtmm,https://www.reddit.com/r/learnmachinelearning/comments/164wtmm/finetuning_an_llm_to_imitate_someone/,9,1693348635.0,"Hello all,

I'm trying to understand how to get an LLM to imitate someone, say Shakespeare. It's easy enough to get all of [Shakespeare's work](https://www.gutenberg.org/ebooks/author/65).

If I've understood the current state of play for LLMs, there are three options:

* Fine tune an LLM
* Vectorize your knowledge using something like ChromaDB. Do a similarity search after each prompt and get the LLM to ""read"" the top n docs
* Do both

I have a feeling that to imitate Shakespeare, fine tuning an LLM might work best.

However, if my understanding is correct, the inputs to finetune an LLM must be formatted this way:

    <human>: ""To be""
    <system>: ""Or not to be""
The gap I'm having trouble bridging is how do I go from a large text file to this input format? The only idea I've come across is format all of the text like so:

    <human>: ""sentence_1""
    <system>: ""sentence_2""

    <human>: ""sentence_2""
    <system>: ""sentence_3""
Are there best practices around this problem? How should I be thinking about this?

I've seen companies like character.ai create bots that imitate [Elon Musk] (https://c.ai/c/6HhWfeDjetnxESEcThlBQtEUo0O8YHcXyHqCgN7b2hY) accurately for example so I know it's doable. I just wonder if they've done it by finetuning an LLM or training one from scratch or something else entirely."
139,learnmachinelearning,llm,comments,2023-07-11 15:10:41,Experienced SW Eng looking to learn ML,BugOk8374,False,0.76,2,14wtzif,https://www.reddit.com/r/learnmachinelearning/comments/14wtzif/experienced_sw_eng_looking_to_learn_ml/,9,1689088241.0,"I have a background in SW Engineering, I'm looking to transition or to work on ML, so I thought about some learning or courses, in a practical way.

When I look at the syllabus of the most widely known ones, like those on Coursera or Udemy, I have the impression that either they are old or not cover recent advances (like LLMs) or they go into too much math theory.

**Are there any balanced courses ? Something practical and at the same time wide enough to cover CNN, DL, RL, Supervised, Unsupervised, LLM, GAN, Generative, and so on ?** So that if I endup in a company I can fit in.

I don't have the impression I would need to know lots of theory because to be honest I do not believe I will become an AI researcher nor will I work on creating new models. There's people with better skills than me for that, and given the training GPU costs it seems like way out of league.  
Hence, I have the impression most of the growth will happen elsewhere.

Just like working on SW does not require everybody to be an expert CPU Designer or Compiler Optimisation Computer Science genius.  
For instance, I think I would like to work on optimising the model's execution from a computational point of view, but I don't find anything so practical, like if optimising was trial and error

Am I misunderstanding something ?

NB: during interviews, they usually ask theoretical questions, even if the work could be done with some mean, median or linear regression. Other times some very case-specific practical questions, like which API, framework or service would you use, like if it was some sort of religion.  
"
140,learnmachinelearning,llm,comments,2023-12-13 20:50:57,What happened after BERT and transformers in NLP?,obergrupenfuer_smith,False,0.97,27,18hqty1,https://www.reddit.com/r/learnmachinelearning/comments/18hqty1/what_happened_after_bert_and_transformers_in_nlp/,9,1702500657.0,"hey guys, stopped following ML in 2019 or so when I became an analyst. I am familiar with the field upto BERT, Transformers, Bi directional transformers.

Now I am interviewing for a company asking for LLM (large language models), so I want to know what are some salient papers which came out in the last couple years so I can read up on them. basically the best performing models. I remember CVPR was for computer vision.. what was the one for NLP?

EDIT: Is transformer the core building block of all these things? I remember reading 'Attention is all you need' paper back in college which was amazing. Any new papers like that in NLP? (Or gen AI?)"
141,learnmachinelearning,llm,comments,2024-01-15 08:18:55,Text Extraction (?) using LLm,Different_Star9899,False,0.67,3,1973rlm,https://www.reddit.com/r/learnmachinelearning/comments/1973rlm/text_extraction_using_llm/,9,1705306735.0,"  

Hello guys,

I want to do a research on using LLM to check and compare the specifications of Materials from different pdf files, but I have no idea where to start since my background is civil engineering and we only want to use LLM as a way to explore the capabilities of a LLM in construction industry. Basically, the process is as follows:

User inputs one project specifications pdf file first, and then inputs other multiple material specifications pdf files (let’s say Air-conditioners), and then the model extracts the information from all those files (eg: voltage, HP, etc) and generates a comparison chart between the project-required specifications and the proposed material specifications, and give out the similarity score (ie: which material is the most suitable for the project).

I am a Masters in Construction Engineering and Management so I have no knowledge about how the LLM works so any advice is welcome. I want to know which technology/method I should research, what would be the general framework, and which model might be the most suitable for this use case.

PS: I know that NLP would be must better for this, but part of my research objective is to compare how easy LLM is to develop compared to the traditional NLP."
142,learnmachinelearning,llm,comments,2023-09-15 19:03:23,"LLM for textbooks, feasible ?",Personal_Definition,False,1.0,2,16jlt3n,https://www.reddit.com/r/learnmachinelearning/comments/16jlt3n/llm_for_textbooks_feasible/,8,1694804603.0,"I want to build a webapp SAAS that allows you to chat with an AI tutor about  specific textbook. Some of these textbooks are in English and others are Arabic, conceptual , no equations or anything.
I couldn't find anything like this. Im a total noob with AI. but I think it can be made using langchain and openai api , can it ?"
143,learnmachinelearning,llm,comments,2023-03-02 16:47:40,Build ChatGPT for Financial Documents with LangChain + Deep Lake,davidbun,False,0.95,171,11g7h03,https://www.reddit.com/r/learnmachinelearning/comments/11g7h03/build_chatgpt_for_financial_documents_with/,8,1677775660.0,"https://preview.redd.it/h9r6hgvfucla1.png?width=2388&format=png&auto=webp&s=5432eac3eeed8583e4309af1fdc7ebecac705796

As the world is increasingly generating vast amounts of financial data, the need for advanced tools to analyze and make sense of it has never been greater. This is where [LangChain](https://github.com/hwchase17/langchain) and [Deep Lake](https://github.com/activeloopai/deeplake) come in, offering a powerful combination of technology to help build a question-answering tool based on financial data. After participating in a LangChain hackathon last week, I created a way to use Deep Lake, the data lake for deep learning (a package my team and I are building) with LangChain. I decided to put together a guide of sorts on how you can approach building your own question-answering tools with  LangChain and Deep Lake as the data store.

Read [the article](https://www.activeloop.ai/resources/ultimate-guide-to-lang-chain-deep-lake-build-chat-gpt-to-answer-questions-on-your-financial-data/) to learn:

1. What is LangChain, what are its benefits and use cases and how you can use to streamline your LLM (Large Language Model) development?  
2. How to use [\#LangChain](https://www.linkedin.com/feed/hashtag/?keywords=langchain&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) and [\#DeepLake](https://www.linkedin.com/feed/hashtag/?keywords=deeplake&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) together to build [\#ChatGPT](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) for your financial documents.  
3. How Deep Lake’s unified and streamable data store enables fast prototyping without the need to recompute embeddings (something that costs time & money).  


I hope you like it, and let me know if you have any questions!"
144,learnmachinelearning,llm,comments,2023-02-02 01:53:28,"Using unsupervised, k-means clustering to train LLMs and convert the clusters into context solidified in nodes in a decision tree, using reinforcement learning and backpropagation to maximize user engagement?",swagonflyyyy,False,0.5,0,10rdant,https://www.reddit.com/r/learnmachinelearning/comments/10rdant/using_unsupervised_kmeans_clustering_to_train/,8,1675302808.0,"I was thinking of an LLM idea, which I'm not sure exists yet, where the model codifies the text database it is trained on in a decision tree, with each node representing a topic generated by clustering words together by frequency and relation into a given context and then using a decision tree to assign a value to each topic and attempting to predict what the next topic will be by searching through the tree and using the highest value at the bottom of the tree.

The model would use reinforcement learning over time to better predict which topics go together and recalibrate the tree. The goal here is to have a bot that can navigate conversations better in a structured, linear manner, just like a 2-way conversation, if that makes sense.

Would decision trees or forests be compatible with LLMs in the future? They're both linear and sequential so I don't see why not.

EDIT: More thoughts:  


 I think the key here would be to gather normal, one-on-one conversations online and use each user's turn as a prediction. For example:  

\- If I said 'abc', my friend will say 'xyz' because that's the pattern that keeps being brought up.  

 \- And since he will have said 'xyz' I will respond with 'DEF' because that way my friend will say 'HIJ' because he tends to say things like that in response.   

\- And this type of conversation has been historically proven to keep the user engaged for as long as possible so I will use my decision tree to calculate the best responses I can make. 

This could be used in practical applications, such as problem-solving, for example, and the model would slowly calibrate itself over time to gear itself towards problem-solving conversations because the user keeps finding the model useful. 

That's a simple example and there would be lots of obstacles to overcome, but would you use a decision tree for this?"
145,learnmachinelearning,llm,comments,2023-09-13 21:52:10,Which LLM that can be run locally gives the best results?,balaena7,False,0.86,5,16i01nw,https://www.reddit.com/r/learnmachinelearning/comments/16i01nw/which_llm_that_can_be_run_locally_gives_the_best/,8,1694641930.0,"I am interested, which of the LLMs on hugging face could be run locally and gives decent results, given a decent retailer GPU (e.g. RTX 4090)? How much do these results compare to ChatGPT?"
146,learnmachinelearning,llm,comments,2024-02-04 10:19:14,What small open source LLM is best for conversations?,Sea_Skill_3479,False,1.0,3,1aikudt,https://www.reddit.com/r/learnmachinelearning/comments/1aikudt/what_small_open_source_llm_is_best_for/,8,1707041954.0,"What small open-source LLM is most effective for social media conversations? I attempted fine-tuning with Llama-2 7B-Q4, but it didn't work on my GPU(getting error-cuda out of memory ). Are there smaller, more suitable models for natural-sounding conversations that can be finetuned with small dataset? I discovered that the Bard response feels more natural than GPT-3.5. How can I get my response close to Bard?"
147,learnmachinelearning,llm,comments,2023-05-25 17:57:47,What NLP/NLU tasks are Generative LLMs bad at?,Smallpaul,False,0.94,13,13rok24,https://www.reddit.com/r/learnmachinelearning/comments/13rok24/what_nlpnlu_tasks_are_generative_llms_bad_at/,7,1685037467.0,"If we put aside questions of cost, what are examples of tasks that are better handled by non-LLM NLP techniques? (I'm including fine-tuned LLMs in the definition of ""LLM"")"
148,learnmachinelearning,llm,comments,2023-12-18 14:12:24,Fresher - Rate my Resume,Kindly_Work_6038,False,0.5,0,18l9n3c,https://www.reddit.com/r/learnmachinelearning/comments/18l9n3c/fresher_rate_my_resume/,8,1702908744.0,"I am looking for an ML/DL internship and I'm going to start applying from the new year. I am hoping this would get me an interview but please feel free to honestly or brutally point out anything wrong. I don't mind.

https://preview.redd.it/8otn44iw927c1.png?width=1158&format=png&auto=webp&s=61b51136c2fa1c7e1b05c56781773db6b0d2948b"
149,learnmachinelearning,llm,comments,2024-02-11 18:29:31,LLM finetuning,IamAriel30,False,0.67,1,1aoescp,https://www.reddit.com/r/learnmachinelearning/comments/1aoescp/llm_finetuning/,8,1707676171.0,"Hey guys!

I started a new project recently at work and the goal is to generate summaries for timeseries data (or their plots) and provide certain suggestions based on that. I do have experience in AI but never really bothered with NLP so I'm fairly new.

I'm a bit lost as to where to start. I thought about using GPT4 API  since Azure offers it, which can become a bit pricey and will still give generic suggestions as finetuning isnt available. My focus now shifted on fine tuning an open source LLM such as Mistral 7B on our data. 

My questions are:
1) is my current approach reasonable?
2) how to perform finetuning, in what form would I feed the data to the model and later generate prompts? The timeseries data is updated daily with new values.

Thank you 😊"
150,learnmachinelearning,llm,comments,2023-07-10 15:16:24,"ChatPDF: What ChatGPT Can't Do, This Can!",JunXiangLin,False,0.7,4,14vww3o,https://www.reddit.com/r/learnmachinelearning/comments/14vww3o/chatpdf_what_chatgpt_cant_do_this_can/,7,1689002184.0,"Believe many of people have been using **ChatGPT** for a while, and you are aware that although ChatGPT is powerful, it has the following limitations:

1. Unable to answer questions about events that occurred after **2021**.
2. Unable to directly upload your own data, such as **PDF, Excel, databases**, etc.
3. Inaccurate in performing **mathematical calculations**.

**Langchain** is a recent trending open-source project, which is a framework for developing Large Language Models (LLMs) applications. It supports the following:

1. Connecting LLM models with **external data sources**, such as PDF, Excel, databases, etc.
2. Allowing interaction between LLM models and other tools, such as **Google search**, enabling internet connectivity.
3. Rapid development of LLM model applications.

Today, I'd like to share a project called **ChatPDF**(strickly called **docGPT**, there're some different), built using the Langchain framework. It allows users to upload local documents and ask questions to the LLM model. In this tool, you can ask AI to summarize articles or inquire about any information in the document. Moreover, by leveraging the Langchain Agent functionality, the LLM model can collaborate with the Google Search API, enabling users to ask questions about current topics!

The project provides a detailed guide on how to create your own **docGPT**. It is built using the Langchain framework and Python Streamlit, which is a free and fast way to create online services. As long as you have an OPENAI API KEY, feel free to give it a try!

I encourage everyone to pay attention to the [Langchain open-source project](https://github.com/hwchase17/langchain) and leverage it to achieve tasks that ChatGPT cannot handle.

[Github Repository](https://github.com/Lin-jun-xiang/docGPT-streamlit/tree/main)

[ChatPDF Application](https://docgpt-app.streamlit.app/)

&#x200B;

https://preview.redd.it/q906a7imm5bb1.png?width=2560&format=png&auto=webp&s=acef45049bab805038f876eea56cc371b8a9a83a"
151,learnmachinelearning,llm,comments,2023-12-24 10:15:41,Do specialized LLMs really need 20+ billion parameters?,NightestOfTheOwls,False,0.8,9,18psibb,https://www.reddit.com/r/learnmachinelearning/comments/18psibb/do_specialized_llms_really_need_20_billion/,7,1703412941.0,"As the title suggests. I feel like current trend in LLM market is to make ""AI to end all AIs"": it can write poems, it can write code, it can write you an essay, it can paraphrase text, it can analyze stocks, it can roleplay, etc. etc.

My question is, isn't it kinda obvious to do what Mistral team and several others are doing and introduce many smaller ""expert"" models that activate based on the nature of request to improve performance?

Alternatively, are there techniques to conveniently ""remove"" unnecessary data from existing models without re-training? For example, I'm not totally sure that a code assist model needs to know what MBTI means and who Dr. Seuss is.

Is it already being done in most popular models and I'm just out of the loop?"
152,learnmachinelearning,llm,comments,2024-02-19 21:51:38,1 year to be NLP-LLM Specialist,Learning_DL,False,0.58,2,1auzu5r,https://www.reddit.com/r/learnmachinelearning/comments/1auzu5r/1_year_to_be_nlpllm_specialist/,7,1708379498.0,"Hi all, 

I am taking 1 year to really learn deeply ML. I have basic knowledge but I want to go deep. I made this plan:

Part 1: 1000h to learn this: 

- Linear regression: 20h
- Logistic regression: 20h
-  NN and Basics (evaluation, optimiser, training, …): 150h
- RNN: 30h
- LSTM: 30h
- Seq2Seq: 50h
- Encoder-Decoder: 50h
- Transformers: 150h
- LLM Models: 250h
- Maths, Stats, Probability: 100h
- NLP Basics: 150h

I will code all the models from scratch to deeply understand them. And I really enjoy this process. 

Part 2: 1000h of applications

Once I have learned this models, I will take 3-5 public datasets and apply all theses models for 4 applications (I am interested in): 

- Text Classification: 200h
- Text Generation: 300h
- IR system  (RAG system): 500h

For each application, I will read related research papers, apply the learned models on the selected dataset, experiments, compare and analyse the results.

In total I will spend 2000h, so it’s 40h a week for 1 year! 

Ps: I want to be good in IR and LLM. 
I am already data scientist - engineer, coding with python. I have a part time job (data scientist for 15h a week). So, I will study on week ends. I am single, no kids and no responsibilities. 

Your thoughts?

"
153,learnmachinelearning,llm,comments,2023-08-25 16:29:53,How do I go about training an open source llm on a postgres database,Dubsteprhino,False,1.0,2,1614122,https://www.reddit.com/r/learnmachinelearning/comments/1614122/how_do_i_go_about_training_an_open_source_llm_on/,7,1692980993.0,"Howdy, 

I'm a backend developer, and management recently asked me to train an llm on our company data. I'm a bit over my head here, and I figured I'd ask for high level advice rather than continuing to go down google rabbit holes.

What I've tried so far:

* I spun up some gpu instances on AWS. Couldn't get llama to work at all, except for using gpt4all, which wasn't very performant and does make a network call to a github page for a list of models.

* I tired following a google cloud tutorial [here](https://cloud.google.com/blog/products/databases/using-pgvector-llms-and-langchain-with-google-cloud-databases). This didn't work in their colab notebook, so I gave up on that since if their own documentation didn't work it didn't seem promising.

Any advice is appreciated!"
154,learnmachinelearning,llm,comments,2023-03-30 19:44:32,Personalize Your Own Language Model with xTuring - A Beginner-Friendly Library,x_ml,False,0.98,55,126x6ua,https://www.reddit.com/r/learnmachinelearning/comments/126x6ua/personalize_your_own_language_model_with_xturing/,7,1680205472.0,"Hi everyone,  


If you are interested in customizing your own language model but don't know where to start, try  [xTuring](https://github.com/stochasticai/xturing).  


xTuring's goal is to empower individuals to fine-tune LLM for their specific tasks with as little as 5 lines of code. With xTuring, you can perform high and low precision fine-tuning with a variety of models, including LLaMA, OPT, Cerebras-GPT, Galactica, BLOOM, and more.   


You can also generate your OWN datasets using powerful models like GPT-3 to train a much smaller model on YOUR specific task. With the latest version, you can also use terminal and web interface to chat with your models.  


Please do check out the repo and show your support if you like our work. Would love if you can also contribute by adding models, raising issues or raising PRs for fixes.  


xTuring Github: [https://github.com/stochasticai/xturing](https://github.com/stochasticai/xturing)

If you are interested in getting involved, I am happy to help you on our Discord: [https://discord.gg/TgHXuSJEk6](https://discord.gg/TgHXuSJEk6)

https://i.redd.it/mvxb7i5fixqa1.gif"
155,learnmachinelearning,llm,comments,2024-01-08 22:39:10,P40 vs P100 for local AI,PaperboyNZ,False,1.0,3,191yctt,https://www.reddit.com/r/learnmachinelearning/comments/191yctt/p40_vs_p100_for_local_ai/,7,1704753550.0,"I'm planning to build a server focused on machine learning, inferencing, and LLM chatbot experiments. The Tesla P40 and P100 are both within my prince range. The P40 offers slightly more VRAM (24gb vs 16gb), but is GDDR5 vs HBM2 in the P100, meaning it has far lower bandwidth, which I believe is important for inferencing. The P100 also has dramatically higher FP16 and FP64 performance than the P40. The P40 achieves 11.7 Tflops at FP32, but only 183 Gflops at FP16 and 367 Gflops at FP64, while the P100 achieves 9.5 Tflops at FP32, and 19 Tflops at FP16 and 4.7 Tflops at FP64. With these factors in mind, and my intended use of inferencing, running LLMs locally, what would be the best setup: two P40s, two P100s, or a P40 and a P100?"
156,learnmachinelearning,llm,comments,2023-12-12 15:00:30,Medical Diagnosis using LLM's,Sad-Ad-4850,False,1.0,1,18gnt86,https://www.reddit.com/r/learnmachinelearning/comments/18gnt86/medical_diagnosis_using_llms/,7,1702393230.0," Hey There,  
I am an undergraduate SE student and basically my FYP project is to create an application that uses LLM and provides prelimenary diagnosis based on reports that the user submits in the form of PDF's or images.  
I am just starting to work on this project and just so yall know i am fairly new to all this and am learning on things on the go.  
One of my first steps would be to use a LLM pretrained model like BioGPT or Meditron but i genuinely cant figure out where to start. I'm spending alot of time on figuring what the whole project pipeline would be like and would really help your feedback if you have any on the matter.  
Thanks! "
157,learnmachinelearning,llm,comments,2023-11-03 07:31:16,Need Career Guidance,Potential_Plant_160,False,0.67,1,17mpma5,https://www.reddit.com/r/learnmachinelearning/comments/17mpma5/need_career_guidance/,7,1698996676.0,"Hi ,I am Basically from Non tech Background and now I am  working as AI developer since past  7 months in a Research Project,since it's a Research project there is not much of Coding and all ,we are still in Data Collection part and in the mean time I am Doing some Kaggle Projects and I am also Learning Deep Learning and NLP and Pytorch frameworks.

But I want to switch into Software Company,but since last 3 months My resume is not getting shortlisted,I think it's because of  Projects ,which I wanna improvise 
Can u guys help me with these 

1.How to Get shortlisted for ML Engineer or AI Developer Role for this much Experience.

2.What type of Projects do I have to do ,if u guys have any resources that would help a lot.

3. Do I have to Do End to End Projects

4. how much Python Proficiency is required to get a job for this much experience and How to improve my Python skills , because I am failing to crack coding rounds in Interviews.

5.which type of Projects I should give more focus to Like NLP, Computer Vision,LLM ,Deep Learning.

6.Do I have to Get any Particular skils other than this.

7.How to Build Portfolio and where can I showcase it and what are the Good Projects I can Do to get shortlisted.

8.Also I wanna do  Data Science Master through online ,Any suggestions?

I found one Master course for Data science in VIT, Tamilnadu,whats ur Opinion about this Course?

9.Which is better Data Science master degree or AI master degree,is it even worth it for me , because I am from NON tech Background."
158,learnmachinelearning,llm,comments,2023-10-07 21:20:01,What would be the best option to utilize a GPU remotely?,AbstractContract,False,0.86,14,172h7ya,https://www.reddit.com/r/learnmachinelearning/comments/172h7ya/what_would_be_the_best_option_to_utilize_a_gpu/,6,1696713601.0,"Specifically I want to be able to use my desktop equipped with a 3060 at home for LLM inference from my laptop and Raspberry Pi based devices. I can connect to my home network via tailscale, but am rather unsure how to access that GPU power easily. The best way I could think of would be using wake on LAN to power up the desktop and then remotely access it to execute code and use mdels hosted on that machine. Would there be a better way and is there a recommended setup for this kind of scenario? "
159,learnmachinelearning,llm,comments,2023-08-04 14:50:48,Looking for recommendations for an LLM chat that focuses on programming,UpvoteBeast,False,0.75,4,15i24x6,https://www.reddit.com/r/learnmachinelearning/comments/15i24x6/looking_for_recommendations_for_an_llm_chat_that/,6,1691160648.0,"Hi everyone,

I am interested in LLM chat that has been fine tune for programming tasks. I've been using chatGPT for helping me write code but having a local version or other alternatives would be good. Does anyone have any recommendations for an LLM that is fine tuned for programming questions?"
160,learnmachinelearning,llm,comments,2023-06-22 23:53:45,What is a policy in Reinforcement Learning (ChatGPT)?,u2uu,False,1.0,4,14gj3fq,https://www.reddit.com/r/learnmachinelearning/comments/14gj3fq/what_is_a_policy_in_reinforcement_learning_chatgpt/,6,1687478025.0,"Hey,

i read about the RLHF in ChatGPT. And in Step 1 there is the title saying: ""training a supervised policy"".

And in step 3 is it then saying: ""The PPO model is initalized from the supervised policy.""

I really dont understand exactly what is meant with policy. It is the neural network itself of the LLM? Is ist the SFT Model from Step 1? 

And if the ""PPO-Model is initalized"" is this in the beginning exactly like the SFT-Model from step 1? 

I read a lot about ""functions"" regarding to ""policy"". But sometimes i read that the policy is a neural network. So i am really confused. Is the policy a seperate entity? Is it not the neural network of the LLM itself?


I am sorry for my english. I would be very happy for help!"
161,learnmachinelearning,llm,comments,2023-11-16 21:34:20,AI/LLM starter kit in open source repo,linamagr,False,0.82,7,17wy4aw,https://www.reddit.com/r/learnmachinelearning/comments/17wy4aw/aillm_starter_kit_in_open_source_repo/,6,1700170460.0,"Share a Github repository to quickly build and start a local application to chat with private documents. The stack used is python,  [\#LangChainAI](https://www.linkedin.com/feed/hashtag/?keywords=langchainai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7130952995793489920) , [\#qdrant\_engine](https://www.linkedin.com/feed/hashtag/?keywords=qdrant_engine&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7130952995793489920) [\#Ollama\_ai](https://www.linkedin.com/feed/hashtag/?keywords=ollama_ai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7130952995793489920) and [\#FastAPI](https://www.linkedin.com/feed/hashtag/?keywords=fastapi&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7130952995793489920)  
)  
[https://github.com/mallahyari/ai-starter-kit](https://github.com/mallahyari/ai-starter-kit)   
"
162,learnmachinelearning,llm,comments,2024-01-18 14:44:44,Project: QA on any PDF document using RAG and VectorDB,Amazing_Life_221,False,0.76,12,199rq4b,https://i.redd.it/c0cfiqr0o7dc1.jpeg,6,1705589084.0,"The Smart PDF Reader is a comprehensive project that harnesses the power of the Retrieval-Augmented Generation (RAG) model over a Large Language Model (LLM) powered by Langchain. Additionally, it utilizes the Pinecone vector database to efficiently store and retrieve vectors associated with PDF documents. This approach enables the extraction of essential information from PDF files without the need for training the model on question-answering datasets.

Find the GitHub repo: [here](https://github.com/Arshad221b/RAG-on-PDF)"
163,learnmachinelearning,llm,comments,2024-02-12 23:44:18,This is highly likely to be a dumb post.. but need a roadmap,liberaltilltheend,False,0.25,0,1apetpy,https://www.reddit.com/r/learnmachinelearning/comments/1apetpy/this_is_highly_likely_to_be_a_dumb_post_but_need/,6,1707781458.0,"I am not a CS student. I majored in Arts. But I have studied Frontend development, so not my first contact with anything CS related.

I want to learn to work with LLMs. I know I need to define ""work"" here, but frankly I am still working out the definition myself.

My goal is to be able to create an LLM-based AI product in about 5 years. So I want to learn as much as possible. So where do I start?

Is this: https://roadmap.sh/ai-data-scientist a good way go about it?

I know Python and basic DSA."
164,learnmachinelearning,llm,comments,2023-04-30 07:37:19,What LLM should I try to run locally? I have a workstation with two RTX 6000 Ada’s.,Worldbuilder87,False,0.83,8,133ityb,https://www.reddit.com/r/learnmachinelearning/comments/133ityb/what_llm_should_i_try_to_run_locally_i_have_a/,6,1682840239.0,
165,learnmachinelearning,llm,comments,2023-04-07 13:31:28,Training opensource LLM (eg Alpaca/GPT4All) on my own docs?,Soc13In,False,1.0,8,12elfp1,https://www.reddit.com/r/learnmachinelearning/comments/12elfp1/training_opensource_llm_eg_alpacagpt4all_on_my/,6,1680874288.0,Is it possible to train an LLM on documents of my organization and ask it questions on that? Like what are the conditions in which a person can be dismissed from service in my organization or what are the requirements for promotion to manager etc. All this information is captured in PDFs. How would one go about doing this?
166,learnmachinelearning,llm,comments,2023-09-14 05:10:15,"Which LLM can I run locally on my MacBook Pro M1 with 16GB memory, need to build a simple RAG Proof of Concept.",sf_d,False,1.0,12,16i9g51,https://www.reddit.com/r/learnmachinelearning/comments/16i9g51/which_llm_can_i_run_locally_on_my_macbook_pro_m1/,6,1694668215.0,"I am in the process of building a simple proof of concept for Retrieval-augmented generation (RAG) and would like this to be locally hosted on my MacBook Pro M1 with 16 GB memory.

What options do I have for the LLM's selection?"
167,learnmachinelearning,llm,comments,2023-05-17 22:48:25,"Transitioning to ml, focusing on LLM",World-Curiosity,False,0.57,1,13kgsjy,https://www.reddit.com/r/learnmachinelearning/comments/13kgsjy/transitioning_to_ml_focusing_on_llm/,5,1684363705.0,"Hello all

I've been a software developer for 20 years now. Very proficient in programming languages and infrastructures.
Now I want to do something ""simple"". Take an open source llm, setup an infrastructure and infer on it. Plus create a few fine tuned models.
I'm looking at some documentations and I really have no idea what they are talking about. Zero. The technology, concepts and terms are meaningless to me.

I was wondering if anyone knows a good course to transition in ml. Or any other method that both teaches hands-on techniques while providing fundamental knowledge.
I prefer to focus on LLM initially as my company have a few ideas around them.

I'm feeling a bit left behind. Please help me to feel relevant still :)"
168,learnmachinelearning,llm,comments,2023-07-27 11:46:34,LLM Guide [Discussion],torspayorryum,False,0.92,27,15azq0q,https://www.reddit.com/r/learnmachinelearning/comments/15azq0q/llm_guide_discussion/,6,1690458394.0,"Nowadays, If we see over the internet that LLM, chatgpt , llma etc are the trending topics and are being discussed. My question is that anyone can help me where to start studying about these topics from scratch ? BERT, Transformer etc all I want to understand everything.

It would be good if you help me out.

Thanks"
169,learnmachinelearning,llm,comments,2023-10-08 06:11:25,How are you evaluating and monitoring LLMs?,GuillerminaCharity,False,0.97,21,172ruzm,https://www.reddit.com/r/learnmachinelearning/comments/172ruzm/how_are_you_evaluating_and_monitoring_llms/,6,1696745485.0,"Question for people who are implementing LLMs (open source, fine tuned, any kind).

1. How do you know that your getting the quality output from the model that you need to ship the feature or model? Are the audits ad hoc data sampling and subjective ""good/bad"" ratings or have you figured out a more rigorous framework? Is it pretty much \~vibes\~ based?
2. What, if any, tools or processes are you putting into place to monitor and observe the LLM when its interacting with real time user data for weeks or months?

Most of the folks I have spoken with are doing very ad hoc sampled output and writing down on post its or in a spreadsheet a subjective quality ratings.

One person had developed a slightly more rigorous 3 question survey on ""is the result factual"", ""is the result cogent"" and ""is the result useful"". Not everyone is logging their LLM responses they show users which feels very risky to me.

Anyone aware of any industry standards being established around this?"
170,learnmachinelearning,llm,comments,2023-05-15 21:21:01,Resource for creating your own personal ChatGPT tailored to your own data,rajatarya,False,0.78,17,13ikxwt,https://www.reddit.com/r/learnmachinelearning/comments/13ikxwt/resource_for_creating_your_own_personal_chatgpt/,6,1684185661.0,"Hey everyone,  


I was trying to create a personal ChatGPT that can answer questions and create expert content based on an existing dataset. I thought there are tons of applications for this, so [I created a workshop](https://app.livestorm.co/xethub/mygpt-free-workshop-build-a-chatgpt-clone-tailored-to-your-data?type=detailed&utm_source=reddit&utm_medium=social&utm_campaign=openaireddit) so you can create your own app - I’m calling it “MyGPT”.  


In this workshop I’ll be covering:

* How to create a Generative AI app using the DaVinci model (the same one used by ChatGPT) 
* How a Generative AI application is structured (the tech stack)
* Integrating your own data into a Large Language Model (LLM)
* Getting started with XetHub (similar to GitHub but easier for ML models)
* Create a Python app that uses Gradio & LangChain

If you’d like to check it out, [sign up here](https://app.livestorm.co/xethub/mygpt-free-workshop-build-a-chatgpt-clone-tailored-to-your-data?type=detailed&utm_source=reddit&utm_medium=social&utm_campaign=openaireddit)!"
171,learnmachinelearning,llm,comments,2023-11-25 04:01:04,First Time Working With Machine Learning,Need_More_Learn,False,1.0,5,183b02m,https://www.reddit.com/r/learnmachinelearning/comments/183b02m/first_time_working_with_machine_learning/,6,1700884864.0,"Hello all,  
I am a new developer trying to get into the field of Machine Learning. For my first project I am trying to tune an LLM (have not decided which one yet, but it will be open source) from a bunch of text articles online. I want the LLM to produce more accurate and detailed results on a certain niche topic. So far I have collected the data in a large text file and cleaned the data (lower cased, removed punctuations, standardized the spacing, removed proper nouns,  and Lemmatized). I have also tokenized the data at sentence level, as I believe this will maintain more of the context and jargon of the writings.   
From Here I am a bit lost. After tokenizing the data what should I do? Should I embed the data? I already tried with fasttext, but it made my file significantly larger. I used both the English Crawler and Wikipedia Versions. Also is it possible to mix the two embedings? Furthermore I am wondering how would I go about doing this without labeling data, since it seems odd to do this with such a large file manually. How would I do MLM or NSP?? Also If I do MLM or NSP, do I no longer need to embed the data into vectors?? Also how exactly should my text files look like before and after embedding and or doing MLM/NSP?  
Thank you and Happy Thanksgiving weekend!"
172,learnmachinelearning,llm,comments,2023-04-01 19:39:01,Fine Tuning + Quantizing LLaMa on rented instance?,dev-matt,False,1.0,1,128we58,https://www.reddit.com/r/learnmachinelearning/comments/128we58/fine_tuning_quantizing_llama_on_rented_instance/,6,1680377941.0,"New researcher here. Out of curiosity, has anyone had success in both fine tuning a pretrained model (llama or open source LLM with weights) on a virtualized/rented gpu instance and then also quantizing the model to run via alpaca.cpp or pyllama etc. for consumer hardware? If so, please reach out. Will pay for your expertise! Or if you know a better approach then let me know.

I've tried with alpaca.cpp, but the training requires docker which won't work on virtualized instance.

I've tried alpaca-lora, but got many errors running the training script.

Still looking at other open source options like Lit-LLaMa and GPT4All."
173,learnmachinelearning,llm,comments,2023-07-12 12:38:52,A roadmap to understand the theory of LLMs,RageA333,False,1.0,7,14xng6t,https://www.reddit.com/r/learnmachinelearning/comments/14xng6t/a_roadmap_to_understand_the_theory_of_llms/,6,1689165532.0,"I wanted to kindly ask for resources for the theory of LLM models. I have a strong mathematical background but a weak understanding on the theoretical side of neural networks. I don't mind starting from the very basics (in fact, I would greatly appreciate a long self-contained approach!)

Thanks for the help!"
174,learnmachinelearning,llm,comments,2023-04-06 12:27:29,[question] What Architecture Or Model Would You Use To Build A Non-Speech Acoustic Encoder?,Simusid,False,1.0,4,12diqjw,https://www.reddit.com/r/learnmachinelearning/comments/12diqjw/question_what_architecture_or_model_would_you_use/,6,1680784049.0,"I have lots of domain specific acoustic data, mostly of mechanical machinery.    I want to build an embedding model of this audio.    My hope is that if well trained, then similar acoustic events will have similar embeddings.    This follows from NLP semantic similarity of two sentences.  

I've been training VITMAE on spectrograms for days and while the loss continues to go down, I'm not encouraged by the results.    I've tried simple autoencoders in the past with time domain inputs and again, not seeing great results.    I'm considering building something with wav2vec 2.0 or wavenet next.

Ideally, I want a ""Large Acoustic Model"" similar in scale and capability to that of a LLM (a lofty goal, I know).    I'd like to hear your thoughts about other approaches to build embedding models for non-speech acoustics."
175,learnmachinelearning,llm,comments,2024-01-15 14:52:58,Need a roadmap or course to get started with llms,ProfessionalIdiot2,False,0.83,4,197ajgs,https://www.reddit.com/r/learnmachinelearning/comments/197ajgs/need_a_roadmap_or_course_to_get_started_with_llms/,6,1705330378.0,"I want to go deep into LLMs, I have some beginner-level knowledge of LLM but get lost while I try to continue further, anyone into LLMs how did you start learning? please provide me with guidance and a proper roadmap or some free courses, I love reading from books and video tutorials both. "
176,learnmachinelearning,llm,comments,2023-06-12 12:48:21,Implement LLM from research paper example LLAMA model?,Accomplished-Clock56,False,0.72,3,147nb20,https://www.reddit.com/r/learnmachinelearning/comments/147nb20/implement_llm_from_research_paper_example_llama/,6,1686574101.0,"For those who have already or doing any implementations of research paper. 

I was asked  to implement Llama paper, 
Can you share any resources or examples  to achive this goal, there are some open source implementations of LlAmA but I'm not sire if they are really implementations  of the LLAMA  paper. 

Any suggestions or resources will help me
Thank you"
177,learnmachinelearning,llm,comments,2023-09-12 11:41:19,"Trying to finetune an LLM. The dataset has 53 rows and 3 columns in user, context, and assistant format. Why is the training taking so long? Please check the screenshots. Ideally, there should be 5-6 iterations/50 -60 steps. Why is that the training is just half done and it is already at 230 steps?",AccomplishedPin4075,False,0.5,0,16gp3gx,https://www.reddit.com/r/learnmachinelearning/comments/16gp3gx/trying_to_finetune_an_llm_the_dataset_has_53_rows/,6,1694518879.0,"&#x200B;

https://preview.redd.it/l2eofculatnb1.png?width=1818&format=png&auto=webp&s=23a1ea43f81baa98203a08dd0d03e7bc7c91103e

https://preview.redd.it/dpusn9ulatnb1.png?width=1844&format=png&auto=webp&s=5e025842967b8e1051e8fe16caaa5cfb99e29392"
178,learnmachinelearning,llm,comments,2023-07-13 15:35:00,Looking for Resources to Learn LLM in depth,EmoryCadet,False,0.95,33,14yo1m0,https://www.reddit.com/r/learnmachinelearning/comments/14yo1m0/looking_for_resources_to_learn_llm_in_depth/,5,1689262500.0,"I have some background in deep learning (e.g. ML courses, training ranking and classification models), but I’m looking for resources (blogs, videos, podcasts, courses) to learn more about LLMs. Are there any resources that you’d recommend?

 "
179,learnmachinelearning,llm,comments,2023-11-04 15:54:36,Approaches to finding topics in short texts,zeoNoeN,False,1.0,2,17nolqt,https://www.reddit.com/r/learnmachinelearning/comments/17nolqt/approaches_to_finding_topics_in_short_texts/,5,1699113276.0,"Full transparency because I want to be honest: This is a problem I face at work, a business will profit from your answers. 

I have a large dataset of customer feedbacks. Texts are short and multilingual (translated to English via another model). I want to find out what customers talk about, what their problems are and what they like about the service they where provided. 

I tried some topic modeling approaches (LSS, LDA, LLM Embeddings + Clustering, LLM „zero-shot“ labels ). The resulting Topics were really messy and didn’t fit to my „intuitive topics“. 

What approaches would everyone here recommend next. If I search for topics manually, could I use ML approaches to support this? How have you talked similar problems in the past? 

Thanks in advance!"
180,learnmachinelearning,llm,comments,2024-01-31 17:35:42,Research Questions about LLMs,DatAndre,False,1.0,2,1afngpx,https://www.reddit.com/r/learnmachinelearning/comments/1afngpx/research_questions_about_llms/,5,1706722542.0,"The company I'm working for is (obviously) trying to deploy some LLM-based applications. At the same time, I should write my master's thesis. 

It would be very convenient to do both the thesis and the work-rated activities on the same project. Therefore, that's what i' m currently doing. 

What could he some possibile research questions? 

In this case, the LLM is used as an API agent (given the prompt, it creates a json payload for an API and returns the result) 

For now, here's what came up to my mind:

- How well does the LLM understand the prompt and generate a successful response? 
- How can we evaluate the correctness of the completion? 
- How does the LLM-UI compare with a traditional UI in terms of efficacy and satisfaction? 
- How affected is the scalability of the system?"
181,learnmachinelearning,llm,comments,2023-12-07 16:22:22,Future of people in Data / ML,Alarming_Scene126,False,0.67,1,18cz84u,https://www.reddit.com/r/learnmachinelearning/comments/18cz84u/future_of_people_in_data_ml/,4,1701966142.0,"With the introduction of Gemini, gpt4, Hugging Face, Artificial General Intelligence and other LLM models is there a risk for people seeking jobs in Data scientist, Analyst, Engineering roles? Can Any Experts/professionals in this field tell us 

1. How the market is going to change with these new tools?

2. Is there any scope for freshers?

3. Is learning ML from the beginning worth it or should beginners learn how to use these tools and models instead?

4. What are the ""not to miss"" tools that are being used recently?

5. Things that related which are important that I missed in these questions."
182,learnmachinelearning,llm,comments,2023-05-20 02:34:38,Resume classification problem,SirPiano,False,1.0,5,13mfioc,https://www.reddit.com/r/learnmachinelearning/comments/13mfioc/resume_classification_problem/,5,1684550078.0,"I am thinking about doing a resume sentiment model.

I want to consider text with context (probably by getting embeddings via a pre trained llm), but I also want to consider the layout of the resume (like how it looks visually via bullet points, spacing, lines)

For example, if the resume is well written but a poor layout then its a bad resume. Also the other way, if the resume has a good layout but poorly written its a bad resume as well.

My question is how can I build a neural network so that it considers context based words as well as the layout to train it on the good or bad target labels."
183,learnmachinelearning,llm,comments,2023-10-28 08:23:24,Why GBDTs are not mentioned in job descriptions?,pg860,False,0.72,8,17i99av,https://www.reddit.com/r/learnmachinelearning/comments/17i99av/why_gbdts_are_not_mentioned_in_job_descriptions/,4,1698481404.0," GBDT allow you to iterate very fast, they require practically no data preprocessing, enable you to incorporate business heuristics directly as features, and immediately show if there is explanatory power in features in relation to the target.

On tabular data problems, they outperform Neural Networks, and many use cases in the industry have tabular datasets.

Because of those characteristics, [they are winning solutions to all tabular competitions on Kaggle](https://jobs-in-data.com/blog/data-science-skills#sota-ml-models)

And yet, somehow they are not very popular.

On the chart below, I summarized learnings from 9,261 job descriptions crawled from 1605 companies in Jun-Sep 2023 (source: [https://jobs-in-data.com/blog/machine-learning-vs-data-scientist](https://jobs-in-data.com/blog/machine-learning-vs-data-scientist))

LGBM, XGboost, Catboost (combined together) are the 19th mentioned skill, e.g. with Tensorflow being x10 more popular.

It seems to me Neural Networks caught the attention of everyone, because of the deep-learning hype, which is justified for image, text, or speech data, but not justified for tabular data, which still represents many use - cases.

Granted, there is for sure some noise in the data generation process of writing job descriptions - some people writing them may not know the exact scope of the role.

But why do those random people know so much more about deep learning, keras, tensorflow, pytorch than GBDT? In other words, why is there a systematic trend in the noise? When the noise has a trend, it ceases to be noise.

https://preview.redd.it/ohef6ocukwwb1.png?width=2560&format=png&auto=webp&s=7c2e0321deb0a9491db09668c94c34d510e77c05

&#x200B;

&#x200B;"
184,learnmachinelearning,llm,comments,2023-06-17 15:49:30,How to Build LLM Applications With LangChain and Openai,mwitiderrick,False,0.9,8,14buddi,https://www.reddit.com/r/learnmachinelearning/comments/14buddi/how_to_build_llm_applications_with_langchain_and/,5,1687016970.0,"LangChain is one the most popular tools for building large language model applications.   You can use LangChain to build various applications, such as question-answering systems and chatbots.   Some of the modules in Langchain include: 

**•** **Models** for supported models and integrations 

**• Prompts** for making it easy to manage prompts 

**• Memory** for managing the memory between different model calls 

**• Indexes** for loading, querying, and updating external data 

**•Chains** for creating subsequent calls to an LLM

 **• Agents** to develop applications where the LLM model can direct itself 

**• Callbacks** for logging and streaming the intermediate steps in a chain 

Today over a thousand subscribers of mlnuggets got a tutorial on how to use LangChain and other language models, such as the ones from Openai, to create a system to transcribe and ask questions to YouTube videos. 

Check it out [https://www.machinelearningnuggets.com/how-to-build-llm-applications-with-langchain-and-openai/](https://www.machinelearningnuggets.com/how-to-build-llm-applications-with-langchain-and-openai/)"
185,learnmachinelearning,llm,comments,2023-10-22 11:38:41,Looking for Resources to Learn LLM in depth,UpvoteBeast,False,0.96,41,17drbyo,https://www.reddit.com/r/learnmachinelearning/comments/17drbyo/looking_for_resources_to_learn_llm_in_depth/,5,1697974721.0,"I have some background in deep learning (e.g. ML courses, training ranking and classification models), but I’m looking for resources (blogs, videos, podcasts, courses) to learn more about LLMs. Are there any resources that you’d recommend?"
186,learnmachinelearning,llm,comments,2024-02-18 14:37:48,What are the subjects taught in university Machine Learning courses in 2024?,adastro,False,0.82,7,1atvhni,https://www.reddit.com/r/learnmachinelearning/comments/1atvhni/what_are_the_subjects_taught_in_university/,5,1708267068.0,"I studied ML at uni just a few years ago, before LLMs became the new focus of the ML industry. Back then, we would study stuff like Naive Bayes, linear/logistic regression, Support Vector Machines, Nearest Neighbor classification, K-Means clustering and Neural Networks. 

After just a couple of years, I feel like this is not the focus anymore. At least, that's what I perceive by looking around: podcasts, newsletters and social media seem to be all about LLMs and generative AI.

I know this is partly a distortion produced by the media, and most ML-related companies don't use any kind of LLM. However, I woke up today wondering: what are new ML students studying these days? Are they prepared for interviewing in this new industry scenario? Do university courses meet their early expectations, or are universities still teaching the same material as a few years ago?  
"
187,learnmachinelearning,llm,comments,2023-10-01 20:37:56,LLM Firewall - Guardrail Tutorial and Quickstart with OpenAI and Colab,Educational_Grass_38,False,0.94,12,16xc53k,https://m.youtube.com/watch?v=EnwVnz07h1I&pp=ygUSR3VhcmRyYWlsIEZpcmV3YWxs,5,1696192676.0,"Been working on a Firewall for devs to use in a few lines of code, to implement a protective layer around LLMs like OpenAI. Firewall has over 20+ detectors out-of-the-box including prompt injections, harmful content, toxicity and common security vulnerabilities.

Google Colab QuickStart: https://github.com/guardrail-ml/guardrail

Developer Docs: https://docs.useguardrail.com

Would appreciate if you could give a star and provide feedback, thanks!"
188,learnmachinelearning,llm,comments,2024-01-22 19:04:12,How do you guys evaluate LLM?,UpvoteBeast,False,1.0,11,19d3ep8,https://www.reddit.com/r/learnmachinelearning/comments/19d3ep8/how_do_you_guys_evaluate_llm/,5,1705950252.0,"how do you guys evaluate LLM? There is online leaderboard: [https://huggingface.co/spaces/HuggingFaceH4/open\_llm\_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

Is there any script that can automatically evaluate our performance offline/benchmark?"
189,learnmachinelearning,llm,comments,2023-04-28 12:28:22,Training a (L)LM with free Colab tier/Midrange GPU?,Every-Dust9140,False,0.83,4,131qajt,https://www.reddit.com/r/learnmachinelearning/comments/131qajt/training_a_llm_with_free_colab_tiermidrange_gpu/,5,1682684902.0,"Hello,

I'm new to machine learning, especially language models. I want to train a model that talks like me (or any other style), but I don't want to spend money on a cloud GPU(s). Is it possible to do it with what I have, or will the model be too small to be useful? 

I have tried to train/fine-tune a number of different models and the only one that would actually work was ""GPT-2"" however output from it was really bad.

From all the recently released LLMs the closest one I got to train successfully was ""LLaMa 7b hf"", I was training it using LoRA and peft, it would train it but then crash during the execution of \`model.save\_pretrained\`

Perhaps I could be understanding it wrong but having tried out multiple LLMs such as LLaMa Alpaca (7B), StableLM (3B) and GPT4ALL, seeing how well they run and understand language, especially given that I only have 8GB of VRAM and 16GB of RAM on my PC, It is surprising to me that free Colab tier with almost double the amount of VRAM can't handle training any of those models?

My question is can someone recommend a model or preferably a GitHub repo/Colab notebook that could help me achieve what I want.

&#x200B;

&#x200B;

TLDR: I am trying to train/fine-tune a LLM using free Colab tier but most of them crash due to out of memory error, can someone help me out without requiring me to pay for GPU(s)."
190,learnmachinelearning,llm,comments,2023-10-13 14:23:10,Authoring another course about LLMs. Learn by Doing LLM Projects.,pmartra,False,0.86,24,176zx1m,https://www.reddit.com/r/learnmachinelearning/comments/176zx1m/authoring_another_course_about_llms_learn_by/,5,1697206990.0,"Hi, I'm working on a course about LLMs on GitHub, it's totally free and under MIT license,  So there are no restrictions.

Here the link: [https://github.com/peremartra/Large-Language-Model-Notebooks-Course](https://github.com/peremartra/Large-Language-Model-Notebooks-Course)

I'm still working on It, but now I'm feeling comfortable with the variety and quality of the content. By the moment is a small repository with just 80 Stars.

My intention is to make the course more accessible to a wider audience, and, if possible, encourage  reporting any issues  encounter or suggesting improvements through the 'Discussion' section.

I'm eager to receive feedback.

Now, I'll provide an overview of the currently available content, and then I'll share a couple of questions I have about how to proceed with the course.

[Large Language Models Course: Learn by Doing LLM Projects.](https://github.com/peremartra/Large-Language-Model-Notebooks-Course)

* Introduction to LLM with OpenAI.
   * Create a first Chatbot using FPT 3.5.
   * Create a Natural Language to SQL Translator using OpenAI.
* Vector Databases with LLM.
   * Influencing Language Models with Information stored in ChromaDB.
* LangChain & LLM Apps.
   * RAG. Use the Data from Dataframes with LLMs.
   * Create a Moderation System using LangChain.
      * OpenAI.
      * GPT\_j.
      * LLama-2.
   * Create a Data Analyst Assistant using a LLM Agent.
* Evaluating LLMs
   * Evaluating Summarization with ROUGE.
* Fine-Tuning & Optimization.
   * Prompt-tuning using PEFT.
   * Fine-Tuning with LoRA.
   * Fine-Tuning a Large Model in a GPU using QLoRA. 

That's all for the moment, but I'm adding new content regularly. I'm working on it only in my spare time (mainly nights when the family goes to sleep).

\_\_\_

I have a doubt, I don't know if add some information about platforms like W&B or Cohere?  or maybe it is a better idea to stay with more Open-Source libraries?

On the other hand, my intention is to develop a couple of projects utilizing the techniques covered in the initial part of the course (which I am currently working on).

Some of these projects will be hosted in the cloud on major platforms such as Azure or GCP, or AWS. Any preference?

Furthermore, there is a plan to create a third section that explains how Large Language Models (LLMs) fit into large-scale enterprise solutions, defining architectures in which LLMs are used but are not the sole components of the project.

I don't intend to create a community outside of GitHub, but I would like the repository to have more activity and not be the one determining the course's direction.

Hope you like it, and lease, feel free to contribute.

&#x200B;"
191,learnmachinelearning,llm,comments,2024-01-04 13:48:39,[D] Collaborative platform to train your models?,New_Detective_1363,False,1.0,2,18ydlay,https://www.reddit.com/r/learnmachinelearning/comments/18ydlay/d_collaborative_platform_to_train_your_models/,5,1704376119.0,"Hi, with my team we are training an OS LLM and want to train it/test it together (control of the experiments, evaluation, comments etc...)

For now we use weight & biases but personally I'm not a huge fan of their UX.

Do you have any other tools that you recommend ?

Thanks!"
192,learnmachinelearning,llm,comments,2023-07-08 16:27:26,Can an average person learn how to build a LLM model?,UpvoteBeast,False,0.79,11,14u8mmm,https://www.reddit.com/r/learnmachinelearning/comments/14u8mmm/can_an_average_person_learn_how_to_build_a_llm/,4,1688833646.0,"Recently, while using ChatGPT, I had a dream for the first time. I want to create a chatbot that can provide a light comfort to people who come for advice. I would like to create an LLM model using Transformer, and use our country's beginner's counseling manual as the basis for the database.

I am aware that there are clear limits to the level of comfort that can be provided. Therefore, if the problem is too complex or serious for this chatbot to handle, I would like to recommend the nearest mental hospital or counseling center based on the user's location. And, if the user can prove that they have visited the hospital (currently considering a direction where the hospital or counseling center can provide direct certification), I would like to create a program that provides simple benefits (such as a free Starbucks coffee coupon).

I also thought about collecting a database of categories related to people's problems (excluding personal information) and selling it to counseling or psychiatric societies. I think this could be a great help to these societies.

The problem is that I have never studied ""even once,"" and I feel scared and fearful of the unfamiliar sensation. I have never considered myself a smart person.

However, I really want to make this happen! Our country is now in a state of constant conflict, and people hate and despise each other due to strong propaganda.

As a result, the birth rate has dropped to less than 1%, leading to a decline in the population. Many people hide their pain inside and have no will to solve it. They just drink with their friends to relieve their pain. This is obviously not a solution. Therefore, Korea has a really serious suicide rate.

I may not be able to solve this problem, but I want to put one small brick to build a big barrier to stop hatred. Can an ordinary person who knows nothing learn the common sense and study needed to build an LLM model? And what direction should one take to study one by one?"
193,learnmachinelearning,llm,comments,2023-09-19 12:47:39,Machine-Learning with JavaScript,Vision157,False,1.0,2,16mq2qa,https://www.reddit.com/r/learnmachinelearning/comments/16mq2qa/machinelearning_with_javascript/,4,1695127659.0,"Hi there!
I'm an interactive designer  that knows how to code with JavaScript for front-end interactive experiences. 

I got curious about machine-learning at the beginning of the years, and I started to learn about basic concepts, theories and methods. However, I noticed that most of the courses are using Python (and it makes complete sense), but since I'm not planning to develop LLM or very complex ML tools, I was wondering if you guys know any good documentation, tutorial, bootcamp or similar to learn how to use Tensorflow.js.

My goal, is to being able to design and develop proof-of-concepts or MVPs.

I used Brain.js in the past for a few projects but definitely looking for to level up my game, and I know that once I get a good understanding of Tensorflow.js, it would be hard to explore Tensorflow using Python (even if I never used it before).

Thank you in advance"
194,learnmachinelearning,llm,comments,2024-02-15 19:26:02,Training LLM on git logs ??,Elegant-Catch-9648,False,1.0,2,1arnr51,https://www.reddit.com/r/learnmachinelearning/comments/1arnr51/training_llm_on_git_logs/,4,1708025162.0," 

Hello everyone,

I had a thought while reading ""Revealing Game Dynamics via Word Embeddings of Gameplay Data"" by Rabii and Cook (2021), where they explored game dynamics using word embeddings of gameplay data. This got me thinking about the potential of applying similar machine learning (ML) techniques to a different kind of data: Git logs.

Git logs are a treasure trove of structured data, chronicling the history of software development through commit messages, timestamps, authorship, and code changes. It dawned on me that training llm on this kind of data would be insane for Coding assistants.

But i didnt find anything mentioning this.

any ideas ?"
195,learnmachinelearning,llm,comments,2023-10-24 16:05:16,Sanity check on ML Build,ikiya13,False,1.0,1,17fgby2,https://www.reddit.com/r/learnmachinelearning/comments/17fgby2/sanity_check_on_ml_build/,4,1698163516.0,"My boss has asked me to spec out an LLM build for him since I have PC-building experience. We want to do fine-tuning and querying of open-source models (such as Flacon 7b, but we are very much still exploring models). He said my budget is $10k.

&#x200B;

&#x200B;

[PCPartPicker Part List](https://pcpartpicker.com/list/QQ3gn6)

|Type|Item|Price|
|:-|:-|:-|
|**CPU**|[AMD Ryzen 9 7950X 4.5 GHz 16-Core Processor](https://pcpartpicker.com/product/22XJ7P/amd-ryzen-9-7950x-45-ghz-16-core-processor-100-100000514wof)|$551.51 @ Amazon|
|**CPU Cooler**|[Deepcool AK620 68.99 CFM CPU Cooler](https://pcpartpicker.com/product/9T92FT/deepcool-ak620-6899-cfm-cpu-cooler-r-ak620-bknnmt-g)|$63.99 @ Amazon|
|**Motherboard**|[Asus ROG STRIX B650-A GAMING WIFI ATX AM5 Motherboard](https://pcpartpicker.com/product/Gjt9TW/asus-rog-strix-b650-a-gaming-wifi-atx-am5-motherboard-rog-strix-b650-a-gaming-wifi)|$222.99 @ Newegg|
|**Memory**|[Corsair Vengeance 128 GB (4 x 32 GB) DDR5-5600 CL40 Memory](https://pcpartpicker.com/product/HKn9TW/corsair-vengeance-128-gb-4-x-32-gb-ddr5-5600-cl40-memory-cmk128gx5m4b5600c40)|$439.99 @ Amazon|
|**Storage**|[Samsung 980 Pro 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/f3cRsY/samsung-980-pro-2-tb-m2-2280-nvme-solid-state-drive-mz-v8p2t0bam)|$138.00 @ Amazon|
|**Video Card**|[NVIDIA Founders Edition GeForce RTX 4090 24 GB Video Card](https://pcpartpicker.com/product/BCGbt6/nvidia-founders-edition-geforce-rtx-4090-24-gb-video-card-900-1g136-2530-000)|$1999.99 @ Amazon|
|**Video Card**|[NVIDIA Founders Edition GeForce RTX 4090 24 GB Video Card](https://pcpartpicker.com/product/BCGbt6/nvidia-founders-edition-geforce-rtx-4090-24-gb-video-card-900-1g136-2530-000)|$1999.99 @ Amazon|
|**Case**|[Fractal Design Torrent ATX Mid Tower Case](https://pcpartpicker.com/product/fv2WGX/fractal-design-torrent-atx-mid-tower-case-fd-c-tor1a-06)|$189.99 @ B&H|
|**Power Supply**|[Corsair AX1600i 1600 W 80+ Titanium Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/cJbwrH/corsair-1600w-80-titanium-certified-fully-modular-atx-power-supply-cp-9020087-na)|$609.99 @ Newegg|
|*Prices include shipping, taxes, rebates, and discounts*|||
|**Total**|**$6216.44**||
|Generated by [PCPartPicker](https://pcpartpicker.com) 2023-10-24 12:03 EDT-0400|||

&#x200B;

H100s and A6000 GPUs were out of the question due to price. I opted for 2 4090s due to their memory capacity and performance, especially with 8-bit, a 7950x for the cores, and maxed out the RAM.

The only hesitation I have is whether the second GPU will work in the bottom PCIe slot, but I think it should even though it's only wired at x4. I read that PCIe lanes don't matter as much, but I just wanted to check. Having a GPU in the bottom slot disables the third M.2 slot, which isn't really a problem as I'll just use the other ones.

My ML experience is pretty novice so I wanted to check with those with more experience. Is this a half-decent build? Thank you all for your time."
196,learnmachinelearning,llm,comments,2023-11-14 16:22:18,do you have any resources or scientific papers for evaluating LLM (especially for a domain-specific task: education),Life_Ask2806,False,0.67,1,17v5u3x,https://www.reddit.com/r/learnmachinelearning/comments/17v5u3x/do_you_have_any_resources_or_scientific_papers/,4,1699978938.0,"hello, i need to work on the state of the art of my research project ""evaluating LLM"" (characterization of existing LLMs, evaluation LL Models , benchmark & experiment, approach, metrics..). do you have any scientific papers or resources please? also can you give me some tips or advices as i want to talk specifically about domain specific task: education.  
thank you!"
197,learnmachinelearning,llm,comments,2023-12-19 21:39:30,Holiday LLM Quick-Start Primer,ACVonnegutteral,False,0.69,7,18mdbbe,https://i.redd.it/njfk72snmb7c1.png,4,1703021970.0,
198,learnmachinelearning,llm,comments,2024-02-20 20:03:33,Has anyone yet used EQ-BENCH benchmark for LLM evaluation?,MBU_NxtDoor,False,0.67,1,1avr9ii,https://www.reddit.com/r/learnmachinelearning/comments/1avr9ii/has_anyone_yet_used_eqbench_benchmark_for_llm/,4,1708459413.0,how to setup config.cfg file?
199,learnmachinelearning,llm,comments,2023-11-27 15:46:54,"I wan't to get into ML, but don't know where to start.",mmario312,False,0.4,0,1855yb6,https://www.reddit.com/r/learnmachinelearning/comments/1855yb6/i_want_to_get_into_ml_but_dont_know_where_to_start/,4,1701100014.0,"So I'm a Software Engineer but quite new to ML.  
I want to find some topic that would be interesting to get deep into, but don't seem to find a staring point. Blindly searching through arxiv org in't really helpful and courses are too general and not really captivating.  
I wan't to find something like LLM, but can't formulate a question to even start googling, etc. What are the fields within ML like LLM? I don't wan't to get into LLMs but maybe there is something else like it that is interesting to get into?  
Also if you want to suggest some websites, youtube channels, twitters, etc. that cover the news in NN, ML in general would be appreciated. "
200,learnmachinelearning,llm,relevance,2024-02-11 18:29:31,LLM finetuning,IamAriel30,False,0.67,1,1aoescp,https://www.reddit.com/r/learnmachinelearning/comments/1aoescp/llm_finetuning/,8,1707676171.0,"Hey guys!

I started a new project recently at work and the goal is to generate summaries for timeseries data (or their plots) and provide certain suggestions based on that. I do have experience in AI but never really bothered with NLP so I'm fairly new.

I'm a bit lost as to where to start. I thought about using GPT4 API  since Azure offers it, which can become a bit pricey and will still give generic suggestions as finetuning isnt available. My focus now shifted on fine tuning an open source LLM such as Mistral 7B on our data. 

My questions are:
1) is my current approach reasonable?
2) how to perform finetuning, in what form would I feed the data to the model and later generate prompts? The timeseries data is updated daily with new values.

Thank you 😊"
201,learnmachinelearning,llm,relevance,2024-01-15 08:18:55,Text Extraction (?) using LLm,Different_Star9899,False,0.78,5,1973rlm,https://www.reddit.com/r/learnmachinelearning/comments/1973rlm/text_extraction_using_llm/,9,1705306735.0,"  

Hello guys,

I want to do a research on using LLM to check and compare the specifications of Materials from different pdf files, but I have no idea where to start since my background is civil engineering and we only want to use LLM as a way to explore the capabilities of a LLM in construction industry. Basically, the process is as follows:

User inputs one project specifications pdf file first, and then inputs other multiple material specifications pdf files (let’s say Air-conditioners), and then the model extracts the information from all those files (eg: voltage, HP, etc) and generates a comparison chart between the project-required specifications and the proposed material specifications, and give out the similarity score (ie: which material is the most suitable for the project).

I am a Masters in Construction Engineering and Management so I have no knowledge about how the LLM works so any advice is welcome. I want to know which technology/method I should research, what would be the general framework, and which model might be the most suitable for this use case.

PS: I know that NLP would be must better for this, but part of my research objective is to compare how easy LLM is to develop compared to the traditional NLP."
202,learnmachinelearning,llm,relevance,2024-01-16 04:47:29,Learn LLM in depth,cho_odama_rasengan,False,1.0,1,197ube8,https://www.reddit.com/r/learnmachinelearning/comments/197ube8/learn_llm_in_depth/,2,1705380449.0,"I want to learn Seq to seq models from scratch to LLM..(Covering all the theory and architecture) in a detailed manner

Can anyone suggest me some good resources?"
203,learnmachinelearning,llm,relevance,2024-02-15 19:26:02,Training LLM on git logs ??,Elegant-Catch-9648,False,1.0,2,1arnr51,https://www.reddit.com/r/learnmachinelearning/comments/1arnr51/training_llm_on_git_logs/,4,1708025162.0," 

Hello everyone,

I had a thought while reading ""Revealing Game Dynamics via Word Embeddings of Gameplay Data"" by Rabii and Cook (2021), where they explored game dynamics using word embeddings of gameplay data. This got me thinking about the potential of applying similar machine learning (ML) techniques to a different kind of data: Git logs.

Git logs are a treasure trove of structured data, chronicling the history of software development through commit messages, timestamps, authorship, and code changes. It dawned on me that training llm on this kind of data would be insane for Coding assistants.

But i didnt find anything mentioning this.

any ideas ?"
204,learnmachinelearning,llm,relevance,2023-12-28 01:47:16,LLM code recommendations,Level-Violinist1858,False,1.0,1,18shrdh,https://www.reddit.com/r/learnmachinelearning/comments/18shrdh/llm_code_recommendations/,1,1703728036.0,I am proficient in ML and NLP but I want to start with LLMs. I have seen few YouTube videos but most of them are theoretical. I am looking something which can guide me with code to start with . Like building some app use case with LLM model. I hear lot of my peers about LLama or Mistral model but I don’t know where to start. Appreciate any resources like GitHub repos or blogs anything . Thanks
205,learnmachinelearning,llm,relevance,2024-01-22 19:04:12,How do you guys evaluate LLM?,UpvoteBeast,False,1.0,12,19d3ep8,https://www.reddit.com/r/learnmachinelearning/comments/19d3ep8/how_do_you_guys_evaluate_llm/,5,1705950252.0,"how do you guys evaluate LLM? There is online leaderboard: [https://huggingface.co/spaces/HuggingFaceH4/open\_llm\_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

Is there any script that can automatically evaluate our performance offline/benchmark?"
206,learnmachinelearning,llm,relevance,2024-02-03 09:50:44,LLM prompt for glossary analysis,RDA92,False,1.0,1,1ahsnj7,https://www.reddit.com/r/learnmachinelearning/comments/1ahsnj7/llm_prompt_for_glossary_analysis/,0,1706953844.0,"I have multiple pages of glossaries which reflect a term - definition format (e.g. USD - United States Dollar and which I would like to store in a python dictionary of {Term : Definition). Since, simple text chunking techniques don't produce reliable results given the lack of text separators, we have started to look into LLM models.

However we struggle to come up with a well-working prompt. So far our zero shot prompts generate only a small fraction of the terms and definitions (those that it returns are in desired format though) and few-shot prompting often weirdly returns the examples used in the prompt. 

Anyone able to point me in a direction on what prompt to use or how to improve the zero shot prompt? 

Thanks!"
207,learnmachinelearning,llm,relevance,2024-02-21 17:14:02,Getting started with training LLM models,TocTheYounger_,False,0.67,1,1awhh4h,https://www.reddit.com/r/learnmachinelearning/comments/1awhh4h/getting_started_with_training_llm_models/,2,1708535642.0,"Hi, I'm new to machine learning an AI, and I've been playing around with Ollama and multiple LLM's. Now I'd like to get started with machine learning. I'd like to hear with what courses or materials did you start with when getting into machine learning? My own goal is to train a ready made small LLM like mistral or Phi to be good at translating a few languages for example. Where did you learn about datasets? Have you used ready made datasets in the beginning or headed straight in creating your own?

&#x200B;

TL;DR: How did you start learning machine learning? "
208,learnmachinelearning,llm,relevance,2023-06-14 09:08:23,"Introducing, OpenLLM 🎉",AaZasDass,False,0.96,88,149302y,https://www.reddit.com/r/learnmachinelearning/comments/149302y/introducing_openllm/,15,1686733703.0,"OpenLLM allows you to run inferences with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps. It includes simple and familiar APIs, enabling easy integration with tools such as LangChain, and BentoML! Discover more at [https://github.com/bentoml/OpenLLM](https://github.com/bentoml/OpenLLM)

To get started, install it with pip: `pip install -U openllm`  Currently, it has support for all major SOTA LLMs, including Falcon, ChatGLM, Dolly V2, StableLM, and more to come!

Some of the feature that is currently wip:

\- Fine-tuning API with `LLM.tuning()`

\- LangChain integration [https://github.com/hwchase17/langchain/pull/6064](https://github.com/hwchase17/langchain/pull/6064)

\- OpenAI Compatible API

    import openai
    
    openai.api_base = ""http://localhost:3000"" # Running with OpenLLM
    
    completion = openai.Completion.create(...)

We are currently actively developing the library, so we would love to hear your thoughts and feedback. Feel free also to join our [discord](https://l.bentoml.com/join-openllm-discord) to meet other fellows, AI application builders, and enthusiasts."
209,learnmachinelearning,llm,relevance,2023-05-25 12:48:49,Visual intuitive explanations of LLM concepts (LLM University),jayalammar,False,0.88,25,13rgwpf,https://llm.university/,1,1685018929.0,
210,learnmachinelearning,llm,relevance,2024-02-19 21:51:38,1 year to be NLP-LLM Specialist,Learning_DL,False,0.58,2,1auzu5r,https://www.reddit.com/r/learnmachinelearning/comments/1auzu5r/1_year_to_be_nlpllm_specialist/,7,1708379498.0,"Hi all, 

I am taking 1 year to really learn deeply ML. I have basic knowledge but I want to go deep. I made this plan:

Part 1: 1000h to learn this: 

- Linear regression: 20h
- Logistic regression: 20h
-  NN and Basics (evaluation, optimiser, training, …): 150h
- RNN: 30h
- LSTM: 30h
- Seq2Seq: 50h
- Encoder-Decoder: 50h
- Transformers: 150h
- LLM Models: 250h
- Maths, Stats, Probability: 100h
- NLP Basics: 150h

I will code all the models from scratch to deeply understand them. And I really enjoy this process. 

Part 2: 1000h of applications

Once I have learned this models, I will take 3-5 public datasets and apply all theses models for 4 applications (I am interested in): 

- Text Classification: 200h
- Text Generation: 300h
- IR system  (RAG system): 500h

For each application, I will read related research papers, apply the learned models on the selected dataset, experiments, compare and analyse the results.

In total I will spend 2000h, so it’s 40h a week for 1 year! 

Ps: I want to be good in IR and LLM. 
I am already data scientist - engineer, coding with python. I have a part time job (data scientist for 15h a week). So, I will study on week ends. I am single, no kids and no responsibilities. 

Your thoughts?

"
211,learnmachinelearning,llm,relevance,2023-06-20 17:56:06,LLM questions,YellowSea11,False,0.5,0,14ei1z9,https://www.reddit.com/r/learnmachinelearning/comments/14ei1z9/llm_questions/,15,1687283766.0,"Hey gang .. two questions . 1) can I make chatGPT local? Meaning I don't want it connected to the internet at all .. is that doable? 
2) can I train it on my data? What are my options there? Ideally I'd love to be able to read data from a table so I could train it on those terms as well."
212,learnmachinelearning,llm,relevance,2023-12-19 21:39:30,Holiday LLM Quick-Start Primer,ACVonnegutteral,False,0.74,9,18mdbbe,https://i.redd.it/njfk72snmb7c1.png,4,1703021970.0,
213,learnmachinelearning,llm,relevance,2023-11-16 15:05:20,Training an LLM to have my friends personality,travy_burr,False,0.89,15,17wp1p7,https://www.reddit.com/r/learnmachinelearning/comments/17wp1p7/training_an_llm_to_have_my_friends_personality/,16,1700147120.0,"Im a Software Engineer looking to learn a bit about ML, and decided a fun first project would be to train an LLM that has my friend's personality.

I have about 22,000 discord messages from my friend, stored in json format. I could get maybe a few thousand more.

So far, I've been able to get the model to use my friends (lets call him Dylan) words and generally have his personality, but it still isn't forming coherent responses. For example, to the question ""What's your opinion on Steve?"" Dypan's LLM might respond ""Steve has the skill to be a good player, but isn't quite there yet. He has the potential to be a pro"". But to the question ""What's your favorite game?"" It would respond ""it's a good game and I had fun playing it, but I don't know if it's a good game"". Pretty nonsensical.

My LLM is fine tuned using GPT2. I trained it for roughly 9.5 hours overnight on a 3080, with a batch size of 32 and gradient accumulation steps at 32. The training resulted in a loss of 4.09. From what I understand, this loss is extremely high.

I think it would be better if I included messages from other people - essentially giving the LLM context (this is how Dylan responds to these words). Can any provide guidance on how to do this? I've done research but can't seem to find anything helpful.

Thank you in advance!"
214,learnmachinelearning,llm,relevance,2024-01-01 13:05:37,Is it good idea to sharpen skills in LLM?,Glad-World-5312,False,0.5,0,18vwdtr,https://www.reddit.com/r/learnmachinelearning/comments/18vwdtr/is_it_good_idea_to_sharpen_skills_in_llm/,11,1704114337.0,"Yeah actually i did my Masters in Big data Analytics. And i did my internship in data visualisation using python (dash,plotly). And in my full time its completely into non technical. So im planning to switch to Gen Ai and i have enough foundation in ML. So whats your thought on switching to Gen Ai (LLM)?"
215,learnmachinelearning,llm,relevance,2024-01-24 15:14:51,Building LLM-based Classifiers from Scratch,alongub,False,0.67,1,19ejd2v,https://www.reddit.com/r/learnmachinelearning/comments/19ejd2v/building_llmbased_classifiers_from_scratch/,0,1706109291.0,"Hey all, I've recorded a short coding session to show you how to build an LLM-based classifier from scratch.

This is a highly underrated use case for LLMs. It allows you to answer yes/no questions or solve other classification tasks at scale.

Example applications include automatically categorizing articles, emails, posts, etc., or even videos (based on their transcripts).

[https://www.youtube.com/watch?v=l7NPMiyuh1M](https://www.youtube.com/watch?v=l7NPMiyuh1M)  
I hope you find it useful :)"
216,learnmachinelearning,llm,relevance,2023-11-12 04:53:39,Resources for llm,No-Percentage7346,False,0.5,0,17tcwtk,https://www.reddit.com/r/learnmachinelearning/comments/17tcwtk/resources_for_llm/,1,1699764819.0,"Can anyone provide good resource for llm and fine-tuning llms?

I am new to llm


Thanks in advance"
217,learnmachinelearning,llm,relevance,2023-07-27 11:46:34,LLM Guide [Discussion],torspayorryum,False,0.92,27,15azq0q,https://www.reddit.com/r/learnmachinelearning/comments/15azq0q/llm_guide_discussion/,6,1690458394.0,"Nowadays, If we see over the internet that LLM, chatgpt , llma etc are the trending topics and are being discussed. My question is that anyone can help me where to start studying about these topics from scratch ? BERT, Transformer etc all I want to understand everything.

It would be good if you help me out.

Thanks"
218,learnmachinelearning,llm,relevance,2024-02-17 17:03:15,Jailbroken: How Does LLM Safety Training Fail?,Personal-Trainer-541,False,0.33,0,1at6mz9,https://www.reddit.com/r/learnmachinelearning/comments/1at6mz9/jailbroken_how_does_llm_safety_training_fail/,0,1708189395.0,"Hi there,

I've created a video [here](https://youtu.be/sKEZChVe6AQ) where I explain why large language models are susceptible to jailbreak as suggested in the “Jailbroken: How Does LLM Safety Training Fail?” paper.

I hope it may be of use to some of you out there. Feedback is more than welcomed! :)"
219,learnmachinelearning,llm,relevance,2024-02-04 10:19:14,What small open source LLM is best for conversations?,Sea_Skill_3479,False,1.0,3,1aikudt,https://www.reddit.com/r/learnmachinelearning/comments/1aikudt/what_small_open_source_llm_is_best_for/,8,1707041954.0,"What small open-source LLM is most effective for social media conversations? I attempted fine-tuning with Llama-2 7B-Q4, but it didn't work on my GPU(getting error-cuda out of memory ). Are there smaller, more suitable models for natural-sounding conversations that can be finetuned with small dataset? I discovered that the Bard response feels more natural than GPT-3.5. How can I get my response close to Bard?"
220,learnmachinelearning,llm,relevance,2024-01-20 11:14:29,New and better ways of grounding an llm?,NightestOfTheOwls,False,1.0,6,19b9kue,https://www.reddit.com/r/learnmachinelearning/comments/19b9kue/new_and_better_ways_of_grounding_an_llm/,3,1705749269.0,"Basically, if I tell an e-commerce assistant that it is a circus bear and ask it to ride a unicycle, I need it to actually remember that it is not, in fact, a circus bear and respond correctly instead of letting itself be gaslit.

I've seen a paper describing some novel method of grounding that managed to achieve insane accuracy but I can't find it anymore. While I browse the ML sub looking for it, will anyone share some of the practical methods of grounding an llm and stopping it from being a yes-man, that persistently work?"
221,learnmachinelearning,llm,relevance,2023-11-04 04:29:28,Brilliant's ML LLM course?,PsychoWorld,False,0.75,2,17ndw6c,https://www.reddit.com/r/learnmachinelearning/comments/17ndw6c/brilliants_ml_llm_course/,0,1699072168.0,"Interested in learning about how AI can be applied in the real world  and its fundamental limitations.

https://brilliant.org/wiki/machine-learning/

Is Brilliant's LLM course a good starting point? Also down to listening to lectures."
222,learnmachinelearning,llm,relevance,2024-02-20 20:03:33,Has anyone yet used EQ-BENCH benchmark for LLM evaluation?,MBU_NxtDoor,False,0.67,1,1avr9ii,https://www.reddit.com/r/learnmachinelearning/comments/1avr9ii/has_anyone_yet_used_eqbench_benchmark_for_llm/,4,1708459413.0,how to setup config.cfg file?
223,learnmachinelearning,llm,relevance,2024-01-23 09:20:48,Word embeddings Word2Vec and Glove: inputs for LLM,Toica_Rasta,False,0.5,0,19dkhm4,https://vitomirj.medium.com/word-embeddings-second-part-f71d6748bf15,0,1706001648.0,
224,learnmachinelearning,llm,relevance,2023-09-15 19:03:23,"LLM for textbooks, feasible ?",Personal_Definition,False,1.0,2,16jlt3n,https://www.reddit.com/r/learnmachinelearning/comments/16jlt3n/llm_for_textbooks_feasible/,8,1694804603.0,"I want to build a webapp SAAS that allows you to chat with an AI tutor about  specific textbook. Some of these textbooks are in English and others are Arabic, conceptual , no equations or anything.
I couldn't find anything like this. Im a total noob with AI. but I think it can be made using langchain and openai api , can it ?"
225,learnmachinelearning,llm,relevance,2024-02-05 17:16:14,Learning about LLM system design — Google Interview,tflbbl,False,0.67,1,1ajls4f,https://www.reddit.com/r/learnmachinelearning/comments/1ajls4f/learning_about_llm_system_design_google_interview/,0,1707153374.0,"Hi, I am having two system design interviews related to LLMs for a job at Google Cloud (Conversational AI Engineer).

Here is what the recruiter told me:

> [...] focused on many different aspects of the design, training, monitoring and performance of Virtual Agents.
> 
> We will ask questions around designing end-to-end architectures of chatbots or similar VA's etc. 

What course do suggest me to follow? What resources?

Thanks a lot in advance."
226,learnmachinelearning,llm,relevance,2023-08-29 22:37:15,Finetuning an LLM to imitate someone,Vanishing-Rabbit,False,0.93,13,164wtmm,https://www.reddit.com/r/learnmachinelearning/comments/164wtmm/finetuning_an_llm_to_imitate_someone/,9,1693348635.0,"Hello all,

I'm trying to understand how to get an LLM to imitate someone, say Shakespeare. It's easy enough to get all of [Shakespeare's work](https://www.gutenberg.org/ebooks/author/65).

If I've understood the current state of play for LLMs, there are three options:

* Fine tune an LLM
* Vectorize your knowledge using something like ChromaDB. Do a similarity search after each prompt and get the LLM to ""read"" the top n docs
* Do both

I have a feeling that to imitate Shakespeare, fine tuning an LLM might work best.

However, if my understanding is correct, the inputs to finetune an LLM must be formatted this way:

    <human>: ""To be""
    <system>: ""Or not to be""
The gap I'm having trouble bridging is how do I go from a large text file to this input format? The only idea I've come across is format all of the text like so:

    <human>: ""sentence_1""
    <system>: ""sentence_2""

    <human>: ""sentence_2""
    <system>: ""sentence_3""
Are there best practices around this problem? How should I be thinking about this?

I've seen companies like character.ai create bots that imitate [Elon Musk] (https://c.ai/c/6HhWfeDjetnxESEcThlBQtEUo0O8YHcXyHqCgN7b2hY) accurately for example so I know it's doable. I just wonder if they've done it by finetuning an LLM or training one from scratch or something else entirely."
227,learnmachinelearning,llm,relevance,2023-11-10 15:29:14,"Looking to build an LLM POC for my company, where do I begin?",fuzedmind,False,0.94,16,17s6ena,https://www.reddit.com/r/learnmachinelearning/comments/17s6ena/looking_to_build_an_llm_poc_for_my_company_where/,26,1699630154.0,"Hello,

I am currently a devops engineer but do have experience in programming. Some data engineering, no ML yet.

We have a vast trove of Google Docs and Sheets in our company that I think could be used to create an efficient search product using an LLM. I would like to essentially feed all this information to a model and be able to ask it where certain documents are, who updated them, links etc. Something simple.

I have looked around a little and saw that Langchain could be helpful here. We have a data engineering team that is indexing all of our documentation so that part is solved already. It looks like I may have to look into Vector databases as well.

Ultimately I just want somewhere to learn how this is done. There is tons of stuff out there and you folks would probably know best."
228,learnmachinelearning,llm,relevance,2023-11-22 06:21:05,Fine-tuning an LLM for custom NER,JustinQueeber,False,1.0,2,1812m6y,https://www.reddit.com/r/learnmachinelearning/comments/1812m6y/finetuning_an_llm_for_custom_ner/,3,1700634065.0,"I am exploring using an LLM to be able to detect the merchant/company name in bank transactions from a bank statement.

e.g.

Input: “POS purchase McDonalds Restaurant NYC Nov20”

Output: “McDonalds”

I know I can take a pre-trained LLM and not do any fine-tuning, but instead just give a few examples of the task in the prompt and then ask it to do the same on a new transaction.

However, is there anyway I can fine-tune an LLM so that given a transaction it will output only the merchant name (and possibly “none” if there is no merchant in the transaction)?

I assume I could either fine-tune for custom NER where “MERCHANT” is the only entity possible. Maybe I can also fine-tune for text generation where given an input transaction, it will then continue the string by appending only the merchant name to the end, but I am not sure how this can be achieved.

Note: 
- The example I gave with “McDonald’s” is oversimplified, when in reality these transactions are extremely messy, contain a lot more noise of things such as long reference numbers, etc.
- Also the structure of the transactions varies wildly so the merchant name can appear anywhere in the string. 
- Lastly, the transactions I am working with are from the Middle East, so there are many less popular merchants, so I want the LLM to learn to predict the merchant name based on the structure of the transaction, rather then depending solely on the LLM having “memorised” popular merchants, such as “McDonald’s”."
229,learnmachinelearning,llm,relevance,2024-02-20 20:03:32,Has anyone yet used EQ-BENCH benchmark for LLM evaluation?,MBU_NxtDoor,False,0.5,0,1avr9i6,https://www.reddit.com/r/learnmachinelearning/comments/1avr9i6/has_anyone_yet_used_eqbench_benchmark_for_llm/,0,1708459412.0,how to setup config.cfg file?
230,learnmachinelearning,llm,relevance,2023-08-27 22:14:43,Running my own LLM,jrdubbleu,False,0.67,1,1633vfg,https://www.reddit.com/r/learnmachinelearning/comments/1633vfg/running_my_own_llm/,2,1693174483.0,"So if I setup an LLM ON AWS Sagemaker, or even locally for that matter, how do I interface with it? I do a lot of R coding and I’ve enjoyed using GPT-4 as a partner and I’d like to tryout CodeLlama and others just for fun. I know this is not especially straightforward. Are there any resources (I have watched a couple YouTube videos etc.) I can use to teach myself?"
231,learnmachinelearning,llm,relevance,2023-10-22 11:38:41,Looking for Resources to Learn LLM in depth,UpvoteBeast,False,0.96,41,17drbyo,https://www.reddit.com/r/learnmachinelearning/comments/17drbyo/looking_for_resources_to_learn_llm_in_depth/,5,1697974721.0,"I have some background in deep learning (e.g. ML courses, training ranking and classification models), but I’m looking for resources (blogs, videos, podcasts, courses) to learn more about LLMs. Are there any resources that you’d recommend?"
232,learnmachinelearning,llm,relevance,2023-08-23 18:03:34,Where to run LLM?,followyourvalues,False,1.0,3,15zbj0r,https://www.reddit.com/r/learnmachinelearning/comments/15zbj0r/where_to_run_llm/,1,1692813814.0,"Hello! I have a project that uses two ML models. DeepFace (the python package) analyzes an image, then is meant to feed the results into an LLM (stable-beluga7B, I think--its big, but not StableBeluga2 big) and this model is via prompt engineering meant to output an affective journal prompt. I have all the code to make it run. Works great!

Problem is -- my laptop does not have a compatible GPU for the LLM model. So, I tried Google colab. That's how I know my PoC works. But when I try to combine both deepface and beluga into the same colab notebook -- it runs out of memory. Even for a paid GPU. I have also attempted to use huggingface spaces and have been unsuccessful so far. The reasonable priced GPU (.60 per hour) doesn't seem to be good enough. 

I also don't really want to spend money. This is for my capstone demo. I just want it all to actually run together! Please help! Is there something simple I can try with AWS? Or any other platform you know?! I really don't want a to have to use a dumber model. GPT2 is terrible for just prompt tuning."
233,learnmachinelearning,llm,relevance,2023-11-16 21:34:20,AI/LLM starter kit in open source repo,linamagr,False,0.82,7,17wy4aw,https://www.reddit.com/r/learnmachinelearning/comments/17wy4aw/aillm_starter_kit_in_open_source_repo/,6,1700170460.0,"Share a Github repository to quickly build and start a local application to chat with private documents. The stack used is python,  [\#LangChainAI](https://www.linkedin.com/feed/hashtag/?keywords=langchainai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7130952995793489920) , [\#qdrant\_engine](https://www.linkedin.com/feed/hashtag/?keywords=qdrant_engine&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7130952995793489920) [\#Ollama\_ai](https://www.linkedin.com/feed/hashtag/?keywords=ollama_ai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7130952995793489920) and [\#FastAPI](https://www.linkedin.com/feed/hashtag/?keywords=fastapi&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7130952995793489920)  
)  
[https://github.com/mallahyari/ai-starter-kit](https://github.com/mallahyari/ai-starter-kit)   
"
234,learnmachinelearning,llm,relevance,2023-11-11 08:27:15,How to start on LLM productionalization?,AMGraduate564,False,0.76,2,17sq78c,https://www.reddit.com/r/learnmachinelearning/comments/17sq78c/how_to_start_on_llm_productionalization/,1,1699691235.0,"I am new to LLM, though I have an understanding of Classical ML (Regression, Classification, etc.). Is there any course available on the background theory of LLMs (from a high level, not too deep) and how to put it into production? I often see acronyms in this sub which I am not aware of.

I have come across [HG NLP course](https://huggingface.co/learn/nlp-course/chapter1/1), would this help me with what I am looking for (not too heavy in theory/math)?"
235,learnmachinelearning,llm,relevance,2024-02-20 22:28:30,ELI5: What does finetuning LLM mean in context of neural networks?,20231027,False,1.0,1,1avuy5n,https://www.reddit.com/r/learnmachinelearning/comments/1avuy5n/eli5_what_does_finetuning_llm_mean_in_context_of/,1,1708468110.0,"Hi,

I am having a hard time picturing, what it means to fine tune an LLM model. Are all the weights relearned? Please help

Thanks. "
236,learnmachinelearning,llm,relevance,2024-01-24 14:04:14,No-code LLM finetuning with LoRA/QLoRA,gvij,False,0.33,0,19ehtnb,https://www.reddit.com/r/learnmachinelearning/comments/19ehtnb/nocode_llm_finetuning_with_loraqlora/,0,1706105054.0,"Hi everyone!

At MonsterAPI, we released a no-code LLM finetuner that enables you to customise LLMs for domain specific tasks using LoRA and QLoRA without you having to setup any infrastructure or finding the right batch size to avoid issues like Out of memory. It does it all for you automatically through one single interface.

It gives complete advanced settings configuration to provide you full control over the pipeline and supports all the latest models from Mixtral 8x7B, Llama 2 70B to even SLMs like Tinyllama!

Read through our release:

[https://www.linkedin.com/pulse/fine-tune-large-language-model-llm-deploy-monsterapis-rohan-paul-rrhzc/](https://www.linkedin.com/pulse/fine-tune-large-language-model-llm-deploy-monsterapis-rohan-paul-rrhzc/)

  
Get started with LLM Fine-tuning:  
[https://monsterapi.ai/finetuning](https://monsterapi.ai/finetuning)

Would appreciate your thoughts and feedback. Feel free to try it out, appreciate your support!"
237,learnmachinelearning,llm,relevance,2023-11-06 07:59:00,Looking for good learning sources around LLM?,DwaywelayTOP,False,0.5,0,17oxtt2,https://www.reddit.com/r/learnmachinelearning/comments/17oxtt2/looking_for_good_learning_sources_around_llm/,2,1699257540.0,"Are there any good content sources that explains all the concepts associated with generative AI (ex: RL, RLHF, transformer, etc) from the ground up in extremely simple language (using analogies/stories of things that would be familiar to say a 10-12 year old)? Also would prefer channels which explain the concepts in a sequential manner (so that easy to follow) and make short and crisp videos

If yes, could you kindly comment below with the suggestions. If not, could you comment whether something like that would be useful to you and ideally why also?

Big thanks in advance 🙏

 "
238,learnmachinelearning,llm,relevance,2023-10-21 22:48:20,Investing time in learning everything about LLM?,DwaywelayTOP,False,0.67,1,17depq3,https://www.reddit.com/r/learnmachinelearning/comments/17depq3/investing_time_in_learning_everything_about_llm/,1,1697928500.0,"I want to be as updated as I can on the basics of the world that is coming. I am starting CS studies in October, but I am knees deep in everything AI right now. Do you think there is much sense in learning prompt engineering, LLM, NLP etc. ? Doing interns in Prompt Engineering? I dropped out of uni I was studying design oriented major but didn't enjoy it. Now I work and save for next year, study and make projects with AI as much as I am able to, of course, and just basically update myself and condition myself to what's coming.  
My question is: 1. Do you find that valuable (99% will probably say yes, but still)  
2. Any tips on what to do, focus on, guides, info, etc.

 "
239,learnmachinelearning,llm,relevance,2024-02-21 15:57:52,How to know whether an LLM-generated response is factually correct? 🤔,Vegetable-Skill-9700,False,0.4,0,1awfjfq,https://www.reddit.com/r/learnmachinelearning/comments/1awfjfq/how_to_know_whether_an_llmgenerated_response_is/,1,1708531072.0,"Ever wondered how developers ensure that the LLM-generated response does not contain any factually wrong information?

I have been getting this question a lot from people eager to learn more about how LLMs work. 

In this post, I'll be walking you through an example on how LLMs verify facts. Further, I have shared some interesting research papers around it.

Let’s say you are asking about fruits to an LLM-powered chatbot  
Query: What are the benefits of fruits?

You retrieve context: Fruits are great for your health, providing a lot of benefits like lowering blood pressure, promoting weight loss, etc. To maintain a healthy diet, you should consume fruits like apples, berries, etc.

And your LLM generates a response:

Fruits are essential to a healthy diet and should be consumed daily. Fruits contain potassium, which helps lower your blood pressure.

There is an interesting way to verify the facts in the generated response!

Let’s break the response into individual facts, such as:

* Fruits are an essential part of a healthy diet
* Fruits should be consumed daily
* Fruits help to lower blood pressure
* Fruits contain potassium

Now we can rate these individual facts, whether they are correct or not.

Some facts are easy to verify, for example:

* Fruits are an essential part of a healthy diet (that’s correct)
* Fruits help to lower blood pressure (that’s correct)
* Fruits contain potassium (this can’t be verified the context has no mention of potassium)

It’s not so obvious to rate individual facts like: “Fruits should be consumed daily”

Some may say fruits are healthy and hence, should be consumed daily. On the other hand, in many cases, especially for people with pre-existing conditions, consuming fruits daily does not equate to a healthy diet. Hence, this can be rated as ambiguous, i.e. something which is not purely right or wrong.

Thus, any individual fact can be classified into 3 categories:

* Completely Right
* Completely Wrong
* Ambiguous

At UpTrain, we use a combination of these individual categories to rate whether a complex LLM-generated response is right or not.

You can know more about it in our docs:

[https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy](https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy)

You can also refer to these research papers to understand more about the concept:

[https://arxiv.org/pdf/2305.14251.pdf](https://arxiv.org/pdf/2305.14251.pdf)

[https://arxiv.org/pdf/2305.06311.pdf](https://arxiv.org/pdf/2305.06311.pdf)"
240,learnmachinelearning,llm,relevance,2023-12-13 12:56:32,5 Tools to Help Build Your LLM Apps,kingabzpro,False,0.5,0,18hg7j7,https://www.reddit.com/r/learnmachinelearning/comments/18hg7j7/5_tools_to_help_build_your_llm_apps/,0,1702472192.0,"In the era of advanced language model applications, developers and data scientists are continuously seeking efficient tools to build, deploy, and manage their projects. As large language models (LLMs) like GPT-4 gain popularity, more people are looking to leverage these powerful models in their own applications. However, working with LLMs can be complex without the right tools.

That's why I've put together this list of five essential tools that can significantly enhance the development and deployment of LLM-powered applications. Whether you're just beginning or are a seasoned ML engineer, these tools will help you be more productive and build higher-quality LLM projects.

**link:** [https://www.kdnuggets.com/5-tools-to-help-build-your-llm-apps](https://www.kdnuggets.com/5-tools-to-help-build-your-llm-apps)"
241,learnmachinelearning,llm,relevance,2023-10-16 16:51:14,Fine-tuning LLM on research papers,Practical_Ad_8782,False,1.0,5,179amux,/r/LanguageTechnology/comments/179aid1/finetuning_llm_on_research_papers/,0,1697475074.0,
242,learnmachinelearning,llm,relevance,2024-01-04 17:21:01,2024 Up-to-date LLM and Computer Vision Learning,Crimson--Chin,False,0.67,1,18yifb9,https://www.reddit.com/r/learnmachinelearning/comments/18yifb9/2024_uptodate_llm_and_computer_vision_learning/,0,1704388861.0,"Hi, this is my first time *ever* using Reddit--let me know if this post could be improved.

I'm looking for any recommendations for learning LLM and Computer vision. Courses on LinkedInLearning, Udemy, Coursera are welcome recommendations. I would prefer something that explains the fundamentals of these models, then builds a project using some of the latest tools.

I have a B.S. in Mathematics and started working on my M.S. in Applied and Computational Mathematics. I have been working on building my skillset to transition into a Data Science career, but currently that is not what I do professionally. That is to say, (1) I'd prefer a resource that goes into the model details, not just a shallow ""what packages to use and code to write,"" and (2) I am not an expert.

I'd love to get a few projects added to my portfolio. I am thinking a LLM project related to healthcare and a Computer vision project related to agriculture or other environment/climate fields. Any recommendations for that would be great as well!

Thank you!"
243,learnmachinelearning,llm,relevance,2024-01-31 23:08:08,Navigate beyond LLM obsession - have a well rounded ML journey,ade17_in,False,0.9,8,1afvoja,https://www.reddit.com/r/learnmachinelearning/comments/1afvoja/navigate_beyond_llm_obsession_have_a_well_rounded/,0,1706742488.0,"Long post ahead - tl;dr - There is more to ML other than LLMs. 

I want to share a crucial aspect of my ML journey - still a noive but overcame a phase when my progress hit a roadblock because I was fixated solely on LLMs. It is a common pitfall I've observed among many aspirants, colleagues and also students. I've seen a notable personal growth incl. focusing on my paper, personal projects/competitions, internship offers (none to 7 after my roadblock from top AI teams) and also my learning curve. 

I was consumed by the allure of training sophisticated language models and understanding the nuances of advanced AI applications. However, this single-minded focus led to a slow and frustrating progress. I found myself grappling with the intricacies of high-level concepts without a solid grounding in the fundamentals.

Moreover, when it came to interviews, especially those centered around LLMs, I faced significant challenges. The questions were often intricate and demanded a deep understanding of complex models and their applications. It became evident that solely relying on LLM-related knowledge was not enough.

Solution? 
I advocate for a well-rounded ML journey that starts with the basics and gradually builds up to advanced topics. In contextual terms - have a optimal learning rate, you know what happens when learning rate is too much - you don't learn anything and eventually keeping jumping here and there and never reaching your goal (minima).

Begin with basics, explore traditional machine learning techniques, try getting into unique domains - medical imaging, autonomous driving, audio processing or even working with algorithms and believe me these fields are not dying anytime soon. 

More importantly - I know it is tough but resist the temptation of fine-tuning LLMs with API tokens. Instead, focus on core projects to understand algorithms and optimize for performance. 

To conclude - 
The journey is as crucial as the destination. Don't rush; explore, experiment, and embrace the challenges. By diversifying your skill set and building a strong foundation, you not only enhance your understanding of LLMs but also position yourself as a versatile ML practitioner ready for real-world challenges.

PS: I'm no expert, it is just my general observation and it really helped me. Please do not dm directly."
244,learnmachinelearning,llm,relevance,2024-01-26 05:27:03,Looking for a way to fine-tune an llm using tensorflow,kitecut,False,0.67,1,1abavv4,https://www.reddit.com/r/learnmachinelearning/comments/1abavv4/looking_for_a_way_to_finetune_an_llm_using/,1,1706246823.0,"I have a big wall of text containing information about plant health and care and a json file containing an array of objects [{question: , answer :}, {question: , answer:}] in this format.

I want to use these two to make a chat interface that can answer general plant health and care stuff

Is there a way to do this on a laptop in a reasonable amount of time

I'm a frontend developer and I really want to make something like this . Any help is really appreciated"
245,learnmachinelearning,llm,relevance,2023-10-18 18:23:55,How to Evaluate Your LLM Applications?,Vegetable-Skill-9700,False,0.67,1,17axl5f,https://www.reddit.com/r/learnmachinelearning/comments/17axl5f/how_to_evaluate_your_llm_applications/,0,1697653435.0,"This a question that I have come across countless number of times on HN, discord/ slack communities and emails. Hope this blog I wrote answers some of your questions: [https://uptrain.ai/blog/how-to-evaluate-your-llm-applications](https://uptrain.ai/blog/how-to-evaluate-your-llm-applications) and please let me know your comments/questions/criticisms in the comment section"
246,learnmachinelearning,llm,relevance,2023-11-30 01:29:07,ML sys design help for fine-tuning llm,WelcomeHefty372,False,1.0,3,18761ng,https://www.reddit.com/r/learnmachinelearning/comments/18761ng/ml_sys_design_help_for_finetuning_llm/,0,1701307747.0,"hey folks, I am pretty new into llm space. our team are experimenting doing some fine-tuning on say llama model. we currently have one VM that come with some GPU capacity.
I am preciously pure backend swe background so has little knowledge in llm or mlops.
my question is, how do I trainkkmg/storing and updating a model utilizing the GPU instead of calling some wrapper API like replicate (or in Google colab), how should I set it up?
I am imagining I can just download llama from hugging face, add my data and train and store in local disk? but how do I retrieve it during inference time? (let's say for now it is a toy project and we use same VM for training and serving)

thank you so much!"
247,learnmachinelearning,llm,relevance,2024-02-13 14:34:44,Is it possible to train a os llm with my own data?,Longjumping_Fruit843,False,0.5,0,1apuw1a,https://www.reddit.com/r/learnmachinelearning/comments/1apuw1a/is_it_possible_to_train_a_os_llm_with_my_own_data/,0,1707834884.0,"Hello, I am working on an internal GPT of some sorts and I dont get the best results with RAG. My question is there a way to download an os llm from huggingface and to continue the training with the internal data. ? "
248,learnmachinelearning,llm,relevance,2023-10-03 12:04:37,Dropbox AI Chat with LLM App,bumurzokov,False,1.0,1,16yq3r7,https://github.com/pathway-labs/dropbox-ai-chat,0,1696334677.0,
249,learnmachinelearning,llm,relevance,2024-01-30 15:38:19,How can I use/finetune LLM to create domain based questionnaires?,Electronic-Mousse-39,False,0.33,0,1aer8sp,https://www.reddit.com/r/learnmachinelearning/comments/1aer8sp/how_can_i_usefinetune_llm_to_create_domain_based/,0,1706629099.0," Hey !  
I have this projet where I wanna create a model that allows its user to generate a survey (a series of questions with possible answers) based on a text inserted by the user !

I'm trying to learn more about LLM models like LLAMA 2 and generative IA so I guess that's my starting point ! any idea or suggestions of articles or notebooks that would help guide me? since I'm new to generative IA i don't really know where to start especially for that kind of project ! any information could be helpful !

Thanks !"
250,learnmachinelearning,llm,relevance,2024-02-02 07:38:00,Jupyter/Colab notebook example for training a tiny LLM from scratch?,HumanSpinach2,False,1.0,2,1agyaec,https://www.reddit.com/r/learnmachinelearning/comments/1agyaec/jupytercolab_notebook_example_for_training_a_tiny/,0,1706859480.0,"Can anyone point me to an example of a notebook that implements and trains a very small LLM using a Transformer architecture? I'm talking something that can be trained with <12GB VRAM and can get semi-sensible results after only 5 hours of training on a 4090. I'm not expecting the model to be useful or good, I just want an example as a starting point for experiments."
251,learnmachinelearning,llm,relevance,2023-08-26 06:05:42,[Tutorial] Build LLM Playground in <10mins.,VideoTo,False,1.0,10,161mw35,https://www.reddit.com/r/learnmachinelearning/comments/161mw35/tutorial_build_llm_playground_in_10mins/,0,1693029942.0,"**tldr;** [**https://docs.litellm.ai/docs/tutorials/first\_playground**](https://docs.litellm.ai/docs/tutorials/first_playground)

Create a playground to **evaluate multiple LLM Providers in less than 10 minutes**. If you want to see this in prod, check out our [website](https://litellm.ai/).

**What will it look like?**

&#x200B;

https://preview.redd.it/s75jp703bekb1.png?width=1920&format=png&auto=webp&s=84432f4c03833156870a6ed445ac3299ff6564cd

**How will we do this?**: We'll build the server and connect it to our template frontend, ending up with a working playground UI by the end!

&#x200B;

**Tutorial** 👉 [https://docs.litellm.ai/docs/tutorials/first\_playground](https://docs.litellm.ai/docs/tutorials/first_playground)"
252,learnmachinelearning,llm,relevance,2023-09-30 13:45:23,How to give LLM full context of codebase?,adlabco,False,0.88,6,16w7959,https://www.reddit.com/r/learnmachinelearning/comments/16w7959/how_to_give_llm_full_context_of_codebase/,3,1696081523.0,"I want an LLM to give advice on refactoring across files in a codebase eg finding areas where functions can be further modularised and reused.

Is there a good way of running an LLM locally to allow for this?"
253,learnmachinelearning,llm,relevance,2023-11-13 20:36:29,RAG & LLM: Pioneering Dynamic Language Model Frontier,Odd_Spite3834,False,1.0,2,17uk707,https://www.qwak.com/post/rag-llm,0,1699907789.0,
254,learnmachinelearning,llm,relevance,2023-05-07 10:45:15,LLM custom dictionary,Tuppitapp1,False,0.86,5,13ak7jk,https://www.reddit.com/r/learnmachinelearning/comments/13ak7jk/llm_custom_dictionary/,2,1683456315.0,Is it possible to extend the token dictionary of LLMs with new custom tokens when fine-tuning? I'm working on a project for generating text that includes terminology and device IDs (+20 character long random strings) that are not in public domain. I imagine it would be easier for the LLM if these were distinct tokens.
255,learnmachinelearning,llm,relevance,2023-06-25 18:45:57,LLM Limitations and Hallucinations,Neurosymbolic,False,1.0,1,14itsmy,https://youtube.com/watch?v=JvLiEdTKKgk&feature=share,0,1687718757.0,
256,learnmachinelearning,llm,relevance,2024-01-25 10:22:22,Can I just give a book for my LLM to train on,kitecut,False,0.5,0,19f691s,https://www.reddit.com/r/learnmachinelearning/comments/19f691s/can_i_just_give_a_book_for_my_llm_to_train_on/,2,1706178142.0,I want to train a model based on plant care and health what kind of data should I feed the model
257,learnmachinelearning,llm,relevance,2023-11-15 20:26:11,Learn to use an LLM on your own computer!,Artistic_Highlight_1,False,0.5,0,17w2ujy,https://www.reddit.com/r/learnmachinelearning/comments/17w2ujy/learn_to_use_an_llm_on_your_own_computer/,0,1700079971.0,Do you want to learn how to use Llama2 locally? Then check out [this tutorial on downloading and running Llama2 on Windows](https://medium.com/dev-genius/downloading-and-running-llama2-for-windows-bf99ef45c855)
258,learnmachinelearning,llm,relevance,2023-06-20 20:31:53,Trim LLM for specific task,gi_beelzebub,False,1.0,1,14ema9u,https://www.reddit.com/r/learnmachinelearning/comments/14ema9u/trim_llm_for_specific_task/,3,1687293113.0,"Say that you've got a huge LLM that's expensive to run.

But you only want to use it for a certain task, say text classification.

So you fine tune the LLM on this specific task.

Afterwards, is there a way to shrink/trim the model down? Since it's an LLM, it is likely proficient at a huge variety of tasks. Perhaps the specific task at hand only requires a very small subset of the nodes and weights to be successful.

Perhaps it is possible to look at average weight activation over a data set, and prune the weights that are always close to 0?"
259,learnmachinelearning,llm,relevance,2023-10-13 14:23:10,Authoring another course about LLMs. Learn by Doing LLM Projects.,pmartra,False,0.86,24,176zx1m,https://www.reddit.com/r/learnmachinelearning/comments/176zx1m/authoring_another_course_about_llms_learn_by/,5,1697206990.0,"Hi, I'm working on a course about LLMs on GitHub, it's totally free and under MIT license,  So there are no restrictions.

Here the link: [https://github.com/peremartra/Large-Language-Model-Notebooks-Course](https://github.com/peremartra/Large-Language-Model-Notebooks-Course)

I'm still working on It, but now I'm feeling comfortable with the variety and quality of the content. By the moment is a small repository with just 80 Stars.

My intention is to make the course more accessible to a wider audience, and, if possible, encourage  reporting any issues  encounter or suggesting improvements through the 'Discussion' section.

I'm eager to receive feedback.

Now, I'll provide an overview of the currently available content, and then I'll share a couple of questions I have about how to proceed with the course.

[Large Language Models Course: Learn by Doing LLM Projects.](https://github.com/peremartra/Large-Language-Model-Notebooks-Course)

* Introduction to LLM with OpenAI.
   * Create a first Chatbot using FPT 3.5.
   * Create a Natural Language to SQL Translator using OpenAI.
* Vector Databases with LLM.
   * Influencing Language Models with Information stored in ChromaDB.
* LangChain & LLM Apps.
   * RAG. Use the Data from Dataframes with LLMs.
   * Create a Moderation System using LangChain.
      * OpenAI.
      * GPT\_j.
      * LLama-2.
   * Create a Data Analyst Assistant using a LLM Agent.
* Evaluating LLMs
   * Evaluating Summarization with ROUGE.
* Fine-Tuning & Optimization.
   * Prompt-tuning using PEFT.
   * Fine-Tuning with LoRA.
   * Fine-Tuning a Large Model in a GPU using QLoRA. 

That's all for the moment, but I'm adding new content regularly. I'm working on it only in my spare time (mainly nights when the family goes to sleep).

\_\_\_

I have a doubt, I don't know if add some information about platforms like W&B or Cohere?  or maybe it is a better idea to stay with more Open-Source libraries?

On the other hand, my intention is to develop a couple of projects utilizing the techniques covered in the initial part of the course (which I am currently working on).

Some of these projects will be hosted in the cloud on major platforms such as Azure or GCP, or AWS. Any preference?

Furthermore, there is a plan to create a third section that explains how Large Language Models (LLMs) fit into large-scale enterprise solutions, defining architectures in which LLMs are used but are not the sole components of the project.

I don't intend to create a community outside of GitHub, but I would like the repository to have more activity and not be the one determining the course's direction.

Hope you like it, and lease, feel free to contribute.

&#x200B;"
260,learnmachinelearning,llm,relevance,2023-10-29 12:27:11,Master LLMs: Top Strategies to Evaluate LLM Performance,OnlyProggingForFun,False,0.5,0,17j1v8b,https://youtu.be/iWlTCBUoru8,0,1698582431.0,
261,learnmachinelearning,llm,relevance,2023-08-09 16:51:38,Generative AI/LLM master thesis topic suggestions?,casualhumanist,False,1.0,1,15mkpk4,https://www.reddit.com/r/learnmachinelearning/comments/15mkpk4/generative_aillm_master_thesis_topic_suggestions/,4,1691599898.0," Hello everyone! The title is pretty self-explanatory. I thought since there might be a lot of students in this thread who are also in search of good master thesis topics, it would be a good idea to start a discussion here! So feel free to suggest anything you think would be interesting!  


 I have been thinking about topics related to **prompt engineering**, as they might need less resources than training a model from scratch, but I am not sure if it is a good idea since evaluation can be a pain!"
262,learnmachinelearning,llm,relevance,2023-11-25 00:31:26,Open Source Code for Training/Fine-tuning > 20B LLM,rodeowrong,False,1.0,2,1836son,https://www.reddit.com/r/learnmachinelearning/comments/1836son/open_source_code_for_trainingfinetuning_20b_llm/,0,1700872286.0,"Hello,

I want to train large language models, specially using deep-speed. Is there any well known open-source code for that?"
263,learnmachinelearning,llm,relevance,2023-07-30 21:38:11,Guidance needed in finetuning an LLM,AccomplishedPin4075,False,1.0,3,15dxaxj,https://www.reddit.com/r/learnmachinelearning/comments/15dxaxj/guidance_needed_in_finetuning_an_llm/,0,1690753091.0,"I am trying to finetune an LLM. The use case is first I have to train the model for knowledge, and next I have to train the model for behavior.   


e.g. say I want to fine-tune a physics problem-solving assistant. Say a student wants help with the problem ""If a ball of 1 kg is moving at 5 meters/sec speed, how much energy is required to stop it"". The assistant may ask the student, ""Do you think the ball has potential energy or kinetic or both"". Based on the answer the assistant will ask another question and so on so that the student figures out the solution by herself.   


In this case, first, the LLM has to have knowledge of laws of motion, energy conversation, momentum conservation, etc.  -- The Knowledge part  


Next, it should be trained to behave like a guide and not a solver which is more of a behavioral training -- The Behavior part  
I am experimenting with Falcon 7B. I understand for the behavioral training I will have to create the dataset and fine-tune the LLM. But how to do the knowledge training given that the max input token supported by most open source LLMs is \~2K.   


Splitting the knowledge doc into sub-docs and indexing won't help for cases where multiple sub-docs have to be referred for a question just for the knowledge part.   


How do I go about it?"
264,learnmachinelearning,llm,relevance,2024-01-09 23:21:48,What Topics to learn for NLP and LLM on a high level,racc15,False,1.0,4,192t0c8,https://www.reddit.com/r/learnmachinelearning/comments/192t0c8/what_topics_to_learn_for_nlp_and_llm_on_a_high/,1,1704842508.0,"Hi,

I want to get into NLP and LLM techniques like BERT, using pretrained models from HuggingFace, LLama etc.

I do NOT plan to conduct research at the very core level and design a new architecture. I am mainly interested in using NLP pipelines for applications like analysin social media data or medical data etc. I am also interested in analysing the biases and maybe if possible, try to improve them slightly by dealing with hallucinations (I have very little idea about this) and try to find how to improve their trustworthiness. 

&#x200B;

I am familiar with deep learning, CNN, LSTM, Transformers. But, I have only used this for biomedical applications like EEG, ECG, X-ray, medical image processing. I have never worked with text based data.

&#x200B;

So, could you please suggest a strategy for quickly getting a high level grasp on these stuff in 2/3 days? At least get familiar with the main concepts I will need to learn. and, understand how to apply these for projects. 

&#x200B;

What do you guys think about this? Is this good?  
 [(25) A comprehensive guide to learning LLMs (Foundational Models) | LinkedIn](https://www.linkedin.com/pulse/comprehensive-guide-learning-llms-foundational-models-yeddula/) "
265,learnmachinelearning,llm,relevance,2023-10-13 20:35:19,How to Reduce LLM Cost And Improve Performance,linamagr,False,0.78,5,1778248,https://www.reddit.com/r/learnmachinelearning/comments/1778248/how_to_reduce_llm_cost_and_improve_performance/,0,1697229319.0,"Different LLMs are available through APIs and have varying prices, with costs differing by up to two orders of magnitude. For example, the prompt cost for 10M tokens can range from $0.2 to $30 depending on the LLM provider.

Have anyone done any comparison or monitoring or solutions for this?

For instance, like the approach here:  [https://open.substack.com/pub/mlnotes/p/how-to-reduce-llm-cost-and-improve?r=164sm1&utm\_campaign=post&utm\_medium=web](https://open.substack.com/pub/mlnotes/p/how-to-reduce-llm-cost-and-improve?r=164sm1&utm_campaign=post&utm_medium=web)

&#x200B;

&#x200B;

&#x200B;"
266,learnmachinelearning,llm,relevance,2023-07-13 15:35:00,Looking for Resources to Learn LLM in depth,EmoryCadet,False,0.97,34,14yo1m0,https://www.reddit.com/r/learnmachinelearning/comments/14yo1m0/looking_for_resources_to_learn_llm_in_depth/,5,1689262500.0,"I have some background in deep learning (e.g. ML courses, training ranking and classification models), but I’m looking for resources (blogs, videos, podcasts, courses) to learn more about LLMs. Are there any resources that you’d recommend?

 "
267,learnmachinelearning,llm,relevance,2023-05-17 22:48:25,"Transitioning to ml, focusing on LLM",World-Curiosity,False,0.62,2,13kgsjy,https://www.reddit.com/r/learnmachinelearning/comments/13kgsjy/transitioning_to_ml_focusing_on_llm/,5,1684363705.0,"Hello all

I've been a software developer for 20 years now. Very proficient in programming languages and infrastructures.
Now I want to do something ""simple"". Take an open source llm, setup an infrastructure and infer on it. Plus create a few fine tuned models.
I'm looking at some documentations and I really have no idea what they are talking about. Zero. The technology, concepts and terms are meaningless to me.

I was wondering if anyone knows a good course to transition in ml. Or any other method that both teaches hands-on techniques while providing fundamental knowledge.
I prefer to focus on LLM initially as my company have a few ideas around them.

I'm feeling a bit left behind. Please help me to feel relevant still :)"
268,learnmachinelearning,llm,relevance,2023-12-31 11:04:40,Serve a Custom LLM Trained with RLHF in - FREE COLAB 📓,Honest-Worth3677,False,1.0,3,18v5l3g,https://www.youtube.com/watch?v=dX27661ZFWc,0,1704020680.0,
269,learnmachinelearning,llm,relevance,2023-10-29 13:01:12,"Article: Autogen, simple and powerful framework to build LLM",bohemianLife1,False,0.67,2,17j2g3z,https://beginai.co/autogen-build-next-gen-llm-applications/,0,1698584472.0,
270,learnmachinelearning,llm,relevance,2024-02-13 16:26:39,"Deploy Mixtral 8x7B, LLaMA 2, and Mistral, on AWS EC2 with vLLM",juliensalinas,False,0.5,0,1apxjn6,https://www.reddit.com/r/learnmachinelearning/comments/1apxjn6/deploy_mixtral_8x7b_llama_2_and_mistral_on_aws/,0,1707841599.0,"Hi everyone,

In 2023, many sophisticated open-source LLMs have become available. However, integrating these AI models into a production environment continues to be a complex task. I made an article that will guide you through deploying some of the top LLMs, namely LLaMA 2 70B, Mistral 7B, and Mixtral 8x7B, on AWS EC2. I employ an inference engine capable of batch processing and distributed inference: vLLM.

vLLM will greatly aid in the implementation of LLaMA 2 and Mixtral because it allows us to use AWS EC2 instances equipped with multiple smaller GPUs (such as the NVIDIA A10) rather than relying on a single large GPU (like the NVIDIA A100 or H100).

See the detailed how-to here: [https://nlpcloud.com/deploy-llama-2-mistral-and-mixtral-on-aws-ec2-with-vllm.html](https://nlpcloud.com/deploy-llama-2-mistral-and-mixtral-on-aws-ec2-with-vllm.html?utm_source=reddit&utm_campaign=dqwerty12-3816-81ed-a26450242ac140019)

In this tutorial I used AWS EC2 but I could have used other vendors of course. The main challenge is the cost of the GPUs and their availability.

Please don't hesitate to share feedbacks about this article, it will be very much appreciated!

Julien"
271,learnmachinelearning,llm,relevance,2023-11-24 15:42:36,[Discussion} Is it possible to built a Multi-LLM Assistant?,SykA196286,False,0.5,0,182uw4r,https://www.reddit.com/r/learnmachinelearning/comments/182uw4r/discussion_is_it_possible_to_built_a_multillm/,0,1700840556.0," 

  
For example with the following structure:

* System = GPT-4 Turbo + Llama2 +3rd LLM (!)+ Google or Bing API for websearch + Langchain + any vectorDB + Document upload + longterm Memory + …

Idee behind it is to get more accurate, updated (websearch) and specialized system or even let the LLms discuss your prompt before completion! Question is also, how shall the interaction of multiple LLMs in a system be organzied (Algorithm, Python Library …)? And what kind of Interaction can/should this be? Master-slave or Multi-Master system?"
272,learnmachinelearning,llm,relevance,2023-09-23 15:20:41,Fine-tune 7B LLM on a Single GPU,mwitiderrick,False,1.0,6,16q76sg,https://www.reddit.com/r/learnmachinelearning/comments/16q76sg/finetune_7b_llm_on_a_single_gpu/,0,1695482441.0,"Fine-tuning a Llama 65B parameter model requires 780 GB of GPU memory.

This kind of compute is outside the purview of most individuals.

Thanks to parameter-efficient fine-tuning strategies, it is now possible to fine-tune a 7B parameter model on a single GPU, like the one offered by Google Colab for free. 

A great example of such a method is LoRA which freezes the weights of the LLM and introduces new weights for training. 

These new weights are based on the frozen ones but small enough to make fine-tuning a single GPU because the trainable parameters have been reduced. 

What's even more interesting about this method is that it doesn't introduce inference latency. 

The  QLoRA method goes further and quantizes the model to 4-bit further reducing the computational requirements. 

In the following article, I dive into these techniques, including: 
• Why you should fine-tune an LLM 
• Fine-tuning vs. prompting 
• How LoRA and QLoRA work
• Fine-tuning a Llama 2 7 billion model with LoRA and QLoRA (with sample notebook) 



https://www.mldive.com/how-to-fine-tune-llama-2-with-lora/"
273,learnmachinelearning,llm,relevance,2023-09-13 21:52:10,Which LLM that can be run locally gives the best results?,balaena7,False,0.75,4,16i01nw,https://www.reddit.com/r/learnmachinelearning/comments/16i01nw/which_llm_that_can_be_run_locally_gives_the_best/,8,1694641930.0,"I am interested, which of the LLMs on hugging face could be run locally and gives decent results, given a decent retailer GPU (e.g. RTX 4090)? How much do these results compare to ChatGPT?"
274,learnmachinelearning,llm,relevance,2023-10-04 21:58:52,Which LLM is best for analyzing code by design metrics?,Phi1ny3,False,0.78,5,16zzzy3,https://www.reddit.com/r/learnmachinelearning/comments/16zzzy3/which_llm_is_best_for_analyzing_code_by_design/,2,1696456732.0,"I'm working on a project, and I haven't decided which LLM to implement into my game.  The general idea is the game assesses what the player puts code in a terminal (language used is Python currently) based on a task/scenario/prompt, and after some time, the LLM will assess the code based on either the overall quality of the output, or on design metrics that the player is currently working on learning.  These are then quantified, and those stats are then fed into a procedural generation algorithm, which then outputs an in-game item for the user.  For an introductory level of programming, which LLM has been most accurate/thorough in analyzing code in this way?  I've looked into Bard, ChatGPT, and GitHub Copilot.  Are there other considerations you would recommend?"
275,learnmachinelearning,llm,relevance,2023-06-11 16:43:03,Large Language Model (LLM) Resources,TheGupta,False,1.0,9,146ymag,https://www.reddit.com/r/learnmachinelearning/comments/146ymag/large_language_model_llm_resources/,0,1686501783.0," **Courses**

* deeplearning.ai
   * [https://learn.deeplearning.ai/chatgpt-prompt-eng](https://learn.deeplearning.ai/chatgpt-prompt-eng/)
   * [https://learn.deeplearning.ai/chatgpt-building-system](https://learn.deeplearning.ai/chatgpt-building-system)
   * [https://learn.deeplearning.ai/langchain](https://learn.deeplearning.ai/langchain/)
* Full Stack Deep Learning
   * [https://fullstackdeeplearning.com/llm-bootcamp/spring-2023](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/)  
[YouTube Playlist](https://www.youtube.com/playlist?list=PL1T8fO7ArWleyIqOy37OVXsP4hFXymdOZ)

**Talks**

* [State of GPT by Andrej Karpathy](https://www.youtube.com/watch?v=bZQun8Y4L2A)
* [Rongyao Huang - Riding the Tailwind of NLP Explosion](https://www.youtube.com/watch?v=2nYhcI7LOi4)

**GitHub Libraries**

* For getting started with LLMs and experimentation
   * [https://github.com/hwchase17/langchain](https://github.com/hwchase17/langchain)
* Other Libraries:
   * [https://github.com/Hannibal046/Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)
   * [https://github.com/FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo)

**Papers**

* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)

&#x200B;

First I posted it on [Kaggle Discussions](https://www.kaggle.com/discussions/general/416483)."
276,learnmachinelearning,llm,relevance,2024-01-30 10:42:57,LFX Mentorship 2024 Spring LLM Projects: Get Paid Building Open Source AI Inference Infra,smileymileycoin,False,0.5,0,1aelkk6,https://www.secondstate.io/articles/lfx-mentorship-spring-2024/,1,1706611377.0,
277,learnmachinelearning,llm,relevance,2023-07-30 21:36:59,Guidance needed on training an LLM,AccomplishedPin4075,False,1.0,1,15dx9wu,https://www.reddit.com/r/learnmachinelearning/comments/15dx9wu/guidance_needed_on_training_an_llm/,0,1690753019.0,"I am trying to finetune an LLM. The use case is first I have to train the model for knowledge, and next I have to train the model for behavior.   


e.g. say I want to fine-tune a physics problem-solving assistant. Say a student wants help with the problem ""If a ball of 1 kg is moving at 5 meters/sec speed, how much energy is required to stop it"". The assistant may ask the student, ""Do you think the ball has potential energy or kinetic or both"". Based on the answer the assistant will ask another question and so on so that the student figures out the solution by herself.   


In this case, first, the LLM has to have knowledge of laws of motion, energy conversation, momentum conservation, etc.  -- The Knowledge part  


Next, it should be trained to behave like a guide and not a solver which is more of a behavioral training -- The Behavior part  
I am experimenting with Falcon 7B. I understand for the behavioral training I will have to create the dataset and fine-tune the LLM. But how to do the knowledge training given that the max input token supported by most open source LLMs is \~2K.   


Splitting the knowledge doc into sub-docs and indexing won't help for cases where multiple sub-docs have to be referred for a question just for the knowledge part.   


How do I go about it?"
278,learnmachinelearning,llm,relevance,2023-11-01 23:15:58,Integrating LLM with Internet Search Capability into Google Sheets Function,Interesting_Pack3655,False,1.0,2,17lp8vw,https://www.reddit.com/r/learnmachinelearning/comments/17lp8vw/integrating_llm_with_internet_search_capability/,1,1698880558.0,"Hello everyone,

I'm working on a project where I aim to enhance Google Sheets with a custom function that leverages a Large Language Model (LLM) to search the internet for information related to specific websites listed in a spreadsheet. The idea is to have the LLM automatically gather and summarize details from the web based on URLs or keywords provided in each row.

However, I've hit a roadblock: the APIs for LLMs that I've come across (like OpenAI's GPT models) don't inherently possess the ability to perform internet searches. They can generate text based on pre-trained data but can't fetch or search for new data in real-time.

Here's what I'm looking to achieve:

1. A user inputs a URL into a cell in Google Sheets.
2. The custom function calls upon an LLM to search the internet for the latest information related to that URL.
3. The LLM then summarizes this information and inputs it into the corresponding cell.

I'm seeking advice on how to approach this problem. Are there any LLMs that offer internet search capabilities, or is there a way to integrate an LLM with a web scraping tool or search API to achieve this functionality? If anyone has experience with similar integrations or can point me toward resources or tools that could help, I'd greatly appreciate it."
279,learnmachinelearning,llm,relevance,2023-04-07 23:24:38,Concurrent Throughput to LLM,Mr_Nice_,False,1.0,1,12f3h56,https://www.reddit.com/r/MachineLearning/comments/12ezjdn/d_llm_concurrent_throughput/,0,1680909878.0,
280,learnmachinelearning,llm,relevance,2024-01-25 09:48:08,A complete list of all the LLM evaluation metrics you need to care about,dillema_max,False,1.0,1,19f5s2d,/r/ChatGPT/comments/19f4gd0/a_complete_list_of_all_the_llm_evaluation_metrics/,0,1706176088.0,
281,learnmachinelearning,llm,relevance,2023-11-30 14:06:24,How to Improve your LLM? Find the Best & Cheapest Solution,OnlyProggingForFun,False,1.0,1,187jbno,https://youtu.be/pHv9SsE4Mb4,0,1701353184.0,
282,learnmachinelearning,llm,relevance,2023-04-14 16:18:14,The LLM Hacker's Handbook,Eriner_,False,1.0,3,12m2j24,https://doublespeak.chat/#/handbook,0,1681489094.0,
283,learnmachinelearning,llm,relevance,2023-06-12 12:48:21,Implement LLM from research paper example LLAMA model?,Accomplished-Clock56,False,0.72,3,147nb20,https://www.reddit.com/r/learnmachinelearning/comments/147nb20/implement_llm_from_research_paper_example_llama/,6,1686574101.0,"For those who have already or doing any implementations of research paper. 

I was asked  to implement Llama paper, 
Can you share any resources or examples  to achive this goal, there are some open source implementations of LlAmA but I'm not sire if they are really implementations  of the LLAMA  paper. 

Any suggestions or resources will help me
Thank you"
284,learnmachinelearning,llm,relevance,2023-10-22 07:47:22,New tech with old tech question for LLM machine build,metaldad2020,False,1.0,2,17do2cv,https://www.reddit.com/r/learnmachinelearning/comments/17do2cv/new_tech_with_old_tech_question_for_llm_machine/,0,1697960842.0,So I'm looking to do a llm machine on a budget and the 3060 12 GB of RAM is a relatively inexpensive card.  on top of that I came across the p40 GPU accelerator so my question is would a 3060 12 GB of RAM be compatible with SLI chain to a p40 or does it need to remain within the Pascal framework of graphics cards?
285,learnmachinelearning,llm,relevance,2024-02-15 16:20:21,"AI for Everyone: FREE Masterclass Courses with DataCamp | ML, DL, LLM & More",UseCreative4765,False,0.5,0,1arj8eu,https://youtu.be/GQ6jYGbja_U?si=YiKN70dLqFVkbvic,0,1708014021.0,
286,learnmachinelearning,llm,relevance,2023-07-20 08:17:37,Fact checking logic for a LLM tool,CaptainOk4061,False,1.0,6,154ktx0,https://www.reddit.com/r/learnmachinelearning/comments/154ktx0/fact_checking_logic_for_a_llm_tool/,0,1689841057.0,"Hi guys, 

Basically I am trying to determine a good logic that would be ideal if you want to fact check something. Say you have a quote in an article for example that you deem out of context or just ridiculous. 

If you had a tool, what questions would you ask it about that statement that would lead you to the source of the statement. 

Example: ""studies have shown that eating apples gives you cancer""

&#x200B;

Validation question examples: questions to ask the machine

* What was the study 
* Who wrote the study
* who is the source
* how many people referred to the source in the past x days

Another component is - what about a source makes it credible? What questions could validate a credible source? what categories of facts should the machine check?

* what type of source is it? (article, blog, journal, video, twitter post direct from the person)

&#x200B;

Let me know your guys thoughts, it's a bit open ended but i'm trying to get a good starting point to build the logic:

here's what I'm starting with.

https://preview.redd.it/djqmrl10x2db1.png?width=1037&format=png&auto=webp&s=f7904aeeb69600acfd4f240eb9954d2dd5b1e6bb"
287,learnmachinelearning,llm,relevance,2023-10-20 10:48:43,How to Customize a Local LLM for Scientific Pattern Matching?,prettyobviousthrow,False,1.0,1,17c8d2c,https://www.reddit.com/r/learnmachinelearning/comments/17c8d2c/how_to_customize_a_local_llm_for_scientific/,0,1697798923.0,"My goal is to run a free, local version of a large language model that is specifically trained to be familiar with the contents of a scientific textbook PDF so that it can perform relevant pattern recognition/comparisons. Specifically I want to be able to pass a large list of non-standardized descriptors to be checked against a standardized list of answers and return the closest option from the standardized list for each non-standardized input. This is to facilitate more efficient cataloging that is currently being done completely manually.

Is this feasible? If so, what would be appropriate topics to learn about/steps to take to achieve it? From basic reading I think that freedom gpt, gpt4all, and Llama-2 would be possible options to get started, but I wanted to see if anyone had recommendations to more efficiently direct my focus.

[Here is an example in ChatGPT demonstrating what I'm trying to do...](https://chat.openai.com/share/91427cfe-3a3c-497a-86fc-1f700bdb208f)

**Q:**

Answer options: Red, Blue, Gray

Test options: Cyan, Maroon, Pink, Silver, Blue, Navy, Fuschia, Black, Green

For each Test option, return the Answer option that it is closest to

**A:** 

* Cyan: Blue
* Maroon: Red
* Pink: Red
* Silver: Gray
* Blue: Blue
* Navy: Blue
* Fuchsia: Red
* Black: Gray
* Green: Blue

In my actual use case, ""Answer options"" contains a few hundred items and ""Test options"" has tens of thousands of items.

I have coding experience with C++, Python, and bash in case any of those are useful but my only experience working with local AI models has been with Whisper.cpp and Faster-Whisper. I have no understanding whatsoever about how the various Whisper models were actually generated, but I assume that I would need to generate/modify a model based on my chosen textbook somehow.

Any help/advice would be greatly appreciated."
288,learnmachinelearning,llm,relevance,2023-10-01 20:37:56,LLM Firewall - Guardrail Tutorial and Quickstart with OpenAI and Colab,Educational_Grass_38,False,1.0,14,16xc53k,https://m.youtube.com/watch?v=EnwVnz07h1I&pp=ygUSR3VhcmRyYWlsIEZpcmV3YWxs,5,1696192676.0,"Been working on a Firewall for devs to use in a few lines of code, to implement a protective layer around LLMs like OpenAI. Firewall has over 20+ detectors out-of-the-box including prompt injections, harmful content, toxicity and common security vulnerabilities.

Google Colab QuickStart: https://github.com/guardrail-ml/guardrail

Developer Docs: https://docs.useguardrail.com

Would appreciate if you could give a star and provide feedback, thanks!"
289,learnmachinelearning,llm,relevance,2023-12-15 12:42:51,How to Fine-Tune your LLM on Tweets! (large language models for investing),OnlyProggingForFun,False,0.56,1,18iz8be,https://youtu.be/3BlLXbbwENo,0,1702644171.0,
290,learnmachinelearning,llm,relevance,2024-01-27 08:50:20,A computer for ML - running LLM locally + using AI tools (Gamings vs Workstation build),RandomUser_88,False,1.0,1,1ac6kq9,https://www.reddit.com/r/learnmachinelearning/comments/1ac6kq9/a_computer_for_ml_running_llm_locally_using_ai/,3,1706345420.0,"Hi there,

I'm looking for advice on a computer build here. I want to make my workstation for ML applications, especially running open-source LLMs locally + using other AI tools. My laptop can't handle any of that, I don't want to run it on the cloud (I want the LLMs to access my personal data). My budget is around 4 - 5k USD.

**I am deciding between two concepts:**

**1) Gaming option:**

* Ryzen 9 7950X3D (I feel that AMD makes much more sense, especially with AM5 socket being used at least till 2025)
* 128 GB RAM DDR5
* GeForce RTX 4090 (potentially two at some point)

Just to let you know, I don't play or plan to start.



**2) Workstation option:**

* Ryzen Threadripper or Intel Xeon
* 256GB RAM DDR4 ECM
* A4500 / A5500 (up to two) / GeForce RTX 4090



What would you recommend? Please don't tell me that I should use clusters, for now, I really want to stay locally.



Thanks!

"
291,learnmachinelearning,llm,relevance,2023-12-19 14:49:23,Recommendation of a Multilingual Large Language Model (LLM) for the Question-Answering Task,techpoweruser,False,0.99,1,18m3mg2,https://www.reddit.com/r/learnmachinelearning/comments/18m3mg2/recommendation_of_a_multilingual_large_language/,0,1702997363.0,"Dear Community.

I would like to request the community for their informed suggestions on the choice of possible LLMs for my task. I wish to *fine tune a multilingual LLM on my custom made QA dataset*. I am looking for the following attributes in a LLM:

* must be **open** and released in either **2022** or **2023** (preferable)
* must have **state-of-the-art** performance, especially in the QA task
* must be pre-trained on **multilingual data**
* must support the **question-answering** task

Thank you."
292,learnmachinelearning,llm,relevance,2023-08-18 18:37:35,Overcoming LLM Context Windows with RAG (Retrieval Augmented Generation),vanlifecoder,False,1.0,2,15usk5c,https://www.reddit.com/r/learnmachinelearning/comments/15usk5c/overcoming_llm_context_windows_with_rag_retrieval/,2,1692383855.0,"Retrieval-Augmented Generation, or RAG, represents an exciting frontier in artificial intelligence and natural language processing. By bridging information retrieval and text generation, RAG can answer questions by finding relevant information and then synthesizing responses in a coherent and contextually rich way.

**[Full Post](https://nux.ai/vocab/rag)**

What is Retrieval-Augmented Generation (RAG)?
---------------------------------------------

RAG is a method that combines two significant aspects:

1.  **Information Retrieval**: This involves searching through large databases or collections of text to find documents that are relevant to a given query.
2.  **Text Generation**: Once relevant documents are found, a model like a Transformer is used to synthesize the information into a coherent and concise response.

RAG models utilize powerful machine learning algorithms to carry out both retrieval and generation tasks.

https://cms.nux.ai/content/images/2023/08/Screen-Shot-2023-08-18-at-1.29.47-PM.png

Why is RAG Important?
---------------------

LLMS have limited context windows. The intuitive response is to increase the size of that context window, but [researchers at Stanford](https://arxiv.org/pdf/2307.03172.pdf?ref=cms.nux.ai) found that doing so actually doesn't correlate to performance (measured by accuracy).

https://cms.nux.ai/content/images/2023/08/Screen-Shot-2023-08-18-at-1.34.55-PM.png

> Models are better at using relevant information that occurs at the very beginning or end of its input context, and performance degrades significantly when models must access and use the information located in the middle of its input context.

So in order to exceed this window, we need to use **Retrieval Augmented Generation.**

Primary Use Cases of RAG
------------------------

### Customer Support

RAG can provide immediate, context-aware responses to customer queries by searching through existing knowledge bases and FAQs.

### Summarization

RAG can analyze large documents, identify the most important information, and condense it into a readable summary.

### Research Assistance

In academic and corporate settings, RAG can sift through vast amounts of research papers and provide concise insights or answers to specific questions.

### Conversational AI

RAG can be employed to build intelligent chatbots that can engage in meaningful dialogues, retrieve relevant information, and generate insightful responses.

Code: Using RAG to Provide Contextual Answers
---------------------------------------------

Here's a code snippet that demonstrates how to use RAG to extract parts of a large document, prompt a question, and generate a conversational answer. This example makes use of the GPT-3.5 model through OpenAI's API.

    import json
    import requests
    
    key = ""API_KEY""
    
    top_n_docs = doc_score_pairs[:5]
    
    # Concatenating the top 5 documents
    text_to_summarize = [doc for doc, score in doc_score_pairs]
    
    # prompt as context
    
    contexts = f""""""
                Question: {query}
                Contexts: {text_to_summarize}
    """"""
    
    content = f""""""
                You are an AI assistant providing helpful advice.
                You are given the following extracted parts of a long document and a question. 
                Provide a conversational answer based on the context provided. 
                You should only provide hyperlinks that reference the context below. 
                Do NOT make up hyperlinks. If you can't find the answer in the context below, 
                just say ""Hmm, I'm not sure. Try one of the links below."" Do NOT try to make up an answer. 
                If the question is not related to the context, politely respond that you are tuned to only answer 
                questions that are related to the context. Do NOT however mention the word ""context""
                in your responses. 
                =========
                {contexts}
                =========
                Answer in Markdown
            """"""
    
    url = ""https://api.openai.com/v1/chat/completions""
    
    payload = json.dumps({
      ""model"": ""gpt-3.5-turbo"",
      ""messages"": [
        {
          ""role"": ""user"",
          ""content"": content
        }
      ]
    })
    headers = {
      'Authorization': f'Bearer {key}',
      'Content-Type': 'application/json'
    }
    
    response = requests.request(""POST"", url, headers=headers, data=payload)
    
    just_text_response = response.json()['choices'][0]['message']['content']
    print(just_text_response)

[Live Example](https://collie.ai/tesla?ref=cms.nux.ai)"
293,learnmachinelearning,llm,relevance,2023-11-27 16:29:09,"Host a reliable LLM on-prem for knowledge base consultation, which to choose from?",durian_pizza,False,0.5,0,1856xt9,https://www.reddit.com/r/learnmachinelearning/comments/1856xt9/host_a_reliable_llm_onprem_for_knowledge_base/,4,1701102549.0,"Hey all, I was tasked to find a suitable LLM that would be able to be run on-prem as this requires certain levels of security.

The obvious first thought went to GPT models, but then I need this to be absolutely precise as if reading word per word out of the documents it will be trained on. The idea is still to re-use a pre-trained model, as I have nowhere near enough material to train a new one, but then using LangChain and other libraries there are ways to host this entirely on my own. Then the model will be further trained on the documents I will give it (mainly PDF files and HTML stuff).

I thought BERT would also be a good choice here, do you have any other suggestions?

&#x200B;"
294,learnmachinelearning,llm,relevance,2023-08-18 18:10:27,Overcoming LLM Context Windows with RAG (Retrieval Augmented Generation),nuxai,False,1.0,1,15urutk,https://www.reddit.com/r/learnmachinelearning/comments/15urutk/overcoming_llm_context_windows_with_rag_retrieval/,0,1692382227.0,"Retrieval-Augmented Generation, or RAG, represents an exciting frontier in artificial intelligence and natural language processing. By bridging information retrieval and text generation, RAG can answer questions by finding relevant information and then synthesizing responses in a coherent and contextually rich way.

**[Full Post](https://nux.ai/vocab/rag)**

What is Retrieval-Augmented Generation (RAG)?
---------------------------------------------

RAG is a method that combines two significant aspects:

1.  **Information Retrieval**: This involves searching through large databases or collections of text to find documents that are relevant to a given query.
2.  **Text Generation**: Once relevant documents are found, a model like a Transformer is used to synthesize the information into a coherent and concise response.

RAG models utilize powerful machine learning algorithms to carry out both retrieval and generation tasks.

https://cms.nux.ai/content/images/2023/08/Screen-Shot-2023-08-18-at-1.29.47-PM.png

Why is RAG Important?
---------------------

LLMS have limited context windows. The intuitive response is to increase the size of that context window, but [researchers at Stanford](https://arxiv.org/pdf/2307.03172.pdf?ref=cms.nux.ai) found that doing so actually doesn't correlate to performance (measured by accuracy).

https://cms.nux.ai/content/images/2023/08/Screen-Shot-2023-08-18-at-1.34.55-PM.png

> Models are better at using relevant information that occurs at the very beginning or end of its input context, and performance degrades significantly when models must access and use the information located in the middle of its input context.

So in order to exceed this window, we need to use **Retrieval Augmented Generation.**

Primary Use Cases of RAG
------------------------

### Customer Support

RAG can provide immediate, context-aware responses to customer queries by searching through existing knowledge bases and FAQs.

### Summarization

RAG can analyze large documents, identify the most important information, and condense it into a readable summary.

### Research Assistance

In academic and corporate settings, RAG can sift through vast amounts of research papers and provide concise insights or answers to specific questions.

### Conversational AI

RAG can be employed to build intelligent chatbots that can engage in meaningful dialogues, retrieve relevant information, and generate insightful responses.

Code: Using RAG to Provide Contextual Answers
---------------------------------------------

Here's a code snippet that demonstrates how to use RAG to extract parts of a large document, prompt a question, and generate a conversational answer. This example makes use of the GPT-3.5 model through OpenAI's API.

    import json
    import requests
    
    key = ""API_KEY""
    
    top_n_docs = doc_score_pairs[:5]
    
    # Concatenating the top 5 documents
    text_to_summarize = [doc for doc, score in doc_score_pairs]
    
    # prompt as context
    
    contexts = f""""""
                Question: {query}
                Contexts: {text_to_summarize}
    """"""
    
    content = f""""""
                You are an AI assistant providing helpful advice.
                You are given the following extracted parts of a long document and a question. 
                Provide a conversational answer based on the context provided. 
                You should only provide hyperlinks that reference the context below. 
                Do NOT make up hyperlinks. If you can't find the answer in the context below, 
                just say ""Hmm, I'm not sure. Try one of the links below."" Do NOT try to make up an answer. 
                If the question is not related to the context, politely respond that you are tuned to only answer 
                questions that are related to the context. Do NOT however mention the word ""context""
                in your responses. 
                =========
                {contexts}
                =========
                Answer in Markdown
            """"""
    
    url = ""https://api.openai.com/v1/chat/completions""
    
    payload = json.dumps({
      ""model"": ""gpt-3.5-turbo"",
      ""messages"": [
        {
          ""role"": ""user"",
          ""content"": content
        }
      ]
    })
    headers = {
      'Authorization': f'Bearer {key}',
      'Content-Type': 'application/json'
    }
    
    response = requests.request(""POST"", url, headers=headers, data=payload)
    
    just_text_response = response.json()['choices'][0]['message']['content']
    print(just_text_response)

[Live Example](https://collie.ai/tesla?ref=cms.nux.ai)"
295,learnmachinelearning,llm,relevance,2023-12-07 21:58:10,"Recent updates on LLM Explorer: 13,000+ models in the database and more",Greg_Z_,False,1.0,4,18d6t7u,https://www.reddit.com/r/learnmachinelearning/comments/18d6t7u/recent_updates_on_llm_explorer_13000_models_in/,0,1701986290.0,"I've recently updated the LLM Explorer ([https://llm.extractum.io](https://llm.extractum.io)) -  a directory that houses more than 13,000 LLMs. For those, who are new to the project: it offers a user-friendly interface for searching and filtering models by categories, comparing them and swiftly identifying superior alternatives (which is notably more convenient and faster than browsing HuggingFace).

What's new:

* The model card has been improved with cleansed and more accurate data.
* Now the model includes references to the base model for fine-tuned and quantized LLMs, as well as a list of the best alternatives (where it's listed), and your can see the list of models that have the same base model (e.g. [https://llm.extractum.io/list/?base\_model=psmathur/orca\_mini\_v3\_7b](https://llm.extractum.io/list/?base_model=psmathur/orca_mini_v3_7b))
* UI improvements to remember the last selected filters/searches for each list/category (e.g. the category ""models that fit 16GB VRAM"" and ""Code Generation"" will have individual filters).
* You can now copy the URL of the model card to your clipboard by clicking on the ""copy"" icon. Refer to the screenshot below for guidance.

I've composed a brief guide on the interface and usage of the LLM Explorer: [https://medium.com/p/de0bfa160c14](https://medium.com/p/de0bfa160c14). This can assist you in becoming more acquainted with the interface and the strategies for finding the most suitable LLM.

I look forward to receiving your feedback and feature requests about the project.

Thanks!

https://preview.redd.it/x4pbkxns2y4c1.png?width=1912&format=png&auto=webp&s=cb0c526ae066f88e4ef436ca5eeee5a321f3ea6d

&#x200B;

https://preview.redd.it/x4iz10jq2y4c1.png?width=1921&format=png&auto=webp&s=16ba7892d7dc812d22eab5ba77582b3c3059b454"
296,learnmachinelearning,llm,relevance,2023-07-27 12:53:45,Does increasing the context of a LLM increase model size?,Clear_Pepper_7142,False,0.76,4,15b14zo,https://www.reddit.com/r/learnmachinelearning/comments/15b14zo/does_increasing_the_context_of_a_llm_increase/,2,1690462425.0,"Given a decoder-only Transformer-based LLM (like GPT), does increasing the context window (i know some models have hit up to 100k, probably more) mean you have to increase the size of the Q/K/V matrices in the multi-headed attention layers in order to attend to the entire context window? That seems inefficient if a lot of your data doesn't fill the context window."
297,learnmachinelearning,llm,relevance,2023-03-21 14:29:47,Doublespeak.chat: an LLM sandbox escape game,Eriner_,False,0.82,12,11xijaq,https://doublespeak.chat,3,1679408987.0,
298,learnmachinelearning,llm,relevance,2023-12-20 03:20:43,One-command line to run Self-hosted Opensource LLM models on Mac/ across devices.,smileymileycoin,False,1.0,1,18mkkj9,https://www.secondstate.io/articles/run-llm-sh/,0,1703042443.0,
299,learnmachinelearning,llm,relevance,2023-08-11 15:22:51,The NeurIPS 2023 LLM Efficiency Challenge Starter Guide,seraschka,False,1.0,4,15obttq,https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/,0,1691767371.0,
