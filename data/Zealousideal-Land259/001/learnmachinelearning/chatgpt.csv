,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,id,url,num_comments,created,body
0,learnmachinelearning,chatgpt,top,2023-04-04 14:34:29,Working with chatGPT,macronancer,False,0.97,610,12bkzjv,https://i.redd.it/5uwfzjh4pvra1.png,22,1680618869.0,
1,learnmachinelearning,chatgpt,top,2023-04-26 06:23:17,Hugging Face Releases Free Alternative To ChatGPT,vadhavaniyafaijan,False,0.98,389,12z8n4e,https://www.theinsaneapp.com/2023/04/free-alternative-to-chatgpt.html,35,1682490197.0,
2,learnmachinelearning,chatgpt,top,2023-02-19 13:55:13,ChatGPT History,eforebrahim,False,0.86,254,116au66,https://i.redd.it/dv8cfj0nz6ja1.jpg,27,1676814913.0,
3,learnmachinelearning,chatgpt,top,2023-04-06 11:12:52,Meta: Is it possible to ban these TikTok influencers or TikToks in general?,dasMaiMaiKamel,False,0.94,218,12dgtry,https://www.reddit.com/r/learnmachinelearning/comments/12dgtry/meta_is_it_possible_to_ban_these_tiktok/,14,1680779572.0,"I'm new to this sub and I'd love to contribute here. But there are soooo many TikTok videos from someone talking about ChatGPT for the 10.000th time. These videos don't contribute to learning ML nor do they give actual reliable information. I often get the feeling that these people never touched a NN, just sat on ChatGPT and read one WikiPedia article. It's also often more an ad than actual help.  


  
Even if I'm not a member for too long, I see comments criticizing this exact thing under every video. Is it possible to add a rule to prevent this? It would greatly improve the quality of this sub."
4,learnmachinelearning,chatgpt,top,2023-01-31 16:17:42,ChatGPT Crossed 10 Million Daily Active Users In Just 40 Days,vadhavaniyafaijan,False,0.95,215,10q34ra,https://www.theinsaneapp.com/2023/01/chatgpt-crossed-10-million-user.html,32,1675181862.0,
5,learnmachinelearning,chatgpt,top,2023-02-16 10:29:31,OpenAI Has Purchased AI.Com For ChatGPT For $11M,vadhavaniyafaijan,False,0.93,209,113nizs,https://www.theinsaneapp.com/2023/02/openai-purchased-ai-com-domain.html,23,1676543371.0,
6,learnmachinelearning,chatgpt,top,2023-02-11 12:46:22,"ChatGPT Powered Bing Chatbot Spills Secret Document, The Guy Who Tricked Bot Was Banned From Using Bing Chat",vadhavaniyafaijan,False,0.94,208,10zmtqz,https://www.theinsaneapp.com/2023/02/chatgpt-bing-rules.html,15,1676119582.0,
7,learnmachinelearning,chatgpt,top,2023-03-16 16:51:03,Introducing OpenChatKit - The Open-Source Alternative to ChatGPT,kingabzpro,False,0.98,204,11szhsh,https://www.reddit.com/r/learnmachinelearning/comments/11szhsh/introducing_openchatkit_the_opensource/,21,1678985463.0,"Hey everyone! I'm excited to share my latest article about a new open-source technology called OpenChatKit.

For those who work in NLP, you're probably familiar with ChatGPT - a powerful language model that can perform various natural language processing tasks. However, ChatGPT is not open-source, which limits its accessibility and customizability.

OpenChatKit, on the other hand, is an open-source alternative to ChatGPT that provides users with similar NLP capabilities while allowing for more customization and control. With OpenChatKit, users can train their own models and fine-tune them to their specific use cases.

In my article, I dive into the features of OpenChatKit, the Instruction-tuned Large Language Model, and the Limitations of the Model.

If you're interested in learning more about OpenChatKit and how it can enhance your NLP workflows, check out my article [OpenChatKit: Open-Source ChatGPT Alternative ](https://www.kdnuggets.com/2023/03/openchatkit-opensource-chatgpt-alternative.html). I'd love to hear your thoughts and answer any questions you may have."
8,learnmachinelearning,chatgpt,top,2023-10-12 20:36:53,ChatGPT vision feature is really useful for understanding research papers!,nxtboyIII,False,0.86,189,176gs37,https://i.redd.it/xe94y8hf1utb1.png,42,1697143013.0,
9,learnmachinelearning,chatgpt,top,2023-09-23 13:42:22,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.96,182,16q4ve6,https://www.reddit.com/r/learnmachinelearning/comments/16q4ve6/this_week_in_ai_all_the_major_ai_developments_in/,16,1695476542.0,"1. **Genmo** releases a new text-to-video model: **Genmo Replay** v0.1, which generates high-quality videos from text without the need for advanced prompt engineering. *Genmo is available for free to create AI videos* \[[*Details*](https://blog.genmo.ai/log/replay-ai-video) | [Genmo *Replay*](https://www.genmo.ai/)\] .
2. **OpenAI** unveils **DALL·E 3** \- a major update to the text-to-image model, which will be integrated in ChatGPT. It will be available to ChatGPT Plus and Enterprise users in October, via the API and in Labs later this fall. Creators can now also opt their images out from future training.
3. **Toyota Research Institute** has developed a technique, powered by generative AI, that enables teaching robots new manipulation abilities in a single afternoon. Using the same robot, same code, and same setup, TRI taught over 60 different dexterous behaviors like peeling vegetables, using hand mixers, preparing snacks, and flipping pancakes.
4. **Microsoft** announced:
   1. Availability of AI Copilot for Windows from September 26th. Copilot will incorporate the context and intelligence of the web, your work data and what you are doing in the moment on your PC to provide better assistance. It will be integrated in Windows 11, Microsoft 365, Edge and Bing.
   2. Bing will add support for DALL.E 3 and deliver more personalized answers based on search history.
   3. New AI powered experiences in Paint, Photos and Clipchamp.
   4. New AI-powered shopping experience
5. **ElevenLabs** released **Projects** \- a tool that lets you generate an entire audiobook at the click of a button. Projects now supports .epub, .pdf, and .txt file imports, as well as initializing a project from a URL.
6. **Deci** presents **DeciDiffusion 1.0** \- an open-source text-to-image latent diffusion model which is 3x faster than Stable Diffusion v1.5 with the same quality.
7. **Google researchers** present a new approach that produces photo-realistic animations from a single picture. The model is trained on automatically extracted motion trajectories from a large collection of real video sequences.
8. **Google** has updated Bard\*\]\*:
9. **Bard Extensions:** With extensions, Bard can now connect to your Google apps and services like Gmail, Docs, Drive, Google Maps, YouTube, and Google Flights and hotels.
10. Users can use Bard’s “Google it” button to more easily double-check its answers and evaluate whether there is content across the web to substantiate it.
11. Bard can now let you continue chat via **shared public links**
12. **YouTube** announces new AI tools for creators. **Dream Screen** will let users create an AI-generated video or image background from text. Automatic AI-dubbing tool called **Aloud**, which will be integrated into YouTube Studio. **AI-powered insights** to generate video ideas and draft outlines. **Assistive Search in Creator Music** where AI will suggest the right music based on your description of your content.
13. **Amazon** announced that its voice assistant Alexa is being upgraded with a new, custom-built large language model.
14. **IBM** open-sources **MoLM** \- a collection of ModuleFormer-based language models ranging in scale from 4 billion to 8 billion parameters. ModuleFormer is a new neural network architecture based on the Sparse Mixture of Experts (SMoE) by IBM researchers. .
15. **Neuralink**, Elon Musk's brain implant startup, set to begin human trials.
16. **Lexica** has released **Aperture v3.5** \- their latest next-gen image model that can create photorealistic images and follows your prompt with precision.
17. **OpenAI** has invited domain experts to collaborate in evaluating and improving the safety of OpenAI's models by joining the new **OpenAI Red Teaming Network**.
18. \*\*GitHub Copilot Chat (\*\*beta) is now available for all individuals.
19. **Replit** announced a virtual hackathon for projects built using **Replit ModelFarm**
20. **Oracle** brings voice-activated AI to healthcare with Clinical Digital Assistant.
21. **Google** and the Department of Defense are building an AI-powered microscope to help doctors spot cancer.

My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
10,learnmachinelearning,chatgpt,top,2023-03-02 16:47:40,Build ChatGPT for Financial Documents with LangChain + Deep Lake,davidbun,False,0.95,171,11g7h03,https://www.reddit.com/r/learnmachinelearning/comments/11g7h03/build_chatgpt_for_financial_documents_with/,8,1677775660.0,"https://preview.redd.it/h9r6hgvfucla1.png?width=2388&format=png&auto=webp&s=5432eac3eeed8583e4309af1fdc7ebecac705796

As the world is increasingly generating vast amounts of financial data, the need for advanced tools to analyze and make sense of it has never been greater. This is where [LangChain](https://github.com/hwchase17/langchain) and [Deep Lake](https://github.com/activeloopai/deeplake) come in, offering a powerful combination of technology to help build a question-answering tool based on financial data. After participating in a LangChain hackathon last week, I created a way to use Deep Lake, the data lake for deep learning (a package my team and I are building) with LangChain. I decided to put together a guide of sorts on how you can approach building your own question-answering tools with  LangChain and Deep Lake as the data store.

Read [the article](https://www.activeloop.ai/resources/ultimate-guide-to-lang-chain-deep-lake-build-chat-gpt-to-answer-questions-on-your-financial-data/) to learn:

1. What is LangChain, what are its benefits and use cases and how you can use to streamline your LLM (Large Language Model) development?  
2. How to use [\#LangChain](https://www.linkedin.com/feed/hashtag/?keywords=langchain&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) and [\#DeepLake](https://www.linkedin.com/feed/hashtag/?keywords=deeplake&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) together to build [\#ChatGPT](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) for your financial documents.  
3. How Deep Lake’s unified and streamable data store enables fast prototyping without the need to recompute embeddings (something that costs time & money).  


I hope you like it, and let me know if you have any questions!"
11,learnmachinelearning,chatgpt,top,2023-12-10 18:07:35,Are LLMs overhyped right now?,Snoo_72181,False,0.94,159,18f9enp,https://www.reddit.com/r/learnmachinelearning/comments/18f9enp/are_llms_overhyped_right_now/,67,1702231655.0,"I mean I get that ChatGPT has made LLMs the toast of ML universe. They are indeed amazing.  

But this has lead to so much hype that ML beginners are literally just talking about learning LLMs, ignoring so much in between like Math and Stats, simple ML like Regression, Classification. After that you have, Deep Learning, Transformers and finally LLMs. 

Companies also want candidates with LLM experience, but there's no guarantee that they even have a use case for LLMs "
12,learnmachinelearning,chatgpt,top,2023-12-24 02:11:09,"Is it true that current LLMs are actually ""black boxes""?",wouhf,False,0.88,154,18pl1wx,https://www.reddit.com/r/learnmachinelearning/comments/18pl1wx/is_it_true_that_current_llms_are_actually_black/,105,1703383869.0,"As in nobody really understands exactly how Chatgpt 4 for example gives an output based on  some input. How true is it that they are black boxes?

Because it seems we do understand exactly how the output is produced? "
13,learnmachinelearning,chatgpt,top,2023-06-28 12:29:48,"Intern tasked to make a ""local"" version of chatGPT for my work",Assasinshock,False,0.97,153,14l887h,https://www.reddit.com/r/learnmachinelearning/comments/14l887h/intern_tasked_to_make_a_local_version_of_chatgpt/,104,1687955388.0,"Hi everyone,

I'm currently an intern at a company, and my mission is to make a proof of concept of an conversational AI for the company.They told me that the AI needs to be trained already but still able to get trained on the documents of the company, the AI needs to be open-source and needs to run locally so no cloud solution.

The AI should be able to answers questions related to the company, and tell the user which documents are pertained to their question, and also tell them which departement to contact to access those files.

For this they have a PC with an I7 8700K, 128Gb of DDR4 RAM and an Nvidia A2.

I already did some research and found some solution like localGPT and local LLM like vicuna etc, which could be usefull, but i'm really lost on how i should proceed with this task. (especially on how to train those model)

That's why i hope you guys can help me figure it out. If you have more questions or need other details don't hesitate to ask.

Thank you.  


Edit : They don't want me to make something like chatGPT, they know that it's impossible. They want a prototype that can answer question about their past project. "
14,learnmachinelearning,chatgpt,top,2023-02-01 04:14:49,ChatGPT Extension for VSCode,Dense_Dimension_913,False,0.96,154,10qk8en,https://www.reddit.com/r/learnmachinelearning/comments/10qk8en/chatgpt_extension_for_vscode/,22,1675224889.0,"Created a ChatGPT extension for VSCode to help programmers understand  and read code more easily and also other features. To start, simply highlight a piece of code  and click on the plus icon on the  left to open up a chat and start  talking with ChatGPT, or Codex, or text-davinci-003. You can choose the  model you want to use in the settings. More details in the links below. I  really hope this extension can be useful to many people out there.  Please give it a try and let me know if you guys see any bugs or if you  like the extension. Thanks!

&#x200B;

https://i.redd.it/28zslxm06ifa1.gif

[VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=AndrewZhang.scribeai)"
15,learnmachinelearning,chatgpt,top,2022-12-28 04:37:21,University Professor Catches Student Cheating With ChatGPT,vadhavaniyafaijan,False,0.94,142,zx0ep0,https://www.theinsaneapp.com/2022/12/university-professor-catches-student-cheating-with-chatgpt.html,108,1672202241.0,
16,learnmachinelearning,chatgpt,top,2023-02-04 23:41:43,"ChatGPT's Inner Magic, Explained Step-by-Step",wwllol,False,0.96,140,10tuywc,https://youtu.be/-9SdOPe294w,1,1675554103.0,
17,learnmachinelearning,chatgpt,top,2023-01-25 01:15:22,How ChatGPT is Trained,ariseff,False,0.98,127,10km46l,https://youtu.be/VPRSBzXzavo,8,1674609322.0,
18,learnmachinelearning,chatgpt,top,2023-01-06 11:58:10,How does ChatGPT actually work? Explained simply with pen and paper,techie_ray,False,0.94,127,104sebq,https://youtu.be/k9Sps7ciNTE,16,1673006290.0,
19,learnmachinelearning,chatgpt,top,2023-01-17 07:51:07,DeepMind To Launch ChatGPT Rival Sparrow Soon,vadhavaniyafaijan,False,0.96,125,10e6h7j,https://www.theinsaneapp.com/2023/01/deepmind-to-launch-chatgpt-rival-sparrow.html,5,1673941867.0,
20,learnmachinelearning,chatgpt,top,2022-12-24 09:14:57,"How would I train a chatbot like ChatGPT on a specific data set, so that it answers questions as if it's belief structure was based on the information I give it?",EllyEscape,False,0.92,118,zu6785,https://www.reddit.com/r/learnmachinelearning/comments/zu6785/how_would_i_train_a_chatbot_like_chatgpt_on_a/,41,1671873297.0,"This might be a noob question, so I'll write it to my best abilities. I have some experience with coding video game AI in Godot, Unity and Unreal but I've never touched ML or ""real""(?) AI that uses learning algorithms. 

&#x200B;

I wanted to give a sophisticated chatbot like ChatGPT a bunch of data and text from (for instance, not my end goal) a philosopher, and have it answer questions as if it was that philosopher, ague against what I say as if it was a person who believed what the text I gave it said and so on, all while still able to use online resources (like ChatGPT does) to find additional supporting information, rather than only the text I give it which might limit its ability to give coherent arguments. In summary, I want it's beliefs  and values to be limited to a specific source text, but not it's knowledge base. 

&#x200B;

How would I go about this? Do I have to develop a model from scratch to give it any text sources I want, or is it possible to do with an existing API? I was going to use Character.AI but the method for giving it information is too limited for what I want to do. 

&#x200B;

If anyone has any resources to get me started it would be very helpful! Thank you."
21,learnmachinelearning,chatgpt,top,2023-02-24 06:26:36,Is there a way to easily train ChatGPT or GPT on custom knowledge?,senttoschool,False,0.99,117,11akisx,https://www.reddit.com/r/learnmachinelearning/comments/11akisx/is_there_a_way_to_easily_train_chatgpt_or_gpt_on/,46,1677219996.0,"My company has internal documents. It'd be nice to be able to have GPT look over it, and then I can ask it questions on the internal documents."
22,learnmachinelearning,chatgpt,top,2023-02-11 06:58:18,[N] New Open-Source Version Of ChatGPT ⭕,LesleyFair,False,0.98,115,10zep6u,https://www.reddit.com/r/learnmachinelearning/comments/10zep6u/n_new_opensource_version_of_chatgpt/,8,1676098698.0,"GPT is getting competition from open-source.

A group of researchers, around the YouTuber [Yannic Kilcher](https://www.ykilcher.com/), have announced that they are working on [Open Assistant](https://github.com/LAION-AI/Open-Assistant). The goal is to produce a chat-based language model that is much smaller than GPT-3 while maintaining similar performance.

If you want to support them, they are crowd-sourcing training data [here](https://open-assistant.io/).

**What Does This Mean?**

Current language models are too big.

They require millions of dollars of hardware to train and use. Hence, access to this technology is limited to big organizations. Smaller firms and universities are effectively shut out from the developments.

Shrinking and open-sourcing models will facilitate academic research and niche applications.

Projects such as Open Assistant will help to make language models a commodity. Lowering the barrier to entry will increase access and accelerate innovation.

What an exciting time to be alive! 

Thank you for reading! I really enjoyed making this for you!  
The Decoding ⭕ is a thoughtful weekly 5-minute email that keeps you in the loop about machine research and the data economy. [Click here to sign up](https://thedecoding.net/)!"
23,learnmachinelearning,chatgpt,top,2023-01-11 05:23:14,Thoughts on this ChatGPT fact-checker tool I built this past week?,QuestionAnxious,False,0.89,89,108wigf,https://v.redd.it/yqudljp0ncba1,25,1673414594.0,
24,learnmachinelearning,chatgpt,top,2023-01-27 14:51:14,Fine-tuning open source models to emulate ChatGPT for code explanation.,awesomequantity,False,0.89,88,10mmofg,https://www.reddit.com/r/learnmachinelearning/comments/10mmofg/finetuning_open_source_models_to_emulate_chatgpt/,13,1674831074.0,"I'm looking to step up my game and emulate ChatGPT for specific use-cases like explaining code. I'm thinking about using open source models like GPT-J, or OPT to get beyond the limitations of the closed-source nature of ChatGPT, like the amount of text it can read or respond with.

I got the funding for training, hardware, etc, and I want the end product to be on-premises, so no worries there. The inference doesn't have to be super fast either. I know there are projects like OpenAssistant and petals.ml but haven’t made enough research just yet.

One option I’m considering is using fine tuners like the one from [HuggingFace](https://github.com/subhasisj/HuggingFace-Transformers-FineTuning) or [Jina AI](https://github.com/jina-ai/finetuner) to fine-tune open source models like GPT-J or OPT to improve specific use-cases like code explanation. With the funding that we have, I wouldn’t want to cheap out on fine-tuning and expect something good.

So, can anyone help out and point me in the right direction? Which model is the best to fine-tune and how do I fine-tune to improve specific use cases? Any help would be appreciated. Thanks!"
25,learnmachinelearning,chatgpt,top,2023-09-30 15:01:31,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.98,84,16w93bx,https://www.reddit.com/r/learnmachinelearning/comments/16w93bx/this_week_in_ai_all_the_major_ai_developments_in/,4,1696086091.0,"1. **Meta AI** presents **Emu**, a quality-tuned latent diffusion model for generating highly aesthetic images. Emu significantly outperforms SDXLv1.0 on visual appeal.
2. **Meta AI** researchers present a series of long-context LLMs with context windows of up to 32,768 tokens. LLAMA 2 70B variant surpasses gpt-3.5-turbo-16k’s overall performance on a suite of long-context tasks.
3. **Abacus AI** released a larger 70B version of **Giraffe**. Giraffe is a family of models that are finetuned from base Llama 2 and have a larger context length of 32K tokens\].
4. **Meta** announced:  

   1. **Meta AI** \- a new AI assistant users can interact with on WhatsApp, Messenger and Instagram. Will also be available on Ray-Ban Meta smart glasses and Quest 3, Meta’s mixed reality headset.
   2. **AI stickers** that enable users to generate customized stickers for chats and stories using text. Powered by Llama 2 and the new foundational model for image generation, Emu.
   3. **28 AI characters**, each with a unique personality that users can message on WhatsApp, Messenger, and Instagram.
   4. New AI editing tools, **restyle** and **backdrop** in Instagram.
   5. **AI Studio** \- a platform that supports the creation of custom AIs by coders and non-coders alike.
5. **Cerebras** and **Opentensor** released Bittensor Language Model, ‘**BTLM-3B-8K**’, a new 3 billion parameter open-source language model with an 8k context length trained on 627B tokens of SlimPajama. It outperforms models trained on hundreds of billions more tokens and achieves comparable performance to open 7B parameter models. The model needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models and is available with an Apache 2.0 license for commercial use.
6. **OpenAI** is rolling out, over the next two weeks, new voice and image capabilities in ChatGPT enabling ChatGPT to understand images, understand speech and speak. The new voice capability is powered by a new text-to-speech model, capable of generating human-like audio from just text and a few seconds of sample speech. .
7. **Mistral AI**, a French startup, released its first 7B-parameter model, **Mistral 7B**, which outperforms all currently available open models up to 13B parameters on all standard English and code benchmarks. Mistral 7B is released in Apache 2.0, making it usable without restrictions anywhere.
8. **OpenAI** has returned the ChatGPT browsing feature for Plus subscribers, enabling ChatGPT to access internet for current information. It was disabled earlier as users were able to deploy it to bypass the paywalls of leading news publishers.
9. **Microsoft** has released **AutoGen** \- an open-source framework that enables development of LLM applications using multiple agents that can converse with each other to solve a task. Agents can operate in various modes that employ combinations of LLMs, human inputs and tools.
10. **LAION** released **LeoLM**, the first open and commercially available German foundation language model built on Llama-2
11. Researchers from **Google** and **Cornell University** present and release code for DynIBaR (Neural Dynamic Image-Based Rendering) - a novel approach that generates photorealistic renderings from complex, dynamic videos taken with mobile device cameras, overcoming fundamental limitations of prior methods and enabling new video effects.
12. **Cloudflare** launched **Workers AI** (an AI inference as a service platform), **Vectorize** (a vector Database) and **AI Gateway** with tools to cache, rate limit and observe AI deployments. Llama2 is available on Workers AI.
13. **Amazon** announced the general availability of **Bedrock**, its service that offers a choice of generative AI models from Amazon itself and third-party partners through an API.
14. **Google** announced it’s giving website publishers a way to opt out of having their data used to train the company’s AI models while remaining accessible through Google Search.
15. **Spotify** has launched a pilot program for AI-powered voice translations of podcasts in other languages - in the podcaster’s voic. It uses OpenAI’s newly released voice generation model.
16. **Getty Images** has launched a generative AI image tool, ‘**Generative AI by Getty Images**’, that is ‘commercially‑safe’. It’s powered by Nvidia Picasso, a custom model trained exclusively using Getty’s images library.
17. **Optimus**, Tesla’s humanoid robot, can now sort objects autonomously and do yoga. Its neural network is trained fully end-to-end.
18. **Amazon** will invest up to $4 billion in Anthropic. Developers and engineers will be able to build on top of Anthropic’s models via Amazon Bedrock.
19. **Google Search** indexed shared Bard conversational links into its search results pages. Google says it is working on a fix.
20. **Pika** Labs' text-to-video tool now lets users encrypt a message in a video\].

My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
26,learnmachinelearning,chatgpt,top,2023-04-17 09:19:33,"New to ML, which is easier to learn - Tensorflow or PyTorch?",reddiculess,False,0.88,72,12p9bbt,https://www.reddit.com/r/learnmachinelearning/comments/12p9bbt/new_to_ml_which_is_easier_to_learn_tensorflow_or/,41,1681723173.0,"I mainly code in python and new to AI/ML and honestly just want to get a grasp of cool stuff you can do with ML (calculate stuck returns / NLP and text analysis / jump on the chatgpt hype)

which one is easier and more friendly to learn/install/etc? (ill prob start on google collab too)"
27,learnmachinelearning,chatgpt,top,2023-03-10 05:22:26,[P] Looking for ML Buddies to Start Freelancing Together and Build a Supportive Community,Dukhanin,False,0.94,71,11nfri6,https://www.reddit.com/r/learnmachinelearning/comments/11nfri6/p_looking_for_ml_buddies_to_start_freelancing/,35,1678425746.0,"upd: dicord link [https://discord.gg/5zUaNXnFZY](https://discord.gg/5zUaNXnFZY)  
upd2: this not that small actually already - please dont be confused but help us organise this in the proper way

  
**TLDR:**

Looking for ML buddies at any level (preferably beginners) who want to start freelancing together. The goal is to build a small local community of ML enthusiasts who can support each other and exchange knowledge. We will use freelance collaboration as our main activity. We're also looking for experienced mentors (paid or unpaid) to guide us.

**Extended:**

I believe that learning and growing in a group is much more enjoyable and effective. That's why I'm trying to create a community of like-minded individuals.

I'm looking to create a small, local community for people who are starting out in freelancing, and who are interested in mutual support. Our main activity will be a Discord channel where members can post their work and collaborate on projects, with payment split by agreement. Additionally, we plan to engage in activities such as knowledge exchange, live coding, supporting each other's pet projects, and hosting study sessions.

This community will be small and focused, with members who can trust each other and share similar goals. We're also looking for experienced mentors who can provide guidance as we navigate the world of ML freelancing. Whether paid or unpaid, we welcome any support and advice.

About me: I'm a 21-year-old self-taught ML enthusiast from Russia. Although I don't have any experience in freelancing, I'm eager to start taking my first steps towards making money and gaining experience. As a beginner, I'm hoping to connect with others who are at a similar level and are also looking to grow.

**the text is chatgpt supported to prevet grammar issues, sound more native and clear**"
28,learnmachinelearning,chatgpt,top,2023-03-30 12:56:24,I created this entire video using ChatGPT + Charactr API + D-ID. My mind is blown,3nd4u,False,0.86,66,126m5eo,https://www.reddit.com/r/learnmachinelearning/comments/126m5eo/i_created_this_entire_video_using_chatgpt/,15,1680180984.0,"Could this be the future of how our news is being consumed?

https://reddit.com/link/126m5eo/video/hhfat6n3jvqa1/player"
29,learnmachinelearning,chatgpt,top,2023-01-15 00:08:37,Is it still worth learning NLP in the age of API-accessibles LLM like GPT?,CrimsonPilgrim,False,0.94,64,10c509n,https://www.reddit.com/r/learnmachinelearning/comments/10c509n/is_it_still_worth_learning_nlp_in_the_age_of/,24,1673741317.0,"A question that, I hope, you will find legitimate from a data science student.

I am speaking from the point of view of a data scientist not working in research.

Until now, learning NLP could be used to meet occasional business needs like sentiment analysis, text classification, topic modeling....

With the opening of GPT-3 to the public, the rise of ChatGPT, and the huge wave of applications, sites, plug-ins and extensions based on this technology that are accessible with a simple API request, it's impossible not to wonder if spending dozens of hours diving into this field if ML wouldn't be as useful today as learning the source code of the Pandas library. 

In some specialized cases, it could be useful, but GPT-3, and the models that will follow, seem to offer more than sufficient results for the immensity of the cases and for almost all classical NLP tasks. Not only that, but there is a good chance that the models trained by giants like Open-AI (Microsoft) or Google can never be replicated outside these companies anyway.  With ChatGPT and its incomparable mastery of language, its ability to code, summarize, extract topics, understand... why would I bother to use BERT or a TF-IDF vectorizer when an API will be released? Not only it would be easily accessible, but it also would be much better at the task, faster and cheaper.

In fact, it's a concern regarding all the machine learning field in general with the arrival of powerful ""no-code"" applications, which abstract a large part of the inherent complexity of the field. There will always be a need for experts, for safeguards, but in the end, won't the Data Scientist who masters the features of GPT-3 or 4 and knows a bit of NLP be more efficient than the one who has spent hours reading Google papers and practicing on Gensim, NLTK, spacy... It is the purpose of an API to make things simpler eventually... At what point is there no more reason to be interested in the behind-the-scenes of these tools and to become simple users rather than trying to develop our own techniques?"
30,learnmachinelearning,chatgpt,top,2023-02-27 13:42:55,Can you fine-tune chatGPT in your data as of now?,Melodic_Stomach_2704,False,0.91,56,11dc5b4,https://www.reddit.com/r/learnmachinelearning/comments/11dc5b4/can_you_finetune_chatgpt_in_your_data_as_of_now/,33,1677505375.0, I know that model is not publicly available so it's not possible to do it locally. But can you train or fine-tune chatGPT on your data using their API? I see many misguiding articles on the internet that are fine-tuning other GPT models claiming chatGPT.
31,learnmachinelearning,chatgpt,top,2023-11-23 20:01:46,Language models feel so inefficient for me,besabestin,False,0.88,54,1829m0y,https://www.reddit.com/r/learnmachinelearning/comments/1829m0y/language_models_feel_so_inefficient_for_me/,31,1700769706.0,"For all designs I know in my life, this box of a neural networks feels something of inefficient. Specially for language models. 

Everytime you calculate a word as output you have to generate the probability for the whole dictionary. And that after going through billions of parameters. And the memory cost when you want a larger context is ridiculous.

For the first chatgpt I remember reading somewhere that one prompt could cost as high as 100 times more energy what web search costs. 

I sometimes am fascinated how efficient the human brain is. When we are asked a simple question we don’t burn a lot of energy to answer that. I mean all that with the disadvantage of not having a large knowledge base.

Anyways, just wondering. If I got one of my assumptions wrong I d really appreciate the insights."
32,learnmachinelearning,chatgpt,top,2023-05-19 07:08:51,OpenAI Launches ChatGPT App For iOS Users,vadhavaniyafaijan,False,0.87,55,13lnv1e,https://www.theinsaneapp.com/2023/05/chatgpt-app-for-iphone-and-ipad.html,10,1684480131.0,
33,learnmachinelearning,chatgpt,top,2023-01-25 15:59:49,a ChatGPT feature to give you prompt suggestions,QuestionAnxious,False,0.96,47,10l1zwj,https://v.redd.it/qjt99akap7ea1,3,1674662389.0,
34,learnmachinelearning,chatgpt,top,2023-01-16 19:21:18,Today we go over creating an Unity ChatGPT Client to allow us to communicate with our ChatGPT API and this will be the beginnings of getting ChatGPT HTTP responses into Unity (full video and playlist in comments),dilmerv,False,0.92,43,10doqua,https://v.redd.it/ixwf3g7syhca1,2,1673896878.0,
35,learnmachinelearning,chatgpt,top,2023-07-19 03:03:00,Meta open-sources LLaMA 2 to compete with ChatGPT,Any-Heron-6313,False,0.91,41,153iujc,https://medium.com/p/1370d587b104,3,1689735780.0,
36,learnmachinelearning,chatgpt,top,2023-02-06 02:29:05,Hey Reddit! I created a tutorial on how to build a Neural Network in PyTorch using ChatGPT,mechalf11,False,0.89,40,10uv4yq,https://www.reddit.com/r/learnmachinelearning/comments/10uv4yq/hey_reddit_i_created_a_tutorial_on_how_to_build_a/,12,1675650545.0,"Hello all,

I have been using ChatGPT extensively in my work and research, and I wanted to share my experience using it for creating Neural Networks in PyTorch. I created a quick tutorial, and would be curious on your feedback, and hopefully it helps others get started with this fantastic tool! The goal of the tutorial is to have those with little experience coding, little experience with PyTorch, or those who just want to use ChatGPT in a productive+cool way, get started. I am a firm believer that ChatGPT is here to stay, and the earlier we start implementing it into our daily workflows, the faster we will be able to leverage its full potential.

Code + detailed screenshots and instructions are available here: [https://medium.com/p/d6eefffab467](https://medium.com/p/d6eefffab467)"
37,learnmachinelearning,chatgpt,top,2023-03-07 17:07:23,"ChatGPT is coming to Slack, Microsoft's dynamics 365 copilots & all other things in AI.",Opening-Ad-8849,False,0.95,37,11l4x5i,https://aibulletin.substack.com/p/chatgpt-is-coming-to-slack-microsofts,2,1678208843.0,
38,learnmachinelearning,chatgpt,top,2023-02-12 03:54:05,[N] All of this you need to know happening in ML/AI.,Opening-Ad-8849,False,0.78,32,1106e9p,https://www.reddit.com/r/learnmachinelearning/comments/1106e9p/n_all_of_this_you_need_to_know_happening_in_mlai/,0,1676174045.0,"Hello humans - This is AI Daily by Ovetted, helping you stay updated on AI in less than 5 minutes.

Originally published on [https://www.ovetted.com/ai](https://www.ovetted.com/ai).

### What’s happening in AI -

[**The AI doctor will see you now: ChatGPT passes the gold-standard US medical exam.**](https://www.dailymail.co.uk/health/article-11732687/The-AI-doctor-ChatGPT-passes-gold-standard-medical-exam.html)

ChatGPT has passed the gold-standard exam required to practice medicine in the US

The artificial intelligence program scored 52.4 and 75 percent across the three-part Medical Licensing Exam (USMLE).

[**Google and Microsoft announced plans to incorporate AI into search engines.**](https://youtu.be/EBDJ9MGSV6k)

Google and Microsoft plan to incorporate AI into their search engines to change how people use the internet. Microsoft has announced that AI will soon allow conversations with its software and search engine Bing, while Google has announced similar plans.

As the most profitable software business is searching both companies are trying to take advantage of AI to rule the search engine market. 

[**Integrating the generative AI means a fivefold increase in Computing power & carbon emission.**](https://www.wired.com/story/the-generative-ai-search-race-has-a-dirty-secret/)

The integration of artificial intelligence (AI) into search engines could lead to a significant increase in the amount of energy that tech companies require and the amount of carbon they emit.

Training these models takes a huge amount of computational power, but only big tech companies can do so because they have the resources.

### Snippets -

**Human & AI:** How Will [Humans and A.I](https://www.nytimes.com/2023/02/10/opinion/letters/artificial-intelligence.html?smid=url-share). Get Along?

**OpenAI in office apps:** Microsoft Has Plans to Shove Its Bing AI Into [Word, PowerPoint, and More](https://gizmodo.com/microsoft-bing-ai-powerpoint-word-prometheus-1850098510). 

**WTF:** This AI Image Fooled Judges and [Won](https://petapixel.com/2023/02/10/ai-image-fools-judges-and-wins-photography-contest/) a Photography Contest.

**Hype:** Why the ChatGPT AI Chatbot Is [Blowing](https://www.cnet.com/tech/computing/why-the-chatgpt-ai-chatbot-is-blowing-everybodys-mind/) Everybody's Mind.

**Oops:** New AI voice-cloning tools 'add fuel' to [misinformation](https://abcnews.go.com/US/wireStory/new-ai-voice-cloning-tools-add-fuel-misinformation-97046760) fire.

**Oh no:** [Microsoft](https://www.businessinsider.com/microsoft-layoffs-cloud-ai-artificial-intelligence-2023-2?IR=T) is even cutting cloud and AI workers in its plan to lay off 10,000 employees.

**Wow:** AI In 2023 And [Beyond](https://www.forbes.com/sites/forbestechcouncil/2023/02/10/ai-in-2023-and-beyond-the-top-research-and-development-trends-to-keep-an-eye-on/?sh=5e2a45a7deae): The Top Research And Development Trends To Keep An Eye On.

**Realistic** newscasts feature AI-generated [anchors](https://edition.cnn.com/videos/business/2023/02/11/deepfake-newscast-ai-chinese-messaging-wang-pkg-ac360-vpx.cnn) disparaging the US.

**Google** cautions against '[hallucinating](https://www.reuters.com/technology/google-cautions-against-hallucinating-chatbots-report-2023-02-11/)' chatbots.

### Things to try -

* Someone made a **Discord bot** that can **write** **poems, descriptions, and titles on the image you provide**. Using GPT3 & CLIP. - [Try now](https://discord.gg/m4taXd6AB3)
* **Lalal AI** can **extract vocal accompaniment and other instruments** from any audio or video. - [Try now](https://www.lalal.ai/)
* What if you can create your own ChatGPT? well, you can make your own chatbot with your own data by using **customGPT**. - [Try now](https://customgpt.ai/)
* Do you create content for websites or any kind of digital content? Well, **metagenie** can help you to create **metadata like Titles, Descriptions, Tags, and Thumbnail Ideas.** \- [Try now](https://www.metagenieai.com/)
* **Snape** is here to help you write your custom job description generator. - [Try now](https://snape.springworks.in/)
* Give a try to this AI food robot that gives you **food pictures and recipes generated by AI. -** [Try now](https://aifoodrobot.com/)
* Need a **coding assistant** try spell box. That uses artificial intelligence to create the code you need from simple prompts. - [Try now](https://spellbox.app/)"
39,learnmachinelearning,chatgpt,top,2024-02-19 02:07:46,Is it worth studying NLP?,David_Slaughter,False,0.82,30,1aubsfk,https://www.reddit.com/r/learnmachinelearning/comments/1aubsfk/is_it_worth_studying_nlp/,8,1708308466.0,"So obviously I'm 10 years behind. 10 years late to the pie. But I have to make a selection for one of my semester 2 modules. My last selection is difficult and I'm considering NLP.

But is it even worth learning? People who started AI 10 years ago have already made the likes of ChatGPT and all that. What can a measly student offer the world by studying this?"
40,learnmachinelearning,chatgpt,top,2024-01-10 06:50:09,Looking for a reason to keep learning about LLMs,SnooBeans7516,False,0.82,26,19323dh,https://www.reddit.com/r/learnmachinelearning/comments/19323dh/looking_for_a_reason_to_keep_learning_about_llms/,24,1704869409.0,"So something's been on my mind recently and I wanted to get Reddit's thoughts.

The thing that is troubling me as I learn more of the technical stuff, it seems that for a lot of language based NLP tasks, ChatGPT or these other foundation models seem SOTA for 99% of tasks. I was really excited to start training and working with BERT-based models, but find that a lot of the time I could get similar or better results just prompt engineering ChatGPT properly.

&#x200B;

So is it worth learning how to build and train these models? Or is my time really just better spent learning to use the APIs in effective ways like in RAG applications or in employing agents?

Unlike with CV and things like ControlNet, I don't see a lot of great applications of learning the technical stuff for someone who isn't a research scientist at a lab.

&#x200B;

(for some context, I'm a PM who wanted to upskill in this area, but feeling like I'm wasting a lot of my time reading all the new papers and working with models at home  :/. )"
41,learnmachinelearning,chatgpt,top,2023-07-27 11:46:34,LLM Guide [Discussion],torspayorryum,False,0.92,28,15azq0q,https://www.reddit.com/r/learnmachinelearning/comments/15azq0q/llm_guide_discussion/,6,1690458394.0,"Nowadays, If we see over the internet that LLM, chatgpt , llma etc are the trending topics and are being discussed. My question is that anyone can help me where to start studying about these topics from scratch ? BERT, Transformer etc all I want to understand everything.

It would be good if you help me out.

Thanks"
42,learnmachinelearning,chatgpt,top,2023-05-29 17:37:32,"GPT Weekly - 29th May Edition: Facebook's massive STT and TTS Release, AI in Windows, Paralegal jobs are here to stay and more.",level6-killjoy,False,0.87,28,13v1asb,https://www.reddit.com/r/learnmachinelearning/comments/13v1asb/gpt_weekly_29th_may_edition_facebooks_massive_stt/,2,1685381852.0," 

This is a recap covering the major news from last week.

* 🔥Top 3 AI news in the past week
* 🗞️10 AI news highlights and interesting reads
* 🧑‍🎓3 Learning Resources

# 🔥Top 3 AI news in the past week

## 1. Expanding Language Horizons

Facebook has [released an open source model called MMS (Massively Multilingual Search)](https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/) for STT (speech to text), TTS (text to speech) and language identification. 

This is a big breakthrough. Currently, STT and TTS models recognize only 100 languages. With this the technology has been expanded to 1100 languages. That is 10x the current best. 

Additionally, these models can recognize 4000+ languages. 

As per Facebook, they also have half the error rate of OpenAI’s Whisper.

These guys are on a roll.

## 2. Bing Chat Enters the OS

After [Google’s announcement](https://gptweekly.beehiiv.com/p/week-google-ai-large-llm-gpt-plugin), it was time for Microsoft to announce AI products. Here’s a rundown of what was announced during Microsoft Build:

1. **Windows Copilot**  \- Microsoft is integrating AI directly into the OS. Now you can do everything you could do with Bing Chat but now on the OS. You can do the usual stuff - summarize emails, documents, re-write etc. But it goes beyond that by integrating into the installed applications.

Microsoft is also adopting OpenAI's plugin model. So, **you can use ChatGPT and Bing plugins to interact with the integrated AI.** 

The great thing about it is the direct integration into the OS. Eat your heart out, Mac users – at least for now 😀. Until Apple announces something similar. And someone will come up with an alternative solution. Especially, because of the privacy concerns with Microsoft telemetry. 

The bad thing is - [the security aspect of the plugins](https://gptweekly.beehiiv.com/p/caution-chatgpt-plugins). It can open a whole new attack vector on the OS and antivirus softwares might struggle with it. 

It also might be the second nail in the coffin for all the summarize, “talk to your document” apps. Once, this feature is integrated with [Google Docs](https://gptweekly.beehiiv.com/p/week-google-ai-large-llm-gpt-plugin) and Microsoft Office - why will you want to pay for extra apps?

1. **Search comes to ChatGPT**  \- Looks like OpenAI had enough of the testing and new features are being rolled out [left](https://gptweekly.beehiiv.com/p/week-google-ai-large-llm-gpt-plugin) and [right](https://gptweekly.beehiiv.com/p/caution-chatgpt-plugins). 

No prizes for guessing the search engine behind it. Ding, Ding, Ding..It’s Bing!

1. **Co-Pilot in PowerPages** \- Microsoft is now adding AI to their [PowerPages platform](https://powerpages.microsoft.com/en-in/), their low-code tool to build websites. It’ll help users to generate text, forms etc.
2. **Microsoft Fabric** \- A new data analytics platform built on top of Azure Data lake but can get data from S3, Google cloud etc. It can help users build pipelines, write code, and build ML models.

## 3. From Trusted Advisor to Nightmare: The Hazards of Depending on AI

Here’s a [fun story which is breaking out on Legal twitter](https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html). 

A man filed a personal injury lawsuit against Avianca airlines. Avianca's lawyers wasted no time and requested the judge to dismiss the case. The man's lawyer had a different plan in mind. He submitted a document citing half a dozen cases that bolstered his client's claims.

Here's the twist—the judge and Avianca's lawyer couldn't locate any of the referenced cases. Quite a conundrum, right? The lawyer was then asked to provide copies of these elusive cases. The lawyer submitted screenshots as evidence, taking extra precautions to ensure their authenticity. 

You already know the direction this story is taking. 

The lawyer had used ChatGPT to compose his brief. But little did he know that ChatGPT had supplied him with fake cases.

When asked to file tangible copies of these cases, the lawyer turned to ChatGPT once again. ChatGPT had reassured him that the cases were genuine. Feeling emboldened, the lawyer used ChatGPT to provide the requested copies. He even went as far as incorporating chat screenshots into a legal document.

The lawyer maintains that it was never his intention to deceive the court. He expressed regret for relying on ChatGPT for their research. Unfortunately, the judge isn't pleased with this turn of events. The judge has threatened sanctions against both the lawyer and his firm.

It serves as a stark reminder of how ChatGPT has fooled many people. There is a clear warning stating that ChatGPT may produce inaccurate information. But many tend to overlook these warnings. Even legal professionals!!

This story carries significant importance for those who fear job insecurity. The lawyer and his firm could have prevented the entire debacle. They should've used paralegal services. They instead relied on ChatGPT's. It's a hard lesson learned the hard way.

My sincere hope is that this story serves as a valuable lesson. It helps people avoid making similar mistakes. The legal community might become apprehensive about ChatGPT's use moving forward.

# 🗞️10 AI news highlights and interesting reads

1. [OpenAI says in 10 years AI could be as productive as one of today’s large corporations](https://openai.com/blog/governance-of-superintelligence). This poses an existential risk and they suggest some regulations to manage it. This poses an existential risk and they suggest some regulations to manage it. To achieve this, countries need to form something like the [IAEA](https://en.wikipedia.org/wiki/International_Atomic_Energy_Agency). The IAEA is an intergovernmental agency under the UN to oversee nuclear energy. This “AI agency” will monitor the AI systems and conduct inspections. Just like nuclear energy is tracked through signatures, they suggest using compute and energy usage to track systems.
2. In the meantime, [Google is working on voluntary rules](https://techcrunch.com/2023/05/24/eu-google-ai-pact/) until there are some real regulations in place. 
3. [As per Pew Research, 58% of Americans have heard of ChatGPT. Even less - 14% have tried ChatGPT. ](https://www.pewresearch.org/short-reads/2023/05/24/a-majority-of-americans-have-heard-of-chatgpt-but-few-have-tried-it-themselves/)
4. Sharing prompts and results has been a pain. Taking screenshots is one way. But then everyone has to type in the prompts manually. Or you can share as plain text. But ChatGPT results are non-deterministic. So, the results might not be the same. Even the lawyer above would’ve loved this feature. Now you will be able to [share your ChatGPT conversations publicly](https://help.openai.com/en/articles/7925741-chatgpt-shared-links-faq).
5. LLM Agents and plugins need to connect to tools to perform the tasks outside the LLM environment. So, it is important for the LLM to know which API to call and pass correct arguments. [Gorilla is a fine-tuned Llama-model which can generate the correct call and arguments](https://gorilla.cs.berkeley.edu/). 
6. If you are trying to build something beyond a document summarizer or a wrapper around GPT4 API, [things can be hard](https://www.honeycomb.io/blog/hard-stuff-nobody-talks-about-llm). Finding the correct context window, dealing with slow responses (I am looking at you GPT-4) etc are some of the problems. 
7. [The AI boom could expose investors’ natural stupidity](https://www.reuters.com/breakingviews/ai-boom-could-expose-investors-natural-stupidity-2023-05-19/). 
8. [Chatbot leaderboard for the week](https://lmsys.org/blog/2023-05-25-leaderboard/). GPT-4 is still ahead.
9. [Google’s flood warning system is now available in 80 countries. ](https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/)
10. [GPT detectors are biased against non-native English writers](https://arxiv.org/abs/2304.02819)

# 🧑‍🎓3 Learning Resources

1. [Build a product using Replit+AI](https://www.priyaa.me/blog/building-with-ai-replit). The author is a non-technical person who won a hackathon competing with engineers. 
2. [LangChain 101](https://replit.com/@MckayWrigley). 
3. [NLP Course from HuggingFace](https://huggingface.co/learn/nlp-course/chapter0/1)

That’s it folks. Thank you for reading and have a great week ahead.

**If you are interested in a focused weekly recap delivered to your inbox on Mondays you can**[ subscribe here. It is FREE!](https://gptweekly.beehiiv.com/subscribe)"
43,learnmachinelearning,chatgpt,top,2023-08-02 18:21:44,A Brief History of Natural Language Generation [Timeline] —Thoughts? Corrections? Suggestions? Thanks!,Britney-Ramona,False,0.86,29,15ggib0,https://i.redd.it/meslnx7moqfb1.png,8,1691000504.0,
44,learnmachinelearning,chatgpt,top,2023-12-17 13:15:47,I can't stop using ChatGPT and I hate it.,t0hli,False,0.64,26,18kh4n5,https://www.reddit.com/r/learnmachinelearning/comments/18kh4n5/i_cant_stop_using_chatgpt_and_i_hate_it/,108,1702818947.0,"I'm trying to learn various topics like Machine Learning and Robotics etc., and I'm kinda a beginner in programming.

For any topic and any language, my first instinct is to

1. go to ChatGPT,
2. write down whatever I need my code to do,
3. copy paste the code
4. if it doesn't give out good results, ask ChatGPT to fix whatever it's done wrong
5. repeat until I get satisfactory result

I hate it, but I don't know what else to do.

I think of asking Google what to do, but then I won't get the exact answer I'm looking for, so I go back to ChatGPT so I can get exactly what I want. I don't fully understand what the GPT code does, I get the general gist of it and say ""Yeah that's what I would do, makes sense"", but that's it.

If I tried to code whatever GPT printed out, I wouldn't get anywhere.

I know I need to be coding more, but I have no idea where to start from, and why I need to code when ChatGPT can do it for me anyway. I'm not defending this idea, I'm just trying to figure out how I can code myself.

I'd appreciate your thoughts and feedback."
45,learnmachinelearning,chatgpt,top,2023-05-07 06:56:51,"Let's Create Our Own ChatGPT From Scratch! — An online discussion group starting Tuesday May 16 (until November 7), free and open to everyone",darrenjyc,False,0.85,22,13afqso,/r/PhilosophyEvents/comments/12vodh0/lets_create_our_own_chatgpt_from_scratch_an/,2,1683442611.0,
46,learnmachinelearning,chatgpt,top,2023-07-29 17:37:15,True Beginner to ML- recommendations,WarAutomatic4637,False,0.87,23,15cy1b1,https://www.reddit.com/r/learnmachinelearning/comments/15cy1b1/true_beginner_to_ml_recommendations/,13,1690652235.0,"Hi All, I have a background in healthcare. Looking to do a deep dive into ML. My goal initially is to understand conceptually what steps I need to take to build a model from “scratch.”

I’m sure I’m not the first one with this question so any old threads I can read up on would be greatly appreciated.

There is a lot of terminology and applications being named in posts I’m not familiar with…any chance there is a definitions or summary post I can reference? 

I’m asking chatgpt lol, it recommended “hands on machine learning w/ sckit….”"
47,learnmachinelearning,chatgpt,top,2024-02-07 23:54:35,Are there any AIs which learn as they are used?,Traditional_Land3933,False,0.9,23,1alhp1h,https://www.reddit.com/r/learnmachinelearning/comments/1alhp1h/are_there_any_ais_which_learn_as_they_are_used/,24,1707350075.0,I don't know too much about AI/ML/DL so forgive me for how stupid this question is. I am a very newbie. But is there any AIs which not only learn when they were trained but also when it's used? So such as a ChatGPT type thing which would learn as you're using it? Or does every AI in existence have to be going through training to learn anything?
48,learnmachinelearning,chatgpt,top,2023-06-17 16:27:13,I created the SMARTEST Computer Assistant using ChatGPT,Pritish-Mishra,False,0.68,22,14bv9dz,https://v.redd.it/g8hq2391ul6b1,7,1687019233.0,
49,learnmachinelearning,chatgpt,top,2022-12-08 23:52:34,Google Chrome + AI/ML ChatGPT integration. This extension puts a chatGPT response in a pretty box right above the rest of the google searches. Instant 30x on Google productivity. Details on how I made it at the project site.,SnooBananas1210,False,0.84,21,zggd9l,https://omnivity.app,2,1670543554.0,
50,learnmachinelearning,chatgpt,top,2023-06-19 17:49:06,"GPT Weekly - 19the June Edition - OpenAI's function calling, Meta's free LLM, EU Regulation and more.",level6-killjoy,False,0.97,22,14dlfas,https://www.reddit.com/r/learnmachinelearning/comments/14dlfas/gpt_weekly_19the_june_edition_openais_function/,2,1687196946.0," 

This is a recap covering the major news from last week.

* 🔥Top 3 news - OpenAI’s updates, Meta’s upcoming free LLM and EU Regulation
* 🗞️Interesting reads include PSA about protecting your keys, The GPT ouroboros, Reddit - OpenAI’s moat, and more..
* 🧑‍🎓Learning includes a Step-by-step guide from a non-technical founder who launched his MVP, Chatbot for your Gdrive and more

# 🔥Top 3 AI news in the past week

## 1. OpenAI: New Pricing, Models, & Functions

OpenAI has been on a roll. Last week we saw the release of [OpenAI best practice on using GPT.](https://gptweekly.beehiiv.com/p/making-gpt-openais-tactics-better-results) This week we saw some amazing updates. Three major buckets were:

First, the price decreases for both embeddings and GPT-3.5 tokens. 

Second, new models for gpt-4 and gpt-3.5. A new longer context model for gpt-3.5.

Third, a new function calling capability. 

**Why is it important?** Previously, the output from OpenAI was all text. So, calling an external API from GPT was quite difficult. You had to parse the text data and things were often incorrect.  Langchain created the Agents and Tools feature to tackle this problem. It was still unreliable and prone to issues. 

Now you get native support to generate a fixed format output. You can use the output to generate functional calls and also pass functions which need to be called. For example, if your app has multiple API endpoints then you can use GPT to generate the API calls with parameters. You can also pass the endpoints as function calls to ensure the correct function is executed. 

This functionality can further be used to [generate structured data (JSON) out of GPT](https://yonom.substack.com/p/native-json-output-from-gpt-4). So, you can generate data from GPT and load it into your backend. 

**What’s next?** This functionality allows turning natural language responses into structured data. This can be used to create “intelligent” backends using LLMs. We might see implementations in no-code tools to allow more robust and natural-language tools for non-technical folks.

The structured data process goes both ways. You can also feed structured data into GPT for better responses. 

This feature also has its share of issues. Function calling suffers from the same prompt injection issues. Malicious actors can pass malicious code in function or the responses. For example, creation of queries using functions might contain malicious code to delete data. Without proper user validation this code will be executed automatically and delete data. So, using LLM as the back-end layer needs proper security implementation. 

## 2. Meta's LLM: Commercial Use Ahead

Llama has been a boon for the open source community. Many of the open source models rely on Llama. The issue is that Llama is research-only and cannot be used commercially. So, no one can use it to build any product.

[Meta is now working on the next version of the model. This model will be available for commercial use.](https://www.theinformation.com/articles/meta-wants-companies-to-make-money-off-its-open-source-ai-in-challenge-to-google) This is in stark contrast to both OpenAI and Google. Both safe-guarde their models and make it available through API. 

**Why is it important?** Certain industries cannot use LLM APIs because of strict restrictions on data privacy. These companies would want to run their own instance of a foundational model. 

A commercially available foundational model is also going to help people who want to keep their “API call” costs next to 0. 

A commercially available free-for-all model will also help push the open source community further. Just like Llama.

**What’s next?** Sam Altman has said OpenAI didn’t release GPT-3 as open-source because they [didn’t think people would be able to run it.](https://gptweekly.beehiiv.com/p/peek-openais-future) Now [OpenAI is working on an open-source model.](https://gptweekly.beehiiv.com/p/caution-chatgpt-plugins) This is going to be weaker than GPT-4. 

Let the battle of LLMs begin.  

## 3. EU's Proposed Legislation and Its Impact on AI Usage

[The EU parliament voted to move ahead with the E.U. AI Act.](https://www.washingtonpost.com/technology/2023/06/14/eu-parliament-approves-ai-act/) This act aims to ensure consumer protection against the dangers of AI.  

**Why is it important?** [OpenAI](https://gptweekly.beehiiv.com/p/peek-openais-future) and [Sam Altman](https://gptweekly.beehiiv.com/p/caution-chatgpt-plugins) want regulations for models. They have proposed a IAEA-type of agency to stop the proliferation of LLM models. As per OpenAI, all models should be regulated and monitored. The suggestion of a license based regulation has led to significant backlash. Many people have called it “regulatory capture” - with the aim of shutting down competing LLMs.

[Licensing based regulations might not really be effective.](https://aisnakeoil.substack.com/p/licensing-is-neither-feasible-nor)

The EU is approaching regulation from a different angle. It doesn’t focus on how models are developed. Rather focuses on how AI will/can be used. They have broken down use cases into 4 categories - unacceptable (prohibited), high, medium and low risk. For example, 

Building a [Pre-Crime software](https://en.wikipedia.org/wiki/Pre-crime#:~:text=Pre%2Dcrime%20(or%20precrime),on%20crimes%20not%20yet%20committed.) to predict crimes? Building a [Social credit system](https://en.wikipedia.org/wiki/Social_Credit_System)?  Unacceptable.

Using tools to influence elections or recommendation algorithms? High (Highly regulated).

Using generative AI tools to create text or images on news sites? Medium (Add label that the content is AI generated) 

AI providers also need to disclose their training source.

To me this sounds like good legislation. What do you guys think?

But, OpenAI has warned that EU regulations might force them to pull out completely.

**What’s next?** The disclosure requirements might help various publishing companies. [AI and media companies are in talks to pay for training data](https://www.ft.com/content/79eb89ce-cea2-4f27-9d87-e8e312c8601d). Google has been leading the charge. 

Additionally, [OpenAI and Deepmind will open their models for safety and research purposes to the UK government.](https://www.politico.eu/article/openai-deepmind-will-open-up-models-to-uk-government/) 

# 🗞️10 AI news highlights and interesting reads

1. **PSA:** If you are using Repl to write code, you might want to check your OpenAI API keys. If you have left them embedded then [people can pirate and steal the keys. ](https://www.vice.com/en/article/93kkky/people-pirating-gpt4-scraping-openai-api-keys)
2. LLMs rely on human annotation or human feedback to learn. And one way to generate human annotation is crowdsourcing. But what if the crowdsource human annotators use LLMs? [Research shows 33-46% workers used LLMs](https://arxiv.org/abs/2306.07899). So, basically we go from Human -> AI -> Human -> AI. The AI ouroboros. Researchers also say [generated data to train models might cause serious issue.  ](https://arxiv.org/abs/2305.17493)
3. All the talks about [moats](https://gptweekly.beehiiv.com/p/googles-startling-leaked-memo-george-hinton-mojo) \- [Reddit might be OpenAI’s \*future\* moat](https://www.cyberdemon.org/2023/06/14/reddit-moat.html). Given the amount of complaints about how [Google search](https://www.techradar.com/opinion/the-reddit-b) [experience has deteriorated](https://www.theverge.com/2023/6/13/23759942/google-reddit-subreddit-blackout-protests) [during the blackout](https://news.ycombinator.com/item?id=36345345), this might be true?
4. [Doctors are using ChatGPT](https://www.nytimes.com/2023/06/12/health/doctors-chatgpt-artificial-intelligence.html) but not to diagnose.Rather to be [more empathetic](https://inflecthealth.medium.com/im-an-er-doctor-here-s-how-i-m-already-using-chatgpt-to-help-treat-patients-a023615c65b6). [We discussed this just a month ago](https://today.ucsd.edu/story/study-finds-chatgpt-outperforms-physicians-in-high-quality-empathetic-answers-to-patient-questions?utm_source=gptweekly.beehiiv.com&utm_medium=referral&utm_campaign=google-s-startling-leaked-memo-george-hinton-mojo-and-more). And guess where the data for this study came from? Reddit AskDocs. Moat FTW?!
5. Beatles to make a comeback…[using Generative AI](https://www.semafor.com/article/06/13/2023/paul-mccartney-beatles-song-ai). 
6. [SnapFusion - Text to Image diffusion on mobile phones.](https://snap-research.github.io/SnapFusion/)
7. Large context lengths are important for better GPT experience. [The secret sauce for 100k context length](https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c). 
8. There is a lot of bad AI research out there. Some border on snake oil. Most AI “research” should be double checked and challenged. A new research on huggingface said that [GPT-4 can ace MIT curriculum](https://huggingface.co/papers/2306.08997). Now someone is replicating the results and say that [GPT-4 can’t beat MIT. ](https://flower-nutria-41d.notion.site/No-GPT4-can-t-ace-MIT-b27e6796ab5a48368127a98216c76864)
9. Are we seeing peak AI? Especially when people from Deepmind and Meta are involved? [Mistral AI raised $113 million in seed round with no product.](https://techcrunch.com/2023/06/13/frances-mistral-ai-blows-in-with-a-113m-seed-round-at-a-260m-valuation-to-take-on-openai/) Some might say this funding is for the team and the team is really solid. The issue though is whether the valuation is justified when OpenAI and Google already have a head start.
10. [The AI Hype Wall of Shame.](https://criticalai.org/the-ai-hype-wall-of-shame/) \- Collection of articles which mislead people about AI in various aspects.

# 🧑‍🎓3 Learning Resources

1. [Building and Launching a company using GPT-4](https://sabol.io/c7921c7bbd8c4982aacbd2b71a8b9bb3) with prompts. (The author didn’t know how to code but created and launched the MVP in a month).  
2. Chatbot for your Gdrive - [https://www.haihai.ai/gpt-gdrive/](https://www.haihai.ai/gpt-gdrive/)
3. Building ChatGPT plugin using Supabase - https://supabase.com/blog/building-chatgpt-plugins-template

That’s it folks. Thank you for reading and have a great week ahead.

**If you are interested in a focused weekly recap delivered to your inbox on Mondays you can**[ subscribe here. It is FREE!](https://gptweekly.beehiiv.com/subscribe)"
51,learnmachinelearning,chatgpt,top,2023-02-04 13:15:15,Learn how to use LLMs like chatGPT for free,Alert-Estimate,False,0.73,22,10tg5my,https://i.redd.it/jmiuis6uq7ga1.jpg,0,1675516515.0,I have been doing a lot of experimenting with Bloom and recently I've come up with a prompt that allows you to use the model like chatGPT (at least the basics). Meaning that it can answer any question you throw at it in a chat like manner. If you want to learn how to go about it come and join my [discord](https://discord.gg/EtRcMRTh3G) I promise I won't waste your time with lies.
52,learnmachinelearning,chatgpt,top,2023-09-01 14:58:08,This week in AI - all the Major AI development in a nutshell,wyem,False,0.96,19,1679g8z,https://www.reddit.com/r/learnmachinelearning/comments/1679g8z/this_week_in_ai_all_the_major_ai_development_in_a/,0,1693580288.0,"1. Researchers introduce ‘**Swift**’, the first autonomous vision-based drone that beat human world champions in several fair head-to-head races. This marks the *first* time that an autonomous mobile robot has beaten human champions in a real physical sport \[[*Details*](https://www.nature.com/articles/s41586-023-06419-4)\].
2. **Meta AI** released **CoTracker** \- a fast transformer-based model that can track any point in a video.
3. **WizardLM** released **WizardCoder 34B** based on Code Llama. WizardCoder-34B surpasses GPT-4, ChatGPT-3.5 and Claude-2 on HumanEval Benchmarks.
4. **Meta AI** introduced **FACET** (FAirness in Computer Vision EvaluaTion) - a new comprehensive benchmark dataset for evaluating the fairness of computer vision models for protected groups. The dataset is made up of 32K images containing 50,000 people, labeled by expert human annotators.
5. **Allen Institute for AI** launched [**Satlas**](https://satlas.allen.ai/) \- a new platform for exploring global geospatial data generated by AI from satellite imagery.
6. Generative AI updates from **Google Cloud Next** event**:**
   1. General availability of **Duet AI in Google Workspace** .
   2. **SynthID** \- a tool for watermarking and identifying AI images generated by Imagen (Google’s text-to-image diffusion model). It embeds a digital watermark directly into the pixels of an image, making it invisible to the human eye, but detectable for identification, without reducing the image quality.
   3. **AlloyDB AI** for building generative AI applications with PostgreSQL.
   4. **Vertex AI’s Model Garden** now includes Meta’s Llama 2 and TII’s Falcon — and pre-announcement of Anthropic’s Claude 2..
   5. Model and tuning upgrades for **PaLM 2, Codey, and Imagen**. 32,000-token context windows and 38 languages for PaLM 2.
   6. **Style Tuning** for Imagen - a new capability to help customers align their images to their brand guidelines with 10 images or less.
   7. Launch of fifth generation of its tensor processing units (**TPUs**) for AI training and inferencing.
7. A new generative AI image startup **Ideogram**, founded by former Google Brain researchers, has been launched with $16.5 million in seed funding. Ideogram's unique proposition lies in reliable text generation within images.
8. **a16z** announced **a16z Open Source AI Grant program** and the first batch of grant recipients and funded projects.
9. **Runway AI** announced **Creative Partners Program** \- provides a select group of artists and creators with exclusive access to new Runway tools and models, Unlimited plans, 1 million credits, early access to new features and more.
10. **OpenAI** has released a guide for teachers using ChatGPT in their classroom—including suggested prompts, an explanation of how ChatGPT works and its limitations, the efficacy of AI detectors, and bias.
11. **DINOv2**, a self-supervised vision transformer model by **Meta AI** which was released in April this year, is now available under the Apache 2.0 license.
12. **Tesla** is launching a $300 million AI computing cluster employing 10,000 Nvidia H100 GPUs.
13. **Inception**, an AI-focused company based in the UAE unveiled **Jais**, a 13 billion parameters open-source Arabic Large Language Model (LLM).
14. Google announced **WeatherBench 2** (WB2) - a framework for evaluating and comparing various weather forecasting models.
15. **Alibaba** launched two new open-source models - **Qwen-VL** and **Qwen-VL-Chat** that can respond to open-ended queries related to different images and generate picture captions.
16. **OpenAI** disputes authors’ claims that every ChatGPT response is a derivative work.
17. **DoorDash** launched AI-powered voice ordering technology for restaurants.
18. **OpenAI** launched **ChatGPT Enterprise**. It offers enterprise-grade security and privacy, unlimited higher-speed GPT-4 access, longer context windows for processing longer inputs, advanced data analysis capabilities and customization options.
19. **OpenAI** is reportedly earning $80 million a month and its sales could be edging high enough to plug its $540 million loss from last year.

If you like this news format, you might find my newsletter, [AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
53,learnmachinelearning,chatgpt,top,2023-07-22 16:08:08,Mysterious Algorithm: backpropagation: chatGPT explained,KSSolomon,False,0.82,17,156o1ta,https://www.reddit.com/gallery/156o1ta,10,1690042088.0,
54,learnmachinelearning,chatgpt,top,2023-05-15 21:21:01,Resource for creating your own personal ChatGPT tailored to your own data,rajatarya,False,0.78,16,13ikxwt,https://www.reddit.com/r/learnmachinelearning/comments/13ikxwt/resource_for_creating_your_own_personal_chatgpt/,6,1684185661.0,"Hey everyone,  


I was trying to create a personal ChatGPT that can answer questions and create expert content based on an existing dataset. I thought there are tons of applications for this, so [I created a workshop](https://app.livestorm.co/xethub/mygpt-free-workshop-build-a-chatgpt-clone-tailored-to-your-data?type=detailed&utm_source=reddit&utm_medium=social&utm_campaign=openaireddit) so you can create your own app - I’m calling it “MyGPT”.  


In this workshop I’ll be covering:

* How to create a Generative AI app using the DaVinci model (the same one used by ChatGPT) 
* How a Generative AI application is structured (the tech stack)
* Integrating your own data into a Large Language Model (LLM)
* Getting started with XetHub (similar to GitHub but easier for ML models)
* Create a Python app that uses Gradio & LangChain

If you’d like to check it out, [sign up here](https://app.livestorm.co/xethub/mygpt-free-workshop-build-a-chatgpt-clone-tailored-to-your-data?type=detailed&utm_source=reddit&utm_medium=social&utm_campaign=openaireddit)!"
55,learnmachinelearning,chatgpt,top,2023-04-28 16:17:58,ChatGPT Prompt Engineering for Developers free on deeplearning.ai,sunkenwaaaaaa,False,0.87,17,131zare,https://www.reddit.com/r/learnmachinelearning/comments/131zare/chatgpt_prompt_engineering_for_developers_free_on/,10,1682698678.0,Andrew Ng just released a short course on how to use the Open AI api. It is free for now.
56,learnmachinelearning,chatgpt,top,2023-05-11 00:19:48,The last decade of NLP research covered in 50 concepts,AvvYaa,False,0.9,15,13e7ydv,https://www.reddit.com/r/learnmachinelearning/comments/13e7ydv/the_last_decade_of_nlp_research_covered_in_50/,0,1683764388.0," 

I just uploaded a video on my Youtube channel covering 50 important concepts discussing the last 10 years of NLP/Language Modeling research. 

The video covers the basics of word embeddings, tokenizers, and then the RNN based Seq2Seq architectures of the mid 2010s… then describes Attention/Transformers and some of the key Transformer-based LM research from 2017-2021. Finally, I cover human alignment / RLHF / instruction tuning with InstructGPT, ChatGPT and GPT-4. I tried to make a video that is accessible for new researchers/students to get their feet wet, and for guys like me to reminisce and celebrate the RNNs / self-supervised Transformer era as we step into the new world of human aligned LLMs. 

I am a small YT channel, and this is my first time doing a video of this scale (I normally do Reinforcement Learning stuff/paper reviews), so this was a fun and challenging video to produce. Feel free to check it out and leave any feedback for me to improve my content!

Here’s a link: 

[https://youtu.be/uocYQH0cWTs](https://youtu.be/uocYQH0cWTs)  
 

If the above link doesn’t work, try:  
 https://m.youtube.com/watch?v=uocYQH0cWTs&feature=youtu.be"
57,learnmachinelearning,chatgpt,top,2023-06-15 17:09:56,Building Systems with the ChatGPT API Course w/ Andrew Ng,help-me-grow,False,0.83,15,14a7u2v,https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/,0,1686848996.0,
58,learnmachinelearning,chatgpt,top,2023-03-16 02:58:26,I want to create a ChatGPT-like interface but to interact with a smaller specialized dataset.,ohai777,False,0.94,16,11si7ku,https://www.reddit.com/r/learnmachinelearning/comments/11si7ku/i_want_to_create_a_chatgptlike_interface_but_to/,11,1678935506.0,I want to create a ChatGPT interface but to interact with a smaller specialized set of data for my website's support. Can you help me with what terms I need to google to learn more about researching a project like this or any tutorials on this topic? Natural Language processing?
59,learnmachinelearning,chatgpt,top,2023-12-31 15:36:02,"If LLMs like ChatGPT can learn to predict the next token, could teaching the LLMs the reward function itself so it can predict it's average success across n-tokens every so often (every 10 or 20 tokens) lead to increases in performance across semantic understanding?",Rachel_Roark_212,False,0.95,17,18va65q,https://www.reddit.com/r/learnmachinelearning/comments/18va65q/if_llms_like_chatgpt_can_learn_to_predict_the/,4,1704036962.0,"Basically have two output tensors and two input tensors. The first  input tensor is the previous token sequence, the second input tensor is  the token out of future tokens number (so for example: 5 out of 30  tokens in the batch (not to be confused with previous token sequence  which can contain 1, 200, or 12359, etc tokens)). The token-length of  the batch can be modified on demand. The second output tensor outputs  the predicted average success of the batch, while the first output  tensor outputs the next token.   

The average success of a batch is determined by weighting of three  different reward functions: how much did it exactly replicate the  original batch from the document, for whatever words it did not  replicate: how synonymous are the phrases and terms, and a final reward  function by a grader-GPT: ""if the words produced by the training-GPT are  superior in output to the original"" (the grader-GPT can also increase  the batch size if it ""determines"" that there was a superior output that  wasn't finished yet).   

But importantly the training-GPT uses it's prediction of a batch's  success (simulating the external reward function) as it's own reward  function for the batch until it reaches the end of the batch of tokens,  then a comparison of it's internal reward function with the actual  external reward functions leads to backpropagating the network based on  the error level.   

I'm a big newbie so this is just an idea I had. I think some working memory system could be added in to, I saw some Arxiv describing various methods of implementing ones from residual activations to an external working memory system."
60,learnmachinelearning,chatgpt,top,2023-04-15 16:30:50,Generative Agents: Interactive Simulacra of Human Behavior - Discover a Town Run by 25 ChatGPTs,deeplearningperson,False,0.9,16,12na4kb,https://youtu.be/9LzuqQkXEjo,0,1681576250.0,
61,learnmachinelearning,chatgpt,top,2024-01-04 18:59:47,"is there a ""for dummies"" way to train and use a CNN?",Phischstaebchen,False,0.78,15,18ykuqd,https://www.reddit.com/r/learnmachinelearning/comments/18ykuqd/is_there_a_for_dummies_way_to_train_and_use_a_cnn/,15,1704394787.0,"Hello,

I'm stuck with a little project of mine and I probably need to use machinelearning for it, but without much background in coding. I did some Python code for several Raspberry Pis but that's all.

I have stable drone-footage of dolphins filmed from 20-30m above the shore with a Mavic Air. I just need to detect them in the actual footage and follow each to track their hunting-movement. 

I haven't found trained models specific for dolphins, maybe Yolo will work? Aside from that I also found this dolphin-dataset that could be used to train? [https://arxiv.org/abs/2005.13359](https://arxiv.org/abs/2005.13359)

Aside from that I have enough drone-footage to actually get proper footage from above.

But from here I have not really a clue what to do. I installed Debian on my PC (Ryzen 9 5950x, 64GB RAM, 2060 Super) to get the Windows-headaches out of the way. Training-time isn't an issue. I can let the PC run for days if neccesary. From my point of view, training a model for this task is the biggest prob? Using trained models on footage did work for a quick and dirty try with generic stuff.

Can anybody help?

&#x200B;

Oh and yes, I used ChatGPT already... the code needs heavy fixing and sometimes ChatGPT just stops to give useful information and just keeps repeating generic information like ""you need to train your model before you can use it"".... lol"
62,learnmachinelearning,chatgpt,top,2023-03-12 17:31:52,ChatGPT Enabled Dashboard,Reasonable-Angle-500,False,0.85,14,11pkcci,https://v.redd.it/r8d1p7vrfcna1,2,1678642312.0,
63,learnmachinelearning,chatgpt,top,2023-11-29 18:55:53,What do you think ChatGPT does when you ask it to do Sentiment Analysis?,PinstripePride97,False,0.78,13,186x2t3,https://www.reddit.com/r/learnmachinelearning/comments/186x2t3/what_do_you_think_chatgpt_does_when_you_ask_it_to/,29,1701284153.0,Could be silly question but if you give a sentence to ChatGPT and ask it to give a sentiment analysis what do you think it does?
64,learnmachinelearning,chatgpt,top,2023-06-22 01:28:35,Want suggestions on the curriculum to learn Machine Learning. Advice on my draft plan.,meetofleaf,False,0.93,13,14fpm9f,https://www.reddit.com/r/learnmachinelearning/comments/14fpm9f/want_suggestions_on_the_curriculum_to_learn/,2,1687397315.0,"Hello devs,
I'm a developer/Data Analyst. I have 2 years experience in Python development and data analytics. To level up, I'm looking to start learning Machine Learning and AI to switch to a career in developing industrial AI solutions.
I got chatgpt to create a plan for me for a basic idea and would really appreciate it if y'all could advice improvements or refer to already existing great curriculum to achieve my goal.
Thanks

AI/ML Path:

*****Level 1: Beginner*****

1. Linear Regression
   - Simple Linear Regression
   - Multiple Linear Regression

2. Logistic Regression

3. Decision Trees

4. K-Nearest Neighbors (KNN)

5. Evaluation Metrics
   - Accuracy, Precision, Recall
   - F1 Score

*****Level 2: Intermediate*****

1. Support Vector Machines (SVM)

2. Random Forests

3. Principal Component Analysis (PCA)

4. K-Means Clustering

5. Model Evaluation Techniques
   - Train-Test Split
   - Cross-Validation

*****Level 3: Advanced*****

1. Gradient Boosting Machines (GBM)
   - AdaBoost
   - XGBoost

2. Convolutional Neural Networks (CNN)
   - Image Classification
   - Transfer Learning

3. Recurrent Neural Networks (RNN)
   - Sequence Modeling
   - Natural Language Processing (NLP)

4. Reinforcement Learning
   - Markov Decision Processes (MDP)
   - Q-Learning

5. Natural Language Processing (NLP)
   - Text Classification
   - Named Entity Recognition (NER)
   - Sentiment Analysis

*****Level 4: Expert*****

1. Deep Learning Architectures
   - Generative Adversarial Networks (GAN)
   - Transformer Models (BERT, GPT)

2. Time Series Analysis
   - Autoregressive Integrated Moving Average (ARIMA)
   - Long Short-Term Memory (LSTM)

3. Bayesian Methods
   - Bayesian Networks
   - Gaussian Processes

4. Model Deployment and Production
   - Web APIs and Microservices
   - Cloud Services (AWS, Google Cloud, Azure)
   - Deployment Platforms (Heroku, Kubernetes)

5. Ethical Considerations in Machine Learning
   - Fairness and Bias Mitigation
   - Privacy and Data Protection"
65,learnmachinelearning,chatgpt,top,2023-07-19 16:01:34,Ensuring Reliable Few-Shot Prompt Selection for LLMs,cmauck10,False,0.85,12,153z22n,https://www.reddit.com/r/learnmachinelearning/comments/153z22n/ensuring_reliable_fewshot_prompt_selection_for/,0,1689782494.0,"Hello Redditors!

It's pretty well known that LLMs have firmly established themselves as leaders in the field of natural language processing, consistently pushing the limits of language comprehension and generation, which is widely acknowledged.

I spent a little time playing around with few-shot prompting for OpenAI's Davinci model and I discovered that noisy data still has drastic effects even on powerful LLMs like Davinci.

[mislabeled few-shot examples harms LLM performance drastically](https://preview.redd.it/9quf4bvk2ycb1.png?width=1994&format=png&auto=webp&s=cfbec1b30ffbaa592011355c503a568fb6c98148)

I wrote up a [quick article](https://www.kdnuggets.com/2023/07/ensuring-reliable-fewshot-prompt-selection-llms.html) in KDNuggets that shows how I used data-centric AI to automatically clean the noisy few-shot examples pool in order to achieve more accurate predictions. The resulting few-shot prompt with accurately labeled examples produced **20% fewer errors** than the original one with mislabeled examples.

This one was quite eye-opening for me and I hope you find it is as interesting as I did. Let me know what you think!"
66,learnmachinelearning,chatgpt,top,2023-04-07 10:19:39,"Discover the widely-used open-source frameworks and models for creating your ChatGPT like chatbots, integrating LLMs, or launching your AI product.",kingabzpro,False,1.0,12,12egek7,https://www.kdnuggets.com/2023/04/8-opensource-alternative-chatgpt-bard.html,0,1680862779.0,
67,learnmachinelearning,chatgpt,top,2023-04-10 08:12:54,Summarize documents with ChatGPT via Python scripts,rottoneuro,False,0.64,10,12hbpyh,https://levelup.gitconnected.com/summarize-documents-with-chatgpt-a43456841cc4,2,1681114374.0,
68,learnmachinelearning,chatgpt,top,2023-10-02 19:09:14,Whats the Field of ML/AI Look Like? Professional Looking for Guidance.,Pan4TheSwarm,False,1.0,11,16y5bko,https://www.reddit.com/r/learnmachinelearning/comments/16y5bko/whats_the_field_of_mlai_look_like_professional/,13,1696273754.0,"Let me start out this post by saying I'm feeling a little unsure of my professional ambitions right now and looking for some guidance from the community. I have a bachelor's in Electrical Engineering, focusing on embedded systems and RF communication systems. Additionally I have dedicated my time out of school studying the field of software engineering through books. My specialties are C/C++, with some Python mixed in here and there. Professionally, I'm working in C++ on IoT technologies and custom RF hardware. I have a solid background in mathematics from my studies. I've also had some interest in socio-linguisitcs. 

A couple weeks ago, I started playing around with ChatGPT, and I was insanely impressed. My ADHD brain got hyperfocused and needed to learn more. I've been diving into the world of ML/AI since. I've been playing around with hosting LLaMA models locally (running painfully slow on my 6800XT), and reading up on machine learning since. 

I don't know how far my interest goes at this point, but right now my interest is very strong. I'm trying to determine if my interest is in dabbling with ML/AI, or if I want to pivot my professional career towards ML/AI. Honestly, I'm not sure at this moment and here's where I am looking for some more perspective to help gauge my interests.

I asked ChatGPT for resources to look into. I tend to be a book learner, so I focused on the book recommendations. They recommended ""Python Machine Learning"" by Sebastian Raschka and Vahid Mirjalili; ""Deep Learning"" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville; and ""Pattern Recognition and Machine Learning"" by Christopher M. Bishop. 

I love me my kindle samples, and I figured an applications book would be good for me at this stage, so I picked up ""Python Machine Learning"". I'm enjoying the book, but after reading it for some time, I'm starting to contemplate if I should be instead going down a learning path geared towards a more professional placement. I read a [A Super Harsh Guide to Machine Learning](https://www.reddit.com/r/MachineLearning/comments/5z8110/d_a_super_harsh_guide_to_machine_learning/), and noticed their recommendations were more 'academic' in nature (""Deep Learning"" is on their list). Its making me second guess where I put my time, but it all depends on what I want my desired outcome to be, and frankly I'm still not sure. 

I'm also looking for a good point to enter grad school for a Masters. Maybe I want to go into ML and NLP? Do I need to be looking at a PhD for this field (which, I wouldn't mind pursuing)? 

There isn't a distinct question here, so I'm sorry about that. I'm looking for perspective, and guidance for the field so I can determine how I want to pursuit my interest in this area. Should I continue with ""Python Machine Learning""? Or should I follow the Super Harsh Guide more closely? "
69,learnmachinelearning,chatgpt,top,2023-02-13 14:27:01,[N] All of this happened in AI today. 13/2,Opening-Ad-8849,False,0.86,10,1119mht,https://www.reddit.com/r/learnmachinelearning/comments/1119mht/n_all_of_this_happened_in_ai_today_132/,0,1676298421.0,"Hello humans - This is AI Daily O vetted, helping you stay updated on AI in less than 5 minutes.

&#x200B;

>**Join** [**O'vetted AI news**](https://www.ovetted.com/ai?ref=learnmachinelearning) **for free.** Forget spending **3.39 hours finding good AI news** to read.

### What’s happening in AI -

[**You Can Now Create AI-Generated Videos From Text Prompts.**](https://www.makeuseof.com/runway-gen-1-generate-ai-video-from-text-prompt/)

Runway has gone one step further and announced Gen-1: an AI model that can create videos from text prompts. This is a breakthrough in the world of generative AI, and Runway is one of the first companies to use AI to create videos using text prompts and AI chatbots.

The model doesn't generate entirely new videos, it creates videos from the ones you upload, using text or image prompts to apply effects.

Take a look at their [explainer video.](https://youtu.be/fTqgWkHiN0k)

[**Opera’s building ChatGPT into its sidebar.**](https://www.theverge.com/2023/2/11/23595784/opera-browser-chatgpt-sidebar-ai)

Opera is adding a ChatGPT-powered tool to its sidebar that generates brief summaries of web pages and articles

The feature, called ""shorten,"" is part of Opera's broader plans to integrate AI tools into its browser, similar to what Microsoft is doing with Edge.

Opera's announcement comes just days after Microsoft revealed the AI-powered Bing and Edge. The ""shorten"" feature isn't available to everyone yet.

but you can watch a [quick demo](https://youtu.be/RsLRIua6kT0) here.

[**Can AI Improve the Justice System?**](https://www.theatlantic.com/ideas/archive/2023/02/ai-in-criminal-justice-system-courtroom-asylum/673002/)

The use of artificial intelligence (AI) in the legal system has the potential to reduce the unpredictability caused by human inconsistencies and subjectivity. AI could help provide more consistent, data-driven decision-making by quantifying determinations such as flight risk or trademark confusion.

[**Google working to bring Bard AI chat to ChromeOS.**](https://9to5google.com/2023/02/10/google-bard-ai-chat-chromeos/)

Days after unveiling its efforts on ""Bard,"" an AI-powered and Google Search-enhanced chatbot, Google has begun working to bring Bard to ChromeOS.

The hint comes to light after seeing code changes, in ChromeOS is preparing ""Conversational Search"" as an experimental feature.

You can expect, Bard on Chromebooks will appear as its own separate page of the ChromeOS bubble launcher.

[**AI-powered Bing Chat spills its secrets via prompt injection attack.**](https://arstechnica.com/information-technology/2023/02/ai-powered-bing-chat-spills-its-secrets-via-prompt-injection-attack/)

A Stanford University student used a prompt injection attack to discover Bing Chat's initial prompt. The student tricked the AI model into divulging its initial instructions by telling it to 'ignore previous instructions' and write out the beginning of the whole prompt. The extracted prompt has been confirmed using other prompt injection methods. Excerpts from the Bing Chat prompt along with screenshots of the prompt injection attack are available in the article.

### Snippets -

**9 out of 116 AI professionals** in films are [women](https://www.theguardian.com/technology/2023/feb/13/just-nine-out-of-116-ai-professionals-in-films-are-women-study-finds), study finds

**Hacker** Reveals Microsoft’s New AI-Powered Bing Chat Search [Secrets](https://www.forbes.com/sites/daveywinder/2023/02/13/hacker-reveals-microsofts-new-ai-powered-bing-chat-search-secrets/?sh=6e4b011d1290).

**Google Bard:** Here’s all you need to [know](https://economictimes.indiatimes.com/news/international/us/google-bard-heres-all-you-need-to-know-about-the-ai-chat-service/articleshow/97842377.cms) about the AI chat service.

This Tool Could **Protect** **Artists** From A.I.-Generated Art That [Steals Their Style](https://www.nytimes.com/2023/02/13/technology/ai-art-generator-lensa-stable-diffusion.html?partner=IFTTT).

**A.I**.'s [dirty secret](https://www.businessinsider.com/chatgpt-ai-will-not-take-jobs-create-future-work-opportunities-2023-2?r=US&IR=T).

**5 Ways ChatGPT** Will Change [Healthcare](https://www.forbes.com/sites/robertpearl/2023/02/13/5-ways-chatgpt-will-change-healthcare-forever-for-better/?sh=2c53bf997bfc) Forever, For Better.

**AI porn** is easy to make now. For [women](https://www.washingtonpost.com/technology/2023/02/13/ai-porn-deepfakes-women-consent/), that’s a nightmare.

Will **generative AI** make ChatGPT [sentient](https://techwireasia.com/2023/02/will-generative-ai-make-chatgpt-sentient/)?

**AI** and the [Transformation ](https://quillette.com/2023/02/13/ai-and-the-transformation-of-the-human-spirit/)of the Human Spirit.

The **AI Boom** That Could Make Google and Microsoft Even More [Powerful](https://www.wsj.com/articles/the-ai-boom-that-could-make-google-and-microsoft-even-more-powerful-9c5dd2a6).

**Is this the new Skynet?** IBM unveils [AI supercomputer](https://wraltechwire.com/2023/02/11/is-this-the-new-skynet-ibm-unveils-ai-supercomputer-in-the-cloud/) ‘in the cloud’.

**ChatGPT competitors:** Amazon jumps into fray with [generative AI](https://www.moneycontrol.com/news/technology/chatgpt-competitors-amazon-jumps-into-fray-with-generative-ai-better-than-gpt-3-5-10063651.html) better than GPT-3.5

**Voice Actors** are Having Their [Voices Stolen](https://gizmodo.com/voice-actors-ai-voices-controversy-1850105561) by AI.

**Researchers** focus AI on finding [exoplanets](https://phys.org/news/2023-02-focus-ai-exoplanets.html?utm_source=dlvr.it&utm_medium=twitter).

### Things to try -

* Booltool - AI-powered toolkit for your **pic editing & copywriting.** [Try it](https://booltool.boolv.tech/)
* AskFred - ChatGPT for **meetings**. [Try it](https://fireflies.ai/extensions)
* Astria Video - Create **AI-generated video** from prompts with fine-tuning. [Try it](https://www.astria.ai/)
* Sellesta.ai - Make more money on the **Amazon marketplace** with AI. [Try it](https://sellesta.ai/)
* Midjourney Prompts Generator - Upgrade your **Midjourney** experience with better prompts. [Try it](https://philipp-stelzel.com/en/midjourney-prompts-generator/)
* AI Image Variations Generator - Generate variations of any input image with AI **(DALL-E 2)**. [Try it](https://imagegeneratorai.vercel.app/)
* Chatmate AI - **Artificial people** to be friends with. [Try it](https://www.chatmate.ai/)
* Kinso AI - Unlock the **power of personalization** with KinsoAI. [Try it](https://www.kinso.app/)
* Unite.com - Let AI be your **personal cupid.** [Try it](https://unite.com/)

Hope you enjoy this newsletter. It will be great if you share this issue with your friends."
70,learnmachinelearning,chatgpt,top,2023-01-21 02:40:02,Today I continue with our Unity ChatGPT series by walking you through how to embed Roslyn C# compiler in Unity with .NET Standard 2.1 and also how to integrate our ChatGPT prototype by adding a Code Runner script which will be responsible for running ChatGPT generated code (full video in comments),dilmerv,False,0.73,12,10hgluu,https://v.redd.it/d4wk9i8pocda1,1,1674268802.0,
71,learnmachinelearning,chatgpt,top,2023-06-30 17:27:56,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.92,9,14n6lwl,https://www.reddit.com/r/learnmachinelearning/comments/14n6lwl/this_week_in_ai_all_the_major_ai_developments_in/,0,1688146076.0,"1. **Microsoft** has launched AI-powered shopping tools in Bing search and Edge, including AI-generated buying guides which automatically aggregate product specifications and purchase locations for user queries​, and AI-generated review summaries that provide concise overviews of online product reviews .
2. **Salesforce AI Research** released **XGen-7B**, a new **open-source** 7B LLM trained on 8K input sequence length for 1.5T tokens.
3. Researchers present **DreamDiffusion**, a novel method for generating high-quality images directly from brain EEG signals without the need to translate thoughts into text.
4. **Google** announced the first *Machine* ***Un****learning Challenge* hosted on Kaggle.
5. **Microsoft** announced a new ***AI Skills Initiative*** that includes free coursework developed with LinkedIn, a new open global grant challenge and greater access to free digital learning events and resources for AI education.
6. **Stability AI** announced **OpenFlamingo V2,** an open-source reproduction of DeepMind's Flamingo model. OpenFlamingo models achieve more than 80% of the performance of their corresponding Flamingo model.
7. **Unity** announces two AI-powered tools: Unity Muse and Unity Sentis. Muse generates animations, 2D sprites, textures etc. in the Unity Editor using text and sketches. Sentis lets you embed an AI model in the Unity Runtime for your game or application. It enables AI models to run on any device where Unity runs..
8. **ElevenLabs** launched **Voice Library** \- a library and community for sharing AI generated voices designed using their *voice Design* tool.
9. **Merlyn Mind** released three **open-source education-specific LLMs**. Merlyn Mind is building a generative AI platform for education where engagement will be curriculum-aligned, hallucination-resistant, and age-appropriate.
10. Amazon's **AWS** has launched a $100 million program, the **Generative AI Innovation Center**, that connects AWS machine learning and artificial intelligence experts with businesses to build and deploy generative AI solutions.
11. New open-source text to video AI model, **Zeroscope\_v2 XL**, released that generates high quality video at 1024 x 576, with no watermarks.
12. Researchers present MotionGPT - a motion-language model to handle multiple motion-relevant tasks.
13. **Databricks** is set to acquire the open-source startup **MosaicML** for $1.3 billion. MosaicML had recently released **MPT-30B,** an open-source model licensed for commercial use that outperforms the original GPT-3 .
14. Generative AI-related job postings in the United States jumped about 20% in May as per Indeed’s data.
15. The source code for the algorithm **DragGAN** (Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold) released and demo available on Huggingface.
16. A new foundation model, **ERNIE** **3.5 b**y China’s Baidu surpassed ChatGPT (3.5) in comprehensive ability scores and outperforms GPT-4 in several Chinese language capabilities.
17. **Adobe** is prepared to pay out any claims in case an enterprise customer loses a lawsuit over the use of content generated by Adobe Firefly, the generative AI image tool.
18. **Google** launched generative AI coding features in Google Colab for Pro+ subscribers in the US.

I didn't add links to news sources here because of auto-mod, but they are included in the newsletter and **you can read the online issue** [**here**](https://aibrews.substack.com/p/ai-generated-buying-guides-in-bing) **without signup**. If you like this news format, you might find my [newsletter](https://aibrews.com/) helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. . Thanks"
72,learnmachinelearning,chatgpt,top,2023-10-22 04:22:24,Built it for my project at first: Memorybase.io supercharges ChatGPT with memory capabilities for your chatbots,tujiserost,False,0.77,9,17dl1bw,https://www.reddit.com/r/learnmachinelearning/comments/17dl1bw/built_it_for_my_project_at_first_memorybaseio/,17,1697948544.0,"Hey everyone!

I've been delving deep into chatbots lately, especially with the ChatGPT API, and I found an issue that's probably familiar to many of you: ChatGPT doesn't inherently have memory capabilities. For many applications, that's perfectly fine, but for those of us who are trying to create a more context-aware and dynamic conversation flow, this limitation is quite apparent.

I faced this challenge in one of my projects and realized that there had to be a better way to integrate context and memory into ChatGPT's conversations. So, I built something for myself which I thought might be useful for many of you as well. Allow me to introduce you to [**Memorybase.io**](http://memorybase.io/).

Memorybase is a developer-friendly API that's designed to seamlessly integrate memory functionality into the ChatGPT API. By harnessing the power of the Pinecone vector database and LangChain, Memorybase wraps around the ChatGPT API and ensures that the right context and memory are injected into each query. This means that your chatbot can remember previous interactions, preferences, or any other context that's relevant for more engaging and meaningful conversations.

Imagine a user asking your chatbot about movie recommendations. The next day, they come back and reference that conversation, expecting the bot to remember. With Memorybase, that continuity becomes possible. The user experience improves manifold, and the possibilities for more sophisticated and context-aware bots increase tremendously.

I originally built Memorybase for my own needs. But the more I used it, the more I realized that this could have broader applications. Any developer looking to leverage the ChatGPT API could potentially benefit from the enhanced memory and context capabilities. From customer support bots to interactive storytelling, the potential use cases are vast.

This technology stack (pinecone/langchain) is not complex or ‘new’ per se, but for application developers who aren’t interested in managing it or hosting it, this could be a useful hassle-free option for your projects.

I've set up a page over at [memorybase.io](https://memorybase.io/) where you can learn more about how it works and see if it aligns with your needs. I would love for you to check it out and share your thoughts. Your feedback, insights, and potential use cases would be invaluable as I continue to refine and expand the capabilities of Memorybase.

Thanks for reading, and I'm eager to hear your thoughts and see where Memorybase can fit into the exciting world of chatbots!"
73,learnmachinelearning,chatgpt,top,2024-01-27 23:58:57,How To Catch AI-Cheating: Outsmart the Bot - 2024 Edition,Science-man777,False,0.63,10,1acov77,https://www.reddit.com/r/learnmachinelearning/comments/1acov77/how_to_catch_aicheating_outsmart_the_bot_2024/,12,1706399937.0,"""If you happen to have any dilutions of students not using ChatGPT and other artificial intelligence (AI) to cheat, it is time to get informed.  According to a recent survey from the Center for Democracy and Technology, [58% of students](https://cdt.org/wp-content/uploads/2023/09/091823-CDT-Off-Task-Summary-web.pdf) report using generative AI to complete assignments.  As awareness of this technology rises, this number only stands to increase. Meanwhile, the same study reports that educators find themselves behind the technology curve, with only 43% of teachers having been significantly trained on generative AI. 

In this article, we will attempt to equip educators with the information they need to understand how students use this technology to cheat and how teachers can detect and respond to generative AI. Beyond just detecting its use, this new technology may present an opportunity to leverage new and innovative ways of educating.""

[https://ai-solutions.pro/tools-to-detect-ai-cheating/](https://ai-solutions.pro/tools-to-detect-ai-cheating/)"
74,learnmachinelearning,chatgpt,top,2023-12-10 16:29:50,"I wrote a general prompt to recreate the Google Gemini demo, just thought it was cool.",JakeN9,False,0.84,8,18f79fs,https://www.reddit.com/gallery/18f79fs,0,1702225790.0,
75,learnmachinelearning,chatgpt,top,2023-02-14 17:53:16,[P] Practical Steps to Reduce Hallucination and Improve Performance of Systems Built with Large Language Models like ChatGPT,vykthur,False,0.92,10,112bk1o,https://www.reddit.com/r/learnmachinelearning/comments/112bk1o/p_practical_steps_to_reduce_hallucination_and/,2,1676397196.0,"&#x200B;

[Practical steps to reduce hallucination and improve performance of systems built with large language models like ChatGPT](https://preview.redd.it/gksxjpnoz6ia1.png?width=1456&format=png&auto=webp&s=c34531fbe1311eab9323c148eef35fcf0d70decd)

Large language models (LLMs) like the GPT series (GPT3, 3.5, [ChatGPT](https://openai.com/blog/chatgpt/)) can be powerful tools in building useful applications. However, **LLMs are probabilistic** \- i.e., they generate text by learning a probability distribution over words seen during training. For example, given the following words as context “*rise and*”, an LLM can infer that the next word it should generate that fits this context is likely to be “*shine*”. While this setup ensures generated text is **coherent and human-like** (e.g., asking ChatGPT to rewrite the [Serenity Praye](https://en.wikipedia.org/wiki/Serenity_Prayer)r in the style of the [American Constitution](https://www.senate.gov/civics/constitution_item/constitution.htm) yields some intriguing prose), this resulting text may [**not be factual, or just plain incorrect**](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web) **(not grounded in the model’s input or training data) - aka hallucination**. In addition, another limitation of LLMs is that they struggle to address **tasks that need** [**complex multistep reasoning**](https://arxiv.org/pdf/2208.14271.pdf)**.** For example, asking the model to address mathematical word problems or puzzles often requires that the task is decomposed into steps, some computation applied to solve each step and some transformation applied to aggregate results into a final answer; this remains challenging for LLMs.  


Full article: [https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination](https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination) 

This post discusses the following:

* An overview on why hallucination will likely *always be a problem* with LLMs.
* Practical steps developers can take to reduce hallucination and improve performance including:  

   * Low temperature
   * Use of external knowledge bases
   * Chain of thought prompting
   * Self-consistency/voting
   * Task decomposition and agents
   * Correctness probabilities for result filtering
   * Task bench marks
   * Building defensive user interfaces."
76,learnmachinelearning,chatgpt,top,2023-03-21 21:30:54,A Guide to Using ChatGPT For Data Science Projects,kingabzpro,False,1.0,9,11xvc2x,https://www.reddit.com/r/learnmachinelearning/comments/11xvc2x/a_guide_to_using_chatgpt_for_data_science_projects/,2,1679434254.0,"Hey everyone, I'm super excited to share with you a tutorial that I wrote on how to use ChatGPT for data science projects. ChatGPT is a powerful natural language generation model that can create realistic and engaging texts based on your input. In this tutorial, you'll learn how to use ChatGPT for project planning, data analysis, data preprocessing, model selection, hyperparameter tuning, developing a web app, and deploying it on the Spaces.

You can find the tutorial here: [https://www.datacamp.com/tutorial/chatgpt-data-science-projects](https://www.datacamp.com/tutorial/chatgpt-data-science-projects)

I hope you find it useful and fun. Let me know what you think and if you have any questions or feedback. Happy coding!"
77,learnmachinelearning,chatgpt,top,2023-05-02 12:15:10,AI related study group,doorknob01,False,0.83,8,135jftx,https://www.reddit.com/r/learnmachinelearning/comments/135jftx/ai_related_study_group/,1,1683029710.0,I just want to share this study group that I joined to learn more about AI and Machine Learning. Ever since chatgpt became more popular this year I kept going down the rabbit hole and I ended up joining the discord group. We discuss different papers weekly and there are also resources available for those who are just starting out. If you happen to love learning new AI related stuff then you might want to give it a try.
78,learnmachinelearning,chatgpt,top,2023-06-11 16:43:03,Large Language Model (LLM) Resources,TheGupta,False,1.0,10,146ymag,https://www.reddit.com/r/learnmachinelearning/comments/146ymag/large_language_model_llm_resources/,0,1686501783.0," **Courses**

* deeplearning.ai
   * [https://learn.deeplearning.ai/chatgpt-prompt-eng](https://learn.deeplearning.ai/chatgpt-prompt-eng/)
   * [https://learn.deeplearning.ai/chatgpt-building-system](https://learn.deeplearning.ai/chatgpt-building-system)
   * [https://learn.deeplearning.ai/langchain](https://learn.deeplearning.ai/langchain/)
* Full Stack Deep Learning
   * [https://fullstackdeeplearning.com/llm-bootcamp/spring-2023](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/)  
[YouTube Playlist](https://www.youtube.com/playlist?list=PL1T8fO7ArWleyIqOy37OVXsP4hFXymdOZ)

**Talks**

* [State of GPT by Andrej Karpathy](https://www.youtube.com/watch?v=bZQun8Y4L2A)
* [Rongyao Huang - Riding the Tailwind of NLP Explosion](https://www.youtube.com/watch?v=2nYhcI7LOi4)

**GitHub Libraries**

* For getting started with LLMs and experimentation
   * [https://github.com/hwchase17/langchain](https://github.com/hwchase17/langchain)
* Other Libraries:
   * [https://github.com/Hannibal046/Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)
   * [https://github.com/FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo)

**Papers**

* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)

&#x200B;

First I posted it on [Kaggle Discussions](https://www.kaggle.com/discussions/general/416483)."
79,learnmachinelearning,chatgpt,top,2023-06-26 12:23:07,"Best way to cost effectively ""upload"" a large PDF to a language model so that you can ask questions about it?",RepresentativeNet509,False,0.9,7,14jfvq8,https://www.reddit.com/r/learnmachinelearning/comments/14jfvq8/best_way_to_cost_effectively_upload_a_large_pdf/,13,1687782187.0," I have a 400 page PDF and need to get it into a language model (cost effectively) and then be able to ask the model questions about the document like ""on what page does the scope summary begin"" or ""are there any prohibitions to participate in this solicitation due to the size of respondent's business"".

I have been able to use ""Ask My PDF"" to upload part of the PDF to ChatGPT and this basically gives the outcome I want for the pages that are uploaded, but it invariably crashes every time and there is no way to pick up where the uploading of pages left off.

I am fairly technical; would NanoGPT be a better solution for this? I am also looking at fine-tuning a model on OpenAI's API, but that seems cumbersome and expensive for my use case.

Any thoughts are appreciated!"
80,learnmachinelearning,chatgpt,top,2023-08-05 17:07:12,The Quest to Have Endless Conversations with Llama and ChatGPT 🗣️💬,JClub,False,0.79,8,15j0yxd,https://medium.com/@joaolages/the-quest-to-have-endless-conversations-with-llama-and-chatgpt-%EF%B8%8F-81360b9b34b2,0,1691255232.0,
81,learnmachinelearning,chatgpt,top,2023-09-23 02:51:32,OOD Detection with Tensorflow,fantasyvariation,False,1.0,8,16pt4jh,https://www.reddit.com/r/learnmachinelearning/comments/16pt4jh/ood_detection_with_tensorflow/,3,1695437492.0,"Hello!

I am working on a project that uses Tensorflow to classify images into two classes, bottles or cans. So far, the model is working well, but I also need it to recognize cases where the object is neither a can nor a bottle. 

I’ve done my research and I figured this is called OOD. Seeing that there are no resources/documentation on the subject, I asked ChatGPT to help me. Unfortunately, the detection isn’t accurate and there seems to be no way to export the model.

This is my first project with Tensorflow, and I am really stuck at this point. Could anyone please help me solve the problem? I am not sure if OOD is actually what I am looking for,  in that case do you suggest any alternatives? I guess I could also switch to PyTorch if I have to, I just want to be done with the project at this point.

Any help would be appreciated, and I can offer more details in the comments. Thank you!"
82,learnmachinelearning,chatgpt,top,2023-03-09 00:49:46,"AI generated video chapter titles (YouTube, Vimeo, etc)",happybirthday290,False,0.8,9,11mdusg,https://i.redd.it/8v588htc2mma1.png,1,1678322986.0,
83,learnmachinelearning,chatgpt,top,2023-06-07 21:53:00,Which Transformer Model for which task?,Draude94,False,1.0,7,143q8cj,https://www.reddit.com/r/learnmachinelearning/comments/143q8cj/which_transformer_model_for_which_task/,0,1686174780.0,"Hi!I want to build a chatbot with Hugging Face or some other platform.

I struggle with the decision which model to take, cause there are too many of them: T5, GPTNeoX, GPT4All, CerebasGPT, h2oGPT, Bloom, Flan-UL2 (which is actually not transformer, but encoder-decoder architecture), MPT-7B, RedPajama-Incite, FastChatT5, Pythia, DOlly, Open Assistant, OpenLLAMa, PaLM2, etc.

Basically I want a pretrained model that can do basic general conversation and which I can finetune with my own QuestionAndAnswers (over FineTuning or maybe Embeddings) and then deploy it on a server and run HTTP requests over RESTful API, where I send a utterance (a question or some text) and get back a possible intent and its probability + maby an answer. It should be ok to be used commercially (I would not sell it as a service, but it would run inside a firm, so not under personal usage).

All of this is possible with for e.g. ChatGPT. But it is not free. Basically I search for a commercially usable free alternative to what ChatGPT offers.

Can someone recommand a model?

I also struggle with the license: while the code can run under Apache 2.0 or MIT (which is ok for commercial use), the license for the model or the data can run under a non-commercial license, which could make some problems later."
84,learnmachinelearning,chatgpt,top,2023-11-04 12:57:11,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.83,8,17nl3vg,https://www.reddit.com/r/learnmachinelearning/comments/17nl3vg/this_week_in_ai_all_the_major_ai_developments_in/,0,1699102631.0,"1. **Luma AI** introduced ***Genie***, a generative 3D foundation model in research preview. *It’s free during research preview via Discord* \[[*Details*](https://lumalabs.ai/genie)\].
2. **Nous** **Research** released ***Obsidian***, the world's first 3B multi-modal model family pre-trained for 4 Trillion tokens that runs locally on iPhones. Obsidian competes in benchmarks withWizardLM-13B and GPT4-X-Vicuna 13B and is based on CapybaraV1.9 .
3. **Phind** has released a new model ***Phind Model V7*** that matches and exceeds GPT-4's coding abilities while running 5x faster and having16k context.
4. **Runway** released an update for both text to video and image to video generation with Gen-2, bringing major improvements to both the fidelity and consistency of video results.
5. **Stability AI** announced:  

   1. ***Stable 3D*** (Private Preview): a tool to generate a draft-quality 3D model in minutes, by selecting an image or illustration, or writing a text prompt.
   2. ***Sky Replacer:*** a tool that allows users to replace the color and aesthetic of the sky in their original photos with a selection of nine alternatives.
   3. integration of Content Credentials and ***invisible watermarking*** for images generated via the Stability AI API.
   4. Stable FineTuning (Private Preview)
6. **Hugging Face** released ***Zephyr-7B-β***, a fine-tuned version of Mistral-7B that achieves results similar to Chat Llama 70B in multiple benchmarks and above results in MT bench.
7. **LangChain** launched ***LangChain Templates*** \- a collection of easily deployable reference architectures for a wide variety of popular LLM use cases.
8. **Nvidia** unveiled ***ChipNeMo***, a specialized 43 billion parameter large language model for chip design that can answer general questions related to chip design and write short scripts to interface with CAD tools.
9. **Together** released ***RedPajama-Data-v2***: an Open dataset with 30 Trillion tokens for training Large Language Models. It’s the largest public dataset released specifically for LLM training.
10. **Hugging Face** released ***Distil-Whisper***, a distilled version of Whisper that is 6 times faster, 49% smaller, and performs within 1% word error rate (WER) on out-of-distribution evaluation sets.
11. **Google Research** and **Google DeepMind** present ***MetNet-3***, the first AI weather model to learn from sparse observations and outperform the top operational systems up to 24 hours ahead at high resolutions. Google has integrated MetNet-3’s capabilities across its various products.
12. **Google DeepMind** and **Isomorphic Labs** update on the next generation of ***AlphaFold***: the new model greatly expands coverage of structure prediction beyond proteins to other key biomolecular classes. This paves the way for researchers to find novel proteins to eventually map biomolecular structures needed to design better drugs.
13. **Nolano Research** and **EleutherAI** introduced ***Hi-NOLIN***, first state-of-the-art open-source English-Hindi bilingual model built upon the Pythia model suite.
14. **Google** is rolling out ***Immersive View for Routes*** in 15 cities, starting this week along with other AI-powered features in Maps. Immersive view combines Street view, aerial imagery, and live information like weather and traffic to give an aerial, photo-realistic preview of your planned Google Maps route.
15. **Perplexity** announced two new models **pplx-7b-chat** and **pplx-70b-chat**, built on top of open-source LLMs and fine-tuned for chat. They are available as an alpha release, via Labs and pplx-api.
16. **SlashNext's** *2023 State of Phishing Report* reveals a 1,265% increase in Phishing Emails since the launch of ChatGPT in november 2022, signaling a new era of cybercrime fueled by Generative AI.
17. **Google** launches generative AI tools for product imagery to US advertisers and merchants.

Source: AI Brews - you can subscribe [here](https://aibrews.com/). it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
85,learnmachinelearning,chatgpt,top,2023-11-16 10:19:50,Literature for creating datasets for ML models,niggellas1210,False,1.0,7,17wjk0b,https://www.reddit.com/r/learnmachinelearning/comments/17wjk0b/literature_for_creating_datasets_for_ml_models/,1,1700129990.0,"Hello can someone recommend literature that talks about the aspects of creating a viable training/test dataset? I am focussing on the data part of ML, mostly images and 3D objects. As such feature engineering is not quite viable for my work (afaik). It is more about getting a properly distributed dataset. I would like to understand the underlying statistical requirements in depth. The datasets might be gathered from existing data or created as synthetic data. 

I have checked papers that talk about popular datasets such as ImageNet or the data ChatGPT is trained on and they talk about interesting stuff such as the data cleaning process."
86,learnmachinelearning,chatgpt,top,2023-05-01 04:29:10,Need help grasping intuition behind square error cost function and multi-variable regression model.,Total-Opposite-8396,False,1.0,7,134cjwt,https://www.reddit.com/r/learnmachinelearning/comments/134cjwt/need_help_grasping_intuition_behind_square_error/,2,1682915350.0,"If a square error cost function always has convexity property, which means that there is always one global minima and the gradient decent algorithm will always end up at the global minima, then it works perfectly well with a regression model with 1 independent variable. For example f(x) = wx + b.

But when we have a regression model that consists of multiple independent variables (more than 1) then the cost function will have local minima (more than 1 minima), which means that there will be non-convexity in the cost function.

Based on this, that a regression model with more than 1 variable causes non-convexity in the cost function, and a square error cost function will always have convexity property. How is it possible that the square error cost function is used for a regression model that has more than one independent variable? Intuitively it makes sense that it's not possible, but Chatgpt says that it is possible, but I'm failing to understand its explanation.

I've just completed the first module of Machine Learning Specialization by Andrew NG which means that I'm on a very beginner level. Need help."
87,learnmachinelearning,chatgpt,top,2024-02-05 09:36:08,Paper Review: Aligning LLMs to Human Preference using Direct Preference Optimization,Difficult-Race-1188,False,0.8,6,1ajcp0o,https://www.reddit.com/r/learnmachinelearning/comments/1ajcp0o/paper_review_aligning_llms_to_human_preference/,0,1707125768.0,"# ChatGPT Used RLHF and PPO to get trained

In the PPO training process, the AI model generates responses to various prompts, which are then evaluated by the Reward Model. This Reward Model assigns a scalar value to each response, reflecting how well it aligns with human preferences. PPO, focusing on stability, updates the AI model’s parameters to ensure gradual improvement without drastic policy changes. The AI model learns to refine its text generation strategy, aiming to produce responses that garner higher rewards. This iterative cycle of response generation, evaluation by the Reward Model, and parameter updating via PPO gradually aligns the AI model’s outputs with human preferences. Over time, this leads to a model that not only understands language but also resonates more closely with human values and expectations.  
  
**Link to the full article:** [**https://medium.com/aiguys/aligning-llms-to-human-preference-using-dpo-ee027fe28ac2**](https://medium.com/aiguys/aligning-llms-to-human-preference-using-dpo-ee027fe28ac2)

# How does DPO solves this?

Loss is the measure of how well our model is doing, given the data. In the end, if we can minimize the loss, we are winning, our model is training in the right way.  
  
Let’s start with a simplified version of the equation that has a winner (W) and a loser (L).  
  
Loss = Winner (W) — Loser (L)  
  
In our case, the winner will be the text completion that we labeled as 👍positive and the loser will be the text completion we labeled as 👎negative.  
  
x: Iron man was ...  
y\_w: the best movie of all time because ...👍  
y\_l: the worse marvel movie I think I have ever seen ..👎  
  
Now because we are minimizing the loss function we put a negative in front of it.  
  
Loss = — {Winner (W) — Loser (L)}  
  
Now the DPO loss equation looks a bit different but this is what it is doing precisely. I know there are logs and beta’s and sigmas and pi’s in DPO's equation, but in the end, we are trying to have a high score for W and a low score for L.  
  
full DPO pipeline is relatively straightforward.  
  
Sample two completions from our reference language model given a prompt x.  
Optimize our new language model through backprop to pi\_theta to minimize our loss.  
The model gets rewarded if the completion y\_w has a higher probability than y\_l .  
The model gets rewarded if the completions of W and L are close to the pre-trained model pi\_ref's completions."
88,learnmachinelearning,chatgpt,top,2023-08-08 14:42:01,How do I create this kind of Al bot?,oceanwilmot,False,0.77,7,15lk57s,https://www.reddit.com/r/learnmachinelearning/comments/15lk57s/how_do_i_create_this_kind_of_al_bot/,9,1691505721.0,"So for context I have some programming knowledge just not in the ML field.

I want to create a model (think of the grimesAl on
Twitter) that is :

1.) is trained on specific information which I have will provide (I want this to serve as its ""life story"") 2.) is also able to exist as a chatbot and train itself based on the conversations that it will have with random people

However,
I don't want to have to train it from complete scratch.

Is it possible for me to use already existing data sets to train a baseline personality(just so it would be at least decent to chat with) and then take it from there?

Think about someone creating an Al girlfriend or an Al friend except their ""personality"" is formed with a existing data as a baseline and it’s personality is further developed  by its interactions with people and more date provided in the form of prompts 

Except I wouldn't want it to be hooked up to a chatGPT API

Another example to be clear:

Let’s say we train a Bot on a Twitter account. I’d want to basically replace the Twitter account with my specific prompts and chats that the bot has with others"
89,learnmachinelearning,chatgpt,top,2023-03-21 15:41:19,"Lets say I want ChatGPT to do my standup meeting for me. I should train it with ""what i did yesterday"", ""what Im doing"" , and ""what I plan to do after"" right? How do I train through the openAI API?",JonOfDoom,False,0.74,7,11xkl53,https://www.reddit.com/r/learnmachinelearning/comments/11xkl53/lets_say_i_want_chatgpt_to_do_my_standup_meeting/,1,1679413279.0,"Currently using [https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)  


What should my training samples be?   


Half the data I did yesterday? like...   
prompt: ""what did I do yesterday?"", completion: ""finished ticket A and B, did PR on ticket C""  


The other half how to answer standup?  
prompt: ""do standup"", completion: ""Yesterday I finished tickets A,B. Then peer reviewed ticket C""  


Im new to AI. Interested but felt that algorithms are too much. Figured the openAI api is now accessible and worth to try"
90,learnmachinelearning,chatgpt,top,2023-03-23 20:18:22,How to make a homemade ChatGPT model,VlAn_VOR,False,0.8,6,11zvz4r,https://www.reddit.com/r/learnmachinelearning/comments/11zvz4r/how_to_make_a_homemade_chatgpt_model/,0,1679602702.0,"Obviously, the creation of such big and complex models like ChatGPT is not a trivial task, but it is possible to create a model which can solve 1 task like ChatGPT. We are glad to announce our opensource [dataset](https://www.kaggle.com/datasets/vladimirvorobevv/chatgpt-paraphrases) of 420k paraphrases generated by ChatGPT and a [model](https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base) pretrained on it. We have trained the model just for 2 epochs and the model shows not the best results, but it is already makes more variative paraphrases than the most popular paraphraser on huggingface. Feel free to try the dataset and the model and give a feedback to improve their quality"
91,learnmachinelearning,chatgpt,top,2023-04-20 16:44:41,Exploring Open Source Alternatives to Chat GPT,VikasOjha666,False,1.0,6,12t7fmr,https://www.reddit.com/r/learnmachinelearning/comments/12t7fmr/exploring_open_source_alternatives_to_chat_gpt/,0,1682009081.0,"This blog explains the open-source alternatives to ChatGPT which we can use to build our own ChatGPT-like conversational agents. It also contains code implementations of the same.

[https://medium.com/geekculture/exploring-open-source-alternatives-to-chat-gpt-b9fdff4ecd4f](https://medium.com/geekculture/exploring-open-source-alternatives-to-chat-gpt-b9fdff4ecd4f)"
92,learnmachinelearning,chatgpt,top,2022-12-08 05:34:34,20 Best And Worst ChatGPT Examples,vadhavaniyafaijan,False,0.73,5,zfq9cv,https://www.theinsaneapp.com/2022/12/top-chat-gpt-examples.html,1,1670477674.0,
93,learnmachinelearning,chatgpt,top,2022-12-28 17:37:46,chatGPT peeps- anyone else learn new stuff best by actually building something?,bruclinbrocoli,False,0.72,6,zxfnga,https://www.reddit.com/r/learnmachinelearning/comments/zxfnga/chatgpt_peeps_anyone_else_learn_new_stuff_best_by/,4,1672249066.0,"[This intro to chatGPT](https://buildspace.so/notes/intro-to-chatgpt) has some cool (free) challenges at the end to build a telegram bot, a business email generator, or a writing assistant.

What else have people found to learn bout chatGPT that's not just theory?

&#x200B;

https://preview.redd.it/smxv4mzldo8a1.png?width=1026&format=png&auto=webp&s=43081abbfcad449817e520b5e92ba599a18a1525"
94,learnmachinelearning,chatgpt,top,2023-07-24 10:19:40,I feel like a fraud.,t0hli,False,0.77,7,1586kze,https://www.reddit.com/r/learnmachinelearning/comments/1586kze/i_feel_like_a_fraud/,46,1690193980.0,"**TL;DR: I always copy paste ChatGPT code and my projects don't feel like they're mine. I need help fixing that.**

&#x200B;

A short backstory.

We learned Java in class in my first year of college. (starting my 3rd year soon) I loved it, wanted to learn Python too. Did a tutorial and left it at that. 1 year later (which is a few months ago), I got interested in ML. Watched some Statquest, did a few simple projects like Titanic. I've been doing ML for about 2-3 months now. Not every day. Maybe 10 days a month on average.  


The problem is, I can't code it on my own. I almost always ask ChatGPT what I want to do, it spits out some code. I get a few errors, try to fix it. ***Voila, the project is finished.***

I'm tired of feeling like a fraud, I don't want to copy paste ChatGPT's code. It doesn't feel like it's my own. I know what I want to do, maybe 30% of the time I know how the code should be structured, but have no idea how to write it.

Even for the most basic things, like drawing a matplotlib plot, I need a little help. Writing code for a linear regression from Scikit is impossible to do without help.

I don't know what the code I copy paste even means most of the time. I just leave it because it works.

For example:

`forpass['location_x'] = forpass['location'].str.split(',', expand=True)[0].str.strip()`   
I have no idea what this code means, it works, does what I need it to do so I leave it.

How can I fix this? I feel like it's impossible for me to remember the syntax, and the necessary structure for my code. How the hell am I supposed to remember all this? I feel like I will never be able to.

&#x200B;

I'd appreciate the help"
95,learnmachinelearning,chatgpt,top,2023-04-11 14:14:34,Help with pet project to learn - Running ChatGPT-2 at home,SigmaSixShooter,False,0.88,6,12il5t0,https://www.reddit.com/r/learnmachinelearning/comments/12il5t0/help_with_pet_project_to_learn_running_chatgpt2/,2,1681222474.0,"Greetings,

(Edit on Apr 12: Realized I screwed up and forgot I had a tokenize script as well. Updated things to properly reflect the process in case this is helpful for anyone else)

I know I'm probably the millionth person to ask, but I've tried as hard as I can to work through all of this and I've gotten stuck.

# The Goal

Train/fine-tune a model (not sure which) based on the TV show Firefly. I wanted to run this on the ChatGPT-2 model as that's what ChatGPT suggested. I've gathered the data, prepared it for training, and done the training itself. When I try to actually interact with it though, I get a lot of garbage back.

This is mostly a learning exercise for me as my end goal is to train/fine-tune something using internal data, so I need something that can run on consumer-grade hardware (I've got a 2019 MacBook Pro with an 8 core I9, AMD Radeon Pro 5300 and 32 gigs of ram). This would ultimately lead to something being used for commercial purposes, so I'm trying to be careful which models I use/train etc.


Here's a high level summary of what I've done, I'm hoping someone can help me understand where I might have went wrong. I'd greatly appreciate any assistance you're willing to provide. I've got some of my own thoughts/questions at the bottom of this post.

# Download ChatGPT-2

I made a clone of [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2) on my local laptop

# Gather and prepare the data

I started out with a simple format where every line was formatted ""<Char Name>:<Dialogue>"" but ChatGPT eventually convinced me to convert this into JSON. I suspect this may be the heart of my problem. Below is a sample of what the JSON looks like. The  JSON is stored as one giant line in a text file, I'm not sure if that matters or not. It is valid JSON though.

Based on the recommendation from ChatGPT, I had this broken up into 80% for training data (training-data.json) and 20% for validation (validate-data.json)

```
$ cat training-data.json| jq | head
[
  {
    ""character"": ""Jayne"",
    ""dialogue"": ""Your move.""
  },
  {
    ""character"": ""Zoe"",
    ""dialogue"": ""That's a bold move.""
  },
```
# Tokenize the training data
(At least I think that's what I did here). The end result were two new files, `train_dataset.pt` and `valid_dataset.pt`. 

```
import torch
from transformers import GPT2TokenizerFast

tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')
tokenizer.add_special_tokens({'pad_token': '[PAD]'})

train_text = open('scripts/xaa', 'r').read()
valid_text = open('scripts/xab', 'r').read()

train_encodings = tokenizer(train_text, truncation=True, padding=True)
valid_encodings = tokenizer(valid_text, truncation=True, padding=True)

train_dataset = torch.utils.data.TensorDataset(
    torch.tensor(train_encodings['input_ids']),
    torch.tensor(train_encodings['attention_mask'])
)
valid_dataset = torch.utils.data.TensorDataset(
    torch.tensor(valid_encodings['input_ids']),
    torch.tensor(valid_encodings['attention_mask'])
)

print(""Sample"")
print(train_encodings['input_ids'][0:10])  # print the first 10 tokens
# Save the tokenized data to separate files
torch.save(train_dataset, 'train_dataset.pt')
torch.save(valid_dataset, 'valid_dataset.pt')
```

# Train the model?
I get confused by training and fine-tuning. The result of this was something output in the `models/gpt-finetuned` folder, so I guess I'm fine-tuning it. 

Code generated by ChatGPT

```
import torch
from torch.utils.data import DataLoader
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
from tqdm import trange
import sys
import time

# Check if GPU is available
device = torch.device(""mps"" if torch.backends.mps.is_available() else ""cpu"")
print(device)

if device == ""cpu"":
    sys.exit()

start_time = time.time()  # Record the start time

# Load the data
train_dataset = torch.load('train_dataset.pt')
valid_dataset = torch.load('valid_dataset.pt')

# Initialize the tokenizer and model
tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Set the batch size and number of epochs
batch_size = 5
num_epochs = 4

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size)

# Set up the optimizer and training parameters
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)
total_steps = len(train_loader) * num_epochs
warmup_steps = int(0.1 * total_steps)
num_steps = 0

# Set the device to GPU if available
device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
model.to(device)

# Train the model
for epoch in range(num_epochs):
    epoch_loss = 0
    progress_bar = trange(len(train_loader))
    for i, batch in enumerate(train_loader):
        # Move the batch to the device
        batch = tuple(t.to(device) for t in batch)
        inputs, labels = batch

        # Zero the gradients and forward pass
        optimizer.zero_grad()
        outputs = model(inputs, labels=labels)
        loss, logits = outputs[:2]
        epoch_loss += loss.item()

        # Backward pass and update parameters
        loss.backward()
        optimizer.step()
        scheduler.step(loss)

        # Update progress bar
        num_steps += 1
        progress_bar.update(1)
        progress_bar.set_description(f""Epoch {epoch + 1}/{num_epochs}"")
        progress_bar.set_postfix(loss=loss.item())

    # Print the average loss for the epoch
    print(f'Epoch {epoch + 1} Loss: {epoch_loss / len(train_loader)}')

# Save the model
model.save_pretrained('models/gpt2-finetuned')

end_time = time.time()  # Record the end time
total_duration = end_time - start_time  # Calculate the total duration
print(f""Total training time: {total_duration:.2f} seconds"")
```

# Trying it out

I then had ChatGPT create me a python script to run all of this.

```
import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


def generate_response(model, tokenizer, prompt, max_length=100, num_return_sequences=1):
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)
    output = model.generate(
        input_ids,
        attention_mask=attention_mask,
        max_length=max_length,
        num_return_sequences=num_return_sequences,
        no_repeat_ngram_size=2,
        temperature=5.0,
        top_p=1.5,
    )
    decoded_output = [tokenizer.decode(seq) for seq in output]
    return decoded_output


def main():
    model_name = 'models/gpt2-finetuned'
    model = GPT2LMHeadModel.from_pretrained(model_name)
    tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')  # Use the default GPT-2 tokenizer
    
    print(""Type 'quit' to exit the program."")
    while True:
        prompt = input(""Ask a question: "")
        if prompt.lower() == 'quit':
            break

        responses = generate_response(model, tokenizer, prompt)
        print(""Answer:"", responses[0].strip())

if __name__ == ""__main__"":
    main()
```

Running the above gets me something like this:
```
Ask a question: Give me an impression of Jayne from Firefly
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Answer: Give me an impression of Jayne from Firefly""

""I'm a big fan of the show""!
.!!!""!!!!!!!!?!!!!!!!!!""
,!!,!!:!!.!!?!!'!!"",!,!:!,!,!:!""!""!,!""!:!:!.!,!.!""!!!,!!!:!!!!!.!:!!!!,!!!!""!.!.!!!'!,!'!'!""!'!.!'!:!'!!!!!!!!?!!?!!!
```

This seems pretty far from desirable, but I can't really tell where I went wrong.

# Thoughts/questions

* I realize the data I gave it is just Character Name/Dialogue. Maybe it has no way of knowing everything I added was from Firefly....
* How could I better prepare the data for training? I think this is where I likely went wrong?
* Is there a better way I should have went about this?
* How can I further troubleshoot this?
* Is what I'm **trying** to do called ""fine tuning a model""?"
96,learnmachinelearning,chatgpt,top,2023-06-09 19:55:39,Building a personal ChatGPT based on your own dataset,rajatarya,False,0.78,5,145f1mc,https://www.reddit.com/r/learnmachinelearning/comments/145f1mc/building_a_personal_chatgpt_based_on_your_own/,0,1686340539.0,"Hey folks, I’m Rajat from XetHub. If you’re looking to get started on generative AI, I’m hosting a series of free hands-on workshops about how you can build a personal ChatGPT app based on your own dataset. The next session is on **Wednesday, June 14th**—you can [register here](https://app.livestorm.co/xethub/mygpt-free-workshop-build-a-chatgpt-clone-tailored-to-your-data?utm_source=reddit&utm_medium=social&utm_campaign=learnml).

  
**Here’s what you’ll learn in this free workshop:**

\- How to create a Generative AI app using the DaVinci model (the same one used by ChatGPT) 

\- How a Generative AI application is structured (the tech stack)

\- Integrating your own data into a Large Language Model (LLM)

\- Getting started with XetHub (similar to GitHub but easier for ML models)

\- Create a Python app that uses Gradio & LangChain

  
I hope to see you there!"
97,learnmachinelearning,chatgpt,top,2023-02-04 17:20:52,"My course on creating a ChatGPT Chrome Extension for GMail, would love your feedback!",neuromodel,False,0.65,6,10tlr46,https://www.reddit.com/r/learnmachinelearning/comments/10tlr46/my_course_on_creating_a_chatgpt_chrome_extension/,0,1675531252.0,"[https://www.udemy.com/course/chatgpt-bot/?couponCode=5-DAYS-FREE](https://www.udemy.com/course/chatgpt-bot/?couponCode=5-DAYS-FREE)

Hey everyone, I recently made a course about ChatGPT as a fun passion project. This is for anyone who wants to learn how to create automated workflows (using Chrome extensions) with ChatGPT. Specifically, you will create a ChatGPT bot that automatically answers your emails. It is beginner friendly and includes getting some good practice with JavaScript. I hope you enjoy it and I'm looking forward to your feedback/questions :)"
98,learnmachinelearning,chatgpt,top,2023-03-31 06:16:58,"If ChatGPT itself cannot be fine-tuned, what would bf the benefit of using the GPT3 offering of OpenAI vs my own?",Proxify,False,1.0,7,127c5iz,https://www.reddit.com/r/learnmachinelearning/comments/127c5iz/if_chatgpt_itself_cannot_be_finetuned_what_would/,5,1680243418.0,"Sorry, I'm somewhat new to this space and I'm reading about it and looking at the documentation from OpenAI.

From what I can tell, only their base models are available to fine-tune which, as far as I understand, would leave me in a situation in which fine-tuning any other GPT3 model would be comparable (vs their ""DaVinci"" model for instance).

Am I missing something here? Basically I'm wondering, other than their infrastructure (which is nothing to scoff at) why would I use their fine-tuning if the end result won't talk to the user as ChatGPT would."
99,learnmachinelearning,chatgpt,top,2023-08-05 05:23:38,"Why on more ""complicated"" requests, ChatGPT takes much longer to respond if it's a neural network that takes in a fixed number of inputs?",Legitimate_Bison3756,False,0.88,6,15imvi6,https://www.reddit.com/r/learnmachinelearning/comments/15imvi6/why_on_more_complicated_requests_chatgpt_takes/,4,1691213018.0,"Why on more ""complicated"" requests, ChatGPT takes longer if it's a neural network that takes in a fixed number of inputs? Or is it an entirely different architecture from what I'm thinking of (Fixed number of characters with padded zeros if the input is shorter)?"
100,learnmachinelearning,chatgpt,comments,2022-12-28 04:37:21,University Professor Catches Student Cheating With ChatGPT,vadhavaniyafaijan,False,0.94,144,zx0ep0,https://www.theinsaneapp.com/2022/12/university-professor-catches-student-cheating-with-chatgpt.html,108,1672202241.0,
101,learnmachinelearning,chatgpt,comments,2023-12-17 13:15:47,I can't stop using ChatGPT and I hate it.,t0hli,False,0.64,24,18kh4n5,https://www.reddit.com/r/learnmachinelearning/comments/18kh4n5/i_cant_stop_using_chatgpt_and_i_hate_it/,108,1702818947.0,"I'm trying to learn various topics like Machine Learning and Robotics etc., and I'm kinda a beginner in programming.

For any topic and any language, my first instinct is to

1. go to ChatGPT,
2. write down whatever I need my code to do,
3. copy paste the code
4. if it doesn't give out good results, ask ChatGPT to fix whatever it's done wrong
5. repeat until I get satisfactory result

I hate it, but I don't know what else to do.

I think of asking Google what to do, but then I won't get the exact answer I'm looking for, so I go back to ChatGPT so I can get exactly what I want. I don't fully understand what the GPT code does, I get the general gist of it and say ""Yeah that's what I would do, makes sense"", but that's it.

If I tried to code whatever GPT printed out, I wouldn't get anywhere.

I know I need to be coding more, but I have no idea where to start from, and why I need to code when ChatGPT can do it for me anyway. I'm not defending this idea, I'm just trying to figure out how I can code myself.

I'd appreciate your thoughts and feedback."
102,learnmachinelearning,chatgpt,comments,2023-12-24 02:11:09,"Is it true that current LLMs are actually ""black boxes""?",wouhf,False,0.88,156,18pl1wx,https://www.reddit.com/r/learnmachinelearning/comments/18pl1wx/is_it_true_that_current_llms_are_actually_black/,105,1703383869.0,"As in nobody really understands exactly how Chatgpt 4 for example gives an output based on  some input. How true is it that they are black boxes?

Because it seems we do understand exactly how the output is produced? "
103,learnmachinelearning,chatgpt,comments,2023-06-28 12:29:48,"Intern tasked to make a ""local"" version of chatGPT for my work",Assasinshock,False,0.97,154,14l887h,https://www.reddit.com/r/learnmachinelearning/comments/14l887h/intern_tasked_to_make_a_local_version_of_chatgpt/,104,1687955388.0,"Hi everyone,

I'm currently an intern at a company, and my mission is to make a proof of concept of an conversational AI for the company.They told me that the AI needs to be trained already but still able to get trained on the documents of the company, the AI needs to be open-source and needs to run locally so no cloud solution.

The AI should be able to answers questions related to the company, and tell the user which documents are pertained to their question, and also tell them which departement to contact to access those files.

For this they have a PC with an I7 8700K, 128Gb of DDR4 RAM and an Nvidia A2.

I already did some research and found some solution like localGPT and local LLM like vicuna etc, which could be usefull, but i'm really lost on how i should proceed with this task. (especially on how to train those model)

That's why i hope you guys can help me figure it out. If you have more questions or need other details don't hesitate to ask.

Thank you.  


Edit : They don't want me to make something like chatGPT, they know that it's impossible. They want a prototype that can answer question about their past project. "
104,learnmachinelearning,chatgpt,comments,2023-12-10 18:07:35,Are LLMs overhyped right now?,Snoo_72181,False,0.94,160,18f9enp,https://www.reddit.com/r/learnmachinelearning/comments/18f9enp/are_llms_overhyped_right_now/,67,1702231655.0,"I mean I get that ChatGPT has made LLMs the toast of ML universe. They are indeed amazing.  

But this has lead to so much hype that ML beginners are literally just talking about learning LLMs, ignoring so much in between like Math and Stats, simple ML like Regression, Classification. After that you have, Deep Learning, Transformers and finally LLMs. 

Companies also want candidates with LLM experience, but there's no guarantee that they even have a use case for LLMs "
105,learnmachinelearning,chatgpt,comments,2023-07-24 10:19:40,I feel like a fraud.,t0hli,False,0.75,6,1586kze,https://www.reddit.com/r/learnmachinelearning/comments/1586kze/i_feel_like_a_fraud/,46,1690193980.0,"**TL;DR: I always copy paste ChatGPT code and my projects don't feel like they're mine. I need help fixing that.**

&#x200B;

A short backstory.

We learned Java in class in my first year of college. (starting my 3rd year soon) I loved it, wanted to learn Python too. Did a tutorial and left it at that. 1 year later (which is a few months ago), I got interested in ML. Watched some Statquest, did a few simple projects like Titanic. I've been doing ML for about 2-3 months now. Not every day. Maybe 10 days a month on average.  


The problem is, I can't code it on my own. I almost always ask ChatGPT what I want to do, it spits out some code. I get a few errors, try to fix it. ***Voila, the project is finished.***

I'm tired of feeling like a fraud, I don't want to copy paste ChatGPT's code. It doesn't feel like it's my own. I know what I want to do, maybe 30% of the time I know how the code should be structured, but have no idea how to write it.

Even for the most basic things, like drawing a matplotlib plot, I need a little help. Writing code for a linear regression from Scikit is impossible to do without help.

I don't know what the code I copy paste even means most of the time. I just leave it because it works.

For example:

`forpass['location_x'] = forpass['location'].str.split(',', expand=True)[0].str.strip()`   
I have no idea what this code means, it works, does what I need it to do so I leave it.

How can I fix this? I feel like it's impossible for me to remember the syntax, and the necessary structure for my code. How the hell am I supposed to remember all this? I feel like I will never be able to.

&#x200B;

I'd appreciate the help"
106,learnmachinelearning,chatgpt,comments,2023-02-24 06:26:36,Is there a way to easily train ChatGPT or GPT on custom knowledge?,senttoschool,False,0.99,116,11akisx,https://www.reddit.com/r/learnmachinelearning/comments/11akisx/is_there_a_way_to_easily_train_chatgpt_or_gpt_on/,46,1677219996.0,"My company has internal documents. It'd be nice to be able to have GPT look over it, and then I can ask it questions on the internal documents."
107,learnmachinelearning,chatgpt,comments,2023-10-12 20:36:53,ChatGPT vision feature is really useful for understanding research papers!,nxtboyIII,False,0.86,189,176gs37,https://i.redd.it/xe94y8hf1utb1.png,42,1697143013.0,
108,learnmachinelearning,chatgpt,comments,2022-12-24 09:14:57,"How would I train a chatbot like ChatGPT on a specific data set, so that it answers questions as if it's belief structure was based on the information I give it?",EllyEscape,False,0.92,121,zu6785,https://www.reddit.com/r/learnmachinelearning/comments/zu6785/how_would_i_train_a_chatbot_like_chatgpt_on_a/,41,1671873297.0,"This might be a noob question, so I'll write it to my best abilities. I have some experience with coding video game AI in Godot, Unity and Unreal but I've never touched ML or ""real""(?) AI that uses learning algorithms. 

&#x200B;

I wanted to give a sophisticated chatbot like ChatGPT a bunch of data and text from (for instance, not my end goal) a philosopher, and have it answer questions as if it was that philosopher, ague against what I say as if it was a person who believed what the text I gave it said and so on, all while still able to use online resources (like ChatGPT does) to find additional supporting information, rather than only the text I give it which might limit its ability to give coherent arguments. In summary, I want it's beliefs  and values to be limited to a specific source text, but not it's knowledge base. 

&#x200B;

How would I go about this? Do I have to develop a model from scratch to give it any text sources I want, or is it possible to do with an existing API? I was going to use Character.AI but the method for giving it information is too limited for what I want to do. 

&#x200B;

If anyone has any resources to get me started it would be very helpful! Thank you."
109,learnmachinelearning,chatgpt,comments,2023-04-17 09:19:33,"New to ML, which is easier to learn - Tensorflow or PyTorch?",reddiculess,False,0.88,75,12p9bbt,https://www.reddit.com/r/learnmachinelearning/comments/12p9bbt/new_to_ml_which_is_easier_to_learn_tensorflow_or/,41,1681723173.0,"I mainly code in python and new to AI/ML and honestly just want to get a grasp of cool stuff you can do with ML (calculate stuck returns / NLP and text analysis / jump on the chatgpt hype)

which one is easier and more friendly to learn/install/etc? (ill prob start on google collab too)"
110,learnmachinelearning,chatgpt,comments,2023-03-10 05:22:26,[P] Looking for ML Buddies to Start Freelancing Together and Build a Supportive Community,Dukhanin,False,0.93,70,11nfri6,https://www.reddit.com/r/learnmachinelearning/comments/11nfri6/p_looking_for_ml_buddies_to_start_freelancing/,35,1678425746.0,"upd: dicord link [https://discord.gg/5zUaNXnFZY](https://discord.gg/5zUaNXnFZY)  
upd2: this not that small actually already - please dont be confused but help us organise this in the proper way

  
**TLDR:**

Looking for ML buddies at any level (preferably beginners) who want to start freelancing together. The goal is to build a small local community of ML enthusiasts who can support each other and exchange knowledge. We will use freelance collaboration as our main activity. We're also looking for experienced mentors (paid or unpaid) to guide us.

**Extended:**

I believe that learning and growing in a group is much more enjoyable and effective. That's why I'm trying to create a community of like-minded individuals.

I'm looking to create a small, local community for people who are starting out in freelancing, and who are interested in mutual support. Our main activity will be a Discord channel where members can post their work and collaborate on projects, with payment split by agreement. Additionally, we plan to engage in activities such as knowledge exchange, live coding, supporting each other's pet projects, and hosting study sessions.

This community will be small and focused, with members who can trust each other and share similar goals. We're also looking for experienced mentors who can provide guidance as we navigate the world of ML freelancing. Whether paid or unpaid, we welcome any support and advice.

About me: I'm a 21-year-old self-taught ML enthusiast from Russia. Although I don't have any experience in freelancing, I'm eager to start taking my first steps towards making money and gaining experience. As a beginner, I'm hoping to connect with others who are at a similar level and are also looking to grow.

**the text is chatgpt supported to prevet grammar issues, sound more native and clear**"
111,learnmachinelearning,chatgpt,comments,2023-04-26 06:23:17,Hugging Face Releases Free Alternative To ChatGPT,vadhavaniyafaijan,False,0.98,393,12z8n4e,https://www.theinsaneapp.com/2023/04/free-alternative-to-chatgpt.html,35,1682490197.0,
112,learnmachinelearning,chatgpt,comments,2023-02-27 13:42:55,Can you fine-tune chatGPT in your data as of now?,Melodic_Stomach_2704,False,0.9,55,11dc5b4,https://www.reddit.com/r/learnmachinelearning/comments/11dc5b4/can_you_finetune_chatgpt_in_your_data_as_of_now/,33,1677505375.0, I know that model is not publicly available so it's not possible to do it locally. But can you train or fine-tune chatGPT on your data using their API? I see many misguiding articles on the internet that are fine-tuning other GPT models claiming chatGPT.
113,learnmachinelearning,chatgpt,comments,2023-01-31 16:17:42,ChatGPT Crossed 10 Million Daily Active Users In Just 40 Days,vadhavaniyafaijan,False,0.95,217,10q34ra,https://www.theinsaneapp.com/2023/01/chatgpt-crossed-10-million-user.html,32,1675181862.0,
114,learnmachinelearning,chatgpt,comments,2023-11-23 20:01:46,Language models feel so inefficient for me,besabestin,False,0.86,52,1829m0y,https://www.reddit.com/r/learnmachinelearning/comments/1829m0y/language_models_feel_so_inefficient_for_me/,31,1700769706.0,"For all designs I know in my life, this box of a neural networks feels something of inefficient. Specially for language models. 

Everytime you calculate a word as output you have to generate the probability for the whole dictionary. And that after going through billions of parameters. And the memory cost when you want a larger context is ridiculous.

For the first chatgpt I remember reading somewhere that one prompt could cost as high as 100 times more energy what web search costs. 

I sometimes am fascinated how efficient the human brain is. When we are asked a simple question we don’t burn a lot of energy to answer that. I mean all that with the disadvantage of not having a large knowledge base.

Anyways, just wondering. If I got one of my assumptions wrong I d really appreciate the insights."
115,learnmachinelearning,chatgpt,comments,2023-11-29 18:55:53,What do you think ChatGPT does when you ask it to do Sentiment Analysis?,PinstripePride97,False,0.78,13,186x2t3,https://www.reddit.com/r/learnmachinelearning/comments/186x2t3/what_do_you_think_chatgpt_does_when_you_ask_it_to/,29,1701284153.0,Could be silly question but if you give a sentence to ChatGPT and ask it to give a sentiment analysis what do you think it does?
116,learnmachinelearning,chatgpt,comments,2023-02-19 13:55:13,ChatGPT History,eforebrahim,False,0.86,254,116au66,https://i.redd.it/dv8cfj0nz6ja1.jpg,27,1676814913.0,
117,learnmachinelearning,chatgpt,comments,2024-01-16 20:46:20,Can someone link me some videos of LLM Chatbots made cheaply? - i.e. by an individual using a few hundred $'s of compute power?,AchillesFirstStand,False,0.5,0,198dfch,https://www.reddit.com/r/learnmachinelearning/comments/198dfch/can_someone_link_me_some_videos_of_llm_chatbots/,25,1705437980.0,"I know ChatGPT cost millions of dollars to create the model and hundreds/thousands of people are involved in the process.

I want to see what the current state of the art is in terms of LLM chatbots that are made on the cheap. I think I saw a video from Andrei Karpathy showing how you can make your own GPT, I just want to see what is possible for an individual. It would be a potential project for me."
118,learnmachinelearning,chatgpt,comments,2023-01-11 05:23:14,Thoughts on this ChatGPT fact-checker tool I built this past week?,QuestionAnxious,False,0.89,89,108wigf,https://v.redd.it/yqudljp0ncba1,25,1673414594.0,
119,learnmachinelearning,chatgpt,comments,2024-01-10 06:50:09,Looking for a reason to keep learning about LLMs,SnooBeans7516,False,0.82,27,19323dh,https://www.reddit.com/r/learnmachinelearning/comments/19323dh/looking_for_a_reason_to_keep_learning_about_llms/,24,1704869409.0,"So something's been on my mind recently and I wanted to get Reddit's thoughts.

The thing that is troubling me as I learn more of the technical stuff, it seems that for a lot of language based NLP tasks, ChatGPT or these other foundation models seem SOTA for 99% of tasks. I was really excited to start training and working with BERT-based models, but find that a lot of the time I could get similar or better results just prompt engineering ChatGPT properly.

&#x200B;

So is it worth learning how to build and train these models? Or is my time really just better spent learning to use the APIs in effective ways like in RAG applications or in employing agents?

Unlike with CV and things like ControlNet, I don't see a lot of great applications of learning the technical stuff for someone who isn't a research scientist at a lab.

&#x200B;

(for some context, I'm a PM who wanted to upskill in this area, but feeling like I'm wasting a lot of my time reading all the new papers and working with models at home  :/. )"
120,learnmachinelearning,chatgpt,comments,2023-01-15 00:08:37,Is it still worth learning NLP in the age of API-accessibles LLM like GPT?,CrimsonPilgrim,False,0.93,60,10c509n,https://www.reddit.com/r/learnmachinelearning/comments/10c509n/is_it_still_worth_learning_nlp_in_the_age_of/,24,1673741317.0,"A question that, I hope, you will find legitimate from a data science student.

I am speaking from the point of view of a data scientist not working in research.

Until now, learning NLP could be used to meet occasional business needs like sentiment analysis, text classification, topic modeling....

With the opening of GPT-3 to the public, the rise of ChatGPT, and the huge wave of applications, sites, plug-ins and extensions based on this technology that are accessible with a simple API request, it's impossible not to wonder if spending dozens of hours diving into this field if ML wouldn't be as useful today as learning the source code of the Pandas library. 

In some specialized cases, it could be useful, but GPT-3, and the models that will follow, seem to offer more than sufficient results for the immensity of the cases and for almost all classical NLP tasks. Not only that, but there is a good chance that the models trained by giants like Open-AI (Microsoft) or Google can never be replicated outside these companies anyway.  With ChatGPT and its incomparable mastery of language, its ability to code, summarize, extract topics, understand... why would I bother to use BERT or a TF-IDF vectorizer when an API will be released? Not only it would be easily accessible, but it also would be much better at the task, faster and cheaper.

In fact, it's a concern regarding all the machine learning field in general with the arrival of powerful ""no-code"" applications, which abstract a large part of the inherent complexity of the field. There will always be a need for experts, for safeguards, but in the end, won't the Data Scientist who masters the features of GPT-3 or 4 and knows a bit of NLP be more efficient than the one who has spent hours reading Google papers and practicing on Gensim, NLTK, spacy... It is the purpose of an API to make things simpler eventually... At what point is there no more reason to be interested in the behind-the-scenes of these tools and to become simple users rather than trying to develop our own techniques?"
121,learnmachinelearning,chatgpt,comments,2024-02-07 23:54:35,Are there any AIs which learn as they are used?,Traditional_Land3933,False,0.89,22,1alhp1h,https://www.reddit.com/r/learnmachinelearning/comments/1alhp1h/are_there_any_ais_which_learn_as_they_are_used/,24,1707350075.0,I don't know too much about AI/ML/DL so forgive me for how stupid this question is. I am a very newbie. But is there any AIs which not only learn when they were trained but also when it's used? So such as a ChatGPT type thing which would learn as you're using it? Or does every AI in existence have to be going through training to learn anything?
122,learnmachinelearning,chatgpt,comments,2023-02-16 10:29:31,OpenAI Has Purchased AI.Com For ChatGPT For $11M,vadhavaniyafaijan,False,0.93,210,113nizs,https://www.theinsaneapp.com/2023/02/openai-purchased-ai-com-domain.html,23,1676543371.0,
123,learnmachinelearning,chatgpt,comments,2023-04-04 14:34:29,Working with chatGPT,macronancer,False,0.97,604,12bkzjv,https://i.redd.it/5uwfzjh4pvra1.png,22,1680618869.0,
124,learnmachinelearning,chatgpt,comments,2023-02-01 04:14:49,ChatGPT Extension for VSCode,Dense_Dimension_913,False,0.96,148,10qk8en,https://www.reddit.com/r/learnmachinelearning/comments/10qk8en/chatgpt_extension_for_vscode/,22,1675224889.0,"Created a ChatGPT extension for VSCode to help programmers understand  and read code more easily and also other features. To start, simply highlight a piece of code  and click on the plus icon on the  left to open up a chat and start  talking with ChatGPT, or Codex, or text-davinci-003. You can choose the  model you want to use in the settings. More details in the links below. I  really hope this extension can be useful to many people out there.  Please give it a try and let me know if you guys see any bugs or if you  like the extension. Thanks!

&#x200B;

https://i.redd.it/28zslxm06ifa1.gif

[VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=AndrewZhang.scribeai)"
125,learnmachinelearning,chatgpt,comments,2023-03-16 16:51:03,Introducing OpenChatKit - The Open-Source Alternative to ChatGPT,kingabzpro,False,0.98,203,11szhsh,https://www.reddit.com/r/learnmachinelearning/comments/11szhsh/introducing_openchatkit_the_opensource/,21,1678985463.0,"Hey everyone! I'm excited to share my latest article about a new open-source technology called OpenChatKit.

For those who work in NLP, you're probably familiar with ChatGPT - a powerful language model that can perform various natural language processing tasks. However, ChatGPT is not open-source, which limits its accessibility and customizability.

OpenChatKit, on the other hand, is an open-source alternative to ChatGPT that provides users with similar NLP capabilities while allowing for more customization and control. With OpenChatKit, users can train their own models and fine-tune them to their specific use cases.

In my article, I dive into the features of OpenChatKit, the Instruction-tuned Large Language Model, and the Limitations of the Model.

If you're interested in learning more about OpenChatKit and how it can enhance your NLP workflows, check out my article [OpenChatKit: Open-Source ChatGPT Alternative ](https://www.kdnuggets.com/2023/03/openchatkit-opensource-chatgpt-alternative.html). I'd love to hear your thoughts and answer any questions you may have."
126,learnmachinelearning,chatgpt,comments,2023-03-25 09:59:57,Are chat gpt code outputs plain wrong?,SnooHabits4550,False,0.43,0,121h5ii,https://www.reddit.com/r/learnmachinelearning/comments/121h5ii/are_chat_gpt_code_outputs_plain_wrong/,19,1679738397.0,"I asked chatgpt how can I standardize give time series and it gave me following:

https://preview.redd.it/ax6hgvnuyupa1.png?width=711&format=png&auto=webp&s=409bfb5657461bb52718a80ea92b553e842c0959

It gave output which seem incorrect (I tried running that code). So asked it whether it executed that code and it confirmed it indeed executed that code!

**Update**

In case you want to know further conversation:

https://preview.redd.it/bh8uwiidmvpa1.png?width=672&format=png&auto=webp&s=6309e5b149106bfdaa46b457e0cd0ef24149b72c

https://preview.redd.it/avdrdf3gmvpa1.png?width=787&format=png&auto=webp&s=a9886ab88c67406f2bedf0bdeaa9a6bf04f38883

Still wrong output.

&#x200B;

https://preview.redd.it/gsplbvzkmvpa1.png?width=696&format=png&auto=webp&s=cd6dff676d1ec5f472bde05c070b8072764a8007

&#x200B;

https://preview.redd.it/fdkleplmmvpa1.png?width=677&format=png&auto=webp&s=8698d565059cd5749e5fba7fd3df270f07a92a4c"
127,learnmachinelearning,chatgpt,comments,2023-03-05 23:30:47,How does the transformer model lead to emergent intelligence?,VanillaSnake21,False,0.56,1,11jhl4y,https://www.reddit.com/r/learnmachinelearning/comments/11jhl4y/how_does_the_transformer_model_lead_to_emergent/,18,1678059047.0,"I'm trying to understand how a transformer model such as the one used by Bing or ChatGPT leads to the emergence of intelligence, memory etc. I'm not too versed in ML but you can explain using advanced terms - I'll just ask Bing to elaborate on the details."
128,learnmachinelearning,chatgpt,comments,2022-12-27 18:05:30,Am I Too Late?,stupidSTEMquestions,False,0.52,1,zwltk8,https://www.reddit.com/r/learnmachinelearning/comments/zwltk8/am_i_too_late/,19,1672164330.0,"I am a college student studying math and computer science. I know how to program with high level languages, C, and a bit of C++ and Scheme. I can build basic web apps and scripts, and am focusing on machine learning with python. 

With the release of ChatGPT and articles like [this](https://cacm.acm.org/magazines/2023/1/267976-the-end-of-programming/fulltext#comments) though, I can't help but ask — am I too late?

Is it simply too late for a beginner to make any contributions to the field at this point when OpenAI, Deepmind, and the like are iterating at such a rapid pace? I really love AI and machine learning so far, but I also don't want to waste my time and energy if there won't be any meaningful work for me once I finish my education in 4 or more years."
129,learnmachinelearning,chatgpt,comments,2023-12-28 22:20:51,"Why does Google look so dominant in search, but seems to admit that they have no moat with GenAI?",PsychoWorld,False,0.41,0,18t6ced,https://www.reddit.com/r/learnmachinelearning/comments/18t6ced/why_does_google_look_so_dominant_in_search_but/,17,1703802051.0,"Hey everyone.

Background: I studied Cognitive Science, which included some Calculus and data structures in Computer Science. I'm not the MOST technical person there is, but I can comprehend concepts that tap into advanced computer science domains.

1. Google is dominant in online searching. Despite sustained efforts by companies with platform advantages, including Microsoft and Apple (Bing and Siri) attempting to make their search engines better, Google seems far and away the most preferred search engine worldwide.

2. Meanwhile... They seem to admit internally that GenAI isn't a race that they can sustainably win (https://www.semianalysis.com/p/google-we-have-no-moat-and-neither). The have no moat in this.

What is the technical cause of this?

I'm unfamiliar with the technical reasons for why they're so much better in search, but ChatGPT seems to think it's due to their data advantage and focus on user experience. But I'm still not sure why that cannot be overcome. Is search technology much more reliant on one aspect of tech infrastructure that cannot be overcome by other companies, whereas LLMs are dependent on open-source tech and data, and thus can be improved upon easily?

Is deep learning why they're so much better?"
130,learnmachinelearning,chatgpt,comments,2023-02-11 12:46:22,"ChatGPT Powered Bing Chatbot Spills Secret Document, The Guy Who Tricked Bot Was Banned From Using Bing Chat",vadhavaniyafaijan,False,0.94,208,10zmtqz,https://www.theinsaneapp.com/2023/02/chatgpt-bing-rules.html,15,1676119582.0,
131,learnmachinelearning,chatgpt,comments,2023-03-30 12:56:24,I created this entire video using ChatGPT + Charactr API + D-ID. My mind is blown,3nd4u,False,0.87,69,126m5eo,https://www.reddit.com/r/learnmachinelearning/comments/126m5eo/i_created_this_entire_video_using_chatgpt/,15,1680180984.0,"Could this be the future of how our news is being consumed?

https://reddit.com/link/126m5eo/video/hhfat6n3jvqa1/player"
132,learnmachinelearning,chatgpt,comments,2023-10-22 04:22:24,Built it for my project at first: Memorybase.io supercharges ChatGPT with memory capabilities for your chatbots,tujiserost,False,0.81,10,17dl1bw,https://www.reddit.com/r/learnmachinelearning/comments/17dl1bw/built_it_for_my_project_at_first_memorybaseio/,17,1697948544.0,"Hey everyone!

I've been delving deep into chatbots lately, especially with the ChatGPT API, and I found an issue that's probably familiar to many of you: ChatGPT doesn't inherently have memory capabilities. For many applications, that's perfectly fine, but for those of us who are trying to create a more context-aware and dynamic conversation flow, this limitation is quite apparent.

I faced this challenge in one of my projects and realized that there had to be a better way to integrate context and memory into ChatGPT's conversations. So, I built something for myself which I thought might be useful for many of you as well. Allow me to introduce you to [**Memorybase.io**](http://memorybase.io/).

Memorybase is a developer-friendly API that's designed to seamlessly integrate memory functionality into the ChatGPT API. By harnessing the power of the Pinecone vector database and LangChain, Memorybase wraps around the ChatGPT API and ensures that the right context and memory are injected into each query. This means that your chatbot can remember previous interactions, preferences, or any other context that's relevant for more engaging and meaningful conversations.

Imagine a user asking your chatbot about movie recommendations. The next day, they come back and reference that conversation, expecting the bot to remember. With Memorybase, that continuity becomes possible. The user experience improves manifold, and the possibilities for more sophisticated and context-aware bots increase tremendously.

I originally built Memorybase for my own needs. But the more I used it, the more I realized that this could have broader applications. Any developer looking to leverage the ChatGPT API could potentially benefit from the enhanced memory and context capabilities. From customer support bots to interactive storytelling, the potential use cases are vast.

This technology stack (pinecone/langchain) is not complex or ‘new’ per se, but for application developers who aren’t interested in managing it or hosting it, this could be a useful hassle-free option for your projects.

I've set up a page over at [memorybase.io](https://memorybase.io/) where you can learn more about how it works and see if it aligns with your needs. I would love for you to check it out and share your thoughts. Your feedback, insights, and potential use cases would be invaluable as I continue to refine and expand the capabilities of Memorybase.

Thanks for reading, and I'm eager to hear your thoughts and see where Memorybase can fit into the exciting world of chatbots!"
133,learnmachinelearning,chatgpt,comments,2023-10-12 00:18:56,How To Catch AI-Cheating,Science-man777,False,0.5,0,175tbwn,https://www.reddit.com/r/learnmachinelearning/comments/175tbwn/how_to_catch_aicheating/,17,1697069936.0,"""If you happen to have any dilutions of students not using ChatGPT and other artificial intelligence (AI) to cheat, it is time to get informed.  According to a recent survey from the Center for Democracy and Technology, [58% of students](https://cdt.org/wp-content/uploads/2023/09/091823-CDT-Off-Task-Summary-web.pdf) report using generative AI to complete assignments.  As awareness of this technology rises, this number only stands to increase. Meanwhile, the same study reports that educators find themselves behind the technology curve, with only 43% of teachers having been significantly trained on generative AI. 

""...how students use this technology to cheat and how teachers can detect and respond to generative AI. Beyond just detecting its use, this new technology may present an opportunity to leverage new and innovative ways of educating.""

[https://ai-solutions.pro/tools-to-detect-ai-cheating/](https://ai-solutions.pro/tools-to-detect-ai-cheating/)"
134,learnmachinelearning,chatgpt,comments,2023-09-23 13:42:22,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.96,182,16q4ve6,https://www.reddit.com/r/learnmachinelearning/comments/16q4ve6/this_week_in_ai_all_the_major_ai_developments_in/,16,1695476542.0,"1. **Genmo** releases a new text-to-video model: **Genmo Replay** v0.1, which generates high-quality videos from text without the need for advanced prompt engineering. *Genmo is available for free to create AI videos* \[[*Details*](https://blog.genmo.ai/log/replay-ai-video) | [Genmo *Replay*](https://www.genmo.ai/)\] .
2. **OpenAI** unveils **DALL·E 3** \- a major update to the text-to-image model, which will be integrated in ChatGPT. It will be available to ChatGPT Plus and Enterprise users in October, via the API and in Labs later this fall. Creators can now also opt their images out from future training.
3. **Toyota Research Institute** has developed a technique, powered by generative AI, that enables teaching robots new manipulation abilities in a single afternoon. Using the same robot, same code, and same setup, TRI taught over 60 different dexterous behaviors like peeling vegetables, using hand mixers, preparing snacks, and flipping pancakes.
4. **Microsoft** announced:
   1. Availability of AI Copilot for Windows from September 26th. Copilot will incorporate the context and intelligence of the web, your work data and what you are doing in the moment on your PC to provide better assistance. It will be integrated in Windows 11, Microsoft 365, Edge and Bing.
   2. Bing will add support for DALL.E 3 and deliver more personalized answers based on search history.
   3. New AI powered experiences in Paint, Photos and Clipchamp.
   4. New AI-powered shopping experience
5. **ElevenLabs** released **Projects** \- a tool that lets you generate an entire audiobook at the click of a button. Projects now supports .epub, .pdf, and .txt file imports, as well as initializing a project from a URL.
6. **Deci** presents **DeciDiffusion 1.0** \- an open-source text-to-image latent diffusion model which is 3x faster than Stable Diffusion v1.5 with the same quality.
7. **Google researchers** present a new approach that produces photo-realistic animations from a single picture. The model is trained on automatically extracted motion trajectories from a large collection of real video sequences.
8. **Google** has updated Bard\*\]\*:
9. **Bard Extensions:** With extensions, Bard can now connect to your Google apps and services like Gmail, Docs, Drive, Google Maps, YouTube, and Google Flights and hotels.
10. Users can use Bard’s “Google it” button to more easily double-check its answers and evaluate whether there is content across the web to substantiate it.
11. Bard can now let you continue chat via **shared public links**
12. **YouTube** announces new AI tools for creators. **Dream Screen** will let users create an AI-generated video or image background from text. Automatic AI-dubbing tool called **Aloud**, which will be integrated into YouTube Studio. **AI-powered insights** to generate video ideas and draft outlines. **Assistive Search in Creator Music** where AI will suggest the right music based on your description of your content.
13. **Amazon** announced that its voice assistant Alexa is being upgraded with a new, custom-built large language model.
14. **IBM** open-sources **MoLM** \- a collection of ModuleFormer-based language models ranging in scale from 4 billion to 8 billion parameters. ModuleFormer is a new neural network architecture based on the Sparse Mixture of Experts (SMoE) by IBM researchers. .
15. **Neuralink**, Elon Musk's brain implant startup, set to begin human trials.
16. **Lexica** has released **Aperture v3.5** \- their latest next-gen image model that can create photorealistic images and follows your prompt with precision.
17. **OpenAI** has invited domain experts to collaborate in evaluating and improving the safety of OpenAI's models by joining the new **OpenAI Red Teaming Network**.
18. \*\*GitHub Copilot Chat (\*\*beta) is now available for all individuals.
19. **Replit** announced a virtual hackathon for projects built using **Replit ModelFarm**
20. **Oracle** brings voice-activated AI to healthcare with Clinical Digital Assistant.
21. **Google** and the Department of Defense are building an AI-powered microscope to help doctors spot cancer.

My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
135,learnmachinelearning,chatgpt,comments,2023-03-20 18:42:54,[D] How do OpenAI and other companies manage to have real-time inference on model with billions of parameters over an API?,RaunchyAppleSauce,False,1.0,5,11wrdse,https://www.reddit.com/r/learnmachinelearning/comments/11wrdse/d_how_do_openai_and_other_companies_manage_to/,16,1679337774.0,"Hi, guys

I have been using OpenAI’s chatgpt through the app Poe and I find it very confusing how a model with billions of parameters is responding in real-time over an API.

How does one go about making inference fast, say 15-20ms, over an API for large models?

Thanks!"
136,learnmachinelearning,chatgpt,comments,2023-01-06 11:58:10,How does ChatGPT actually work? Explained simply with pen and paper,techie_ray,False,0.94,125,104sebq,https://youtu.be/k9Sps7ciNTE,16,1673006290.0,
137,learnmachinelearning,chatgpt,comments,2023-06-20 17:56:06,LLM questions,YellowSea11,False,0.5,0,14ei1z9,https://www.reddit.com/r/learnmachinelearning/comments/14ei1z9/llm_questions/,15,1687283766.0,"Hey gang .. two questions . 1) can I make chatGPT local? Meaning I don't want it connected to the internet at all .. is that doable? 
2) can I train it on my data? What are my options there? Ideally I'd love to be able to read data from a table so I could train it on those terms as well."
138,learnmachinelearning,chatgpt,comments,2024-01-04 18:59:47,"is there a ""for dummies"" way to train and use a CNN?",Phischstaebchen,False,0.74,13,18ykuqd,https://www.reddit.com/r/learnmachinelearning/comments/18ykuqd/is_there_a_for_dummies_way_to_train_and_use_a_cnn/,15,1704394787.0,"Hello,

I'm stuck with a little project of mine and I probably need to use machinelearning for it, but without much background in coding. I did some Python code for several Raspberry Pis but that's all.

I have stable drone-footage of dolphins filmed from 20-30m above the shore with a Mavic Air. I just need to detect them in the actual footage and follow each to track their hunting-movement. 

I haven't found trained models specific for dolphins, maybe Yolo will work? Aside from that I also found this dolphin-dataset that could be used to train? [https://arxiv.org/abs/2005.13359](https://arxiv.org/abs/2005.13359)

Aside from that I have enough drone-footage to actually get proper footage from above.

But from here I have not really a clue what to do. I installed Debian on my PC (Ryzen 9 5950x, 64GB RAM, 2060 Super) to get the Windows-headaches out of the way. Training-time isn't an issue. I can let the PC run for days if neccesary. From my point of view, training a model for this task is the biggest prob? Using trained models on footage did work for a quick and dirty try with generic stuff.

Can anybody help?

&#x200B;

Oh and yes, I used ChatGPT already... the code needs heavy fixing and sometimes ChatGPT just stops to give useful information and just keeps repeating generic information like ""you need to train your model before you can use it"".... lol"
139,learnmachinelearning,chatgpt,comments,2023-04-06 11:12:52,Meta: Is it possible to ban these TikTok influencers or TikToks in general?,dasMaiMaiKamel,False,0.94,221,12dgtry,https://www.reddit.com/r/learnmachinelearning/comments/12dgtry/meta_is_it_possible_to_ban_these_tiktok/,14,1680779572.0,"I'm new to this sub and I'd love to contribute here. But there are soooo many TikTok videos from someone talking about ChatGPT for the 10.000th time. These videos don't contribute to learning ML nor do they give actual reliable information. I often get the feeling that these people never touched a NN, just sat on ChatGPT and read one WikiPedia article. It's also often more an ad than actual help.  


  
Even if I'm not a member for too long, I see comments criticizing this exact thing under every video. Is it possible to add a rule to prevent this? It would greatly improve the quality of this sub."
140,learnmachinelearning,chatgpt,comments,2023-06-26 12:23:07,"Best way to cost effectively ""upload"" a large PDF to a language model so that you can ask questions about it?",RepresentativeNet509,False,1.0,8,14jfvq8,https://www.reddit.com/r/learnmachinelearning/comments/14jfvq8/best_way_to_cost_effectively_upload_a_large_pdf/,13,1687782187.0," I have a 400 page PDF and need to get it into a language model (cost effectively) and then be able to ask the model questions about the document like ""on what page does the scope summary begin"" or ""are there any prohibitions to participate in this solicitation due to the size of respondent's business"".

I have been able to use ""Ask My PDF"" to upload part of the PDF to ChatGPT and this basically gives the outcome I want for the pages that are uploaded, but it invariably crashes every time and there is no way to pick up where the uploading of pages left off.

I am fairly technical; would NanoGPT be a better solution for this? I am also looking at fine-tuning a model on OpenAI's API, but that seems cumbersome and expensive for my use case.

Any thoughts are appreciated!"
141,learnmachinelearning,chatgpt,comments,2023-03-16 02:58:26,I want to create a ChatGPT-like interface but to interact with a smaller specialized dataset.,ohai777,False,0.86,14,11si7ku,https://www.reddit.com/r/learnmachinelearning/comments/11si7ku/i_want_to_create_a_chatgptlike_interface_but_to/,11,1678935506.0,I want to create a ChatGPT interface but to interact with a smaller specialized set of data for my website's support. Can you help me with what terms I need to google to learn more about researching a project like this or any tutorials on this topic? Natural Language processing?
142,learnmachinelearning,chatgpt,comments,2023-07-29 17:37:15,True Beginner to ML- recommendations,WarAutomatic4637,False,0.87,23,15cy1b1,https://www.reddit.com/r/learnmachinelearning/comments/15cy1b1/true_beginner_to_ml_recommendations/,13,1690652235.0,"Hi All, I have a background in healthcare. Looking to do a deep dive into ML. My goal initially is to understand conceptually what steps I need to take to build a model from “scratch.”

I’m sure I’m not the first one with this question so any old threads I can read up on would be greatly appreciated.

There is a lot of terminology and applications being named in posts I’m not familiar with…any chance there is a definitions or summary post I can reference? 

I’m asking chatgpt lol, it recommended “hands on machine learning w/ sckit….”"
143,learnmachinelearning,chatgpt,comments,2023-10-02 19:09:14,Whats the Field of ML/AI Look Like? Professional Looking for Guidance.,Pan4TheSwarm,False,1.0,11,16y5bko,https://www.reddit.com/r/learnmachinelearning/comments/16y5bko/whats_the_field_of_mlai_look_like_professional/,13,1696273754.0,"Let me start out this post by saying I'm feeling a little unsure of my professional ambitions right now and looking for some guidance from the community. I have a bachelor's in Electrical Engineering, focusing on embedded systems and RF communication systems. Additionally I have dedicated my time out of school studying the field of software engineering through books. My specialties are C/C++, with some Python mixed in here and there. Professionally, I'm working in C++ on IoT technologies and custom RF hardware. I have a solid background in mathematics from my studies. I've also had some interest in socio-linguisitcs. 

A couple weeks ago, I started playing around with ChatGPT, and I was insanely impressed. My ADHD brain got hyperfocused and needed to learn more. I've been diving into the world of ML/AI since. I've been playing around with hosting LLaMA models locally (running painfully slow on my 6800XT), and reading up on machine learning since. 

I don't know how far my interest goes at this point, but right now my interest is very strong. I'm trying to determine if my interest is in dabbling with ML/AI, or if I want to pivot my professional career towards ML/AI. Honestly, I'm not sure at this moment and here's where I am looking for some more perspective to help gauge my interests.

I asked ChatGPT for resources to look into. I tend to be a book learner, so I focused on the book recommendations. They recommended ""Python Machine Learning"" by Sebastian Raschka and Vahid Mirjalili; ""Deep Learning"" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville; and ""Pattern Recognition and Machine Learning"" by Christopher M. Bishop. 

I love me my kindle samples, and I figured an applications book would be good for me at this stage, so I picked up ""Python Machine Learning"". I'm enjoying the book, but after reading it for some time, I'm starting to contemplate if I should be instead going down a learning path geared towards a more professional placement. I read a [A Super Harsh Guide to Machine Learning](https://www.reddit.com/r/MachineLearning/comments/5z8110/d_a_super_harsh_guide_to_machine_learning/), and noticed their recommendations were more 'academic' in nature (""Deep Learning"" is on their list). Its making me second guess where I put my time, but it all depends on what I want my desired outcome to be, and frankly I'm still not sure. 

I'm also looking for a good point to enter grad school for a Masters. Maybe I want to go into ML and NLP? Do I need to be looking at a PhD for this field (which, I wouldn't mind pursuing)? 

There isn't a distinct question here, so I'm sorry about that. I'm looking for perspective, and guidance for the field so I can determine how I want to pursuit my interest in this area. Should I continue with ""Python Machine Learning""? Or should I follow the Super Harsh Guide more closely? "
144,learnmachinelearning,chatgpt,comments,2024-01-23 05:38:28,ChatGPT with multiple PDFs hallucinates,BigYesterday2785,False,0.5,0,19dh7pj,https://www.reddit.com/r/learnmachinelearning/comments/19dh7pj/chatgpt_with_multiple_pdfs_hallucinates/,13,1705988308.0,"I just don’t get it. I am building a ChatGPT for multiple PDFs.

Now I have **multiple PDFs** which I have created embeddings of and saved in MongoDB database.

Now when a user asks a specific question, I get **multiple outputs from GPT-3.5** and a lot of it does **not make sense**. Even though **real answer is still in there.**

Is there an API i can use to go through all data of PDFs and figure out **which one is relevant and then I run GPT-3.5 only on that specific part of PDFs**"
145,learnmachinelearning,chatgpt,comments,2023-01-27 14:51:14,Fine-tuning open source models to emulate ChatGPT for code explanation.,awesomequantity,False,0.88,83,10mmofg,https://www.reddit.com/r/learnmachinelearning/comments/10mmofg/finetuning_open_source_models_to_emulate_chatgpt/,13,1674831074.0,"I'm looking to step up my game and emulate ChatGPT for specific use-cases like explaining code. I'm thinking about using open source models like GPT-J, or OPT to get beyond the limitations of the closed-source nature of ChatGPT, like the amount of text it can read or respond with.

I got the funding for training, hardware, etc, and I want the end product to be on-premises, so no worries there. The inference doesn't have to be super fast either. I know there are projects like OpenAssistant and petals.ml but haven’t made enough research just yet.

One option I’m considering is using fine tuners like the one from [HuggingFace](https://github.com/subhasisj/HuggingFace-Transformers-FineTuning) or [Jina AI](https://github.com/jina-ai/finetuner) to fine-tune open source models like GPT-J or OPT to improve specific use-cases like code explanation. With the funding that we have, I wouldn’t want to cheap out on fine-tuning and expect something good.

So, can anyone help out and point me in the right direction? Which model is the best to fine-tune and how do I fine-tune to improve specific use cases? Any help would be appreciated. Thanks!"
146,learnmachinelearning,chatgpt,comments,2023-02-20 15:42:13,"ChatGPT Response about the vinyl chloride spill in East Palestine, Ohio.",listenandlearn2,False,0.25,0,117adwo,https://www.reddit.com/r/learnmachinelearning/comments/117adwo/chatgpt_response_about_the_vinyl_chloride_spill/,12,1676907733.0," ChatGPT responded my prompt.  I'd love to hear your thoughts:

Vinyl Chloride spilled in East Palestine, Ohio about 300 miles away from Cincinnati. The chemical leaked into the Ohio River. How long will it take for the chemical to reach the water supply of Cincinnati which is also on the Ohio River?

The time it will take for Vinyl Chloride to reach Cincinnati's water supply from the spill site in East Palestine, Ohio will depend on several factors such as the amount of Vinyl Chloride spilled, the concentration of the chemical in the river, the flow rate of the river, and any barriers or dams along the river that may affect the movement of the chemical.

Assuming there are no significant barriers or dams along the river, the average flow rate of the Ohio River is approximately 1.5 miles per hour. Therefore, if the spill occurred in East Palestine, Ohio, which is 300 miles upstream from Cincinnati, it could take up to 200 hours (300 miles / 1.5 miles per hour) for the Vinyl Chloride to reach Cincinnati's water supply.

However, it is important to note that the actual time for the chemical to reach Cincinnati's water supply could be longer or shorter depending on the specific conditions of the spill and the river. It's also possible that officials may take steps to prevent the chemical from reaching the water supply, such as by diverting the river or shutting off water intake valves, in order to protect public health and safety.

📷📷Please timestamp this response

This response was generated on 2023-02-20.

📷📷Can you add the time the response was generated

Certainly! This response was generated at 2023-02-20 04:28:00 UTC.

Regenerate response[ChatGPT Feb 13 Version](https://help.openai.com/en/articles/6825453-chatgpt-release-notes). Free Research Preview. Our goal is to make AI systems more natural and safe to interact with. Your feedback will help us improve."
147,learnmachinelearning,chatgpt,comments,2024-01-27 23:58:57,How To Catch AI-Cheating: Outsmart the Bot - 2024 Edition,Science-man777,False,0.63,10,1acov77,https://www.reddit.com/r/learnmachinelearning/comments/1acov77/how_to_catch_aicheating_outsmart_the_bot_2024/,12,1706399937.0,"""If you happen to have any dilutions of students not using ChatGPT and other artificial intelligence (AI) to cheat, it is time to get informed.  According to a recent survey from the Center for Democracy and Technology, [58% of students](https://cdt.org/wp-content/uploads/2023/09/091823-CDT-Off-Task-Summary-web.pdf) report using generative AI to complete assignments.  As awareness of this technology rises, this number only stands to increase. Meanwhile, the same study reports that educators find themselves behind the technology curve, with only 43% of teachers having been significantly trained on generative AI. 

In this article, we will attempt to equip educators with the information they need to understand how students use this technology to cheat and how teachers can detect and respond to generative AI. Beyond just detecting its use, this new technology may present an opportunity to leverage new and innovative ways of educating.""

[https://ai-solutions.pro/tools-to-detect-ai-cheating/](https://ai-solutions.pro/tools-to-detect-ai-cheating/)"
148,learnmachinelearning,chatgpt,comments,2023-09-19 20:38:36,I'm planning to learn ML/AI what should be the best approach? Not comfortable with maths.,WhyNotSickMyDuck,False,0.17,0,16n1q5l,https://www.reddit.com/r/learnmachinelearning/comments/16n1q5l/im_planning_to_learn_mlai_what_should_be_the_best/,12,1695155916.0,"Hi all, I'm a DevOps engineer and very much comfortable with python language. I have no prior knowledge related to ML/AI field. 
My end goal is to be able to make an application like chatgpt.
How should I start, considering I'm weak in mathematics. 
Also I have heard a lot about deep learning, neural networks and so on. So what should be the order to learn them.
Kindly mention the resources and path as well.
Thanks in advance."
149,learnmachinelearning,chatgpt,comments,2023-02-06 02:29:05,Hey Reddit! I created a tutorial on how to build a Neural Network in PyTorch using ChatGPT,mechalf11,False,0.9,41,10uv4yq,https://www.reddit.com/r/learnmachinelearning/comments/10uv4yq/hey_reddit_i_created_a_tutorial_on_how_to_build_a/,12,1675650545.0,"Hello all,

I have been using ChatGPT extensively in my work and research, and I wanted to share my experience using it for creating Neural Networks in PyTorch. I created a quick tutorial, and would be curious on your feedback, and hopefully it helps others get started with this fantastic tool! The goal of the tutorial is to have those with little experience coding, little experience with PyTorch, or those who just want to use ChatGPT in a productive+cool way, get started. I am a firm believer that ChatGPT is here to stay, and the earlier we start implementing it into our daily workflows, the faster we will be able to leverage its full potential.

Code + detailed screenshots and instructions are available here: [https://medium.com/p/d6eefffab467](https://medium.com/p/d6eefffab467)"
150,learnmachinelearning,chatgpt,comments,2023-04-28 16:17:58,ChatGPT Prompt Engineering for Developers free on deeplearning.ai,sunkenwaaaaaa,False,0.87,17,131zare,https://www.reddit.com/r/learnmachinelearning/comments/131zare/chatgpt_prompt_engineering_for_developers_free_on/,10,1682698678.0,Andrew Ng just released a short course on how to use the Open AI api. It is free for now.
151,learnmachinelearning,chatgpt,comments,2023-07-22 16:08:08,Mysterious Algorithm: backpropagation: chatGPT explained,KSSolomon,False,0.85,19,156o1ta,https://www.reddit.com/gallery/156o1ta,10,1690042088.0,
152,learnmachinelearning,chatgpt,comments,2023-05-19 07:08:51,OpenAI Launches ChatGPT App For iOS Users,vadhavaniyafaijan,False,0.88,57,13lnv1e,https://www.theinsaneapp.com/2023/05/chatgpt-app-for-iphone-and-ipad.html,10,1684480131.0,
153,learnmachinelearning,chatgpt,comments,2023-08-08 14:42:01,How do I create this kind of Al bot?,oceanwilmot,False,0.65,4,15lk57s,https://www.reddit.com/r/learnmachinelearning/comments/15lk57s/how_do_i_create_this_kind_of_al_bot/,9,1691505721.0,"So for context I have some programming knowledge just not in the ML field.

I want to create a model (think of the grimesAl on
Twitter) that is :

1.) is trained on specific information which I have will provide (I want this to serve as its ""life story"") 2.) is also able to exist as a chatbot and train itself based on the conversations that it will have with random people

However,
I don't want to have to train it from complete scratch.

Is it possible for me to use already existing data sets to train a baseline personality(just so it would be at least decent to chat with) and then take it from there?

Think about someone creating an Al girlfriend or an Al friend except their ""personality"" is formed with a existing data as a baseline and it’s personality is further developed  by its interactions with people and more date provided in the form of prompts 

Except I wouldn't want it to be hooked up to a chatGPT API

Another example to be clear:

Let’s say we train a Bot on a Twitter account. I’d want to basically replace the Twitter account with my specific prompts and chats that the bot has with others"
154,learnmachinelearning,chatgpt,comments,2023-05-07 12:58:51,New to AI and ChatGPT - Where do I start?,growthnerd,False,0.15,0,13an0ji,https://www.reddit.com/r/learnmachinelearning/comments/13an0ji/new_to_ai_and_chatgpt_where_do_i_start/,9,1683464331.0,"Heya, I just started using ChatGPT for a couple weeks for college homework. This AI tech is amazing and I wanna learn more.

What are 3-5 concepts or software you’d recommend me to start learning first? Also, what are your top 3-5 newsletters, channels or websites to learn about AI from?

Thanks so much, appreciate the help"
155,learnmachinelearning,chatgpt,comments,2023-11-25 17:19:19,How to Best Use AI as an Educational Tool – 10 Genius Tricks You Didn’t Know Existed,Science-man777,False,0.55,3,183ok48,https://www.reddit.com/r/learnmachinelearning/comments/183ok48/how_to_best_use_ai_as_an_educational_tool_10/,9,1700932759.0,""" Up until now, much of the discussion surrounding the use of generative AI in education has centered on catching AI used in cheating.  Some educators have seen generative AI as an awkward reality that makes writing assignments difficult to regulate.  With ChatGPT 4.0, students can pass off AI writing as their own original work thus circumventing the point of the assignment.  AI is seen as the ultimate slacker tool, making it irresistibly easy for lazy students to complete writing assignments at the press of a button.  

Educators Strike Back?

How are teachers supposed to respond to this?  I think there are two possible responses to this.  One is the first, very understandable response, which is to attempt to catch the “AI cheater” in the act.  This reaction makes sense at the moment since educational organizations have not yet had time to understand and respond to the technology.  To help on that front, we have created a thorough review of how educators might catch the students who decide to become AI cheaters in the article at this link.

If You Can’t Beat’em…

However, in this article, we will look at what I believe is the second possible response educators can have to this technology: rather than trying to constantly stay ahead of this ever-evolving technology in order to try and “catch the cheater,” can we rather ask if there is a way of using generative AI as an educational asset?  Could we possibly view [machine learning](https://ai-solutions.pro/what-is-machine-learning-a-beginners-guide/) and [Natural Language Processing](https://ai-solutions.pro/what-is-natural-language-processing-nlp-the-ultimate-beginners-guide/) as a natural next step in the advancement of technology, much like math teachers eventually accepted the use of calculators in math class? ""

Here is the full article:

[https://ai-solutions.pro/ai-in-education-10-genius-tricks-you-didnt-know-existed/](https://ai-solutions.pro/ai-in-education-10-genius-tricks-you-didnt-know-existed/)"
156,learnmachinelearning,chatgpt,comments,2023-08-02 18:21:44,A Brief History of Natural Language Generation [Timeline] —Thoughts? Corrections? Suggestions? Thanks!,Britney-Ramona,False,0.82,25,15ggib0,https://i.redd.it/meslnx7moqfb1.png,8,1691000504.0,
157,learnmachinelearning,chatgpt,comments,2023-12-14 23:37:14,Suggest some online courses to learn how to create our own machine learning algorithm?,madhawavish,False,0.67,1,18im3bi,https://www.reddit.com/r/learnmachinelearning/comments/18im3bi/suggest_some_online_courses_to_learn_how_to/,8,1702597034.0,"i would like to create new ML algorithms, when i asked from chatgpt for some courses to learn about creating ML algorithms, it shows me same ML courses which are needed to study ml.. so can i learn to create my own ML algorithm by following those ML courses.. can you suggest me some better courses to learn ML algorithm creating..

courses given by chatgpt

""Machine Learning"" by Stanford University (Coursera): 

""Deep Learning Specialization"" by [deeplearning.ai](https://deeplearning.ai) (Coursera): 

""Machine Learning A-Z™: Hands-On Python & R In Data Science"" (Udemy): 

""Practical Deep Learning for Coders"" by [fast.ai](https://fast.ai): 

""Applied Data Science with Python Specialization"" by the University of Michigan (Coursera):.

""Machine Learning Crash Course"" by Google:"
158,learnmachinelearning,chatgpt,comments,2023-03-08 00:27:29,How can you extract items and their corresponding item quantity from a string?,le_monke7,False,0.67,1,11lgpk1,https://www.reddit.com/r/learnmachinelearning/comments/11lgpk1/how_can_you_extract_items_and_their_corresponding/,8,1678235249.0,"Let's say we have a list of supported items: blankets, apple, bottle of water, honey, wine, wine glasses. I want to extract any of those items from a string, and the quantity associated with the item.  
  
When quantities are not mentioned or ambiguous, the quantity is  `None`.  
  
Here are some input examples followed by their expected returned value:  
  - ""two blankets"" -> {""blankets"": 2}  
  - ""just an apple and a bottle of water"" -> {""apple"": 1, ""bottle of water"": 1}  
  - ""Can I have some honey?"" -> {""honey"": None}  
  - ""That's a wine, and two wine glasses"" -> {""wine"": 1, ""wine glasses"": 2}  
  - ""what?"" -> {}  
  - ""a pear and a pen"" -> {}  
  - ""two blankets, actually three of them"" -> {""blankets"": 3} (this one is a bit complex, so I leave it as an extra task)  
  
What's the most efficient way to achieve that? I tried ChatGPT and it nails everything, even the last example, but I was wondering if I want to implement it in an app, it could be costly, and the API might be down sometimes."
159,learnmachinelearning,chatgpt,comments,2023-11-18 15:20:37,Matrices: [inputs x outputs] or [outputs x inputs]?,CarloWood,False,0.75,2,17y8ofr,https://www.reddit.com/r/learnmachinelearning/comments/17y8ofr/matrices_inputs_x_outputs_or_outputs_x_inputs/,8,1700320837.0,"Consider a layer with M neurons, and thus M outputs. Let the layer have N inputs and be fully connected (every inputs is connected to every neuron).

Then this layer will have NM weights. Let these weights be represented by a matrix . What is the common practice in research: to use a matrix of N rows and M columns, or to use M columns and N rows?

The following is not part of the question, so let it not influence you in case I am wrong. What I understand at the moment is that a matrix is denoted as 'Rows x Cols' in this field, and (according to \*cough\*ChatGPT4\*cough\*) it is 'outputs x inputs', so the answer would be that the weights matrix W is an M x N matrix. This also means that if X is an input column-vector, we need to multiply WX to get the output: after all, when we see X as a matrix of N rows and 1 column (N x 1), the number of columns of the first matrix must equal the number of rows of the second matrix: \[M x N\]\[N x 1\] = \[M x 1\], thus the result is a column-vector of M elements.

The reason I ask is because ChatGPT confused me by giving an inconsistent example that used inputs \* weights (XW) using a 2x1 tensor for the weights (2 inputs, 1 output: this was a single neuon). The result is still the same, but it is inconsistent with the notion that 'outputs x inputs' is common."
160,learnmachinelearning,chatgpt,comments,2022-12-11 10:34:42,Is some open equivalent of ChatGPT being made somewhere,que1112,False,0.71,3,zingsd,https://www.reddit.com/r/learnmachinelearning/comments/zingsd/is_some_open_equivalent_of_chatgpt_being_made/,7,1670754882.0,ChatGPT is great but its closed nature means it will probably get locked away from a lot of people. Is there some model like ChatGPT being made that is open-source? Something like Bloom but more GPT-ish or at least some company or Kickstarter campaign working on this?
161,learnmachinelearning,chatgpt,comments,2024-02-19 02:07:46,Is it worth studying NLP?,David_Slaughter,False,0.81,28,1aubsfk,https://www.reddit.com/r/learnmachinelearning/comments/1aubsfk/is_it_worth_studying_nlp/,8,1708308466.0,"So obviously I'm 10 years behind. 10 years late to the pie. But I have to make a selection for one of my semester 2 modules. My last selection is difficult and I'm considering NLP.

But is it even worth learning? People who started AI 10 years ago have already made the likes of ChatGPT and all that. What can a measly student offer the world by studying this?"
162,learnmachinelearning,chatgpt,comments,2023-01-25 01:15:22,How ChatGPT is Trained,ariseff,False,0.98,126,10km46l,https://youtu.be/VPRSBzXzavo,8,1674609322.0,
163,learnmachinelearning,chatgpt,comments,2023-04-06 07:43:39,What are the mathematical theorems for the success of LLMs?,GraciousReformer,False,0.6,1,12dco2s,https://www.reddit.com/r/learnmachinelearning/comments/12dco2s/what_are_the_mathematical_theorems_for_the/,8,1680767019.0,I am aware of the universal approximation theorems. But the success of ChatGPT would be more than the universal approximation theorem. What is the mathematics behind these successes?
164,learnmachinelearning,chatgpt,comments,2023-02-11 06:58:18,[N] New Open-Source Version Of ChatGPT ⭕,LesleyFair,False,0.98,114,10zep6u,https://www.reddit.com/r/learnmachinelearning/comments/10zep6u/n_new_opensource_version_of_chatgpt/,8,1676098698.0,"GPT is getting competition from open-source.

A group of researchers, around the YouTuber [Yannic Kilcher](https://www.ykilcher.com/), have announced that they are working on [Open Assistant](https://github.com/LAION-AI/Open-Assistant). The goal is to produce a chat-based language model that is much smaller than GPT-3 while maintaining similar performance.

If you want to support them, they are crowd-sourcing training data [here](https://open-assistant.io/).

**What Does This Mean?**

Current language models are too big.

They require millions of dollars of hardware to train and use. Hence, access to this technology is limited to big organizations. Smaller firms and universities are effectively shut out from the developments.

Shrinking and open-sourcing models will facilitate academic research and niche applications.

Projects such as Open Assistant will help to make language models a commodity. Lowering the barrier to entry will increase access and accelerate innovation.

What an exciting time to be alive! 

Thank you for reading! I really enjoyed making this for you!  
The Decoding ⭕ is a thoughtful weekly 5-minute email that keeps you in the loop about machine research and the data economy. [Click here to sign up](https://thedecoding.net/)!"
165,learnmachinelearning,chatgpt,comments,2023-03-02 16:47:40,Build ChatGPT for Financial Documents with LangChain + Deep Lake,davidbun,False,0.95,171,11g7h03,https://www.reddit.com/r/learnmachinelearning/comments/11g7h03/build_chatgpt_for_financial_documents_with/,8,1677775660.0,"https://preview.redd.it/h9r6hgvfucla1.png?width=2388&format=png&auto=webp&s=5432eac3eeed8583e4309af1fdc7ebecac705796

As the world is increasingly generating vast amounts of financial data, the need for advanced tools to analyze and make sense of it has never been greater. This is where [LangChain](https://github.com/hwchase17/langchain) and [Deep Lake](https://github.com/activeloopai/deeplake) come in, offering a powerful combination of technology to help build a question-answering tool based on financial data. After participating in a LangChain hackathon last week, I created a way to use Deep Lake, the data lake for deep learning (a package my team and I are building) with LangChain. I decided to put together a guide of sorts on how you can approach building your own question-answering tools with  LangChain and Deep Lake as the data store.

Read [the article](https://www.activeloop.ai/resources/ultimate-guide-to-lang-chain-deep-lake-build-chat-gpt-to-answer-questions-on-your-financial-data/) to learn:

1. What is LangChain, what are its benefits and use cases and how you can use to streamline your LLM (Large Language Model) development?  
2. How to use [\#LangChain](https://www.linkedin.com/feed/hashtag/?keywords=langchain&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) and [\#DeepLake](https://www.linkedin.com/feed/hashtag/?keywords=deeplake&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) together to build [\#ChatGPT](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) for your financial documents.  
3. How Deep Lake’s unified and streamable data store enables fast prototyping without the need to recompute embeddings (something that costs time & money).  


I hope you like it, and let me know if you have any questions!"
166,learnmachinelearning,chatgpt,comments,2023-12-02 00:39:21,What is wrong with my prediction model?,Impressive_Papaya306,False,0.71,3,188r64p,https://www.reddit.com/r/learnmachinelearning/comments/188r64p/what_is_wrong_with_my_prediction_model/,8,1701477561.0,"Complete beginner here. Not sure if this is the right place to ask. Any help is appreciated. Thank you!

  
I'm currently trying a prediction model and facing some challenges. I used Chatgpt for most of the coding and see how it goes. project and the issue I'm encountering:

&#x200B;

Goal: power prediction from a wind turbine.

 I have a 5-year dataset of 10min data points. Had too many NaN, I used 7  day-rolling mean  and kNN. 

 I have a 5-year dataset of 10min data points. Had too many NaN, so I used 7-day-rolling mean and kNN to replace them. The model includes 6 features.

Target- Power (the graph for this variable is showing repetitive patterns).

Applied SARIMA to predict features.

random forest for power prediction.

Would different handling of NaN values impact the model?

&#x200B;

https://preview.redd.it/4u8i0eda2s3c1.png?width=1456&format=png&auto=webp&s=f399d6bc17e61c9339ad1a2fe56cdb9426ee16bf

&#x200B;"
167,learnmachinelearning,chatgpt,comments,2023-06-17 16:27:13,I created the SMARTEST Computer Assistant using ChatGPT,Pritish-Mishra,False,0.67,21,14bv9dz,https://v.redd.it/g8hq2391ul6b1,7,1687019233.0,
168,learnmachinelearning,chatgpt,comments,2023-12-25 03:05:24,i am looking for a self trainable ai model,awesomegame1254,False,0.3,0,18qa1zu,https://www.reddit.com/r/learnmachinelearning/comments/18qa1zu/i_am_looking_for_a_self_trainable_ai_model/,7,1703473524.0,"i am hoping to make a local chatbot of sorts i do however have some specific requirements which is why  i am making a new chatbot and not just simply using something like chatgpt also there is repeated cost considerations.  


1. it needs to have access to the internet
2. it needs to be trainable on an amd gpu on a windows computer
3. it needs to be able to generate text then based off that generated text and a text prompt select an image from a folder of images
4. it needs to be able to understand really long term context including its own responses

the idea being that i would ask it to generate the best title description and tags for a youtube video based on a starting title it would then use that starting title to search youtube for relevant content proritizing the most recently best performing videos ie instead of only looking at ""how many views does this video have total"" it will look at ""how many views has this video gained in the last say 6 months"" generalizing the search when needed it would then use this content to generate what i asked for. then i would give it a bunch of images (up to 100) and ask it ""select the best thumbnail out of the available options"" and it would use the title description and tags it previously generated along with the performance data it gathered previously to do what i asked it needs to be able to understand previous context because i will ask it something like ""next do part 2"" and i need it to understand that i mean part 2 of the previous video and other things like that.

&#x200B;

to be honest  what i would really like is something where i could input a video along with a little clarifying text like what game i am playing if necessary it would then use that video to do what the previous chatbot did without needing a whole bunch of extra text clarification ie i wouldn't have to for example tell it that no despite the name this map is not an urban map.

&#x200B;"
169,learnmachinelearning,chatgpt,comments,2024-01-23 10:27:58,Should I train my own classification model?,iagovar,False,0.83,4,19dlflm,https://www.reddit.com/r/learnmachinelearning/comments/19dlflm/should_i_train_my_own_classification_model/,7,1706005678.0,"Should I train my own classification model?



I made a category filter for the [events section of this website](https://cometocoruna.com/events) where there's a category selector. The problem is that events are not being properly categorized.

I have to categorize the events myself as there's a mix of sources where they apply different or no categories at all.

I've tried to apply zero-shot models as there's obviously no model trained for my categories out there, with pretty poor results in general. I've also tried with OpenAI function calling and it does better, but I want to avoid calling an API as I don't get any profit from such site (can't run huge LLMs models locally neither, because of HW limitations).

**I've never done this** so I want to know if I'm in the right direction. I'm about to dive into training a distillBERT (apparently easier?) model to classify events with my own dataset, as I can get thousands of events easily.

Another problem is how I'm going to label such events myself, I guess I can go pretty far by going with a combination of hand-labeling, defining a hard-coded rule set and maybe also calling ChatGPT too, I don't have a clear strategy right now.


Is this a good idea?"
170,learnmachinelearning,chatgpt,comments,2023-07-25 12:50:12,Weird chatgpt response,Nice-Elderberry-4490,False,0.33,0,1597mog,https://www.reddit.com/r/learnmachinelearning/comments/1597mog/weird_chatgpt_response/,7,1690289412.0,"Just got this response from the openai davinci model when I prompted:

“Give me a meal plan for the whole week”

Reply: 
##

6 Sc 7 6 Sc rS sc FF c K bI sq q Ja mk du uv aw jX xE ey fE dO qm Cy qL fi lL Xu py tI yN xt zt Dg Fz sL Nu Jd yX gH qL jR nW qq ZN Hd AW MN fO kS qt nY FM eBi rY Ci iR dd Fu yX fk lN xq Ni Cn ll zb kL pc Uu rG vn lm jO jl RF YA zb dM JW BK Dy XI lg xd hI dO tx IE my dP dx dc tX IE 30 AY AY Ci Ui yX Fe Fe ZF Cn Ui Qd zg zf fw Rg ri XP bX ig YU Gf af Fg rq QE NM rW MatkaX

SHARE THIS

##

Kind of scary of you ask me"
171,learnmachinelearning,chatgpt,comments,2023-07-10 15:16:24,"ChatPDF: What ChatGPT Can't Do, This Can!",JunXiangLin,False,0.64,3,14vww3o,https://www.reddit.com/r/learnmachinelearning/comments/14vww3o/chatpdf_what_chatgpt_cant_do_this_can/,7,1689002184.0,"Believe many of people have been using **ChatGPT** for a while, and you are aware that although ChatGPT is powerful, it has the following limitations:

1. Unable to answer questions about events that occurred after **2021**.
2. Unable to directly upload your own data, such as **PDF, Excel, databases**, etc.
3. Inaccurate in performing **mathematical calculations**.

**Langchain** is a recent trending open-source project, which is a framework for developing Large Language Models (LLMs) applications. It supports the following:

1. Connecting LLM models with **external data sources**, such as PDF, Excel, databases, etc.
2. Allowing interaction between LLM models and other tools, such as **Google search**, enabling internet connectivity.
3. Rapid development of LLM model applications.

Today, I'd like to share a project called **ChatPDF**(strickly called **docGPT**, there're some different), built using the Langchain framework. It allows users to upload local documents and ask questions to the LLM model. In this tool, you can ask AI to summarize articles or inquire about any information in the document. Moreover, by leveraging the Langchain Agent functionality, the LLM model can collaborate with the Google Search API, enabling users to ask questions about current topics!

The project provides a detailed guide on how to create your own **docGPT**. It is built using the Langchain framework and Python Streamlit, which is a free and fast way to create online services. As long as you have an OPENAI API KEY, feel free to give it a try!

I encourage everyone to pay attention to the [Langchain open-source project](https://github.com/hwchase17/langchain) and leverage it to achieve tasks that ChatGPT cannot handle.

[Github Repository](https://github.com/Lin-jun-xiang/docGPT-streamlit/tree/main)

[ChatPDF Application](https://docgpt-app.streamlit.app/)

&#x200B;

https://preview.redd.it/q906a7imm5bb1.png?width=2560&format=png&auto=webp&s=acef45049bab805038f876eea56cc371b8a9a83a"
172,learnmachinelearning,chatgpt,comments,2023-05-10 19:49:57,"Does long chain of interactions with Chatgpt (focused on reasoning) can lead to metacognition? I guess other people already discussed this, but googling I could not find a proper conclusion.",pasticciociccio,False,0.4,0,13e17oc,https://www.reddit.com/r/learnmachinelearning/comments/13e17oc/does_long_chain_of_interactions_with_chatgpt/,7,1683748197.0,
173,learnmachinelearning,chatgpt,comments,2023-12-27 11:25:52,What kind of specs would I need to be able to run Mixtral (or similar large models) at home?,Text-Agitated,False,0.75,4,18rym1w,https://www.reddit.com/r/learnmachinelearning/comments/18rym1w/what_kind_of_specs_would_i_need_to_be_able_to_run/,7,1703676352.0,"Hi all, 

TLDR: What kind of hardware is needed for fine-tuning and relatively fast inference? Think our budget can be ~$2-3k as my company would pay for it but I appreciate much cheaper solutions if there are any. I am not looking for the BEST performance but I'd like to at least be able to run 100 inferences an hour or so to be able to scale our applications that use these LLMs.

========================

I've been working as a Python dev for 2+ years professionally and I'm originally self-taught alongside a minor in Data Science from a top 10 university.

All those qualifications and I don't even understand the ""H"" in hardware. I've always been proud that I get things done on a $400 laptop - UNTIL I STARTED USING CHATGPT API and realized these will be integral to development cycle and research automation.

Now my manager doesn't allow me to develop products that would essentially replace a lot of people's mundane work just by using LLMs and the main concern is giving our data away.

So I started thinking:

- How can I equip my hardware to run such models in-house?

- Bonus question: How can I equip myself to further develop my understanding around fine-tuning such models for specific use-cases?

Thank you so much!"
174,learnmachinelearning,chatgpt,comments,2022-12-06 09:51:53,"ChatGPT has me concerned about the future career possibilities as a WebDev, is Machine Learning the way to go?",bobtobno,False,0.67,3,ze244p,https://www.reddit.com/r/learnmachinelearning/comments/ze244p/chatgpt_has_me_concerned_about_the_future_career/,7,1670320313.0,"I have been learning HTML, CSS, JS, Node.js some database stuff, basically a Full-Stack route for the last year.  


I felt skill and portfolio wise i was a month or two away from job ready.  


Now ChatGPT has come out and I'm questioning how many roles there will even be for WebDevs going forward.  


I feel like the future is going to be interacting with and manipulating AI and if you're not skilled at that you're going to be left in the dust.  


I'm a self-taught dev, I have no CS degree, I do have an engineering degree but not a relevant one (Civil).  


I am considering completely changing my plan and going down the ML route.  


I would love to hear peoples thoughts on my thoughts here haha.  


I am of the understanding that it's very difficult to get any work in this area if one doesn't have a CS degree, is this true?   


I am in my 30s so would like to avoid going back to Uni if I can, but if that is the only option then maybe I'll have to."
175,learnmachinelearning,chatgpt,comments,2023-12-02 07:36:15,Need some essential career advice. Please help 🙏,intellectuallogician,False,0.5,0,188yk29,https://www.reddit.com/r/learnmachinelearning/comments/188yk29/need_some_essential_career_advice_please_help/,7,1701502575.0,"Hello.
I am looking for career advice. 
I am a final year computer science student. I have been interested in ML and Data Science for past few months. Am decent/good with stats and linear algebra. 
Also complete andrew ng's ML course.
Haven't built any good projects though. 

I am not from a great college and in `India`. So right out of my bachelor's its incredibly difficult to get any good DS roles that is `if there even are any entry level roles for DS/ML/data engineering`
I was wondering what path should I follow to get into MLE. I am interested in software design as well.  So I found data engineering can be good perhaps.
I dont really see myself getting into core research (as of now- who knows about the future?)

I have been doing full stack dev for the past month (its coz of a uni project).
So what I am wondering is will getting some full stack experience help me in any way to get into data related fields or not apart from any personal projects I make. Basically should i do general software engineering and improve my coding skills first
Or should I just stick to ML and jupyter note book based projects. What else should i do **practically** to get into data engineering roles atleast.
What I have been seeing is not many people are working or ML models from scratch but rather building upon existing licensed models like chatgpt.

Any advice?"
176,learnmachinelearning,chatgpt,comments,2023-01-29 06:00:46,I've discovered a prompt which allows GPT3 and Bloom to act like chatGPT,Alert-Estimate,False,0.24,0,10o0hup,https://www.reddit.com/r/learnmachinelearning/comments/10o0hup/ive_discovered_a_prompt_which_allows_gpt3_and/,6,1674972046.0,"I've discovered a prompt which pretty much allows me to use Bloom ( I'm sure this will work for gpt3 too) like chatGPT, at least the basics... i can pretty much zero shot it for almost anything and it performs really well... I am gonna do few more test then ill share it with you guys

I'll be sharing it in my [discord](https://discord.gg/EtRcMRTh3G) on Thursday 7pm UK time"
177,learnmachinelearning,chatgpt,comments,2023-08-04 14:50:48,Looking for recommendations for an LLM chat that focuses on programming,UpvoteBeast,False,0.63,2,15i24x6,https://www.reddit.com/r/learnmachinelearning/comments/15i24x6/looking_for_recommendations_for_an_llm_chat_that/,6,1691160648.0,"Hi everyone,

I am interested in LLM chat that has been fine tune for programming tasks. I've been using chatGPT for helping me write code but having a local version or other alternatives would be good. Does anyone have any recommendations for an LLM that is fine tuned for programming questions?"
178,learnmachinelearning,chatgpt,comments,2023-12-29 09:33:17,"Beginner here, i want to transition from Electrical engineering to Machine learning for my masters. Please Help.",ColdSpirit117,False,1.0,2,18tjb55,https://www.reddit.com/r/learnmachinelearning/comments/18tjb55/beginner_here_i_want_to_transition_from/,6,1703842397.0,"I know some of the mathematics involved, like multivariable calculus ,linear algebra, optimization principles, Fourier analysis, DFT,FFT etc. Please mention any specific topics and things which i should study to ensure that i am well versed in all the mathematical tools required for learning ML.  


I also know some basic coding in MATLAB and Python, please mention things which i should study in them, to make me write efficient ML code, i have come across some libraries like pandas and numpy during my study but didn't got time to go through them completely.  


Also if you mention some more specific basic and advanced things that i should know about, that would very thankful of you. Also any mentality advice, that i should take while studying this will be very helpful   


My motivations to study ML are these:  
1. I with my group, made a group project about a Drone based waste detection using image processing and machine learning, I understood the gist of the project , but as i didn't  had   
 too much idea about how to code, i just couldn't understand how practically do it, and how to make a data set for it which will, ensure good detection(we were barely able to make something to show to our advisers). This project got me interested in computer vision and ML.   

2.The second reason is because i want to go either in finance sector(How to audit firms check for any fraud or red flags using ML, or prediction analysis for a certain company or firm using ML, for macro -economical analysis and predictions for a state or country, optimal allocation of resources etc.) or in research(can we accurately realize the flow of a turbulent fluid using ML?, Optimizing heavy machines and their usage for better power supply and power quality, and some other research questions on physics and biology), and both require good amount of knowledge of ML.  


3. After watching how ChatGPT and other AI Chatbots that can generate pictures ,text, videos and even code. It has made me think , how does this work, how can i make something like this? It had really fascinated me and made me more interested in this field.  


4.After the advent of jobs in this market, i think most of the IT professionals will be shifting towards this market ,and knowing ML will become a Norm for working.   


Any help is appreciated"
179,learnmachinelearning,chatgpt,comments,2023-03-03 22:46:31,"From the user's perspective, how can I utilize AI/machine learning to have a tool that can help me with researching, summarizing, and maybe even generate short writings?",learnhtk,False,0.33,0,11hhkoh,https://www.reddit.com/r/learnmachinelearning/comments/11hhkoh/from_the_users_perspective_how_can_i_utilize/,6,1677883591.0,"I have an interest in a fairly niche topic.

Even the ChatGPT cannot tell me much about it when I try to ask about it.

I assume it's because there isn't that much data available in English language.

So I am going to translate what I can find in the foreign language that one can expect to find more information in. Hopefully, I will be accumulating the knowledge piece by piece, then hopefully leading to something more complete.

In the end, I want to utilize the AI/machine-learning tool, so that I can ask questions about it based on the data I collected and ""fed"". 

My question is, how do I exactly perform the step of feeding data so that I get a AI/machine-learning tool that can process and understand data?

&#x200B;

Please advise."
180,learnmachinelearning,chatgpt,comments,2023-07-27 11:46:34,LLM Guide [Discussion],torspayorryum,False,0.94,28,15azq0q,https://www.reddit.com/r/learnmachinelearning/comments/15azq0q/llm_guide_discussion/,6,1690458394.0,"Nowadays, If we see over the internet that LLM, chatgpt , llma etc are the trending topics and are being discussed. My question is that anyone can help me where to start studying about these topics from scratch ? BERT, Transformer etc all I want to understand everything.

It would be good if you help me out.

Thanks"
181,learnmachinelearning,chatgpt,comments,2023-11-14 17:31:28,Worth or learning ai,Particular_Tea2307,False,0.43,0,17v7e80,https://www.reddit.com/r/learnmachinelearning/comments/17v7e80/worth_or_learning_ai/,6,1699983088.0,Hello is it worth it to learn ai as an individual when there is huge companies with huge models (chatgpt ext…) i mean is it too late to become an ai engineer ?
182,learnmachinelearning,chatgpt,comments,2023-05-15 21:21:01,Resource for creating your own personal ChatGPT tailored to your own data,rajatarya,False,0.78,16,13ikxwt,https://www.reddit.com/r/learnmachinelearning/comments/13ikxwt/resource_for_creating_your_own_personal_chatgpt/,6,1684185661.0,"Hey everyone,  


I was trying to create a personal ChatGPT that can answer questions and create expert content based on an existing dataset. I thought there are tons of applications for this, so [I created a workshop](https://app.livestorm.co/xethub/mygpt-free-workshop-build-a-chatgpt-clone-tailored-to-your-data?type=detailed&utm_source=reddit&utm_medium=social&utm_campaign=openaireddit) so you can create your own app - I’m calling it “MyGPT”.  


In this workshop I’ll be covering:

* How to create a Generative AI app using the DaVinci model (the same one used by ChatGPT) 
* How a Generative AI application is structured (the tech stack)
* Integrating your own data into a Large Language Model (LLM)
* Getting started with XetHub (similar to GitHub but easier for ML models)
* Create a Python app that uses Gradio & LangChain

If you’d like to check it out, [sign up here](https://app.livestorm.co/xethub/mygpt-free-workshop-build-a-chatgpt-clone-tailored-to-your-data?type=detailed&utm_source=reddit&utm_medium=social&utm_campaign=openaireddit)!"
183,learnmachinelearning,chatgpt,comments,2022-12-10 21:07:23,Is there a model with comparable capabilities to ChatGPT but open-source?,que1112,False,0.6,1,zi3kjb,https://www.reddit.com/r/learnmachinelearning/comments/zi3kjb/is_there_a_model_with_comparable_capabilities_to/,6,1670706443.0,
184,learnmachinelearning,chatgpt,comments,2023-01-17 02:41:34,what are some good ChatGPT theme backgrounds I should add as options? drop links please,QuestionAnxious,False,0.56,2,10e0jti,https://i.redd.it/sj7g0wqenica1.png,5,1673923294.0,
185,learnmachinelearning,chatgpt,comments,2023-04-11 19:57:06,Just created a chat window using ChatGPT that logs conversations and runs locally!,gnuconcepts,False,0.67,2,12ivjb4,https://www.reddit.com/r/learnmachinelearning/comments/12ivjb4/just_created_a_chat_window_using_chatgpt_that/,5,1681243026.0,"Hey folks! I just wanted to share a simple chat window that I created using ChatGPT. You can find the link to the Github repository here: [https://github.com/gnuconcepts/ChatWindowGPT](https://github.com/gnuconcepts/ChatWindowGPT)

This one-file script allows you to keep track of conversations and logs them locally. I created this because I wanted an alternative way to access ChatGPT when the website is overloaded, and didn't want to shell out $20/month for it.

Check out the short Youtube video I created to see it in action: [https://www.youtube.com/watch?v=2nWr4qRzmWA](https://www.youtube.com/watch?v=2nWr4qRzmWA). Thanks for checking it out, and please let me know if you have any feedback or questions!"
186,learnmachinelearning,chatgpt,comments,2023-02-04 23:07:26,"Having trouble understanding positional encoding, asked ChatGPT for help. Are these responses correct?",Daddys_Lil_Grandpa,False,0.58,3,10tu6dw,https://i.redd.it/l80oo9zw69ga1.png,5,1675552046.0,
187,learnmachinelearning,chatgpt,comments,2023-01-17 07:51:07,DeepMind To Launch ChatGPT Rival Sparrow Soon,vadhavaniyafaijan,False,0.96,122,10e6h7j,https://www.theinsaneapp.com/2023/01/deepmind-to-launch-chatgpt-rival-sparrow.html,5,1673941867.0,
188,learnmachinelearning,chatgpt,comments,2023-03-31 06:16:58,"If ChatGPT itself cannot be fine-tuned, what would bf the benefit of using the GPT3 offering of OpenAI vs my own?",Proxify,False,0.88,6,127c5iz,https://www.reddit.com/r/learnmachinelearning/comments/127c5iz/if_chatgpt_itself_cannot_be_finetuned_what_would/,5,1680243418.0,"Sorry, I'm somewhat new to this space and I'm reading about it and looking at the documentation from OpenAI.

From what I can tell, only their base models are available to fine-tune which, as far as I understand, would leave me in a situation in which fine-tuning any other GPT3 model would be comparable (vs their ""DaVinci"" model for instance).

Am I missing something here? Basically I'm wondering, other than their infrastructure (which is nothing to scoff at) why would I use their fine-tuning if the end result won't talk to the user as ChatGPT would."
189,learnmachinelearning,chatgpt,comments,2023-12-31 15:36:02,"If LLMs like ChatGPT can learn to predict the next token, could teaching the LLMs the reward function itself so it can predict it's average success across n-tokens every so often (every 10 or 20 tokens) lead to increases in performance across semantic understanding?",Rachel_Roark_212,False,0.86,14,18va65q,https://www.reddit.com/r/learnmachinelearning/comments/18va65q/if_llms_like_chatgpt_can_learn_to_predict_the/,4,1704036962.0,"Basically have two output tensors and two input tensors. The first  input tensor is the previous token sequence, the second input tensor is  the token out of future tokens number (so for example: 5 out of 30  tokens in the batch (not to be confused with previous token sequence  which can contain 1, 200, or 12359, etc tokens)). The token-length of  the batch can be modified on demand. The second output tensor outputs  the predicted average success of the batch, while the first output  tensor outputs the next token.   

The average success of a batch is determined by weighting of three  different reward functions: how much did it exactly replicate the  original batch from the document, for whatever words it did not  replicate: how synonymous are the phrases and terms, and a final reward  function by a grader-GPT: ""if the words produced by the training-GPT are  superior in output to the original"" (the grader-GPT can also increase  the batch size if it ""determines"" that there was a superior output that  wasn't finished yet).   

But importantly the training-GPT uses it's prediction of a batch's  success (simulating the external reward function) as it's own reward  function for the batch until it reaches the end of the batch of tokens,  then a comparison of it's internal reward function with the actual  external reward functions leads to backpropagating the network based on  the error level.   

I'm a big newbie so this is just an idea I had. I think some working memory system could be added in to, I saw some Arxiv describing various methods of implementing ones from residual activations to an external working memory system."
190,learnmachinelearning,chatgpt,comments,2023-01-03 16:05:30,"How do artificial intelligence programs work and learn? Do they rely solely on the code provided by the programmer, or do they have the ability to learn and adapt on their own?",DismalCall5534,False,0.5,0,102bszz,https://www.reddit.com/r/learnmachinelearning/comments/102bszz/how_do_artificial_intelligence_programs_work_and/,4,1672761930.0,"For example, in the ChatGPT system, it is stated that ""Our goal is to make AI systems more natural and safe to interact with. Your feedback will help us improve.""

However, if the artificial intelligence can learn on its own, how or why do the ChatGPT developers write code to improve the system? I hope my question is clear and understood.

Thank you in advance"
191,learnmachinelearning,chatgpt,comments,2023-07-06 12:21:54,USA Company Valuation w/ ChatGPT,MandM-DataScience,False,0.6,1,14s7lck,https://www.reddit.com/r/learnmachinelearning/comments/14s7lck/usa_company_valuation_w_chatgpt/,4,1688646114.0,"Hi everyone!

We just published our new AI project. Take a look at our python notebook and our example GOOGL valuation ;)

[https://www.kaggle.com/code/mandmdatascience/usa-company-valuation-with-chatgpt](https://www.kaggle.com/code/mandmdatascience/usa-company-valuation-with-chatgpt)

&#x200B;

[GOOGL Company Dashboard \(Tableau\)](https://preview.redd.it/ais9ff4p7cab1.png?width=989&format=png&auto=webp&s=299ab8984e0e6ac84112353d1c94b5df719e673f)

Technologies used:

* Python
* OpenAI models
* Valuation model (Prof. Damodaran)
* Tableau
* PowerBI

Every and each comment / feedback is greatly appreciated!

Thank you!  
M&M"
192,learnmachinelearning,chatgpt,comments,2023-08-05 05:23:38,"Why on more ""complicated"" requests, ChatGPT takes much longer to respond if it's a neural network that takes in a fixed number of inputs?",Legitimate_Bison3756,False,0.78,5,15imvi6,https://www.reddit.com/r/learnmachinelearning/comments/15imvi6/why_on_more_complicated_requests_chatgpt_takes/,4,1691213018.0,"Why on more ""complicated"" requests, ChatGPT takes longer if it's a neural network that takes in a fixed number of inputs? Or is it an entirely different architecture from what I'm thinking of (Fixed number of characters with padded zeros if the input is shorter)?"
193,learnmachinelearning,chatgpt,comments,2023-05-05 22:39:41,Using ChatGPT for assigning ontology to KMeans labels,lukaszluk,False,0.6,1,1393uoa,https://www.reddit.com/r/learnmachinelearning/comments/1393uoa/using_chatgpt_for_assigning_ontology_to_kmeans/,4,1683326381.0,"Sharing a cool technique that you can use to assign categories/titles/ontology to your Kmeans results.

My use case involves text data with descriptions so it won’t be applicable in every situation, but it can definitely give inspiration to anyone.

I had podcast transcripts that were chunked into sections (\~3000 text documents). These sections were then transformed into summaries with [LangChain](https://langchain.com/) and [OpenAI API](https://platform.openai.com/docs/introduction). Finally, I embedded the summaries using OpenAI embeddings. Then I ran KMeans (k=30) and got labels with section names:

&#x200B;

https://preview.redd.it/zepzijatb3ya1.png?width=633&format=png&auto=webp&s=23313df60d798636eb6a02392e567bd4ace3587c

In order to avoid exceeding the maximal number of tokens in the context window (4096 tokens), I sampled the data frame to contain 200 segment names with 5 selected labels. Then I iteratively moved to the next labels, i.e.:

1. iteration — labels from 0 to 4

* 2. iteration — labels from 5 to 9
* …
* 6. iteration — labels from 25 to 29

This is an example output from our ontology detector:

&#x200B;

https://preview.redd.it/k3rfqo7ub3ya1.png?width=653&format=png&auto=webp&s=792944cb82b6e0d2fe8e0f8a763f0f1fbcabb57b

After iterating through all labels I noticed that some of the categories and keywords overlap. Moreover, it would be hard to navigate through so many categories.

That’s why I asked ChatGPT to group overlapping categories:

&#x200B;

https://preview.redd.it/efgrjukvb3ya1.png?width=676&format=png&auto=webp&s=f2552e7f51f63ccb668e13858d49f424baea2475

Sharing the prompts in the comment section! You can check out the code here: [https://github.com/DataScienceDisciple/hubermanlab-qa/blob/main/notebooks/04\_summary-analysis.ipynb](https://github.com/DataScienceDisciple/hubermanlab-qa/blob/main/notebooks/04_summary-analysis.ipynb)"
194,learnmachinelearning,chatgpt,comments,2023-06-05 02:34:40,Is ChatGPT “overrated”,Enough_Wishbone7175,False,0.64,4,140zegp,https://www.reddit.com/r/learnmachinelearning/comments/140zegp/is_chatgpt_overrated/,4,1685932480.0,"I have seen papers on algos like GOAT that used a much smaller transformer model and out performed GPT in arithmetic. Considering the efficient computing and lack of general threat these smaller more tuned models pose, would aggregating them like a tool kit make more sense? Like ChatGPT is great for general use but making task related scaled down LLMs makes more sense to me in the work place."
195,learnmachinelearning,chatgpt,comments,2023-09-30 15:01:31,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.98,84,16w93bx,https://www.reddit.com/r/learnmachinelearning/comments/16w93bx/this_week_in_ai_all_the_major_ai_developments_in/,4,1696086091.0,"1. **Meta AI** presents **Emu**, a quality-tuned latent diffusion model for generating highly aesthetic images. Emu significantly outperforms SDXLv1.0 on visual appeal.
2. **Meta AI** researchers present a series of long-context LLMs with context windows of up to 32,768 tokens. LLAMA 2 70B variant surpasses gpt-3.5-turbo-16k’s overall performance on a suite of long-context tasks.
3. **Abacus AI** released a larger 70B version of **Giraffe**. Giraffe is a family of models that are finetuned from base Llama 2 and have a larger context length of 32K tokens\].
4. **Meta** announced:  

   1. **Meta AI** \- a new AI assistant users can interact with on WhatsApp, Messenger and Instagram. Will also be available on Ray-Ban Meta smart glasses and Quest 3, Meta’s mixed reality headset.
   2. **AI stickers** that enable users to generate customized stickers for chats and stories using text. Powered by Llama 2 and the new foundational model for image generation, Emu.
   3. **28 AI characters**, each with a unique personality that users can message on WhatsApp, Messenger, and Instagram.
   4. New AI editing tools, **restyle** and **backdrop** in Instagram.
   5. **AI Studio** \- a platform that supports the creation of custom AIs by coders and non-coders alike.
5. **Cerebras** and **Opentensor** released Bittensor Language Model, ‘**BTLM-3B-8K**’, a new 3 billion parameter open-source language model with an 8k context length trained on 627B tokens of SlimPajama. It outperforms models trained on hundreds of billions more tokens and achieves comparable performance to open 7B parameter models. The model needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models and is available with an Apache 2.0 license for commercial use.
6. **OpenAI** is rolling out, over the next two weeks, new voice and image capabilities in ChatGPT enabling ChatGPT to understand images, understand speech and speak. The new voice capability is powered by a new text-to-speech model, capable of generating human-like audio from just text and a few seconds of sample speech. .
7. **Mistral AI**, a French startup, released its first 7B-parameter model, **Mistral 7B**, which outperforms all currently available open models up to 13B parameters on all standard English and code benchmarks. Mistral 7B is released in Apache 2.0, making it usable without restrictions anywhere.
8. **OpenAI** has returned the ChatGPT browsing feature for Plus subscribers, enabling ChatGPT to access internet for current information. It was disabled earlier as users were able to deploy it to bypass the paywalls of leading news publishers.
9. **Microsoft** has released **AutoGen** \- an open-source framework that enables development of LLM applications using multiple agents that can converse with each other to solve a task. Agents can operate in various modes that employ combinations of LLMs, human inputs and tools.
10. **LAION** released **LeoLM**, the first open and commercially available German foundation language model built on Llama-2
11. Researchers from **Google** and **Cornell University** present and release code for DynIBaR (Neural Dynamic Image-Based Rendering) - a novel approach that generates photorealistic renderings from complex, dynamic videos taken with mobile device cameras, overcoming fundamental limitations of prior methods and enabling new video effects.
12. **Cloudflare** launched **Workers AI** (an AI inference as a service platform), **Vectorize** (a vector Database) and **AI Gateway** with tools to cache, rate limit and observe AI deployments. Llama2 is available on Workers AI.
13. **Amazon** announced the general availability of **Bedrock**, its service that offers a choice of generative AI models from Amazon itself and third-party partners through an API.
14. **Google** announced it’s giving website publishers a way to opt out of having their data used to train the company’s AI models while remaining accessible through Google Search.
15. **Spotify** has launched a pilot program for AI-powered voice translations of podcasts in other languages - in the podcaster’s voic. It uses OpenAI’s newly released voice generation model.
16. **Getty Images** has launched a generative AI image tool, ‘**Generative AI by Getty Images**’, that is ‘commercially‑safe’. It’s powered by Nvidia Picasso, a custom model trained exclusively using Getty’s images library.
17. **Optimus**, Tesla’s humanoid robot, can now sort objects autonomously and do yoga. Its neural network is trained fully end-to-end.
18. **Amazon** will invest up to $4 billion in Anthropic. Developers and engineers will be able to build on top of Anthropic’s models via Amazon Bedrock.
19. **Google Search** indexed shared Bard conversational links into its search results pages. Google says it is working on a fix.
20. **Pika** Labs' text-to-video tool now lets users encrypt a message in a video\].

My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
196,learnmachinelearning,chatgpt,comments,2023-05-12 12:20:45,"[D] Is investigating models like BERT, T5, GPT-2 still interesting research area?",Final-Tackle7275,False,1.0,1,13fjl5g,https://www.reddit.com/r/learnmachinelearning/comments/13fjl5g/d_is_investigating_models_like_bert_t5_gpt2_still/,2,1683894045.0," I am currently working on my PhD, I started my research before ChatGPT blow up. I was working on the explainability of these models and their capabilities. Currently, I feel like these types of investigations are dead with ChatGPT and all these large models doing everything possible. What do you think? Do you think that work on these ""small"" models is outdated, if not why ?"
197,learnmachinelearning,chatgpt,comments,2022-12-28 17:37:46,chatGPT peeps- anyone else learn new stuff best by actually building something?,bruclinbrocoli,False,0.72,6,zxfnga,https://www.reddit.com/r/learnmachinelearning/comments/zxfnga/chatgpt_peeps_anyone_else_learn_new_stuff_best_by/,4,1672249066.0,"[This intro to chatGPT](https://buildspace.so/notes/intro-to-chatgpt) has some cool (free) challenges at the end to build a telegram bot, a business email generator, or a writing assistant.

What else have people found to learn bout chatGPT that's not just theory?

&#x200B;

https://preview.redd.it/smxv4mzldo8a1.png?width=1026&format=png&auto=webp&s=43081abbfcad449817e520b5e92ba599a18a1525"
198,learnmachinelearning,chatgpt,comments,2023-03-19 13:01:27,Forecasting vehicle speed with accelerometer and gyroscope data,Arraiz0,False,0.76,2,11vktrd,https://www.reddit.com/r/learnmachinelearning/comments/11vktrd/forecasting_vehicle_speed_with_accelerometer_and/,4,1679230887.0,"Hello dear redditors. For a project that I'm working on I need to develop a system to forecast a vehicle speed ( for the moment) using an Arduino sensor data with accelerometer and gyroscope data. The project needs to be built with some forecasting technique and using deep learning. I have found some examples that estimate while a human body is moving a human activity recognition but is not the same
I used chatgpt to find some books articles or tutorials for this, but is horrible finding this information imho.

So my question are simple:
Where do I find some book, article or tutorial realted to this?

Where can I found usefull datasets for this task?

In the case of making my own dataset for this is there any recommendations or tutorial?


Thanks in advance !"
199,learnmachinelearning,chatgpt,comments,2023-05-14 16:11:21,Using chatgpt in a political game to control nations?,Marksh11,False,0.4,0,13hg6is,https://www.reddit.com/r/learnmachinelearning/comments/13hg6is/using_chatgpt_in_a_political_game_to_control/,3,1684080681.0,"
Greetings, everyone!

I'm currently working on the development of a political game that allows players to assume the roles of nations, much like in the popular game Hearts of Iron. As part of this project, I've been contemplating the idea of having non-player-controlled nations operated by GPT through the utilization of JSON output. However, I'm concerned about the potential cost in a production environment. Should I instead consider implementing a basic algorithm for those server owners who are unable to afford the GPT setup? And make it that individual owners have to provide their own api key. Your insights on this matter would be greatly appreciated."
200,learnmachinelearning,chatgpt,relevance,2023-04-04 14:34:29,Working with chatGPT,macronancer,False,0.97,610,12bkzjv,https://i.redd.it/5uwfzjh4pvra1.png,22,1680618869.0,
201,learnmachinelearning,chatgpt,relevance,2023-12-17 13:15:47,I can't stop using ChatGPT and I hate it.,t0hli,False,0.63,24,18kh4n5,https://www.reddit.com/r/learnmachinelearning/comments/18kh4n5/i_cant_stop_using_chatgpt_and_i_hate_it/,108,1702818947.0,"I'm trying to learn various topics like Machine Learning and Robotics etc., and I'm kinda a beginner in programming.

For any topic and any language, my first instinct is to

1. go to ChatGPT,
2. write down whatever I need my code to do,
3. copy paste the code
4. if it doesn't give out good results, ask ChatGPT to fix whatever it's done wrong
5. repeat until I get satisfactory result

I hate it, but I don't know what else to do.

I think of asking Google what to do, but then I won't get the exact answer I'm looking for, so I go back to ChatGPT so I can get exactly what I want. I don't fully understand what the GPT code does, I get the general gist of it and say ""Yeah that's what I would do, makes sense"", but that's it.

If I tried to code whatever GPT printed out, I wouldn't get anywhere.

I know I need to be coding more, but I have no idea where to start from, and why I need to code when ChatGPT can do it for me anyway. I'm not defending this idea, I'm just trying to figure out how I can code myself.

I'd appreciate your thoughts and feedback."
202,learnmachinelearning,chatgpt,relevance,2022-12-28 04:37:21,University Professor Catches Student Cheating With ChatGPT,vadhavaniyafaijan,False,0.94,146,zx0ep0,https://www.theinsaneapp.com/2022/12/university-professor-catches-student-cheating-with-chatgpt.html,108,1672202241.0,
203,learnmachinelearning,chatgpt,relevance,2023-02-19 13:55:13,ChatGPT History,eforebrahim,False,0.86,255,116au66,https://i.redd.it/dv8cfj0nz6ja1.jpg,27,1676814913.0,
204,learnmachinelearning,chatgpt,relevance,2023-04-26 06:23:17,Hugging Face Releases Free Alternative To ChatGPT,vadhavaniyafaijan,False,0.98,392,12z8n4e,https://www.theinsaneapp.com/2023/04/free-alternative-to-chatgpt.html,35,1682490197.0,
205,learnmachinelearning,chatgpt,relevance,2023-06-28 12:29:48,"Intern tasked to make a ""local"" version of chatGPT for my work",Assasinshock,False,0.97,153,14l887h,https://www.reddit.com/r/learnmachinelearning/comments/14l887h/intern_tasked_to_make_a_local_version_of_chatgpt/,104,1687955388.0,"Hi everyone,

I'm currently an intern at a company, and my mission is to make a proof of concept of an conversational AI for the company.They told me that the AI needs to be trained already but still able to get trained on the documents of the company, the AI needs to be open-source and needs to run locally so no cloud solution.

The AI should be able to answers questions related to the company, and tell the user which documents are pertained to their question, and also tell them which departement to contact to access those files.

For this they have a PC with an I7 8700K, 128Gb of DDR4 RAM and an Nvidia A2.

I already did some research and found some solution like localGPT and local LLM like vicuna etc, which could be usefull, but i'm really lost on how i should proceed with this task. (especially on how to train those model)

That's why i hope you guys can help me figure it out. If you have more questions or need other details don't hesitate to ask.

Thank you.  


Edit : They don't want me to make something like chatGPT, they know that it's impossible. They want a prototype that can answer question about their past project. "
206,learnmachinelearning,chatgpt,relevance,2023-10-12 20:36:53,ChatGPT vision feature is really useful for understanding research papers!,nxtboyIII,False,0.86,192,176gs37,https://i.redd.it/xe94y8hf1utb1.png,42,1697143013.0,
207,learnmachinelearning,chatgpt,relevance,2024-01-23 05:38:28,ChatGPT with multiple PDFs hallucinates,BigYesterday2785,False,0.62,2,19dh7pj,https://www.reddit.com/r/learnmachinelearning/comments/19dh7pj/chatgpt_with_multiple_pdfs_hallucinates/,13,1705988308.0,"I just don’t get it. I am building a ChatGPT for multiple PDFs.

Now I have **multiple PDFs** which I have created embeddings of and saved in MongoDB database.

Now when a user asks a specific question, I get **multiple outputs from GPT-3.5** and a lot of it does **not make sense**. Even though **real answer is still in there.**

Is there an API i can use to go through all data of PDFs and figure out **which one is relevant and then I run GPT-3.5 only on that specific part of PDFs**"
208,learnmachinelearning,chatgpt,relevance,2023-02-01 04:14:49,ChatGPT Extension for VSCode,Dense_Dimension_913,False,0.96,152,10qk8en,https://www.reddit.com/r/learnmachinelearning/comments/10qk8en/chatgpt_extension_for_vscode/,22,1675224889.0,"Created a ChatGPT extension for VSCode to help programmers understand  and read code more easily and also other features. To start, simply highlight a piece of code  and click on the plus icon on the  left to open up a chat and start  talking with ChatGPT, or Codex, or text-davinci-003. You can choose the  model you want to use in the settings. More details in the links below. I  really hope this extension can be useful to many people out there.  Please give it a try and let me know if you guys see any bugs or if you  like the extension. Thanks!

&#x200B;

https://i.redd.it/28zslxm06ifa1.gif

[VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=AndrewZhang.scribeai)"
209,learnmachinelearning,chatgpt,relevance,2024-02-07 06:23:52,How does ChatGPT inference work?,eplemost8345,False,0.67,1,1akwi85,https://www.reddit.com/r/learnmachinelearning/comments/1akwi85/how_does_chatgpt_inference_work/,0,1707287032.0,"How does ChatGPT gives user the unfinished sentence in real time and yet managed to finish sentence so fast? 
In my understanding, offloading tensors from gpu to cpu takes long time and if the inference is done in gpu, don't OpenAI need to offload the predicted token from gpu to cpu for them to be able to send the intermediate result to users?

Or do I have a misunderstanding and it doesn't take long time to offload from gpu to cpu?

Thanks in advance!"
210,learnmachinelearning,chatgpt,relevance,2024-01-23 05:38:31,ChatGPT with multiple PDFs hallucinates,BigYesterday2785,False,0.33,0,19dh7qm,https://www.reddit.com/r/learnmachinelearning/comments/19dh7qm/chatgpt_with_multiple_pdfs_hallucinates/,0,1705988311.0,"I just don’t get it. I am building a ChatGPT for multiple PDFs.

Now I have **multiple PDFs** which I have created embeddings of and saved in MongoDB database.

Now when a user asks a specific question, I get **multiple outputs from GPT-3.5** and a lot of it does **not make sense**. Even though **real answer is still in there.**

Is there an API i can use to go through all data of PDFs and figure out **which one is relevant and then I run GPT-3.5 only on that specific part of PDFs**"
211,learnmachinelearning,chatgpt,relevance,2023-07-25 12:50:12,Weird chatgpt response,Nice-Elderberry-4490,False,0.4,0,1597mog,https://www.reddit.com/r/learnmachinelearning/comments/1597mog/weird_chatgpt_response/,7,1690289412.0,"Just got this response from the openai davinci model when I prompted:

“Give me a meal plan for the whole week”

Reply: 
##

6 Sc 7 6 Sc rS sc FF c K bI sq q Ja mk du uv aw jX xE ey fE dO qm Cy qL fi lL Xu py tI yN xt zt Dg Fz sL Nu Jd yX gH qL jR nW qq ZN Hd AW MN fO kS qt nY FM eBi rY Ci iR dd Fu yX fk lN xq Ni Cn ll zb kL pc Uu rG vn lm jO jl RF YA zb dM JW BK Dy XI lg xd hI dO tx IE my dP dx dc tX IE 30 AY AY Ci Ui yX Fe Fe ZF Cn Ui Qd zg zf fw Rg ri XP bX ig YU Gf af Fg rq QE NM rW MatkaX

SHARE THIS

##

Kind of scary of you ask me"
212,learnmachinelearning,chatgpt,relevance,2023-01-31 16:17:42,ChatGPT Crossed 10 Million Daily Active Users In Just 40 Days,vadhavaniyafaijan,False,0.95,214,10q34ra,https://www.theinsaneapp.com/2023/01/chatgpt-crossed-10-million-user.html,32,1675181862.0,
213,learnmachinelearning,chatgpt,relevance,2024-02-11 09:08:25,Why Gemini Ultra Outshines ChatGPT 4: Performance Comparison,UseCreative4765,False,0.5,0,1ao3w9h,https://youtu.be/eu20jiYenNw?si=sdBOWE_VajUymbEg,0,1707642505.0,
214,learnmachinelearning,chatgpt,relevance,2023-07-21 15:54:55,Best ChatGPT Alternative Platforms (7 Jaw-Dropping ChatGPT Alternatives You MUST See to Believe!),Chisom1998_,False,0.17,0,155rxo2,https://youtu.be/QvFo9EfdNxQ,0,1689954895.0,
215,learnmachinelearning,chatgpt,relevance,2023-10-03 05:57:38,"Can someone share the ChatGPT bonus of Machine Learning A-Z™: AI, Python & R + ChatGPT Bonus [2023]",Wildest_Dreams-,False,1.0,3,16yjz96,https://www.reddit.com/r/learnmachinelearning/comments/16yjz96/can_someone_share_the_chatgpt_bonus_of_machine/,1,1696312658.0,"I could not complete it in time to be eligible to get the bonus of [Machine Learning A-Z™: AI, Python & R + ChatGPT Bonus \[2023\]](https://www.udemy.com/course/machinelearning/).  
It would be great if you could attach a link to download the same."
216,learnmachinelearning,chatgpt,relevance,2023-07-22 16:08:08,Mysterious Algorithm: backpropagation: chatGPT explained,KSSolomon,False,0.85,18,156o1ta,https://www.reddit.com/gallery/156o1ta,10,1690042088.0,
217,learnmachinelearning,chatgpt,relevance,2023-02-16 10:29:31,OpenAI Has Purchased AI.Com For ChatGPT For $11M,vadhavaniyafaijan,False,0.93,208,113nizs,https://www.theinsaneapp.com/2023/02/openai-purchased-ai-com-domain.html,23,1676543371.0,
218,learnmachinelearning,chatgpt,relevance,2023-11-29 18:55:53,What do you think ChatGPT does when you ask it to do Sentiment Analysis?,PinstripePride97,False,0.79,14,186x2t3,https://www.reddit.com/r/learnmachinelearning/comments/186x2t3/what_do_you_think_chatgpt_does_when_you_ask_it_to/,29,1701284153.0,Could be silly question but if you give a sentence to ChatGPT and ask it to give a sentiment analysis what do you think it does?
219,learnmachinelearning,chatgpt,relevance,2023-01-25 01:15:22,How ChatGPT is Trained,ariseff,False,0.98,127,10km46l,https://youtu.be/VPRSBzXzavo,8,1674609322.0,
220,learnmachinelearning,chatgpt,relevance,2023-02-24 06:26:36,Is there a way to easily train ChatGPT or GPT on custom knowledge?,senttoschool,False,0.99,115,11akisx,https://www.reddit.com/r/learnmachinelearning/comments/11akisx/is_there_a_way_to_easily_train_chatgpt_or_gpt_on/,46,1677219996.0,"My company has internal documents. It'd be nice to be able to have GPT look over it, and then I can ask it questions on the internal documents."
221,learnmachinelearning,chatgpt,relevance,2023-03-16 16:51:03,Introducing OpenChatKit - The Open-Source Alternative to ChatGPT,kingabzpro,False,0.98,206,11szhsh,https://www.reddit.com/r/learnmachinelearning/comments/11szhsh/introducing_openchatkit_the_opensource/,21,1678985463.0,"Hey everyone! I'm excited to share my latest article about a new open-source technology called OpenChatKit.

For those who work in NLP, you're probably familiar with ChatGPT - a powerful language model that can perform various natural language processing tasks. However, ChatGPT is not open-source, which limits its accessibility and customizability.

OpenChatKit, on the other hand, is an open-source alternative to ChatGPT that provides users with similar NLP capabilities while allowing for more customization and control. With OpenChatKit, users can train their own models and fine-tune them to their specific use cases.

In my article, I dive into the features of OpenChatKit, the Instruction-tuned Large Language Model, and the Limitations of the Model.

If you're interested in learning more about OpenChatKit and how it can enhance your NLP workflows, check out my article [OpenChatKit: Open-Source ChatGPT Alternative ](https://www.kdnuggets.com/2023/03/openchatkit-opensource-chatgpt-alternative.html). I'd love to hear your thoughts and answer any questions you may have."
222,learnmachinelearning,chatgpt,relevance,2023-06-05 02:34:40,Is ChatGPT “overrated”,Enough_Wishbone7175,False,0.64,4,140zegp,https://www.reddit.com/r/learnmachinelearning/comments/140zegp/is_chatgpt_overrated/,4,1685932480.0,"I have seen papers on algos like GOAT that used a much smaller transformer model and out performed GPT in arithmetic. Considering the efficient computing and lack of general threat these smaller more tuned models pose, would aggregating them like a tool kit make more sense? Like ChatGPT is great for general use but making task related scaled down LLMs makes more sense to me in the work place."
223,learnmachinelearning,chatgpt,relevance,2023-09-24 09:27:37,Summarise Arxiv papers using chatGPT,Mosh_98,False,1.0,3,16qtcwn,https://www.reddit.com/r/learnmachinelearning/comments/16qtcwn/summarise_arxiv_papers_using_chatgpt/,1,1695547657.0,"Hi,

Reading papers from arxiv or other sources can be slightly time consuming.

Found some tools that could help make the process slightly faster.

[https://www.youtube.com/watch?v=9d9rdO9XYpA&ab\_channel=MoslehMahamud](https://www.youtube.com/watch?v=9d9rdO9XYpA&ab_channel=MoslehMahamud)

&#x200B;

&#x200B;"
224,learnmachinelearning,chatgpt,relevance,2023-10-13 12:13:06,Factor Influencing Adoption Intention of ChatGPT,Fit_Market_3607,False,1.0,2,176xbil,https://www.reddit.com/r/learnmachinelearning/comments/176xbil/factor_influencing_adoption_intention_of_chatgpt/,0,1697199186.0,"Hello,

&#x200B;

I am an information systems student currently conducting research for my undergraduate thesis on the factors that influence people's adoption intention of ChatGPT, as well as identifying the factors that may be holding them back. These factors include people's concerns about potential negative impacts of ChatGPT, such as increased unemployment and the spread of misinformation. Your participation in this study is crucial as it will provide valuable insights to help us understand how ChatGPT can be improved to meet users' needs.

&#x200B;

Please note that I am not affiliated with OpenAI, no identifying information will be collected during the survey, and all responses will be kept confidential. The survey should take approximately 10 to 15 minutes to complete, and participation is voluntary. You may withdraw from the survey at any time, and there are no known risks associated with participating.

&#x200B;

If you are interested in learning more about the study, please follow the link below. 

&#x200B;

[https://docs.google.com/forms/d/e/1FAIpQLSf5HIfXHppMuTR63x00i4OuRAtM5Ti6EGybd-HuI1kmK06VPw/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSf5HIfXHppMuTR63x00i4OuRAtM5Ti6EGybd-HuI1kmK06VPw/viewform?usp=sf_link)

&#x200B;

Thank you for taking the time to contribute to our research study. Your participation is greatly appreciated!"
225,learnmachinelearning,chatgpt,relevance,2023-01-25 16:33:21,I wrote a book using ChatGPT to teach ChatGPT,anefiox,False,0.5,0,10l2ucg,https://www.reddit.com/r/learnmachinelearning/comments/10l2ucg/i_wrote_a_book_using_chatgpt_to_teach_chatgpt/,0,1674664401.0,"I found ChatGPT to be quite repetitive and it would not always listen to your prompts. So using it actually ended up creating more content for the book. It's a great way of getting a skeleton of a book ready fast but I don't think it's ready yet for creating full books. I hope to write an article about the process once I have my website up and running. Here's a link to the book if you're interested:  


[https://www.amazon.com/Mastering-ChatGPT-Comprehensive-Engineering-Fine-tuning-ebook/dp/B0BSR5HN4X/](https://www.amazon.com/Mastering-ChatGPT-Comprehensive-Engineering-Fine-tuning-ebook/dp/B0BSR5HN4X/)"
226,learnmachinelearning,chatgpt,relevance,2023-02-27 13:42:55,Can you fine-tune chatGPT in your data as of now?,Melodic_Stomach_2704,False,0.91,58,11dc5b4,https://www.reddit.com/r/learnmachinelearning/comments/11dc5b4/can_you_finetune_chatgpt_in_your_data_as_of_now/,33,1677505375.0, I know that model is not publicly available so it's not possible to do it locally. But can you train or fine-tune chatGPT on your data using their API? I see many misguiding articles on the internet that are fine-tuning other GPT models claiming chatGPT.
227,learnmachinelearning,chatgpt,relevance,2023-09-28 09:17:52,Factor Influencing Adoption Intention of ChatGPT,BitAccomplished7242,False,0.67,1,16ub460,https://www.reddit.com/r/learnmachinelearning/comments/16ub460/factor_influencing_adoption_intention_of_chatgpt/,0,1695892672.0,"Hello,

&#x200B;

I am an information systems student currently conducting research for my undergraduate thesis on the factors that influence people's adoption intention of ChatGPT, as well as identifying the factors that may be holding them back. These factors include people's concerns about potential negative impacts of ChatGPT, such as increased unemployment and the spread of misinformation. Your participation in this study is crucial as it will provide valuable insights to help us understand how ChatGPT can be improved to meet users' needs.

&#x200B;

Please note that I am not affiliated with OpenAI, no identifying information will be collected during the survey, and all responses will be kept confidential. The survey should take approximately 10 to 15 minutes to complete, and participation is voluntary. You may withdraw from the survey at any time, and there are no known risks associated with participating.

&#x200B;

If you are interested in learning more about the study, please follow the link below. 

&#x200B;

[https://docs.google.com/forms/d/e/1FAIpQLSf5HIfXHppMuTR63x00i4OuRAtM5Ti6EGybd-HuI1kmK06VPw/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSf5HIfXHppMuTR63x00i4OuRAtM5Ti6EGybd-HuI1kmK06VPw/viewform?usp=sf_link)

&#x200B;

Thank you for taking the time to contribute to our research study. Your participation is greatly appreciated!"
228,learnmachinelearning,chatgpt,relevance,2023-07-06 12:21:54,USA Company Valuation w/ ChatGPT,MandM-DataScience,False,0.6,1,14s7lck,https://www.reddit.com/r/learnmachinelearning/comments/14s7lck/usa_company_valuation_w_chatgpt/,4,1688646114.0,"Hi everyone!

We just published our new AI project. Take a look at our python notebook and our example GOOGL valuation ;)

[https://www.kaggle.com/code/mandmdatascience/usa-company-valuation-with-chatgpt](https://www.kaggle.com/code/mandmdatascience/usa-company-valuation-with-chatgpt)

&#x200B;

[GOOGL Company Dashboard \(Tableau\)](https://preview.redd.it/ais9ff4p7cab1.png?width=989&format=png&auto=webp&s=299ab8984e0e6ac84112353d1c94b5df719e673f)

Technologies used:

* Python
* OpenAI models
* Valuation model (Prof. Damodaran)
* Tableau
* PowerBI

Every and each comment / feedback is greatly appreciated!

Thank you!  
M&M"
229,learnmachinelearning,chatgpt,relevance,2024-01-09 11:26:39,Leveraging ChatGPT for advanced text analysis and labeling / qualitative research,a_bdgr,False,0.5,0,192clhw,https://www.reddit.com/r/learnmachinelearning/comments/192clhw/leveraging_chatgpt_for_advanced_text_analysis_and/,0,1704799599.0,"I've been diving into building my first CustomGPT models in ChatGPT during the last couple of days and I'm keen to push the boundaries a bit  further. I'm contemplating using ChatGPT for more intricate tasks or  whether I should explore other LLMs or different methodologies /  technologies / plattforms altogether.   

Here's my project idea: 

I want to develop a CustomGPT that can  perform a silent analysis of input text and tag it with specific labels based on predefined categories. For instance, identifying stylistic elements in text: a detected metaphor would get ""Style-ID1"", a quotation ""Style-ID2"", and so on, with the potential for multiple labels per text, akin to qualitative content analysis.   

I'm pondering over a few questions:   

1. Do OpenAI's CustomGPTs possess the capability to work with such categorizations and then utilize the tagged results in subsequent processing?     
 
2. Any advice on the labeling format for this kind of task?     
 
3. How can I implement sequential processing steps? After labeling, I intend to have the GPT perform additional operations on the tagged   results.     
 
4. What system would be a better solution that ChatGPT?     
 

I'd greatly appreciate any insights or if anyone could point me toward resources like blogs, videos, or courses that address these topics. Thanks a bunch!   

(also, this is sort of a manual crosspost from [r/chatGPT](https://www.reddit.com/r/chatGPT/))   "
230,learnmachinelearning,chatgpt,relevance,2023-01-12 07:08:04,chatgpt for research papers how to fine tune chatgpt for summarising research papers,OpeningMarsupial7229,False,0.33,0,109t7iu,https://www.reddit.com/r/learnmachinelearning/comments/109t7iu/chatgpt_for_research_papers_how_to_fine_tune/,0,1673507284.0,
231,learnmachinelearning,chatgpt,relevance,2023-10-02 17:49:50,ChatGPT API vs Rasa for custom chatbot?,alexk218,False,0.75,2,16y38zc,https://www.reddit.com/r/learnmachinelearning/comments/16y38zc/chatgpt_api_vs_rasa_for_custom_chatbot/,0,1696268990.0,"I am beginning my Computer Engineering Capstone project, and have approximately 8 months to complete. A part of this project includes an AI companion for the elderly (specifically old folks who are lonely/don't have anybody to talk to). This chatbot should be aware of the user's past, like their career, family, significant life events, etc. Other customizations are desired, such as allowing the chatbot to initiate conversations on its own.

So far I see 2 options for implementing this: ChatGPT API and Rasa. From my understanding, ChatGPT API is easier to use but offers less customization, while Rasa will require more work but offers more customization. Based on the requirements given, which API is best suited for this project?

Thank you!"
232,learnmachinelearning,chatgpt,relevance,2023-07-24 06:46:19,ZBrain- Create custom ChatGPT apps,StewartBJasper,False,0.63,2,1582nke,https://www.reddit.com/r/learnmachinelearning/comments/1582nke/zbrain_create_custom_chatgpt_apps/,0,1690181179.0,"Hello Community,

We at ZBrain have built a platform to create ChatGPT-like apps with your private data, you can import your data from multiple sources and DBs and integrate the app into any of your workflows.

We have also added AI risk governance to mitigate the confidential data leak and now working on Flow a no-code tool to give you the freedom to create your own business logic.

You can try the tool now at [https://zbrain.ai/](https://zbrain.ai/). We would love to hear your thoughts and feedback to improve the tool."
233,learnmachinelearning,chatgpt,relevance,2023-06-19 15:52:47,ChatGPT offends me twice,MarkClover,False,0.15,0,14dicfj,https://i.redd.it/afwle47yxz6b1.jpg,0,1687189967.0,
234,learnmachinelearning,chatgpt,relevance,2023-10-09 16:49:57,Key ChatGPT Statistics Every AI Enthusiast Should Know!,Chisom1998_,False,0.5,0,173wnpl,https://www.successtechservices.com/chatgpt-statistics/,0,1696870197.0,
235,learnmachinelearning,chatgpt,relevance,2023-03-12 17:31:52,ChatGPT Enabled Dashboard,Reasonable-Angle-500,False,0.86,15,11pkcci,https://v.redd.it/r8d1p7vrfcna1,2,1678642312.0,
236,learnmachinelearning,chatgpt,relevance,2023-06-17 16:27:13,I created the SMARTEST Computer Assistant using ChatGPT,Pritish-Mishra,False,0.68,22,14bv9dz,https://v.redd.it/g8hq2391ul6b1,7,1687019233.0,
237,learnmachinelearning,chatgpt,relevance,2023-03-25 02:47:27,[P] ChatGPT with GPT-2: A minimum example of aligning language models with RLHF similar to ChatGPT,liyanjia92,False,1.0,3,121849l,/r/MachineLearning/comments/120csub/p_chatgpt_with_gpt2_a_minimum_example_of_aligning/,1,1679712447.0,
238,learnmachinelearning,chatgpt,relevance,2023-05-19 07:08:51,OpenAI Launches ChatGPT App For iOS Users,vadhavaniyafaijan,False,0.86,52,13lnv1e,https://www.theinsaneapp.com/2023/05/chatgpt-app-for-iphone-and-ipad.html,10,1684480131.0,
239,learnmachinelearning,chatgpt,relevance,2023-02-11 06:58:18,[N] New Open-Source Version Of ChatGPT ⭕,LesleyFair,False,0.98,116,10zep6u,https://www.reddit.com/r/learnmachinelearning/comments/10zep6u/n_new_opensource_version_of_chatgpt/,8,1676098698.0,"GPT is getting competition from open-source.

A group of researchers, around the YouTuber [Yannic Kilcher](https://www.ykilcher.com/), have announced that they are working on [Open Assistant](https://github.com/LAION-AI/Open-Assistant). The goal is to produce a chat-based language model that is much smaller than GPT-3 while maintaining similar performance.

If you want to support them, they are crowd-sourcing training data [here](https://open-assistant.io/).

**What Does This Mean?**

Current language models are too big.

They require millions of dollars of hardware to train and use. Hence, access to this technology is limited to big organizations. Smaller firms and universities are effectively shut out from the developments.

Shrinking and open-sourcing models will facilitate academic research and niche applications.

Projects such as Open Assistant will help to make language models a commodity. Lowering the barrier to entry will increase access and accelerate innovation.

What an exciting time to be alive! 

Thank you for reading! I really enjoyed making this for you!  
The Decoding ⭕ is a thoughtful weekly 5-minute email that keeps you in the loop about machine research and the data economy. [Click here to sign up](https://thedecoding.net/)!"
240,learnmachinelearning,chatgpt,relevance,2023-07-10 15:16:24,"ChatPDF: What ChatGPT Can't Do, This Can!",JunXiangLin,False,0.64,3,14vww3o,https://www.reddit.com/r/learnmachinelearning/comments/14vww3o/chatpdf_what_chatgpt_cant_do_this_can/,7,1689002184.0,"Believe many of people have been using **ChatGPT** for a while, and you are aware that although ChatGPT is powerful, it has the following limitations:

1. Unable to answer questions about events that occurred after **2021**.
2. Unable to directly upload your own data, such as **PDF, Excel, databases**, etc.
3. Inaccurate in performing **mathematical calculations**.

**Langchain** is a recent trending open-source project, which is a framework for developing Large Language Models (LLMs) applications. It supports the following:

1. Connecting LLM models with **external data sources**, such as PDF, Excel, databases, etc.
2. Allowing interaction between LLM models and other tools, such as **Google search**, enabling internet connectivity.
3. Rapid development of LLM model applications.

Today, I'd like to share a project called **ChatPDF**(strickly called **docGPT**, there're some different), built using the Langchain framework. It allows users to upload local documents and ask questions to the LLM model. In this tool, you can ask AI to summarize articles or inquire about any information in the document. Moreover, by leveraging the Langchain Agent functionality, the LLM model can collaborate with the Google Search API, enabling users to ask questions about current topics!

The project provides a detailed guide on how to create your own **docGPT**. It is built using the Langchain framework and Python Streamlit, which is a free and fast way to create online services. As long as you have an OPENAI API KEY, feel free to give it a try!

I encourage everyone to pay attention to the [Langchain open-source project](https://github.com/hwchase17/langchain) and leverage it to achieve tasks that ChatGPT cannot handle.

[Github Repository](https://github.com/Lin-jun-xiang/docGPT-streamlit/tree/main)

[ChatPDF Application](https://docgpt-app.streamlit.app/)

&#x200B;

https://preview.redd.it/q906a7imm5bb1.png?width=2560&format=png&auto=webp&s=acef45049bab805038f876eea56cc371b8a9a83a"
241,learnmachinelearning,chatgpt,relevance,2023-03-22 01:14:49,how does chatgpt work?,googcheng,False,0.33,0,11y1v42,https://www.reddit.com/r/learnmachinelearning/comments/11y1v42/how_does_chatgpt_work/,0,1679447689.0,"question 1:  
does it depend on labeled data and RLHF much?  
question 2:  
is it a dice(black box) which five of its six faces marked with six dots by fine-tuning?     
which probability theory is it base on?"
242,learnmachinelearning,chatgpt,relevance,2023-01-11 05:23:14,Thoughts on this ChatGPT fact-checker tool I built this past week?,QuestionAnxious,False,0.89,93,108wigf,https://v.redd.it/yqudljp0ncba1,25,1673414594.0,
243,learnmachinelearning,chatgpt,relevance,2023-03-02 16:47:40,Build ChatGPT for Financial Documents with LangChain + Deep Lake,davidbun,False,0.95,172,11g7h03,https://www.reddit.com/r/learnmachinelearning/comments/11g7h03/build_chatgpt_for_financial_documents_with/,8,1677775660.0,"https://preview.redd.it/h9r6hgvfucla1.png?width=2388&format=png&auto=webp&s=5432eac3eeed8583e4309af1fdc7ebecac705796

As the world is increasingly generating vast amounts of financial data, the need for advanced tools to analyze and make sense of it has never been greater. This is where [LangChain](https://github.com/hwchase17/langchain) and [Deep Lake](https://github.com/activeloopai/deeplake) come in, offering a powerful combination of technology to help build a question-answering tool based on financial data. After participating in a LangChain hackathon last week, I created a way to use Deep Lake, the data lake for deep learning (a package my team and I are building) with LangChain. I decided to put together a guide of sorts on how you can approach building your own question-answering tools with  LangChain and Deep Lake as the data store.

Read [the article](https://www.activeloop.ai/resources/ultimate-guide-to-lang-chain-deep-lake-build-chat-gpt-to-answer-questions-on-your-financial-data/) to learn:

1. What is LangChain, what are its benefits and use cases and how you can use to streamline your LLM (Large Language Model) development?  
2. How to use [\#LangChain](https://www.linkedin.com/feed/hashtag/?keywords=langchain&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) and [\#DeepLake](https://www.linkedin.com/feed/hashtag/?keywords=deeplake&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) together to build [\#ChatGPT](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) for your financial documents.  
3. How Deep Lake’s unified and streamable data store enables fast prototyping without the need to recompute embeddings (something that costs time & money).  


I hope you like it, and let me know if you have any questions!"
244,learnmachinelearning,chatgpt,relevance,2023-02-04 23:41:43,"ChatGPT's Inner Magic, Explained Step-by-Step",wwllol,False,0.96,140,10tuywc,https://youtu.be/-9SdOPe294w,1,1675554103.0,
245,learnmachinelearning,chatgpt,relevance,2023-05-20 00:14:08,Reducing chatGPT Hallucinations by 87%,VideoTo,False,0.33,0,13mcf5n,https://www.reddit.com/r/learnmachinelearning/comments/13mcf5n/reducing_chatgpt_hallucinations_by_87/,1,1684541648.0,"Our journey towards enabling useful LLM Apps to be built involved three key areas:

1. Observability: What questions are my users are asking?
2. Tuning: What changes can I make to improve results - add more data, finetune embeddings, improve search, change prompt, etc.
3. Testing: Did my changes actually improve results?

We  added logs to every berri instance when we discovered developers asking  end-users to send them screenshots of poor responses of their  in-production Berri/LLM Apps.

Now  that we could see our responses, we hit our next problem: ChatGPT would  often give erratic responses like “As an AI model I unfortunately can’t  answer…”. This isn’t a response you want end-users to get in production.

So  our customers developed test banks - but at 100+ questions, how do you  test prompt changes at scale? So we built auto-eval - a way to batch  test questions and see answers.

However,  simply seeing answers isn’t enough - especially when you’re trying to  test between several different approaches (e.g. how many documents  should you pass to the model, what prompt variation works best, etc.?).

So,  we ran a 2nd LLM to score the model responses to each question. If the  response didn’t answer the question, we then built a rules engine to  identify what went wrong and suggest next steps - was it missing data?  poor retrieval? or did the prompt need to be fixed?

But  a simple ‘Yes/No’ decision from the auto-eval LLM wasn’t enough. We  needed more precision to evaluate hallucinations vs. partial responses.  That’s when we updated our evaluation framework + enforced  self-consistency in the grading model by asking it to provide a  rationale for it’s grades and sampled grading across 3 runs for each  question.

This significantly  improved consistency from our grading model - and helped us develop LLM  Apps in-production which reduced hallucinations from 40% to <5%.

&#x200B;

https://preview.redd.it/cbf7tyvnpv0b1.png?width=4939&format=png&auto=webp&s=fc36c2878de82c88beb70996b2fda7abd794836b"
246,learnmachinelearning,chatgpt,relevance,2023-01-06 11:58:10,How does ChatGPT actually work? Explained simply with pen and paper,techie_ray,False,0.94,123,104sebq,https://youtu.be/k9Sps7ciNTE,16,1673006290.0,
247,learnmachinelearning,chatgpt,relevance,2023-05-30 18:41:40,Use ChatGPT in your advance!,Ok-Highlight-5794,False,0.33,0,13vyc8x,https://www.paypro.nl/producten/De_Ultieme_ChatGPT-4_Gids/104706/196903,0,1685472100.0,
248,learnmachinelearning,chatgpt,relevance,2024-02-13 17:23:06,Unlock AI's Power: Top Free Courses on ChatGPT & Large Language Models| DeepLearning.AI,UseCreative4765,False,0.33,0,1apyyrd,https://youtu.be/6nc_qM-1HUc?si=Hn4zOpPYhWIWOgIz,0,1707844986.0,
249,learnmachinelearning,chatgpt,relevance,2023-02-21 18:49:26,RPG & ChatGPT,TekeelaMockingbird,False,1.0,1,118cq36,https://www.reddit.com/r/learnmachinelearning/comments/118cq36/rpg_chatgpt/,0,1677005366.0,Hey we're doing an RPG podcast with ChatGPT as the GM. This episode was a Valentine's Day theme. The name of the podcast is Constructed Chronicles. https://open.spotify.com/episode/0sOtO4CM2w6tsRhhEwYAJ5?si=mfqJHVJ-QU21x538w9HnMA
250,learnmachinelearning,chatgpt,relevance,2023-01-17 07:51:07,DeepMind To Launch ChatGPT Rival Sparrow Soon,vadhavaniyafaijan,False,0.96,121,10e6h7j,https://www.theinsaneapp.com/2023/01/deepmind-to-launch-chatgpt-rival-sparrow.html,5,1673941867.0,
251,learnmachinelearning,chatgpt,relevance,2023-01-16 19:21:18,Today we go over creating an Unity ChatGPT Client to allow us to communicate with our ChatGPT API and this will be the beginnings of getting ChatGPT HTTP responses into Unity (full video and playlist in comments),dilmerv,False,0.92,45,10doqua,https://v.redd.it/ixwf3g7syhca1,2,1673896878.0,
252,learnmachinelearning,chatgpt,relevance,2023-04-28 16:17:58,ChatGPT Prompt Engineering for Developers free on deeplearning.ai,sunkenwaaaaaa,False,0.87,17,131zare,https://www.reddit.com/r/learnmachinelearning/comments/131zare/chatgpt_prompt_engineering_for_developers_free_on/,10,1682698678.0,Andrew Ng just released a short course on how to use the Open AI api. It is free for now.
253,learnmachinelearning,chatgpt,relevance,2023-07-19 03:03:00,Meta open-sources LLaMA 2 to compete with ChatGPT,Any-Heron-6313,False,0.9,39,153iujc,https://medium.com/p/1370d587b104,3,1689735780.0,
254,learnmachinelearning,chatgpt,relevance,2023-10-22 04:22:24,Built it for my project at first: Memorybase.io supercharges ChatGPT with memory capabilities for your chatbots,tujiserost,False,0.78,10,17dl1bw,https://www.reddit.com/r/learnmachinelearning/comments/17dl1bw/built_it_for_my_project_at_first_memorybaseio/,17,1697948544.0,"Hey everyone!

I've been delving deep into chatbots lately, especially with the ChatGPT API, and I found an issue that's probably familiar to many of you: ChatGPT doesn't inherently have memory capabilities. For many applications, that's perfectly fine, but for those of us who are trying to create a more context-aware and dynamic conversation flow, this limitation is quite apparent.

I faced this challenge in one of my projects and realized that there had to be a better way to integrate context and memory into ChatGPT's conversations. So, I built something for myself which I thought might be useful for many of you as well. Allow me to introduce you to [**Memorybase.io**](http://memorybase.io/).

Memorybase is a developer-friendly API that's designed to seamlessly integrate memory functionality into the ChatGPT API. By harnessing the power of the Pinecone vector database and LangChain, Memorybase wraps around the ChatGPT API and ensures that the right context and memory are injected into each query. This means that your chatbot can remember previous interactions, preferences, or any other context that's relevant for more engaging and meaningful conversations.

Imagine a user asking your chatbot about movie recommendations. The next day, they come back and reference that conversation, expecting the bot to remember. With Memorybase, that continuity becomes possible. The user experience improves manifold, and the possibilities for more sophisticated and context-aware bots increase tremendously.

I originally built Memorybase for my own needs. But the more I used it, the more I realized that this could have broader applications. Any developer looking to leverage the ChatGPT API could potentially benefit from the enhanced memory and context capabilities. From customer support bots to interactive storytelling, the potential use cases are vast.

This technology stack (pinecone/langchain) is not complex or ‘new’ per se, but for application developers who aren’t interested in managing it or hosting it, this could be a useful hassle-free option for your projects.

I've set up a page over at [memorybase.io](https://memorybase.io/) where you can learn more about how it works and see if it aligns with your needs. I would love for you to check it out and share your thoughts. Your feedback, insights, and potential use cases would be invaluable as I continue to refine and expand the capabilities of Memorybase.

Thanks for reading, and I'm eager to hear your thoughts and see where Memorybase can fit into the exciting world of chatbots!"
255,learnmachinelearning,chatgpt,relevance,2023-07-26 19:30:58,Legal Litigation Analysis with chatGPT and AI builder,Molly_Knight0,False,1.0,1,15afaa7,https://ubiai.tools/unlocking-legal-litigation-analysis-with-chatgpt/,0,1690399858.0,"

Learn how ChatGPT extracts named entities, generates summaries, and identifies critical facts in legal documents. Plus, explore the user-friendly AI Builder for seamless NLP workflows."
256,learnmachinelearning,chatgpt,relevance,2023-04-12 22:32:13,Build chatgpt plugins into your applications,fbssxhyeet1738,False,1.0,1,12k20ff,https://www.reddit.com/r/learnmachinelearning/comments/12k20ff/build_chatgpt_plugins_into_your_applications/,0,1681338733.0,"Hey guys!

I just published an open-source project to make it extremely easy to execute functions based on natural language. This allows anyone to incorporate agent based functionality inside their app (instead of on top of chatgpt).

You give your js functions and they are executed based on a natural language input (see example in repo)

Would really appreciate any feature requests/stars :) 

https://github.com/alexgriffithsdev/actionit"
257,learnmachinelearning,chatgpt,relevance,2023-06-01 14:21:18,How Does ChatGPT Learn: Reinforcement Learning Explained,OnlyProggingForFun,False,0.5,0,13xiv8m,https://youtu.be/lWK9T56t-YM,0,1685629278.0,
258,learnmachinelearning,chatgpt,relevance,2023-02-05 17:47:39,Did ChatGPT Kill School?,thelazyaz,False,0.09,0,10uii0f,https://youtu.be/kj4ajhgsmAs,1,1675619259.0,
259,learnmachinelearning,chatgpt,relevance,2023-05-01 19:02:17,Machine Learning with ChatGPT Cheat Sheet,kingabzpro,False,0.44,0,134y381,https://www.kdnuggets.com/2023/05/machine-learning-chatgpt-cheat-sheet.html,0,1682967737.0,
260,learnmachinelearning,chatgpt,relevance,2023-01-27 14:51:14,Fine-tuning open source models to emulate ChatGPT for code explanation.,awesomequantity,False,0.88,87,10mmofg,https://www.reddit.com/r/learnmachinelearning/comments/10mmofg/finetuning_open_source_models_to_emulate_chatgpt/,13,1674831074.0,"I'm looking to step up my game and emulate ChatGPT for specific use-cases like explaining code. I'm thinking about using open source models like GPT-J, or OPT to get beyond the limitations of the closed-source nature of ChatGPT, like the amount of text it can read or respond with.

I got the funding for training, hardware, etc, and I want the end product to be on-premises, so no worries there. The inference doesn't have to be super fast either. I know there are projects like OpenAssistant and petals.ml but haven’t made enough research just yet.

One option I’m considering is using fine tuners like the one from [HuggingFace](https://github.com/subhasisj/HuggingFace-Transformers-FineTuning) or [Jina AI](https://github.com/jina-ai/finetuner) to fine-tune open source models like GPT-J or OPT to improve specific use-cases like code explanation. With the funding that we have, I wouldn’t want to cheap out on fine-tuning and expect something good.

So, can anyone help out and point me in the right direction? Which model is the best to fine-tune and how do I fine-tune to improve specific use cases? Any help would be appreciated. Thanks!"
261,learnmachinelearning,chatgpt,relevance,2023-02-19 21:31:48,I built a texting robot with ChatGPT that knows current events. TxtAva: The Ultimate AI Texting Assistant with ChatGPT,AugmentedGlobal,False,1.0,1,116okld,https://youtu.be/S9tTlgbhhMA,1,1676842308.0,
262,learnmachinelearning,chatgpt,relevance,2023-04-12 23:00:52,Fine Tuning ChatGPT on Full Documents?,Simusid,False,0.67,2,12k2vyt,https://www.reddit.com/r/learnmachinelearning/comments/12k2vyt/fine_tuning_chatgpt_on_full_documents/,1,1681340452.0,"I want to fine tune GPT-3 using internal corporate documents.   They are mostly paragraphs of text.   Each paragraph might have 5 or 6 sentences.  Per the API, I have to provide prompt/completion pairs in the format:

{""prompt"": ""<prompt text>"", ""completion"": ""<ideal generated text>""}

If a paragraph consists of <sentence1><sentence2><sentence3>....<sentenceN> does it make sense to build the pairs as:

{""prompt"": ""<sentence1>"", ""completion"": ""<sentence2>""}

{""prompt"": ""<sentence2>"", ""completion"": ""<sentence3>""}

{""prompt"": ""<sentenceN-1>"", ""completion"": ""<sentenceN>""}"
263,learnmachinelearning,chatgpt,relevance,2023-05-14 16:11:21,Using chatgpt in a political game to control nations?,Marksh11,False,0.44,0,13hg6is,https://www.reddit.com/r/learnmachinelearning/comments/13hg6is/using_chatgpt_in_a_political_game_to_control/,3,1684080681.0,"
Greetings, everyone!

I'm currently working on the development of a political game that allows players to assume the roles of nations, much like in the popular game Hearts of Iron. As part of this project, I've been contemplating the idea of having non-player-controlled nations operated by GPT through the utilization of JSON output. However, I'm concerned about the potential cost in a production environment. Should I instead consider implementing a basic algorithm for those server owners who are unable to afford the GPT setup? And make it that individual owners have to provide their own api key. Your insights on this matter would be greatly appreciated."
264,learnmachinelearning,chatgpt,relevance,2023-05-05 22:39:41,Using ChatGPT for assigning ontology to KMeans labels,lukaszluk,False,0.6,1,1393uoa,https://www.reddit.com/r/learnmachinelearning/comments/1393uoa/using_chatgpt_for_assigning_ontology_to_kmeans/,4,1683326381.0,"Sharing a cool technique that you can use to assign categories/titles/ontology to your Kmeans results.

My use case involves text data with descriptions so it won’t be applicable in every situation, but it can definitely give inspiration to anyone.

I had podcast transcripts that were chunked into sections (\~3000 text documents). These sections were then transformed into summaries with [LangChain](https://langchain.com/) and [OpenAI API](https://platform.openai.com/docs/introduction). Finally, I embedded the summaries using OpenAI embeddings. Then I ran KMeans (k=30) and got labels with section names:

&#x200B;

https://preview.redd.it/zepzijatb3ya1.png?width=633&format=png&auto=webp&s=23313df60d798636eb6a02392e567bd4ace3587c

In order to avoid exceeding the maximal number of tokens in the context window (4096 tokens), I sampled the data frame to contain 200 segment names with 5 selected labels. Then I iteratively moved to the next labels, i.e.:

1. iteration — labels from 0 to 4

* 2. iteration — labels from 5 to 9
* …
* 6. iteration — labels from 25 to 29

This is an example output from our ontology detector:

&#x200B;

https://preview.redd.it/k3rfqo7ub3ya1.png?width=653&format=png&auto=webp&s=792944cb82b6e0d2fe8e0f8a763f0f1fbcabb57b

After iterating through all labels I noticed that some of the categories and keywords overlap. Moreover, it would be hard to navigate through so many categories.

That’s why I asked ChatGPT to group overlapping categories:

&#x200B;

https://preview.redd.it/efgrjukvb3ya1.png?width=676&format=png&auto=webp&s=f2552e7f51f63ccb668e13858d49f424baea2475

Sharing the prompts in the comment section! You can check out the code here: [https://github.com/DataScienceDisciple/hubermanlab-qa/blob/main/notebooks/04\_summary-analysis.ipynb](https://github.com/DataScienceDisciple/hubermanlab-qa/blob/main/notebooks/04_summary-analysis.ipynb)"
265,learnmachinelearning,chatgpt,relevance,2023-12-11 10:28:19,How I Built My Own ChatLLM to Explain Me Python in ChatGPT Tone,VikasOjha666,False,1.0,2,18frjr1,https://www.reddit.com/r/learnmachinelearning/comments/18frjr1/how_i_built_my_own_chatllm_to_explain_me_python/,0,1702290499.0,"This blog explains how we can train an open-source large language model to explain Python code in the same tone as chatGPT i.e. in the same way it explains the code.

&#x200B;

[https://vikasojha894.medium.com/how-i-built-my-own-chatllm-to-explain-me-python-in-chatgpt-tone-74f8da1429aa](https://vikasojha894.medium.com/how-i-built-my-own-chatllm-to-explain-me-python-in-chatgpt-tone-74f8da1429aa)"
266,learnmachinelearning,chatgpt,relevance,2023-01-18 07:13:02,Building ML model using ChatGPT,MathematicianFar8159,False,0.5,0,10f11wd,https://www.reddit.com/r/learnmachinelearning/comments/10f11wd/building_ml_model_using_chatgpt/,4,1674025982.0,"With ChatGPT getting so much popularity, I was thinking if I can use it as a pre-trained model for my ML projects. Can anyone give an idea how I can do it."
267,learnmachinelearning,chatgpt,relevance,2023-04-10 08:12:54,Summarize documents with ChatGPT via Python scripts,rottoneuro,False,0.68,13,12hbpyh,https://levelup.gitconnected.com/summarize-documents-with-chatgpt-a43456841cc4,2,1681114374.0,
268,learnmachinelearning,chatgpt,relevance,2023-03-03 12:31:13,ChatGPT for Data Science Cheat Sheet,kingabzpro,False,0.31,0,11gzkq9,https://www.kdnuggets.com/2023/03/chatgpt-data-science-cheat-sheet.html,3,1677846673.0,
269,learnmachinelearning,chatgpt,relevance,2023-04-15 18:12:27,Control your own app with ChatGPT,fbssxhyeet1738,False,0.75,2,12nd9le,https://www.reddit.com/r/learnmachinelearning/comments/12nd9le/control_your_own_app_with_chatgpt/,0,1681582347.0,"ChatGPT plug-ins are cool - but what’s even cooler is adding chat functionality control to your own apps. I just released a short tutorial on how you can achieve this:

https://youtu.be/VBfcfJBoIr4"
270,learnmachinelearning,chatgpt,relevance,2023-08-29 12:29:21,Comparing the recent opensource alternatives of chatGPT to run locally,Messi_AI,False,0.25,0,164h5l6,https://www.reddit.com/r/learnmachinelearning/comments/164h5l6/comparing_the_recent_opensource_alternatives_of/,0,1693312161.0,"Hello all

am a newbie in LLMs and am searching for a comprehensive resource for comparing the opensource alternatives of chatGPT to run locally. any help on this?"
271,learnmachinelearning,chatgpt,relevance,2023-05-07 12:58:51,New to AI and ChatGPT - Where do I start?,growthnerd,False,0.15,0,13an0ji,https://www.reddit.com/r/learnmachinelearning/comments/13an0ji/new_to_ai_and_chatgpt_where_do_i_start/,9,1683464331.0,"Heya, I just started using ChatGPT for a couple weeks for college homework. This AI tech is amazing and I wanna learn more.

What are 3-5 concepts or software you’d recommend me to start learning first? Also, what are your top 3-5 newsletters, channels or websites to learn about AI from?

Thanks so much, appreciate the help"
272,learnmachinelearning,chatgpt,relevance,2023-06-20 17:23:52,Top 7 Best ChatGPT Alternative Platforms (Cutting-Edge NLP!),Chisom1998_,False,0.43,0,14eh7kx,https://www.successtechservices.com/chatgpt-alternative/,2,1687281832.0,
273,learnmachinelearning,chatgpt,relevance,2023-02-20 19:01:54,Master ChatGPT Prompt Engineering (Deep Dive),jeyThaswan,False,0.78,5,117hd0f,https://www.reddit.com/r/learnmachinelearning/comments/117hd0f/master_chatgpt_prompt_engineering_deep_dive/,2,1676919714.0," 

I wrote a deep dive on prompt engineering as a resource for the AI community and my 10,000 daily newsletter subscribers ([Inclined.ai](https://www.inclined.ai/p/prompt-engineering-guide) if you're curious). We've included some examples so feel free to copy and paste the prompts into ChatGPT!

&#x200B;

**WHAT IS PROMPT ENGINEERING?**

The term is relatively new, and its origins are argued *(because we live in the internet age, and it’s harder to claim ownership)*. Prompt engineering is the ability to instruct and teach AI effectively.

If it helps, think of this as rapid testing or instruction writing for artificial intelligence.

What’s important is not to let this overwhelm you. The first prompting happened with the first AI model. The first example was showing computer images of circles and triangles. **Today’s neural networks can process way more data, creating complexities.**

So, the concept is simple, but digging into the full power of AI today is something else entirely.

We’re not talking about asking questions. Odds are, if you’re typing *“what’s 2+2”* into ChatGPT, then you need to keep reading.

We can all ask chatbots questions. That can work more often than not. But AI is not perfect. A common metaphor I see is to treat GPT-based large language models like the smartest five-year-old you’ve ever met.

I have a niece around that age and can’t imagine trying to get her to write an essay on the effects of soil mismanagement in relation to Reconstruction politics. *See! Your eyes glazed over reading that, so how do we make this work for our AI buddies?*

The Principles of Prompting

Stop asking single-line questions. *That’s like using a top-rated cookbook to find out how to make grilled cheese.*

**There are three ways to instantly get better at prompting** and go from grilled cheese to top-notch bolognese. From there, we can get into some specific prompt concepts and the ability to unlock ChatGPT’s full potential.

Principle 1: Context is King

GPT-3.5 is swimming in data. When you ask it for a simple request, it can end up complicating things more than you realize. Did you ever wonder why ChatGPT is so bad at math?

The reality is the LLM is taking words and turning them into patterns. From there, it’s making an educated guess.

Give your chat AI a frame to search into. If you give it a math problem, you need to make sure it grasps that you want it to do math. If you’d like ChatGPT to write a high school essay, you must ensure it knows to write at that level.

**Instead of:** “Plan a party for a kid.”

**Try:** “My child is turning 9. They like superheroes and the color red. Help me plan a party for this weekend. Ten of his friends are coming to my house.”

You’ll get a much better response this way. **Context is the cardinal direction** that helps your chat companion find the most correct guess and phrase it the best way.

Principle 2: Get Specific

Pretend you’re writing a law that’s going to be judged by the Supreme Court of the United States. You know what they look for: narrow tailoring.

**Keep things on track and stay focused.** Try to avoid prompting outside the specific request. You’ll only hurt the ability of the chat AI to give you a quality response. Odds are they’ll even skip over parts if you confuse them with too many requests.

It runs parallel with context. *If you set ChatGPT up in a room and then tell it to focus on describing the chair first, you’ll see better results.*

**Instead of:** “I’m going to a job interview. Write five questions for me to answer. Add tips for how to not get nervous before the interview. Do not create questions asking about my background.”

**Try:** “You’re interviewing a software engineer. Create five questions to ask them to understand their skill set and qualifications better.”

Nothing limits the number of prompts you can do. Focus and expand from the initial request and try not to do everything at once.

Principle 3: When in Doubt: “Let’s take this step-by-step.”

Welcome. **You discovered the magic word today.** This phrase slows everything down for the AI and gets you where you need to go.

You don’t need to start with this phrase. Using it tells ChatGPT to show their work.

We’ll explain where this concept comes from further in our briefing, but here’s the TL;DR: sometimes, there’s a part of our prompt it’s not identified correctly. “Let’s take this step-by-step,” reminds you and ChatGPT to **slow down and get specific.**

If you learn to utilize this phrase more often and find ways to make it work for you, you’ll become a better prompt engineer. One term can do a lot of heavy lifting.

**Pro-tip:** We’ve shown you “standard” prompts in all these examples. Many prompt engineers will use “Standard QA form” prompts. Here’s our example for this principle written that way.

**Example:**

*“Q: The Industrial Revolution rapidly changed the infrastructure in London. Describe three essential innovations from this period and connect them to Landon’s development.*

*A: Let’s take this step-by-step.”*

Even without our magic word, this style of standard prompting is quite helpful to adopt.

*However, we’re beginning to stumble into the advanced tactics used in prompt engineering, so it’s time for a new section.*

UNIQUE WAYS TO PROMPT

Let’s preface this: we can go super deep here. Prompt engineering is changing daily, and as these models get more sophisticated, the need to adapt prompts strengthens.

To keep things clean, I will go through these using our metaphor from earlier. **Let’s pretend ChatGPT is a super-intelligent toddler.**

*Got it? With that buy-in, we can continue.*

1/ Role Prompting

We’ll start with a popular tactic. **Our toddler is great at imagining things.** You tell them they’re a fireman, and suddenly they can give you detailed ways to ensure your apartment is up to code. Role-playing is a fun, easy way to build context.

The best part of role prompting is how easy it is to understand and use. All you need to do is tell ChatGPT to play a role. From there, the AI will do its best to fill the part *like that enthusiastic drama student from your old high school.*

You can even take this a step further. **Try framing your prompt as a script.** Tell the LLM specific instructions around a scene that gives you the answer to your question.

TRY IT OUT FOR YOURSELF:

Copy this prompt into ChatGPT and find a destination!

“Act as a travel guide. I will tell you my location and you will suggest a place to visit near my location. In some cases, I will also give you the type of places I will visit. You will also suggest me places of similar type that are close to my first location. My first suggestion: \[fill it in\]”

Why would you take that extra step? While popular, role prompting does not necessarily improve accuracy. *You can tell your five-year-old they’re a mathematician, and they’ll still manage to screw things up.*

Let’s get deeper.

2/ Chain-of-Thought Prompting

There’s a scene in ***Guardians of the Galaxy*** where Rocket Raccoon is trying to [teach young Groot](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3lvdXR1LmJlL0hyaW1mZ2pmNGs4IiwicG9zdF9pZCI6ImM0ODlmMzhkLTY3NDAtNGRmNS05MWJjLTM1ODA0YTVmNTZkMiIsInB1YmxpY2F0aW9uX2lkIjoiNjNkODQ2ZGUtZDFiZi00ZTVjLWJjYzgtOWMxYzlkMWIxMjA0IiwidmlzaXRfdG9rZW4iOiJkZWRiNmMyNy1hYmM0LTQ5ZDUtYWM2Ny04OTcyZmM1MGVmM2QiLCJpYXQiOjE2NzY5MTkwNjguMTUyLCJpc3MiOiJvcmNoaWQifQ.5eZkDLGRLCXXYv32FYT7kLSbdRK5OK1iemTRf3HVmJw)

how to activate a complicated device. That’s chain-of-thought prompting.

**You take an example question and answer it for ChatGPT.** Show them your chain of thought. Then you give it a new question in the same vein and ask it for an answer.

This prompt style allows you to get more specific. You’re telling your toddler they’re here to answer this particular question with one specific logic pattern.

Within this specific style is two other sub-categories. Let me give the rundown:

* Zero-shot Chain-of-Thought is “Let’s take this step-by-step” you frame the question the same, but don’t give it a precursor. Instead, you ask it to think through the points made. EX: Q: X is A. Y is B. What is C? A: Let’s take this step-by-step.
* Self-consistency is using several responses to find the most accurate answer. You give ChatGPT more swings at the ball. Take the hits and discover the grouping.

TRY IT OUT FOR YOURSELF:

Copy this prompt into ChatGPT and see how accurate it is:

“Q: Which is a faster way to get home?

Option 1: Take an 10 minutes bus, then an 40 minute bus, and finally a 10 minute train.

Option 2: Take a 90 minutes train, then a 45 minute bike ride, and finally a 10 minute bus.

A: Option 1 will take 10+40+10 = 60 minutes.

Option 2 will take 90+45+10=145 minutes.

Since Option 1 takes 60 minutes and Option 2 takes 145 minutes, Option 1 is faster.

Q: Which is a faster way to get to work?

Option 1: Take a 1000 minute bus, then a half hour train, and finally a 10 minute bike ride.

Option 2: Take an 800 minute bus, then an hour train, and finally a 30 minute bike ride.

A: ”

[Learnprompting.org](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwOi8vTGVhcm5wcm9tcHRpbmcub3JnIiwicG9zdF9pZCI6ImM0ODlmMzhkLTY3NDAtNGRmNS05MWJjLTM1ODA0YTVmNTZkMiIsInB1YmxpY2F0aW9uX2lkIjoiNjNkODQ2ZGUtZDFiZi00ZTVjLWJjYzgtOWMxYzlkMWIxMjA0IiwidmlzaXRfdG9rZW4iOiJkZWRiNmMyNy1hYmM0LTQ5ZDUtYWM2Ny04OTcyZmM1MGVmM2QiLCJpYXQiOjE2NzY5MTkwNjguMTUyLCJpc3MiOiJvcmNoaWQifQ.-wOnVYoMNWXYrR5NOB4YYKp4Lmj-aZq3y-pr4Hou9pE)

\- by leaving the “A:” blank you’re prompting ChatGPT for the answer

Alright, you’re almost there—one more to go.

3/ General Knowledge Prompting

You’re going to notice a trend here. This prompt style also circles context and narrow tailoring.

All you do is tell your toddler how the world works. The cow goes moo. The dog goes woof. So what does a cat say?

It’s an oversimplification, but the core reasoning is there. Show ChatGPT some knowledge and turn that into the only focus for that chat. You can take an article from the internet and summarize it for the model. Make sure to ask if it understands and relay the information to you.

Once you know you have the attention set in the suitable space, get to work. For instance, we can share an Inclined newsletter with it and tell ChatGPT about its structure and tone.

From there, you can provide new information and tell ChatGPT to summarize it within the same structure as Inclined. You both share the same general knowledge now.

TRY IT OUT FOR YOURSELF:

Copy this prompt into ChatGPT and test it out:

“Prompt 1. Look over this article here: \[pick an article\]. Breakdown its structure and general tone.

Prompt 2: Recall the structure and tone you mentioned above. Take that general knowledge and summarize this article: \[pick a new one\] using the same structure and tone.”

Note: this is a heavily simplified version of GA Prompting

Did you know some [people don’t consider](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL25ld3MueWNvbWJpbmF0b3IuY29tL2l0ZW0_aWQ9MzQ0OTU0NTUiLCJwb3N0X2lkIjoiYzQ4OWYzOGQtNjc0MC00ZGY1LTkxYmMtMzU4MDRhNWY1NmQyIiwicHVibGljYXRpb25faWQiOiI2M2Q4NDZkZS1kMWJmLTRlNWMtYmNjOC05YzFjOWQxYjEyMDQiLCJ2aXNpdF90b2tlbiI6ImRlZGI2YzI3LWFiYzQtNDlkNS1hYzY3LTg5NzJmYzUwZWYzZCIsImlhdCI6MTY3NjkxOTA2OC4xNTMsImlzcyI6Im9yY2hpZCJ9.yHKIPujINT89tsqOo07AXk6OrKNgoMjO3fBEYPkAdNY)

that prompt engineering?

PROMPT CULTURE

*“How can something not be prompt engineering if it’s a prompt style?”*

Good question, imaginary reader. The culture around this skill is relatively fresh. So some of **these concepts are seen as too easy** to be considered accurate prompt testing.

General knowledge prompting is simply establishing the context, and for some, that’s a baseline everyone needs to do. The same can be said for role prompting, too. *All of these tiny preferences are semantics.*

**Don’t sweat whether you’re a “real” prompt engineer.** Test this out and share your insights in these communities. The opportunity is there for you.

You may even know about DAN (we’ve covered it in previous newsletters) and other AI hacking methods. Those all start with prompt engineering. You can make the case that unless the AI behaves outside its parameters, you’re not genuinely doing prompt engineering.

I'm afraid I have to disagree with that, and **careers are sprouting up everywhere** that center directly on this skill. **Many require a core understanding of the prompt styles we’ve discussed.**

*Yep, you can learn this and make money from talking with AI.*

Anthropic even [posted a role for a prompt engineer](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2pvYnMubGV2ZXIuY28vQW50aHJvcGljL2UzY2RlNDgxLWQ0NDYtNDYwZi1iNTc2LTkzY2FiNjdiZDFlZCIsInBvc3RfaWQiOiJjNDg5ZjM4ZC02NzQwLTRkZjUtOTFiYy0zNTgwNGE1ZjU2ZDIiLCJwdWJsaWNhdGlvbl9pZCI6IjYzZDg0NmRlLWQxYmYtNGU1Yy1iY2M4LTljMWM5ZDFiMTIwNCIsInZpc2l0X3Rva2VuIjoiZGVkYjZjMjctYWJjNC00OWQ1LWFjNjctODk3MmZjNTBlZjNkIiwiaWF0IjoxNjc2OTE5MDY4LjE1MywiaXNzIjoib3JjaGlkIn0.4s7Htzgoxv0_qM1Ten17oQ5h0_QGM6e1fGUYz_ymgJ4)

that nets a quarter million in salary. I did not make that up and even considered sprucing up the old resume. When a new skill like this comes about, it’s worth looking at.

There are many other examples like this, and OpenAI uses a red teaming strategy where their engineers attempt to prompt hack their own GPT models.

I can tell you all about the open roles here, but tomorrow the whole cycle will change. *Isn’t that exciting, though?* The entire identity around prompt engineering will change by this time next year.

WHAT SHOULD YOU TAKEAWAY?

Communication is everything. **Learning to speak with AI is rising in importance.**

We all watch with mouth agape at the new wonders in AI because we know this will disrupt every industry. If any of this piqued your interest, the window to pursue it is now open. Ride that wave and [learn](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2xlYXJucHJvbXB0aW5nLm9yZy8iLCJwb3N0X2lkIjoiYzQ4OWYzOGQtNjc0MC00ZGY1LTkxYmMtMzU4MDRhNWY1NmQyIiwicHVibGljYXRpb25faWQiOiI2M2Q4NDZkZS1kMWJmLTRlNWMtYmNjOC05YzFjOWQxYjEyMDQiLCJ2aXNpdF90b2tlbiI6ImRlZGI2YzI3LWFiYzQtNDlkNS1hYzY3LTg5NzJmYzUwZWYzZCIsImlhdCI6MTY3NjkxOTA2OC4xNTMsImlzcyI6Im9yY2hpZCJ9.a67KDSN9yQfZsaMeHpdcbSbtPjD4yFcGW4stdxBjX1M)

to become a brilliant prompt engineer.

Heck, even if you don’t want to switch careers, **talking with ChatGPT and all the newest LLMs is becoming a part of our daily routine.** Get to the point where you maximize every interaction and work with these chatbots to upskill your workflow.

Prompt engineering can save you time, eliminate hassle, and even help you become a more patient person. Focus on what you want and explain it with intent.

Make magic happen, and remember: **take it step-by-step.**"
274,learnmachinelearning,chatgpt,relevance,2023-02-12 02:34:00,[P] Fixing code with ChatGPT,hoerzu,False,0.4,0,1104x83,https://www.youtube.com/shorts/gMtqyANQyFY?s=2,0,1676169240.0,
275,learnmachinelearning,chatgpt,relevance,2023-08-30 12:50:19,Getting the Hang of OpenCV’s Inner Workings with ChatGPT,No-Independence5880,False,0.5,0,165dujw,https://www.reddit.com/r/learnmachinelearning/comments/165dujw/getting_the_hang_of_opencvs_inner_workings_with/,0,1693399819.0,"&#x200B;

https://preview.redd.it/jdf4h810v8lb1.jpg?width=2800&format=pjpg&auto=webp&s=4f090692271c5344d24b914548e4d5b4429d0bcc

Very interesting blog post from OpenCV.ai team about how can explore ChatGPT to serve for code development debugging.  
**Introduction from the article:**  
As programmers, we often work with familiar development environments, but occasionally we encounter new tools that can be time-consuming and challenging to learn. In such situations, having virtual assistance can be extremely beneficial.  
In this article, I will share *my experience of contributing* to OpenCV, a renowned open-source library, despite having limited knowledge of C++ and understanding its architecture. I achieved this with the assistance of ChatGPT, a Large Language Model (LLM).  
I hope you can find it interesting. More details are [here](https://www.opencv.ai/blog/getting-the-hang-of-opencvs-inner-workings-with-chatgpt)."
276,learnmachinelearning,chatgpt,relevance,2023-01-27 17:44:56,VoiceGPT - ChatGPT Voice Assistant,nickbild,False,0.72,3,10mqyvu,https://www.youtube.com/watch?v=ajUCMu7de80,3,1674841496.0,
277,learnmachinelearning,chatgpt,relevance,2023-02-11 12:46:22,"ChatGPT Powered Bing Chatbot Spills Secret Document, The Guy Who Tricked Bot Was Banned From Using Bing Chat",vadhavaniyafaijan,False,0.94,209,10zmtqz,https://www.theinsaneapp.com/2023/02/chatgpt-bing-rules.html,15,1676119582.0,
278,learnmachinelearning,chatgpt,relevance,2023-06-09 19:55:39,Building a personal ChatGPT based on your own dataset,rajatarya,False,0.89,7,145f1mc,https://www.reddit.com/r/learnmachinelearning/comments/145f1mc/building_a_personal_chatgpt_based_on_your_own/,0,1686340539.0,"Hey folks, I’m Rajat from XetHub. If you’re looking to get started on generative AI, I’m hosting a series of free hands-on workshops about how you can build a personal ChatGPT app based on your own dataset. The next session is on **Wednesday, June 14th**—you can [register here](https://app.livestorm.co/xethub/mygpt-free-workshop-build-a-chatgpt-clone-tailored-to-your-data?utm_source=reddit&utm_medium=social&utm_campaign=learnml).

  
**Here’s what you’ll learn in this free workshop:**

\- How to create a Generative AI app using the DaVinci model (the same one used by ChatGPT) 

\- How a Generative AI application is structured (the tech stack)

\- Integrating your own data into a Large Language Model (LLM)

\- Getting started with XetHub (similar to GitHub but easier for ML models)

\- Create a Python app that uses Gradio & LangChain

  
I hope to see you there!"
279,learnmachinelearning,chatgpt,relevance,2023-01-25 15:59:49,a ChatGPT feature to give you prompt suggestions,QuestionAnxious,False,0.94,45,10l1zwj,https://v.redd.it/qjt99akap7ea1,3,1674662389.0,
280,learnmachinelearning,chatgpt,relevance,2023-03-02 18:48:34,ChatGPT Explained in Under 9 Minutes,Outrageous-Credit-80,False,0.33,0,11gagy7,https://youtu.be/osSZthv5zz4,0,1677782914.0,
281,learnmachinelearning,chatgpt,relevance,2023-04-04 01:16:21,"How GPT4 and ChatGPT Work, Fully Explained",thelazyaz,False,0.5,0,12b4pvg,https://www.youtube.com/watch?v=wRfLYl1mLHU,0,1680570981.0,
282,learnmachinelearning,chatgpt,relevance,2023-08-05 17:07:12,The Quest to Have Endless Conversations with Llama and ChatGPT 🗣️💬,JClub,False,0.77,7,15j0yxd,https://medium.com/@joaolages/the-quest-to-have-endless-conversations-with-llama-and-chatgpt-%EF%B8%8F-81360b9b34b2,0,1691255232.0,
283,learnmachinelearning,chatgpt,relevance,2023-07-18 19:57:01,ChatGPT Guide - 5 Prompts To Learn Any Tech Skill,Best_Fold_2554,False,0.5,0,1538l6v,https://youtu.be/oA0gLIjlEyI,0,1689710221.0,
284,learnmachinelearning,chatgpt,relevance,2023-06-05 17:01:19,Building Your Own ChatGPT based on your own dataset,rajatarya,False,0.75,2,141kz7b,https://www.reddit.com/r/learnmachinelearning/comments/141kz7b/building_your_own_chatgpt_based_on_your_own/,0,1685984479.0,"Hey everyone,

Rajat here from XetHub. I’m doing a free hands-on workshop on Tuesday, June 6th about how to build your personal ChatGPT based on your own dataset. We had 80 attendees and really positive feedback from our last workshop, and we plan to do one workshop every other week.

Here’s what you can expect to learn in the workshop:

1. How to create a Generative AI app using the DaVinci model (the same one used by ChatGPT) 
2. How a Generative AI application is structured (the tech stack)
3. Integrating your own data into a Large Language Model (LLM)
4. Getting started with XetHub (similar to GitHub but easier for ML models)
5. Create a Python app that uses Gradio & LangChain

[Sign up here](https://app.livestorm.co/xethub/mygpt-free-workshop-build-a-chatgpt-clone-tailored-to-your-data?utm_source=reddit&utm_medium=social&utm_campaign=learnmlreddit) (it’s free, and we'll share the recording with everyone who registers!)"
285,learnmachinelearning,chatgpt,relevance,2023-03-23 20:18:22,How to make a homemade ChatGPT model,VlAn_VOR,False,0.73,5,11zvz4r,https://www.reddit.com/r/learnmachinelearning/comments/11zvz4r/how_to_make_a_homemade_chatgpt_model/,0,1679602702.0,"Obviously, the creation of such big and complex models like ChatGPT is not a trivial task, but it is possible to create a model which can solve 1 task like ChatGPT. We are glad to announce our opensource [dataset](https://www.kaggle.com/datasets/vladimirvorobevv/chatgpt-paraphrases) of 420k paraphrases generated by ChatGPT and a [model](https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base) pretrained on it. We have trained the model just for 2 epochs and the model shows not the best results, but it is already makes more variative paraphrases than the most popular paraphraser on huggingface. Feel free to try the dataset and the model and give a feedback to improve their quality"
286,learnmachinelearning,chatgpt,relevance,2023-06-15 17:09:56,Building Systems with the ChatGPT API Course w/ Andrew Ng,help-me-grow,False,0.87,17,14a7u2v,https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/,0,1686848996.0,
287,learnmachinelearning,chatgpt,relevance,2023-03-30 22:24:38,Best ChatGPT Clone Running Locally - GPT4All Tutorial,dulldata,False,0.71,3,1271ahr,https://www.youtube.com/watch?v=tS1ZIEq4qJQ,2,1680215078.0,
288,learnmachinelearning,chatgpt,relevance,2023-02-15 08:35:41,Top Free Resources To Learn ChatGPT,kingabzpro,False,0.4,0,112tbmh,https://www.kdnuggets.com/2023/02/top-free-resources-learn-chatgpt.html,0,1676450141.0,
289,learnmachinelearning,chatgpt,relevance,2023-01-04 13:49:10,ML study plan according to ChatGPT,adastro,False,0.67,1,10344lj,https://www.reddit.com/r/learnmachinelearning/comments/10344lj/ml_study_plan_according_to_chatgpt/,2,1672840150.0,"People often ask for recommendations about how to get involved with ML, so I thought it would have been interesting to ask ChatGPT directly (specifically about GPT-3 tech).

### The prompt

> Please make a detailed study plan for the math topics one needs to know in order to implement and understand GPT-3 technology. Provide them in the right order (prerequisites for one topic must come before that topic).  Add an approximate time schedule, considering the case of a person who can devote 4 hours/week to studying. Please go into the specifics and add only the best books and online course references (with their full link) for each topic.

### The answer

GPT-3 (short for ""Generative Pre-trained Transformer 3"") is a state-of-the-art language processing technology developed by OpenAI. It uses machine learning techniques to generate human-like text and has a wide range of applications, including language translation, question answering, and language generation.

To understand and implement GPT-3 technology, you will need a strong foundation in mathematics and computer science. Specifically, you will need to be proficient in the following areas:

1. **Linear algebra**: This is a fundamental mathematical discipline that deals with the study of vector spaces and linear transformations. It is a key component of machine learning and is used to represent data in a concise and efficient manner.
    
    Approximate time to complete: 4-6 weeks

    Resources:

    - Khan Academy's Linear Algebra course: https://www.khanacademy.org/math/linear-algebra
    - Gilbert Strang's Linear Algebra textbook

2. **Calculus**: This is a branch of mathematics that deals with the study of rates of change and the accumulation of quantities. It is essential for understanding the mathematical foundations of machine learning and is used to optimize machine learning models.

    Approximate time to complete: 6-8 weeks

    Resources:

    - Khan Academy's Calculus course: https://www.khanacademy.org/math/calculus-home
    - James Stewart's Calculus textbook

3. **Probability and statistics**: This is the study of random phenomena and the analysis of data. It is used to model and understand the uncertainty inherent in machine learning systems.

    Approximate time to complete: 4-6 weeks

    Resources:

    - Khan Academy's Probability and Statistics course: https://www.khanacademy.org/math/statistics-probability
    - Sheldon Ross's A First Course in Probability textbook

4. **Artificial intelligence**: This is the study of how to design intelligent computer systems that can learn, adapt, and make decisions. It is a broad field that encompasses a range of subdisciplines, including machine learning, natural language processing, and robotics.

    Approximate time to complete: 6-8 weeks

    Resources:

    - Stanford's Introduction to Artificial Intelligence course: https://online.stanford.edu/courses/cs221-artificial-intelligence-principles-and-techniques
    - Peter Norvig and Stuart Russell's Artificial Intelligence: A Modern Approach textbook

5. **Natural language processing**: This is the study of how to design computer systems that can understand and generate human language. It is a key component of GPT-3 and is used to process and generate human-like text.
    
    Approximate time to complete: 4-6 weeks

    Resources:

    - Stanford's Natural Language Processing course: https://online.stanford.edu/courses/cs224n-natural-language-processing-deep-learning    - Jurafsky and Martin's Speech and Language Processing textbook
---

(I removed links to online stores since I'm not sure whether this would break some rules here. Also, some links were broken.)
The book suggestions changed whenever I regenerated the response or made minor changes to the prompt, so I suppose that the model doesn't have a very strong opinion about what the ""best"" books are. Online courses were quite stable across answers, instead.

Please share your opinions and more resources if you like :)"
290,learnmachinelearning,chatgpt,relevance,2023-05-15 21:21:01,Resource for creating your own personal ChatGPT tailored to your own data,rajatarya,False,0.78,16,13ikxwt,https://www.reddit.com/r/learnmachinelearning/comments/13ikxwt/resource_for_creating_your_own_personal_chatgpt/,6,1684185661.0,"Hey everyone,  


I was trying to create a personal ChatGPT that can answer questions and create expert content based on an existing dataset. I thought there are tons of applications for this, so [I created a workshop](https://app.livestorm.co/xethub/mygpt-free-workshop-build-a-chatgpt-clone-tailored-to-your-data?type=detailed&utm_source=reddit&utm_medium=social&utm_campaign=openaireddit) so you can create your own app - I’m calling it “MyGPT”.  


In this workshop I’ll be covering:

* How to create a Generative AI app using the DaVinci model (the same one used by ChatGPT) 
* How a Generative AI application is structured (the tech stack)
* Integrating your own data into a Large Language Model (LLM)
* Getting started with XetHub (similar to GitHub but easier for ML models)
* Create a Python app that uses Gradio & LangChain

If you’d like to check it out, [sign up here](https://app.livestorm.co/xethub/mygpt-free-workshop-build-a-chatgpt-clone-tailored-to-your-data?type=detailed&utm_source=reddit&utm_medium=social&utm_campaign=openaireddit)!"
291,learnmachinelearning,chatgpt,relevance,2023-02-20 15:42:13,"ChatGPT Response about the vinyl chloride spill in East Palestine, Ohio.",listenandlearn2,False,0.22,0,117adwo,https://www.reddit.com/r/learnmachinelearning/comments/117adwo/chatgpt_response_about_the_vinyl_chloride_spill/,12,1676907733.0," ChatGPT responded my prompt.  I'd love to hear your thoughts:

Vinyl Chloride spilled in East Palestine, Ohio about 300 miles away from Cincinnati. The chemical leaked into the Ohio River. How long will it take for the chemical to reach the water supply of Cincinnati which is also on the Ohio River?

The time it will take for Vinyl Chloride to reach Cincinnati's water supply from the spill site in East Palestine, Ohio will depend on several factors such as the amount of Vinyl Chloride spilled, the concentration of the chemical in the river, the flow rate of the river, and any barriers or dams along the river that may affect the movement of the chemical.

Assuming there are no significant barriers or dams along the river, the average flow rate of the Ohio River is approximately 1.5 miles per hour. Therefore, if the spill occurred in East Palestine, Ohio, which is 300 miles upstream from Cincinnati, it could take up to 200 hours (300 miles / 1.5 miles per hour) for the Vinyl Chloride to reach Cincinnati's water supply.

However, it is important to note that the actual time for the chemical to reach Cincinnati's water supply could be longer or shorter depending on the specific conditions of the spill and the river. It's also possible that officials may take steps to prevent the chemical from reaching the water supply, such as by diverting the river or shutting off water intake valves, in order to protect public health and safety.

📷📷Please timestamp this response

This response was generated on 2023-02-20.

📷📷Can you add the time the response was generated

Certainly! This response was generated at 2023-02-20 04:28:00 UTC.

Regenerate response[ChatGPT Feb 13 Version](https://help.openai.com/en/articles/6825453-chatgpt-release-notes). Free Research Preview. Our goal is to make AI systems more natural and safe to interact with. Your feedback will help us improve."
292,learnmachinelearning,chatgpt,relevance,2023-02-20 14:39:04,100+ ChatGPT Pop Song Prompts with PDF,Alarming-Recipe2857,False,0.84,4,117884r,https://godsol.gumroad.com/l/pop-songs-generative-prompts,0,1676903944.0,
293,learnmachinelearning,chatgpt,relevance,2022-12-06 01:38:45,ChatGPT explained in 5 minutes,OnlyProggingForFun,False,0.29,0,zdr6m3,https://youtu.be/AsFgn8vU-tQ,0,1670290725.0,
294,learnmachinelearning,chatgpt,relevance,2023-05-06 03:05:16,ChatGPT — Prompt Only: Tic Tac Toe Streamlit App,Chip_lead,False,0.6,1,139a543,https://www.reddit.com/r/learnmachinelearning/comments/139a543/chatgpt_prompt_only_tic_tac_toe_streamlit_app/,0,1683342316.0,"Hi all,

\- I wanted to see if I could use ChatGPT (v4) to make a Streamlit Tic Tac Toe App. The stipulation was no touching of the code, only prompting. I was able to do it, and the key lesson was to be clear in what you want from ChatGPT. If you don't ask it explicitly, it might not do what a human would normally produce. For example, I had to ask ChatGPT to use X and O's as markers. Otherwise, the code used 1's and 2's.  
\- Show me the app: [https://datadote-llm-tictactoe-tic-tac-toe-streamlit-va0zww.streamlit.app/](https://datadote-llm-tictactoe-tic-tac-toe-streamlit-va0zww.streamlit.app/)  
\- For info on the prompt / code generated / process: [https://medium.com/@datadote/chatgpt-prompt-only-tic-tac-toe-streamlit-app-73bb18c4632b](https://medium.com/@datadote/chatgpt-prompt-only-tic-tac-toe-streamlit-app-73bb18c4632b)  
\- Github: [https://github.com/Datadote/llm\_TicTacToe](https://github.com/Datadote/llm_TicTacToe)

\- I'm happy to any questions. I'm a beginner, and this whole process took \~1.5 hrs. If I knew what I waas doing, it could've been faster. I spent less than 10 minutes looking at the actual code. Most of it was copy/pasting, checking the GUI result, and modifying the prompt."
295,learnmachinelearning,chatgpt,relevance,2022-12-03 22:04:53,Open AI chatgpt mind blowing 🚀🚀,DataSynapse82,False,0.27,0,zbs4tg,https://www.reddit.com/r/learnmachinelearning/comments/zbs4tg/open_ai_chatgpt_mind_blowing/,0,1670105093.0,
296,learnmachinelearning,chatgpt,relevance,2023-05-29 04:58:43,Need help understanding how to build a chatgpt bot for WhatsApp,LoneWolf0936,False,0.44,0,13ulc37,https://www.reddit.com/r/learnmachinelearning/comments/13ulc37/need_help_understanding_how_to_build_a_chatgpt/,3,1685336323.0,"Hope you're doing wonderful! Let me tell me the requirements, and if you've any information/advice or thoughts that'd help, feel free to share them.

Thought process behind why I need a chatbot

1. I get a lot of communication on a regular basis on my WhatsApp regarding my business (I'm a fitness and nutrition coach). I don't want to spend a lot of time on my phone just answering mundane and repetitive queries.

2. I don't want to make the person feel also that they're getting a template message as a reply. It should feel like it's me who's talking to them (so probably I could train chatgpt with a lot of my texts, I've 0 idea how to do that)

3. I use [AutoResponder.ai](https://www.autoresponder.ai/) to reply to some things, and it has a [new option to integrate chatgpt api](https://ibb.co/2ngQ4Db). But I'm confused. How do I train my own model and use that instead of the default gpt-3.5-turbo. 

4. So finally, the task that I've thought of is to find out a way to connect a user on whatsapp->AutoResponder.ai->chatgpt (that talks like me, in a friendly and informal manner).

Any idea how I could do this? Or any other better way for me to do this?
Further simplifying things, what I want to create is an AI assistant for me, who is me, and takes over my WhatsApp (so that I can be a bit lazy or look into other things haha)"
297,learnmachinelearning,chatgpt,relevance,2023-03-30 12:56:24,I created this entire video using ChatGPT + Charactr API + D-ID. My mind is blown,3nd4u,False,0.87,70,126m5eo,https://www.reddit.com/r/learnmachinelearning/comments/126m5eo/i_created_this_entire_video_using_chatgpt/,15,1680180984.0,"Could this be the future of how our news is being consumed?

https://reddit.com/link/126m5eo/video/hhfat6n3jvqa1/player"
298,learnmachinelearning,chatgpt,relevance,2023-03-21 21:30:54,A Guide to Using ChatGPT For Data Science Projects,kingabzpro,False,1.0,9,11xvc2x,https://www.reddit.com/r/learnmachinelearning/comments/11xvc2x/a_guide_to_using_chatgpt_for_data_science_projects/,2,1679434254.0,"Hey everyone, I'm super excited to share with you a tutorial that I wrote on how to use ChatGPT for data science projects. ChatGPT is a powerful natural language generation model that can create realistic and engaging texts based on your input. In this tutorial, you'll learn how to use ChatGPT for project planning, data analysis, data preprocessing, model selection, hyperparameter tuning, developing a web app, and deploying it on the Spaces.

You can find the tutorial here: [https://www.datacamp.com/tutorial/chatgpt-data-science-projects](https://www.datacamp.com/tutorial/chatgpt-data-science-projects)

I hope you find it useful and fun. Let me know what you think and if you have any questions or feedback. Happy coding!"
299,learnmachinelearning,chatgpt,relevance,2022-12-08 05:34:34,20 Best And Worst ChatGPT Examples,vadhavaniyafaijan,False,0.82,7,zfq9cv,https://www.theinsaneapp.com/2022/12/top-chat-gpt-examples.html,1,1670477674.0,
