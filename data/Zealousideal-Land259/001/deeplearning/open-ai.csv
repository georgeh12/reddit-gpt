,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,id,url,num_comments,created,body
0,deeplearning,open-ai,top,2023-03-03 21:08:20,Meta’s LLaMa weights leaked on torrent... and the best thing about it is someone put up a PR to replace the google form in the repo with it 😂,RandomForests92,False,0.99,185,11hezvk,https://i.redd.it/olnsv438alla1.jpg,23,1677877700.0,
1,deeplearning,open-ai,top,2023-01-27 10:45:48,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.95,118,10mhyek,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
2,deeplearning,open-ai,top,2020-08-05 10:58:06,image-GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model,OnlyProggingForFun,False,0.94,96,i437pt,https://www.youtube.com/watch?v=FwXQ568_io0,6,1596625086.0,
3,deeplearning,open-ai,top,2021-12-29 08:03:22,I wrote a program with OpenAI's Codex that fixes errors,tomd_96,False,0.94,95,rr2wme,https://v.redd.it/jupdtry6vf881,6,1640765002.0,
4,deeplearning,open-ai,top,2023-04-05 01:36:40,Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT,Time_Key8052,False,0.95,87,12c43uu,https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/,19,1680658600.0,"Vicuna : ChatGPT Alternative, Open-Source, High Quality and Low Cost 

&#x200B;

[ Relative Response Quality Assessed by GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a072b91)

Vicuna-13B has demonstrated competitive performance against other open-source models, such as Stanford Alpaca, by fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.

Evaluation using GPT-4 as a judge shows that Vicuna-13B achieves more than 90% of the quality of OpenAI ChatGPT and Google Bard AI, while outperforming other models such as Meta LLaMA (Large Language Model Meta AI) and Stanford Alpaca in more than 90% of cases.

The cost of training Vicuna-13B is approximately $300.

The training and serving code, along with an online demo, are publicly available for non-commercial use.

&#x200B;

More Information : [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT](https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)

Discord Server : [https://discord.gg/h6kCZb72G7](https://discord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysorg](https://twitter.com/lmsysorg)"
5,deeplearning,open-ai,top,2023-01-19 07:55:49,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.9,70,10fw22o,https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/,11,1674114949.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=webp&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
6,deeplearning,open-ai,top,2023-11-16 08:28:46,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,0.84,67,17whz6e,https://v.redd.it/qx68wbuf7o0c1,7,1700123326.0,
7,deeplearning,open-ai,top,2021-07-28 17:45:57,"OpenAI Releases Triton, An Open-Source Python-Like GPU Programming Language For Neural Networks",techsucker,False,0.94,65,otf0fs,https://www.reddit.com/r/deeplearning/comments/otf0fs/openai_releases_triton_an_opensource_pythonlike/,5,1627494357.0,"OpenAI released their newest language, [Triton](https://github.com/openai/triton). This open-source programming language that enables researchers to write highly efficient GPU code for AI workloads is Python-compatible and comes with the ability of a user to write in as few as 25 lines, something on par with what an expert could achieve. OpenAI claims this makes it possible to reach peak hardware performance without much effort, making creating more complex workflows easier than ever before!

Researchers in the field of Deep Learning often rely on native framework operators. However, this can be problematic because it requires many temporary tensors to work, which may hurt performance at scale for neural networks. Writing specialized GPU kernels is a more convenient solution, but surprisingly difficult due to intricacies when programming them according to GPUs. It was challenging to find a system that provides the flexibility and speed required while also being easy enough for developers to understand. This has led researchers at OpenAI in improving Triton, which was initially founded by one of their teammates.

Quick Read: [https://www.marktechpost.com/2021/07/28/openai-releases-triton-an-open-source-python-like-gpu-programming-language-for-neural-networks/](https://www.marktechpost.com/2021/07/28/openai-releases-triton-an-open-source-python-like-gpu-programming-language-for-neural-networks/) 

Paper: http://www.eecs.harvard.edu/\~htk/publication/2019-mapl-tillet-kung-cox.pdf

Github: https://github.com/openai/triton"
8,deeplearning,open-ai,top,2020-02-24 15:44:25,Everything you need to know about computer vision in one repo,frlazzeri,False,0.94,67,f8t2cf,https://www.reddit.com/r/deeplearning/comments/f8t2cf/everything_you_need_to_know_about_computer_vision/,5,1582559065.0,"*This post was co-authored by JS Tan, Patrick Buehler, Anupam Sharma and Jun Ki Min.*

In recent years, we’ve seen extraordinary growth in Computer Vision, with applications in image understanding, search, mapping, semi-autonomous or autonomous vehicles and many more .

The ability for models to understand actions in a video , a task that was unthinkable just a few years ago , is now something that we can achieve with relatively high accuracy and in near real-time.

However, the field is not particularly welcoming for newcomers. Without prior experience or guidance, building an accurate classifier can easily take weeks. Unless you’re ready to spend a long-time learning computer vision, it’s extremely hard to master the basics, let alone begin to explore some of the cutting-edge technologies in the field. Even for computer vision experts, building a quick Proof of Concept (POC) can be non trivial and could easily end up taking many days to put together.

At [Microsoft ](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri), we have been working for many years on diverse Computer Vision solutions for our customers and collected our learning into our new public [Microsoft](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) repository: [Custom vision repo](https://github.com/microsoft/ComputerVision-recipes?WT.mc_id=medium-article-lazzeri).

The goal of [this repository](https://github.com/microsoft/ComputerVision-recipes?WT.mc_id=medium-article-lazzeri) is to provide examples and best practice guidelines for building computer vision systems on [Azure](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) , and to share this with the open-source community . More specifically, our goal was to create a [repository](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) that will help us to provide solutions rapidly to the community and to customers that we work with , or with on-boarding new team members who may have expertise in data science, but not specifically in computer vision. From mastering some of the most common scenarios in the field, like image classification, object detection , and image similarity, to exploring cutting edge scenarios like activity recognition and crowd counting, [this repo](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) will guide you through building models, fine-tuning them, and using them in real-world scenarios.

We’re kicking off our repo with **5 scenarios.** You can find the links to the repos here:

* [Classification](https://github.com/microsoft/computervision-recipes/tree/master/scenarios/classification?WT.mc_id=medium-article-lazzeri)
* [Similarity](https://github.com/microsoft/computervision-recipes/tree/master/scenarios/similarity?WT.mc_id=medium-article-lazzeri)
* [Detection](https://github.com/microsoft/computervision-recipes/tree/master/scenarios/detection?WT.mc_id=medium-article-lazzeri)
* [Action Recognition](https://github.com/microsoft/computervision-recipes/tree/master/contrib/action_recognition?WT.mc_id=medium-article-lazzeri)
* [Crowd Counting](https://github.com/microsoft/computervision-recipes/tree/master/contrib/crowd_counting?WT.mc_id=medium-article-lazzeri)

Rather than creating implementations from scratch, we draw from popular state-of-the-art libraries (e.g. fast.ai and [torchvision ](https://pytorch.org/docs/stable/torchvision/index.html)), and we build additional utility around loading image data, optimizing models , and evaluating models. In addition, we aim to answer the frequently asked questions, try to explain the deep learning intuitions, and highlight common pitfalls.

Whether you a re an expert in computer vision or just getting your hands wet, we believe [this repository](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) offers something for you . For the beginner, [this repo](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) will guide you through building a state-of-the-art model and help you develop an intuition for the craft. For the experts, this repository can quickly get you to a strong baseline model which is easy to extend using custom Python/PyTorch code. In addition, the repository also aims to provide support with:

1. [The full data science process](https://docs.microsoft.com/azure/machine-learning/team-data-science-process/overview?WT.mc_id=medium-article-lazzeri).
2. [The tooling to succeed on Azure](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri).

We hope that these examples and utilities will make it easier and faster for developers to create custom vision applications.

# The Data Science Process

The [Computer Vision Recipes GitHub repository](https://github.com/microsoft/ComputerVision-recipes?WT.mc_id=medium-article-lazzeri) shows you how to approach the five key steps of the data science process and provides utilities to enrich each of the steps :

1. **Evaluating** — Evaluate your model. Depending on the metric you’re interested in optimizing, you may want to explore different methods of evaluation.
2. **Model selection and optimization** — Tun e and optimize hyperparameters to get the highest performing model. Because Computer Vision models are often computationally costly, we show you how to seamlessly scale your parameter tuning into Azure .
3. **Operationalizing** — Operationalize models in a production environment on Azure by deploying it onto Kubernetes.

Inside the computer vision recipes [repo,](https://github.com/microsoft/ComputerVision-recipes?WT.mc_id=medium-article-lazzeri) we have added a lot of utility to support common tasks such as loading data sets in the format expected by different algorithms, splitting training/test data, and evaluating model outputs .

This computer vision repository also has deep integration with the [Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) to complement your work locally. We provide code examples on how you can optionally and easily scale your training into the cloud, and how you can deploy your models for production workloads.

**Azure Cognitive Services**

Note that for certain computer vision problems, you may not need to build your own models. Instead, pre-built or easily customizable solutions exist which do not require any custom coding or machine learning expertise.

* [Vision Services](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/?WT.mc_id=medium-article-lazzeri) are a set of pre-trained REST APIs which can be called for image tagging, OCR, video analytics, and more. These APIs work out of the box and require minimal expertise in machine learning but have limited customization capabilities. See the various demos available to get a feel for the functionality (e.g. Computer Vision).
* [Custom Vision](https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/?WT.mc_id=medium-article-lazzeri) is a SaaS service to train and deploy a model as a REST API given a user-provided training set. All steps including image upload, annotation, and model deployment can be performed using either the UI or a Python SDK. Training image classification or object detection models can be achieved with minimal machine learning expertise. The Custom Vision offers more flexibility than using the pre-trained cognitive services APIs but requires the user to bring and annotate their own data.

Before using the Computer Vision repository, we strongly recommend evaluating if these can sufficiently solve your problem.

To give you a sense of how you can use our repo to build a state of the art (SOTA) model, here is a preview of how simple it is to create an Object Detection model. Of course, you can go much deeper and add custom PyTorch code, but getting started is as simple as this :

**1. Load your data**

The first step is to load your data — we help you do this with a simple object that automatically parses your data and the annotations:

`from utils_cv.detection.data import DetectionLoader data = DetectionLoader(""path/to/data"")`

**2. Train/fine-tune your model**

Then we create a ‘learner’ object that helps you manage and train your model. By default, it will use torchvision’s Faster R-CNN model. But you can easily switch it out.

`from utils_cv.detection.model import DetectionLearner detector = DetectionLearner(data) detector.fit()`

**3. Evaluate**

Finally, lets evaluate our model using the built-in helper functions. We can look at the precision and recall curves to give us a sense of how our model is performing.

`from utils_cv.detection.plot import plot_pr_curves eval = detector.evaluate() plot_pr_curves(eval)`

As we continue to build out of repository, we will be looking for new computer vision scenarios to unlock . Feel free to reach out to [cvbp@microsoft.com](mailto:cvbp@microsoft.com) or post an issue if you wish to see us cover a scenario .

# Additional resources to learn more

To learn more, you can read the following articles and notebooks:

* [Custom vision repo](https://github.com/microsoft/ComputerVision-recipes?WT.mc_id=medium-article-lazzeri)
* Original article: [https://techcommunity.microsoft.com/t5/azure-ai/nearly-everything-you-need-to-know-about-computer-vision-in-one/ba-p/1070311](https://techcommunity.microsoft.com/t5/azure-ai/nearly-everything-you-need-to-know-about-computer-vision-in-one/ba-p/1070311)
* [Vision Services](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/?WT.mc_id=medium-article-lazzeri) on Azure
* [Custom Vision](https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/?WT.mc_id=medium-article-lazzeri) on Azure
* Portfolio of Azure Machine Learning Notebooks: [aka.ms/AzureMLServiceGithub](https://aka.ms/AzureMLServiceGithub)
* Azure Machine Learning: [aka.ms/AzureMLservice](https://aka.ms/AzureMLservice)
* Get started with Azure ML: [aka.ms/GetStartedAzureML](https://aka.ms/GetStartedAzureML)
* Automated Machine Learning Documentation: [aka.ms/AutomatedMLDocs](https://aka.ms/AutomatedMLDocs)
* What is Automated Machine Learning? [aka.ms/AutomatedML](https://aka.ms/AutomatedML)
* Python Microsoft: [aka.ms/PythonMS](https://aka.ms/PythonMS)
* Azure ML for VS Code: [aka.ms/AzureMLforVSCode](https://aka.ms/AzureMLforVSCode)"
9,deeplearning,open-ai,top,2020-05-22 15:23:28,Open AI and Microsoft Can Generate Python Code,cmillionaire9,False,0.89,65,golbq4,https://youtu.be/y5-wzgIySb4,6,1590161008.0,
10,deeplearning,open-ai,top,2021-04-17 14:31:07,*Semantic* Video Search with OpenAI’s CLIP Neural Network (link in comments!),designer1one,False,0.98,66,msrsc4,https://i.redd.it/as278qmzuqt61.gif,9,1618669867.0,
11,deeplearning,open-ai,top,2023-01-20 08:53:58,Gotcha,actual_rocketman,False,0.94,65,10gs1ik,https://i.redd.it/yyh41pnje7da1.jpg,5,1674204838.0,
12,deeplearning,open-ai,top,2020-01-30 18:53:20,OpenAI goes all-in on Facebook's Pytorch machine learning framework,emptyplate,False,0.95,67,ewa9jc,https://venturebeat.com/2020/01/30/openai-facebook-pytorch-google-tensorflow/,0,1580410400.0,
13,deeplearning,open-ai,top,2023-06-05 04:33:14,How Open Ai’s Andrej Karpathy Made One of the Best Tutorials in Deep Learning,0ssamaak0,False,0.93,62,141282u,https://www.reddit.com/r/deeplearning/comments/141282u/how_open_ais_andrej_karpathy_made_one_of_the_best/,3,1685939594.0,"I want you to check [my review](https://medium.com/@0ssamaak0/how-open-ais-andrej-karpathy-made-one-of-the-best-tutorials-in-deep-learning-e6b6445a2d05) on Andrej Karpathy amazing work on explaining how GPT is built

[GitHub Repo](https://github.com/0ssamaak0/Karpathy-Neural-Networks-Zero-to-Hero) for code & more details

&#x200B;

https://preview.redd.it/z204zwtzn44b1.png?width=720&format=png&auto=webp&s=095ea00991ebb295f48b70436456b1f283a50df1"
14,deeplearning,open-ai,top,2021-07-15 17:06:55,"EleutherAI Researchers Open-Source GPT-J, A Six-Billion Parameter Natural Language Processing (NLP) AI Model Based On GPT-3",techsucker,False,1.0,57,okx5hm,https://www.reddit.com/r/deeplearning/comments/okx5hm/eleutherai_researchers_opensource_gptj_a/,5,1626368815.0,"[GPT-J](https://www.eleuther.ai/), a six-billion-parameter natural language processing (NLP) AI model based on GPT-3, has been open-sourced by a team of EleutherAI researchers. The model was trained on an open-source text [dataset of 800GB](https://pile.eleuther.ai/) and was comparable with a GPT-3 model of similar size.

The model was trained using Google Cloud’s v3-256 TPUs using EleutherAI’s Pile dataset, which took about five weeks. GPT-J achieves accuracy similar to OpenAI’s reported findings for their 6.7B parameter version of GPT-3 on standard NLP benchmark workloads. The model code, pre-trained weight files, a Colab notebook, and a sample web page are included in EleutherAI’s release.

Story: [https://www.marktechpost.com/2021/07/15/eleutherai-researchers-open-source-gpt-j-a-six-billion-parameter-natural-language-processing-nlp-ai-model-based-on-gpt-3/](https://www.marktechpost.com/2021/07/15/eleutherai-researchers-open-source-gpt-j-a-six-billion-parameter-natural-language-processing-nlp-ai-model-based-on-gpt-3/) 

Github repository for GPT-J: https://github.com/kingoflolz/mesh-transformer-jax

Colab Notebook: https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab\_demo.ipynb

Web Demo: https://6b.eleuther.ai/"
15,deeplearning,open-ai,top,2023-04-26 09:55:48,"Google researchers achieve performance breakthrough, rendering Stable Diffusion images in sub-12 seconds on a mobile phone. Generative AI models running on your mobile phone is nearing reality.",Lewenhart87,False,0.96,54,12zclny,https://www.reddit.com/r/deeplearning/comments/12zclny/google_researchers_achieve_performance/,3,1682502948.0,"**What's important to know:**

&#x200B;

*  Stable Diffusion is an \\\~1-billion parameter model that is typically resource intensive. DALL-E sits at 3.5B parameters, so there are even heavier models out there.
*  Researchers at Google layered in a series of four GPU optimizations to enable Stable Diffusion 1.4 to run on a Samsung phone and generate images in under 12 seconds. RAM usage was also reduced heavily.
* **Their breakthrough isn't device-specific; rather it's a generalized approach that can add improvements to all latent diffusion models.** Overall image generation time decreased by 52% and 33% on a Samsung S23 Ultra and an iPhone 14 Pro, respectively.
*  Running generative AI locally on a phone, without a data connection or a cloud server, opens up a host of possibilities. This is just an example of how rapidly this space is moving as Stable Diffusion only just released last fall, and in its initial versions was slow to run on a hefty RTX 3080 desktop GPU.

&#x200B;

As small form-factor devices can run their own generative AI models, what does that mean for the future of computing? Some very exciting applications could be possible.

&#x200B;

If you're curious, the paper (very technical) [can be accessed here.](https://arxiv.org/abs/2304.11267)"
16,deeplearning,open-ai,top,2023-01-05 20:07:38,Greg Yang's work on a rigorous mathematical theory for neural networks,IamTimNguyen,False,0.96,53,1048oc1,https://www.reddit.com/r/deeplearning/comments/1048oc1/greg_yangs_work_on_a_rigorous_mathematical_theory/,3,1672949258.0,"Greg Yang is a mathematician and AI researcher at Microsoft Research who for the past several years has done incredibly original theoretical work in the understanding of large artificial neural networks. In our whiteboard conversation, we get a sample of Greg's work, which goes under the name ""Tensor Programs"" and currently spans five highly technical papers. The route chosen to compress Tensor Programs into the scope of a conversational video is to place its main concepts under the umbrella of one larger, central, and time-tested idea: that of taking a large N limit. This occurs most famously in the Law of Large Numbers and the Central Limit Theorem, which then play a fundamental role in the branch of mathematics known as Random Matrix Theory (RMT). We review this foundational material and then show how Tensor Programs (TP) generalizes this classical work, offering new proofs of RMT.

We conclude with the applications of Tensor Programs to a (rare!) rigorous theory of neural networks. This includes applications to a rigorous proof for the existence of the Neural Network Gaussian Process and Neural Tangent Kernel for a general class of architectures, the existence of infinite-width feature learning limits, and the muP parameterization enabling hyperparameter transfer from smaller to larger networks.

&#x200B;

https://preview.redd.it/y79bih0f7aaa1.png?width=1280&format=png&auto=webp&s=7cf2bde3408e58f3d7dd6e15fbcd3dc103404147

https://preview.redd.it/0hvembyf7aaa1.png?width=1200&format=png&auto=webp&s=9a9889d47630e6c12cd4d192750c63d2bff1e422

Youtube: [https://youtu.be/1aXOXHA7Jcw](https://youtu.be/1aXOXHA7Jcw)

Apple Podcasts: [https://podcasts.apple.com/us/podcast/the-cartesian-cafe/id1637353704](https://podcasts.apple.com/us/podcast/the-cartesian-cafe/id1637353704)

Spotify: [https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG](https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG)

RSS: [https://feed.podbean.com/cartesiancafe/feed.xml](https://feed.podbean.com/cartesiancafe/feed.xml)"
17,deeplearning,open-ai,top,2019-06-16 16:50:21,NVIDIA's AI that generates photorealistic images from your doodles is now in beta stage and open for all to try out,azmodeus99,False,0.96,51,c1c0q6,https://techunalt.com/gaugan-turns-your-scribbles-into-beautiful-landscapes/,4,1560703821.0,
18,deeplearning,open-ai,top,2019-09-19 10:07:59,Google to open AI lab in Bangalore,adssidhu86,False,0.89,51,d6bwsm,https://www.blog.google/around-the-globe/google-asia/google-research-india-ai-lab-bangalore/amp/,0,1568887679.0,
19,deeplearning,open-ai,top,2021-07-12 00:39:45,"Adding guassian noise to Discriminator layers in GAN helps really stablizing training, generates sharper images and avoid mode collapse.",0x00groot,False,0.98,51,oigdgg,https://www.reddit.com/r/deeplearning/comments/oigdgg/adding_guassian_noise_to_discriminator_layers_in/,8,1626050385.0,"Very simple tweak which isn't usually seen in basic GAN tutorials. Might be helpful if you are new to GANs and it's just not converging.

[256x256px, results in 3 hours on colab.](https://preview.redd.it/zum8fg2xgoa71.png?width=901&format=png&auto=webp&s=64dd09e02f7c7418a698eab274026d273175211c)

I am also new to GANs and just learning. I first noticed this when learning about GANs last year in tensorflow. I followed the most basic tutorial from tf docs. But results were always smudgy, fuzzy and not convincing, and easily collapsing, especially at resolutions >= 128x128. But adding Gaussian noise to each layer of Discriminator dramatically made the results much better. Inspiration was from some ganhacks and papers adding noise to just the input or generator, but haven't seen results for discriminator.

Found similar results when implementing the same in Pytorch recently. Models with same architecture, config and seed. Only difference is adding of guassian noise to discriminator layers gives much better results. Have had success in training 128x128 and 256x256 face generation in just a few hours on colab.

Below are few results. Using very basic convolutional gan architecture.

[128x128 Results with guassian noise in discriminator layers on celeba](https://preview.redd.it/w6jmzssvgoa71.png?width=1782&format=png&auto=webp&s=94158edc0cf52b6dbe82eae176e49e3334d9fb82)

[128x128 Results without noise on celeba, easily collapsed.](https://preview.redd.it/uqsr8uvugoa71.png?width=1785&format=png&auto=webp&s=ce3dcdcece1893dec277e31eac1eb7734cd7da0d)

Also results on Flickr dataset 256x256 resolution in 3 hours.

[256x256 Flickr dataset with guassian noise in discriminator layers](https://preview.redd.it/vd72llwsgoa71.png?width=1782&format=png&auto=webp&s=8ce6240bf7b8e50e86ad81cc6af5a3e6d2400f99)

Ofcourse results aren't too crazy and still contain artifacts as this is a very basic architecture and trained for a short time. But not bad and much better as compared to without gaussian noise in discriminator.

More results runs and logs of runs with no noise, noise decay, adding noise to only generator layers, adding noise only to input, both generator and discriminator can be found here. Open different runs to see more outputs at different timesteps.: [https://wandb.ai/shivamshrirao/facegan\_pytorch](https://wandb.ai/shivamshrirao/facegan_pytorch)

View more 256px results [here](https://wandb.ai/shivamshrirao/facegan_pytorch/runs/1424g5hk).

Pytorch code: [https://github.com/ShivamShrirao/facegan\_pytorch](https://github.com/ShivamShrirao/facegan_pytorch)

Tensorflow code (bit old): [https://github.com/ShivamShrirao/GANs\_TF\_2.0](https://github.com/ShivamShrirao/GANs_TF_2.0)

[256x256px Flickr dataset](https://preview.redd.it/m8hxbzrqgoa71.png?width=901&format=png&auto=webp&s=24a4abde9496b96a93b7abf85f739d87335229fa)"
20,deeplearning,open-ai,top,2023-03-29 14:13:46,AI Startup Cerebras releases open source ChatGPT-like alternative models,Time_Key8052,False,0.94,47,125pbbf,https://gpt4chatgpt.tistory.com/entry/Cerebras-releases-open-source-ChatGPT-like-alternative-models,14,1680099226.0,
21,deeplearning,open-ai,top,2023-03-25 04:24:49,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.91,45,121agx4,https://www.reddit.com/r/deeplearning/comments/121agx4/do_we_really_need_100b_parameters_in_a_large/,54,1679718289.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
22,deeplearning,open-ai,top,2020-08-27 10:20:19,GenRL: PyTorch-First Reinforcement Learning library,sharadchitlangia,False,0.87,42,ihii3i,https://www.reddit.com/r/deeplearning/comments/ihii3i/genrl_pytorchfirst_reinforcement_learning_library/,5,1598523619.0,"Github: [https://github.com/SforAiDl/genrl](https://github.com/SforAiDl/genrl)

Reinforcement learning research is moving faster than ever before. In order to keep up with the growing trend and ensure that RL research remains reproducible, GenRL aims to aid faster paper reproduction and benchmarking by providing the following main features:

* **PyTorch-first**: Modular, Extensible and Idiomatic Python
* **Tutorials and Documentation:** *We have over 20 tutorials assuming no knowledge of RL concepts. Basic explanations of algorithms in Bandits, Contextual Bandits, RL, Deep RL, etc.*
* **Unified Trainer and Logging class**: code reusability and high-level UI
* **Ready-made algorithm implementations**: ready-made implementations of popular RL algorithms.
* **Faster Benchmarking**: automated hyperparameter tuning, environment implementations, etc.

The core of our library is centered around RL, having policies, values, actor critics, etc. And with trainers and loggers, the only part to care about is to have the right functions implemented and everything else is taken care of!

By integrating these features into GenRL, we aim to eventually support **any new algorithm implementation in less than 100 lines**. **We're also looking for more Open Source Contributors!**

Currently, the library has implementations of popular classical and Deep RL agents that ready to be deployed. Apart from these, various Bandit algorithms are a part of GenRL. It has various abstraction layers that make the addition of new algorithms easy for the user. Do give us a star!

&#x200B;

[Vanilla DQN](https://preview.redd.it/d8rjc9zltij51.png?width=1548&format=png&auto=webp&s=55adf6bef31c0e720867cc2628ac8ca29b5b6f6a)

[Training a DoubleDQN would only require changing a single function](https://preview.redd.it/cg4cua1ltij51.png?width=1784&format=png&auto=webp&s=980f31b95ad3ea0e924065508043da3b2cbf6cba)

[Training a DuelingDQN would only require changing a single function](https://preview.redd.it/o97uu41ltij51.png?width=1682&format=png&auto=webp&s=97db9c8f6eb0cd0664cd59f85ed20375aa9d4ab8)"
23,deeplearning,open-ai,top,2020-09-11 15:37:20,[R] OpenAI ‘GPT-f’ Delivers SOTA Performance in Automated Mathematical Theorem Proving,Yuqing7,False,0.95,44,iqsul7,https://www.reddit.com/r/deeplearning/comments/iqsul7/r_openai_gptf_delivers_sota_performance_in/,2,1599838640.0,"San Francisco-based AI research laboratory OpenAI has added another member to its popular GPT (Generative Pre-trained Transformer) family. In a new paper, OpenAI researchers introduce GPT-f, an automated prover and proof assistant for the Metamath formalization language.

Here is a quick read: [OpenAI ‘GPT-f’ Delivers SOTA Performance in Automated Mathematical Theorem Proving](https://syncedreview.com/2020/09/10/openai-gpt-f-delivers-sota-performance-in-automated-mathematical-theorem-proving/)

The paper *Generative Language Modeling for Automated Theorem Proving* is on [arXiv](https://arxiv.org/pdf/2009.03393.pdf)."
24,deeplearning,open-ai,top,2020-10-25 19:26:17,Google AI Introduces Performer: A Generalized Attention Framework based on the Transformer architecture,ai-lover,False,0.92,45,jhzd4q,https://www.reddit.com/r/deeplearning/comments/jhzd4q/google_ai_introduces_performer_a_generalized/,2,1603653977.0,"[Transformer model](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html), a deep learning framework, has achieved state-of-the-art results across diverse domains, including [natural language](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html), [conversation](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html), [images](https://openai.com/blog/image-gpt/), and even [music](https://magenta.tensorflow.org/music-transformer). The core block of any Transformer architecture is the [attention module](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html), which computes similarity scores for all pairs of positions in an input sequence. Since it requires quadratic computation time and quadratic memory size of the storing matrix, with the increase in the input sequence’s length, its efficiency decreases.

Thus, for long-range attention, one of the most common methods is [sparse attention](https://openai.com/blog/sparse-transformer/). It reduces the complexity by computing selective similarity scores from the sequence, based on various methods. There are still certain limitations like unavailability of efficient sparse-matrix multiplication operations on all accelerators, lack of theoretical guarantees, insufficiency to address the full range of problems, etc.

**Introduction to “Performer”**

To resolve these issues, Google AI introduces the [Performer](https://arxiv.org/abs/2009.14794), a Transformer architecture with attention mechanisms that scale linearly. The framework is implemented by [Fast Attention Via Positive Orthogonal Random Features (FAVOR+) algorithm](https://github.com/google-research/google-research/tree/master/performer/fast_self_attention), providing scalable low-variance and unbiased estimation of attention mechanisms expressed by random feature maps decompositions (in particular, regular softmax-attention). Mapping helps in preserving linear space and time complexity.

**Full Summary:** [https://www.marktechpost.com/2020/10/25/google-ai-introduces-performer-a-generalized-attention-framework-based-on-the-transformer-architecture/](https://www.marktechpost.com/2020/10/25/google-ai-introduces-performer-a-generalized-attention-framework-based-on-the-transformer-architecture/) 

**Github:** [https://github.com/google-research/google-research](https://github.com/google-research/google-research) 

**Paper:** [https://arxiv.org/pdf/2009.14794.pdf](https://arxiv.org/pdf/2009.14794.pdf)"
25,deeplearning,open-ai,top,2019-02-18 23:13:02,The Power of AI Generated Stories,00hello,False,0.79,36,as3f4f,https://www.reddit.com/r/deeplearning/comments/as3f4f/the_power_of_ai_generated_stories/,24,1550531582.0,"For the past 3 years, I've made a modest income generating genre fiction novels using deep learning and publishing them. By A/B testing, constant iteration and moving fast using many different pen-names I've been able to discover and serve tiny niches a human author would have trouble even finding. Most of the credit goes to a large and painstakingly annotated data set (which oddly enough, occurred to me just a few hours after my father died).  I'm continuously in awe of how powerful the ability to tell people a fictional story about the world is but more alarmingly, that often times the only difference between my books and many books I see under ""non-fiction"" is the category we each selected in the drop down menu.

&#x200B;

No matter what your opinion is on Open AI's decision to restrict their model,  this technology has much more profound and dangerous implications than most people realize. Whether you want people to build a pyramid, believe in Jesus or buy a stock, stories are how you program people and cultures. Yuval Noah Harari makes a good case in his books that our ability to share and collectively believe in fictional stories is what made us the dominant species on the planet.

&#x200B;

That being said, I now have a 240 GB training set of over 2.7 million narratives from fiction and non-fiction, about 85-90% English, each with very structuralized meta data, including the names of the central people in the narrative, directional graphs about their relationships, tags of their behavioural traits, tags of narrative themes, outcomes, points of view etc. etc. I have more data than my AI skills or my computational resources can effectively utilize. If there's anyone here with a very strong DL and NLP background who I can partner with to get access to the resources needed to train on my entire data set, please let me know.

&#x200B;

Edit: Just so there's no ambiguity, No I did not generate the 2.7 million narratives in the data set. That's a much, much larger version of the original data set I trained the first model with 3 years ago. That's what this whole post is about. I intend to train a brand new model with that."
26,deeplearning,open-ai,top,2023-02-02 08:55:09,Any of you know a local and Open Source equivalent to Eleven Labs text to speech AI ?,lordnyrox,False,0.95,41,10rlbc4,https://www.reddit.com/r/deeplearning/comments/10rlbc4/any_of_you_know_a_local_and_open_source/,43,1675328109.0,
27,deeplearning,open-ai,top,2023-02-11 06:59:00,⭕ New Open-Source Version Of ChatGPT,LesleyFair,False,0.86,37,10zepkt,https://www.reddit.com/r/deeplearning/comments/10zepkt/new_opensource_version_of_chatgpt/,3,1676098740.0,"GPT is getting competition from open-source.

A group of researchers, around the YouTuber [Yannic Kilcher](https://www.ykilcher.com/), have announced that they are working on [Open Assistant](https://github.com/LAION-AI/Open-Assistant). The goal is to produce a chat-based language model that is much smaller than GPT-3 while maintaining similar performance.

If you want to support them, they are crowd-sourcing training data [here](https://open-assistant.io/).

**What Does This Mean?**

Current language models are too big.

They require millions of dollars of hardware to train and use. Hence, access to this technology is limited to big organizations. Smaller firms and universities are effectively shut out from the developments.

Shrinking and open-sourcing models will facilitate academic research and niche applications.

Projects such as Open Assistant will help to make language models a commodity. Lowering the barrier to entry will increase access and accelerate innovation.

What an exciting time to be alive! 

Thank you for reading! I really enjoyed making this for you!  
The Decoding ⭕ is a thoughtful weekly 5-minute email that keeps you in the loop about machine research and the data economy. [Click here to sign up](https://thedecoding.net/)!"
28,deeplearning,open-ai,top,2020-09-06 21:14:59,"Facebook AI open-sources Opacus, a new high-speed library for training PyTorch models with differential privacy (DP)",ai-lover,False,0.86,33,inu5du,https://www.reddit.com/r/deeplearning/comments/inu5du/facebook_ai_opensources_opacus_a_new_highspeed/,2,1599426899.0,"With the growing interest in machine learning (ML), the use of differential privacy is trending in analytics. It’s a mathematical rigorous framework for quantifying the anonymization of sensitive data. Keeping in mind this growing interest, Facebook AI launched Opacus.

Opacus is the new high-speed library for training PyTorch models with differential privacy. With minimal required code modifications, this library supports training and has minimal impact on training performance. Thus, It provides an easier path to adopt differential privacy in machine learning and boost research. Compared to the existing state-of-the-art methods, Opacus has a significant advantage of being more scalable.

Blog: [https://www.marktechpost.com/2020/09/06/facebook-ai-open-sources-opacus-a-new-high-speed-library-for-training-pytorch-models-with-differential-privacy-dp/](https://www.marktechpost.com/2020/09/06/facebook-ai-open-sources-opacus-a-new-high-speed-library-for-training-pytorch-models-with-differential-privacy-dp/)

Github: [https://github.com/pytorch/opacus?](https://github.com/pytorch/opacus?)"
29,deeplearning,open-ai,top,2022-11-03 23:55:15,BlogNLP: AI Writing Tool,britdev,False,1.0,38,ylj1ux,https://www.reddit.com/r/deeplearning/comments/ylj1ux/blognlp_ai_writing_tool/,9,1667519715.0,"Hey everyone,

I created this web app using Open AI's GPT-3 (Davinci model). The purpose here is to provide a free tool to allow people to generate blog content/outlines/headlines and help with writer's block. Will continue to improve it over time, but just a side project I figured would provide some value to you all. Hope you all enjoy and please share ❤️

[https://www.blognlp.com/](https://www.blognlp.com/)"
30,deeplearning,open-ai,top,2021-06-14 06:34:33,"This Chinese Super Scale Intelligence Model, ‘Wu Dao 2.0’, Claims To Be Trained Using 1.75 Trillion Parameters, Surpassing All Prior Models to Achieve a New Breakthrough in Deep Learning",ai-lover,False,0.92,39,nzgkj3,https://www.reddit.com/r/deeplearning/comments/nzgkj3/this_chinese_super_scale_intelligence_model_wu/,9,1623652473.0,"Deep learning is one area of technology where ambitiousness has no barriers. According to a recent announcement by [The Beijing Academy of Artificial Intelligence (BAAI)](https://www.baai.ac.cn/), in China, yet another milestone has been achieved in the field with its “Wu Dao” AI system. The [GPT 3](https://www.marktechpost.com/2020/08/02/gpt-3-a-new-breakthrough-in-language-generator/) brought in new interest for all the AI researchers, the super scale pre training models. By this approach and making use of 175 billion parameters, it managed to achieve exceptional performance results across the natural language processing tasks (NLP). However, the lacking component is its inability to have any form of cognitive abilities or common sense. Therefore, despite the size, even these models cannot indulge in tasks such as open dialogues, visual reasoning, and so on. With Wu Dao, the researchers plan to address this issue. This is China’s first attempt at a home-grown super-scale intelligent model system. 

Article: [https://www.marktechpost.com/2021/06/13/this-chinese-super-scale-intelligence-model-wu-dao-2-0-claims-to-be-trained-using-1-75-trillion-parameters-surpassing-all-prior-models-to-achieve-a-new-breakthrough-in-deep-learning/](https://www.marktechpost.com/2021/06/13/this-chinese-super-scale-intelligence-model-wu-dao-2-0-claims-to-be-trained-using-1-75-trillion-parameters-surpassing-all-prior-models-to-achieve-a-new-breakthrough-in-deep-learning/?_ga=2.13897584.636390090.1623335762-488125022.1618729090)

Reference: [https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/](https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/)"
31,deeplearning,open-ai,top,2021-10-10 02:22:26,Generate READMEs Using AI,tomd_96,False,0.97,36,q4z8wm,https://www.reddit.com/r/deeplearning/comments/q4z8wm/generate_readmes_using_ai/,2,1633832546.0,"&#x200B;

https://i.redd.it/nbnqpn0f9js71.gif

You can use this program I wrote to generate readmes: [https://github.com/tom-doerr/codex-readme](https://github.com/tom-doerr/codex-readme)

It's far from perfect, but I'm still surprised how well it works if you give it a few tries. It often generates interesting usage examples and explains the available command line options.

You probably won't use this for larger projects, but I think this can make sense for small projects or single scripts. Many small scripts are very useful but might never be published because of the work that is required to document and explain it. Using this AI might assist you with that.

Be aware that you need to get access to OpenAI's Codex API to use it.

What do you think?"
32,deeplearning,open-ai,top,2021-06-18 15:37:27,"[R] Game On! MIT, Allen AI & Microsoft Open-Source a Suite of AI Programming Puzzles",Yuqing7,False,0.99,35,o2rxis,https://www.reddit.com/r/deeplearning/comments/o2rxis/r_game_on_mit_allen_ai_microsoft_opensource_a/,1,1624030647.0,"A research team from MIT, Allen Institute for AI and Microsoft Research open-sources Python Programming Puzzles (P3), a novel programming challenge suite that captures the essence of puzzles and can be used to teach and evaluate an AI's programming proficiency. 

Here is a quick read: [Game On! MIT, Allen AI & Microsoft Open-Source a Suite of AI Programming Puzzles.](https://syncedreview.com/2021/06/18/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-44/)

The paper *Programming Puzzles* is on [arXiv](https://arxiv.org/abs/2106.05784)."
33,deeplearning,open-ai,top,2020-12-30 12:54:23,Paper reading group,porpkcab,False,0.97,34,kn1r63,https://www.reddit.com/r/deeplearning/comments/kn1r63/paper_reading_group/,63,1609332863.0,"Hi, fellow (ex-)scholars!

TL;DR: I want to set up a paper reading group in current machine learning research. If you want to join, please reply or DM me.

I obtained a master's degree in AI two years ago. Since then, I feel like it's hard to keep up with research in deep learning and stay in the loop. I can imagine that there are multiple scholars and ex-scholars like me who want to stay up-to-date with current research, broaden and deepen their knowledge in AI, meet like-minded people, discuss ideas, and improve their presentation skills. So I was thinking that it might be a fun idea to start a paper reading group. If you are interested, please reply or DM me, and we'll see if we can set something up.

I would be very open to suggestions and want this to be a joint effort. However, currently, I had the following in mind: Every two/three weeks we organize a meeting over zoom. In this meeting, one participant will present a summary of the chosen paper, probably using some slides. All other participants have read the paper to some degree such that we can ask questions and discuss the paper and its implications after the presentation. On Slack, we will appoint the next presenter. He/she can pick their own paper, but we will also start a poll where everyone can add suggestions and vote for the papers they are most interested in. Ideally, I would say that the group is not too big (~5-30 max.), and is somewhat fixed. This way, we can get to know each other a bit, create some accountability, and keep the discussions lively!
In terms of topics, I'd say that anything AI-related goes; Theoretical or applied research, NLP, CV, from fundamental papers on SVM's to state-of-the deep learning research, but also evolutionary algorithms, knowledge graphs, and anything in-between. Based on current research interests, deep learning research of the last few years will probably be the popular topic though.

Looking forward to discussing papers with you!

Edit: Wow, that's quite some interest. I believe you should be able to join with this link: 👋  Let’s move this to Slack! We’ve got 56 folks from the team there already. You can sign up here: https://join.slack.com/t/aischolars/shared_invite/zt-kwbtz1fq-vaQaoXKBps9masEX7wYXkg"
34,deeplearning,open-ai,top,2021-02-18 14:52:00,The world's largest scale Turing Test / Do you think OpenAI's GPT3 is good enough to pass the Turing Test?,theaicore,False,0.93,36,lmog2d,https://www.theaicore.com/imitationgame?utm_source=reddit,11,1613659920.0,
35,deeplearning,open-ai,top,2021-08-25 00:54:24,"DeepMind Open-Sources Perceiver IO, A General-Purpose Deep Learning Model Architecture That Handles A Wide Range of Data and Tasks",techsucker,False,0.98,32,pb0hmd,https://www.reddit.com/r/deeplearning/comments/pb0hmd/deepmind_opensources_perceiver_io_a/,2,1629852864.0,"Recently, DeepMind has open-sourced Perceiver IO–a general-purpose deep learning model architecture that can handle many different types of inputs and outputs. This “drop-in” replacement for Transformers is powerful enough to outperform baseline models without being constrained by domain knowledge.

A new [preprint on arXiv describes Perceiver IO](https://arxiv.org/pdf/2107.14795.pdf), a more general version of the AI architecture that can produce many different outputs from multiple inputs. This means it is applicable to real-world domains like language and vision as well as difficult games like StarCraft II. Unlike Perceiver, Perceiver IO is an advanced model that overcomes the limitation of only being able to produce very simple outputs by learning how to flexibly query the latent space.

[4 Min Read](https://www.marktechpost.com/2021/08/24/deepmind-open-sources-perceiver-io-a-general-purpose-deep-learning-model-architecture-that-handles-a-wide-range-of-data-and-tasks/) | [Paper](https://arxiv.org/pdf/2107.14795.pdf) | [Codes](https://github.com/deepmind/deepmind-research/tree/master/perceiver)"
36,deeplearning,open-ai,top,2020-04-24 00:46:30,"Who Invented Backpropagation? Hinton Says He Didn’t, but His Work Made It Popular",Yuqing7,False,0.84,32,g6yq11,https://www.reddit.com/r/deeplearning/comments/g6yq11/who_invented_backpropagation_hinton_says_he_didnt/,6,1587689190.0,"One might think that news of the 2019 Honda Prize being awarded to Dr. Geoffrey Hinton “for his pioneering research in the field of deep learning in artificial intelligence (AI)” would prompt the machine learning community to toast the man they call the “Godfather of Deep Learning.” Instead, the gloves came off and what ensued was an unexpected Internet dust-up.

Jürgen Schmidhuber started it. In a blog[ post](http://people.idsia.ch/~juergen/critique-honda-prize-hinton.html#I), the Scientific Director of The Swiss AI Lab IDSI called out the Honda Prize for crediting Hinton with inventing backpropagation, among other things. Schmidhuber argued that “**Hinton has made significant contributions to artificial neural networks (NNs) and deep learning, but Honda credits him for fundamental inventions of others whom he did not cite.**”

Schmidhuber identified what he said were “six false and/or misleading attributions of credit to Dr. Hinton” in the press release. “I’ll point out,” he wrote, “that Hinton’s most visible publications failed to mention essential relevant prior work — this may explain some of Honda’s misattributions.”

The 6,300 word document, ***Critique of Honda Prize for Dr. Hinton*****, was published on Tuesday on the The Swiss AI Lab IDSIA (Istituto Dalle Molle di Studi sull’Intelligenza Artificiale) website. The opening line reads: “We must stop crediting the wrong people for inventions made by others.”**

Today, Hinton, University Professor Emeritus at the University of Toronto, responded on [Reddit](https://www.reddit.com/r/MachineLearning/comments/g5ali0/d_schmidhuber_critique_of_honda_prize_for_dr/fo8rew9?utm_source=share&utm_medium=web2x), “**I have never claimed that I invented backpropagation.** David Rumelhart invented it independently long after people in other fields had invented it. It is true that when we first published we did not know the history so there were previous inventors that we failed to cite. **What I have claimed is that I was the person to clearly demonstrate that backpropagation could learn interesting internal representations and that this is what made it popular.”**

Read more: [Who Invented Backpropagation? Hinton Says He Didn’t, but His Work Made It Popular](https://medium.com/syncedreview/who-invented-backpropagation-hinton-says-he-didnt-but-his-work-made-it-popular-e0854504d6d1)"
37,deeplearning,open-ai,top,2019-05-21 21:08:18,Facebook Open-Sources Pythia for Vision and Language Multimodal AI Models,Yuqing7,False,0.93,31,brfxn0,https://medium.com/syncedreview/facebook-open-sources-pythia-for-vision-and-language-multimodal-ai-models-be480644b538,4,1558472898.0,
38,deeplearning,open-ai,top,2020-04-21 23:00:56,How does the talktotransformer website work so fast?,parrot15,False,0.93,30,g5prf5,https://www.reddit.com/r/deeplearning/comments/g5prf5/how_does_the_talktotransformer_website_work_so/,5,1587510056.0,"At [https://talktotransformer.com/](https://talktotransformer.com/), you can type a prompt and the transformer will autogenerate the text for you using OpenAI's GPT-2 1.5 billion parameter model.

I'm not asking how GPT-2 works, I'm asking something else. When I ran the GPT-2 1.5B model on the free TPU in Google Colab, it took around 20 to 40ish seconds to generate around the same about of text as the website generates per prompt.

And yet, the website is somehow generating text from the prompt almost instantaneously using the 1.5B model. This is on top of all the X number of people who must be using the website at the same time I am, so it is doing text generation concurrently and near-instantaneously using a gigantic model.

I am very confused. Did the creator of the website just use a lot more TPUs/GPUs behind the scenes and is letting them run 24/7 (I don't think so because that would cost a shit ton of money), or am I missing something fundamental here?

This same question can be applied to this website too: [https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is](https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is)

Please keep in mind that I'm relatively new to all of this. Thanks in advance!"
39,deeplearning,open-ai,top,2022-03-12 04:56:16,Microsoft’s Latest Machine Learning Research Introduces μTransfer: A New Technique That Can Tune The 6.7 Billion Parameter GPT-3 Model Using Only 7% Of The Pretraining Compute,No_Coffee_4638,False,0.98,31,tc8u6k,https://www.reddit.com/r/deeplearning/comments/tc8u6k/microsofts_latest_machine_learning_research/,0,1647060976.0,"Scientists conduct trial and error procedures which experimenting, that many times lear to freat scientific breakthroughs. Similarly, foundational research provides for developing large-scale AI systems theoretical insights that reduce the amount of trial and error required and can be very cost-effective.

Microsoft team tunes massive neural networks that are too expensive to train several times. For this, they employed a specific parameterization that maintains appropriate hyperparameters across varied model sizes. The used µ-Parametrization (or µP, pronounced “myu-P”) is a unique way to learn all features in the infinite-width limit. The researchers collaborated with the OpenAI team to test the method’s practical benefit on various realistic cases.

Studies have shown that training large neural networks because their behavior changes as they grow in size are uncertain. Many works suggest heuristics that attempt to maintain consistency in the activation scales at initialization. However, as training progresses, this uniformity breaks off at various model widths.

[**CONTINUE READING MY SUMMARY ON THIS RESEARCH**](https://www.marktechpost.com/2022/03/11/microsofts-latest-machine-learning-research-introduces-%ce%bctransfer-a-new-technique-that-can-tune-the-6-7-billion-parameter-gpt-3-model-using-only-7-of-the-pretraining-compute/)

Paper: https://www.microsoft.com/en-us/research/uploads/prod/2021/11/TP5.pdf

Github:https://github.com/microsoft/mup

https://i.redd.it/7jrt9r3awvm81.gif"
40,deeplearning,open-ai,top,2021-12-02 17:02:12,Facebook AI and University of Guelph Open-Sources Graph HyperNetworks (GHN-2): A Meta-Model That Predicts Starting Parameters For Deep-Learning Neural Networks,techsucker,False,0.93,29,r7bx6y,https://www.reddit.com/r/deeplearning/comments/r7bx6y/facebook_ai_and_university_of_guelph_opensources/,1,1638464532.0,"In machine learning pipelines, deep learning has proved successful in automating feature design. However, many researchers reveal that the techniques for improving neural network parameters are still mostly hand-crafted and computationally inefficient.

To overcome such shortcomings, Facebook AI Research (FAIR) and the University of Guelph have released an updated [Graph HyperNetworks (GHN-2) meta-model](https://arxiv.org/pdf/2110.13100.pdf) that predicts starting parameters for deep-learning neural networks. With no extra training, GHN-2 runs in less than a second on a CPU and predicts values for computer vision (CV) networks that reach up to 77 percent top-1 accuracy on CIFAR-10.

The researchers created the DeepNets-1M dataset to solve the problem of guessing initial parameters for deep-learning models. This dataset contains one million examples of neural network architectures expressed as computational graphs. They then employed meta-learning to train a modified graph hyper-network (GHN) using this dataset, which can be used to forecast parameters for a network architecture that has never been seen before. Even for architectures far larger than the ones used in training, the resulting meta-model performs better at the task. The meta-model revealed parameters that achieved 60% accuracy on CIFAR-10 with no gradient updates when used to start a 24M-parameter ResNet-50. 

Quick Read: [https://www.marktechpost.com/2021/12/02/facebook-ai-and-university-of-guelph-open-sources-graph-hypernetworks-ghn-2-a-meta-model-that-predicts-starting-parameters-for-deep-learning-neural-networks/](https://www.marktechpost.com/2021/12/02/facebook-ai-and-university-of-guelph-open-sources-graph-hypernetworks-ghn-2-a-meta-model-that-predicts-starting-parameters-for-deep-learning-neural-networks/) 

GitHub: https://github.com/facebookresearch/ppuda

Paper: https://arxiv.org/pdf/2110.13100.pdf"
41,deeplearning,open-ai,top,2019-05-15 23:29:08,Where to learn neural network architecture design,DongDilly,False,1.0,32,bp5931,https://www.reddit.com/r/deeplearning/comments/bp5931/where_to_learn_neural_network_architecture_design/,12,1557962948.0,"So people at Deep Mind and OpenAi comes up with great models that are really innovatively designed. Can someone please tell me how and where I can learn the skill to design a neural network for a specific problem.
 Thank you"
42,deeplearning,open-ai,top,2021-04-07 15:53:57,"Baidu Releases ‘PaddlePaddle’ 2.0, Its Deep Learning Platform, With New Features Including Dynamic Graphs, Reorganized APIs (Documentation, Github link included)",techsucker,False,0.95,29,mm571y,https://www.reddit.com/r/deeplearning/comments/mm571y/baidu_releases_paddlepaddle_20_its_deep_learning/,1,1617810837.0,"Baidu Brain, a Chinese core AI technology engine, announces the release of PaddlePaddle 2.0. PaddlePaddle (PArallel Distributed Deep LEarning)) is an open-sourced AI platform released by Baidu Brain in 2016 to apply deep learning(DL) to many products at Baidu, such as NLP (Natural Language Processing), translation, and image processing.

PaddlePaddle’s latest version has features like dynamic (computational) graphs, a new API system, distributed training for trillion-parameter models, and better hardware support.

Summary: [https://www.marktechpost.com/2021/04/07/baidu-releases-paddlepaddle-2-0-its-deep-learning-platform-with-new-features-including-dynamic-graphs-reorganized-apis/](https://www.marktechpost.com/2021/04/07/baidu-releases-paddlepaddle-2-0-its-deep-learning-platform-with-new-features-including-dynamic-graphs-reorganized-apis/) 

API Documentation: [https://www.paddlepaddle.org.cn/documentation/docs/en/api/index\_en.html](https://www.paddlepaddle.org.cn/documentation/docs/en/api/index_en.html) 

GitHub: [https://github.com/PaddlePaddle](https://github.com/PaddlePaddle) 

Gitee: [https://gitee.com/paddlepaddle](https://gitee.com/paddlepaddle)"
43,deeplearning,open-ai,top,2021-07-06 16:56:22,What OpenAI and GitHub’s “AI pair programmer” means for the software industry,bendee983,False,0.93,31,oez127,https://bdtechtalks.com/2021/07/05/openai-github-gpt-3-copilot/,6,1625590582.0,
44,deeplearning,open-ai,top,2018-03-20 01:18:32,"Intel AI: nGraph, A New Open Source Compiler for Deep Learning Systems",dayman56,False,1.0,28,85owce,https://ai.intel.com/ngraph-a-new-open-source-compiler-for-deep-learning-systems/?,0,1521508712.0,
45,deeplearning,open-ai,top,2022-02-09 12:27:17,"Tiny ML for Big Hearts on an 8-bit Microcontroller Predict the possibility of arrhythmias on an 8- bit Microcontroller, without sending the corresponding sensor data to the cloud.",literallair,False,0.9,27,soceka,https://www.reddit.com/r/deeplearning/comments/soceka/tiny_ml_for_big_hearts_on_an_8bit_microcontroller/,3,1644409637.0,"**Things used in this project**  
***Hardware components:***  
Arduino Mega 2560  
***Software apps and online services:***  
Neuton Tiny Machine Learning  
**Story**

In the course of the pandemic, the interest in creating more innovative medical devices has run high, as recent years showed how unpredictable the situation in healthcare can be. Never before have we faced such an acute need for masks, ventilators, oxygen cylinders, and other must-have devices to conquer the pandemic.

All this has become a trigger to develop devices that can work autonomously for a long time, without access to the internet or cloud, just on batteries with ultra-low power consumption.

And most importantly, it's vital that such devices can be made by a wider range of people, even without in-depth technical skills. Perhaps, you’ve heard the story about two engineers from Lombardy who, at the peak of the epidemic in Italy, really saved their city as they began to print plastic valves for ventilators on 3D printers in their office and provided them to hospitals for free. The pandemic unified all, even those who were far from medicine before. You can read the full story from the same [source](https://www.forbes.com/sites/amyfeldman/2020/03/19/talking-with-the-italian-engineers-who-3d-printed-respirator-parts-for-hospitals-with-coronavirus-patients-for-free/?sh=63b2450778f1).

&#x200B;

https://preview.redd.it/fwyyj94ewsg81.png?width=587&format=png&auto=webp&s=cd61a972d17117dec160cb9e8c7162cde7473292

I would also love to share a simple example, so to say - a super user-friendly concept. My goal is to show that any user without data science knowledge at all can make the smallest medical devices smarter (yes, even an 8-bit microcontroller) with the help of tiny ML solutions. Let’s go! Introduction:

In this tutorial, I’d like to provide a vivid example of how the tiny ML approach can help to predict whether there is an impending arrhythmia or not, by running inferences on the microcontroller, without sending the corresponding sensor data to the cloud.

Let’s learn how to train and embed a compact machine learning model into the 8-bit ATmega2560 microcontroller. I deliberately chose such a memory-constrained and primitive microcontroller to show how simple and smart tiny ML devices can be.

Let's start by training a model. I took the original dataset from this resource: [https://www.physionet.org/content/ptbdb/1.0.0/](https://www.physionet.org/content/ptbdb/1.0.0/).

This dataset contains the signals of heart rate oscillations. The signals correspond to electrocardiogram (ECG) shapes of heartbeats for the normal case and the cases affected by different arrhythmias and myocardial infarction.

The goal was to detect abnormal heartbeats affected by arrhythmias and myocardial infarction based on electrocardiogram shapes. All the samples were cropped, downsampled, and padded with zeroes, if necessary, to the fixed dimension of 187.

The final element of each row denotes the class to which that example belongs.

**Features, target, and target metric:**

* 0...186 - sample description
* target - class of sample (0 - normal heartbeat, 1 - heartbeat affected by arrhythmias or myocardial infarction)

I combined all the cases into a CSV file and split it into a dataset for training (11, 641 rows), and a file to make an inference (2, 911 rows). Amplitudes of contractions of the heart muscle act as features for training the model. [Here](https://model.here/) you can download preprocessed training and test datasets that we used for model training and prediction on new data.

**Procedure.Step 1:TinyML Model Training**

For the AI part of my project, I chose the Neuton Tiny ML platform. Having a special algorithm under the hood, Neuton automatically creates an optimal model in terms of accuracy and compactness. And the best part - the model doesn’t need to be compressed (which is perfect since I needed a very small model that would support the 8-bit architecture).

Next, I uploaded a CSV file and selected the column that should be trained to predict. In my case, this was a column where it was indicated whether there was arrhythmia on the cardiogram or not (1 - yes and 0 - no). Since I needed to embed the model into an 8-bit microcontroller, I selected such a setting in the interface (8-bit support) and started the training. Everything happened automatically.

&#x200B;

[Tiny ML model training](https://reddit.com/link/soceka/video/12abdepfwsg81/player)

The model was trained. To assess its quality, I chose the Area Under the Curve. My model turned out really small and accurate:

*Area Under the Curve = 0.96, Model Size = 0.7 Kb, Number of Coefficients = 253.*

**Step 2:Embedding into a Microcontroller**

After that, I downloaded the archive. It appeared immediately upon the completion of training.

The archive contained:

* Information about the model

Files with weight and meta-information in two formats, binary, and HEX, are used in the calculation process.

* Calculator

A set of functions that is an add-on to Neuton's algorithm providing inferences. For instance, the calculator includes functions for loading a model, calling call-back functions like transferring data, receiving calculation results, etc.

* Neuton Library

An algorithm that performs calculations.

* Implementation file

A file in which you can set the logic of actions for the results of calculations based on your business requirements.

&#x200B;

[Embedding into Microcontroller](https://preview.redd.it/mq3ki06hwsg81.png?width=740&format=png&auto=webp&s=36b5ab6fc36b2cd36004bc5abc13eb7a25c9ac50)

As you see, the archive folder contained all the necessary files, that simply could be transferred to the microcontrollers firmware project.

Since I did not have a real cardiograph, I streamed data from a computer. To do this, I developed a simple protocol that consisted of a header, a data section, and a checksum. The packet header had the following structure:

typedef struct

{

uint16\_t preamble;

uint16\_t size;

uint8\_t type;

uint8\_t error;

uint8\_t reserved\[2\];

}

PacketHeader;

I also provided packets with information about the number of model inputs, data transfers for performing predictions, as well as a report on the memory consumed by the calculator RAM and Flash and prediction time:

typedef enum

{

TYPE\_ERROR = 0,

TYPE\_MODEL\_INFO,

TYPE\_DATASET\_INFO,

TYPE\_DATASET\_SAMPLE,

TYPE\_PERF\_REPORT,

}

PacketType;

Then I developed a packet parser that would receive a stream of bytes from the USB-UART interface of the system board as input and, upon receiving a packet with the correct checksum, will activate the callback function for processing data packets.

Let's open the user\_app.c file and create a neural network object:

Static NeuralNet neuralNet = { 0 };

To initialize the neural network object, I called the CalculatorInit function. Upon successful initialization, the callback function CalculatorOnInit was called, in which I loaded the model from the *model.c* file.

For prediction, I called the CalculatorRunInference function. This function, in its turn, activates three callback functions: before and after the prediction, as well as the one that contains the results of the prediction. I filled them in: in the CalculatorOnInferenceStart function I started, and in the CalculatorOnInferenceEnd function I stopped the timer and calculated the minimum, maximum, and average value of the prediction time.

In the CalculatorOnInferenceResult function, I analyzed the class probabilities for the presence/absence of arrhythmia. Upon its absence, I turned on the green LED, but if the arrhythmia was detected, it was the red one. I connected the LEDs to GPIO ports 52 and 53 and sent prediction results to the computer.

In the sketch file, I initialized the neural network object, packet parser, GPIO, and UART ports:

void setup()

{

pinMode(LED\_RED, OUTPUT);

pinMode(LED\_GREEN, OUTPUT);

led\_red(1);

led\_green(1);

initialised = (0 == app\_init());

initialised &= (0 == parser\_init(channel\_on\_valid\_packet, app\_inputs\_size()));

Serial.begin(230400);

}

And I wrote a code to call the parser when receiving data from the UART:

void loop()

{

if (!initialised)

{

while(1)

{

led\_red(1);

led\_green(0);

delay(100);

led\_red(0);

led\_green(1);

delay(100);

}

}

while (Serial.available() > 0)

parser\_parse(Serial.read());

}

Let's compile and upload the sketch to the system board (the ""Verify"" and ""Upload"" buttons). Success!

**Step 3:Running Inference on the Microcontroller**

To emulate the work of a cardiograph, I wrote a simple desktop application using the *libuv* library. The application performed the following actions:

* Sending the vector to the device on which the prediction took place
* Receiving a response from the device, regarding whether the sent cardiogram contained arrhythmia or not, and displaying the response

The interaction between the computer on which the application was running, and the microcontroller on which the prediction was performed occurred through the protocol that was described above in the article. Since the device was connected to the computer via a serial port, the communication took place in a binary format.I programmed the microcontroller so that when an arrhythmia was detected, I could see a red light, if not — then a green light lighted up. Find the link to a video showing how it works below.

&#x200B;

[Arrhythmia is not detected](https://reddit.com/link/soceka/video/t2atoxuiwsg81/player)

&#x200B;

[Arrhythmia is detected](https://reddit.com/link/soceka/video/q0ghyfekwsg81/player)

*Note: When performing similar operations using TensorFlow, we spend most of the time on manual selection of the neural network architecture and its parameters, model conversion, compression, and reduction of the number of operations but I still didn’t manage to embed the model into an 8-bit microcontroller.*  
**Conclusion.**

The pandemic has revealed that healthcare is in need of innovations, and I hope that a big boom in medical-edge devices awaits us. We need more devices that are not afraid of power and Internet outages. Devices that are very cheap and can be easily created by any guy in his office. Stay safe and have arrhythmia only from great love!"
46,deeplearning,open-ai,top,2018-11-14 15:56:16,OpenAI Founder: Short-Term AGI Is a Serious Possibility,gwen0927,False,0.89,27,9x1aqi,https://medium.com/syncedreview/openai-founder-short-term-agi-is-a-serious-possibility-368424f7462f,5,1542210976.0,
47,deeplearning,open-ai,top,2023-04-12 05:21:13,Is OpenAI’s Study On The Labor Market Impacts Of AI Flawed?,LesleyFair,False,0.96,26,12jb4xz,https://www.reddit.com/r/deeplearning/comments/12jb4xz/is_openais_study_on_the_labor_market_impacts_of/,1,1681276873.0,"[Example img\_name](https://preview.redd.it/f3hrmeet1eta1.png?width=1451&format=png&auto=webp&s=20e20b142a2f88c3d495177e540f34bc8ea4312b)

We all have heard an uncountable amount of predictions about how AI will ***terk err jerbs!***

However, here we have a proper study on the topic from OpenAI and the University of Pennsylvania. They investigate how Generative Pre-trained Transformers (GPTs) could automate tasks across different occupations \[1\].

Although I’m going to discuss how the study comes with a set of “imperfections”, the findings still make me really excited. The findings suggest that machine learning is going to deliver some serious productivity gains.

People in the data science world fought tooth and nail for years to squeeze some value out of incomplete data sets from scattered sources while hand-holding people on their way toward a data-driven organization. At the same time, the media was flooded with predictions of omniscient AI right around the corner.

*Let’s dive in and take an* exciting glimpse into the future of labor markets\*!\*

# What They Did

The study looks at all US occupations. It breaks them down into tasks and assesses the possible level of for each task. They use that to estimate how much automation is possible for a given occupation.

The researchers used the [O\*NET database,](https://www.onetcenter.org/database.html) which is an occupation database specifically for the U.S. market. It lists 1,016 occupations along with its standardized descriptions of tasks.

The researchers annotated each task once manually and once using GPT-4. Thereby, each task was labeled as either somewhat (<50%) or significantly (>50%) automatable through LLMs. In their judgment, they considered both the direct “exposure” of a task to GPT as well as to a secondary GPT-powered system, e.g. LLMs integrated with image generation systems.

To reiterate, a higher “exposure” means that an occupation is more likely to get automated.

Lastly, they enriched the occupation data with wages and demographic information. This was used to determine whether e. g. high or low-paying jobs are at higher risk to be automated.

So far so good. This all sounds pretty decent. Sure, there is a lot of qualitative judgment going into their data acquisition process. However, we gotta cut them some slag. These kinds of studies always struggle to get any hard data and so far they did a good job.

However, there are a few obvious things to criticize. But before we get to that let’s look at their results.

# Key Findings

The study finds that 80% of the US workforce, across all industries, could have at least some tasks affected. Even more significantly, 19% of occupations are expected to have at least half of their tasks significantly automated!

Furthermore, they find that higher levels of automation exposure are associated with:

* Programming and writing skills
* Higher wages (contrary to previous research!)
* Higher levels of education (Bachelor’s and up)

Lower levels of exposure are associated with:

* Science and critical thinking skills
* Manual work and tasks that might potentially be done using physical robots

This is somewhat unsurprising. We of course know that LLMs will likely not increase productivity in the plumbing business. However, their findings underline again how different this wave is. In the past, simple and repetitive tasks fell prey to automation.

*This time it’s the suits!*

If we took this study at face value, many of us could start thinking about life as full-time pensioners.

But not so fast! This, like all the other studies on the topic, has a number of flaws.

# Necessary Criticism

First, let’s address the elephant in the room!

OpenAI co-authored the study. They have a vested interest in the hype around AI, both for commercial and regulatory reasons. Even if the external researchers performed their work with the utmost thoroughness and integrity, which I am sure they did, the involvement of OpenAI could have introduced an unconscious bias.

*But there’s more!*

The occupation database contains over 1000 occupations broken down into tasks. Neither GPT-4 nor the human labelers can possibly have a complete understanding of all the tasks across all occupations. Hence, their judgment about how much a certain task can be automated has to be rather hand-wavy in many cases.

Flaws in the data also arise from the GPT-based labeling itself.

The internet is flooded with countless sensationalist articles about how AI will replace jobs. It is hard to gauge whether this actually causes GPT models to be more optimistic when it comes to their own impact on society. However, it is possible and should not be neglected.

The authors do also not really distinguish between labor-augmenting and labor-displacing effects and it is hard to know what “affected by” or “exposed to LLMs” actually means. Will people be replaced or will they just be able to do more?

Last but not least, lists of tasks most likely do not capture all requirements in a given occupation. For instance ""making someone feel cared for"" can be an essential part of a job but might be neglected in such a list.

# Take-Away And Implications

GPT models have the world in a frenzy - rightfully so.

Nobody knows whether 19% of knowledge work gets heavily automated or if it is only 10%.

As the dust settles, we will begin to see how the ecosystem develops and how productivity in different industries can be increased. Time will tell whether foundational LLMs, specialized smaller models, or vertical tools built on top of APIs will be having the biggest impact.

In any case, these technologies have the potential to create unimaginable value for the world. At the same time, change rarely happens without pain. I strongly believe in human ingenuity and our ability to adapt to change. All in all, the study - flaws aside - represents an honest attempt at gauging the future.

Efforts like this and their scrutiny are our best shot at navigating the future. Well, or we all get chased out of the city by pitchforks.

Jokes aside!

What an exciting time for science and humanity!

As always, I really enjoyed making this for you and I sincerely hope you found value in it!

If you are not subscribed to the newsletter yet, [click here to sign up](https://thedecoding.net/)! I send out a thoughtful 5-minute email every week to keep you in the loop about machine learning research and the data economy.

*Thank you for reading and I see you next week ⭕!*

**References:**

\[1\] [https://arxiv.org/abs/2303.10130](https://arxiv.org/abs/2303.10130)"
48,deeplearning,open-ai,top,2019-04-18 05:03:00,Face Recognition: An Introduction for Beginners,spmallick,False,0.89,24,beho8c,https://www.reddit.com/r/deeplearning/comments/beho8c/face_recognition_an_introduction_for_beginners/,1,1555563780.0,"Face Recognition has been one of the most researched Computer Vision areas till date. So, it is natural to have too much information overload around the same.   
In our latest article, we have tried to simplify the topic and hope that it serves as a beginners' guide on Face Recognition.  
Feel free to comment if you think we have missed out on anything important.

[https://www.learnopencv.com/face-recognition-an-introduction-for-beginners/](https://www.learnopencv.com/face-recognition-an-introduction-for-beginners/) 

Mention reviews and what you want us to work next, in the comments!

P.S : More articles ( with code ) to come.  
[\#LearnOpenCV](https://www.facebook.com/hashtag/learnopencv?source=feed_text&epa=HASHTAG&__xts__%5B0%5D=68.ARA-mqImCdcmNoesLTLnq1cHNx1qybEcB2vWv0U-LCVooLDVbt6twgiEf_1ZdH1eFnuxGrlk-qovZie_GKHbK9JuHS7gwOzoQlew_P-o3By52IF6bkino3DlFiv1hW6__v-RJ83fIE3I4z6rmAlqKS7J5zP2ZG__d1RAql-eqhVXFtX7VUWedJ7uwZTMLaykIT2Ickd_8UC5VfMo5xc6HT5nOG3OIaht8E_lQnjxIG4LcGyYA4W2Vmm9FEPdvEyEW_JXGb_bsi33Jv0TCRtHyQT0EvR_DOIyo51B6ykXO3DkAHtMplZAjpPE3XW6dU2hPSsDuWWhqBMQ78GQuEwY3OE&__tn__=%2ANK-R) [\#OpenCV](https://www.facebook.com/hashtag/opencv?source=feed_text&epa=HASHTAG&__xts__%5B0%5D=68.ARA-mqImCdcmNoesLTLnq1cHNx1qybEcB2vWv0U-LCVooLDVbt6twgiEf_1ZdH1eFnuxGrlk-qovZie_GKHbK9JuHS7gwOzoQlew_P-o3By52IF6bkino3DlFiv1hW6__v-RJ83fIE3I4z6rmAlqKS7J5zP2ZG__d1RAql-eqhVXFtX7VUWedJ7uwZTMLaykIT2Ickd_8UC5VfMo5xc6HT5nOG3OIaht8E_lQnjxIG4LcGyYA4W2Vmm9FEPdvEyEW_JXGb_bsi33Jv0TCRtHyQT0EvR_DOIyo51B6ykXO3DkAHtMplZAjpPE3XW6dU2hPSsDuWWhqBMQ78GQuEwY3OE&__tn__=%2ANK-R) [\#MachineLearning](https://www.facebook.com/hashtag/machinelearning?source=feed_text&epa=HASHTAG&__xts__%5B0%5D=68.ARA-mqImCdcmNoesLTLnq1cHNx1qybEcB2vWv0U-LCVooLDVbt6twgiEf_1ZdH1eFnuxGrlk-qovZie_GKHbK9JuHS7gwOzoQlew_P-o3By52IF6bkino3DlFiv1hW6__v-RJ83fIE3I4z6rmAlqKS7J5zP2ZG__d1RAql-eqhVXFtX7VUWedJ7uwZTMLaykIT2Ickd_8UC5VfMo5xc6HT5nOG3OIaht8E_lQnjxIG4LcGyYA4W2Vmm9FEPdvEyEW_JXGb_bsi33Jv0TCRtHyQT0EvR_DOIyo51B6ykXO3DkAHtMplZAjpPE3XW6dU2hPSsDuWWhqBMQ78GQuEwY3OE&__tn__=%2ANK-R) [\#DeepLearning](https://www.facebook.com/hashtag/deeplearning?source=feed_text&epa=HASHTAG&__xts__%5B0%5D=68.ARA-mqImCdcmNoesLTLnq1cHNx1qybEcB2vWv0U-LCVooLDVbt6twgiEf_1ZdH1eFnuxGrlk-qovZie_GKHbK9JuHS7gwOzoQlew_P-o3By52IF6bkino3DlFiv1hW6__v-RJ83fIE3I4z6rmAlqKS7J5zP2ZG__d1RAql-eqhVXFtX7VUWedJ7uwZTMLaykIT2Ickd_8UC5VfMo5xc6HT5nOG3OIaht8E_lQnjxIG4LcGyYA4W2Vmm9FEPdvEyEW_JXGb_bsi33Jv0TCRtHyQT0EvR_DOIyo51B6ykXO3DkAHtMplZAjpPE3XW6dU2hPSsDuWWhqBMQ78GQuEwY3OE&__tn__=%2ANK-R) [\#AI](https://www.facebook.com/hashtag/ai?source=feed_text&epa=HASHTAG&__xts__%5B0%5D=68.ARA-mqImCdcmNoesLTLnq1cHNx1qybEcB2vWv0U-LCVooLDVbt6twgiEf_1ZdH1eFnuxGrlk-qovZie_GKHbK9JuHS7gwOzoQlew_P-o3By52IF6bkino3DlFiv1hW6__v-RJ83fIE3I4z6rmAlqKS7J5zP2ZG__d1RAql-eqhVXFtX7VUWedJ7uwZTMLaykIT2Ickd_8UC5VfMo5xc6HT5nOG3OIaht8E_lQnjxIG4LcGyYA4W2Vmm9FEPdvEyEW_JXGb_bsi33Jv0TCRtHyQT0EvR_DOIyo51B6ykXO3DkAHtMplZAjpPE3XW6dU2hPSsDuWWhqBMQ78GQuEwY3OE&__tn__=%2ANK-R)[\#ComputerVision](https://www.facebook.com/hashtag/computervision?source=feed_text&epa=HASHTAG&__xts__%5B0%5D=68.ARA-mqImCdcmNoesLTLnq1cHNx1qybEcB2vWv0U-LCVooLDVbt6twgiEf_1ZdH1eFnuxGrlk-qovZie_GKHbK9JuHS7gwOzoQlew_P-o3By52IF6bkino3DlFiv1hW6__v-RJ83fIE3I4z6rmAlqKS7J5zP2ZG__d1RAql-eqhVXFtX7VUWedJ7uwZTMLaykIT2Ickd_8UC5VfMo5xc6HT5nOG3OIaht8E_lQnjxIG4LcGyYA4W2Vmm9FEPdvEyEW_JXGb_bsi33Jv0TCRtHyQT0EvR_DOIyo51B6ykXO3DkAHtMplZAjpPE3XW6dU2hPSsDuWWhqBMQ78GQuEwY3OE&__tn__=%2ANK-R) [\#FaceRecognition](https://www.facebook.com/hashtag/facerecognition?source=feed_text&epa=HASHTAG&__xts__%5B0%5D=68.ARA-mqImCdcmNoesLTLnq1cHNx1qybEcB2vWv0U-LCVooLDVbt6twgiEf_1ZdH1eFnuxGrlk-qovZie_GKHbK9JuHS7gwOzoQlew_P-o3By52IF6bkino3DlFiv1hW6__v-RJ83fIE3I4z6rmAlqKS7J5zP2ZG__d1RAql-eqhVXFtX7VUWedJ7uwZTMLaykIT2Ickd_8UC5VfMo5xc6HT5nOG3OIaht8E_lQnjxIG4LcGyYA4W2Vmm9FEPdvEyEW_JXGb_bsi33Jv0TCRtHyQT0EvR_DOIyo51B6ykXO3DkAHtMplZAjpPE3XW6dU2hPSsDuWWhqBMQ78GQuEwY3OE&__tn__=%2ANK-R)

https://preview.redd.it/xpftawm8gys21.jpg?width=960&format=pjpg&auto=webp&s=f2c439d456909762fe461354e1df2566e4f8a3c2"
49,deeplearning,open-ai,top,2020-11-09 12:04:21,This AI Predicts if You Have Covd-19 from Your Cough [Paper Analysis],diabulusInMusica,False,0.82,25,jqwf4r,https://www.reddit.com/r/deeplearning/comments/jqwf4r/this_ai_predicts_if_you_have_covd19_from_your/,6,1604923461.0,"Reliable and quick diagnostic tools are fundamental to fight the Covid-19 pandemic 😷 😷 Researchers at MIT published a paper 🎓 🎓 introducing an AI-Audio based diagnostic system that produces accurate Covid-19 diagnoses. The diagnosis is carried out by analysing cough sounds.

In my new video, I break down the paper providing a high-level overview (useful for non-technical people) and a detailed technical account.  

The paper is titled “COVID-19 Artificial Intelligence Diagnosis using only Cough Recordings” and was published in the IEEE Open Journal of Engineering in Medicine and Biology in September 2020.

Here’s the video:

[https://www.youtube.com/watch?v=Skzuva3chIM&list=PL-wATfeyAMNoxL33ZF2TRgq9AuDAynYTx&index=2](https://www.youtube.com/watch?v=Skzuva3chIM&list=PL-wATfeyAMNoxL33ZF2TRgq9AuDAynYTx&index=2)"
50,deeplearning,open-ai,top,2020-05-06 20:43:15,AI Summarizes Scientific Papers,HenryAILabs,False,0.9,26,ges589,https://www.reddit.com/r/deeplearning/comments/ges589/ai_summarizes_scientific_papers/,0,1588797795.0,"This video explores the TLDR dataset from AI2!

This is a really interesting dataset of about 4K papers in Machine Learning and a 1-written summary from the author.

The authors come up with a clever way to bootstrap additional data from peer-reviewed comments on OpenReview.

This video also explores the BART model from Facebook AI, a strong baseline used for abstractive summarization on the TLDR dataset!

This is a really exciting application for NLP to build tools that help with scientific research!

https://youtu.be/5WJZgSwRUSQ"
51,deeplearning,open-ai,top,2022-05-06 17:22:43,IVY: An Open-Source Tool To Make Deep Learning Code Compatible Across Frameworks,No_Coffee_4638,False,0.94,25,ujsi2t,https://www.reddit.com/r/deeplearning/comments/ujsi2t/ivy_an_opensource_tool_to_make_deep_learning_code/,1,1651857763.0,"As ML aficionados, we’ve all come across interesting projects on GitHub only to discover that they are not in the framework we want and are familiar with. It can be tedious at times to reimplement the whole codebase in our framework, let alone deal with any errors that may arise throughout the process. It is a tedious chore that no one wants to do. Isn’t it good to have something that doesn’t care what framework you’re using? It will provide you with code in your desired framework, whether it is JAX, PyTorch, MXNet, Numpy, or TensorFlow. This is what [IVY ](https://github.com/unifyai/ivy)is attempting to do by unifying all ML frameworks.

The number of open-source machine learning projects has surged significantly over the past. This is evident by the fast-growing number of Github repositories using the keyword Deep learning. Because of different frameworks, code sharability has been considerably hampered. Aside from that, many frameworks become obsolete in comparison to newer frameworks. For software development where collaboration is vital, this is a significant bottleneck. As newer frameworks come into the scene framework-specific code quickly becomes obsolete, and transferring code across frameworks is akin to reinventing the wheel.

[Continue Reading](https://www.marktechpost.com/2022/05/06/ivy-an-open-source-tool-to-make-deep-learning-code-compatible-across-frameworks/)

**GitHub**: [https://github.com/unifyai/ivy](https://github.com/unifyai/ivy)

**Paper:** https://arxiv.org/pdf/2102.02886.pdf

**Project:** [https://lets-unify.ai/ivy/](https://lets-unify.ai/ivy/)"
52,deeplearning,open-ai,top,2023-12-16 15:22:29,Is there any alternative for OpenAI API?,CrazyProgramm,False,0.91,26,18jtffj,https://www.reddit.com/r/deeplearning/comments/18jtffj/is_there_any_alternative_for_openai_api/,24,1702740149.0, So I am from Sri Lanka and our university is going to organize a competition and we need OpenAI API for it but we don't have money to afford it. Is there any alternative API you guys know 
53,deeplearning,open-ai,top,2023-05-17 02:26:44,OpenAI CEO asking for government's license for building AI . WHAT THE ACTUAL FUCK?,Angry_Grandpa_,False,0.83,24,13joq2b,/r/singularity/comments/13jbc76/openai_ceo_asking_for_governments_license_for/,8,1684290404.0,
54,deeplearning,open-ai,top,2023-07-10 20:23:30,How to rent GPU's for a few hours a day without paying too much?,FelipeReigosa,False,0.88,24,14w56ae,https://www.reddit.com/r/deeplearning/comments/14w56ae/how_to_rent_gpus_for_a_few_hours_a_day_without/,17,1689020610.0,"I'm not sure if this is the right place to ask this, but I've started getting into AI, I've played a little with a few open source NeRF projects and stable diffusion but my gpu just isn't powerful enough to do more than the basics. I've tried using linode gpu instances and that's almost perfect, I create an instance and have ssh access to an ubuntu with a powerful gpu and the hourly rate is pretty good at $1.5. The problem is that I can't persist the system state (packages installed, data downloaded etc) in a easy way except downloading the disk image which would take too long and be impractical. And if I let the instance running even when I'm not using it it's $1000 a month which is unacceptable for me, I just want it a few hours a day. Am I missing something, is there a way to save the linode state in a way that's practical? If not is there another service that offers this? (ssh access to an instance on an hourly basis and permanent storage of the stopped system state for a reasonable amount per month). What do you guys use?"
55,deeplearning,open-ai,top,2021-12-24 15:48:32,[R] OpenAI Releases GLIDE: A Scaled-Down Text-to-Image Model That Rivals DALL-E Performance,Yuqing7,False,0.93,25,rnovk0,https://www.reddit.com/r/deeplearning/comments/rnovk0/r_openai_releases_glide_a_scaleddown_texttoimage/,1,1640360912.0,"An OpenAI research team proposes GLIDE (Guided Language-to-Image Diffusion for Generation and Editing) for high-quality synthetic image generation. Human evaluators prefer GLIDE samples over DALL-E’s, and the model size is much smaller (3.5 billion vs. 12 billion parameters). 

Here is a quick read: [OpenAI Releases GLIDE: A Scaled-Down Text-to-Image Model That Rivals DALL-E Performance.](https://syncedreview.com/2021/12/24/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-173/)

The paper *GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models* is on [arXiv](https://arxiv.org/abs/2112.10741)."
56,deeplearning,open-ai,top,2021-01-28 20:39:28,Optical Flow estimation using Deep Learning,spmallick,False,0.93,25,l783nc,https://www.reddit.com/r/deeplearning/comments/l783nc/optical_flow_estimation_using_deep_learning/,0,1611866368.0," A couple of weeks back we covered optical flow algorithms implemented in OpenCV.  


Starting with major improvements in image classification in 2012, Deep Learning based techniques have improved accuracy of many algorithms in computer vision including object detection, image segmentation, pose estimation, depth estimation, and even optical flow.  


Today, we are sharing a post on a deep learning-based optical flow algorithm. We cover  


1. **FlowNet**: The first DL architecture for optical flow
2. **RAFT**: The state of the art DL architecture for optical flow.

Without future ado, here is the link to the post  


[https://learnopencv.com/optical-flow-using-deep-learning-raft/](https://learnopencv.com/optical-flow-using-deep-learning-raft/)  


[**#AI**](https://www.linkedin.com/feed/hashtag/?keywords=ai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#ComputerVision**](https://www.linkedin.com/feed/hashtag/?keywords=computervision&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#ML**](https://www.linkedin.com/feed/hashtag/?keywords=ml&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#ArtificialIntelligence**](https://www.linkedin.com/feed/hashtag/?keywords=artificialintelligence&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#MachineLearning**](https://www.linkedin.com/feed/hashtag/?keywords=machinelearning&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#OpenCV**](https://www.linkedin.com/feed/hashtag/?keywords=opencv&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#DL**](https://www.linkedin.com/feed/hashtag/?keywords=dl&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#DeepLearning**](https://www.linkedin.com/feed/hashtag/?keywords=deeplearning&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#deeplearningai**](https://www.linkedin.com/feed/hashtag/?keywords=deeplearningai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313)  


The python code is linked below  


​[https://github.com/spmallick/learnopencv/tree/master/Optical-Flow-Estimation-using-Deep-Learning-RAFT](https://click.convertkit-mail.com/wvuprg03qpsgh7nl00bl/g3hnhwuelrdxerfr/aHR0cHM6Ly9naXRodWIuY29tL3NwbWFsbGljay9sZWFybm9wZW5jdi90cmVlL21hc3Rlci9PcHRpY2FsLUZsb3ctRXN0aW1hdGlvbi11c2luZy1EZWVwLUxlYXJuaW5nLVJBRlQ=) 

https://preview.redd.it/r4olvm9qw4e61.png?width=600&format=png&auto=webp&s=bb21049376bad07242ff62b20da584d98c1573b6"
57,deeplearning,open-ai,top,2022-10-02 23:31:56,"Researchers at Activeloop AI Introduce ‘Deep Lake,’ an Open-Source Lakehouse for Deep Learning Applications",ai-lover,False,0.89,22,xu39gj,https://www.reddit.com/r/deeplearning/comments/xu39gj/researchers_at_activeloop_ai_introduce_deep_lake/,4,1664753516.0,"A data lake is a centralized repository where enterprises may store structured, unstructured, and semi-structured data. Data lakes improve data management, governance, and analysis. Furthermore, they enable breaking down data silos and discovering previously concealed insights in diverse data sources. Traditionally, first-generation data lakes gathered data into distributed storage systems such as HDFS or AWS S3. Unorganized data collections transformed data lakes into “data swamps,” giving birth to the second generation of data lakes led by Delta, Iceberg, and Hudi. They work only on top of standardized structured formats such as Parquet, ORC, and Avro and offer capabilities like as time travel, ACID transactions, and schema evolution.

To conduct analytical queries, data lakes easily interface with query engines like as Presto, Athena, Hive, and Photon. They also interface to frameworks like as Hadoop, Spark, and Airflow for ETL pipeline maintenance. In turn, the combination of data lakes and query engines with explicit compute and storage separation resulted in the introduction of systems such as Lakehouse that serve as alternatives to data warehouses such as Snowflake, BigQuery, Redshift, and Clickhouse. During the last decade, deep learning has surpassed standard machine learning algorithms for dealing with unstructured and complicated data such as text, photos, videos, and audio.

[Continue reading](https://www.marktechpost.com/2022/10/02/researchers-at-activeloop-ai-introduce-deep-lake-an-open-source-lakehouse-for-deep-learning-applications/) | *heck out the* [*paper*](https://arxiv.org/pdf/2209.10785v1.pdf) *and* [*github*](https://github.com/activeloopai/deeplake)"
58,deeplearning,open-ai,top,2023-01-07 15:11:47,Review Request: MS in AI Grad Student with 3+ years of relevant experience trying to apply for Summer Internships '23 (posting here because I need domain-specific feedback),animikhaich,False,0.73,23,105r933,https://i.redd.it/0g3k2udk0naa1.jpg,12,1673104307.0,
59,deeplearning,open-ai,top,2023-06-28 12:14:12,What are the best deep learning books that are still up-to-date?,Mr_Funkedeli,False,1.0,24,14l7wbl,https://www.reddit.com/r/deeplearning/comments/14l7wbl/what_are_the_best_deep_learning_books_that_are/,6,1687954452.0,"Hello r/deeplearning,

I am trying to jump into the field of deep learning. I have some past experience with ML and DL, and I have even completed [fast.ai](https://fast.ai)'s course ""Practical Deep Learning for Coders Part 1"". I want to take it to the next step though, and many have recommended that I follow a book. All of the books I can find though, seem to be quite outdated, especially since ML and DL have progressed so much in the last year alone.

**My question basically is, what are some good DL books that are still up to date in terms of whats covered and how it the information is presented. Bonus if it uses Pytorch, as that is what I want to learn.** 

Also feel free to recommend courses that I could take as well. I'm open to everything!

&#x200B;

Thanks!"
60,deeplearning,open-ai,top,2023-04-25 17:53:47,"Microsoft releases SynapseMl v0.11 with support for ChatGPT, GPT-4, causal learning, and more",mhamilton723,False,0.9,24,12yqpnp,https://www.reddit.com/r/deeplearning/comments/12yqpnp/microsoft_releases_synapseml_v011_with_support/,0,1682445227.0,"Today Microsoft launched SynapseML v0.11 with support for ChatGPT, GPT-4, distributed training of huggingface and torchvision models, an ONNX Model hub integration, Causal Learning with EconML, 10x memory reductions for LightGBM, and a newly refactored integration with Vowpal Wabbit. To learn more check out our release notes and please feel give us a star if you enjoy the project!

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.11.0](https://github.com/microsoft/SynapseML/releases/tag/v0.11.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919)

Thank you to all the contributors in the community who made the release possible!

&#x200B;

[What's new in SynapseML v0.11](https://preview.redd.it/9pqj1mowj2wa1.png?width=4125&format=png&auto=webp&s=a358e73760c847a09cc76f2ed17dc58e15aed5ed)"
61,deeplearning,open-ai,top,2022-12-07 00:33:41,Are currently state of art model for logical/common-sense reasoning all based on NLP(LLM)?,Accomplished-Bill-45,False,0.94,22,zen8l4,https://www.reddit.com/r/deeplearning/comments/zen8l4/are_currently_state_of_art_model_for/,6,1670373221.0,"Not very familiar with NLP, but I'm playing around with OpenAI's ChatGPT; particularly impressed by its reasoning, and its thought-process.

Are all good reasoning models derived from NLP (LLM) models with RL training method at the moment?

What are some papers/research team to read/follow to understand this area better and stay on updated?

&#x200B;

&#x200B;

for ChatGPT. I've tested it with following cases

Social reasoning ( which does a good job; such as: if I'm going to attend meeting tonight. I have a suit, but its dirty and size doesn't fit. another option is just wear underwear, the underwear is clean and fit in size. Which one should I wear to attend the meeting. )

Psychological reasoning ( it did a bad job.I asked it to infer someone's intention given his behaviours, expression, talks etc.)

Solving math question ( it’s ok, better then Minerva)

Asking LSAT logic game questions ( it gives its thought process, but failed to give correct answers)

I also wrote up a short mystery novel, ( like 200 words, with context) ask if it can tell is the victim is murdered or committed suicide; if its murdered, does victim knows the killer etc. It actually did ok job on this one if the context is clearly given that everyone can deduce some conclusion using common sense."
62,deeplearning,open-ai,top,2019-11-09 23:25:27,TensorLayer Team Released Reinforcement Learning Algorithm Baseline-RLzoo,quantumiracle,False,0.97,23,du3jol,https://www.reddit.com/r/deeplearning/comments/du3jol/tensorlayer_team_released_reinforcement_learning/,1,1573341927.0,"Recently,  in order to enable the industry to better use the cutting-edge  reinforcement learning algorithms, the TensorLayer Reinforcement   Learning Team has released a complete library of reinforcement learning   baseline algorithms for the industry — RLzoo. TensorLayer is an  extended  library based on TensorFlow for better supports of basic  neural network  construction and diverse neural network applications.  The RLzoo project  is the first comprehensive open source algorithm  library with  TensorLayer 2.0 and TensorFlow 2.0 since the release of  TensorFlow 2.0.  The library currently supports OpenAI Gym, DeepMind  Control Suite and  other large-scale simulation environments, such as  the robotic learning  environment RLBench, etc.

Link of full post: [https://medium.com/@zhding96/tensorlayer-team-released-reinforcement-learning-algorithm-baseline-rlzoo-2663cfb77904](https://medium.com/@zhding96/tensorlayer-team-released-reinforcement-learning-algorithm-baseline-rlzoo-2663cfb77904)

Link of RLzoo: [https://github.com/tensorlayer/RLzoo](https://github.com/tensorlayer/RLzoo)

Link of RL tutorial: [https://github.com/tensorlayer/tensorlayer/tree/master/examples/reinforcement\_learning](https://github.com/tensorlayer/tensorlayer/tree/master/examples/reinforcement_learning)

Slack group: [https://app.slack.com/client/T5SHUUKNJ/D5SJDERU7](https://app.slack.com/client/T5SHUUKNJ/D5SJDERU7)"
63,deeplearning,open-ai,top,2019-02-14 17:19:15,OpenAI Guards Its ML Model Code & Data to Thwart Malicious Usage,gwen0927,False,0.96,22,aqm35j,https://medium.com/syncedreview/openai-guards-its-ml-model-code-data-to-thwart-malicious-usage-d9f7e9c43cd0,4,1550164755.0,
64,deeplearning,open-ai,top,2020-07-02 18:38:37,[N] Top US AI Research Institutes and Tech Companies Support National AI Research Cloud,Yuqing7,False,0.87,21,hk2nuf,https://www.reddit.com/r/deeplearning/comments/hk2nuf/n_top_us_ai_research_institutes_and_tech/,0,1593715117.0,"Leading US universities engaged in AI research have joined tech giants Google, Amazon Web Services, Microsoft, IBM, and NVIDIA in backing legislation aimed at establishing a roadmap for the creation of a national AI research cloud. Organizations such as the Allen Institute for AI, IEEE, and OpenAI are also supporting the bipartisan and bicameral *National AI Research Resource Task Force Act.*

 Here is a quick read: [Top US AI Research Institutes and Tech Companies Support National AI Research Cloud](https://syncedreview.com/2020/07/02/top-us-ai-research-institutes-and-tech-companies-support-national-ai-research-cloud/)"
65,deeplearning,open-ai,top,2021-07-08 04:51:47,"Exploration of GitHub Copilot, OpenAI Codex-based AI coding assistant that translates natural language into code",techn0_cratic,False,0.76,23,og08xq,https://youtu.be/GTG_bcFdcLQ,0,1625719907.0,
66,deeplearning,open-ai,top,2022-10-12 20:21:40,"I've built an Auto Subtitled Video Generator using Streamlit and OpenAI Whisper, hosted on HuggingFace spaces.",Batuhan_Y,False,0.96,23,y2edmn,https://www.reddit.com/r/deeplearning/comments/y2edmn/ive_built_an_auto_subtitled_video_generator_using/,0,1665606100.0,"All you have to do is input a YouTube video link and get a video with subtitles (alongside with .txt, .vtt, .srt files).

Whisper can translate 98 different languages to English. If you want to give it a try;

Link of the app: [https://huggingface.co/spaces/BatuhanYilmaz/Auto-Subtitled-Video-Generator](https://huggingface.co/spaces/BatuhanYilmaz/Auto-Subtitled-Video-Generator)

&#x200B;

https://reddit.com/link/y2edmn/video/r49plzsgoft91/player"
67,deeplearning,open-ai,top,2021-02-08 20:07:44,[Best Practices] on how to organize deep learning projects,kk_ai,False,0.82,20,lfki86,https://www.reddit.com/r/deeplearning/comments/lfki86/best_practices_on_how_to_organize_deep_learning/,1,1612814864.0,"In this article you’ll see how to structure work on deep learning projects — from the inception to deployment, and everything in between. You will learn:

- About the lifecycle of the project.
- Importance of defining an objective or goal of the project.
- Collecting data based on the requirements of the project.
- Model training and results exploration including:
    - Establishing baselines for better results.
    -Adopting techniques and approaches from the existing open-source state-of-the-art models research papers and code repositories.
    - Experiment tracking and management management 
- Model refinement techniques to avoid underfitting and overfitting like:
    - Controlling hyperparameters
    - Regularisation
    - Pruning
- Testing and evaluating your project before deployment.
- Model deployment
- Project maintenance

[Structuring deep learning projects](https://neptune.ai/blog/how-to-organize-deep-learning-projects-best-practices?utm_source=reddit&utm_medium=post&utm_campaign=blog-how-to-organize-deep-learning-projects-best-practices&utm_content=deeplearning)"
68,deeplearning,open-ai,top,2021-12-03 15:41:55,"[R] Warsaw U, Google & OpenAI’s Terraformer Achieves a 37x Speedup Over Dense Baselines on 17B Transformer Decoding",Yuqing7,False,0.96,22,r81wny,https://www.reddit.com/r/deeplearning/comments/r81wny/r_warsaw_u_google_openais_terraformer_achieves_a/,1,1638546115.0,"In the new paper Sparse is Enough in Scaling Transformers, a research team from the University of Warsaw, Google Research and OpenAI proposes Scaling Transformers, a family of novel transformers that leverage sparse layers to scale efficiently and perform unbatched decoding much faster than original transformers, enabling fast inference on long sequences even with limited memory. 

Here is a quick read: [Warsaw U, Google & OpenAI’s Terraformer Achieves a 37x Speedup Over Dense Baselines on 17B Transformer Decoding.](https://syncedreview.com/2021/12/03/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-158/)

The paper *Sparse is Enough in Scaling Transformers* is on [arXiv](https://arxiv.org/abs/2111.12763)."
69,deeplearning,open-ai,top,2022-06-15 15:33:57,New open-source that accelerates AI training (~1.5-2x as of now) without requiring you to change your training setup,emilec___,False,0.97,25,vcx8dy,https://www.reddit.com/r/deeplearning/comments/vcx8dy/new_opensource_that_accelerates_ai_training_152x/,4,1655307237.0,"Hi everybody,

I have been working for a while on improving AI inference efficiency/speed, and many times I have been asked if the same could be done to make training faster as well. Training can be a bottleneck, often costly and slowing development.

The answer is clearly a yes, many well know that. Training can be streamlined at different levels.

1. One can change the way training is performed (**algorithmic optimization**) by trying to achieve faster or earlier convergence. You can change the learning rate, the scheduling policy, the training recipe or replace one level with another that requires less computational resources.
2. Another option is to apply **precision reduction techniques**, which involve a trade-off of some precision to achieve a smaller memory footprint and a faster model. For example, one could ""prune"" the non-critical or redundant parts of the neural network graph (pruning), take advantage of the properties of sparse matrices (sparsity), reduce the size of the activations and model weights from 32 or 16 bits to 8 bits or even to 4 or 1 bit (quantization).
3. Moving down, closer to the hardware, you could optimize the way the model is mapped to the hardware (**compilers**) and the way the model accesses data in memory (**data loading**), making better use of computer resources. This can be achieved by storing the data closer to the processor and converting the neural network calculations into compiled binaries so that the CPU or GPU can execute them readily.
4. Also, you can increase the amount of computing resources used for training and further accelerate training by parallelizing computations across multiple machines (**distributed computing**).

Along these lines, I decided to create an open-source that works on optimizing the full computing stack. This allows people to benefit from the **compound acceleration** provided by these 4 levels of optimization techniques, and early results are promising. Moreover, the library also aims to make training acceleration very ""accessible,"" so that everyone can use these optimization techniques, which are often complex to be implemented today. This is achieved through the use of **class decorators**, a Python constructor analogous to Java's @annotations. In short, you can simply insert these annotations (e.g. @accelerate\_model() ) into your code and the decorator will make sure that you use your computing resources to the fullest.

This library is called nebulgym and so far it **accelerates training by 30%-50%** and I definitely believe there is room to reach 70%-90% or more. The library will evolve over time and support other use cases. And if you would like to contribute or just give feedback, it will be super appreciated! [https://github.com/nebuly-ai/nebulgym](https://github.com/nebuly-ai/nebulgym).

Here's a snippet of training with nebulgym decorators (`@accelerate_dataset` and `@accelerate_model`)

```
@accelerate_dataset()
class MyDataset{…}

@accelerate_model()
class MyModel{…}

#Train your model as you usually do
```

About the **tech behind the open-source**, as of now the library works on 3 building blocks: acceleration of the data loading process, and acceleration of both back and forward propagation through sparse strategies and efficient compilations.

Regarding the latter aspect, nebulgym leverages Rammer \[1\], a DNN compiler design that optimizes the execution of DNN workloads on massively parallel accelerators. It generates an efficient static spatio-temporal schedule for a DNN at compile time to minimize scheduling overhead. It maximizes hardware utilization by holistically exploiting parallelism through inter- and intra- operator co-scheduling. It achieves this by proposing several novel, hardware neutral, and clean abstractions for the computation tasks and the hardware accelerators. These abstractions expose a much richer scheduling space to Rammer, which employs several heuristics to explore this space and finds efficient schedules.

On top of this, nebulgym computes only a small subset of the full gradient to update the model parameters in back propagation \[2\]. The gradient vectors are sparsified so that only the elements with top magnitude are kept. As a result, a smaller fraction of the weight matrix is modified, leading to a linear reduction in the computational cost.

Nebulgym also changes the way data is loaded, with the goal of eliminating any time when the processor is not processing but waiting for data to load. Indeed, a default data loader reads the data from your storage and performs some user-set preprocessing (e.g. converting the data to normalized tensors, removing biases, resizing images, etc.), and then transfers the data to the model. This process is repeated for each data and for each epoch. The data loader introduced in nebulgym at first epoch performs the same tasks (data loading and preprocessing) but writes/saves the preprocessed data (in parallel) to a fast access memory, which is usually SSD memory if available. This \[a\] slows down the first epoch slightly (\~20% slower during testing), but starting with the second epoch thereafter \[b\] preprocessing will not be computed again and \[c\] data will be transferred from fast-access memory to RAM (in parallel) to make maximum use of memory bandwidth. This speeds up all the following epochs and prevents data loading from becoming a bottleneck for the entire training process, which happens in many cases.

And that's it. Give it a try, and leave a star on [github](https://github.com/nebuly-ai/nebulgym) if you like the concept :) Also feedback would be very much appreciated! And if you want to discuss/chat about AI optimization, with other AI researchers we are organizing reading groups and other knowledge-sharing activities on [this channel](https://discord.gg/RbeQMu886J) launched recently.

[\[1\]](https://arxiv.org/pdf/1706.06197.pdf) Xu Sun, Xuancheng Ren, Shuming Ma, Houfeng Wang. meProp: Sparsified Back Propagation for Accelerated Deep Learning.

[\[2\]](https://www.usenix.org/system/files/osdi20-ma.pdf) Lingxiao Ma, Zhiqiang Xie, Zhi Yang, Jilong Xue, Youshan Miao, Wei Cui, Wenxiang Hu, Fan Yang, Lintao Zhang, and Lidong Zhou. Rammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks with Reduced Overfitting."
70,deeplearning,open-ai,top,2022-10-03 18:57:54,Use YOLOv5 tensorflow.js models to speed up annotation,RandomForests92,False,1.0,19,xus40c,https://www.reddit.com/r/deeplearning/comments/xus40c/use_yolov5_tensorflowjs_models_to_speed_up/,7,1664823474.0,"Hi everyone! I'm Piotr and for several years I have been developing a small open-source project for labeling photos - [makesense.ai](https://makesense.ai/). I added a new feature this weekend. You can use \[YOLOv5\]([https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5)) models to automatically annotate photos. You can choose one of the models pre-trained on the COCO dataset, but most importantly you can load your own custom models. Just drag and drop the tensorflow.js model to the editor and you are good to go. Everything runs in the browser - no backend, so it is completely free. Let me know what you think! I'm super excited about that project.

By the way, I have created an NPM package, which can also make it easier for you to deploy YOLOv5 in the browser. [https://github.com/SkalskiP/yolov5js](https://github.com/SkalskiP/yolov5js)"
71,deeplearning,open-ai,top,2020-03-07 08:41:37,Call for Paper: International Workshop on Federated Learning at IJCAI 2020,abyliss,False,0.87,18,fesj0j,https://www.reddit.com/r/deeplearning/comments/fesj0j/call_for_paper_international_workshop_on/,0,1583570497.0,"**International  Workshop on Federated Learning for User Privacy and Data  Confidentiality in Conjunction with IJCAI 2020 (FL-IJCAI'20)**

**Submission Due**: April 26, 2020 (23:59 UTC-12)  
**Notification Due**: May 24, 2020 (23:59 UTC-12)

**Workshop Date**: July 13, 2020 (tentative)  
**Venue**: Pacifico Yokohama, Yokohama, Japan  
(with online meeting contingency plan)

**Call for Papers**

Privacy  and security are becoming a key concern in our digital age.  Companies  and organizations are collecting a wealth of data on a daily  basis.  Data owners have to be very cautious while exploiting the values  in the  data, since the most useful data for machine learning often tend  to be  confidential. Increasingly strict data privacy regulations such as  the  European Union’s General Data Protection Regulation (GDPR) bring  new  legislative challenges to the big data and artificial intelligence  (AI)  community. Many operations in the big data domain, such as merging   user data from various sources for building an AI model, will be   considered illegal under the new regulatory framework if they are   performed without explicit user authorization. More resources about  federated learning can be found [**here**](http://federated-learning.org/).

In  order to explore how the AI research community can adapt to  this new  regulatory reality, we organize this one-day workshop in  conjunction  with the 29th International Joint Conference on Artificial  Intelligence  (IJCAI-20). The workshop will focus on machine learning  systems  adhering to the privacy-preserving and security principles.  Technical  issues include but not limit to data collection, integration,  training  and modelling, both in the centralized and distributed setting.  The  workshop intends to provide a forum to discuss the open problems  and  share the most recent and ground-breaking work on the study and   application of secure and privacy-preserving compliant machine learning.   Both theoretical and application-based contributions are welcome. The   FL series of workshops seek to explore new ideas with particular focus   on addressing the following challenges:

* Security  and Regulation Compliance: How to meet the security and  compliance  requirements? Does the solution ensure data privacy and model  security?
* Collaboration  and Expansion Solution: Does the solution connect  different business  partners from various parties and industries? Does  the solution exploit  and extend the value of data while observing user  privacy and data  security?
* Promotion  & Empowerment: Is the solution sustainable and  intelligent? Does  it include incentive mechanisms to encourage parties  to participate on a  continuous basis? Does it promote a stable and  win-win business  ecosystem?

We welcome  submissions on recent advances in privacy-preserving,  secure machine  learning and artificial intelligence systems. All  accepted papers will  be presented during the workshop. At least one  author of each accepted  paper is expected to represent it at the  workshop. Topics include but  not limit to:

Techniques

1. Adversarial learning, data poisoning, adversarial examples, adversarial robustness, black box attacks
2. Architecture and privacy-preserving learning protocols
3. Federated learning and distributed privacy-preserving algorithms
4. Human-in-the-loop for privacy-aware machine learning
5. Incentive mechanism and game theory
6. Privacy aware knowledge driven federated learning
7. Privacy-preserving  techniques (secure multi-party computation,  homomorphic encryption,  secret sharing techniques, differential privacy)  for machine learning
8. Responsible, explainable and interpretability of AI
9. Security for privacy
10. Trade-off between privacy and efficiency

Applications

1. Approaches to make AI GDPR-compliant
2. Crowd intelligence
3. Data value and economics of data federation
4. Open-source frameworks for distributed learning
5. Safety and security assessment of AI solutions
6. Solutions to data security and small-data challenges in industries
7. Standards of data privacy and security

Position, perspective, and vision papers are also welcome.

**Special Benchmarking Track**  
In  addition, the workshop will also encourage researchers to  demonstrate  and test their ideas based on a set of benchmark datasets ([https://dataset.fedai.org/#/](https://dataset.fedai.org/#/)).   To this end, the special benchmarking track calls for submissions that   evaluate the proposed methods using the benchmark datasets. If your   submission uses the aforementioned datasets for experimental evaluation,   please select option (B) or (C) from the ""**Submission Details**"" dropdown list.

For enquiries, please email to [flijcai20@easychair.org](mailto:flijcai20@easychair.org).

**Submission Instructions**

Submissions  should be between 4 to 7 pages following the IJCAI-20  template.  Formatting guidelines, including LaTeX styles and a Word  template, can  be found at: [https://www.ijcai.org/authors\_kit](https://www.ijcai.org/authors_kit).   We do not accept submissions of work currently under review. The   submissions should include author details as we do not carry out blind   review.

Submission link: [https://easychair.org/conferences/?conf=flijcai20](https://easychair.org/conferences/?conf=flijcai20)

**Awards**

One **Best Paper Award** and one **Best Student Paper Award** will be given out during the workshop. One **Special Track Distinguished Paper Award** winner will be selected from the Special Benchmarking Track submissions.

**Organizing Committee**

Steering Committee Chair:

* Qiang Yang (WeBank, China/Hong Kong University of Science and Technology, Hong Kong)
* General Co-Chairs:
* Lixin Fan (WeBank, China)
* Martin Pelikan (Apple, USA)
* Program Co-Chairs:
* Han Yu (Nanyang Technological University, Singapore)
* Yiran Chen (Duke University, USA)
* Local Arrangements Co-Chairs:
* Kilho Shin (Gakushuin University, Japan)
* Takayuki Ito (Nagoya Institute of Technology, Japan)
* Tianyu Zhang (WeBank, China)
* Special Track Co-Chairs:
* Bingsheng He (National University of Singapore, Singapore)
* Di Jiang (WeBank, China)
* Yang Liu (WeBank, China)
* Publicity Co-Chairs:
* Boyang Li (Nanyang Technological University, Singapore)
* Lingjuan Lyu (National University of Singapore, Singapore)
* Web Chair:
* Jun Lin (Nanyang Technological University, Singapore)

**Program Committee**

* Aleksei	Triastcyn (Ecole Polytechnique Fédérale de Lausanne, Switzerland)
* Anit Kumar	Sahu	(Bosch Center for Artificial Intelligence)
* Bao	Wang	(University of California, USA)
* Boi	Faltings	(Ecole Polytechnique Fédérale de Lausanne, Switzerland)
* Chaoyang	He	(University of Southern California, USA)
* Dimitrios	Papadopoulos	(The Hong Kong University of Science and Technology, Hong Kong)
* Fabio	Casati	(Servicenow, USA)
* Guodong	Long	(University of Technology, Sydney)
* Jalaj	Upadhyay	(Apple, USA)
* Jianshu	Weng	(AI Singapore, Singapore)
* Jianyu	Wang	(Carnegie Mellon University, USA)
* Jun	Zhao	(Nanyang Technological University, Singapore)
* Konstantin	Mishchenko	(King Abdullah University of Science and Technology, Saudi Arabia)
* Leye	Wang	(Peking University, China)
* Lifeng	Sun	(Tsinghua University, China)
* Mingshu	Cong	(The University of Hong Kong, Hong Kong)
* Nguyen	Tran	(The University of Sydney, Australia)
* Pallika	Kanani	(Oracle Labs, USA)
* Paul Pu	Liang	(Carnegie Mellon University, USA)
* Pengwei	Xing	(Nanyang Technological University, Singapore)
* Peter	Richtarik	(King Abdullah University of Science and Technology, Saudi Arabia / University of Edinburgh, UK)
* Praneeth	Vepakomma	(Massachusetts Institute of Technology, USA)
* Rui-Xiao	Zhang	(Tsinghua University, China)
* Seong Joon	Oh	(Clova AI Research, LINE Plus Corp., South Korea)
* Sewoong	Oh	(University of Illinois at Urbana-Champaign, USA)
* Shiqiang	Wang	(IBM, USA)
* Tribhuvanesh	Orekondy	(Max Planck Institute for Informatics, Germany)
* Virendra	Marathe	(Oracle Labs, USA)
* Xi	Weng	(Peking University, China)
* Xin	Yao	(Tsinghua University, China)
* Xu	Guo	(Nanyang Technological University, Singapore)
* Yan	Kang	(Webank, China)
* Yang	Zhang	(CISPA Helmholtz Center for Information Security, Germany)
* Yihan	Jiang	(University of Washington, USA)
* Yiqiang	Chen	(Institute of Computing Technology, Chinese Academy of Sciences, China)
* Yongxin	Tong	(Beihang University, China)
* Zelei Liu	Liu	(Nanyang Technological University, Singapore)
* Zheng	Xu	(University of Maryland, USA)
* Zhicong	Liang	(The Hong Kong University of Science and Technology, Hong Kong)
* Zichen	Chen	(Nanyang Technological University, Singapore)"
72,deeplearning,open-ai,top,2023-02-01 09:00:18,"Python wrapper of OpenAI's New AI classifier tool, which detects whether the paragraph was generated by ChatGPT, GPT models, or written by humans",StoicBatman,False,0.92,21,10qouv9,https://www.reddit.com/r/deeplearning/comments/10qouv9/python_wrapper_of_openais_new_ai_classifier_tool/,3,1675242018.0,"OpenAI has developed a new AI classifier tool which detects whether the content (paragraph, code, etc.) was generated by #ChatGPT, #GPT-based large language models or written by humans.

Here is a python wrapper of openai model to detect if a text is written by humans or generated by ChatGPT, GPT models

Github: [https://github.com/promptslab/openai-detector](https://github.com/promptslab/openai-detector)  
Openai release: [https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/](https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/)

If you are interested in #PromptEngineering, #LLMs, #ChatGPT and other latest research discussions, please consider joining our discord [discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)  


https://preview.redd.it/9d8ooeg2ljfa1.png?width=1358&format=png&auto=webp&s=0de7ccc5cd16d6bbad3dcfe7d8441547a6196fc8"
73,deeplearning,open-ai,top,2022-06-07 06:12:32,Looking for help for hire,Ok_Wish4469,False,1.0,19,v6o5n2,https://www.reddit.com/r/deeplearning/comments/v6o5n2/looking_for_help_for_hire/,8,1654582352.0," 

I'm both a collector and a coin dealer. I look through tens of thousands of coins a week for rare dates, errors, etc. But as I get older, my eyes are not what they use to be. So it's getting somewhat difficult for me to see the key details on the coin. So I decided to make a setup that can look through coins for me. I've been greatly influenced by this machine that does everything I want, but I need something a lot smaller.

[https://youtu.be/k7okDtRRCcY](https://youtu.be/k7okDtRRCcY)

I do have a basic background in coding and how it works. But I have little experience with making an AI. I've watched many video tutorials and I now understand clearly how an AI learns. I think the best route is to use Python, TensorFlow, and open-cv. But I keep getting some kind of errors that have been a major roadblock for me.

If this is relevant. My company setup is a ryzen 9 5900x. 3080 gpu and has 64gb of ram.

I'm looking for someone who can guide me through installing and training an AI model. I will compensate for your time, either in money or in collectible coins. What I mean for collectable coins is good quality coins. Not those cheapy coins you pick up from gift shops. But actually pieces of history. I've got silver coins, I've got a ton of English coins from 1600s-1800s. You can check out my ebay store to get a idea of what I have to offer. [https://www.ebay.com/sch/uncommoncentscoins/m.html?\_nkw&\_armrs=1&\_ipg&\_from&LH\_Complete=1&LH\_Sold=1&rt=nc&\_trksid=p2046732.m1684](https://www.ebay.com/sch/uncommoncentscoins/m.html?_nkw&_armrs=1&_ipg&_from&LH_Complete=1&LH_Sold=1&rt=nc&_trksid=p2046732.m1684)"
74,deeplearning,open-ai,top,2023-03-09 00:50:22,"AI generated video chapter titles (YouTube, Vimeo, etc)",happybirthday290,False,0.88,17,11mdvb9,https://i.redd.it/h6utxsxg2mma1.png,1,1678323022.0,
75,deeplearning,open-ai,top,2021-07-26 08:07:10,Voice Cloning Model for AI chatbot,Accurate_Tale,False,0.95,19,oruaz3,https://www.reddit.com/r/deeplearning/comments/oruaz3/voice_cloning_model_for_ai_chatbot/,1,1627286830.0,"Hello! I am working on a voice-driven AI chatbot and i want to give the user the option to  customize the voice of the chatbot after getting an audio clip from user! now i know about the famous real time voice cloning method [https://github.com/CorentinJ/Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning)

But i also want to explore other open source models for my project to see if there is some other model more suitable for my project! Suggestions will be helpful!

Thanks in advance"
76,deeplearning,open-ai,top,2018-08-02 18:20:16,"Snark AI Update: Jupyter, Docker and Fast.AI",snarkai,False,0.91,19,941s7i,https://www.reddit.com/r/deeplearning/comments/941s7i/snark_ai_update_jupyter_docker_and_fastai/,0,1533234016.0,"We are Sergiy, Davit and Jason, founders of Snark AI ([https://snark.ai](https://snark.ai)). We're taking advantage of the idle GPUs in enterprises' private GPU cloud to provide l[ow-cost GPUs](https://lab.snark.ai/pricing) for Deep Learning training and deployment on semi-decentralized servers. We started Snark AI during our PhD programs at Princeton University working on hardware specific deep learning inference optimization and large scale distributed deep learning training.

Since our previous blogpost about [Unchaining GPUs for Deep Learning](https://blog.usejournal.com/blockchain-gpus-unchained-running-neural-networks-without-hurting-mining-hash-rate-38a88728a1c9), [Hacker News Launch](https://news.ycombinator.com/item?id=17491604) and [TechCrunch](https://techcrunch.com/2018/07/25/snark-ai-looks-to-help-companies-get-on-demand-access-to-idle-gpus/) article , we have worked hard with our early users to provide low-cost GPUs for Deep Learning through a very simple interface. 

    $pip3 install snark

https://reddit.com/link/941s7i/video/p1zxe3680qd11/player

Throughout working with them, we learnt so much about their workflow and came up with some important feature updates. We are ready to share them today.

    $snark start

**Preinstalled DL Frameworks**

We have standardized deep learning frameworks pre-installed for you in different pod types. Currently we support **pytorch** pod with pytorch + caffe2, **tensorflow** pod with tensorflow + keras, **mxnet** pod and more. We've installed all dependencies in our deep learning pods. Try this

    $snark start -t pytorch

**Customizeble GPU specs**

It's very easy to choose number of GPUs you need. Running

    $snark start -t pytorch -g 1
    $snark start -t pytorch -g 2 
    $snark start -t pytorch -g 3

will give you 1 GPU, 2 GPU and 3 GPUs on a same machine.

**Persistent Storage**

Files in your home folder are stored persistently across all running pods. No need to attach volumes. Everything is just there when you login to your pod. We found this very convenient on two aspects:

1. Reconfigure hardware specs at ease. Want to scale up training with more GPUs? Simply stop the old pod and start a new pod with more GPUs. Your old files and installed packages will still be there with the new hardware spec!​​
2. Easier instance management. Now you can easily stop at any point, take a break (without being charged for GPU time) and then start the instance again.

**Jupyter Support**

We are releasing simple Jupyter access. 

    $snark start --jupyter

Just open your local browser with [http://localhost:8888](http://localhost:8888). You will need to copy the token from the CLI for security reasons. Start experimenting on a remote GPU quickly. You can also open a manual port as you would do in SSH in case you want to run e.g. Tensorboard

    $snark start -L 6006:localhost:6006

Jupyter lab next to come.

**Docker (beta)**

If you need to run your custom container with your prebuilt environment, here you go. One requirement would be to make sure that the base of the docker is Ubuntu such that we can easily wrap connection to the container.

    $snark start -t custom --docker_image username/image:tag

If you need other images please reach us and we will support it. This feature is still in beta and your feedback would really help us to improve.

[**Fast.ai**](https://Fast.ai) **Ready**

Once you have persistent storage, Jupyter access and customized dockers, what else you might need to hack Deep Learning? We also provide ready [Fast.ai](https://Fast.ai) courses for your ease to start learning Deep Learning.

    $snark start --pod_type fast.ai --jupyter

If you have more feature requests or feedback happy to chat with you on our website."
77,deeplearning,open-ai,top,2023-06-14 14:17:47,"Launch of Aim on Hugging Face Spaces!!! 🤗 Visualize tracked logs - metrics, h-params and other training metadata and seamlessly share your training results with anyone.",tatyanaaaaaa,False,0.91,19,14992gk,https://www.reddit.com/r/deeplearning/comments/14992gk/launch_of_aim_on_hugging_face_spaces_visualize/,0,1686752267.0,"Hi r/deeplearning community!

Excited to share with you the launch of Aim on Hugging Face Spaces. 🤗🥳

Now Hugging Face users can share their training results alongside with models and datasets on the Hub in a few clicks.

https://preview.redd.it/7t14wsq1sz5b1.jpg?width=1500&format=pjpg&auto=webp&s=95743025496899338e64ea297c22f8573eadde0a

Aim is an open-source, self-hosted AI Metadata tracking tool. It provides a performant and powerful UI for exploring and comparing metadata, such as training runs or AI agents executions. Additionally, its SDK enables programmatic access to tracked metadata — perfect for automations and Jupyter Notebook analysis.

When navigating to your Aim Space, you'll see the Aim homepage, which provides a quick glance at your training statistics and an overview of your logs. 👇

[Home page](https://preview.redd.it/fnyglu83sz5b1.jpg?width=1500&format=pjpg&auto=webp&s=7e8c783a76d6863df7d50eb26b7d0d85bdf29ff0)

Open the individual run page to find all the insights related to that run, including tracked hyper-parameters, metric results, system information (CLI args, env vars, Git info, etc.) and visualizations. 📊

[Runs page](https://preview.redd.it/65xhiol5sz5b1.jpg?width=1500&format=pjpg&auto=webp&s=e261b38ec3013784794355d607d4861ed7dd3408)

Take your training results analysis to the next level with Aim's Explorers - tools that allow to deeply compare tracked metadata across runs. 🚀

Metrics Explorer, for instance, enables you to query tracked metrics and perform advanced manipulations such as grouping metrics, aggregation, smoothing, adjusting axes scales and other complex interactions.

[Metrics explorer](https://preview.redd.it/oa10cdf7sz5b1.jpg?width=1500&format=pjpg&auto=webp&s=7b293f3cbda4c7876a258a47f130bfa1f8583aed)

Explorers provide fully Python-compatible expressions for search, allowing to query metadata with ease. In addition to Metrics Explorer, Aim offers a suite of Explorers designed to help you explore and compare a variety of media types, including images, text, audio, and Plotly figures.

[Images explorer](https://preview.redd.it/ae2a8z2bsz5b1.jpg?width=1500&format=pjpg&auto=webp&s=ed2d7923406b43aad4969fc36d6039dc42f1cf1c)

One more thing 👀

Having Aim logs hosted on Hugging Face Hub, you can embed it in notebooks and websites.

See Aim in Action with Existing Demos on the Hub, Neural machine translation task: [https://huggingface.co/spaces/aimstack/nmt](https://huggingface.co/spaces/aimstack/nmt)

That's not all!! 🤯

To learn more, checkout the full guide [here](https://aimstack.io/blog/integrations/launching-aim-on-hugging-face-spaces).

Support Aim by dropping a star on GitHub: [https://github.com/aimhubio/aim](https://github.com/aimhubio/aim)

Hope you enjoyed reading and thanks for your time! Feel free to share your thoughts, would love to read them. 🤗"
78,deeplearning,open-ai,top,2023-09-29 14:02:33,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.91,18,16vch0x,https://www.reddit.com/r/deeplearning/comments/16vch0x/this_week_in_ai_all_the_major_ai_developments_in/,2,1695996153.0,"1. **Meta AI** presents **Emu**, a quality-tuned latent diffusion model for generating highly aesthetic images. Emu significantly outperforms SDXLv1.0 on visual appeal.
2. **Meta AI** researchers present a series of long-context LLMs with context windows of up to 32,768 tokens. LLAMA 2 70B variant surpasses gpt-3.5-turbo-16k’s overall performance on a suite of long-context tasks.
3. **Abacus AI** released a larger 70B version of **Giraffe**. Giraffe is a family of models that are finetuned from base Llama 2 and have a larger context length of 32K tokens\].
4. **Cerebras** and **Opentensor** released Bittensor Language Model, ‘**BTLM-3B-8K**’, a new 3 billion parameter open-source language model with an 8k context length trained on 627B tokens of SlimPajama. It outperforms models trained on hundreds of billions more tokens and achieves comparable performance to open 7B parameter models. The model needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models and is available with an Apache 2.0 license for commercial use.
5. **OpenAI** is rolling out, over the next two weeks, new voice and image capabilities in ChatGPT enabling ChatGPT to understand images, understand speech and speak. The new voice capability is powered by a new text-to-speech model, capable of generating human-like audio from just text and a few seconds of sample speech. .
6. **Mistral AI**, a French startup, released its first 7B-parameter model, **Mistral 7B**, which outperforms all currently available open models up to 13B parameters on all standard English and code benchmarks. Mistral 7B is released in Apache 2.0, making it usable without restrictions anywhere.
7. **Microsoft** has released **AutoGen** \- an open-source framework that enables development of LLM applications using multiple agents that can converse with each other to solve a task. Agents can operate in various modes that employ combinations of LLMs, human inputs and tools.
8. **LAION** released **LeoLM**, the first open and commercially available German foundation language model built on Llama-2
9. Researchers from **Google** and **Cornell University** present and release code for DynIBaR (Neural Dynamic Image-Based Rendering) - a novel approach that generates photorealistic renderings from complex, dynamic videos taken with mobile device cameras, overcoming fundamental limitations of prior methods and enabling new video effects.
10. **Cloudflare** launched **Workers AI** (an AI inference as a service platform), **Vectorize** (a vector Database) and **AI Gateway** with tools to cache, rate limit and observe AI deployments. Llama2 is available on Workers AI.
11. **Amazon** announced the general availability of **Bedrock**, its service that offers a choice of generative AI models from Amazon itself and third-party partners through an API.
12. **Spotify** has launched a pilot program for AI-powered voice translations of podcasts in other languages - in the podcaster’s voic. It uses OpenAI’s newly released voice generation model.
13. **Getty Images** has launched a generative AI image tool, ‘**Generative AI by Getty Images**’, that is ‘commercially‑safe’. It’s powered by Nvidia Picasso, a custom model trained exclusively using Getty’s images library.
14. **Optimus**, Tesla’s humanoid robot, can now sort objects autonomously and do yoga. Its neural network is trained fully end-to-end.
15. **Amazon** will invest up to $4 billion in Anthropic. Developers and engineers will be able to build on top of Anthropic’s models via Amazon Bedrock.
16. **Google Search** indexed shared Bard conversational links into its search results pages. Google says it is working on a fix.

&#x200B;

  
My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
79,deeplearning,open-ai,top,2023-08-15 04:46:43,OpenAI Notebooks which are really helpful.,vishank97,False,0.91,19,15rihgo,https://www.reddit.com/r/deeplearning/comments/15rihgo/openai_notebooks_which_are_really_helpful/,3,1692074803.0,"The OpenAI cookbook is one of the most underrated and underused developer resources available today. Here are 7 notebooks you should know about:

1. Improve LLM reliability:  
[https://github.com/openai/openai-cookbook/blob/main/techniques\_to\_improve\_reliability.md](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)
2. Embedding long text inputs:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Embedding\_long\_inputs.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb)
3. Dynamic masks with DALLE:  
[https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How\_to\_create\_dynamic\_masks\_with\_DALL-E\_and\_Segment\_Anything.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How_to_create_dynamic_masks_with_DALL-E_and_Segment_Anything.ipynb)
4. Function calling to find places nearby:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Function\_calling\_finding\_nearby\_places.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb)
5. Visualize embeddings in 3D:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing\_embeddings\_in\_3D.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_3D.ipynb)
6. Pre and post-processing of Whisper transcripts:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Whisper\_processing\_guide.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb)
7. Search, Retrieval, and Chat:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Question\_answering\_using\_a\_search\_API.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb)

Big thanks to the creators of these notebooks!"
80,deeplearning,open-ai,top,2022-07-15 13:18:45,Have fun and learn AI at the Reinforcement Learning Hackathon on July 23rd!,zakrzzz,False,1.0,17,vzojmv,https://www.reddit.com/r/deeplearning/comments/vzojmv/have_fun_and_learn_ai_at_the_reinforcement/,1,1657891125.0,"One day to immerse yourself in technology that is a first for companies and engineers around the world!

To help you begin your immersion in AI as effectively as possible, we've prepared experts to assist you all the way.

Not without a competitive component, the winners will receive worthy prizes that will help them successfully use advanced technologies for their projects.

So come join us and learn everything you need to know about RL!

[Register here](https://lablab.ai/event/reinforcement-learning-openai-gym?utm_medium=23&utm_source=Reddit&utm_campaign=RL1&utm_term=Hackathon)

[Reinforcement Learning OpenAI Gym Hackathon](https://preview.redd.it/vm4gf3pviqb91.png?width=1920&format=png&auto=webp&s=bb3ba85c6e4d5b3069c742a3e38e6c3d70d6843a)"
81,deeplearning,open-ai,top,2023-01-28 06:34:28,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,0.95,16,10n8c80,https://www.reddit.com/r/deeplearning/comments/10n8c80/a_python_module_to_generate_optimized_prompts/,0,1674887668.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features 🚀

* 🧙‍♀️ NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* 🔨 Easily add one-shot, two-shot, or few-shot examples to the prompt
* ✌ Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* 💥 Custom examples and samples can be easily added to the prompt
* 💰 Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Example](https://preview.redd.it/sjvhtd8b3nea1.png?width=1236&format=png&auto=webp&s=e9a3a28f41f59cd25fe8e95bd1fca56b15f27a6e)

&#x200B;

https://preview.redd.it/fnb05bys3nea1.png?width=1398&format=png&auto=webp&s=096f4e2cbd0a71e795f30cc5e3720316b5e5caf6"
82,deeplearning,open-ai,top,2022-02-28 10:15:37,TinyML Monitoring Air Quality an 8-bit Microcontroller,literallair,False,0.91,18,t3ce8c,https://www.reddit.com/r/deeplearning/comments/t3ce8c/tinyml_monitoring_air_quality_an_8bit/,2,1646043337.0,"I’d like to share my experiment on how to easily create your own tiny machine learning model and run inferences on a microcontroller to detect the concentration of various gases. I will illustrate the whole process with my example of detecting the concentration of benzene (С6H6(GT)) based on the concentration of other recorded compounds.

Things I used in this project: Arduino Mega 2560, Neuton Tiny ML software

To my mind, such simple solutions may contribute to improving the air pollution problem which now causes serious concerns. In fact, the World Health Organization estimates that over seven million people die prematurely each year from diseases caused by air pollution. Can you imagine that?

As such, more and more organizations, responsible for monitoring emissions, need to have effective tools at their disposal to monitor the air quality in a timely way, and TinyML solutions seem to be the best technology for that. They are quite low-energy and cheap to produce, as well as they don’t require a permanent Internet connection. I believe these factors will promote the mass implementation of TinyML as a great opportunity to create AI-based devices and successfully solve various challenges.

Therefore, in my experiment, I take the most primitive 8-bit MCU to show that even such a device today can have ML models in it.

Dataset description:

My dataset contained 5875 rows of hourly averaged responses from an array of oxide chemical sensors that were located on the field in a polluted area in Italy, at road level. Hourly averaged concentrations for CO, Non-Metanic Hydrocarbons, Benzene, Total Nitrogen Oxides (NOx), and Nitrogen Dioxide (NO2) were provided.

It is a regression problem.

Target metric – MAE (Mean Absolute Error). Target - C6H6(GT).

Attribute Information:RH - Relative Humidity

AH - Absolute Humidity

T - Temperature in °C;

PT08.S3(NOx) - Tungsten oxide. Hourly averaged sensor response (nominally NOx targeted);

PT08.S4(NO2) - Tungsten oxide. Hourly averaged sensor response (nominally NO2 targeted);

PT08.S5(O3) - Indium oxide. Hourly averaged sensor response (nominally O3 targeted);

PT08.S1(CO) - (Tin oxide) hourly averaged sensor response (nominally CO targeted);

CO(GT) - True hourly averaged concentration CO in mg/m\^3 (reference analyzer);

PT08.S2(NMHC) - Titania. hourly averaged sensor response (nominally NMHC targeted);

You can see more details and download the dataset here: ​​[https://archive.ics.uci.edu/ml/datasets/air+qualityProcedure](https://archive.ics.uci.edu/ml/datasets/air+qualityProcedure):

Step 1: Model Training

The model was created and trained with a free tool, Neuton TinyML, as I needed a super compact model that would fit into a tiny microcontroller with 8-bit precision. I tried to make such a model with the help of TensorFlow before, but it was too large to run operations on 8 bit.

To train the model, I converted the dataset into a CSV file, uploaded it to the platform, and selected the column that should be trained to make predictions.  


&#x200B;

https://preview.redd.it/m6awqd82ujk81.png?width=1899&format=png&auto=webp&s=a4ae08e004503ccd73f9255b13c55297f0da91fe

&#x200B;

https://preview.redd.it/3ttfxv73ujk81.png?width=1901&format=png&auto=webp&s=4546be70ff77ce7fa711c3ad55e007fed62ac592

The trained model had the following characteristics:  
The model turned out to be super compact, having only 38 coefficients and 0.234 KB in size!  


&#x200B;

https://preview.redd.it/614e6s68ujk81.png?width=1900&format=png&auto=webp&s=6f02a6f6aea2f94e6cedcabda15e09a2bb6ab467

Additionally, I created models with TF and TF Lite and measured metrics on the same dataset. The comparison speaks louder than words. Also, as I said above, TF models still cannot run operations on 8 bits, but it was interesting for me to use just such a primitive device.  


&#x200B;

https://preview.redd.it/4jiion69ujk81.png?width=1497&format=png&auto=webp&s=a0e1c055a63e548676c94638a321222c063365fd

Step 2: Embedding into a Microcontroller

Upon completion of training, I downloaded the archive which contained all the necessary files, including meta-information about the model in two formats (binary, and HEX), calculator, Neuton library, and the implementation file.  


&#x200B;

https://preview.redd.it/1j1t0m5fujk81.png?width=1900&format=png&auto=webp&s=554643f89da2f472143d69c995c31a0b9d05d582

Since I couldn’t run the experiment in field conditions with real gases, I developed a simple protocol to stream data from a computer.

Step 3: Running Inference on the Microcontroller

I connected a microcontroller on which the prediction was performed to a computer via a serial port, so signals were received in a binary format.

The microcontroller was programmed to turn on the red LED if the concentration of benzene was exceeded, and the green LED - if the concentration was within permitted limits. Check out the videos below to see how it worked.  


&#x200B;

https://reddit.com/link/t3ce8c/video/mk4lhaawtjk81/player

In this case, the concentration of benzene is within reasonable bounds (<15 mg/m3).  


&#x200B;

https://reddit.com/link/t3ce8c/video/1nsbsroxtjk81/player

In this case, the concentration of benzene exceeds the limits (>15 mg/m3).

Conclusion

My example vividly illustrates how everyone can easily use the TinyML approach to create compact but smart devices, even with 8-bit precision. I’m convinced that the low production costs and high efficiency of TinyML open up enormous opportunities for its worldwide implementation.

Due to the absence of the need to involve technical specialists, in this particular case, even non-data scientists can rapidly build super compact models and locate smart AI-driven devices throughout the area to monitor air quality in real-time. To my mind, it’s really inspiring that such small solutions can help us improve the environmental situation on a global scale!"
83,deeplearning,open-ai,top,2019-11-20 09:44:48,[Research] Announcing Kaolin - PyTorch Library for Accelerating 3D Deep Learning Research,cdossman,False,0.9,15,dyzv2d,https://www.reddit.com/r/deeplearning/comments/dyzv2d/research_announcing_kaolin_pytorch_library_for/,0,1574243088.0," A group of researchers who were working at NVIDIA has introduced Kaolin, a new PyTorch library with an aim to accelerate 3D deep learning research. Kaolin is home for future 3D DL research and you are welcome to make contributions. 

\#PyTorch #3DdeepLearning #ai #aiResearch #educateai #openSourceSoftware

Read more: https://medium.com/ai%C2%B3-theory-practice-business/pytorch-library-for-accelerating-3d-deep-learning-research-6b83df2073bf"
84,deeplearning,open-ai,top,2023-11-15 10:30:36,"GPT-4 Turbo: Die Zukunft der Künstlichen Intelligenz, entwickelt von OpenAI",Webglobic_tech,False,0.86,15,17vqtlk,https://webglobic.com/magazine/,0,1700044236.0,
85,deeplearning,open-ai,top,2020-12-15 23:04:59,[R] NeurIPS 2020 | Teaching Transformers New Tricks,Yuqing7,False,0.95,17,kdws3z,https://www.reddit.com/r/deeplearning/comments/kdws3z/r_neurips_2020_teaching_transformers_new_tricks/,0,1608073499.0,"Transformers are a class of attention-based neural architectures that have enabled advanced pretrained language models such as Google’s BERT and OpenAI’s GPT series and produced numerous breakthroughs in speech recognition and other natural language processing (NLP) tasks since their debut in 2017. Transformers perform exceptionally well on problems with sequential data, and have more recently been extended to reinforcement learning, computer vision and symbolic mathematics.

This year, 22 Transformer-related research papers were accepted by NeurIPS, the world’s most prestigious machine learning conference. *Synced* has selected ten of these works to showcase the latest Transformer trends — from extended use of the neural architecture to innovative advancements in technique, architectural design changes and more.

Here is a quick read: [NeurIPS 2020 | Teaching Transformers New Tricks](https://syncedreview.com/2020/12/15/neurips-2020-teaching-transformers-new-tricks/)"
86,deeplearning,open-ai,top,2022-07-06 11:22:15,Reinforcement Learning without Reward Engineering (reproducing OpenAI paper with crowdsourcing),Euphetar,False,0.9,13,vsnnv9,https://medium.com/p/60c63402c59f,0,1657106535.0,
87,deeplearning,open-ai,top,2021-03-08 09:57:12,"Intent and Action Classification, analyze Chinese News and the Crypto market, train a classifier that understands 100+ languages, translate between 200 + languages, answer questions, summarize text and much more on NLU 1.1.3",CKL-IT,False,0.78,14,m0cio8,https://www.reddit.com/r/deeplearning/comments/m0cio8/intent_and_action_classification_analyze_chinese/,1,1615197432.0,"# Intent and Action Classification,  analyze Chinese News and the Crypto market, train a classifier that understands 100+ languages, translate between 200 + languages, answer questions, summarize text, and much more in NLU 1.1.3 

## NLU 1.1.3 Release Notes
We are very excited to announce that the latest NLU release comes with a new pretrained Intent Classifier and NER Action Extractor for text related to
music, restaurants, and movies trained on the SNIPS dataset. Make sure to check out the models hub and the easy 1-liners for more info!

In addition to that, new NER and Embedding models for Bengali are now available

Finally, there is a new NLU Webinar with 9 accompanying tutorial notebooks which teach you  a lot of things and is segmented into the following parts :

- Part1: Easy 1 Liners 
  - Spell checking/Sentiment/POS/NER/ BERTtology embeddings
- Part2: Data analysis and NLP tasks on [Crypto News Headline dataset](https://www.kaggle.com/kashnitsky/news-about-major-cryptocurrencies-20132018-40k)
  - Preprocessing and extracting Emotions, Keywords, Named Entities and visualize them
- Part3: NLU Multi-Lingual 1 Liners with [Microsoft's Marian Models](https://marian-nmt.github.io/publications/)
  - Translate between 200+ languages (and classify lang afterward)
- Part 4: Data analysis and NLP tasks on [Chinese News Article Dataset](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/4_Unsupervise_Chinese_Keyword_Extraction_NER_and_Translation_from_Chinese_News.ipynb)
  - Word Segmentation, Lemmatization, Extract Keywords, Named Entities and translate to english
- Part 5: Train a sentiment Classifier that understands 100+ Languages
  - Train on a french sentiment dataset and predict the sentiment of 100+ languages with [language-agnostic BERT Sentence Embedding](https://arxiv.org/abs/2007.01852)
- Part 6: Question answering, Summarization, Squad and more with [Google's T5](https://arxiv.org/abs/1910.10683)
  - T5 Question answering and 18 + other NLP tasks ([SQUAD](https://arxiv.org/abs/1606.05250) / [GLUE](https://arxiv.org/abs/1804.07461) / [SUPER GLUE](https://super.gluebenchmark.com/))


### New Models

#### NLU 1.1.3 New Non-English Models

| Language | nlu.load() reference                                         | Spark NLP Model reference                                    | Type                  |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------- |
| Bengali  | [bn.ner.cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengali_cc_300d_bn.html) | [ bengaliner_cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengali_cc_300d_bn.html) | NerDLModel    |
| Bengali  | [bn.embed](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | [bengali_cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | NerDLModel            |
| Bengali  | [bn.embed.cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | [bengali_cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | Word Embeddings Model (Alias)    |
| Bengali  | [bn.embed.glove](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) | [bengali_cc_300d](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html) |  Word Embeddings Model (Alias)|





#### NLU 1.1.3 New English Models

|Language | nlu.load() reference | Spark NLP Model reference | Type |
|---------|---------------------|----------------------------|------|
| English | [en.classify.snips](https://nlp.johnsnowlabs.com/2021/02/15/nerdl_snips_100d_en.html) |[nerdl_snips_100d](https://nlp.johnsnowlabs.com/2021/02/15/nerdl_snips_100d_en.html)     | NerDLModel |
| English | [en.ner.snips](https://nlp.johnsnowlabs.com/2021/02/15/classifierdl_use_snips_en.html) |[classifierdl_use_snips](https://nlp.johnsnowlabs.com/2021/02/15/classifierdl_use_snips_en.html)|ClassifierDLModel|




### New NLU Webinar
#### [State-of-the-art Natural Language Processing for 200+ Languages with 1 Line of code](https://events.johnsnowlabs.com/state-of-the-art-natural-language-processing-for-200-languages-with-1-line-of-code)


##### Talk Abstract 
Learn to harness the power of 1,000+ production-grade & scalable NLP models for 200+ languages - all available with just 1 line of Python code by leveraging the open-source NLU library, which is powered by the widely popular Spark NLP.

John Snow Labs has delivered over 80 releases of Spark NLP to date, making it the most widely used NLP library in the enterprise and providing the AI community with state-of-the-art accuracy and scale for a variety of common NLP tasks. The most recent releases include pre-trained models for over 200 languages - including languages that do not use spaces for word segmentation algorithms like Chinese, Japanese, and Korean, and languages written from right to left like Arabic, Farsi, Urdu, and Hebrew. All software and models are free and open source under an Apache 2.0 license.

This webinar will show you how to leverage the multi-lingual capabilities of Spark NLP & NLU - including automated language detection for up to 375 languages, and the ability to perform translation, named entity recognition, stopword removal, lemmatization, and more in a variety of language families. We will create Python code in real-time and solve these problems in just 30 minutes. The notebooks will then be made freely available online.

You can watch the [video here,](https://events.johnsnowlabs.com/state-of-the-art-natural-language-processing-for-200-languages-with-1-line-of-code) 

### NLU 1.1.3 New Notebooks and tutorials


#### New Webinar Notebooks

1. [NLU basics, easy 1-liners (Spellchecking, sentiment, NER, POS, BERT](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/0_liners_intro.ipynb)
2. [Analyze Crypto News dataset with Keyword extraction, NER, Emotional distribution, and stemming](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/1_NLU_base_features_on_dataset_with_YAKE_Lemma_Stemm_classifiers_NER_.ipynb)
3. [Translate Crypto News dataset between 300 Languages with the Marian Model (German, French, Hebrew examples)](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/2_multilingual_translation_with_marian_intro.ipynb)
4. [Translate Crypto News dataset between 300 Languages with the Marian Model (Hindi, Russian, Chinese examples)](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/3_more_multi_lingual_NLP_translation_Asian_languages_with_Marian.ipynb)
5. [Analyze Chinese News Headlines with Chinese Word Segmentation, Lemmatization, NER, and Keyword extraction](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/4_Unsupervise_Chinese_Keyword_Extraction_NER_and_Translation_from_Chinese_News.ipynb)
6. [Train a Sentiment Classifier that will understand 100+ languages on just a French Dataset with the powerful Language Agnostic Bert Embeddings](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/5_multi_lingual_sentiment_classifier_training_for_over_100_languages.ipynb)
7. [Summarize text and Answer Questions with T5](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/6_T5_question_answering_and_Text_summarization.ipynb)
8. [Solve any task in 1 line from SQUAD, GLUE and SUPER GLUE with T5](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/7_T5_SQUAD_GLUE_SUPER_GLUE_TASKS.ipynb)
9. [Overview of models for various languages](https://github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/8_Multi_lingual_ner_pos_stop_words_sentiment_pretrained.ipynb)





#### New easy NLU 1-liners in NLU 1.1.3

####  [Detect actions in general commands related to music, restaurant, movies.](https://nlp.johnsnowlabs.com/2021/02/15/nerdl_snips_100d_en.html)


```python
nlu.load(""en.classify.snips"").predict(""book a spot for nona gray  myrtle and alison at a top-rated brasserie that is distant from wilson av on nov  the 4th  2030 that serves ouzeri"",output_level = ""document"")
```

outputs :

|                                               ner_confidence | entities                                                     | document                                                     | Entities_Classes                                             |
| -----------------------------------------------------------: | :----------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| [1.0, 1.0, 0.9997000098228455, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9990000128746033, 1.0, 1.0, 1.0, 0.9965000152587891, 0.9998999834060669, 0.9567000269889832, 1.0, 1.0, 1.0, 0.9980000257492065, 0.9991999864578247, 0.9988999962806702, 1.0, 1.0, 0.9998999834060669] | ['nona gray myrtle and alison', 'top-rated', 'brasserie', 'distant', 'wilson av', 'nov the 4th 2030', 'ouzeri'] | book a spot for nona gray myrtle and alison at a top-rated brasserie that is distant from wilson av on nov the 4th 2030 that serves ouzeri | ['party_size_description', 'sort', 'restaurant_type', 'spatial_relation', 'poi', 'timeRange', 'cuisine'] |

####  [Named Entity Recognition (NER) Model in Bengali (bengaliner_cc_300d)](https://nlp.johnsnowlabs.com/2021/02/10/bengaliner_cc_300d_bn.html)


```python
# Bengali for: 'Iajuddin Ahmed passed Matriculation from Munshiganj High School in 1947 and Intermediate from Munshiganj Horganga College in 1950.'
nlu.load(""bn.ner.cc_300d"").predict(""১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন"",output_level = ""document"")
```

outputs :

| ner_confidence                                                                                                                                                                                                                                                                                                                                                                                                                       | entities                                                                           | Entities_Classes   | document                                                                                                                         |
|---------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|:--------------------------------------|
| [0.9987999796867371, 0.9854000210762024, 0.8604000210762024, 0.6686999797821045, 0.5289999842643738, 0.7009999752044678, 0.7684999704360962, 0.9979000091552734, 0.9976000189781189, 0.9930999875068665, 0.9994000196456909, 0.9879000186920166, 0.7407000064849854, 0.9215999841690063, 0.7657999992370605, 0.39419999718666077, 0.9124000072479248, 0.9932000041007996, 0.9919999837875366, 0.995199978351593, 0.9991999864578247] | ['সালে', 'ইয়াজউদ্দিন আহম্মেদ', 'মুন্সিগঞ্জ উচ্চ বিদ্যালয়', 'সালে', 'মুন্সিগঞ্জ হরগঙ্গা কলেজ'] | ['TIME', 'PER', 'ORG', 'TIME', 'ORG'] | ১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন |

#### [Identify intent in general text - SNIPS dataset](https://nlp.johnsnowlabs.com/2021/02/15/classifierdl_use_snips_en.html)


```python
nlu.load(""en.ner.snips"").predict(""I want to bring six of us to a bistro in town that serves hot chicken sandwich that is within the same area"",output_level = ""document"")
```

outputs :


| document | snips | snips_confidence|
|----------|------|------------------|
| I want to bring six of us to a bistro in town that serves hot chicken sandwich that is within the same area | BookRestaurant |                  1 |


#### [Word Embeddings for Bengali (bengali_cc_300d)](https://nlp.johnsnowlabs.com/2021/02/10/bengali_cc_300d_bn.html)




```python
# Bengali for : 'Iajuddin Ahmed passed Matriculation from Munshiganj High School in 1947 and Intermediate from Munshiganj Horganga College in 1950.'
nlu.load(""bn.embed"").predict(""১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন"",output_level = ""document"")
```

outputs :

|                                                     document | bn_embed_embeddings                                          |
| -----------------------------------------------------------: | :----------------------------------------------------------- |
| ১৯৪৮ সালে ইয়াজউদ্দিন আহম্মেদ মুন্সিগঞ্জ উচ্চ বিদ্যালয় থেকে মেট্রিক পাশ করেন এবং ১৯৫০ সালে মুন্সিগঞ্জ হরগঙ্গা কলেজ থেকে ইন্টারমেডিয়েট পাশ করেন | [-0.0828      0.0683      0.0215     ...  0.0679     -0.0484...] |



### NLU 1.1.3 Enhancements
- Added automatic conversion  to Sentence Embeddings of Word Embeddings when there is no Sentence Embedding Avaiable and a model needs the converted version to run.


### NLU 1.1.3 Bug Fixes
- Fixed a bug that caused `ur.sentiment` NLU pipeline to build incorrectly
- Fixed a bug that caused `sentiment.imdb.glove` NLU pipeline to build incorrectly
- Fixed a bug that caused `en.sentiment.glove.imdb` NLU pipeline to build incorrectly
- Fixed a bug that caused Spark 2.3.X environments to crash.

### NLU Installation

```bash
# PyPi
!pip install nlu pyspark==2.4.7
#Conda
# Install NLU from Anaconda/Conda
conda install -c johnsnowlabs nlu
```

### Additional NLU ressources

- [NLU Website](https://nlu.johnsnowlabs.com/)
- [All NLU Tutorial Notebooks](https://nlu.johnsnowlabs.com/docs/en/notebooks)
- [NLU Videos and Blogposts on NLU](https://nlp.johnsnowlabs.com/learn#pythons-nlu-library)
- [NLU on Github](https://github.com/JohnSnowLabs/nlu)
- [Suggestions or Questions? Contact us in Slack!](https://join.slack.com/t/spark-nlp/shared_invite/zt-lutct9gm-kuUazcyFKhuGY3_0AMkxqA)"
88,deeplearning,open-ai,top,2021-11-01 14:33:01,"[R] Warsaw U, OpenAI and Google’s Hourglass Hierarchical Transformer Model Outperforms Transformer Baselines",Yuqing7,False,0.86,15,qkf9xu,https://www.reddit.com/r/deeplearning/comments/qkf9xu/r_warsaw_u_openai_and_googles_hourglass/,2,1635777181.0,"A team from the University of Warsaw, OpenAI and Google Research proposes Hourglass, a hierarchical transformer language model that operates on shortened sequences to alleviate transformers’ huge computation burdens. 

Here is a quick read: [Warsaw U, OpenAI and Google’s Hourglass Hierarchical Transformer Model Outperforms Transformer Baselines.](https://syncedreview.com/2021/11/01/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-135/)

The paper *Hierarchical Transformers Are More Efficient Language Models* is on [arXiv](https://arxiv.org/abs/2110.13711)."
89,deeplearning,open-ai,top,2019-03-12 08:19:55,Year-Long AI Fellowships,the_new_scientist,False,0.94,14,b05a06,https://www.reddit.com/r/deeplearning/comments/b05a06/yearlong_ai_fellowships/,3,1552378795.0,"Other than Google, Facebook, and OpenAI, what other companies offer 1 year research fellowships for post masters students?"
90,deeplearning,open-ai,top,2020-12-21 06:13:10,Classification with Localization: Convert any Keras Classifier to a Detector,spmallick,False,0.93,13,khbkqt,https://www.reddit.com/r/deeplearning/comments/khbkqt/classification_with_localization_convert_any/,0,1608531190.0,"When it comes to common applications of Deep Learning in Computer Vision, the two answers that pop up in anyone's minds are image classification and object detection. Unfortunately, writing the code for your own object detector in PyTorch or Keras is a difficult task.  


In this blog, we will introduce a method for carrying out classification with localization using a simple image classifier in TensorFlow.  


[https://www.learnopencv.com/classification-with-localization/](https://www.learnopencv.com/classification-with-localization/)  
[**#AI**](https://www.linkedin.com/feed/hashtag/?keywords=ai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#ComputerVision**](https://www.linkedin.com/feed/hashtag/?keywords=computervision&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#ML**](https://www.linkedin.com/feed/hashtag/?keywords=ml&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#ArtificialIntelligence**](https://www.linkedin.com/feed/hashtag/?keywords=artificialintelligence&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#MachineLearning**](https://www.linkedin.com/feed/hashtag/?keywords=machinelearning&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#OpenCV**](https://www.linkedin.com/feed/hashtag/?keywords=opencv&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#DL**](https://www.linkedin.com/feed/hashtag/?keywords=dl&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#DeepLearning**](https://www.linkedin.com/feed/hashtag/?keywords=deeplearning&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#deeplearningai**](https://www.linkedin.com/feed/hashtag/?keywords=deeplearningai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) 

https://i.redd.it/5h42v18jfh661.gif"
91,deeplearning,open-ai,top,2019-03-21 15:50:03,Artificial Intelligence : Open AI - Flow-based Deep Generative Models,gokulbalex,False,0.84,12,b3s4rm,https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html#types-of-generative-models,0,1553183403.0,
92,deeplearning,open-ai,top,2021-09-26 16:13:31,I added Codex (GitHub Copilot) to the terminal,tomd_96,False,1.0,13,pvwvz7,https://www.reddit.com/r/deeplearning/comments/pvwvz7/i_added_codex_github_copilot_to_the_terminal/,1,1632672811.0,"&#x200B;

https://i.redd.it/8ww6msiugvp71.gif

You can now let Zsh write code for you using the plugin I wrote: [https://github.com/tom-doerr/zsh\_codex](https://github.com/tom-doerr/zsh_codex)

All you need to provide is a comment or a variable name and the plugin will use OpenAI's Codex AI (powers GitHub Copilot) to write the corresponding code.

Be aware that you do need to get access to the Codex API."
93,deeplearning,open-ai,top,2018-12-31 16:36:18,2018 In Review: 10 Open-Sourced AI Datasets,gwen0927,False,0.88,12,ab8ob8,https://medium.com/syncedreview/2018-in-review-10-open-sourced-ai-datasets-696b3b49801f,0,1546274178.0,
94,deeplearning,open-ai,top,2022-10-18 16:52:15,Fully automated video generation - connecting OpenAI's Whisper with Stable Diffusion. Tutorial & code coming soon!,hayAbhay,False,0.94,12,y7cbf4,https://youtu.be/xRcoeUgD4GY,8,1666111935.0,
95,deeplearning,open-ai,top,2023-12-20 21:36:11,[Blogpost] Top Python Libraries of 2023,No_Dig_7017,False,0.88,12,18n5wzb,https://www.reddit.com/r/deeplearning/comments/18n5wzb/blogpost_top_python_libraries_of_2023/,4,1703108171.0,"Hello Python Community!

We're thrilled to present our 9th edition of the **Top Python Libraries and tools**, where we've scoured the Python ecosystem for the most innovative and impactful developments of the year.

This year, it’s been the boom of Generative AI and Large Language Models (LLMs) which have influenced our picks. Our team has meticulously reviewed and categorized over 100 libraries, ensuring we highlight both the mainstream and the hidden gems.

**Explore the entire list with in-depth descriptions here**: [](https://tryolabs.com/blog/top-python-libraries-2023)

Here’s a glimpse of our top 10 picks:

1. [LiteLLM](https://github.com/BerriAI/litellm) — Call any LLM using OpenAI format, and more.
2. [PyApp](https://github.com/ofek/pyapp) — Deploy self-contained Python applications anywhere.
3. [Taipy](https://github.com/Avaiga/taipy) — Build UIs for data apps, even in production.
4. [MLX](https://github.com/ml-explore/mlx) — Machine learning on Apple silicon with NumPy-like API.
5. [Unstructured](https://github.com/Unstructured-IO/unstructured) — The ultimate toolkit for text preprocessing.
6. [ZenML](https://github.com/zenml-io/zenml) and [AutoMLOps](https://github.com/GoogleCloudPlatform/automlops) — Portable, production-ready MLOps pipelines.
7. [WhisperX](https://github.com/m-bain/whisperX) — Speech recognition with word-level timestamps & diarization.
8. [AutoGen](https://github.com/microsoft/autogen) — LLM conversational collaborative suite.
9. [Guardrails](https://github.com/guardrails-ai/guardrails) — Babysit LLMs so they behave as intended.
10. [Temporian](https://github.com/google/temporian) — The “Pandas” built for preprocessing temporal data.

Our selection criteria prioritize innovation, robust maintenance, and the potential to spark interest across a variety of programming fields. Alongside our top picks, we've put significant effort into the long tail, showcasing a wide range of tools and libraries that are valuable to the Python community.

A huge thank you to the individuals and teams behind these libraries. Your contributions are the driving force behind the Python community's growth and innovation. 🚀🚀🚀

**What do you think of our 2023 lineup? Did we miss any library that deserves recognition?** Your feedback is vital to help us refine our selection each year.

Edit: updated the post body so the links are directly here in reddit."
96,deeplearning,open-ai,top,2023-04-02 18:10:37,Should we draw inspiration from Deep learning/Computer vision world for fine-tuning LLMs?,Vegetable-Skill-9700,False,0.75,11,129t3tl,https://www.reddit.com/r/deeplearning/comments/129t3tl/should_we_draw_inspiration_from_deep/,8,1680459037.0,"With HuggingGPT, BloombergGPT, and OpenAI's chatGPT store, it looks like the world is moving towards specialized GPTs for specialized tasks. What do you think are the best tips & tricks when it comes to fine-tuning and refining these task-specific GPTs?

Over the last decade, I have built many computer vision models (for human pose estimation, action classification, etc.), and our general approach was always based on Transfer learning. Take a state-of-the-art public model and fine-tune it by collecting data for the given use case.

Do you think that paradigm still holds true for LLMs?

Based on my experience, I believe observing the model's performance as it interacts with real-world data, identifying failure cases (where the model's outputs are wrong), and using them to create a high-quality retraining dataset will be the key.

&#x200B;

P.S. I am building an open-source project UpTrain ([https://github.com/uptrain-ai/uptrain](https://github.com/uptrain-ai/uptrain)), which helps data scientists to do so. We just wrote a blog on how this principle can be applied to fine-tune an LLM for a conversation summarization task. Check it out here: [https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation\_summarization](https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation_summarization)"
97,deeplearning,open-ai,top,2021-09-03 00:19:43,OpenAI's Codex in Vim,tomd_96,False,0.88,12,pgu4ez,https://www.reddit.com/r/deeplearning/comments/pgu4ez/openais_codex_in_vim/,1,1630628383.0,"&#x200B;

https://i.redd.it/7xui0u2ml6l71.gif

You can now let your editor write Python code for you using the Vim plugin I wrote: [https://github.com/tom-doerr/vim\_codex](https://github.com/tom-doerr/vim_codex)

All you need to provide is a docstring and the plugin will use OpenAI's Codex AI (powers GitHub Copilot) to write the corresponding code.

Be aware that you do need to get access to the Codex API."
98,deeplearning,open-ai,top,2023-06-29 01:09:36,SAM + Stable Diffusion for Text-to-Image Inpainting,Anmorgan24,False,0.8,11,14lqxl4,https://www.reddit.com/r/deeplearning/comments/14lqxl4/sam_stable_diffusion_for_texttoimage_inpainting/,4,1688000976.0,"New generative fill tools allow users to easily add, extend, or remove content from images with simple text prompts. But how can you implement this on your own images, using open source foundation models?

In my new full-code end-to-end tutorial, learn how to perform image inpainting and outpainting on any image using SAM + Stable Diffusion.

https://ai.plainenglish.io/sam-stable-diffusion-for-text-to-image-inpainting-55398a84497c"
99,deeplearning,open-ai,top,2023-06-29 19:49:38,"Open Orca, an open sourced replication of Microsofts Orca is in development! Heres the dataset!",Alignment-Lab-AI,False,1.0,12,14mejzk,https://www.reddit.com/r/deeplearning/comments/14mejzk/open_orca_an_open_sourced_replication_of/,2,1688068178.0,"Today we are releasing a dataset that lets open source models learn to think like GPT-4!

We call this Open Orca, as a tribute to the team who has released the Orca paper describing the data collection methods we have attempted to replicate in an open-source manner for the benefit of humanity.

With this data, we expect new open source models to be developed which are smaller, faster, and smarter than ever before because were going to be the ones doing the developing!

[https://huggingface.co/datasets/Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)

We'd like to give special recognition to the following contributors for their significant efforts and dedication:

caseus

Eric Hartford

NanoBit

Pankaj

winddude

Rohan

[http://alignmentlab.ai/:](http://alignmentlab.ai/:)

Entropi

neverendingtoast

AtlasUnified

AutoMeta

lightningRalf

NanoBit

caseus

The Orca paper has been replicated to as fine of a degree of precision as a motley crew of ML nerds toiling for weeks could pull off (a very high degree).

We will be releasing trained Orca models as the training currently in progress completes.

The dataset is still in final cleanup, and we will continue with further augmentations beyond the base Orca data in due time.

Right now, we are testing our fifth iteration of Orca on a subset of the final data, and are just about to jump into the final stages!

Many thanks to NanoBit and Caseus, makers of Axolotl \[[https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)\] for lending us their expertise on the platform that developed and trained manticore, minotaur, and many others!

If you want to follow along, meet the devs, ask us questions, get involved, or check out our other projects, such as:

Landmark Attention

[https://twitter.com/Yampeleg's](https://twitter.com/Yampeleg's) recently announced context extension method, which outperforms rope (were going to push this one later today)

EDIT: We've been made aware that Eric Hartford, a team member who chose to depart our team yesterday after some internal discussion of our grievances, has made claims to be the sole originator of the Open Orca project and to claim the work as his own. We wish to clarify that this was a team effort from the outset, and he was one of over a dozen data scientists, machine learning engineers, and other specialists who have been involved in this project from the outset.

Eric joined the team with the mutual understanding that we were all to be treated as equals and get our due credit for involvement, as well as say in group decisions.

He made snap decisions on behalf of the team contrary to long term plans, including announcing the project publicly on his blog, and implying that he was the sole originator and project lead.

We attempted to reconcile this internally, but he chose to depart from the team.

As such, we elected to release the data publicly in advance of original plans.

We have appropriately attributed he and all other contributors, as was originally planned.

We thank Eric for his contributions to the project and wish him well on his individual endeavors.

This repo is the original repo from which the entire team had agreed to work out of and publish out of from the outset.

Eric's repo represents his duplication and augmentation of the team's collective effort, initiated after he had chosen to depart the team."
100,deeplearning,open-ai,comments,2020-12-30 12:54:23,Paper reading group,porpkcab,False,0.97,35,kn1r63,https://www.reddit.com/r/deeplearning/comments/kn1r63/paper_reading_group/,63,1609332863.0,"Hi, fellow (ex-)scholars!

TL;DR: I want to set up a paper reading group in current machine learning research. If you want to join, please reply or DM me.

I obtained a master's degree in AI two years ago. Since then, I feel like it's hard to keep up with research in deep learning and stay in the loop. I can imagine that there are multiple scholars and ex-scholars like me who want to stay up-to-date with current research, broaden and deepen their knowledge in AI, meet like-minded people, discuss ideas, and improve their presentation skills. So I was thinking that it might be a fun idea to start a paper reading group. If you are interested, please reply or DM me, and we'll see if we can set something up.

I would be very open to suggestions and want this to be a joint effort. However, currently, I had the following in mind: Every two/three weeks we organize a meeting over zoom. In this meeting, one participant will present a summary of the chosen paper, probably using some slides. All other participants have read the paper to some degree such that we can ask questions and discuss the paper and its implications after the presentation. On Slack, we will appoint the next presenter. He/she can pick their own paper, but we will also start a poll where everyone can add suggestions and vote for the papers they are most interested in. Ideally, I would say that the group is not too big (~5-30 max.), and is somewhat fixed. This way, we can get to know each other a bit, create some accountability, and keep the discussions lively!
In terms of topics, I'd say that anything AI-related goes; Theoretical or applied research, NLP, CV, from fundamental papers on SVM's to state-of-the deep learning research, but also evolutionary algorithms, knowledge graphs, and anything in-between. Based on current research interests, deep learning research of the last few years will probably be the popular topic though.

Looking forward to discussing papers with you!

Edit: Wow, that's quite some interest. I believe you should be able to join with this link: 👋  Let’s move this to Slack! We’ve got 56 folks from the team there already. You can sign up here: https://join.slack.com/t/aischolars/shared_invite/zt-kwbtz1fq-vaQaoXKBps9masEX7wYXkg"
101,deeplearning,open-ai,comments,2023-03-25 04:24:49,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.92,46,121agx4,https://www.reddit.com/r/deeplearning/comments/121agx4/do_we_really_need_100b_parameters_in_a_large/,54,1679718289.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
102,deeplearning,open-ai,comments,2023-02-02 08:55:09,Any of you know a local and Open Source equivalent to Eleven Labs text to speech AI ?,lordnyrox,False,0.95,38,10rlbc4,https://www.reddit.com/r/deeplearning/comments/10rlbc4/any_of_you_know_a_local_and_open_source/,43,1675328109.0,
103,deeplearning,open-ai,comments,2023-04-05 19:44:10,AI vs Humans: Can You Tell the Difference?,YoutubeStruggle,False,0.67,4,12cvkvu,https://www.reddit.com/r/deeplearning/comments/12cvkvu/ai_vs_humans_can_you_tell_the_difference/,29,1680723850.0,"We would greatly appreciate your feedback on our[AI Content Detector](https://ai-content-detector.online/) that detects text generated by ChatGPT, a large language model trained by OpenAI. Our aim is to provide a reliable tool for distinguishing between human-written text and machine-generated text, and we would love to hear your thoughts on how effective the tool is in achieving this goal. Specifically, we would like to know if you found the site easy to navigate if the results provided were accurate, and if there are any additional features you would like to see implemented. Your feedback will help us to continue improving the site and provide the best possible experience for our users. Thank you in advance for your valuable input!"
104,deeplearning,open-ai,comments,2023-11-23 12:13:26,OpenAI Q* Rumours,MIKOLAJslippers,False,0.59,5,181zwb8,https://www.reddit.com/r/deeplearning/comments/181zwb8/openai_q_rumours/,30,1700741606.0,Anyone know any juicy rumours about the capabilities of this internal Q* project at OpenAI that has supposedly catalysed some of the recent dramatics?
105,deeplearning,open-ai,comments,2019-02-18 23:13:02,The Power of AI Generated Stories,00hello,False,0.81,39,as3f4f,https://www.reddit.com/r/deeplearning/comments/as3f4f/the_power_of_ai_generated_stories/,24,1550531582.0,"For the past 3 years, I've made a modest income generating genre fiction novels using deep learning and publishing them. By A/B testing, constant iteration and moving fast using many different pen-names I've been able to discover and serve tiny niches a human author would have trouble even finding. Most of the credit goes to a large and painstakingly annotated data set (which oddly enough, occurred to me just a few hours after my father died).  I'm continuously in awe of how powerful the ability to tell people a fictional story about the world is but more alarmingly, that often times the only difference between my books and many books I see under ""non-fiction"" is the category we each selected in the drop down menu.

&#x200B;

No matter what your opinion is on Open AI's decision to restrict their model,  this technology has much more profound and dangerous implications than most people realize. Whether you want people to build a pyramid, believe in Jesus or buy a stock, stories are how you program people and cultures. Yuval Noah Harari makes a good case in his books that our ability to share and collectively believe in fictional stories is what made us the dominant species on the planet.

&#x200B;

That being said, I now have a 240 GB training set of over 2.7 million narratives from fiction and non-fiction, about 85-90% English, each with very structuralized meta data, including the names of the central people in the narrative, directional graphs about their relationships, tags of their behavioural traits, tags of narrative themes, outcomes, points of view etc. etc. I have more data than my AI skills or my computational resources can effectively utilize. If there's anyone here with a very strong DL and NLP background who I can partner with to get access to the resources needed to train on my entire data set, please let me know.

&#x200B;

Edit: Just so there's no ambiguity, No I did not generate the 2.7 million narratives in the data set. That's a much, much larger version of the original data set I trained the first model with 3 years ago. That's what this whole post is about. I intend to train a brand new model with that."
106,deeplearning,open-ai,comments,2023-12-16 15:22:29,Is there any alternative for OpenAI API?,CrazyProgramm,False,0.91,26,18jtffj,https://www.reddit.com/r/deeplearning/comments/18jtffj/is_there_any_alternative_for_openai_api/,24,1702740149.0, So I am from Sri Lanka and our university is going to organize a competition and we need OpenAI API for it but we don't have money to afford it. Is there any alternative API you guys know 
107,deeplearning,open-ai,comments,2023-03-03 21:08:20,Meta’s LLaMa weights leaked on torrent... and the best thing about it is someone put up a PR to replace the google form in the repo with it 😂,RandomForests92,False,0.99,185,11hezvk,https://i.redd.it/olnsv438alla1.jpg,23,1677877700.0,
108,deeplearning,open-ai,comments,2023-04-05 04:41:38,Universities for masters,IshanDandekar,False,1.0,5,12c8m14,https://www.reddit.com/r/deeplearning/comments/12c8m14/universities_for_masters/,21,1680669698.0,"Hello people, I am in my end of 3rd of degree graduate program (Bachelors in Data Science). Now that I am near the end, I have started to think about further studies and masters. I live in India. My relatives and elders told that there are better opportunities outside. I have started to prepare for the GRE exam, but I am clueless about the universities that it'll offer me.

I am interested in Artificial Intelligence  rather than the business analytics part of Data Science. I have decided to go for masters, rather than looking for jobs after my graduation. Please suggest good universities that are good for masters in AI. Doesn't matter which country, I am first trying to look for universities and then filter according to countries.

Edit: I know many people will question that if I have a data science degree then why go for masters in AI. I know I will have to learn everything again, I am hoping it'll open a better job market for me."
109,deeplearning,open-ai,comments,2023-04-05 01:36:40,Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT,Time_Key8052,False,0.95,85,12c43uu,https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/,19,1680658600.0,"Vicuna : ChatGPT Alternative, Open-Source, High Quality and Low Cost 

&#x200B;

[ Relative Response Quality Assessed by GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a072b91)

Vicuna-13B has demonstrated competitive performance against other open-source models, such as Stanford Alpaca, by fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.

Evaluation using GPT-4 as a judge shows that Vicuna-13B achieves more than 90% of the quality of OpenAI ChatGPT and Google Bard AI, while outperforming other models such as Meta LLaMA (Large Language Model Meta AI) and Stanford Alpaca in more than 90% of cases.

The cost of training Vicuna-13B is approximately $300.

The training and serving code, along with an online demo, are publicly available for non-commercial use.

&#x200B;

More Information : [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT](https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)

Discord Server : [https://discord.gg/h6kCZb72G7](https://discord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysorg](https://twitter.com/lmsysorg)"
110,deeplearning,open-ai,comments,2023-07-10 20:23:30,How to rent GPU's for a few hours a day without paying too much?,FelipeReigosa,False,0.88,25,14w56ae,https://www.reddit.com/r/deeplearning/comments/14w56ae/how_to_rent_gpus_for_a_few_hours_a_day_without/,17,1689020610.0,"I'm not sure if this is the right place to ask this, but I've started getting into AI, I've played a little with a few open source NeRF projects and stable diffusion but my gpu just isn't powerful enough to do more than the basics. I've tried using linode gpu instances and that's almost perfect, I create an instance and have ssh access to an ubuntu with a powerful gpu and the hourly rate is pretty good at $1.5. The problem is that I can't persist the system state (packages installed, data downloaded etc) in a easy way except downloading the disk image which would take too long and be impractical. And if I let the instance running even when I'm not using it it's $1000 a month which is unacceptable for me, I just want it a few hours a day. Am I missing something, is there a way to save the linode state in a way that's practical? If not is there another service that offers this? (ssh access to an instance on an hourly basis and permanent storage of the stopped system state for a reasonable amount per month). What do you guys use?"
111,deeplearning,open-ai,comments,2023-12-24 18:24:47,Q*,sunnymorgue,False,0.1,0,18q0qc1,https://www.reddit.com/r/deeplearning/comments/18q0qc1/q/,17,1703442287.0,"Got a question, a lot of people on this board dumb down OpenAI's Q\* to just reinforcement learning; if that's the case then why did Sam get outed of OpenAI temporarily? If it were just RL then there would be no reason to fire him over some insignificant thing. Making premature remarks like that is pretty crazy if you ask me, just saying. "
112,deeplearning,open-ai,comments,2023-01-27 10:45:48,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.95,117,10mhyek,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
113,deeplearning,open-ai,comments,2023-03-29 14:13:46,AI Startup Cerebras releases open source ChatGPT-like alternative models,Time_Key8052,False,0.96,50,125pbbf,https://gpt4chatgpt.tistory.com/entry/Cerebras-releases-open-source-ChatGPT-like-alternative-models,14,1680099226.0,
114,deeplearning,open-ai,comments,2019-05-15 23:29:08,Where to learn neural network architecture design,DongDilly,False,0.98,31,bp5931,https://www.reddit.com/r/deeplearning/comments/bp5931/where_to_learn_neural_network_architecture_design/,12,1557962948.0,"So people at Deep Mind and OpenAi comes up with great models that are really innovatively designed. Can someone please tell me how and where I can learn the skill to design a neural network for a specific problem.
 Thank you"
115,deeplearning,open-ai,comments,2023-01-07 15:11:47,Review Request: MS in AI Grad Student with 3+ years of relevant experience trying to apply for Summer Internships '23 (posting here because I need domain-specific feedback),animikhaich,False,0.73,24,105r933,https://i.redd.it/0g3k2udk0naa1.jpg,12,1673104307.0,
116,deeplearning,open-ai,comments,2022-06-07 06:12:32,Looking for help for hire,Ok_Wish4469,False,1.0,19,v6o5n2,https://www.reddit.com/r/deeplearning/comments/v6o5n2/looking_for_help_for_hire/,8,1654582352.0," 

I'm both a collector and a coin dealer. I look through tens of thousands of coins a week for rare dates, errors, etc. But as I get older, my eyes are not what they use to be. So it's getting somewhat difficult for me to see the key details on the coin. So I decided to make a setup that can look through coins for me. I've been greatly influenced by this machine that does everything I want, but I need something a lot smaller.

[https://youtu.be/k7okDtRRCcY](https://youtu.be/k7okDtRRCcY)

I do have a basic background in coding and how it works. But I have little experience with making an AI. I've watched many video tutorials and I now understand clearly how an AI learns. I think the best route is to use Python, TensorFlow, and open-cv. But I keep getting some kind of errors that have been a major roadblock for me.

If this is relevant. My company setup is a ryzen 9 5900x. 3080 gpu and has 64gb of ram.

I'm looking for someone who can guide me through installing and training an AI model. I will compensate for your time, either in money or in collectible coins. What I mean for collectable coins is good quality coins. Not those cheapy coins you pick up from gift shops. But actually pieces of history. I've got silver coins, I've got a ton of English coins from 1600s-1800s. You can check out my ebay store to get a idea of what I have to offer. [https://www.ebay.com/sch/uncommoncentscoins/m.html?\_nkw&\_armrs=1&\_ipg&\_from&LH\_Complete=1&LH\_Sold=1&rt=nc&\_trksid=p2046732.m1684](https://www.ebay.com/sch/uncommoncentscoins/m.html?_nkw&_armrs=1&_ipg&_from&LH_Complete=1&LH_Sold=1&rt=nc&_trksid=p2046732.m1684)"
117,deeplearning,open-ai,comments,2021-02-18 14:52:00,The world's largest scale Turing Test / Do you think OpenAI's GPT3 is good enough to pass the Turing Test?,theaicore,False,0.93,34,lmog2d,https://www.theaicore.com/imitationgame?utm_source=reddit,11,1613659920.0,
118,deeplearning,open-ai,comments,2023-01-19 07:55:49,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.89,68,10fw22o,https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/,11,1674114949.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=webp&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
119,deeplearning,open-ai,comments,2024-02-13 14:38:25,"This is my foundation block or sandwich stack AI model... Criticisms and inputs are welcome, formulated by openAI scholarGPT - thanks in advance.",AskACapperDOTcom,False,0.25,0,1apuyv4,/r/OpenAI/comments/1apf8jy/this_is_my_foundation_block_or_sandwich_stack_ai/,10,1707835105.0,
120,deeplearning,open-ai,comments,2022-11-03 23:55:15,BlogNLP: AI Writing Tool,britdev,False,1.0,36,ylj1ux,https://www.reddit.com/r/deeplearning/comments/ylj1ux/blognlp_ai_writing_tool/,9,1667519715.0,"Hey everyone,

I created this web app using Open AI's GPT-3 (Davinci model). The purpose here is to provide a free tool to allow people to generate blog content/outlines/headlines and help with writer's block. Will continue to improve it over time, but just a side project I figured would provide some value to you all. Hope you all enjoy and please share ❤️

[https://www.blognlp.com/](https://www.blognlp.com/)"
121,deeplearning,open-ai,comments,2021-06-14 06:34:33,"This Chinese Super Scale Intelligence Model, ‘Wu Dao 2.0’, Claims To Be Trained Using 1.75 Trillion Parameters, Surpassing All Prior Models to Achieve a New Breakthrough in Deep Learning",ai-lover,False,0.88,33,nzgkj3,https://www.reddit.com/r/deeplearning/comments/nzgkj3/this_chinese_super_scale_intelligence_model_wu/,9,1623652473.0,"Deep learning is one area of technology where ambitiousness has no barriers. According to a recent announcement by [The Beijing Academy of Artificial Intelligence (BAAI)](https://www.baai.ac.cn/), in China, yet another milestone has been achieved in the field with its “Wu Dao” AI system. The [GPT 3](https://www.marktechpost.com/2020/08/02/gpt-3-a-new-breakthrough-in-language-generator/) brought in new interest for all the AI researchers, the super scale pre training models. By this approach and making use of 175 billion parameters, it managed to achieve exceptional performance results across the natural language processing tasks (NLP). However, the lacking component is its inability to have any form of cognitive abilities or common sense. Therefore, despite the size, even these models cannot indulge in tasks such as open dialogues, visual reasoning, and so on. With Wu Dao, the researchers plan to address this issue. This is China’s first attempt at a home-grown super-scale intelligent model system. 

Article: [https://www.marktechpost.com/2021/06/13/this-chinese-super-scale-intelligence-model-wu-dao-2-0-claims-to-be-trained-using-1-75-trillion-parameters-surpassing-all-prior-models-to-achieve-a-new-breakthrough-in-deep-learning/](https://www.marktechpost.com/2021/06/13/this-chinese-super-scale-intelligence-model-wu-dao-2-0-claims-to-be-trained-using-1-75-trillion-parameters-surpassing-all-prior-models-to-achieve-a-new-breakthrough-in-deep-learning/?_ga=2.13897584.636390090.1623335762-488125022.1618729090)

Reference: [https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/](https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/)"
122,deeplearning,open-ai,comments,2023-06-12 12:52:19,Local and open-source equivalent to HeyGen Text-to-Speech (TTS) AI?,Dark_Cloud_Games,False,0.75,4,147ne32,https://www.reddit.com/r/deeplearning/comments/147ne32/local_and_opensource_equivalent_to_heygen/,8,1686574339.0,"Do any of you know a local and open-source equivalent to HeyGen Text-to-Speech (**TTS**) **AI**? Or even if they use any third-party API? Unlike Eleven Labs (limited to English), HeyGen's technology allows for multiple accents. "
123,deeplearning,open-ai,comments,2021-04-17 14:31:07,*Semantic* Video Search with OpenAI’s CLIP Neural Network (link in comments!),designer1one,False,0.99,66,msrsc4,https://i.redd.it/as278qmzuqt61.gif,9,1618669867.0,
124,deeplearning,open-ai,comments,2019-04-23 01:17:39,"Deep Learning Build, Need some Expertise :)",Atralb,False,0.83,4,bga5d0,https://www.reddit.com/r/deeplearning/comments/bga5d0/deep_learning_build_need_some_expertise/,8,1555982259.0,"Hi guys,

&#x200B;

So I'm planning to build my own computer for making deep learning experiments, but also want to be able to use it as a private server simultaneously, and be able to use it with browser, heavy code editor, etc... sometimes.  
For reference I want to make personal deep learning training experiments in music (and more generally sound) analysis and maybe a bit of video game AI applications experiments (not in relation with the music thing).

&#x200B;

It's my first time building a pc but I will have an experienced friend there to help me when the critical time comes.

&#x200B;

So I mainly followed this guide [https://medium.com/the-mission/how-to-build-the-perfect-deep-learning-computer-and-save-thousands-of-dollars-9ec3b2eb4ce2](https://medium.com/the-mission/how-to-build-the-perfect-deep-learning-computer-and-save-thousands-of-dollars-9ec3b2eb4ce2) in my research along with a bit of that one [https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/](https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/) and many others.

&#x200B;

I don't have very much money but I want to use in potential professional use and I'm in the post-graduation-first-job phase as an IA engineer and paid 1k2 for at least 6 months in an internship so I thought asking a 2k-advance on my parents was a good budget. High end powerful experiment while maintaining not exorbitant prices relatively to my financial means.

&#x200B;

&#x200B;

Therefore, with 2000EUR in mind then, I settle with this temporary build (sorry for the different language) :  
**- Power : Corsair HX1200i** [https://www.amazon.fr/gp/product/B00S8HY0BW/ref=ox\_sc\_act\_title\_2?smid=A1X6FK5RDHNB96&psc=1](https://www.amazon.fr/gp/product/B00S8HY0BW/ref=ox_sc_act_title_2?smid=A1X6FK5RDHNB96&psc=1)

**-** **Motherboard : Gigabyte X399 Aorus Pro, AMD X399**  [https://www.amazon.fr/gp/product/B07KF7M46X/ref=ox\_sc\_act\_title\_3?smid=A1X6FK5RDHNB96&psc=1](https://www.amazon.fr/gp/product/B07KF7M46X/ref=ox_sc_act_title_3?smid=A1X6FK5RDHNB96&psc=1)  
**- Storage : Samsung SSD 970 EVO NVMe M.2 (1TB)- MZ-V7E1T0BW**  [https://www.amazon.fr/gp/product/B07CGJNLBB/ref=ox\_sc\_act\_title\_4?smid=A862111F3B7OV&psc=1](https://www.amazon.fr/gp/product/B07CGJNLBB/ref=ox_sc_act_title_4?smid=A862111F3B7OV&psc=1)  
**- CPU : AMD Ryzen 7 Threadripper 1920X**   [https://www.amazon.fr/gp/product/B074CBJHCT/ref=ox\_sc\_act\_title\_5?smid=A1X6FK5RDHNB96&psc=1](https://www.amazon.fr/gp/product/B074CBJHCT/ref=ox_sc_act_title_5?smid=A1X6FK5RDHNB96&psc=1)

**- GPU : A GTX 1080 ti** but got no amazon link, cause things move fast for it on amazon, so I give it time for when I see a good deal (you can see from 550 to 800)

So first I have a couple questions :

* **GPU :** So here my prime argument is getting the threshold of 10Gb of VRAM. Secondly, in the guide I provided, it is said that the performance (at least in deep learning) is proportional to the amount of CUDA cores, so I made calculation that in my country the 1080 GTX is the best deal. What do you think of these choices in relation to the practice I want to make of it ? Do you have any other thoughts regarding the GPU ? Maybe RTX 2070 or 2080 is better for the price ?
* **Case :** I got it was really important to take dimensions in account when looking for the case.  
The guy is suggesting this (150EUR 270\*465\*476mm) : [https://www.amazon.fr/Lian-Li-PC-O11AIR-Bo%C3%AEtier-pour/dp/B07FJ6PMKW/ref=sr\_1\_fkmrnull\_1?\_\_mk\_fr\_FR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&keywords=%3A+Lian-Li+PC-O11AIR&qid=1555175276&s=gateway&sr=8-1-fkmrnull](https://www.amazon.fr/Lian-Li-PC-O11AIR-Bo%C3%AEtier-pour/dp/B07FJ6PMKW/ref=sr_1_fkmrnull_1?__mk_fr_FR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&keywords=%3A+Lian-Li+PC-O11AIR&qid=1555175276&s=gateway&sr=8-1-fkmrnull)  
Considering the 1080 measures 370\*280\*114mm (for a Gigabyte) Do you think this won't go (50EUR 470\*201\*429mm) : [https://www.amazon.fr/dp/B00RORBQNW/ref=psdc\_430338031\_t1\_B00XPUFY0I](https://www.amazon.fr/dp/B00RORBQNW/ref=psdc_430338031_t1_B00XPUFY0I) ?
* **Fans :** Besides, considering the motherboard configuration (available here [https://c1.neweggimages.com/ProductImageCompressAll1280/13-145-109-V01.jpg](https://c1.neweggimages.com/ProductImageCompressAll1280/13-145-109-V01.jpg)), and considering the case dimension (470\*201\*429) and the GPU dimension and its placing on the motherboard, do you think a fan like this one would be able to fit in :  
Noctua NH-U9 TR4-SP3 [https://www.amazon.fr/gp/product/B074DXFB66/ref=ox\_sc\_saved\_title\_2?smid=A38F5RZ72I2JQ&psc=1](https://www.amazon.fr/gp/product/B074DXFB66/ref=ox_sc_saved_title_2?smid=A38F5RZ72I2JQ&psc=1) ?  
We've done a bit of calculation and considering where the different components will be placed on the motherboard, and, while it was very approximate, we found it would be at least a bit difficult.
* **CPU :** I've done my research mostly last weekend so I don't remember everything but I know I settled for the 1920X cause of the numerous cores and because I kinda believed (but really vaguely) it was better for the price than Intel.But I have since heard that unless using 4 GPUs it was not necessary putting 400EUR for a CPU and maybe taking a 200-300 one and putting the rest of the money in a better gpu was not a bad idea. What is your take on this ?
* **Network Card :** For experienced Deep learning engineers do you think going for a 10Gb Network card like this [https://www.amazon.fr/gp/product/B071JR2ZW8/ref=ox\_sc\_saved\_title\_3?smid=A1X6FK5RDHNB96&psc=1](https://www.amazon.fr/gp/product/B071JR2ZW8/ref=ox_sc_saved_title_3?smid=A1X6FK5RDHNB96&psc=1) a good idea or not really useful ?I currently have 800Mb/s down rates with my laptop in Ethernet, and I don't see myself having to download Terabytes of data so I thought it was more of a gadget than anything, but I perfectly may be wrong.
* **Power :** Is 1200W too much with that build (if the guy's calcuations are right and I understood well, I've done that : GPU 250W (500W for future second card) + 180 W CPU + 150W = 830W => 1000W and the 1200W is only 25EUR more) ?Is it even overkill to have more than what I need since it wil apparently consume more for nothing and so inflate the electricity bill ?
* **RAM :** Are 16Gb enough for this (10Gb model training + 3 Gb for smth else +  3 System (maybe it's a bit short) ?

&#x200B;

&#x200B;

I think that's all, but I may have forgotten something (you know 420 is just 3 days ago :p), I will maybe add something later. By the way, I'm still open to anything else you might think and will be glad to read you.

Thanks a lot in advance for your help !"
125,deeplearning,open-ai,comments,2020-08-05 17:45:51,[N] YogaDL: a better approach to data loading for deep learning models,neilc,False,0.8,8,i49zqz,https://www.reddit.com/r/deeplearning/comments/i49zqz/n_yogadl_a_better_approach_to_data_loading_for/,8,1596649551.0,"[YogaDL](https://determined.ai/blog/yogadl-announcement/) is a new approach to data loading for deep learning models. It is essentially a caching layer that wraps your existing data loading code and provides random access to the data set in a high-performance way, which enables efficient data shuffling, sharding, and checkpoint/restart. We were inspired to build Yoga in part by the [challenges we encountered using tf.data](https://determined.ai/blog/tf-dataset-the-bad-parts/) to accomplish similar tasks.

YogaDL currently supports tf.data as an input API, and supports caching data sets on local storage, AWS, and GCS. Support for more input APIs and more storage types is on the roadmap. YogaDL is open source under the Apache 2.0 license. YogaDL is brought to you by the same team that is building the [Determined](https://github.com/determined-ai/determined) deep learning training platform, but it can be used outside of Determined.

For more, check out the [announcement blog post](https://determined.ai/blog/yogadl-announcement/), the [documentation](https://yogadl.readthedocs.io/en/latest/yogadl.html), or [GitHub](https://github.com/determined-ai/yogadl)."
126,deeplearning,open-ai,comments,2023-04-02 18:10:37,Should we draw inspiration from Deep learning/Computer vision world for fine-tuning LLMs?,Vegetable-Skill-9700,False,0.79,13,129t3tl,https://www.reddit.com/r/deeplearning/comments/129t3tl/should_we_draw_inspiration_from_deep/,8,1680459037.0,"With HuggingGPT, BloombergGPT, and OpenAI's chatGPT store, it looks like the world is moving towards specialized GPTs for specialized tasks. What do you think are the best tips & tricks when it comes to fine-tuning and refining these task-specific GPTs?

Over the last decade, I have built many computer vision models (for human pose estimation, action classification, etc.), and our general approach was always based on Transfer learning. Take a state-of-the-art public model and fine-tune it by collecting data for the given use case.

Do you think that paradigm still holds true for LLMs?

Based on my experience, I believe observing the model's performance as it interacts with real-world data, identifying failure cases (where the model's outputs are wrong), and using them to create a high-quality retraining dataset will be the key.

&#x200B;

P.S. I am building an open-source project UpTrain ([https://github.com/uptrain-ai/uptrain](https://github.com/uptrain-ai/uptrain)), which helps data scientists to do so. We just wrote a blog on how this principle can be applied to fine-tune an LLM for a conversation summarization task. Check it out here: [https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation\_summarization](https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation_summarization)"
127,deeplearning,open-ai,comments,2017-11-11 17:08:56,Setting mean and std of REWARDS in reinforcement learning - a question,closedloopy,False,1.0,4,7c9k9y,https://www.reddit.com/r/deeplearning/comments/7c9k9y/setting_mean_and_std_of_rewards_in_reinforcement/,8,1510420136.0,"In the great post [pong to pixels](http://karpathy.github.io/2016/05/31/rl/) by Karpathy, and more explicitly in his code [here](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5#file-pg-pong-py-L108) we see that he sets the mean of the rewards to 0 and the standard deviation to 1. This confuses me because that means that half of the rewards will be greater than zero, and the other less than zero. Now, lets assume this array of rewards came from an episode that we liked (good performance) then we'd want to reinforce the behavior. But as far as I can tell half of the actions will be associated with positive reward (and thus encouraged) and the other half with negative reward (and thus discourage). 


*Can anyone help me get a better intuition about why he does this? An example by Pytorch follows:*


PyTorch has in [their demo](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py) a solution of solving the [cart pole](https://gym.openai.com/envs/CartPole-v0/) from open ai gym, and the solution does the same thing in terms of modifying the rewards:

First we have the raw rewards (all ones. The longer the pole stays balanced, the more reward we get):

    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
      1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
      1.,  1.,  1.,  1.,  1.]

Then we [discount](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L62) them:

    [26.76966303456023, 26.02996266117195, 25.282790566840355, 24.528071279636723, 23.76572856528962, 
    22.995685419484467, 22.21786406008532, 21.4321859192781, 20.638571635634445, 19.8369410460954, 
    19.027213177874142, 18.209306240276913, 17.383137616441328, 16.54862385499124, 
    15.705680661607312, 14.854222890512437, 13.994164535871148, 13.12541872310217, 
    12.247897700103202, 11.361512828387072, 10.466174574128356, 9.561792499119552, 8.64827525163591, 
    7.72553055720799, 6.793465209301, 5.8519850599, 4.90099501, 3.9403989999999998, 2.9701, 1.99, 1.0]

Then finally we apply the [mean of 0 and std of 1](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py#L65):

     [1.6023, 1.5001, 1.3965, 1.2929, 1.1897,
     1.0875, 0.9859, 0.8842, 0.7819, 0.6796,
     0.5768, 0.4728, 0.3672, 0.2595, 0.1500,
     0.0383, -0.0761, -0.1939, -0.3137, -0.4315,
    -0.5477, -0.6630, -0.7778, -0.8916, -1.0049
    -1.1172, -1.2289, -1.3396, -1.4498, -1.5600, -1.6695]

To be clear, I've seen [this post](http://cs231n.github.io/neural-networks-2/) where we learn about data pre-processing and the value of updating mean and std of INPUT, but that is on the INPUT DATA, not the REWARDS. My question is why we would apply this transformation to the REWARDS.

Thank you!"
128,deeplearning,open-ai,comments,2023-12-30 10:35:16,"Questions regarding LLM project, using RAG",curiKINGous,False,0.63,2,18ucvjf,https://www.reddit.com/r/deeplearning/comments/18ucvjf/questions_regarding_llm_project_using_rag/,8,1703932516.0,"\- I have to make a llm project using rag. Basically a chat bot where i ll provide document and it will answer prompt. I will be using lang chain. 

\- I wanted to ask, do i have to purchase open AI API? my teacher told to purchase open ai api, but I would like to find other ways. 

\- can anyone share any documentation / site / tutoiral helpful for what Iam aiming to build. "
129,deeplearning,open-ai,comments,2022-10-18 16:52:15,Fully automated video generation - connecting OpenAI's Whisper with Stable Diffusion. Tutorial & code coming soon!,hayAbhay,False,1.0,12,y7cbf4,https://youtu.be/xRcoeUgD4GY,8,1666111935.0,
130,deeplearning,open-ai,comments,2021-07-12 00:39:45,"Adding guassian noise to Discriminator layers in GAN helps really stablizing training, generates sharper images and avoid mode collapse.",0x00groot,False,0.98,52,oigdgg,https://www.reddit.com/r/deeplearning/comments/oigdgg/adding_guassian_noise_to_discriminator_layers_in/,8,1626050385.0,"Very simple tweak which isn't usually seen in basic GAN tutorials. Might be helpful if you are new to GANs and it's just not converging.

[256x256px, results in 3 hours on colab.](https://preview.redd.it/zum8fg2xgoa71.png?width=901&format=png&auto=webp&s=64dd09e02f7c7418a698eab274026d273175211c)

I am also new to GANs and just learning. I first noticed this when learning about GANs last year in tensorflow. I followed the most basic tutorial from tf docs. But results were always smudgy, fuzzy and not convincing, and easily collapsing, especially at resolutions >= 128x128. But adding Gaussian noise to each layer of Discriminator dramatically made the results much better. Inspiration was from some ganhacks and papers adding noise to just the input or generator, but haven't seen results for discriminator.

Found similar results when implementing the same in Pytorch recently. Models with same architecture, config and seed. Only difference is adding of guassian noise to discriminator layers gives much better results. Have had success in training 128x128 and 256x256 face generation in just a few hours on colab.

Below are few results. Using very basic convolutional gan architecture.

[128x128 Results with guassian noise in discriminator layers on celeba](https://preview.redd.it/w6jmzssvgoa71.png?width=1782&format=png&auto=webp&s=94158edc0cf52b6dbe82eae176e49e3334d9fb82)

[128x128 Results without noise on celeba, easily collapsed.](https://preview.redd.it/uqsr8uvugoa71.png?width=1785&format=png&auto=webp&s=ce3dcdcece1893dec277e31eac1eb7734cd7da0d)

Also results on Flickr dataset 256x256 resolution in 3 hours.

[256x256 Flickr dataset with guassian noise in discriminator layers](https://preview.redd.it/vd72llwsgoa71.png?width=1782&format=png&auto=webp&s=8ce6240bf7b8e50e86ad81cc6af5a3e6d2400f99)

Ofcourse results aren't too crazy and still contain artifacts as this is a very basic architecture and trained for a short time. But not bad and much better as compared to without gaussian noise in discriminator.

More results runs and logs of runs with no noise, noise decay, adding noise to only generator layers, adding noise only to input, both generator and discriminator can be found here. Open different runs to see more outputs at different timesteps.: [https://wandb.ai/shivamshrirao/facegan\_pytorch](https://wandb.ai/shivamshrirao/facegan_pytorch)

View more 256px results [here](https://wandb.ai/shivamshrirao/facegan_pytorch/runs/1424g5hk).

Pytorch code: [https://github.com/ShivamShrirao/facegan\_pytorch](https://github.com/ShivamShrirao/facegan_pytorch)

Tensorflow code (bit old): [https://github.com/ShivamShrirao/GANs\_TF\_2.0](https://github.com/ShivamShrirao/GANs_TF_2.0)

[256x256px Flickr dataset](https://preview.redd.it/m8hxbzrqgoa71.png?width=901&format=png&auto=webp&s=24a4abde9496b96a93b7abf85f739d87335229fa)"
131,deeplearning,open-ai,comments,2023-05-17 02:26:44,OpenAI CEO asking for government's license for building AI . WHAT THE ACTUAL FUCK?,Angry_Grandpa_,False,0.83,24,13joq2b,/r/singularity/comments/13jbc76/openai_ceo_asking_for_governments_license_for/,8,1684290404.0,
132,deeplearning,open-ai,comments,2022-12-07 00:33:41,Are currently state of art model for logical/common-sense reasoning all based on NLP(LLM)?,Accomplished-Bill-45,False,0.94,21,zen8l4,https://www.reddit.com/r/deeplearning/comments/zen8l4/are_currently_state_of_art_model_for/,6,1670373221.0,"Not very familiar with NLP, but I'm playing around with OpenAI's ChatGPT; particularly impressed by its reasoning, and its thought-process.

Are all good reasoning models derived from NLP (LLM) models with RL training method at the moment?

What are some papers/research team to read/follow to understand this area better and stay on updated?

&#x200B;

&#x200B;

for ChatGPT. I've tested it with following cases

Social reasoning ( which does a good job; such as: if I'm going to attend meeting tonight. I have a suit, but its dirty and size doesn't fit. another option is just wear underwear, the underwear is clean and fit in size. Which one should I wear to attend the meeting. )

Psychological reasoning ( it did a bad job.I asked it to infer someone's intention given his behaviours, expression, talks etc.)

Solving math question ( it’s ok, better then Minerva)

Asking LSAT logic game questions ( it gives its thought process, but failed to give correct answers)

I also wrote up a short mystery novel, ( like 200 words, with context) ask if it can tell is the victim is murdered or committed suicide; if its murdered, does victim knows the killer etc. It actually did ok job on this one if the context is clearly given that everyone can deduce some conclusion using common sense."
133,deeplearning,open-ai,comments,2022-10-03 18:57:54,Use YOLOv5 tensorflow.js models to speed up annotation,RandomForests92,False,1.0,20,xus40c,https://www.reddit.com/r/deeplearning/comments/xus40c/use_yolov5_tensorflowjs_models_to_speed_up/,7,1664823474.0,"Hi everyone! I'm Piotr and for several years I have been developing a small open-source project for labeling photos - [makesense.ai](https://makesense.ai/). I added a new feature this weekend. You can use \[YOLOv5\]([https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5)) models to automatically annotate photos. You can choose one of the models pre-trained on the COCO dataset, but most importantly you can load your own custom models. Just drag and drop the tensorflow.js model to the editor and you are good to go. Everything runs in the browser - no backend, so it is completely free. Let me know what you think! I'm super excited about that project.

By the way, I have created an NPM package, which can also make it easier for you to deploy YOLOv5 in the browser. [https://github.com/SkalskiP/yolov5js](https://github.com/SkalskiP/yolov5js)"
134,deeplearning,open-ai,comments,2023-11-16 08:28:46,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,0.84,68,17whz6e,https://v.redd.it/qx68wbuf7o0c1,7,1700123326.0,
135,deeplearning,open-ai,comments,2022-12-03 19:29:01,BlogNLP: AI Blog Writing Tool,britdev,False,0.88,12,zboc8w,https://www.reddit.com/r/deeplearning/comments/zboc8w/blognlp_ai_blog_writing_tool/,7,1670095741.0,"Hey everyone,

I developed this web app with Open AI's GPT-3 to provide a free, helpful resource for generating blog content, outlines, and more - so you can beat writer's block! I'm sure you'll find it useful and I'd really appreciate it if you shared it with others ❤️.

[https://www.blognlp.com/](https://www.blognlp.com/)"
136,deeplearning,open-ai,comments,2023-12-22 00:15:45,"I'm torn between RL and CV, which one is better for finding research position at big companies?",Trevorego,False,0.77,12,18o1kme,https://www.reddit.com/r/deeplearning/comments/18o1kme/im_torn_between_rl_and_cv_which_one_is_better_for/,7,1703204145.0,"I'm learning ML/DL and now I need to choose a path to follow and specialize in. I like both RL & CV, and luckily I have the opportunity to work with both subjects with my professors. However, I'm just clueless which one to learn, or maybe I should say learn first because very likely I'll also learn and required to work with the other one in future, but for now I can't decide. They both look very interesting, both looks challenging, both looks fun. Thus, now I consider which one would be a better choice for finding a research position at a big company like OpenAI, Google, etc. I'm open to any advise and really need one."
137,deeplearning,open-ai,comments,2022-01-19 02:30:37,Sharing some deep learning tools that I have been working on!,Baddoby,False,1.0,8,s7f3xm,https://www.reddit.com/r/deeplearning/comments/s7f3xm/sharing_some_deep_learning_tools_that_i_have_been/,6,1642559437.0,"Hey All, I am so excited to share the hard work of almost two years. We have built an AI platform ([https://deque.app](https://deque.app/)) that includes tools such as:

1. Notebook with live coediting & experiment tracking built in using TensorBoard or MLFlow. No installation needed and all the data is always preserved. Notebooks also use OpenAI’s Cortex for code generation.
2. Large scale training jobs with built in support for Multi GPU and Multi Node
3. Endpoints - deploy models with few lines of code with elastic scaling
4. Drive (auto mounted) all completely managed within a beautifully designed app. You can even do Multi-Node distributed training with single click.

In terms of compute infrastructure, we offer a minimum of 10% discount on AWS hourly rate and support SPOT based training. You are also welcome to bring your own GCP or AWS credentials.

If you like to try, feel free to sign up or email me at [team@deque.app](mailto:team@deque.app). :-)

Cheers & Thanks!"
138,deeplearning,open-ai,comments,2020-04-24 00:46:30,"Who Invented Backpropagation? Hinton Says He Didn’t, but His Work Made It Popular",Yuqing7,False,0.86,32,g6yq11,https://www.reddit.com/r/deeplearning/comments/g6yq11/who_invented_backpropagation_hinton_says_he_didnt/,6,1587689190.0,"One might think that news of the 2019 Honda Prize being awarded to Dr. Geoffrey Hinton “for his pioneering research in the field of deep learning in artificial intelligence (AI)” would prompt the machine learning community to toast the man they call the “Godfather of Deep Learning.” Instead, the gloves came off and what ensued was an unexpected Internet dust-up.

Jürgen Schmidhuber started it. In a blog[ post](http://people.idsia.ch/~juergen/critique-honda-prize-hinton.html#I), the Scientific Director of The Swiss AI Lab IDSI called out the Honda Prize for crediting Hinton with inventing backpropagation, among other things. Schmidhuber argued that “**Hinton has made significant contributions to artificial neural networks (NNs) and deep learning, but Honda credits him for fundamental inventions of others whom he did not cite.**”

Schmidhuber identified what he said were “six false and/or misleading attributions of credit to Dr. Hinton” in the press release. “I’ll point out,” he wrote, “that Hinton’s most visible publications failed to mention essential relevant prior work — this may explain some of Honda’s misattributions.”

The 6,300 word document, ***Critique of Honda Prize for Dr. Hinton*****, was published on Tuesday on the The Swiss AI Lab IDSIA (Istituto Dalle Molle di Studi sull’Intelligenza Artificiale) website. The opening line reads: “We must stop crediting the wrong people for inventions made by others.”**

Today, Hinton, University Professor Emeritus at the University of Toronto, responded on [Reddit](https://www.reddit.com/r/MachineLearning/comments/g5ali0/d_schmidhuber_critique_of_honda_prize_for_dr/fo8rew9?utm_source=share&utm_medium=web2x), “**I have never claimed that I invented backpropagation.** David Rumelhart invented it independently long after people in other fields had invented it. It is true that when we first published we did not know the history so there were previous inventors that we failed to cite. **What I have claimed is that I was the person to clearly demonstrate that backpropagation could learn interesting internal representations and that this is what made it popular.”**

Read more: [Who Invented Backpropagation? Hinton Says He Didn’t, but His Work Made It Popular](https://medium.com/syncedreview/who-invented-backpropagation-hinton-says-he-didnt-but-his-work-made-it-popular-e0854504d6d1)"
139,deeplearning,open-ai,comments,2020-07-14 13:10:30,Kickstarter Campaign for OpenCV AI Kit (OAK),spmallick,False,0.82,7,hr184y,https://www.reddit.com/r/deeplearning/comments/hr184y/kickstarter_campaign_for_opencv_ai_kit_oak/,6,1594732230.0," The Kickstarter Campaign for OpenCV AI Kit (OAK) goes live on July 14, 9 AM Eastern Time.  


[https://www.kickstarter.com/projects/opencv/opencv-ai-kit](https://www.kickstarter.com/projects/opencv/opencv-ai-kit)  


What is OAK?  


OpenCV AI Kit (OAK) is a smart camera based on Intel® Myriad X™. There are two variants of OAK.  


OAK-1 is a single camera solution that can do neural inference (image classification, object detection, segmentation and a lot more) on the device.  


OAK-D is our Spatial AI solution. It comes with a stereo camera in addition to the standard RGB camera.  


We have come up with super attractive pricing. The early bird prices are limited to 200 smart cameras of each kind.   


OAK-1 : $79 \[Early Bird Price\] and $99 \[Kickstarter Price\]  
OAK-D : $129 \[Early Bird Price\] and $149 \[Kickstarter Price\]  


For the price of a webcam, you can buy a smart camera that can not only do neural inference on the device, it can also do depth estimation in real time.  


It is not only a good solution for companies wanting to build an industrial smart camera, it is also an excellent platform for students, programmers, engineers and hobbyists to get a taste of Spatial AI and Edge AI.  


The two cameras will come with excellent software support.

https://preview.redd.it/liz6xcf5ota51.jpg?width=1024&format=pjpg&auto=webp&s=e7d0086ebde200333a9bbac09deeedaaf1411ba9"
140,deeplearning,open-ai,comments,2022-09-07 21:39:59,Open Source/free alternative to Topaz Labs Video Enhance AI - Video Enhance AI,lordnyrox,False,0.79,5,x8hd95,https://www.reddit.com/r/deeplearning/comments/x8hd95/open_sourcefree_alternative_to_topaz_labs_video/,5,1662586799.0,"Hi, I'm looking for a free/open source alternative to
Topaz Labs Video Enhance AI - Video Enhance AI"
141,deeplearning,open-ai,comments,2023-05-03 09:28:44,"[D] [P] Need help in my Thesis project ""A comparison study of EEG analysis by Deep Learning vs Expert board cerrtified Neurologist analysis for 100 patient data",drajaytripathi,False,0.71,3,136fkpu,https://www.reddit.com/r/deeplearning/comments/136fkpu/d_p_need_help_in_my_thesis_project_a_comparison/,6,1683106124.0,"Hi

 

I am a Doctor /Physician from india, currently doing residency in Neurology superspeciality from a hospital in India

I am in stage of planning for a comparative study between Deep Learning AI solution for EEG analysis vs Expert Neurologist Analysis reporting, that shall be part of my Thesis and will be published as a paper afterwords.

We will take data of approx 100 patients who are advised for EEG

In our setup, **""Clarity software""** is used for EEG and file extension produced is .eeg

Please help me in suggesting Open source solutions that can be used in this study.

Till now i have found only 1 open source model **(aka BRAINCODE**) that can be used (I will try to make a setup and analyse its feasability , it looks like it can be used as far as i can understnad from its Github reprository ([https://github.com/braindecode/braindecode/](https://github.com/braindecode/braindecode/))

another Private company **BITBRAIN,** also has similar solution

Also i will need a way to convert .EEG extension files ([https://filext.com/file-extension/EEG](https://filext.com/file-extension/EEG)) to convert to any needed format for the model

PLease help me in this reserch work"
142,deeplearning,open-ai,comments,2021-07-06 16:56:22,What OpenAI and GitHub’s “AI pair programmer” means for the software industry,bendee983,False,0.92,30,oez127,https://bdtechtalks.com/2021/07/05/openai-github-gpt-3-copilot/,6,1625590582.0,
143,deeplearning,open-ai,comments,2020-05-22 15:23:28,Open AI and Microsoft Can Generate Python Code,cmillionaire9,False,0.89,67,golbq4,https://youtu.be/y5-wzgIySb4,6,1590161008.0,
144,deeplearning,open-ai,comments,2020-11-09 12:04:21,This AI Predicts if You Have Covd-19 from Your Cough [Paper Analysis],diabulusInMusica,False,0.82,26,jqwf4r,https://www.reddit.com/r/deeplearning/comments/jqwf4r/this_ai_predicts_if_you_have_covd19_from_your/,6,1604923461.0,"Reliable and quick diagnostic tools are fundamental to fight the Covid-19 pandemic 😷 😷 Researchers at MIT published a paper 🎓 🎓 introducing an AI-Audio based diagnostic system that produces accurate Covid-19 diagnoses. The diagnosis is carried out by analysing cough sounds.

In my new video, I break down the paper providing a high-level overview (useful for non-technical people) and a detailed technical account.  

The paper is titled “COVID-19 Artificial Intelligence Diagnosis using only Cough Recordings” and was published in the IEEE Open Journal of Engineering in Medicine and Biology in September 2020.

Here’s the video:

[https://www.youtube.com/watch?v=Skzuva3chIM&list=PL-wATfeyAMNoxL33ZF2TRgq9AuDAynYTx&index=2](https://www.youtube.com/watch?v=Skzuva3chIM&list=PL-wATfeyAMNoxL33ZF2TRgq9AuDAynYTx&index=2)"
145,deeplearning,open-ai,comments,2023-02-20 21:27:58,Fine tuning a GPT for text generation,nashcaps2724,False,0.9,8,117l2vf,https://www.reddit.com/r/deeplearning/comments/117l2vf/fine_tuning_a_gpt_for_text_generation/,6,1676928478.0,"Hi all, let me lay out my problem…

Imagine there are two corpora, Corpus A (100,000~) and Corpus B (20,000,000~). 

Individuals create reports for corpus A based on the information in corpus B. 

My idea was to pretrain a GPT on corpus A, and fine tune it to take documents from corpus B as an input, and output text in the style of corpus A (essentially a mix of text generation and summarization). 

Is this something folks think is even feasible? Should I be pretaining the GPT on both corpora or just corpus A? I thought of both fine tuning an OpenAI GPT and training from scratch. 

Any advice would be welcome!"
146,deeplearning,open-ai,comments,2023-07-29 20:02:45,"Promptify 2.0: More Structured, More Powerful LLMs with Prompt-Optimization, Prompt-Engineering, and Structured Json Parsing with GPT-n Models! 🚀",StoicBatman,False,0.63,2,15d1fs8,https://www.reddit.com/r/deeplearning/comments/15d1fs8/promptify_20_more_structured_more_powerful_llms/,6,1690660965.0,"Hello fellow coders and AI enthusiasts!

First up, a huge Thank You for making Promptify a hit with **over** [**2.3k+ stars on Github**](https://github.com/promptslab/Promptify) ! 🌟

Back in 2022, we were the first one to tackle the common challenge of uncontrolled, unstructured outputs from large language models like GPT-3. , and your support has pushed us to keep improving.Today, we're thrilled to share some major updates that make Promptify even more powerful

&#x200B;

https://preview.redd.it/29ajik9xmyeb1.png?width=1510&format=png&auto=webp&s=3c3bfeebd6ba5e878885b079510a8972cc72c3b8

&#x200B;

* **Unified Architecture 🧭**: Introducing Prompter, Model & Pipeline Solution
* **Detailed Output Logs 📔**: Comprehensive structured JSON format output within the log folder.
* **Wider Model Support 🤝**: Supporting models from OpenAI, Azure, Cohere, Anthropic, Huggingface and more - think of it as your universal language model adapter.
* **Robust Parser 🦸‍♂️**: Parser to handle incomplete or unstructured JSON outputs from any LLMs.
* **Ready-Made Jinja Templates 📝**: Jinja prompt templates for NER, Text Classification, QA, Relation-Extraction, Tabular data, etc.
* **Database Integration 🔗**: Soon, Promptify directly to Mongodb integration. Stay tuned!
* **Effortless Embedding Generation 🧬**: Generate embeddings from various LLMs effortlessly with the new update.

&#x200B;

https://preview.redd.it/k50gmbxymyeb1.png?width=2160&format=png&auto=webp&s=ef063a7a0594eccac5674bd60d7adce193eecc3f

Check out the examples and take Promptify for a spin on GitHub. If you like what you see, we'd be honored if you gave us a star!

* **Github**: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* **Colab:** [Try Now on Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)
* **Explore Other Cool Open Source LLM Tools:** [https://github.com/promptslab](https://github.com/promptslab)

Join **1.6k+ Promptify users on Discord** to dive deep into prompt engineering, discuss the latest with LLMs, and advance NLP research together: [https://discord.com/invite/m88xfYMbK6](https://discord.com/invite/m88xfYMbK6)Thank you again for your support - here's to more structured AI!

&#x200B;"
147,deeplearning,open-ai,comments,2023-02-01 15:20:25,Launching my first-ever open-source project and it might make your ChatGPT answers better,Vegetable-Skill-9700,False,0.75,2,10qx9po,https://www.reddit.com/r/deeplearning/comments/10qx9po/launching_my_firstever_opensource_project_and_it/,6,1675264825.0,"I am building UpTrain - an open-source ML diagnostic toolkit that recently got investment from YCombinator.

As you know no ML model is 100% accurate, and, further, their accuracy deteriorates over time 😣. Additionally, due to the black boxiness ⬛ nature of Large Language models, it's challenging to identify and fix their problems.

The tool helps ML practitioners to:
1. Understand how their models are performing in production
2. Catch edge cases and outliers to help them refine their models
3. Allow them to define custom monitors to catch under-performing data-points
4. Retrain the model on them to improve its accuracy

You can check out the project here: https://github.com/uptrain-ai/uptrain. Would love to hear feedback from the community!"
148,deeplearning,open-ai,comments,2023-01-10 14:36:18,TypeError: 'module' object is not callable,ContributionFun3037,False,0.5,0,108bbsj,https://www.reddit.com/r/deeplearning/comments/108bbsj/typeerror_module_object_is_not_callable/,6,1673361378.0,"I'm new to deep learning, and I'm currently trying to wrap my head over Reinforcement learning by using open ai gym(to train agents i.e).  

    import gymnasium as gym
    from stable_baselines3 import PPO
    from stable_baselines3.common.vec_env import dummy_vec_env
    from stable_baselines3.common.evaluation import evaluate_policy
    from stable_baselines3.ppo import MlpPolicy
    import os
    
    log_path=os.path.join('Training', 'Logs')
    
    env = gym.make(""CartPole-v1"", render_mode=""human"")
    env= dummy_vec_env([lambda:env])
    model= PPO(MlpPolicy, env, verbose=1, tensorboard_log=log_path)

I can see the cartpole window and after exiting I'm getting this error and I don't know why.

    env= dummy_vec_env([lambda:env])
    TypeError: 'module' object is not callable

The tutorial I'm following is almost 2 years old and I suspect there have been plenty of changes to many pip packages- which I'm now trying to install and run (as shown in the tutorial). Can you pls tell me what I'm doing wrong and also pls source me any good(and updated) beginner reinforcement learning tutorial(if they are available)."
149,deeplearning,open-ai,comments,2022-12-15 21:40:38,laptop for Data Science and Scientific Computing: proart vs legion 7i vs thinkpad p16/p1-gen5,macORnvidia,False,1.0,1,zmwxkh,https://www.reddit.com/r/deeplearning/comments/zmwxkh/laptop_for_data_science_and_scientific_computing/,5,1671140438.0,"laptop for Data Science and Scientific Computing: proart vs legion 7i vs thinkpad p16/p1-gen5


I'm looking at four laptop for DS. Not really interested in gaming, just the gpu, good cpu and massive ram. So that kind of brings me to the gaming laptop segment. 

**Main uses:**

- Data preprocessing, Prototyping cuda, rapids ai for accelerating classical data science and machine learning, DL inferencing, building conda enabled containers, 3D modeling/rendering and simulations using python, NLP, openCV, pytorch



1. Thinkpad p16:  4200$/3900$ (64 vs 32 gb ram)

64gb/32gb ddr5, i9 12900hx, rtx a4500 16gb vram, 1 TB, 3480 vs 2400, 230W power adapter 



2. Thinkpad p1 gen5:  3900$

32gb ddr5, i9 12900h vpro, rtx 3080ti 16gb vram, 1 TB, 2560 vs 1600, 230W power adapter



3. Asus Proart studiobook: 2999$

32gb ddr5, i7 12700h, rtx 3080ti 16gb vram, 2 TB, 3840 vs 2400 4K OLED, 330W power adaptor 



4. Legion 7i: 3500$

32gb ddr5, i9 12900hx, rtx 3080ti 16gb vram, 2 TB, 2560 vs 1600 165hz,  300W power adaptor



I love how beautiful and robust legion 7i is but based on the price difference I'm also leaning towards asus proart in case i7 12th gen isn't too bad to work with."
150,deeplearning,open-ai,comments,2023-06-28 12:14:12,What are the best deep learning books that are still up-to-date?,Mr_Funkedeli,False,0.97,22,14l7wbl,https://www.reddit.com/r/deeplearning/comments/14l7wbl/what_are_the_best_deep_learning_books_that_are/,6,1687954452.0,"Hello r/deeplearning,

I am trying to jump into the field of deep learning. I have some past experience with ML and DL, and I have even completed [fast.ai](https://fast.ai)'s course ""Practical Deep Learning for Coders Part 1"". I want to take it to the next step though, and many have recommended that I follow a book. All of the books I can find though, seem to be quite outdated, especially since ML and DL have progressed so much in the last year alone.

**My question basically is, what are some good DL books that are still up to date in terms of whats covered and how it the information is presented. Bonus if it uses Pytorch, as that is what I want to learn.** 

Also feel free to recommend courses that I could take as well. I'm open to everything!

&#x200B;

Thanks!"
151,deeplearning,open-ai,comments,2020-08-05 10:58:06,image-GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model,OnlyProggingForFun,False,0.93,92,i437pt,https://www.youtube.com/watch?v=FwXQ568_io0,6,1596625086.0,
152,deeplearning,open-ai,comments,2021-12-29 08:03:22,I wrote a program with OpenAI's Codex that fixes errors,tomd_96,False,0.94,96,rr2wme,https://v.redd.it/jupdtry6vf881,6,1640765002.0,
153,deeplearning,open-ai,comments,2023-01-20 08:53:58,Gotcha,actual_rocketman,False,0.94,64,10gs1ik,https://i.redd.it/yyh41pnje7da1.jpg,5,1674204838.0,
154,deeplearning,open-ai,comments,2021-07-28 17:45:57,"OpenAI Releases Triton, An Open-Source Python-Like GPU Programming Language For Neural Networks",techsucker,False,0.94,68,otf0fs,https://www.reddit.com/r/deeplearning/comments/otf0fs/openai_releases_triton_an_opensource_pythonlike/,5,1627494357.0,"OpenAI released their newest language, [Triton](https://github.com/openai/triton). This open-source programming language that enables researchers to write highly efficient GPU code for AI workloads is Python-compatible and comes with the ability of a user to write in as few as 25 lines, something on par with what an expert could achieve. OpenAI claims this makes it possible to reach peak hardware performance without much effort, making creating more complex workflows easier than ever before!

Researchers in the field of Deep Learning often rely on native framework operators. However, this can be problematic because it requires many temporary tensors to work, which may hurt performance at scale for neural networks. Writing specialized GPU kernels is a more convenient solution, but surprisingly difficult due to intricacies when programming them according to GPUs. It was challenging to find a system that provides the flexibility and speed required while also being easy enough for developers to understand. This has led researchers at OpenAI in improving Triton, which was initially founded by one of their teammates.

Quick Read: [https://www.marktechpost.com/2021/07/28/openai-releases-triton-an-open-source-python-like-gpu-programming-language-for-neural-networks/](https://www.marktechpost.com/2021/07/28/openai-releases-triton-an-open-source-python-like-gpu-programming-language-for-neural-networks/) 

Paper: http://www.eecs.harvard.edu/\~htk/publication/2019-mapl-tillet-kung-cox.pdf

Github: https://github.com/openai/triton"
155,deeplearning,open-ai,comments,2018-11-14 15:56:16,OpenAI Founder: Short-Term AGI Is a Serious Possibility,gwen0927,False,0.91,26,9x1aqi,https://medium.com/syncedreview/openai-founder-short-term-agi-is-a-serious-possibility-368424f7462f,5,1542210976.0,
156,deeplearning,open-ai,comments,2020-08-27 10:20:19,GenRL: PyTorch-First Reinforcement Learning library,sharadchitlangia,False,0.87,42,ihii3i,https://www.reddit.com/r/deeplearning/comments/ihii3i/genrl_pytorchfirst_reinforcement_learning_library/,5,1598523619.0,"Github: [https://github.com/SforAiDl/genrl](https://github.com/SforAiDl/genrl)

Reinforcement learning research is moving faster than ever before. In order to keep up with the growing trend and ensure that RL research remains reproducible, GenRL aims to aid faster paper reproduction and benchmarking by providing the following main features:

* **PyTorch-first**: Modular, Extensible and Idiomatic Python
* **Tutorials and Documentation:** *We have over 20 tutorials assuming no knowledge of RL concepts. Basic explanations of algorithms in Bandits, Contextual Bandits, RL, Deep RL, etc.*
* **Unified Trainer and Logging class**: code reusability and high-level UI
* **Ready-made algorithm implementations**: ready-made implementations of popular RL algorithms.
* **Faster Benchmarking**: automated hyperparameter tuning, environment implementations, etc.

The core of our library is centered around RL, having policies, values, actor critics, etc. And with trainers and loggers, the only part to care about is to have the right functions implemented and everything else is taken care of!

By integrating these features into GenRL, we aim to eventually support **any new algorithm implementation in less than 100 lines**. **We're also looking for more Open Source Contributors!**

Currently, the library has implementations of popular classical and Deep RL agents that ready to be deployed. Apart from these, various Bandit algorithms are a part of GenRL. It has various abstraction layers that make the addition of new algorithms easy for the user. Do give us a star!

&#x200B;

[Vanilla DQN](https://preview.redd.it/d8rjc9zltij51.png?width=1548&format=png&auto=webp&s=55adf6bef31c0e720867cc2628ac8ca29b5b6f6a)

[Training a DoubleDQN would only require changing a single function](https://preview.redd.it/cg4cua1ltij51.png?width=1784&format=png&auto=webp&s=980f31b95ad3ea0e924065508043da3b2cbf6cba)

[Training a DuelingDQN would only require changing a single function](https://preview.redd.it/o97uu41ltij51.png?width=1682&format=png&auto=webp&s=97db9c8f6eb0cd0664cd59f85ed20375aa9d4ab8)"
157,deeplearning,open-ai,comments,2020-02-24 15:44:25,Everything you need to know about computer vision in one repo,frlazzeri,False,0.95,67,f8t2cf,https://www.reddit.com/r/deeplearning/comments/f8t2cf/everything_you_need_to_know_about_computer_vision/,5,1582559065.0,"*This post was co-authored by JS Tan, Patrick Buehler, Anupam Sharma and Jun Ki Min.*

In recent years, we’ve seen extraordinary growth in Computer Vision, with applications in image understanding, search, mapping, semi-autonomous or autonomous vehicles and many more .

The ability for models to understand actions in a video , a task that was unthinkable just a few years ago , is now something that we can achieve with relatively high accuracy and in near real-time.

However, the field is not particularly welcoming for newcomers. Without prior experience or guidance, building an accurate classifier can easily take weeks. Unless you’re ready to spend a long-time learning computer vision, it’s extremely hard to master the basics, let alone begin to explore some of the cutting-edge technologies in the field. Even for computer vision experts, building a quick Proof of Concept (POC) can be non trivial and could easily end up taking many days to put together.

At [Microsoft ](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri), we have been working for many years on diverse Computer Vision solutions for our customers and collected our learning into our new public [Microsoft](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) repository: [Custom vision repo](https://github.com/microsoft/ComputerVision-recipes?WT.mc_id=medium-article-lazzeri).

The goal of [this repository](https://github.com/microsoft/ComputerVision-recipes?WT.mc_id=medium-article-lazzeri) is to provide examples and best practice guidelines for building computer vision systems on [Azure](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) , and to share this with the open-source community . More specifically, our goal was to create a [repository](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) that will help us to provide solutions rapidly to the community and to customers that we work with , or with on-boarding new team members who may have expertise in data science, but not specifically in computer vision. From mastering some of the most common scenarios in the field, like image classification, object detection , and image similarity, to exploring cutting edge scenarios like activity recognition and crowd counting, [this repo](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) will guide you through building models, fine-tuning them, and using them in real-world scenarios.

We’re kicking off our repo with **5 scenarios.** You can find the links to the repos here:

* [Classification](https://github.com/microsoft/computervision-recipes/tree/master/scenarios/classification?WT.mc_id=medium-article-lazzeri)
* [Similarity](https://github.com/microsoft/computervision-recipes/tree/master/scenarios/similarity?WT.mc_id=medium-article-lazzeri)
* [Detection](https://github.com/microsoft/computervision-recipes/tree/master/scenarios/detection?WT.mc_id=medium-article-lazzeri)
* [Action Recognition](https://github.com/microsoft/computervision-recipes/tree/master/contrib/action_recognition?WT.mc_id=medium-article-lazzeri)
* [Crowd Counting](https://github.com/microsoft/computervision-recipes/tree/master/contrib/crowd_counting?WT.mc_id=medium-article-lazzeri)

Rather than creating implementations from scratch, we draw from popular state-of-the-art libraries (e.g. fast.ai and [torchvision ](https://pytorch.org/docs/stable/torchvision/index.html)), and we build additional utility around loading image data, optimizing models , and evaluating models. In addition, we aim to answer the frequently asked questions, try to explain the deep learning intuitions, and highlight common pitfalls.

Whether you a re an expert in computer vision or just getting your hands wet, we believe [this repository](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) offers something for you . For the beginner, [this repo](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) will guide you through building a state-of-the-art model and help you develop an intuition for the craft. For the experts, this repository can quickly get you to a strong baseline model which is easy to extend using custom Python/PyTorch code. In addition, the repository also aims to provide support with:

1. [The full data science process](https://docs.microsoft.com/azure/machine-learning/team-data-science-process/overview?WT.mc_id=medium-article-lazzeri).
2. [The tooling to succeed on Azure](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri).

We hope that these examples and utilities will make it easier and faster for developers to create custom vision applications.

# The Data Science Process

The [Computer Vision Recipes GitHub repository](https://github.com/microsoft/ComputerVision-recipes?WT.mc_id=medium-article-lazzeri) shows you how to approach the five key steps of the data science process and provides utilities to enrich each of the steps :

1. **Evaluating** — Evaluate your model. Depending on the metric you’re interested in optimizing, you may want to explore different methods of evaluation.
2. **Model selection and optimization** — Tun e and optimize hyperparameters to get the highest performing model. Because Computer Vision models are often computationally costly, we show you how to seamlessly scale your parameter tuning into Azure .
3. **Operationalizing** — Operationalize models in a production environment on Azure by deploying it onto Kubernetes.

Inside the computer vision recipes [repo,](https://github.com/microsoft/ComputerVision-recipes?WT.mc_id=medium-article-lazzeri) we have added a lot of utility to support common tasks such as loading data sets in the format expected by different algorithms, splitting training/test data, and evaluating model outputs .

This computer vision repository also has deep integration with the [Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) to complement your work locally. We provide code examples on how you can optionally and easily scale your training into the cloud, and how you can deploy your models for production workloads.

**Azure Cognitive Services**

Note that for certain computer vision problems, you may not need to build your own models. Instead, pre-built or easily customizable solutions exist which do not require any custom coding or machine learning expertise.

* [Vision Services](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/?WT.mc_id=medium-article-lazzeri) are a set of pre-trained REST APIs which can be called for image tagging, OCR, video analytics, and more. These APIs work out of the box and require minimal expertise in machine learning but have limited customization capabilities. See the various demos available to get a feel for the functionality (e.g. Computer Vision).
* [Custom Vision](https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/?WT.mc_id=medium-article-lazzeri) is a SaaS service to train and deploy a model as a REST API given a user-provided training set. All steps including image upload, annotation, and model deployment can be performed using either the UI or a Python SDK. Training image classification or object detection models can be achieved with minimal machine learning expertise. The Custom Vision offers more flexibility than using the pre-trained cognitive services APIs but requires the user to bring and annotate their own data.

Before using the Computer Vision repository, we strongly recommend evaluating if these can sufficiently solve your problem.

To give you a sense of how you can use our repo to build a state of the art (SOTA) model, here is a preview of how simple it is to create an Object Detection model. Of course, you can go much deeper and add custom PyTorch code, but getting started is as simple as this :

**1. Load your data**

The first step is to load your data — we help you do this with a simple object that automatically parses your data and the annotations:

`from utils_cv.detection.data import DetectionLoader data = DetectionLoader(""path/to/data"")`

**2. Train/fine-tune your model**

Then we create a ‘learner’ object that helps you manage and train your model. By default, it will use torchvision’s Faster R-CNN model. But you can easily switch it out.

`from utils_cv.detection.model import DetectionLearner detector = DetectionLearner(data) detector.fit()`

**3. Evaluate**

Finally, lets evaluate our model using the built-in helper functions. We can look at the precision and recall curves to give us a sense of how our model is performing.

`from utils_cv.detection.plot import plot_pr_curves eval = detector.evaluate() plot_pr_curves(eval)`

As we continue to build out of repository, we will be looking for new computer vision scenarios to unlock . Feel free to reach out to [cvbp@microsoft.com](mailto:cvbp@microsoft.com) or post an issue if you wish to see us cover a scenario .

# Additional resources to learn more

To learn more, you can read the following articles and notebooks:

* [Custom vision repo](https://github.com/microsoft/ComputerVision-recipes?WT.mc_id=medium-article-lazzeri)
* Original article: [https://techcommunity.microsoft.com/t5/azure-ai/nearly-everything-you-need-to-know-about-computer-vision-in-one/ba-p/1070311](https://techcommunity.microsoft.com/t5/azure-ai/nearly-everything-you-need-to-know-about-computer-vision-in-one/ba-p/1070311)
* [Vision Services](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/?WT.mc_id=medium-article-lazzeri) on Azure
* [Custom Vision](https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/?WT.mc_id=medium-article-lazzeri) on Azure
* Portfolio of Azure Machine Learning Notebooks: [aka.ms/AzureMLServiceGithub](https://aka.ms/AzureMLServiceGithub)
* Azure Machine Learning: [aka.ms/AzureMLservice](https://aka.ms/AzureMLservice)
* Get started with Azure ML: [aka.ms/GetStartedAzureML](https://aka.ms/GetStartedAzureML)
* Automated Machine Learning Documentation: [aka.ms/AutomatedMLDocs](https://aka.ms/AutomatedMLDocs)
* What is Automated Machine Learning? [aka.ms/AutomatedML](https://aka.ms/AutomatedML)
* Python Microsoft: [aka.ms/PythonMS](https://aka.ms/PythonMS)
* Azure ML for VS Code: [aka.ms/AzureMLforVSCode](https://aka.ms/AzureMLforVSCode)"
158,deeplearning,open-ai,comments,2021-03-21 21:06:17,Choosing the right GPU,Cemilian,False,0.5,0,ma65w7,https://www.reddit.com/r/deeplearning/comments/ma65w7/choosing_the_right_gpu/,5,1616360777.0,"Hello, 

I've been burning my brain out for days to figure out what kind of GPU's I'll use on my pc setup. 

I am a student, and already know some basics on various programming languages (especially for web) and some algorithm.

But as I am very curious about AI and deep/machine learning, I want to improve myself on those topics either.

So, I need to learn python first. And I also want to play with some open source applications, like deepfake and other stuff. 

&#x200B;

Last time was about two years ago, i wanted to run such a software on my PC. I had double RX580's installed, so i was very curious and eager. Then a big frustration.... It wasn't gonna work on AMD GPU's...

Is this still the same?

My only GPU choices are: 

Either one RX580 (maybe two, not sure)

or two nVidia 1050 Ti's.

&#x200B;

I know that 1050Ti's are very low on memory and doesnt support SLI to work together at least for games. But for deep/machine learning stuff, is this the same case? Can't I get any more benefits of installing two of them? 

&#x200B;

I've tried to research about this a lot, but couldn't even find proper sources. 

In a forum, I got an advice to use RX 580 for personal and rent amazon or google cloud services for AI/ML. That sounds reasonable to me.

&#x200B;

What would you suggest?

Is there still no room for AMD GPU's in this world?

Don't I get anything enough with a double 1050Ti setup?

Should I focus on rental services?

&#x200B;

I am a total beginner. So I might even have not asked the right questions. Please orient me...

Thanks in advance!

*P.S. for mods : if anything wrong with the post, please let me know and allow to fix it. Don't just press delete for all the text....*

[View Poll](https://www.reddit.com/poll/ma65w7)"
159,deeplearning,open-ai,comments,2024-01-13 04:24:32,"Idea / Proposal for someone smarter than I am. Devs/Coders, please hear me out.",DriestBum,False,0.2,0,195ff66,https://www.reddit.com/r/deeplearning/comments/195ff66/idea_proposal_for_someone_smarter_than_i_am/,5,1705119872.0,"Disclaimer: you're probably more knowledgeable than me, have more experience, and are far better at coding than me. Keep that in mind.

Here's the TLDR: create a new program (perhaps using Transformers/Tiny LLM, perhaps not needing LLMs at all) that scans, details, and analyzes the users exact hardware setup, and ultimately determines the optimal LLM/quant/settings/config for that specific hardware configuration (and perhaps use case, like general chat/role play/instruct/coding...).

Why? 

Because of the multitude of posts we see of new people trying to get into open source LLMs, but having no understanding of where they should start, what is possible on their machine, and how to configure the model for their needs. 

I was one of these people, and only after long periods of reading guides/tutorials/wikis/model cards/etc was I finally able to get a working model on my machine with decent speed and quality. For a person who is tech minded, but not a coder, and not familiar with anything other than straight forward download .exe and go, I had to start at square 1 and figure out GitHub, Linux, and all the backend llamma.ccp and whatnot. It took forever, I'm just a regular tech consumer, I don't know how to build shit. Mind you, it was a valuable experience, but I guarentee many people would have given up without pressing on and figuring it all out.

If we want greater adoption in the open source space, an easier on boarding process would be a God send for people like me. I would have paid for it! Easily! I put out offers for people to create a docker container so I could just click ""run"". The offers to build it were way out of my budget, so I was forced to grind it out and stumble my way through the steep learning curve. 

I'm not a unique case. I'm someone who has used ChatGPT, played with Spaces on Hugging Face, and really wanted a local LLM to use with sensitive local data that I couldn't trust OpenAI or anyone with. I understood the value, but I didn't have the ability to spin up a model without massive amounts of homework. 

So anyway, that's the idea. A stand alone program, or utility, that analyzes a user's hardware capability, suggests appropriate specs/config for a model, and gives a bullet point list of what to do to get that specific model working in a numbered list of steps. 

I would have paid $100 for that shortcut. Easily. I almost paid 20x that for a container of an old model. Just to get it working. 

To reiterate, the problem is:

New people asking ""what do I need to run x?"" or ""can my x machine run this model?"" Or ""what's the beat coding model I can use with x machine"". 

Solution:

A utility that's only purpose is to examine the user's hardware and recommend optimal models, quants, and settings profile (for webui / lm studio...) 

Just an idea. 

Please DM if anyone is interested in a collaboration. I'm a corporate finance person, with a marketing degree/background, I'm not a developer. I can't build it, but I could hype it, and potentially sell it. I know the target market, and it's large and narrow enough to be worthwhile. 

Thanks for coming to my lunch and learn presentation. And I'm totally aware of how I pitched a close source business idea to an open source sub, I know, but the people who could/want to do this are here. I'm not going to lose sleep over offending a commie or two."
160,deeplearning,open-ai,comments,2024-01-16 19:44:53,How to make my text digital clone?,Zestyclose_Memory535,False,0.5,0,198bwl5,https://www.reddit.com/r/deeplearning/comments/198bwl5/how_to_make_my_text_digital_clone/,5,1705434293.0,"Google gave me extremely dubious results, so I’m writing here.

For a long time, I’ve had the idea in my head of taking all our correspondence with a friend, 100,000 messages from Telegram, and further training a ready-made model using this. A **small model** was planned, because she should not know about everything in the world, *like me*.

Of course, I understand that the data will have to be sorted and structured, and what is equally important, the model must **speak like me** in my language. In this case, the russian language.

I **don’t have** a lot of computing power for training, the maximum is free Kaggle (analogous to the nasty Google Colab), as well as my local video cards of GTX 8 GB and RTX 16 GB.

*Ideally*, I would like the model to simulate my communication style, my slang and my cursed emoji. It would be interesting to communicate with myself, albeit in a reduced form, and also to leave my clone in case I leave.

Advise the vector **where I should move**, I can figure it out myself. It seems to me that I can’t handle models that weigh 4 gigabytes, and I need a smaller model. I already have experience with Dreambooth, OpenAI API, local LLMs, I know how to program, I understand computer technology, I have initial experience in developing neural networks, and gym openai. So I'm not too afraid of complex things and terms."
161,deeplearning,open-ai,comments,2021-03-17 12:21:53,"We're building an ML-based (more info in footnote) collaborative workspace for teams, allowing you to map, find, and collaborate on your docs, notes, links. Just opened up our waitlist, feedback and comments (and support of course) welcome!",natalieberlin,False,0.6,2,m6z6ga,https://www.reddit.com/r/deeplearning/comments/m6z6ga/were_building_an_mlbased_more_info_in_footnote/,5,1615983713.0,"Hello, community! We're building an AI-(graph)-based tool for file & content management and are just onboarding our first alpha & beta users!

https://preview.redd.it/5y2kiujmzkn61.png?width=2224&format=png&auto=webp&s=a0bffbcfbd99023f7697995ae8ac87cb47a4d2fd

It's already **difficult to keep track** of your files, codes, contracts, email attachments, and PDFs, and gets worse when you include your Github/Notion pages or other web links. Put this into a remote setting, where you communicate via Slack or MS Teams, and you see how [**30%**](https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/the-social-economy) **of the average ""knowledge worker's""** (that's us) **time is spent searching and locating files, as well as communicating & collaborating internally 🤯**

We're tackling the problem with an intuitive UI and a powerful machine-learning core to bring structure and transparency into your work content across your docs, files, Google Drive, Dropbox, Slack, MS Teams, and browser bookmarks.

If you would like to give it a try, add yourself to the waitlist and get early access to our beta version, we'd be super happy: ➡️ [https://reason.al](https://reason.al/?utm_source=reddit.com&utm_medium=referral&utm_campaign=comm-21-009-cd)

❗ If you fill in the survey and are onboarded on the closed beta, you'll get the chance to get grandfathered (and give the lifelong premium to 4 other friends 😻).

\_\_\_

Footnote:

💡 In a nutshell, the underlying ML surfaces the right content among work files or highlights outdated/unreliable/faulty files before you open them. It is based on (combined 😇 ) more than 10 years of our founders' [/u/btabibian](https://www.reddit.com/u/btabibian/) [/u/musically\_ut](https://www.reddit.com/u/musically_ut/) research:

[https://dl.acm.org/doi/abs/10.1145/3038912.3052672](https://dl.acm.org/doi/abs/10.1145/3038912.3052672)

[https://arxiv.org/abs/1905.05305](https://arxiv.org/abs/1905.05305)

[http://proceedings.mlr.press/v124/tabibian20a](http://proceedings.mlr.press/v124/tabibian20a)

[https://www.pnas.org/content/116/10/3988.short](https://www.pnas.org/content/116/10/3988.short)

[https://dl.acm.org/doi/abs/10.1145/3018661.3018685](https://dl.acm.org/doi/abs/10.1145/3018661.3018685)

[https://dl.acm.org/doi/abs/10.1145/2939672.2939875](https://dl.acm.org/doi/abs/10.1145/2939672.2939875)

[https://arxiv.org/abs/1805.09360](https://arxiv.org/abs/1805.09360)"
162,deeplearning,open-ai,comments,2023-11-23 15:47:45,A deeper look at the Q* Model as a combination of A* algorithms and Deep Q-learning networks.,Ok-Judgment-1181,False,0.55,4,18240yl,https://www.reddit.com/r/deeplearning/comments/18240yl/a_deeper_look_at_the_q_model_as_a_combination_of/,5,1700754465.0,"Hey, folks! Buckle up because the recent buzz in the AI sphere has been nothing short of an intense rollercoaster. Rumors about a groundbreaking AI, enigmatically named Q\* (pronounced Q-Star), have been making waves, closely tied to a chaotic series of events that rocked OpenAI and came to light after the [abrupt firing of their CEO](https://edition.cnn.com/2023/11/17/tech/sam-altman-departs-open-ai/index.html) \- Sam Altman ( [u/samaltman](https://www.reddit.com/u/samaltman/) **)**.

There are several questions I would like to entertain, such as the impacts of Sam Altman's firing, the most probable reasons behind it, and the possible monopoly on highly efficient AI technologies that Microsoft is striving to have. However, all these things are too much for 1 Reddit post, so here **I will attempt to explain why Q\* is a BIG DEAL, as well as go more in-depth on the theory of combining Q-learning and A\* algorithms**.

At the core of this whirlwind is an AI (Q\*) that aces grade-school math but does so without relying on external aids like Wolfram. It may possibly be a paradigm-shattering breakthrough, transcending AI stereotypes of information repeaters and stochastic parrots which showcases iterative learning, intricate logic, and highly effective long-term strategizing.

This milestone isn't just about numbers; it's about unlocking an AI's capacity to navigate the single-answer world of mathematics, potentially revolutionizing reasoning across scientific research realms, and breaking barriers previously thought insurmountable.

What are A\* algorithms and Q-learning?:

From both the name and rumored capabilities, the Q\* is very likely to be an AI agent that combines A\* Algorithms for planning and Q-learning for action optimization. Let me explain.

[A\* algorithms](https://theory.stanford.edu/~amitp/GameProgramming/AStarComparison.html) serve as powerful tools for finding the shortest path between two points in a graph or a map while efficiently navigating obstacles. Their primary purpose lies in optimizing route planning in scenarios where finding the most efficient path is crucial. These algorithms are known to balance accuracy and efficiency with the notable capabilities being: Shortest Path Finding, Adaptability to Obstacles, and their computational Efficiency / Optimality (heuristic estimations).

However, applying A\* algorithms to a chatbot AI involves leveraging its pathfinding capabilities in a rather different context. While chatbots typically don’t navigate physical spaces, **they do traverse complex information landscapes to find the most relevant responses or solutions to user queries**. Hope you see where I´m going with this, but just in case let's talk about Q-learning for a bit.

Connecting the dots even further, let's think of [Q-learning](https://builtin.com/artificial-intelligence/deep-q-learning) as us giving the AI a constantly expanding cheat sheet, helping it decide the best actions based on past experiences. However, in complex scenarios with vast states and actions, maintaining a mammoth cheat sheet becomes unwieldy and hinders our progress toward AGI due to elevated compute requirements. Deep Q-learning steps in, utilizing neural networks to approximate the Q-value function rather than storing it outright.

Instead of a colossal Q-table, the network maps input states to action-Q-value pairs. It's like having a compact cheat sheet tailored to navigate complex scenarios efficiently, giving AI agents the ability to pick actions based on the [Epsilon-Greedy approach](https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/)—sometimes randomly exploring, sometimes relying on the best-known actions predicted by the networks. Normally DQNs (or [Deep Q-networks](https://www.tensorflow.org/agents/tutorials/0_intro_rl)), use two neural networks—the main and target networks—sharing the same architecture but differing in weights. Periodically, their weights synchronize, enhancing learning and stabilizing the process, this last point is highly important to understand as it may become the key to a model being capable of **self-improvement** which is quite a tall feat to achieve. This point however is driven further if we consider the [Bellman equation](https://www.geeksforgeeks.org/bellman-equation/), which basically states that with each action, the networks update weights using the equation utilizing Experience replay—a sampling and training technique based on past actions— which helps the AI learn in small batches **without necessitating training after every step**.

*I must also mention that Q\*'s potential is not just a math whiz but rather* ***a gateway to scaling abstract goal navigation*** *as we do in our heads when we plan things, however, if achieved at an AI scale we would likely get highly efficient, realistic and logical plans to virtually any query or goal (highly malicious, unethical or downright savage goals included)...*

Finally, there are certain **pushbacks and challenges** to overcome with these systems which I will underline below, HOWEVER, with the recent news surrounding OpenAI, I have a feeling that smarter people have found ways of tackling these challenges efficiently enough to have a huge impact of the industry if word got out.

To better understand possible challenges I would like to give you a hypothetical example of a robot that is tasked with solving a maze, where the starting point is user queries and the endpoint is a perfectly optimized completion of said query, with the maze being the World Wide Web.

Just like a complex maze, the web can be labyrinthine, filled with myriad paths and dead ends. And although the A\* algorithm helps the model seek the shortest path, certain intricate websites or information silos can confuse the robot, leading it down convoluted pathways instead of directly to the optimal solution (problems with web crawling on certain sites).

By utilizing A\* algorithms the AI is also able to adapt to the ever-evolving landscape of the web, with content updates, new sites, and changing algorithms. However, due to the speed being shorter than the web expansion, it may fall behind as it plans based on an initial representation of the web. When new information emerges or websites alter their structures, the algorithm might fail to adjust promptly, impacting the robot's navigation.

On the other hand, let's talk about the challenges that may arise when applying Q-learning. Firstly it would be limited sample efficiency, where the robot may pivot into a fraction of the web content or stick to a specific subset of websites, it might not gather enough diverse data to make well-informed decisions across the entire breadth of the internet therefore failing to satisfy user query with utmost efficiency.

And secondly, problems may arise when tackling [high-dimensional data](https://www.statology.org/high-dimensional-data/). The web encompasses a vast array of data types, from text to multimedia, interactive elements, and more. Deep Q-learning struggles with high-dimensional data (That is data where the number of features in a dataset exceeds the number of observations, due to this fact we will never have a deterministic answer). In this case, if our robot encounters sites with complex structures or extensive multimedia content, processing all this information efficiently becomes a significant challenge.

To combat these issues and integrate these approaches one must find a balance between optimizing pathfinding efficiency while swiftly adapting to the dynamic, multifaceted nature of the Web to provide users with the most relevant and efficient solutions to their queries.

To conclude, there are plenty of rumors floating around the Q\* and Gemini models as giving AI the ability to plan is highly rewarding due to the increased capabilities however it is also quite a risky move in itself. This point is further supported by the constant reminders that we need better AI safety protocols and guardrails in place before continuing research and risking achieving our goal just for it to turn on us, but I'm sure you've already heard enough of those.So, are we teetering on the brink of a paradigm shift in AI, or are these rumors just a flash in the pan? Share your thoughts on this intricate and evolving AI saga—it's a front-row seat to the future!

I know the post came out lengthy and pretty dense, but I hope this post was helpful to you! Please do remember that this is mere speculation based on multiple news articles, research, and rumors currently speculating regarding the nature of Q\*, take the post with a grain of salt :)

**Edit:** After several requests, I would like to mention an Arxiv paper on a very similar topic I've discussed in the post:

***A\* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks*** ([https://arxiv.org/abs/2102.04518v2](https://arxiv.org/abs/2102.04518v2))

*Let us all push the veil of ignorance back and the frontier of discovery forward.*"
163,deeplearning,open-ai,comments,2020-04-21 23:00:56,How does the talktotransformer website work so fast?,parrot15,False,0.97,33,g5prf5,https://www.reddit.com/r/deeplearning/comments/g5prf5/how_does_the_talktotransformer_website_work_so/,5,1587510056.0,"At [https://talktotransformer.com/](https://talktotransformer.com/), you can type a prompt and the transformer will autogenerate the text for you using OpenAI's GPT-2 1.5 billion parameter model.

I'm not asking how GPT-2 works, I'm asking something else. When I ran the GPT-2 1.5B model on the free TPU in Google Colab, it took around 20 to 40ish seconds to generate around the same about of text as the website generates per prompt.

And yet, the website is somehow generating text from the prompt almost instantaneously using the 1.5B model. This is on top of all the X number of people who must be using the website at the same time I am, so it is doing text generation concurrently and near-instantaneously using a gigantic model.

I am very confused. Did the creator of the website just use a lot more TPUs/GPUs behind the scenes and is letting them run 24/7 (I don't think so because that would cost a shit ton of money), or am I missing something fundamental here?

This same question can be applied to this website too: [https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is](https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is)

Please keep in mind that I'm relatively new to all of this. Thanks in advance!"
164,deeplearning,open-ai,comments,2023-12-14 07:11:43,Which file formats in LLMs are parsed vs indexed raw? Could this be an opportunity for optimization?,brainhack3r,False,0.5,0,18i3198,https://www.reddit.com/r/deeplearning/comments/18i3198/which_file_formats_in_llms_are_parsed_vs_indexed/,5,1702537903.0,"I'm trying to dive deeper into the file formats and parsing of LLMs.

It seems like they have a native understanding of specific file formats like HTML, LaTeX, etc.  

What about file encodings like UTF-8 ? 

Does the parser first open the file in the correct encoding, then start parsing out tokens?

I was thinking that, at LEAST for text, it might be a lot more efficient to not think of documents as having a format but instead represent them visually in memory after they are rendered.

Maybe this could be optimized where each character of text could have metadata like whether it's bold or not.  

Wouldn't this radically compress the number of parameters we need for the model?

There wouldn't be separate documents for HTML or LaTeX or PDF. It's just all text.

The main issue is that there would need to be an output encoder such that it could take the ""visual"" representation, then render that as a specific encoding based on what the user wants.

This way you could render the output to PDF or LaTeX or whatever and the main model's output is just the intermediate format, then another AI encoder renders that to what the user wants as output."
165,deeplearning,open-ai,comments,2021-01-04 10:07:49,AI that generates rap music lyrics (Open-Source),_Xeon__,False,0.86,11,kq5vz1,https://www.reddit.com/r/deeplearning/comments/kq5vz1/ai_that_generates_rap_music_lyrics_opensource/,5,1609754869.0,"hello, guys I recently build a deep learning project with python, Keras, and TensorFlow that uses LSTMs text generating method to generate rap and hip hop music lyrics. All the code and the models are in the Github repo([https://github.com/YigitGunduc/Spectrum](https://github.com/YigitGunduc/Spectrum)) if you want to train the model with your own dataset, the project is flexible enough to do it by only changing the data folder. I also build a website([https://spectrumapp.herokuapp.com/](https://spectrumapp.herokuapp.com/)) if you guys check it out I would be so happy."
166,deeplearning,open-ai,comments,2021-07-15 17:06:55,"EleutherAI Researchers Open-Source GPT-J, A Six-Billion Parameter Natural Language Processing (NLP) AI Model Based On GPT-3",techsucker,False,1.0,57,okx5hm,https://www.reddit.com/r/deeplearning/comments/okx5hm/eleutherai_researchers_opensource_gptj_a/,5,1626368815.0,"[GPT-J](https://www.eleuther.ai/), a six-billion-parameter natural language processing (NLP) AI model based on GPT-3, has been open-sourced by a team of EleutherAI researchers. The model was trained on an open-source text [dataset of 800GB](https://pile.eleuther.ai/) and was comparable with a GPT-3 model of similar size.

The model was trained using Google Cloud’s v3-256 TPUs using EleutherAI’s Pile dataset, which took about five weeks. GPT-J achieves accuracy similar to OpenAI’s reported findings for their 6.7B parameter version of GPT-3 on standard NLP benchmark workloads. The model code, pre-trained weight files, a Colab notebook, and a sample web page are included in EleutherAI’s release.

Story: [https://www.marktechpost.com/2021/07/15/eleutherai-researchers-open-source-gpt-j-a-six-billion-parameter-natural-language-processing-nlp-ai-model-based-on-gpt-3/](https://www.marktechpost.com/2021/07/15/eleutherai-researchers-open-source-gpt-j-a-six-billion-parameter-natural-language-processing-nlp-ai-model-based-on-gpt-3/) 

Github repository for GPT-J: https://github.com/kingoflolz/mesh-transformer-jax

Colab Notebook: https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab\_demo.ipynb

Web Demo: https://6b.eleuther.ai/"
167,deeplearning,open-ai,comments,2023-12-28 21:36:23,"The best current models (Dolphin, Mixtral, Solar, Noromaid) and where to try them",Horror_Echo6243,False,0.8,6,18t59yu,https://www.reddit.com/r/deeplearning/comments/18t59yu/the_best_current_models_dolphin_mixtral_solar/,5,1703799383.0," 

I just saw a lot of people talking about this models so if you want to test them i found this websites that have all of them

\- [infermatic.ai](https://infermatic.ai/) (all of them)

\- [https://replicate.com/tomasmcm/solar-10.7b-instruct-v1.0](https://replicate.com/tomasmcm/solar-10.7b-instruct-v1.0) (for solar)

\- [https://huggingface.co/chat](https://huggingface.co/chat) (for mixtral)

Let me know if you find more, I'd like to know

And heres a little resume if you don't know what each model is for

Dolphin : An uncensored model derived from an open-source dataset, it uses instructions from FLANv2 enhanced with GPT-4 and GPT-3.5 completions​​.

Mixtral : An advanced text generation model using a Mix of Experts architecture

Solar : domain specialization and optimization. It's recognized for its high performance and efficiency

Noromaid: Storywriting and roleplay"
168,deeplearning,open-ai,comments,2024-02-13 13:16:57,Savant 0.2.7 is out: computer vision framework optimized for Nvidia,ivan_kudryavtsev,False,1.0,10,1apt8dr,https://www.reddit.com/r/deeplearning/comments/1apt8dr/savant_027_is_out_computer_vision_framework/,5,1707830217.0,"[Savant](https://github.com/insight-platform/Savant) is an advanced computer vision framework based on Nvidia DeepStream and TensorRT - best-in-class technologies for data center and edge computer vision on Nvidia hardware.

In 0.2.7, Savant received four new demos, and now it includes 26 demos covering detection, classification, segmentation, key points, object tracking, and GANs.

New demos in the 0.2.7 are:

* RT-DETR usage sample;
* CuPy-based in-GPU postprocessing for YOLOV8-Seg;
* Serving PyTorch CUDA Inference in Savant pipeline;
* Oriented bounding box detection and tracking.

New features in 0.2.7:

* **Grafana/Prometheus integration**. In addition to OpenTelemetry tracing, you can use Grafana and Prometheus to investigate how your pipeline operates. Developers can export their custom metrics as well.
* **Buffer adapter**. A special intermediary adapter, allowing to survive traffic bursts and pipeline slowdowns. It implements a mechanism allowing the video processing deferring by slow pipeline elements. The adapter has Prometheus/Grafana integration, so you always know how many elements await processing. It is based on RocksDB.
* **Compile-only mode**. When the pipeline is launched for the first time, it compiles models in TensorRT engines. It can easily take minutes. Previously, developers could not separate compilation and evaluation. Now they can.
* **Shutdown handler in PyFunc**. This new API allows handling pipeline shutdown operations properly to release resources and notify 3rd-party systems about the termination.
* **Message filtering on Ingress and Egress**. This advanced feature allows developers to modify raw encoded streams and their metadata with simple callbacks. For example, you can select only key frames for processing and drop the rest or remove video pieces without detected objects.
* **Model post-processing on GPU.** With a new feature, developers can instruct the framework to access model output tensors directly from GPU memory without downloading them to the CPU memory.
* **GPU memory representation functions**. In the release, we provide functions for conversion memory buffers between OpenCV GpuMat, PyTorch GPU tensors, and CuPy tensors.
* **Advanced object attribute modification operations**. In the release, we implemented a set of new operations allowing developers to modify object attributes in a more convenient way.
* **Queue utilization API for PyFunc**. Savant allows adding queues between PyFuncs to implement parallel processing and traffic burst management. With the release, we implemented a new API allowing developers to access queues deployed in the pipeline to request their utilization.

Full release [notes](https://b.savant-ai.io/2024/02/07/0-2-7-release-notes/).

Don’t forget to join our [Discord](https://discord.gg/KVAfGBsZGd), where we help users. To know more about Savant, read our article: [Ten reasons to consider Savant for your computer vision project](https://b.savant-ai.io/2023/10/24/ten-reasons-to-consider-savant-for-your-computer-vision-project/)."
169,deeplearning,open-ai,comments,2019-05-21 21:08:18,Facebook Open-Sources Pythia for Vision and Language Multimodal AI Models,Yuqing7,False,0.91,30,brfxn0,https://medium.com/syncedreview/facebook-open-sources-pythia-for-vision-and-language-multimodal-ai-models-be480644b538,4,1558472898.0,
170,deeplearning,open-ai,comments,2020-11-03 08:45:53,[Tutorial] TensorBoard advanced features,kk_ai,False,0.5,0,jn6y8i,https://www.reddit.com/r/deeplearning/comments/jn6y8i/tutorial_tensorboard_advanced_features/,4,1604393153.0,"There are various tools for measuring, and ultimately bettering, the performance of a deep learning model.

Here, I wanted to take a closer look at **TensorBoard - open source visualization toolkit for deep learning**. I think that it is worth to spend some time to learn more about it, as it lets you do many things:

1. visualize images,
1. check model weights,
1. visualize the model’s architecture,
1. send a visual of the confusion matrix,
1. inspect hyper-parameter tuning results.
1. **log metrics from libs outside TF ecosystem, for example PyTorch or XGBoost.**

Additionally you can work with **TensorFlow Profiler** and **debugger** in TensorBoard.

To explain all the options mentioned above, we created comprehensive guide that will show you how to do all these!

[Deep Dive into TensorBoard: Tutorial With Examples](https://neptune.ai/blog/tensorboard-tutorial?utm_source=reddit&utm_medium=post&utm_campaign=blog-tensorboard-tutorial&utm_content=deeplearning)

Do you use TensorBoard? What are your experiences?"
171,deeplearning,open-ai,comments,2019-06-16 16:50:21,NVIDIA's AI that generates photorealistic images from your doodles is now in beta stage and open for all to try out,azmodeus99,False,0.96,51,c1c0q6,https://techunalt.com/gaugan-turns-your-scribbles-into-beautiful-landscapes/,4,1560703821.0,
172,deeplearning,open-ai,comments,2023-12-20 21:36:11,[Blogpost] Top Python Libraries of 2023,No_Dig_7017,False,0.88,13,18n5wzb,https://www.reddit.com/r/deeplearning/comments/18n5wzb/blogpost_top_python_libraries_of_2023/,4,1703108171.0,"Hello Python Community!

We're thrilled to present our 9th edition of the **Top Python Libraries and tools**, where we've scoured the Python ecosystem for the most innovative and impactful developments of the year.

This year, it’s been the boom of Generative AI and Large Language Models (LLMs) which have influenced our picks. Our team has meticulously reviewed and categorized over 100 libraries, ensuring we highlight both the mainstream and the hidden gems.

**Explore the entire list with in-depth descriptions here**: [](https://tryolabs.com/blog/top-python-libraries-2023)

Here’s a glimpse of our top 10 picks:

1. [LiteLLM](https://github.com/BerriAI/litellm) — Call any LLM using OpenAI format, and more.
2. [PyApp](https://github.com/ofek/pyapp) — Deploy self-contained Python applications anywhere.
3. [Taipy](https://github.com/Avaiga/taipy) — Build UIs for data apps, even in production.
4. [MLX](https://github.com/ml-explore/mlx) — Machine learning on Apple silicon with NumPy-like API.
5. [Unstructured](https://github.com/Unstructured-IO/unstructured) — The ultimate toolkit for text preprocessing.
6. [ZenML](https://github.com/zenml-io/zenml) and [AutoMLOps](https://github.com/GoogleCloudPlatform/automlops) — Portable, production-ready MLOps pipelines.
7. [WhisperX](https://github.com/m-bain/whisperX) — Speech recognition with word-level timestamps & diarization.
8. [AutoGen](https://github.com/microsoft/autogen) — LLM conversational collaborative suite.
9. [Guardrails](https://github.com/guardrails-ai/guardrails) — Babysit LLMs so they behave as intended.
10. [Temporian](https://github.com/google/temporian) — The “Pandas” built for preprocessing temporal data.

Our selection criteria prioritize innovation, robust maintenance, and the potential to spark interest across a variety of programming fields. Alongside our top picks, we've put significant effort into the long tail, showcasing a wide range of tools and libraries that are valuable to the Python community.

A huge thank you to the individuals and teams behind these libraries. Your contributions are the driving force behind the Python community's growth and innovation. 🚀🚀🚀

**What do you think of our 2023 lineup? Did we miss any library that deserves recognition?** Your feedback is vital to help us refine our selection each year.

Edit: updated the post body so the links are directly here in reddit."
173,deeplearning,open-ai,comments,2022-10-02 23:31:56,"Researchers at Activeloop AI Introduce ‘Deep Lake,’ an Open-Source Lakehouse for Deep Learning Applications",ai-lover,False,0.9,23,xu39gj,https://www.reddit.com/r/deeplearning/comments/xu39gj/researchers_at_activeloop_ai_introduce_deep_lake/,4,1664753516.0,"A data lake is a centralized repository where enterprises may store structured, unstructured, and semi-structured data. Data lakes improve data management, governance, and analysis. Furthermore, they enable breaking down data silos and discovering previously concealed insights in diverse data sources. Traditionally, first-generation data lakes gathered data into distributed storage systems such as HDFS or AWS S3. Unorganized data collections transformed data lakes into “data swamps,” giving birth to the second generation of data lakes led by Delta, Iceberg, and Hudi. They work only on top of standardized structured formats such as Parquet, ORC, and Avro and offer capabilities like as time travel, ACID transactions, and schema evolution.

To conduct analytical queries, data lakes easily interface with query engines like as Presto, Athena, Hive, and Photon. They also interface to frameworks like as Hadoop, Spark, and Airflow for ETL pipeline maintenance. In turn, the combination of data lakes and query engines with explicit compute and storage separation resulted in the introduction of systems such as Lakehouse that serve as alternatives to data warehouses such as Snowflake, BigQuery, Redshift, and Clickhouse. During the last decade, deep learning has surpassed standard machine learning algorithms for dealing with unstructured and complicated data such as text, photos, videos, and audio.

[Continue reading](https://www.marktechpost.com/2022/10/02/researchers-at-activeloop-ai-introduce-deep-lake-an-open-source-lakehouse-for-deep-learning-applications/) | *heck out the* [*paper*](https://arxiv.org/pdf/2209.10785v1.pdf) *and* [*github*](https://github.com/activeloopai/deeplake)"
174,deeplearning,open-ai,comments,2019-07-18 09:57:58,"Help needed: copy mechanism, seq2seq",BalazsSzalontai,False,1.0,1,ceqgi3,https://www.reddit.com/r/deeplearning/comments/ceqgi3/help_needed_copy_mechanism_seq2seq/,4,1563443878.0,"I thought my task to solve was simple, but after weeks of trying and searching on Google I couldn't get it working.

The task: a simple seq2seq model, where the input is a short text (some lines), and  the output is a 3-token-long text, which consist only of tokens from the input. In other words, I want to give it a text as an input, and it should give me back 3 tokens from that input as the output. 

The input text is pretty structured, since it's python source code, and the output should be three of the ""main"" variables. (meaning of ""main"" is not important here, since it's part of a bigger project)

I have found an open source github project called CopyNet, I think using that could be the solution, but I wasn't able to get that working either.

I now have a seq2seq model, where it learns from input-output pairs, but it's just guessing, and I can't see any logical predictions, even if I have it train for hours on a powerful GPU.

I have a basic understanding of seq2seq models, I am using keras and have used fast ai before. I feel really stucked, and I'd appreciate any kind of help, such as some example code where copy mechanism is used so that I can understand how it works."
175,deeplearning,open-ai,comments,2019-02-14 17:19:15,OpenAI Guards Its ML Model Code & Data to Thwart Malicious Usage,gwen0927,False,0.93,21,aqm35j,https://medium.com/syncedreview/openai-guards-its-ml-model-code-data-to-thwart-malicious-usage-d9f7e9c43cd0,4,1550164755.0,
176,deeplearning,open-ai,comments,2023-03-14 08:23:23,Question on study options,CareerHour4671,False,1.0,6,11r0l52,https://www.reddit.com/r/deeplearning/comments/11r0l52/question_on_study_options/,3,1678782203.0,"I started my career as a quant then programmer, then data scientist and now work for Bloomberg.

I've been using ML for years but have not really worked with NLP and with the recent advances in LLMs the penny dropped that our working world is about to start changing very quickly.

Are there any AI MSc degrees that are aligned to this space that are open to part time study?

Or, should I just dive into the books as the MSc would not be specific enough.

I did an MSc in quant mathematics a few years ago after a break of 20 years from my Physics BSc and found it pretty broad and tbh not all that useful.

Anyway. Just seeing what people's thoughts are 

Cheers"
177,deeplearning,open-ai,comments,2023-11-02 14:11:24,Do you need resources for training large ML models ASAP?,nebius_com,False,0.38,0,17m4d7q,https://www.reddit.com/r/deeplearning/comments/17m4d7q/do_you_need_resources_for_training_large_ml/,4,1698934284.0,"Hello, everyone who deals with ML model training.  
We have just opened access to Nebius AI — our AI-centric cloud platform. It's ready for intensive ML workloads, including LLM and Gen AI.  
We have a good number of NVIDIA® H100 Tensor Core GPUs that can be used on-demand or with reserved resources.  
The platform provides not only GPUs but also a training-ready cloud platform with up to 3.2Tb/s per host InfiniBand network. The platform includes Managed Kubernetes for multi-node training, as well as a Marketplace with ready-to-use OS images, ML-focused applications, and tools.  
If you need resources for training large ML models ASAP, reach out to us via our website — we currently have no waiting lists for H100. Learn more https://nebius.ai"
178,deeplearning,open-ai,comments,2022-06-15 15:33:57,New open-source that accelerates AI training (~1.5-2x as of now) without requiring you to change your training setup,emilec___,False,1.0,25,vcx8dy,https://www.reddit.com/r/deeplearning/comments/vcx8dy/new_opensource_that_accelerates_ai_training_152x/,4,1655307237.0,"Hi everybody,

I have been working for a while on improving AI inference efficiency/speed, and many times I have been asked if the same could be done to make training faster as well. Training can be a bottleneck, often costly and slowing development.

The answer is clearly a yes, many well know that. Training can be streamlined at different levels.

1. One can change the way training is performed (**algorithmic optimization**) by trying to achieve faster or earlier convergence. You can change the learning rate, the scheduling policy, the training recipe or replace one level with another that requires less computational resources.
2. Another option is to apply **precision reduction techniques**, which involve a trade-off of some precision to achieve a smaller memory footprint and a faster model. For example, one could ""prune"" the non-critical or redundant parts of the neural network graph (pruning), take advantage of the properties of sparse matrices (sparsity), reduce the size of the activations and model weights from 32 or 16 bits to 8 bits or even to 4 or 1 bit (quantization).
3. Moving down, closer to the hardware, you could optimize the way the model is mapped to the hardware (**compilers**) and the way the model accesses data in memory (**data loading**), making better use of computer resources. This can be achieved by storing the data closer to the processor and converting the neural network calculations into compiled binaries so that the CPU or GPU can execute them readily.
4. Also, you can increase the amount of computing resources used for training and further accelerate training by parallelizing computations across multiple machines (**distributed computing**).

Along these lines, I decided to create an open-source that works on optimizing the full computing stack. This allows people to benefit from the **compound acceleration** provided by these 4 levels of optimization techniques, and early results are promising. Moreover, the library also aims to make training acceleration very ""accessible,"" so that everyone can use these optimization techniques, which are often complex to be implemented today. This is achieved through the use of **class decorators**, a Python constructor analogous to Java's @annotations. In short, you can simply insert these annotations (e.g. @accelerate\_model() ) into your code and the decorator will make sure that you use your computing resources to the fullest.

This library is called nebulgym and so far it **accelerates training by 30%-50%** and I definitely believe there is room to reach 70%-90% or more. The library will evolve over time and support other use cases. And if you would like to contribute or just give feedback, it will be super appreciated! [https://github.com/nebuly-ai/nebulgym](https://github.com/nebuly-ai/nebulgym).

Here's a snippet of training with nebulgym decorators (`@accelerate_dataset` and `@accelerate_model`)

```
@accelerate_dataset()
class MyDataset{…}

@accelerate_model()
class MyModel{…}

#Train your model as you usually do
```

About the **tech behind the open-source**, as of now the library works on 3 building blocks: acceleration of the data loading process, and acceleration of both back and forward propagation through sparse strategies and efficient compilations.

Regarding the latter aspect, nebulgym leverages Rammer \[1\], a DNN compiler design that optimizes the execution of DNN workloads on massively parallel accelerators. It generates an efficient static spatio-temporal schedule for a DNN at compile time to minimize scheduling overhead. It maximizes hardware utilization by holistically exploiting parallelism through inter- and intra- operator co-scheduling. It achieves this by proposing several novel, hardware neutral, and clean abstractions for the computation tasks and the hardware accelerators. These abstractions expose a much richer scheduling space to Rammer, which employs several heuristics to explore this space and finds efficient schedules.

On top of this, nebulgym computes only a small subset of the full gradient to update the model parameters in back propagation \[2\]. The gradient vectors are sparsified so that only the elements with top magnitude are kept. As a result, a smaller fraction of the weight matrix is modified, leading to a linear reduction in the computational cost.

Nebulgym also changes the way data is loaded, with the goal of eliminating any time when the processor is not processing but waiting for data to load. Indeed, a default data loader reads the data from your storage and performs some user-set preprocessing (e.g. converting the data to normalized tensors, removing biases, resizing images, etc.), and then transfers the data to the model. This process is repeated for each data and for each epoch. The data loader introduced in nebulgym at first epoch performs the same tasks (data loading and preprocessing) but writes/saves the preprocessed data (in parallel) to a fast access memory, which is usually SSD memory if available. This \[a\] slows down the first epoch slightly (\~20% slower during testing), but starting with the second epoch thereafter \[b\] preprocessing will not be computed again and \[c\] data will be transferred from fast-access memory to RAM (in parallel) to make maximum use of memory bandwidth. This speeds up all the following epochs and prevents data loading from becoming a bottleneck for the entire training process, which happens in many cases.

And that's it. Give it a try, and leave a star on [github](https://github.com/nebuly-ai/nebulgym) if you like the concept :) Also feedback would be very much appreciated! And if you want to discuss/chat about AI optimization, with other AI researchers we are organizing reading groups and other knowledge-sharing activities on [this channel](https://discord.gg/RbeQMu886J) launched recently.

[\[1\]](https://arxiv.org/pdf/1706.06197.pdf) Xu Sun, Xuancheng Ren, Shuming Ma, Houfeng Wang. meProp: Sparsified Back Propagation for Accelerated Deep Learning.

[\[2\]](https://www.usenix.org/system/files/osdi20-ma.pdf) Lingxiao Ma, Zhiqiang Xie, Zhi Yang, Jilong Xue, Youshan Miao, Wei Cui, Wenxiang Hu, Fan Yang, Lintao Zhang, and Lidong Zhou. Rammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks with Reduced Overfitting."
179,deeplearning,open-ai,comments,2021-07-04 10:03:35,MNIST on the browser! (Running PyTorch models client-side),harjyotbagga,False,0.67,4,odhgkh,https://www.reddit.com/r/deeplearning/comments/odhgkh/mnist_on_the_browser_running_pytorch_models/,4,1625393015.0," I was recently studying about user privacy in Machine / Deep Learning and how a lot of applications are moving towards deploying their model client-side (on the browser itself). While TF-JS is a popular choice amongst people. I came to know about onnx.js, which is a JavaScript library to run models from the browser. The biggest advantage of ONNX is that it allows interoperability across different open source AI frameworks, which itself offers more flexibility for AI frameworks adoption.

So I decided to implement the very famous MNIST dataset model using PyTorch & help make predictions on the client side. You can check my project here: [https://github.com/harjyotbagga/MNIST-on-the-web](https://github.com/harjyotbagga/MNIST-on-the-web) or it's implementation here: [https://bugz-mnist.herokuapp.com/](https://bugz-mnist.herokuapp.com/)

If you like the project, please leave a star, it motivates me to work harder :)"
180,deeplearning,open-ai,comments,2023-05-01 01:55:30,AI for training on 3d models,KarlanMitchell,False,1.0,8,1349kon,https://www.reddit.com/r/deeplearning/comments/1349kon/ai_for_training_on_3d_models/,4,1682906130.0,"So I've got an idea for my industry, dental, and have been playing with certain tools, but don't think there are very many options.  I'm not super experienced in AI, do have a programming background, and don't mind fussy or convoluted processes.

My idea:
To train a model for generating certain dental restorations using a wealth of 3d models and restorations which have been already created.  With a series of tools, or modules, trained to identify certain attributes so it can be fed into specificly trained models.

My issue:
Some of the libraries I've played with are specific to point clouds (without normals) and more organic, non scientific, 3d models for applications like art, fun, and video games.

My solution:
While my end project will be closed source, I'm interested in writing an open source library to take x/y and/or z ""slices"" of a 3d model (particularly multiple models with the ""output"" model marked accordingly) and generating images or arrays with adjustable percision (for different applications) for feeding into more traditional training suites as I can't seem to find anything open for training on 3d models (presumably Nvidia has something, but i can only find text to model ""magic boxes"", which seem to be more of a novelty).  Additionally my theoretical software would take the ai generated image/array slices and output a STL.

My concerns:
*Would this be useful?
*I almost certainly missed an open project that caters to 3d model training.
*What is a good suite for feeding multiple dimensional arrays which can remember the last array for continuity of the final output (we'll say images as it easier for the theory to imagine slices of a 3d model)?

Appreciate the read, hopefully it wasn't too vague as it's still in planning stages.  Any pointing in the right direction, or even setting me straight is welcome."
181,deeplearning,open-ai,comments,2020-10-15 18:53:10,Get TensorFlow to utilize RX 5700 XT?,Meetite,False,0.5,0,jbtqwv,https://www.reddit.com/r/deeplearning/comments/jbtqwv/get_tensorflow_to_utilize_rx_5700_xt/,4,1602787990.0,"Hey everyone,

I have an RX 5700 XT (not originally purchased for deep learning, it's just what I have) and was wondering if it's possible to get TensorFlow to utilize it for model training. I've been looking around to see if anyone has found a way to do it but everything I can find is either ""don't get a navi card"" which isn't useful since I already own one or is incredibly vague and talks about how there isn't good support with ROCm yet. Best i've found is using OpenCL as a workaround, but instructions for that aren't very clear for TensorFlow and i'm not exactly clear on if that would actually improve performance as opposed to just running on my CPU (A 3700x) as I already do.

I'm pretty new to this (currently finishing my first AI/deep learning undergrad course right now), so I'm not very knowledgeable about the hardware aspect of deep learning or what is and isn't compatible, so go easy on me.  


If it changes anything, I'm running Windows 10.

Assistance would be greatly appreciated."
182,deeplearning,open-ai,comments,2017-07-21 13:22:52,Very hard to work around continuous latent code in InfoGAN,kudo1026,False,0.75,2,6onvaq,https://www.reddit.com/r/deeplearning/comments/6onvaq/very_hard_to_work_around_continuous_latent_code/,4,1500643372.0,"It is always a pleasure to work with InfoGAN as it can unsupervisedly decode the structure of the data. The discrete latent worked very well for me to categorize different trajectories. However, I've been a little frustrated recently as I'm trying to implement the continuous latent code.

I first tried the mnist example given by the original InfoGAN paper, and it worked fine as the first continuous latent code represents the leaning angle and the second continuous latent represents the width. However, when I tried it on my own toy data sets,(something like
[first][1], [second][2]) nothing really works out. I expect to see continuous variation on my output, but they are instead completely random, like the latent code doesn't work at all.

I use convolutional and deconvolutional networks to discriminate and generate samples and follow the same manner for the discrete latent code as in the original openAI implementation. My treatment towards the continuous latent code is a little different, I have my discriminating network output continuous output with the same dimension as the input continuous latent code, and then try to minimize the MSE of the two. On the contrary, openaAI was modelling the code as a Gaussian distribution, and calculate its distance based on the distribution property.


    loss_c_cont = tf.reduce_mean(tf.reduce_sum(tf.square(c_cont_fake - c_cont), 1))


I think about it for a while, but don't think it could cause a huge difference as the gaussian distribution distance also mainly takes into account the square of epsilon = (x_var - mean).

    return tf.reduce_sum(
            - 0.5 * np.log(2 * np.pi) - tf.log(stddev + TINY) - 0.5 * tf.square(epsilon),
            reduction_indices=1,
        )

So my question would be: is this really the problem that caused all the problems? Or is there anything I need to pay more attention to but I did not notice?

Or in a more general sense, is there anything special about what kind of characteristics of the data can be represented by the continuous latent code? Is there any limitations to the representation capability of the continuous latent code(translation or rotation)? Can anyone share their experience about what kind of features they represented using the continuous latent code except for the examples given by the openAI paper?

I'd really welcome all your thoughts! Thanks!

P.S. Another thing I noticed was when I try to use a 5-category discrete code(in one-hot encoding) and 1-dim continuous code, it will take 5 digit for the discrete code and only 1 digit for the continuous code in the feeding input noise to the generator. Is it somehow unbalanced? Each of these code actually just represents one characteristic of the dataset. In mnist dataset, it could either number, width or rotation angle. Can this be a problem?


  [1]: https://i.stack.imgur.com/YXAMm.jpg
  [2]: https://i.stack.imgur.com/WNRTM.jpg
"
183,deeplearning,open-ai,comments,2023-06-29 01:09:36,SAM + Stable Diffusion for Text-to-Image Inpainting,Anmorgan24,False,0.8,11,14lqxl4,https://www.reddit.com/r/deeplearning/comments/14lqxl4/sam_stable_diffusion_for_texttoimage_inpainting/,4,1688000976.0,"New generative fill tools allow users to easily add, extend, or remove content from images with simple text prompts. But how can you implement this on your own images, using open source foundation models?

In my new full-code end-to-end tutorial, learn how to perform image inpainting and outpainting on any image using SAM + Stable Diffusion.

https://ai.plainenglish.io/sam-stable-diffusion-for-text-to-image-inpainting-55398a84497c"
184,deeplearning,open-ai,comments,2023-04-26 09:55:48,"Google researchers achieve performance breakthrough, rendering Stable Diffusion images in sub-12 seconds on a mobile phone. Generative AI models running on your mobile phone is nearing reality.",Lewenhart87,False,0.95,52,12zclny,https://www.reddit.com/r/deeplearning/comments/12zclny/google_researchers_achieve_performance/,3,1682502948.0,"**What's important to know:**

&#x200B;

*  Stable Diffusion is an \\\~1-billion parameter model that is typically resource intensive. DALL-E sits at 3.5B parameters, so there are even heavier models out there.
*  Researchers at Google layered in a series of four GPU optimizations to enable Stable Diffusion 1.4 to run on a Samsung phone and generate images in under 12 seconds. RAM usage was also reduced heavily.
* **Their breakthrough isn't device-specific; rather it's a generalized approach that can add improvements to all latent diffusion models.** Overall image generation time decreased by 52% and 33% on a Samsung S23 Ultra and an iPhone 14 Pro, respectively.
*  Running generative AI locally on a phone, without a data connection or a cloud server, opens up a host of possibilities. This is just an example of how rapidly this space is moving as Stable Diffusion only just released last fall, and in its initial versions was slow to run on a hefty RTX 3080 desktop GPU.

&#x200B;

As small form-factor devices can run their own generative AI models, what does that mean for the future of computing? Some very exciting applications could be possible.

&#x200B;

If you're curious, the paper (very technical) [can be accessed here.](https://arxiv.org/abs/2304.11267)"
185,deeplearning,open-ai,comments,2020-10-23 12:00:37,"Is there a way for a closed domain chatbot to build using seq2seq, generative modeling or other methods like RNNs?",jr_1995,False,1.0,10,jglqyi,https://www.reddit.com/r/deeplearning/comments/jglqyi/is_there_a_way_for_a_closed_domain_chatbot_to/,3,1603454437.0,"Let's say I have a closed-domain chatbot, and it's knowledge base is in Finance. Thus, of course I want the chatbot to answer questions that the user might have like, ""How is the best way to save money?"", or ""What are my spending habits like the past 3 months?"". However, I would also want the chatbot to be able to remember the conversation (up to a certain date) as well as model what generative models like [BlenderBot](https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/) (Facebook AI Research) or [Meena](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html) (Google AI) are able to do. The ability to respond to anyway the conversation flows? 

&#x200B;

Is it a question of just building out my intents to classify? Or should I be looking to involve both intent classification and seq2seq modeling? If so, how would one do that? Any blogs, tutorials would be helpful! Thanks!"
186,deeplearning,open-ai,comments,2023-11-20 17:13:43,Implementing Object Detection/Instance Segmentation,Prestigious_Ice_5992,False,1.0,1,17ztgw3,https://www.reddit.com/r/deeplearning/comments/17ztgw3/implementing_object_detectioninstance_segmentation/,3,1700500423.0,"I  want to dig deeper into Object Detection (OD) /Instance Segmentation  (IS) architectures, and eventually begin to experiment and create my own  models/customise SOTA models.  What resources/advice would you give  someone about to embark on this journey?

I  have found most resources (blog posts/YouTube playlists/ books) just  touch on the high-level concepts of OD & IS and then utilize nicely  packaged libraries/frameworks such as torchvision/MMDetection etc.  However, the actual how-to implementation of the models is ignored. I  don't want to code these directly from scratch, I am happy to use  PyTorch functions such as nn.conv2d, but I would like to have the  ability to create/customise models using these functions.

Also, if anyone has gone through this, what is a realistic time frame to learn this in your opinion?

Some extra info :

* I  am a PhD student, so I do have the time to completely commit myself to  this journey. I have a few pieces of work under my belt that apply SOTA  models, but I now want to experiment with developing a new method/model.
* I have gone through typical deep learning computer vision resources such as CS231n, Andrew Ng [DeepLearning.ai](https://deeplearning.ai/)  chapter on CNN, learn pytorch for deep learning: zero to mastery book  (this was great but again object detection was glossed over in terms of  implementing the models)
* I'm  also quite open to learning this directly with a framework, with a  preference for MMdetection, as I have 2 years of experience using it  (just using off-the-shelf models) . However, there is a lack of  documentation for the low-level stuff, and the comments are quite sparse  for the code in the low-levels.

Thanks in advance!"
187,deeplearning,open-ai,comments,2021-09-19 16:23:16,Measuring labor activity in manufacturing plant using AI: How to implement?,vaclavm,False,0.5,0,prb3hw,https://www.reddit.com/r/deeplearning/comments/prb3hw/measuring_labor_activity_in_manufacturing_plant/,3,1632068596.0,"I am a team member of a small startup manufacturing ""plant"" in Central Europe and we are currently solving issues with labor productivity.

We are looking for a solution to detect persons in three specific rectangular zones (see a picture) on a camera stream or a set of images (let's say at least 3-5 pictures per minute). As an output, I only need a percentage of time/pictures there was a person detected in each zone per hour. I was thinking of training a model to detect persons in these rectangular zones.

&#x200B;

[Example of camera snapshot](https://preview.redd.it/lo1jsgkejho71.jpg?width=1620&format=pjpg&auto=webp&s=3b3accef31a7cd69be48c260664769675cd1b47a)

As we are self-funded, we cannot afford corporate-grade products, so we are looking for a smart and preferably open source solution, or at least a not expensive one. I had an AI class at a college, so I know basic principles and I have some coding skills and lot of experience with DIY hacks using web services, apps and scripts, connecting them via APIs, etc., but I am not a programmer. I think I can create this myself instead of buying a product, but I am bit struggling, even though I googled a lot. I basically need a quick insight on how to get started and what technologies use... 

I know that DNN could do this. I am thinking of training my own DNN on a set of input classified pictures, because I'm afraid the camera resolution is not good enough for standard person detection mechanism. Nevertheless, I frankly don't know how and where to start.

Do you have any ideas?"
188,deeplearning,open-ai,comments,2024-02-16 03:20:25,Unbelieve New Text To Video AI Model By OpenAI - 37 Demo Videos - Still Can't Believe Real - Watch All Videos 4K With A Nice Music - This Will Change Perception Of Reality Forever,CeFurkan,False,0.64,4,1aryk48,https://www.youtube.com/watch?v=VlJYmHNRQZQ&deeplearning,3,1708053625.0,
189,deeplearning,open-ai,comments,2019-09-04 14:26:25,MSc programs in Deep Learning?,deepblue-,False,0.75,2,czll4f,https://www.reddit.com/r/deeplearning/comments/czll4f/msc_programs_in_deep_learning/,3,1567607185.0,"Hi, I graduated awhile with a BSc in Mathematics. I was also a research assistant in mathematical modelling and data analytics while in school. Unfortunately, I have not found a job in the field, usually applying for data analyst positions. I started doing the deeplearning.ai specialization on coursera and really enjoyed it -- almost finished, just finishing up the last course in the specialization. I plan on completing Andrew Ng's machine learning specialization and the tensorflow in practice specialization over the next little while while I work and save up money. I have doubts this will land me a job, but I remain hopeful. Most jobs seem to require at least an MSc and a lot of experience, at least where I live. I, at the very least, want to be able to show that I've been doing something while not in school and not working in the field when I go to apply for a program.

I have always wanted to go back to school for an MSc and maybe a PhD. I would really like to go back and do my MSc in deep learning, but I am having trouble finding programs. I was wondering if anyone knew any good programs for this? Particularly in Canada but I'm open to international. Would these programs be considered under computer science? The program I'd prefer would be more in-depth in the mathematics behind it.

EDIT: Found the perfect program for me https://www.ualberta.ca/computing-science/graduate-studies/programs-and-admissions/statistical-machine-learning"
190,deeplearning,open-ai,comments,2020-10-07 02:59:27,Wanting to contribute to projects or research,walid_idk,False,0.67,1,j6j9zc,https://www.reddit.com/r/deeplearning/comments/j6j9zc/wanting_to_contribute_to_projects_or_research/,3,1602039567.0,"Hi everyone, I hope you're safe and healthy.

I recently graduated and I worked on deep learning for my graduation project (arrhythmia classification using raw ECG signal and LSTM) And it was fun. I would like to continue on the domain of ML/DL and since borders are closed on my country (no AI related jobs here) I have plenty of time  before I start applying for jobs.

So if you have any projects or research you need help with or you know any great open source project that I can contribute to, please let me know. Thanks in advance 😊."
191,deeplearning,open-ai,comments,2023-04-02 12:37:38,[N] Software 3.0 Blog Post Release 🔥,DragonLord9,False,0.7,9,129k24i,https://www.reddit.com/r/deeplearning/comments/129k24i/n_software_30_blog_post_release/,3,1680439058.0,"Hi all, excited to share my blog post on [**Software 3.0**](https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm_campaign=post&utm_medium=web)

https://preview.redd.it/9b4hjkkhugra1.png?width=1500&format=png&auto=webp&s=e341f3ab4c3c8abb206df8daa17428a297ff61e2

The blog post offers an insightful read on the new GPT-powered programming paradigm where the new programming language is simply ""*English*"", as well as recent developments in AI.

The post was originally written before GPT-4 release, and the predictions seem to have held surprisingly well. Knowledge cutoff date 28 Feb 2023.

Please read and share!! Happy to answer any follow-ups here or on DM 😊

Tweet: [https://twitter.com/DivGarg9/status/1642229948185280521?s=20](https://twitter.com/DivGarg9/status/1642229948185280521?s=20)

Blog: [https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm\_campaign=post&utm\_medium=web](https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm_campaign=post&utm_medium=web)"
192,deeplearning,open-ai,comments,2023-01-05 20:07:38,Greg Yang's work on a rigorous mathematical theory for neural networks,IamTimNguyen,False,0.96,51,1048oc1,https://www.reddit.com/r/deeplearning/comments/1048oc1/greg_yangs_work_on_a_rigorous_mathematical_theory/,3,1672949258.0,"Greg Yang is a mathematician and AI researcher at Microsoft Research who for the past several years has done incredibly original theoretical work in the understanding of large artificial neural networks. In our whiteboard conversation, we get a sample of Greg's work, which goes under the name ""Tensor Programs"" and currently spans five highly technical papers. The route chosen to compress Tensor Programs into the scope of a conversational video is to place its main concepts under the umbrella of one larger, central, and time-tested idea: that of taking a large N limit. This occurs most famously in the Law of Large Numbers and the Central Limit Theorem, which then play a fundamental role in the branch of mathematics known as Random Matrix Theory (RMT). We review this foundational material and then show how Tensor Programs (TP) generalizes this classical work, offering new proofs of RMT.

We conclude with the applications of Tensor Programs to a (rare!) rigorous theory of neural networks. This includes applications to a rigorous proof for the existence of the Neural Network Gaussian Process and Neural Tangent Kernel for a general class of architectures, the existence of infinite-width feature learning limits, and the muP parameterization enabling hyperparameter transfer from smaller to larger networks.

&#x200B;

https://preview.redd.it/y79bih0f7aaa1.png?width=1280&format=png&auto=webp&s=7cf2bde3408e58f3d7dd6e15fbcd3dc103404147

https://preview.redd.it/0hvembyf7aaa1.png?width=1200&format=png&auto=webp&s=9a9889d47630e6c12cd4d192750c63d2bff1e422

Youtube: [https://youtu.be/1aXOXHA7Jcw](https://youtu.be/1aXOXHA7Jcw)

Apple Podcasts: [https://podcasts.apple.com/us/podcast/the-cartesian-cafe/id1637353704](https://podcasts.apple.com/us/podcast/the-cartesian-cafe/id1637353704)

Spotify: [https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG](https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG)

RSS: [https://feed.podbean.com/cartesiancafe/feed.xml](https://feed.podbean.com/cartesiancafe/feed.xml)"
193,deeplearning,open-ai,comments,2021-07-24 12:15:43,OpenAI's New Code Generator: GitHub Copilot (and Codex) | This AI Generates Code From Words,OnlyProggingForFun,False,0.25,0,oqov6e,https://youtu.be/az3oVVkTFB8,3,1627128943.0,
194,deeplearning,open-ai,comments,2023-02-11 06:59:00,⭕ New Open-Source Version Of ChatGPT,LesleyFair,False,0.86,37,10zepkt,https://www.reddit.com/r/deeplearning/comments/10zepkt/new_opensource_version_of_chatgpt/,3,1676098740.0,"GPT is getting competition from open-source.

A group of researchers, around the YouTuber [Yannic Kilcher](https://www.ykilcher.com/), have announced that they are working on [Open Assistant](https://github.com/LAION-AI/Open-Assistant). The goal is to produce a chat-based language model that is much smaller than GPT-3 while maintaining similar performance.

If you want to support them, they are crowd-sourcing training data [here](https://open-assistant.io/).

**What Does This Mean?**

Current language models are too big.

They require millions of dollars of hardware to train and use. Hence, access to this technology is limited to big organizations. Smaller firms and universities are effectively shut out from the developments.

Shrinking and open-sourcing models will facilitate academic research and niche applications.

Projects such as Open Assistant will help to make language models a commodity. Lowering the barrier to entry will increase access and accelerate innovation.

What an exciting time to be alive! 

Thank you for reading! I really enjoyed making this for you!  
The Decoding ⭕ is a thoughtful weekly 5-minute email that keeps you in the loop about machine research and the data economy. [Click here to sign up](https://thedecoding.net/)!"
195,deeplearning,open-ai,comments,2023-02-01 09:00:18,"Python wrapper of OpenAI's New AI classifier tool, which detects whether the paragraph was generated by ChatGPT, GPT models, or written by humans",StoicBatman,False,0.92,21,10qouv9,https://www.reddit.com/r/deeplearning/comments/10qouv9/python_wrapper_of_openais_new_ai_classifier_tool/,3,1675242018.0,"OpenAI has developed a new AI classifier tool which detects whether the content (paragraph, code, etc.) was generated by #ChatGPT, #GPT-based large language models or written by humans.

Here is a python wrapper of openai model to detect if a text is written by humans or generated by ChatGPT, GPT models

Github: [https://github.com/promptslab/openai-detector](https://github.com/promptslab/openai-detector)  
Openai release: [https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/](https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/)

If you are interested in #PromptEngineering, #LLMs, #ChatGPT and other latest research discussions, please consider joining our discord [discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)  


https://preview.redd.it/9d8ooeg2ljfa1.png?width=1358&format=png&auto=webp&s=0de7ccc5cd16d6bbad3dcfe7d8441547a6196fc8"
196,deeplearning,open-ai,comments,2021-03-30 10:41:10,"this is not overfitting but something else, right?",Sewing31,False,1.0,1,mgd1up,https://www.reddit.com/r/deeplearning/comments/mgd1up/this_is_not_overfitting_but_something_else_right/,3,1617100870.0,"I trained a VAE on a dataset containing 1k images. The VAE itself is convolutional, downsamples 256x256 rgb images four times before reconstruction and uses both relu and BatchNorm Layers as well as ResNet-like Skip Connection prior and after the bottleneck. Input is normalized to unit scale and no data augmentation takes place. The modal consists in total roughly of about 5e6 parameters and the visual inspection of the reconstructed images look decent enough. This is represented for both training and validation runs by the orange curve. No overfitting occurs as far as I can tell. However, if I use the entire dataset, i.e. \~ 35k training images and 5k validation images, the same model performs like shown by the green curve. Taking into account only the lower validation plot, I had said, this is overfitting, although it would have struck me as odd, that the same model that performed decently on the 1k images dataset now overfits on a much larger dataset. But also the training loss shows a clear inflection point around epoch 15.

https://preview.redd.it/1yh10yel95q61.png?width=846&format=png&auto=webp&s=8db4867ff3c6e334d2bf91a5addd80f29b0f50f6

Can somebody tell me a likely source of error here? Is the models capacity not large enough to sufficiently capture the complexity of the data? I dont think so, since I am using a s[ota convolutional VAE used by OpenAI](https://github.com/openai/DALL-E/blob/master/dall_e/encoder.py) for a dataset comprising millions of images.

The context is, that I am trying to learn a discrete vocabulary of latent codes, i.e. have a discrete learnable embedding to sort of quantize the otherwise continuous latent outputs of the encoder that are then used to reconstruct the input image via the decoder [cf. this code snippet](https://github.com/karpathy/deep-vector-quantization/blob/main/dvq/model/quantize.py). So the idea is not to generate random sampled from noise but to learn an efficient notebook, i.e. bottleneck that captures the essentials of the data set. The decoder then outputs a prob distribution for every pixel over the 255 possible values 8 bit images can take o. The KL (assuming a uniform prior to encourage uniform use of all possible vocabulary entries) is currently weighted with 1.

Any help would be appreciated. Thank you in advance"
197,deeplearning,open-ai,comments,2022-02-09 12:27:17,"Tiny ML for Big Hearts on an 8-bit Microcontroller Predict the possibility of arrhythmias on an 8- bit Microcontroller, without sending the corresponding sensor data to the cloud.",literallair,False,0.94,29,soceka,https://www.reddit.com/r/deeplearning/comments/soceka/tiny_ml_for_big_hearts_on_an_8bit_microcontroller/,3,1644409637.0,"**Things used in this project**  
***Hardware components:***  
Arduino Mega 2560  
***Software apps and online services:***  
Neuton Tiny Machine Learning  
**Story**

In the course of the pandemic, the interest in creating more innovative medical devices has run high, as recent years showed how unpredictable the situation in healthcare can be. Never before have we faced such an acute need for masks, ventilators, oxygen cylinders, and other must-have devices to conquer the pandemic.

All this has become a trigger to develop devices that can work autonomously for a long time, without access to the internet or cloud, just on batteries with ultra-low power consumption.

And most importantly, it's vital that such devices can be made by a wider range of people, even without in-depth technical skills. Perhaps, you’ve heard the story about two engineers from Lombardy who, at the peak of the epidemic in Italy, really saved their city as they began to print plastic valves for ventilators on 3D printers in their office and provided them to hospitals for free. The pandemic unified all, even those who were far from medicine before. You can read the full story from the same [source](https://www.forbes.com/sites/amyfeldman/2020/03/19/talking-with-the-italian-engineers-who-3d-printed-respirator-parts-for-hospitals-with-coronavirus-patients-for-free/?sh=63b2450778f1).

&#x200B;

https://preview.redd.it/fwyyj94ewsg81.png?width=587&format=png&auto=webp&s=cd61a972d17117dec160cb9e8c7162cde7473292

I would also love to share a simple example, so to say - a super user-friendly concept. My goal is to show that any user without data science knowledge at all can make the smallest medical devices smarter (yes, even an 8-bit microcontroller) with the help of tiny ML solutions. Let’s go! Introduction:

In this tutorial, I’d like to provide a vivid example of how the tiny ML approach can help to predict whether there is an impending arrhythmia or not, by running inferences on the microcontroller, without sending the corresponding sensor data to the cloud.

Let’s learn how to train and embed a compact machine learning model into the 8-bit ATmega2560 microcontroller. I deliberately chose such a memory-constrained and primitive microcontroller to show how simple and smart tiny ML devices can be.

Let's start by training a model. I took the original dataset from this resource: [https://www.physionet.org/content/ptbdb/1.0.0/](https://www.physionet.org/content/ptbdb/1.0.0/).

This dataset contains the signals of heart rate oscillations. The signals correspond to electrocardiogram (ECG) shapes of heartbeats for the normal case and the cases affected by different arrhythmias and myocardial infarction.

The goal was to detect abnormal heartbeats affected by arrhythmias and myocardial infarction based on electrocardiogram shapes. All the samples were cropped, downsampled, and padded with zeroes, if necessary, to the fixed dimension of 187.

The final element of each row denotes the class to which that example belongs.

**Features, target, and target metric:**

* 0...186 - sample description
* target - class of sample (0 - normal heartbeat, 1 - heartbeat affected by arrhythmias or myocardial infarction)

I combined all the cases into a CSV file and split it into a dataset for training (11, 641 rows), and a file to make an inference (2, 911 rows). Amplitudes of contractions of the heart muscle act as features for training the model. [Here](https://model.here/) you can download preprocessed training and test datasets that we used for model training and prediction on new data.

**Procedure.Step 1:TinyML Model Training**

For the AI part of my project, I chose the Neuton Tiny ML platform. Having a special algorithm under the hood, Neuton automatically creates an optimal model in terms of accuracy and compactness. And the best part - the model doesn’t need to be compressed (which is perfect since I needed a very small model that would support the 8-bit architecture).

Next, I uploaded a CSV file and selected the column that should be trained to predict. In my case, this was a column where it was indicated whether there was arrhythmia on the cardiogram or not (1 - yes and 0 - no). Since I needed to embed the model into an 8-bit microcontroller, I selected such a setting in the interface (8-bit support) and started the training. Everything happened automatically.

&#x200B;

[Tiny ML model training](https://reddit.com/link/soceka/video/12abdepfwsg81/player)

The model was trained. To assess its quality, I chose the Area Under the Curve. My model turned out really small and accurate:

*Area Under the Curve = 0.96, Model Size = 0.7 Kb, Number of Coefficients = 253.*

**Step 2:Embedding into a Microcontroller**

After that, I downloaded the archive. It appeared immediately upon the completion of training.

The archive contained:

* Information about the model

Files with weight and meta-information in two formats, binary, and HEX, are used in the calculation process.

* Calculator

A set of functions that is an add-on to Neuton's algorithm providing inferences. For instance, the calculator includes functions for loading a model, calling call-back functions like transferring data, receiving calculation results, etc.

* Neuton Library

An algorithm that performs calculations.

* Implementation file

A file in which you can set the logic of actions for the results of calculations based on your business requirements.

&#x200B;

[Embedding into Microcontroller](https://preview.redd.it/mq3ki06hwsg81.png?width=740&format=png&auto=webp&s=36b5ab6fc36b2cd36004bc5abc13eb7a25c9ac50)

As you see, the archive folder contained all the necessary files, that simply could be transferred to the microcontrollers firmware project.

Since I did not have a real cardiograph, I streamed data from a computer. To do this, I developed a simple protocol that consisted of a header, a data section, and a checksum. The packet header had the following structure:

typedef struct

{

uint16\_t preamble;

uint16\_t size;

uint8\_t type;

uint8\_t error;

uint8\_t reserved\[2\];

}

PacketHeader;

I also provided packets with information about the number of model inputs, data transfers for performing predictions, as well as a report on the memory consumed by the calculator RAM and Flash and prediction time:

typedef enum

{

TYPE\_ERROR = 0,

TYPE\_MODEL\_INFO,

TYPE\_DATASET\_INFO,

TYPE\_DATASET\_SAMPLE,

TYPE\_PERF\_REPORT,

}

PacketType;

Then I developed a packet parser that would receive a stream of bytes from the USB-UART interface of the system board as input and, upon receiving a packet with the correct checksum, will activate the callback function for processing data packets.

Let's open the user\_app.c file and create a neural network object:

Static NeuralNet neuralNet = { 0 };

To initialize the neural network object, I called the CalculatorInit function. Upon successful initialization, the callback function CalculatorOnInit was called, in which I loaded the model from the *model.c* file.

For prediction, I called the CalculatorRunInference function. This function, in its turn, activates three callback functions: before and after the prediction, as well as the one that contains the results of the prediction. I filled them in: in the CalculatorOnInferenceStart function I started, and in the CalculatorOnInferenceEnd function I stopped the timer and calculated the minimum, maximum, and average value of the prediction time.

In the CalculatorOnInferenceResult function, I analyzed the class probabilities for the presence/absence of arrhythmia. Upon its absence, I turned on the green LED, but if the arrhythmia was detected, it was the red one. I connected the LEDs to GPIO ports 52 and 53 and sent prediction results to the computer.

In the sketch file, I initialized the neural network object, packet parser, GPIO, and UART ports:

void setup()

{

pinMode(LED\_RED, OUTPUT);

pinMode(LED\_GREEN, OUTPUT);

led\_red(1);

led\_green(1);

initialised = (0 == app\_init());

initialised &= (0 == parser\_init(channel\_on\_valid\_packet, app\_inputs\_size()));

Serial.begin(230400);

}

And I wrote a code to call the parser when receiving data from the UART:

void loop()

{

if (!initialised)

{

while(1)

{

led\_red(1);

led\_green(0);

delay(100);

led\_red(0);

led\_green(1);

delay(100);

}

}

while (Serial.available() > 0)

parser\_parse(Serial.read());

}

Let's compile and upload the sketch to the system board (the ""Verify"" and ""Upload"" buttons). Success!

**Step 3:Running Inference on the Microcontroller**

To emulate the work of a cardiograph, I wrote a simple desktop application using the *libuv* library. The application performed the following actions:

* Sending the vector to the device on which the prediction took place
* Receiving a response from the device, regarding whether the sent cardiogram contained arrhythmia or not, and displaying the response

The interaction between the computer on which the application was running, and the microcontroller on which the prediction was performed occurred through the protocol that was described above in the article. Since the device was connected to the computer via a serial port, the communication took place in a binary format.I programmed the microcontroller so that when an arrhythmia was detected, I could see a red light, if not — then a green light lighted up. Find the link to a video showing how it works below.

&#x200B;

[Arrhythmia is not detected](https://reddit.com/link/soceka/video/t2atoxuiwsg81/player)

&#x200B;

[Arrhythmia is detected](https://reddit.com/link/soceka/video/q0ghyfekwsg81/player)

*Note: When performing similar operations using TensorFlow, we spend most of the time on manual selection of the neural network architecture and its parameters, model conversion, compression, and reduction of the number of operations but I still didn’t manage to embed the model into an 8-bit microcontroller.*  
**Conclusion.**

The pandemic has revealed that healthcare is in need of innovations, and I hope that a big boom in medical-edge devices awaits us. We need more devices that are not afraid of power and Internet outages. Devices that are very cheap and can be easily created by any guy in his office. Stay safe and have arrhythmia only from great love!"
198,deeplearning,open-ai,comments,2018-11-23 16:48:23,DeepBrain Chain opens up trial for AI cloud computing network.,DeepBrainChain,False,0.69,8,9zptn5,https://www.reddit.com/r/deeplearning/comments/9zptn5/deepbrain_chain_opens_up_trial_for_ai_cloud/,3,1542991703.0,"DeepBrain Chain has opened up our distributed AI Cloud training net to trial users for free over the next month.

We have 1, 2, 4 and 8GPU servers available for use.

You need to follow these steps to get free access:

1. Register on our website
2. Head to the AI training net portal on your personal account page
3. Click  AI training net from the side tab and install the DBC client -  set  your machine ID (click the little blue ? to find your machine ID)
4. You automatically receive 500 DBC (around 10 training hours on our 2GPU servers)

Github  guides/user manuals and resources are available through the  training  net/github portal from the training net page on DBC website."
199,deeplearning,open-ai,comments,2023-06-05 04:33:14,How Open Ai’s Andrej Karpathy Made One of the Best Tutorials in Deep Learning,0ssamaak0,False,0.92,62,141282u,https://www.reddit.com/r/deeplearning/comments/141282u/how_open_ais_andrej_karpathy_made_one_of_the_best/,3,1685939594.0,"I want you to check [my review](https://medium.com/@0ssamaak0/how-open-ais-andrej-karpathy-made-one-of-the-best-tutorials-in-deep-learning-e6b6445a2d05) on Andrej Karpathy amazing work on explaining how GPT is built

[GitHub Repo](https://github.com/0ssamaak0/Karpathy-Neural-Networks-Zero-to-Hero) for code & more details

&#x200B;

https://preview.redd.it/z204zwtzn44b1.png?width=720&format=png&auto=webp&s=095ea00991ebb295f48b70436456b1f283a50df1"
200,deeplearning,open-ai,relevance,2023-11-23 12:13:26,OpenAI Q* Rumours,MIKOLAJslippers,False,0.61,6,181zwb8,https://www.reddit.com/r/deeplearning/comments/181zwb8/openai_q_rumours/,30,1700741606.0,Anyone know any juicy rumours about the capabilities of this internal Q* project at OpenAI that has supposedly catalysed some of the recent dramatics?
201,deeplearning,open-ai,relevance,2024-02-19 09:22:12,open Ai CLIP,Life-Chard6717,False,0.5,0,1auj8ry,https://www.reddit.com/r/deeplearning/comments/1auj8ry/open_ai_clip/,0,1708334532.0,does it run on raspberry pi 4?
202,deeplearning,open-ai,relevance,2024-02-06 18:19:49,"Edgen: A Local, Open Source GenAI Server Alternative to OpenAI in Rust",EdgenAI,False,1.0,12,1akghpe,https://www.reddit.com/r/deeplearning/comments/1akghpe/edgen_a_local_open_source_genai_server/,0,1707243589.0,"⚡Edgen: Local, private GenAI server alternative to OpenAI. No GPU required. Run AI models locally: LLMs (Llama2, Mistral, Mixtral...), Speech-to-text (whisper) and many others.

Our goal with⚡Edgen is to make privacy-centric, local development accessible to more people, offering full compliance with OpenAI's API. It's made for those who prioritize data privacy and want to experiment with or deploy AI models locally with a Rust based infrastructure.

We'd love for this community to be among the first to try it out, give feedback, and contribute to its growth. 

Check it out here:  [GitHub - edgenai/edgen: ⚡ Edgen: Local, private GenAI server alternative to OpenAI. No GPU required. Run AI models locally: LLMs (Llama2, Mistral, Mixtral...), Speech-to-text (whisper) and many others.](https://github.com/edgenai/edgen) "
203,deeplearning,open-ai,relevance,2023-12-16 15:22:29,Is there any alternative for OpenAI API?,CrazyProgramm,False,0.88,25,18jtffj,https://www.reddit.com/r/deeplearning/comments/18jtffj/is_there_any_alternative_for_openai_api/,24,1702740149.0, So I am from Sri Lanka and our university is going to organize a competition and we need OpenAI API for it but we don't have money to afford it. Is there any alternative API you guys know 
204,deeplearning,open-ai,relevance,2023-11-16 08:28:46,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,0.84,68,17whz6e,https://v.redd.it/qx68wbuf7o0c1,7,1700123326.0,
205,deeplearning,open-ai,relevance,2023-11-16 08:28:39,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,1.0,1,17whz4d,https://v.redd.it/qx68wbuf7o0c1,0,1700123319.0,
206,deeplearning,open-ai,relevance,2023-11-04 12:53:57,Controlling Chrome browser using OpenAI APIs,smtabatabaie,False,1.0,2,17nl1v4,https://www.reddit.com/r/deeplearning/comments/17nl1v4/controlling_chrome_browser_using_openai_apis/,0,1699102437.0,"Hi, I wanted to develop a Chrome extension that can do some tasks on the browser for users using OpenAI APIs. I wanted to ask if you guys know any ways to create something that has access to users' browsers and can accomplish tasks. Will really appreciate any help. Thanks"
207,deeplearning,open-ai,relevance,2023-08-15 04:46:43,OpenAI Notebooks which are really helpful.,vishank97,False,0.94,20,15rihgo,https://www.reddit.com/r/deeplearning/comments/15rihgo/openai_notebooks_which_are_really_helpful/,3,1692074803.0,"The OpenAI cookbook is one of the most underrated and underused developer resources available today. Here are 7 notebooks you should know about:

1. Improve LLM reliability:  
[https://github.com/openai/openai-cookbook/blob/main/techniques\_to\_improve\_reliability.md](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)
2. Embedding long text inputs:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Embedding\_long\_inputs.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb)
3. Dynamic masks with DALLE:  
[https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How\_to\_create\_dynamic\_masks\_with\_DALL-E\_and\_Segment\_Anything.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/dalle/How_to_create_dynamic_masks_with_DALL-E_and_Segment_Anything.ipynb)
4. Function calling to find places nearby:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Function\_calling\_finding\_nearby\_places.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Function_calling_finding_nearby_places.ipynb)
5. Visualize embeddings in 3D:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing\_embeddings\_in\_3D.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_3D.ipynb)
6. Pre and post-processing of Whisper transcripts:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Whisper\_processing\_guide.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb)
7. Search, Retrieval, and Chat:  
[https://github.com/openai/openai-cookbook/blob/main/examples/Question\_answering\_using\_a\_search\_API.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_a_search_API.ipynb)

Big thanks to the creators of these notebooks!"
208,deeplearning,open-ai,relevance,2024-02-13 14:38:25,"This is my foundation block or sandwich stack AI model... Criticisms and inputs are welcome, formulated by openAI scholarGPT - thanks in advance.",AskACapperDOTcom,False,0.25,0,1apuyv4,/r/OpenAI/comments/1apf8jy/this_is_my_foundation_block_or_sandwich_stack_ai/,10,1707835105.0,
209,deeplearning,open-ai,relevance,2023-05-09 18:05:08,"Building with LLMs, ChatGPT, and Working at OpenAI With Logan Kilpatrick (Dev Rel @OpenAI) - What's AI episode 11",OnlyProggingForFun,False,0.45,0,13d13qo,https://youtu.be/zz4U3X3PD4s,0,1683655508.0,
210,deeplearning,open-ai,relevance,2023-11-10 13:55:32,"Order in which OpenAI ""short courses"" should be taken",KA_IL_AS,False,0.67,3,17s4irx,https://www.reddit.com/r/deeplearning/comments/17s4irx/order_in_which_openai_short_courses_should_be/,2,1699624532.0,"As you all know OpenAI has released a whole lot of  ""Short Courses"" lately and they're good too. I've taken their prompt engineering course months ago when it was released, it was super helpful.  
But here's the thing they've released a lot of courses after that, and now I don't know in what order I should be taking them.  
Any thoughts and advices on this ? It'll be super helpful"
211,deeplearning,open-ai,relevance,2024-01-23 20:03:12,How to create a project using Langchain which will utilise OpenAI api,Rare-Breed420,False,0.25,0,19dxhp4,https://www.reddit.com/r/deeplearning/comments/19dxhp4/how_to_create_a_project_using_langchain_which/,1,1706040192.0,
212,deeplearning,open-ai,relevance,2023-05-17 02:26:44,OpenAI CEO asking for government's license for building AI . WHAT THE ACTUAL FUCK?,Angry_Grandpa_,False,0.83,24,13joq2b,/r/singularity/comments/13jbc76/openai_ceo_asking_for_governments_license_for/,8,1684290404.0,
213,deeplearning,open-ai,relevance,2024-02-21 15:00:34,"⚡Edgen now supports Vulkan, CUDA and Metal | Open Source and local GenAI server alternative to OpenAI's API. Supports all GGUF models, across Windows, Mac and Linux with one 30MB download.",EdgenAI,False,0.81,3,1awe3tn,https://www.reddit.com/r/deeplearning/comments/1awe3tn/edgen_now_supports_vulkan_cuda_and_metal_open/,0,1708527634.0,"Our goal with⚡Edgen is to make privacy-centric, local GenAI app development accessible to more people.

It is compliant with OpenAI's API and built in  🦀 Rust so it can be natively compiled into Windows, Linux and MacOS (with a 30MB executable).

We'd love for this community to be among the first to try it out and provide feedback!

Check out⚡Edgen on GitHub: [GitHub - edgenai/edgen: ⚡ Edgen: Local, private GenAI server alternative to OpenAI.](https://github.com/edgenai/edgen)

And keep an an eye out for future releases:

* Speech to Text
* Embeddings Endpoint
* Multimodal Endpoint
* Text to Image Endpoint

&#x200B;"
214,deeplearning,open-ai,relevance,2023-06-15 11:39:53,OpenAI function calling - tutorial,mildlyoverfitted,False,0.78,5,14a06nr,https://youtu.be/_B7F_6nTVEg,0,1686829193.0,
215,deeplearning,open-ai,relevance,2023-12-21 18:13:33,Langchain vs. LlamaIndex vs. OpenAI GPTs: Which one should you use?,OnlyProggingForFun,False,0.75,2,18ntbqc,https://youtu.be/g84uWgVXVYg,1,1703182413.0,
216,deeplearning,open-ai,relevance,2023-11-15 10:30:36,"GPT-4 Turbo: Die Zukunft der Künstlichen Intelligenz, entwickelt von OpenAI",Webglobic_tech,False,0.86,16,17vqtlk,https://webglobic.com/magazine/,0,1700044236.0,
217,deeplearning,open-ai,relevance,2023-01-25 22:06:47,OpenAi's breakthrough,bradasm,False,0.21,0,10lb7k3,https://www.reddit.com/r/deeplearning/comments/10lb7k3/openais_breakthrough/,2,1674684407.0,[https://twitter.com/make\_mhe/status/1618255363580755968](https://twitter.com/make_mhe/status/1618255363580755968)
218,deeplearning,open-ai,relevance,2023-01-27 10:45:48,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.95,120,10mhyek,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
219,deeplearning,open-ai,relevance,2023-04-12 05:21:13,Is OpenAI’s Study On The Labor Market Impacts Of AI Flawed?,LesleyFair,False,0.94,25,12jb4xz,https://www.reddit.com/r/deeplearning/comments/12jb4xz/is_openais_study_on_the_labor_market_impacts_of/,1,1681276873.0,"[Example img\_name](https://preview.redd.it/f3hrmeet1eta1.png?width=1451&format=png&auto=webp&s=20e20b142a2f88c3d495177e540f34bc8ea4312b)

We all have heard an uncountable amount of predictions about how AI will ***terk err jerbs!***

However, here we have a proper study on the topic from OpenAI and the University of Pennsylvania. They investigate how Generative Pre-trained Transformers (GPTs) could automate tasks across different occupations \[1\].

Although I’m going to discuss how the study comes with a set of “imperfections”, the findings still make me really excited. The findings suggest that machine learning is going to deliver some serious productivity gains.

People in the data science world fought tooth and nail for years to squeeze some value out of incomplete data sets from scattered sources while hand-holding people on their way toward a data-driven organization. At the same time, the media was flooded with predictions of omniscient AI right around the corner.

*Let’s dive in and take an* exciting glimpse into the future of labor markets\*!\*

# What They Did

The study looks at all US occupations. It breaks them down into tasks and assesses the possible level of for each task. They use that to estimate how much automation is possible for a given occupation.

The researchers used the [O\*NET database,](https://www.onetcenter.org/database.html) which is an occupation database specifically for the U.S. market. It lists 1,016 occupations along with its standardized descriptions of tasks.

The researchers annotated each task once manually and once using GPT-4. Thereby, each task was labeled as either somewhat (<50%) or significantly (>50%) automatable through LLMs. In their judgment, they considered both the direct “exposure” of a task to GPT as well as to a secondary GPT-powered system, e.g. LLMs integrated with image generation systems.

To reiterate, a higher “exposure” means that an occupation is more likely to get automated.

Lastly, they enriched the occupation data with wages and demographic information. This was used to determine whether e. g. high or low-paying jobs are at higher risk to be automated.

So far so good. This all sounds pretty decent. Sure, there is a lot of qualitative judgment going into their data acquisition process. However, we gotta cut them some slag. These kinds of studies always struggle to get any hard data and so far they did a good job.

However, there are a few obvious things to criticize. But before we get to that let’s look at their results.

# Key Findings

The study finds that 80% of the US workforce, across all industries, could have at least some tasks affected. Even more significantly, 19% of occupations are expected to have at least half of their tasks significantly automated!

Furthermore, they find that higher levels of automation exposure are associated with:

* Programming and writing skills
* Higher wages (contrary to previous research!)
* Higher levels of education (Bachelor’s and up)

Lower levels of exposure are associated with:

* Science and critical thinking skills
* Manual work and tasks that might potentially be done using physical robots

This is somewhat unsurprising. We of course know that LLMs will likely not increase productivity in the plumbing business. However, their findings underline again how different this wave is. In the past, simple and repetitive tasks fell prey to automation.

*This time it’s the suits!*

If we took this study at face value, many of us could start thinking about life as full-time pensioners.

But not so fast! This, like all the other studies on the topic, has a number of flaws.

# Necessary Criticism

First, let’s address the elephant in the room!

OpenAI co-authored the study. They have a vested interest in the hype around AI, both for commercial and regulatory reasons. Even if the external researchers performed their work with the utmost thoroughness and integrity, which I am sure they did, the involvement of OpenAI could have introduced an unconscious bias.

*But there’s more!*

The occupation database contains over 1000 occupations broken down into tasks. Neither GPT-4 nor the human labelers can possibly have a complete understanding of all the tasks across all occupations. Hence, their judgment about how much a certain task can be automated has to be rather hand-wavy in many cases.

Flaws in the data also arise from the GPT-based labeling itself.

The internet is flooded with countless sensationalist articles about how AI will replace jobs. It is hard to gauge whether this actually causes GPT models to be more optimistic when it comes to their own impact on society. However, it is possible and should not be neglected.

The authors do also not really distinguish between labor-augmenting and labor-displacing effects and it is hard to know what “affected by” or “exposed to LLMs” actually means. Will people be replaced or will they just be able to do more?

Last but not least, lists of tasks most likely do not capture all requirements in a given occupation. For instance ""making someone feel cared for"" can be an essential part of a job but might be neglected in such a list.

# Take-Away And Implications

GPT models have the world in a frenzy - rightfully so.

Nobody knows whether 19% of knowledge work gets heavily automated or if it is only 10%.

As the dust settles, we will begin to see how the ecosystem develops and how productivity in different industries can be increased. Time will tell whether foundational LLMs, specialized smaller models, or vertical tools built on top of APIs will be having the biggest impact.

In any case, these technologies have the potential to create unimaginable value for the world. At the same time, change rarely happens without pain. I strongly believe in human ingenuity and our ability to adapt to change. All in all, the study - flaws aside - represents an honest attempt at gauging the future.

Efforts like this and their scrutiny are our best shot at navigating the future. Well, or we all get chased out of the city by pitchforks.

Jokes aside!

What an exciting time for science and humanity!

As always, I really enjoyed making this for you and I sincerely hope you found value in it!

If you are not subscribed to the newsletter yet, [click here to sign up](https://thedecoding.net/)! I send out a thoughtful 5-minute email every week to keep you in the loop about machine learning research and the data economy.

*Thank you for reading and I see you next week ⭕!*

**References:**

\[1\] [https://arxiv.org/abs/2303.10130](https://arxiv.org/abs/2303.10130)"
220,deeplearning,open-ai,relevance,2023-05-11 02:01:13,OpenAI & GPT Dictionary of Vocabulary. Generative AI Terms To Know In 2023,OnlyProggingForFun,False,0.75,4,13ea348,https://youtu.be/q4G6X09NEu4,1,1683770473.0,
221,deeplearning,open-ai,relevance,2023-08-28 07:12:44,OpenAI introduces fine-tuning capabilities for GPT-3.5 Turbo,intengineering,False,1.0,4,163f3fp,https://interestingengineering.com/innovation/openai-introduces-fine-tuning-capabilities-for-gpt-35-turbo,1,1693206764.0,
222,deeplearning,open-ai,relevance,2023-09-26 18:24:05,"OpenAI’s GPT-4 with vision still has flaws, paper reveals",Nalix01,False,0.33,0,16svoeg,https://www.reddit.com/r/deeplearning/comments/16svoeg/openais_gpt4_with_vision_still_has_flaws_paper/,1,1695752645.0,"OpenAI initially promoted GPT-4's ability to analyze and interpret images alongside text, but has since limited these features due to concerns about misuse and privacy. A recent paper sheds light on the efforts to mitigate these issues and the ongoing challenges GPT-4 faces in interpreting images accurately and responsibly.

If you want to stay ahead of the curve in AI and tech, [look here first](https://dupple.com/techpresso?utm_source=reddit&utm_medium=social&utm_campaign=post).

**Image Analysis Concerns**

* **Abuse and Privacy Issues:** OpenAI limited GPT-4's image features due to potential misuse and privacy violations.
* **Mitigation Efforts:** The company is working on safeguards to prevent malicious use and bias in GPT-4’s image analysis.

**Performance Issues**

* **Inaccurate Inferences:** GPT-4V can make incorrect inferences, combining text strings wrongly and missing details.
* **Identification Issues:** Struggles with identifying dangerous substances or chemicals and gives wrong medical imaging responses.

**Discrimination and Bias**

* **Misunderstood Symbols:** GPT-4V doesn't grasp the nuances of certain hate symbols.
* **Discrimination:** Shows bias against certain sexes and body types, relating responses mainly to body weight and body positivity.

[Source (Tech Crunch)](https://techcrunch.com/2023/09/26/openais-gpt-4-with-vision-still-has-flaws-paper-reveals/#:~:text=The%20paper%20reveals%20that%20GPT,facts%20in%20an%20authoritative%20tone)

**PS:** **If you enjoyed this post**, you’ll love my [ML-powered newsletter](https://dupple.com/techpresso?utm_source=reddit&utm_medium=social&utm_campaign=post) that summarizes the best AI/tech news from 50+ media. It’s already being read by **7,000+** **professionals** from **OpenAI, Google, Meta**…"
223,deeplearning,open-ai,relevance,2023-12-15 04:50:13,OpenAI's Next Move: ChatGPT 4.5 Upgrade in the Works? Sam Altman Clarifies,Damanjain,False,0.11,0,18is4sm,https://thebuzz.news/article/openai-chatgpt-4-5-leak/11787/,1,1702615813.0,
224,deeplearning,open-ai,relevance,2024-01-30 12:52:34,Open source tools in Data Centric AI,ifcarscouldspeak,False,1.0,1,1aenplq,/r/DataCentricAI/comments/1ael9ql/open_source_tools_in_dcai_to_try_this_week/,0,1706619154.0,
225,deeplearning,open-ai,relevance,2023-06-18 21:47:43,Demystifying OpenAI Procurement: An Overview of Processes and Pricing,digital-bolkonsky,False,0.78,5,14cw6o0,/r/ai_cost/comments/14cw61j/demystifying_openai_procurement_an_overview_of/,0,1687124863.0,
226,deeplearning,open-ai,relevance,2023-03-10 13:52:36,OpenAI's Python API walk-through,Combination-Fun,False,0.67,1,11npswy,https://www.reddit.com/r/deeplearning/comments/11npswy/openais_python_api_walkthrough/,0,1678456356.0,"If you are getting started with OpenAI's newest Python API, you don't have to spend too much time familiarising yourself with how to use it. Here is a video that walks through the different possiblities, API parameters and finally shares a demo app: [https://youtu.be/dcnfhtuL7qA](https://youtu.be/dcnfhtuL7qA) 

Hope its useful. 

https://preview.redd.it/z1aczihy2xma1.png?width=1920&format=png&auto=webp&s=d280af7e119ceac77bfdd9196add395037721b8f"
227,deeplearning,open-ai,relevance,2023-04-08 15:25:44,[P] Blog post explaining CLIP by OpenAI,pmgautam_,False,0.57,1,12fpa1l,/r/MachineLearning/comments/12fp9nq/p_blog_post_explaining_clip_by_openai/,0,1680967544.0,
228,deeplearning,open-ai,relevance,2023-10-14 08:45:17,Open Source AI Book,HorrorNo8851,False,0.67,1,177l2g1,https://www.reddit.com/r/deeplearning/comments/177l2g1/open_source_ai_book/,0,1697273117.0,"*Clarity in the current fast-paced mess of Open Source innovation*

Check it out: [https://github.com/premAI-io/state-of-open-source-ai](https://github.com/premAI-io/state-of-open-source-ai)"
229,deeplearning,open-ai,relevance,2022-12-08 20:36:26,Daath AI Parser is an open-source application that uses OpenAI to parse visible text of HTML elements.,softcrater,False,1.0,11,zgarkj,https://github.com/kagermanov27/daath-ai-parser,0,1670531786.0,
230,deeplearning,open-ai,relevance,2023-06-05 04:33:14,How Open Ai’s Andrej Karpathy Made One of the Best Tutorials in Deep Learning,0ssamaak0,False,0.93,61,141282u,https://www.reddit.com/r/deeplearning/comments/141282u/how_open_ais_andrej_karpathy_made_one_of_the_best/,3,1685939594.0,"I want you to check [my review](https://medium.com/@0ssamaak0/how-open-ais-andrej-karpathy-made-one-of-the-best-tutorials-in-deep-learning-e6b6445a2d05) on Andrej Karpathy amazing work on explaining how GPT is built

[GitHub Repo](https://github.com/0ssamaak0/Karpathy-Neural-Networks-Zero-to-Hero) for code & more details

&#x200B;

https://preview.redd.it/z204zwtzn44b1.png?width=720&format=png&auto=webp&s=095ea00991ebb295f48b70436456b1f283a50df1"
231,deeplearning,open-ai,relevance,2024-02-16 03:20:25,Unbelieve New Text To Video AI Model By OpenAI - 37 Demo Videos - Still Can't Believe Real - Watch All Videos 4K With A Nice Music - This Will Change Perception Of Reality Forever,CeFurkan,False,0.64,4,1aryk48,https://www.youtube.com/watch?v=VlJYmHNRQZQ&deeplearning,3,1708053625.0,
232,deeplearning,open-ai,relevance,2024-02-05 12:26:35,Is there any tutorial/guide on how to efficiently deploy hugging face open-source LLMs? both for testing and then in production? what is the cost I can expect compared to OpenAI API?,HappyDataGuy,False,1.0,2,1ajfaxw,https://www.reddit.com/r/deeplearning/comments/1ajfaxw/is_there_any_tutorialguide_on_how_to_efficiently/,2,1707135995.0,
233,deeplearning,open-ai,relevance,2022-10-06 01:31:57,OpenAI's Most Recent Model: Whisper (explained),OnlyProggingForFun,False,0.71,3,xwsibr,https://youtu.be/uFOkMme19Zs,1,1665019917.0,
234,deeplearning,open-ai,relevance,2023-09-02 20:51:00,Open source AI biolab early days,Known_Proof_8037,False,1.0,5,168dh1s,https://www.reddit.com/r/deeplearning/comments/168dh1s/open_source_ai_biolab_early_days/,0,1693687860.0,"I'm quite new to biotech/drug discovery and don't have money for my own lab, so I decided maybe it's possible to create one with AI/ML stuff. I created this open source project with a friend of mine [https://github.com/BasedLabs/NoLabs](https://github.com/BasedLabs/NoLabs)

It's not like a real promotion, but I'd appreciate github stars haha

If someone checks this out with good knowledge of how the industry works and leaves some feedback it'd be just awesome, hope to make this project useful for people (but still can't understand what direction to take without feedback, that's why I'm posting here)"
235,deeplearning,open-ai,relevance,2023-08-15 15:09:36,How to run OpenAI CLIP with UI for Image Retrieval and Filtering your dataset - Supervisely,tdionis,False,1.0,2,15rvdc7,https://supervisely.com/blog/openai-clip-for-image-retrieval-and-filtering-computer-vision-datasets-tutorial/,0,1692112176.0,
236,deeplearning,open-ai,relevance,2022-02-12 16:58:26,OpenAI Embeddings API,HenryAILabs,False,1.0,2,sqvwgq,https://www.reddit.com/r/deeplearning/comments/sqvwgq/openai_embeddings_api/,0,1644685106.0,"Hey everyone! I recently had the opportunity to interview Arvind Neelakantan from OpenAI about these ideas related to their Embeddings API. This video summarizes my takeaways and provides background to the key points discussed on the podcast. I really hope you find this informative / useful!  
[https://youtu.be/vyf2sqhAm4Y](https://youtu.be/vyf2sqhAm4Y)"
237,deeplearning,open-ai,relevance,2024-01-26 15:49:19,"I want to create a text analysis app using Langchain with Open AI api ,can some give me rough steps I can take to build one",Rare-Breed420,False,0.44,0,1able41,https://www.reddit.com/r/deeplearning/comments/1able41/i_want_to_create_a_text_analysis_app_using/,2,1706284159.0,
238,deeplearning,open-ai,relevance,2022-11-30 19:14:16,OpenAI's new impressive Conversational LLM - ChatGPT,dulldata,False,1.0,1,z90966,https://www.youtube.com/watch?v=2VJZky25rIs,0,1669835656.0,
239,deeplearning,open-ai,relevance,2023-03-02 07:25:30,Good news for builders! OpenAI Releases APIs To ChatGPT and Whisper,LesleyFair,False,0.69,6,11fwcxf,https://www.reddit.com/r/deeplearning/comments/11fwcxf/good_news_for_builders_openai_releases_apis_to/,0,1677741930.0,"If you were as disappointed as I was when you saw that access to Meta's LLaMA models is limited to researchers, you are going to like this.  


[APIs to ChatGPT and OpenAI's speech-to-text model whisper](https://openai.com/blog/introducing-chatgpt-and-whisper-apis) are available as of yesterday. Through system-wide optimizations, they claim to have reduced inference costs by 90%. They now price ChatGPT at $0.002 per 1000 tokens. Dedicated instances are available for speedup and make economic sense if you process \~450M tokens a day.  


Machine learning progress continues to be as fast as a banana peal skating on warm vaseline. 

If you found this useful and want to stay in the loop, consider subscribing to The Decoding. I send out a weekly 5-minute newsletter that keeps professionals in the loop about machine learning and the data economy. [Click here to subscribe!](https://thedecoding.net/)"
240,deeplearning,open-ai,relevance,2023-10-19 07:35:32,Any good open sorce ai voce cloning for voicechangers?,Which-Temperature-94,False,0.4,0,17bdj92,https://www.reddit.com/r/deeplearning/comments/17bdj92/any_good_open_sorce_ai_voce_cloning_for/,0,1697700932.0,"I want to recreate my friends voices using a voice changer, are there any good open sorce github voice cloners?"
241,deeplearning,open-ai,relevance,2022-12-04 09:59:19,OpenAI’s ChatGPT is unbelievable good in telling stories!,Far_Pineapple770,False,0.33,0,zc5tc3,/r/MachineLearning/comments/zc5sg6/d_openais_chatgpt_is_unbelievable_good_in_telling/,0,1670147959.0,
242,deeplearning,open-ai,relevance,2022-09-23 16:28:06,OpenAI Whisper: SOTA Speech To Text With Microphone Demo,l33thaxman,False,0.86,9,xm26oq,https://www.reddit.com/r/deeplearning/comments/xm26oq/openai_whisper_sota_speech_to_text_with/,0,1663950486.0,"OpenAI has released a Speech To Text model that nears human performance.  This video goes over the basics of the model, as well as how to run it with a microphone.

[https://youtu.be/nwPaRSlDSaY](https://youtu.be/nwPaRSlDSaY)"
243,deeplearning,open-ai,relevance,2024-01-23 15:33:19,Open Source AI with Vinod Valloppillil and Bob van Luijt - Weaviate Podcast #86!,CShorten,False,1.0,1,19dr1j7,https://www.reddit.com/r/deeplearning/comments/19dr1j7/open_source_ai_with_vinod_valloppillil_and_bob/,0,1706023999.0,"Open-Source LLMs are currently one of the biggest trends in AI to be preparing for! Vinod has a remarkable history with open-source in software, having famously published the ""Halloween Documents"", a set of internal strategy memos at Microsoft explaining the oncoming wave of open-source. Vinod shares some fascinating insights on how building business around open-source has evolved and then Bob sparks the debate around ""stateless"" models.

Bob has been intensely exploring the new business philosophy around AI models, making connections to traditional ways of thinking about stateless and stateful software. For example, an mp3 file of a song is stateless, whereas say, your playlist, and the application that serves it, is stateful. AI models sit at an interesting intersection of this, the model itself is stateless, but once you connect it to your data with RAG (Retrieval-Augmented Generation), for example, you make it stateful. The podcast discusses this viewpoint and the nuances of it. Vinod pushes back on the analogy, preferring to describe the models as ""pre-baked"". I think this is such an interesting topic on the future of value capture in AI models, curious what people think!

The podcast then continues to discuss the future of RAG. Vinod outlines several directions the technology may emerge, such as joint end-to-end training, training one model or the other, models tailor made for attending to retrieved context, task-specific models, and the resurgence of knowledge graphs and explicit representations of knowledge. This sparks further debate around implicit / explicit representations of knowledge in LLMs and RAG systems, and the exciting, but counter-intuitive research around knowledge editing such as ROME, MEMIT, GRACE, ...

I hope this quick synopsis inspires your interest in the podcast! There are many other topics covered such as MemGPT and Gorilla, DSPy, and Generative Feedback Loops!

YouTube: [https://www.youtube.com/watch?v=ySNX2cPh5Tk](https://www.youtube.com/watch?v=ySNX2cPh5Tk)

Spotify: [https://spotifyanchor-web.app.link/e/v4FIPyf5AGb](https://spotifyanchor-web.app.link/e/v4FIPyf5AGb)"
244,deeplearning,open-ai,relevance,2022-08-09 16:28:47,Microsoft Announces new Integrations with OpenAI and MLFlow,mhamilton723,False,1.0,2,wk7r6l,https://www.reddit.com/r/deeplearning/comments/wk7r6l/microsoft_announces_new_integrations_with_openai/,0,1660062527.0,"Today Microsoft launched SynapseML v0.10.0 with 175-billion parameter OpenAI language models, full support for .NET, Python, R, Scala, and Java, and an integration with MLflow, and much more. Check out the full release notes, leave a star, and explore SnyapseML

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.10.0](https://github.com/microsoft/SynapseML/releases/tag/v0.10.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/exciting-new-release-of-synapseml/ba-p/3589606](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/exciting-new-release-of-synapseml/ba-p/3589606)

&#x200B;

https://preview.redd.it/4sfexdqnspg91.jpg?width=4125&format=pjpg&auto=webp&s=d894599f7bf6fa5dd3289c23133903260d00de96"
245,deeplearning,open-ai,relevance,2023-09-18 11:30:42,"Intel OpenVINO 2023.1.0 released, open-source toolkit for optimizing and deploying AI inference",reps_up,False,1.0,1,16lsz93,https://github.com/openvinotoolkit/openvino,0,1695036642.0,
246,deeplearning,open-ai,relevance,2021-12-29 08:03:22,I wrote a program with OpenAI's Codex that fixes errors,tomd_96,False,0.94,99,rr2wme,https://v.redd.it/jupdtry6vf881,6,1640765002.0,
247,deeplearning,open-ai,relevance,2023-03-16 00:03:09,OpenAI's GPT 4 is out and it's multimodal! What we know so far,gordicaleksa,False,0.29,0,11sdx6l,https://www.youtube.com/watch?v=FY9Nlkoq4GI&t=2s&ab_channel=AleksaGordi%C4%87-TheAIEpiphany,1,1678924989.0,
248,deeplearning,open-ai,relevance,2022-07-16 16:09:00,How OpenAI Reduces risks for DALL·E 2,OnlyProggingForFun,False,1.0,1,w0k20k,https://youtu.be/qh3_DnteGD0,0,1657987740.0,
249,deeplearning,open-ai,relevance,2022-04-12 18:14:26,"DALL-E 2, the future of AI research, and OpenAI’s business model",bendee983,False,0.82,7,u25kcq,https://bdtechtalks.com/2022/04/11/openai-dall-e-2/,0,1649787266.0,
250,deeplearning,open-ai,relevance,2023-03-29 14:13:46,AI Startup Cerebras releases open source ChatGPT-like alternative models,Time_Key8052,False,0.94,45,125pbbf,https://gpt4chatgpt.tistory.com/entry/Cerebras-releases-open-source-ChatGPT-like-alternative-models,14,1680099226.0,
251,deeplearning,open-ai,relevance,2022-09-22 19:32:49,OpenAI Whisper powered Gradio App for Automatic Subtitle Video Generation,dulldata,False,1.0,4,xlawi6,https://www.youtube.com/watch?v=x_uxzgTg1U0,0,1663875169.0,
252,deeplearning,open-ai,relevance,2022-10-25 09:22:34,Understanding OpenAI's new speech recognition system that can beat humans,Difficult-Race-1188,False,0.63,2,yd0cfh,https://www.reddit.com/r/deeplearning/comments/yd0cfh/understanding_openais_new_speech_recognition/,1,1666689754.0,[https://medium.com/aiguys/openai-whisper-robust-speech-recognition-c103daf9add](https://medium.com/aiguys/openai-whisper-robust-speech-recognition-c103daf9add)
253,deeplearning,open-ai,relevance,2021-07-28 17:45:57,"OpenAI Releases Triton, An Open-Source Python-Like GPU Programming Language For Neural Networks",techsucker,False,0.95,68,otf0fs,https://www.reddit.com/r/deeplearning/comments/otf0fs/openai_releases_triton_an_opensource_pythonlike/,5,1627494357.0,"OpenAI released their newest language, [Triton](https://github.com/openai/triton). This open-source programming language that enables researchers to write highly efficient GPU code for AI workloads is Python-compatible and comes with the ability of a user to write in as few as 25 lines, something on par with what an expert could achieve. OpenAI claims this makes it possible to reach peak hardware performance without much effort, making creating more complex workflows easier than ever before!

Researchers in the field of Deep Learning often rely on native framework operators. However, this can be problematic because it requires many temporary tensors to work, which may hurt performance at scale for neural networks. Writing specialized GPU kernels is a more convenient solution, but surprisingly difficult due to intricacies when programming them according to GPUs. It was challenging to find a system that provides the flexibility and speed required while also being easy enough for developers to understand. This has led researchers at OpenAI in improving Triton, which was initially founded by one of their teammates.

Quick Read: [https://www.marktechpost.com/2021/07/28/openai-releases-triton-an-open-source-python-like-gpu-programming-language-for-neural-networks/](https://www.marktechpost.com/2021/07/28/openai-releases-triton-an-open-source-python-like-gpu-programming-language-for-neural-networks/) 

Paper: http://www.eecs.harvard.edu/\~htk/publication/2019-mapl-tillet-kung-cox.pdf

Github: https://github.com/openai/triton"
254,deeplearning,open-ai,relevance,2023-02-01 09:00:18,"Python wrapper of OpenAI's New AI classifier tool, which detects whether the paragraph was generated by ChatGPT, GPT models, or written by humans",StoicBatman,False,0.86,19,10qouv9,https://www.reddit.com/r/deeplearning/comments/10qouv9/python_wrapper_of_openais_new_ai_classifier_tool/,3,1675242018.0,"OpenAI has developed a new AI classifier tool which detects whether the content (paragraph, code, etc.) was generated by #ChatGPT, #GPT-based large language models or written by humans.

Here is a python wrapper of openai model to detect if a text is written by humans or generated by ChatGPT, GPT models

Github: [https://github.com/promptslab/openai-detector](https://github.com/promptslab/openai-detector)  
Openai release: [https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/](https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/)

If you are interested in #PromptEngineering, #LLMs, #ChatGPT and other latest research discussions, please consider joining our discord [discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)  


https://preview.redd.it/9d8ooeg2ljfa1.png?width=1358&format=png&auto=webp&s=0de7ccc5cd16d6bbad3dcfe7d8441547a6196fc8"
255,deeplearning,open-ai,relevance,2023-03-03 23:02:37,Meta new large lanugage model (similar to OpenAI one) called LLaMA is leaked via torrent,aipaintr,False,0.92,9,11hhzeg,https://github.com/facebookresearch/llama/pull/73/files,2,1677884557.0,
256,deeplearning,open-ai,relevance,2023-05-29 19:09:29,Open Source AI and the Challenges Ahead,chainless-coder,False,0.81,3,13v3lll,https://blog.opncbr.com/post/open_source_ai/,0,1685387369.0,
257,deeplearning,open-ai,relevance,2021-09-03 00:19:43,OpenAI's Codex in Vim,tomd_96,False,0.84,12,pgu4ez,https://www.reddit.com/r/deeplearning/comments/pgu4ez/openais_codex_in_vim/,1,1630628383.0,"&#x200B;

https://i.redd.it/7xui0u2ml6l71.gif

You can now let your editor write Python code for you using the Vim plugin I wrote: [https://github.com/tom-doerr/vim\_codex](https://github.com/tom-doerr/vim_codex)

All you need to provide is a docstring and the plugin will use OpenAI's Codex AI (powers GitHub Copilot) to write the corresponding code.

Be aware that you do need to get access to the Codex API."
258,deeplearning,open-ai,relevance,2021-07-06 16:56:22,What OpenAI and GitHub’s “AI pair programmer” means for the software industry,bendee983,False,0.92,30,oez127,https://bdtechtalks.com/2021/07/05/openai-github-gpt-3-copilot/,6,1625590582.0,
259,deeplearning,open-ai,relevance,2023-06-30 20:46:28,"What is better for a startup with €0 investment, using the OpenAI API or deploying a model like Falcon40b?",ByMarcosDesigns,False,0.67,1,14nbmxr,https://www.reddit.com/r/deeplearning/comments/14nbmxr/what_is_better_for_a_startup_with_0_investment/,0,1688157988.0,"First of all I would like to wish a good day to all the readers of this message, my name is Marc and I am one of the three boys and girls who are promoting a project called AldQuest, an application focused on helping students with the use of artificial intelligence light to test their abilities or be able to solve their doubts. 

Right now we have verified that our client would pay for the application but we are having a problem with the prices that the client can pay.  We have to make the costs of the application as low as possible but using the OpenAI API is a problem for us, we could say that it makes the prices go up.  

We are thinking of deploying our own fine-tuned model based on Falcon 40b, but as we are new to this we don't know how much money it would take to do it in a service like @runpod or @lambdalabs. 

What would you recommend we do?"
260,deeplearning,open-ai,relevance,2023-06-16 13:10:11,Data-centric AI playbook 🏀📚 (open source project),44sps,False,1.0,7,14awhld,https://www.reddit.com/r/deeplearning/comments/14awhld/datacentric_ai_playbook_open_source_project/,0,1686921011.0,"There are a lot of tricks and great open source tools available to build high quality datasets. However, these resources are often not fully used for two reasons: It is difficult to know all the tools and it is sometimes too much work to integrate them.

We are collecting useful workflow snippets in a data-centric AI playbook. We have just started with a few select plays covering things like:

1️ Understanding data distributions with embeddings

2️ Finding label inconsistencies 

3️ Detecting leakage

4️ Identifying data drift

You can find the repo with notebooks here: [https://github.com/Renumics/spotlight/tree/main/playbook](https://github.com/Renumics/spotlight/tree/main/playbook)  
Link to docs with additional description:   
[https://renumics.com/next/docs/playbook](https://renumics.com/next/docs/playbook)  


Feedback is highly appreciated.   


&#x200B;

&#x200B;"
261,deeplearning,open-ai,relevance,2022-07-06 11:22:15,Reinforcement Learning without Reward Engineering (reproducing OpenAI paper with crowdsourcing),Euphetar,False,1.0,17,vsnnv9,https://medium.com/p/60c63402c59f,0,1657106535.0,
262,deeplearning,open-ai,relevance,2023-02-02 08:55:09,Any of you know a local and Open Source equivalent to Eleven Labs text to speech AI ?,lordnyrox,False,0.95,38,10rlbc4,https://www.reddit.com/r/deeplearning/comments/10rlbc4/any_of_you_know_a_local_and_open_source/,43,1675328109.0,
263,deeplearning,open-ai,relevance,2023-11-17 12:00:13,"AI Portal Gun: A Comprehensive Open Source Guide to Free AI Resources for Mastering Artificial Intelligence – Books, Courses, Articles, Research Papers, Codes, Projects and More.",Jiraiya27s,False,0.4,0,17xdi60,https://www.portalgunai.org/,0,1700222413.0,
264,deeplearning,open-ai,relevance,2022-10-18 16:52:15,Fully automated video generation - connecting OpenAI's Whisper with Stable Diffusion. Tutorial & code coming soon!,hayAbhay,False,1.0,12,y7cbf4,https://youtu.be/xRcoeUgD4GY,8,1666111935.0,
265,deeplearning,open-ai,relevance,2022-02-09 19:59:11,How to render OpenAi Gym in Google Colaboratory,sakshman,False,1.0,1,someat,https://www.reddit.com/r/deeplearning/comments/someat/how_to_render_openai_gym_in_google_colaboratory/,0,1644436751.0,
266,deeplearning,open-ai,relevance,2022-10-04 01:00:17,Create your own speech to text application with Whisper from OpenAI and Flask,hellopaperspace,False,0.82,7,xv0x55,https://blog.paperspace.com/whisper-openai-flask-application-deployment/,0,1664845217.0,
267,deeplearning,open-ai,relevance,2022-12-28 16:01:41,Andrew Huberman transcripts app - high-quality transcription using OpenAI's largest Whisper model (see comment),gordicaleksa,False,1.0,7,zxd7yd,https://www.hubermantranscripts.com/,2,1672243301.0,
268,deeplearning,open-ai,relevance,2022-05-11 18:43:47,Nftopia.ai - Visual semantic search for NFTs using OpenAI's CLIP (x-post /r/MachineLearning),hayAbhay,False,0.57,1,ungtzb,https://www.reddit.com/r/deeplearning/comments/ungtzb/nftopiaai_visual_semantic_search_for_nfts_using/,0,1652294627.0,"Hi! My colleagues & I indexed about 1.1 million NFT images from across 21 thousand collections using OpenAI's CLIP and put it behind a website. Please let us know what you think!

Specifically, we embed all these images using clip & expose two functionalities. The search bar embeds the input query & retrieves similar images. The ""+more like this"" is image search that retrieves other NFTs similar to it in the image space. We use Pinecone to power the approximate nearest neighbor search. The web stack uses Django in the front + flask app that interfaces with Pinecone.

(Warning: we've noticed several NSFW NFTs that can pop up occasionally even for unrelated search queries)"
269,deeplearning,open-ai,relevance,2023-04-28 17:08:03,"How to prepare for executive questions like ""What are Azure/OpenAI enterprise products and how much do they cost?""",digital-bolkonsky,False,0.33,0,1322k6f,/r/procurement/comments/131ype1/how_to_prepare_for_executive_questions_like_what/,0,1682701683.0,
270,deeplearning,open-ai,relevance,2022-06-15 16:07:22,Join us for the OpenAI GPT-3 Deep Learning Labs Hackathon!,zakrzzz,False,0.9,8,vcxzp4,https://www.reddit.com/r/deeplearning/comments/vcxzp4/join_us_for_the_openai_gpt3_deep_learning_labs/,0,1655309242.0,"We are waiting for all of you, AI enthusiasts with coding experience and without, on the 24th - 26th of June to help you turn your ground-breaking ideas into reality!

Register here - [https://lablab.ai/event/gpt3-online](https://lablab.ai/event/gpt3-online)  


https://preview.redd.it/mhr93wgo6t591.png?width=1600&format=png&auto=webp&s=07e23c79830db8061eb300f76b64588b01219ebc"
271,deeplearning,open-ai,relevance,2023-06-12 12:52:19,Local and open-source equivalent to HeyGen Text-to-Speech (TTS) AI?,Dark_Cloud_Games,False,0.87,6,147ne32,https://www.reddit.com/r/deeplearning/comments/147ne32/local_and_opensource_equivalent_to_heygen/,8,1686574339.0,"Do any of you know a local and open-source equivalent to HeyGen Text-to-Speech (**TTS**) **AI**? Or even if they use any third-party API? Unlike Eleven Labs (limited to English), HeyGen's technology allows for multiple accents. "
272,deeplearning,open-ai,relevance,2020-05-22 15:23:28,Open AI and Microsoft Can Generate Python Code,cmillionaire9,False,0.89,68,golbq4,https://youtu.be/y5-wzgIySb4,6,1590161008.0,
273,deeplearning,open-ai,relevance,2021-12-21 16:58:03,OpenAI Gym Custom Environments Dynamically Changing Action Space,zhy1024,False,0.75,2,rlix7g,https://www.reddit.com/r/deeplearning/comments/rlix7g/openai_gym_custom_environments_dynamically/,0,1640105883.0," Hello everyone,

I'm currently doing a robotics grasping project using Reinforcement Learning. My agent's action space is discrete, but the issue is that for different states my action space may change as some actions are invalid for some states (valid action list for one state will be checked and given by some functions in my code), how can I fit my custom environment into openai gym format so that I can test some of their baselines algorithms?

Thank you in advance!"
274,deeplearning,open-ai,relevance,2022-02-11 16:47:38,Interview with Arvind Neelakantan about the OpenAI Embeddings API,HenryAILabs,False,1.0,2,sq3s05,https://www.reddit.com/r/deeplearning/comments/sq3s05/interview_with_arvind_neelakantan_about_the/,0,1644598058.0,"Hey everyone!   


The release of OpenAI's Embeddings API has been quite the story! I had the pleasure to interview Arvind Neelakantan on miscellaneous topics pertaining to these new advances in Search: [https://www.youtube.com/watch?v=uFxfZ0vLsoU](https://www.youtube.com/watch?v=uFxfZ0vLsoU)  


Additional Background on this:  
OpenAI Embeddings API Blog Post: [https://openai.com/blog/introducing-text-and-code-embeddings/](https://openai.com/blog/introducing-text-and-code-embeddings/)

Nils Reimers' Response (OpenAI GPT-3 Text Embeddings - Really a new state-of-the-art in dense text embeddings?): [https://medium.com/@nils\_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9](https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9)

Yannic Kilcher on the topic: [https://www.youtube.com/watch?v=5skIqoO3ku0](https://www.youtube.com/watch?v=5skIqoO3ku0)"
275,deeplearning,open-ai,relevance,2021-04-17 14:31:07,*Semantic* Video Search with OpenAI’s CLIP Neural Network (link in comments!),designer1one,False,0.98,66,msrsc4,https://i.redd.it/as278qmzuqt61.gif,9,1618669867.0,
276,deeplearning,open-ai,relevance,2021-07-15 19:36:18,OpenAI experiments in Minecraft and Coding,HenryAILabs,False,0.75,2,ol06ad,https://www.reddit.com/r/deeplearning/comments/ol06ad/openai_experiments_in_minecraft_and_coding/,0,1626377778.0,"Here are some ideas on how recent papers from OpenAI may come together: 

[https://www.youtube.com/watch?v=\_84MxLMpA3c](https://www.youtube.com/watch?v=_84MxLMpA3c)

Here are full explanations of the individual papers:

Evaluation of Large Language Models Trained on Code Data: 

[https://www.youtube.com/watch?v=1hJdBNYTNmQ](https://www.youtube.com/watch?v=1hJdBNYTNmQ)

Multi-Task Curriculum Learning in a Complex, Visual, Hard-Exploration Domain: Minecraft: 

[https://www.youtube.com/watch?v=aYBliqTTsGI](https://www.youtube.com/watch?v=aYBliqTTsGI)"
277,deeplearning,open-ai,relevance,2022-05-16 16:17:25,OpenAI GPT-3 & Codex Hackathon - Deep Learning Labs Stockholm,zakrzzz,False,1.0,2,uqzmxs,https://www.reddit.com/r/deeplearning/comments/uqzmxs/openai_gpt3_codex_hackathon_deep_learning_labs/,0,1652717845.0," Hello everyone!

Join us this weekend for the Deep Learning Hackathon in Stockholm! We are teaming up with WeWork and OpenAI to bring you an event focused on exploring the latest AI technologies: GPT-3 and Codex. This is a great opportunity to learn, build cool stuff, and meet interesting people. All levels of experience are welcome.

So if you are in Stockholm this weekend, we'll be happy to have you! Also if you know someone who might be interested in participating, let them know, I would be very grateful!

And if you won't be in Stockholm, you can watch the event live at [https://www.twitch.tv/deeplearninglabs](https://www.twitch.tv/deeplearninglabs) We will have some interesting Keynote sessions, fireside chat, and of course teams' demo presentations.

Register here: [https://sthlm.dllhack.com/](https://sthlm.dllhack.com/)

If you have any questions, I'll be happy to answer."
278,deeplearning,open-ai,relevance,2022-06-23 08:39:18,Deep dive into the OpenAI CLIP's code | Machine Learning Coding Series,gordicaleksa,False,0.67,1,vis6xu,https://youtu.be/jwZQD0Cqz4o,0,1655973558.0,
279,deeplearning,open-ai,relevance,2022-09-07 21:39:59,Open Source/free alternative to Topaz Labs Video Enhance AI - Video Enhance AI,lordnyrox,False,0.84,4,x8hd95,https://www.reddit.com/r/deeplearning/comments/x8hd95/open_sourcefree_alternative_to_topaz_labs_video/,5,1662586799.0,"Hi, I'm looking for a free/open source alternative to
Topaz Labs Video Enhance AI - Video Enhance AI"
280,deeplearning,open-ai,relevance,2022-04-07 03:33:41,OpenAI's DALL·E 2 ! Text-to-Image Generation Explained,OnlyProggingForFun,False,0.75,2,ty3ysp,https://youtu.be/rdGVbPI42sA,1,1649302421.0,
281,deeplearning,open-ai,relevance,2023-09-05 09:01:07,"Global AI Ecosystem Launched as Open-Source Infrastructure for Industry Knowledge, Analytics and Community!",valentyna_stotska,False,0.67,1,16aizeo,https://www.reddit.com/r/deeplearning/comments/16aizeo/global_ai_ecosystem_launched_as_opensource/,0,1693904467.0,"[Global AI Ecosystem](https://www.linkedin.com/company/global-ai-ecosystem/) Launched as Open-Source Infrastructure for Industry Knowledge, Analytics and Community!

Global AI Ecosystem is a first-of-a-kind open-source, decentralized and non-profit AI knowledge, analytics and community-building platform. Developed with the support of [AI Industry Analytics](https://www.linkedin.com/company/ai-industry-analytics/) and [Deep Knowledge Group](https://www.linkedin.com/company/deep-knowledge-ventures/) the platform represents the go-to, democratized and universally accessible space for community interaction, collaboration, discussion, content sharing and knowledge generation in the global AI community.

Among its initial range of features is the Global AI Ecosystem IT Platform, which provides interactive, searchable and filterable databases and profiles of 50,000 companies, 20,000 investors, 2,500 R&D hubs, and 300 government entities. This release also includes a range of interactive and static mindmaps designed to give users access to a comprehensive view of the entire AI Industry based on AI applications in each sector and regional distribution.

[https://www.linkedin.com/posts/global-ai-ecosystem\_ai-community-ecosystem-activity-7104720383001473024-Rwi-?utm\_source=share&utm\_medium=member\_desktop](https://www.linkedin.com/posts/global-ai-ecosystem_ai-community-ecosystem-activity-7104720383001473024-Rwi-?utm_source=share&utm_medium=member_desktop)"
282,deeplearning,open-ai,relevance,2022-07-24 13:36:42,OpenAI GLIDE (Diffusion) | ML Coding series | Towards Photorealistic Image Generation and Editing,gordicaleksa,False,0.72,6,w6vst8,https://youtu.be/c1GwVg3lt1c,0,1658669802.0,
283,deeplearning,open-ai,relevance,2021-06-19 09:55:10,Use OpenAI's CLIP for interesting usecases,adeshgautam,False,1.0,3,o3cjlm,https://www.reddit.com/r/deeplearning/comments/o3cjlm/use_openais_clip_for_interesting_usecases/,0,1624096510.0,"Check out my article if you find it interesting

  
[https://adeshg7.medium.com/build-your-own-search-engine-using-openais-clip-and-fastapi-part-1-89995aefbcdd](https://adeshg7.medium.com/build-your-own-search-engine-using-openais-clip-and-fastapi-part-1-89995aefbcdd)"
284,deeplearning,open-ai,relevance,2022-10-12 20:21:40,"I've built an Auto Subtitled Video Generator using Streamlit and OpenAI Whisper, hosted on HuggingFace spaces.",Batuhan_Y,False,0.93,21,y2edmn,https://www.reddit.com/r/deeplearning/comments/y2edmn/ive_built_an_auto_subtitled_video_generator_using/,0,1665606100.0,"All you have to do is input a YouTube video link and get a video with subtitles (alongside with .txt, .vtt, .srt files).

Whisper can translate 98 different languages to English. If you want to give it a try;

Link of the app: [https://huggingface.co/spaces/BatuhanYilmaz/Auto-Subtitled-Video-Generator](https://huggingface.co/spaces/BatuhanYilmaz/Auto-Subtitled-Video-Generator)

&#x200B;

https://reddit.com/link/y2edmn/video/r49plzsgoft91/player"
285,deeplearning,open-ai,relevance,2021-08-19 04:40:51,Remaking OpenAI Codex space game with Baby Yoda,techn0_cratic,False,1.0,1,p780oe,https://youtu.be/bqt29KmiTpU,0,1629348051.0,
286,deeplearning,open-ai,relevance,2023-10-25 01:25:27,How we Built an Open-Source RAG-based ChatGPT Web App: Meet Our new AI Tutor!,OnlyProggingForFun,False,0.6,1,17ft66e,https://youtu.be/7ytyK6u3aAk,0,1698197127.0,
287,deeplearning,open-ai,relevance,2023-03-07 16:20:57,"We tracked mentions of OpenAI, Bing, and Bard across social media to find out who's the most talked about in Silicon Valley",yachay_ai,False,0.43,0,11l3ofp,https://www.reddit.com/r/deeplearning/comments/11l3ofp/we_tracked_mentions_of_openai_bing_and_bard/,2,1678206057.0,"[Posts about OpenAI, Bing, and Bard in the San Francisco Bay Area and Silicon Valley](https://preview.redd.it/tliq31mjecma1.png?width=1286&format=png&auto=webp&s=c5042544103fbf453172bc7e01c3efb6ad9a9451)

Have you been following the news on the conversational AI race? We used social media data and [geolocation models](https://github.com/1712n/yachay-public/tree/master/conf_geotagging_model) to find posts about OpenAI, Bing, and Bard in the Silicon Valley and San Francisco Bay Area for the last two weeks to see which one received the most mentions.

First, we filtered social media data with the keywords ""openai,"" ""bing,"" ""bard,"" and then we predicted coordinates for the social media posts by using our text-based geolocation models. After selecting texts which received a confidence score higher than 0.8, we plotted their coordinates as company logos on a leaflet map using Python and the folium library, restricting the map to the bounding box of the San Francisco Bay Area and Silicon Valley.

We analyzed over 300 social media posts and found that roughly 54.5% of the time, OpenAI was the most talked about. Bing made second place with around 27.2%, and then Bard came in last with 18.3%.

You can check out the full map [here](https://1712n.github.io/yachay-public/maps/chatbots/).

OpenAI may be winning the AI race at the moment, but it's not the end yet. Let us know what other AI projects you're following, and we'll check them out."
288,deeplearning,open-ai,relevance,2021-07-24 12:15:43,OpenAI's New Code Generator: GitHub Copilot (and Codex) | This AI Generates Code From Words,OnlyProggingForFun,False,0.14,0,oqov6e,https://youtu.be/az3oVVkTFB8,3,1627128943.0,
289,deeplearning,open-ai,relevance,2021-07-08 04:51:47,"Exploration of GitHub Copilot, OpenAI Codex-based AI coding assistant that translates natural language into code",techn0_cratic,False,0.76,22,og08xq,https://youtu.be/GTG_bcFdcLQ,0,1625719907.0,
290,deeplearning,open-ai,relevance,2023-07-03 06:33:17,Sweep: Open Source AI junior developer that writes and fixes it's own pull requests,williamsweep,False,0.71,6,14pb43s,https://www.reddit.com/r/deeplearning/comments/14pb43s/sweep_open_source_ai_junior_developer_that_writes/,0,1688365997.0,"Hi!

I'm one of the developers of Sweep AI (YC S23). You can tell Sweep about bugs and feature requests and Sweep will create a PR with code changes.

You can use Sweep Chat to ideate and clarify requirements with Sweep, eventually creating a pull request. Then anywhere GitHub takes text (PR comments, code comments), Sweep can read it and tweak the PR.

We built Sweep by integrating search + GPT4-32k into Github, with a couple more tricks :D.

To find out more:

&#x200B;

* Get started with Sweep at [https://github.com/sweepai/sweep#-getting-started](https://github.com/sweepai/sweep#-getting-started)
* See more details at [https://sweep.dev/](https://sweep.dev/) and our docs at [https://docs.sweep.dev/](https://docs.sweep.dev/)"
291,deeplearning,open-ai,relevance,2021-07-16 11:44:12,OpenAI Codex shows the limits of large language models,bendee983,False,0.76,2,olf7u3,https://bdtechtalks.com/2021/07/15/openai-codex-ai-programming/,0,1626435852.0,
292,deeplearning,open-ai,relevance,2023-01-06 19:33:06,Open Source AI Image Classifier with Automatic Dataset Creator,softcrater,False,0.6,1,10535k2,https://github.com/serpapi/serapis-ai-image-classifier,0,1673033586.0,
293,deeplearning,open-ai,relevance,2021-12-29 02:58:00,OpenAI GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models,gordicaleksa,False,1.0,2,rqxc0g,https://www.youtube.com/watch?v=lvv4N2nf-HU,1,1640746680.0,
294,deeplearning,open-ai,relevance,2021-01-13 20:28:24,OpenAI CLIP: ConnectingText and Images (Paper Explained),myyoucef,False,0.88,12,kwp46g,https://www.youtube.com/watch?v=T9XSU0pKX2E,0,1610569704.0,
295,deeplearning,open-ai,relevance,2019-08-22 10:10:49,Open AI - GPT2,Tamil94,False,0.6,1,ctvcer,https://www.reddit.com/r/deeplearning/comments/ctvcer/open_ai_gpt2/,0,1566468649.0,"GPT 2 was released and anyone tried .if yes ,please tell us the efficiency ..

we are trying to use that for machine translation.will it helpful for that...."
296,deeplearning,open-ai,relevance,2021-08-10 22:11:44,Demo - Improved OpenAI Codex that translates natural language to code,rshpkamil,False,0.86,10,p1zkqp,https://www.reddit.com/r/deeplearning/comments/p1zkqp/demo_improved_openai_codex_that_translates/,0,1628633504.0,https://openai.com/blog/openai-codex/
297,deeplearning,open-ai,relevance,2019-09-19 10:07:59,Google to open AI lab in Bangalore,adssidhu86,False,0.89,49,d6bwsm,https://www.blog.google/around-the-globe/google-asia/google-research-india-ai-lab-bangalore/amp/,0,1568887679.0,
298,deeplearning,open-ai,relevance,2021-11-01 14:33:01,"[R] Warsaw U, OpenAI and Google’s Hourglass Hierarchical Transformer Model Outperforms Transformer Baselines",Yuqing7,False,0.85,13,qkf9xu,https://www.reddit.com/r/deeplearning/comments/qkf9xu/r_warsaw_u_openai_and_googles_hourglass/,2,1635777181.0,"A team from the University of Warsaw, OpenAI and Google Research proposes Hourglass, a hierarchical transformer language model that operates on shortened sequences to alleviate transformers’ huge computation burdens. 

Here is a quick read: [Warsaw U, OpenAI and Google’s Hourglass Hierarchical Transformer Model Outperforms Transformer Baselines.](https://syncedreview.com/2021/11/01/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-135/)

The paper *Hierarchical Transformers Are More Efficient Language Models* is on [arXiv](https://arxiv.org/abs/2110.13711)."
299,deeplearning,open-ai,relevance,2020-11-16 12:57:16,"OpenAI RL workshop, blog link in the comments",OneUpWallStreet,False,0.5,0,jv6axz,https://youtu.be/fdY7dt3ijgY,1,1605531436.0,
