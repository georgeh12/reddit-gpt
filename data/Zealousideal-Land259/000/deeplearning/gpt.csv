,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,id,url,num_comments,created,body
0,deeplearning,gpt,top,2022-12-02 01:35:02,GPT-3 Generated Rap Battle between Yann LeCun & Gary Marcus,hayAbhay,False,0.99,139,za73dc,https://i.redd.it/ybfcfvez1e3a1.png,16,1669944902.0,
1,deeplearning,gpt,top,2023-01-27 10:45:48,‚≠ï What People Are Missing About Microsoft‚Äôs $10B Investment In OpenAI,LesleyFair,False,0.95,117,10mhyek,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI‚Äôs models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let‚Äôs first look at Sam Altman‚Äôs backstory.

*Let‚Äôs go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI ‚Äî one of the most exciting and impactful organizations in all of tech.

However, OpenAI ‚Äî the rocket ship of AI innovation ‚Äî is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn‚Äôt take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can‚Äôt they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT‚Äôs outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That‚Äôs gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI‚Äôs profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI‚Äôs API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don‚Äôt have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman‚Äôs foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft‚Äôs sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI ‚Äî whatever that looks like ‚Äî the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ‚≠ï.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)‚Äã

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)‚Äã

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)‚Äã

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)‚Äã

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)‚Äã

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
2,deeplearning,gpt,top,2020-08-17 19:53:20,Personal GPT-3 project üöÄ: Guess the movie! You can't recall the name of that movie you watched? You know what the movie's about but you just can't remember its name? I used the GPT-3 model to solve this problem! Just feed it a small description of the movie/tv show and it will do the rest.,CallmeMehdi25,False,0.96,116,iblhzl,https://i.redd.it/z12t847vamh51.gif,29,1597694000.0,
3,deeplearning,gpt,top,2020-08-05 10:58:06,image-GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model,OnlyProggingForFun,False,0.94,97,i437pt,https://www.youtube.com/watch?v=FwXQ568_io0,6,1596625086.0,
4,deeplearning,gpt,top,2023-04-05 01:36:40,Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT,Time_Key8052,False,0.95,84,12c43uu,https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/,19,1680658600.0,"Vicuna : ChatGPT Alternative, Open-Source, High Quality and Low Cost 

&#x200B;

[ Relative Response Quality Assessed by GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a072b91)

Vicuna-13B has demonstrated competitive performance against other open-source models, such as Stanford Alpaca, by fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.

Evaluation using GPT-4 as a judge shows that Vicuna-13B achieves more than 90% of the quality of OpenAI ChatGPT and Google Bard AI, while outperforming other models such as Meta LLaMA (Large Language Model Meta AI) and Stanford Alpaca in more than 90% of cases.

The cost of training Vicuna-13B is approximately $300.

The training and serving code, along with an online demo, are publicly available for non-commercial use.

&#x200B;

More Information : [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT](https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)

Discord Server : [https://discord.gg/h6kCZb72G7](https://discord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysorg](https://twitter.com/lmsysorg)"
5,deeplearning,gpt,top,2023-04-28 18:10:08,The Little Book of Deep Learning is a 140 page (phone-formatted!) technical introduction of the necessary background for denoising diffusion and GPT models. BY-NC-SA.,FrancoisFleuret,False,0.97,81,1325a0j,https://fleuret.org/public/lbdl.pdf,6,1682705408.0,
6,deeplearning,gpt,top,2023-01-19 07:55:49,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.9,70,10fw22o,https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/,11,1674114949.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=webp&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ‚≠ï, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
7,deeplearning,gpt,top,2023-11-16 08:28:46,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,0.84,71,17whz6e,https://v.redd.it/qx68wbuf7o0c1,7,1700123326.0,
8,deeplearning,gpt,top,2023-01-11 14:41:25,What do you all think about these ‚ÄúSEO is Dead‚Äù articles?,Aggressive-Twist-252,False,0.91,66,1096byl,https://www.reddit.com/r/deeplearning/comments/1096byl/what_do_you_all_think_about_these_seo_is_dead/,3,1673448085.0,"I keep seeing [articles](https://jina.ai/news/seo-is-dead-long-live-llmo/) like this over the years and it made me wonder. Is SEO really dead? Or will it evolve? Back then I kept wondering if it‚Äôs true or not. Some believe SEO is dead, some don‚Äôt. But now with tools like Chat GPT and Midjourney, I think it‚Äôs time to take a look back and see how this might change SEO or if it will ‚Äúkill‚Äù SEO.

I keep seeing threads and discussions seeing how people are excited and worried at the same time with how AI might be able to do a better job. But the way I see it, AI content still needs a person to tell it what to do and make the writing look nice. And also I think that the internet will have a lot of writing that was made by AI and that might change how we find things online. You might also see a ton of content being written by AI and trigger some plagiarism detectors and have a lot of websites get penalized. Hopefully the internet won‚Äôt be filled with boilerplate copy/pasted content coming from Chat GPT.

Well we have Google to filter out trash content anyway. But I know Google has some issues lately that they need to fix. One is that they also have AI that can help people find things on the internet with their search engine, and they need to make sure they are still the best in terms of search. 

The second is that Google needs to find a way to tell if something is really good or not, like how some websites that show art do. Google wants to show the best thing first, but it's hard because sometimes the thing that is the best is also something that Google's customers want people to see. It‚Äôs possible that some AI generated contentSo it's kind of tricky.

I have a feeling companies that already make SEO-writing and checking bots are gonna roll out some fresh new models soon. They're gonna be even better than before. These bots are going to write some good articles and product descriptions that are almost perfect. It almost looks like a human wrote the article or description. And all a human will do is quickly check for any false claims and write a headline that doesn't sound like a robot wrote it. 

We can only really tell 5-10 years from now. In the meantime, I‚Äôll probably go back practicing some handyman skills and also go back teaching people how to drive and also be a service driver. These jobs I had in the past were way different from what I am earning now but if the worst comes to worst, at least I have these physical skills ready."
9,deeplearning,gpt,top,2023-01-29 22:39:07,[P] We built a browser extension that unlocks browser mode capabilities using ChatGPT: MULTI¬∑ON: AI Web Co-Pilot powered by ChatGPT,DragonLord9,False,0.95,67,10okyg3,https://v.redd.it/2hw47h0b82fa1,8,1675031947.0,
10,deeplearning,gpt,top,2023-01-20 08:53:58,Gotcha,actual_rocketman,False,0.93,61,10gs1ik,https://i.redd.it/yyh41pnje7da1.jpg,5,1674204838.0,
11,deeplearning,gpt,top,2023-06-05 04:33:14,How Open Ai‚Äôs Andrej Karpathy Made One of the Best Tutorials in Deep Learning,0ssamaak0,False,0.91,57,141282u,https://www.reddit.com/r/deeplearning/comments/141282u/how_open_ais_andrej_karpathy_made_one_of_the_best/,3,1685939594.0,"I want you to check [my review](https://medium.com/@0ssamaak0/how-open-ais-andrej-karpathy-made-one-of-the-best-tutorials-in-deep-learning-e6b6445a2d05) on Andrej Karpathy amazing work on explaining how GPT is built

[GitHub Repo](https://github.com/0ssamaak0/Karpathy-Neural-Networks-Zero-to-Hero) for code & more details

&#x200B;

https://preview.redd.it/z204zwtzn44b1.png?width=720&format=png&auto=webp&s=095ea00991ebb295f48b70436456b1f283a50df1"
12,deeplearning,gpt,top,2021-07-15 17:06:55,"EleutherAI Researchers Open-Source GPT-J, A Six-Billion Parameter Natural Language Processing (NLP) AI Model Based On GPT-3",techsucker,False,0.98,55,okx5hm,https://www.reddit.com/r/deeplearning/comments/okx5hm/eleutherai_researchers_opensource_gptj_a/,5,1626368815.0,"[GPT-J](https://www.eleuther.ai/), a six-billion-parameter natural language processing (NLP) AI model based on GPT-3, has been open-sourced by a team of EleutherAI researchers. The model was trained on an open-source text [dataset of 800GB](https://pile.eleuther.ai/) and was comparable with a GPT-3 model of similar size.

The model was trained using Google Cloud‚Äôs v3-256 TPUs using EleutherAI‚Äôs Pile dataset, which took about five weeks. GPT-J achieves accuracy similar to OpenAI‚Äôs reported findings for their 6.7B parameter version of GPT-3 on standard NLP benchmark workloads. The model code, pre-trained weight files, a Colab notebook, and a sample web page are included in EleutherAI‚Äôs release.

Story: [https://www.marktechpost.com/2021/07/15/eleutherai-researchers-open-source-gpt-j-a-six-billion-parameter-natural-language-processing-nlp-ai-model-based-on-gpt-3/](https://www.marktechpost.com/2021/07/15/eleutherai-researchers-open-source-gpt-j-a-six-billion-parameter-natural-language-processing-nlp-ai-model-based-on-gpt-3/) 

Github repository for GPT-J: https://github.com/kingoflolz/mesh-transformer-jax

Colab Notebook: https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab\_demo.ipynb

Web Demo: https://6b.eleuther.ai/"
13,deeplearning,gpt,top,2021-04-26 16:38:23,[R] Google and UC Berkeley Propose Green Strategies for Large Neural Network Training,Yuqing7,False,0.95,56,mz1v2c,https://www.reddit.com/r/deeplearning/comments/mz1v2c/r_google_and_uc_berkeley_propose_green_strategies/,1,1619455103.0,"A research team from Google and the University of California, Berkeley calculates the energy use and carbon footprint of large-scale models T5, Meena, GShard, Switch Transformer and GPT-3, and identifies methods and publication guidelines that could help reduce their CO2e footprint.

Here is a quick read: [Google and UC Berkeley Propose Green Strategies for Large Neural Network Training](https://syncedreview.com/2021/04/26/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-5/).

The paper *Carbon Emissions and Large Neural Network Training* is on [arXiv](https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf)."
14,deeplearning,gpt,top,2020-07-28 12:31:26,GPT-3 writes my SQL queries for me,Independent-Square32,False,0.87,54,hzdthe,https://youtu.be/WlMHYEFt2uA,5,1595939486.0,
15,deeplearning,gpt,top,2023-03-29 14:13:46,AI Startup Cerebras releases open source ChatGPT-like alternative models,Time_Key8052,False,0.94,46,125pbbf,https://gpt4chatgpt.tistory.com/entry/Cerebras-releases-open-source-ChatGPT-like-alternative-models,14,1680099226.0,
16,deeplearning,gpt,top,2023-03-25 04:24:49,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.92,46,121agx4,https://www.reddit.com/r/deeplearning/comments/121agx4/do_we_really_need_100b_parameters_in_a_large/,54,1679718289.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
17,deeplearning,gpt,top,2020-09-11 15:37:20,[R] OpenAI ‚ÄòGPT-f‚Äô Delivers SOTA Performance in Automated Mathematical Theorem Proving,Yuqing7,False,0.93,42,iqsul7,https://www.reddit.com/r/deeplearning/comments/iqsul7/r_openai_gptf_delivers_sota_performance_in/,2,1599838640.0,"San Francisco-based AI research laboratory OpenAI has added another member to its popular GPT (Generative Pre-trained Transformer) family. In a new paper, OpenAI researchers introduce GPT-f, an automated prover and proof assistant for the Metamath formalization language.

Here is a quick read: [OpenAI ‚ÄòGPT-f‚Äô Delivers SOTA Performance in Automated Mathematical Theorem Proving](https://syncedreview.com/2020/09/10/openai-gpt-f-delivers-sota-performance-in-automated-mathematical-theorem-proving/)

The paper *Generative Language Modeling for Automated Theorem Proving* is on [arXiv](https://arxiv.org/pdf/2009.03393.pdf)."
18,deeplearning,gpt,top,2023-02-15 09:25:30,[P] From ‚Äúiron manual‚Äù to ‚ÄúIron Man‚Äù ‚Äî Augmenting GPT for fast editable memory to enable context aware question & answering,skeltzyboiii,False,1.0,43,112u10p,https://i.redd.it/yujf2enambia1.gif,7,1676453130.0,
19,deeplearning,gpt,top,2020-10-25 19:26:17,Google AI Introduces Performer: A Generalized Attention Framework based on the Transformer architecture,ai-lover,False,0.89,40,jhzd4q,https://www.reddit.com/r/deeplearning/comments/jhzd4q/google_ai_introduces_performer_a_generalized/,2,1603653977.0,"[Transformer model](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html), a deep learning framework, has achieved state-of-the-art results across diverse domains, including¬†[natural language](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html),¬†[conversation](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html),¬†[images](https://openai.com/blog/image-gpt/), and even¬†[music](https://magenta.tensorflow.org/music-transformer). The core block of any Transformer architecture is the¬†[attention module](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html), which computes similarity scores for all pairs of positions in an input sequence. Since it requires quadratic computation time and quadratic memory size of the storing matrix, with the increase in the input sequence‚Äôs length, its efficiency decreases.

Thus, for long-range attention, one of the most common methods is¬†[sparse attention](https://openai.com/blog/sparse-transformer/). It reduces the complexity by computing selective similarity scores from the sequence, based on various methods. There are still certain limitations like unavailability of efficient sparse-matrix multiplication operations on all accelerators, lack of theoretical guarantees, insufficiency to address the full range of problems, etc.

**Introduction to ‚ÄúPerformer‚Äù**

To resolve these issues, Google AI introduces the¬†[Performer](https://arxiv.org/abs/2009.14794), a Transformer architecture with attention mechanisms that scale linearly. The framework is implemented by¬†[Fast Attention Via Positive Orthogonal Random Features (FAVOR+) algorithm](https://github.com/google-research/google-research/tree/master/performer/fast_self_attention), providing scalable¬†low-variance¬†and¬†unbiased¬†estimation of attention mechanisms expressed by random feature maps decompositions (in particular, regular softmax-attention). Mapping helps in preserving linear space and time complexity.

**Full Summary:** [https://www.marktechpost.com/2020/10/25/google-ai-introduces-performer-a-generalized-attention-framework-based-on-the-transformer-architecture/](https://www.marktechpost.com/2020/10/25/google-ai-introduces-performer-a-generalized-attention-framework-based-on-the-transformer-architecture/) 

**Github:**¬†[https://github.com/google-research/google-research](https://github.com/google-research/google-research) 

**Paper:**¬†[https://arxiv.org/pdf/2009.14794.pdf](https://arxiv.org/pdf/2009.14794.pdf)"
20,deeplearning,gpt,top,2020-09-18 10:45:02,GPT-3: new AI can write like a human but don't mistake that for thinking ‚Äì neuroscientist,PowerOfLove1985,False,0.81,37,iv3rnz,https://theconversation.com/gpt-3-new-ai-can-write-like-a-human-but-dont-mistake-that-for-thinking-neuroscientist-146082,14,1600425902.0,
21,deeplearning,gpt,top,2022-12-02 20:59:47,Everyone: AI will make it easy to spread misinformation; Me: Stop hitting yourself GPT3!,hayAbhay,False,0.91,40,zaxg5m,https://i.redd.it/mrf9rz0ltj3a1.png,0,1670014787.0,
22,deeplearning,gpt,top,2023-02-11 06:59:00,‚≠ï New Open-Source Version Of ChatGPT,LesleyFair,False,0.88,40,10zepkt,https://www.reddit.com/r/deeplearning/comments/10zepkt/new_opensource_version_of_chatgpt/,3,1676098740.0,"GPT is getting competition from open-source.

A group of researchers, around the YouTuber [Yannic Kilcher](https://www.ykilcher.com/), have announced that they are working on [Open Assistant](https://github.com/LAION-AI/Open-Assistant). The goal is to produce a chat-based language model that is much smaller than GPT-3 while maintaining similar performance.

If you want to support them, they are crowd-sourcing training data [here](https://open-assistant.io/).

**What Does This Mean?**

Current language models are too big.

They require millions of dollars of hardware to train and use. Hence, access to this technology is limited to big organizations. Smaller firms and universities are effectively shut out from the developments.

Shrinking and open-sourcing models will facilitate academic research and niche applications.

Projects such as Open Assistant will help to make language models a commodity. Lowering the barrier to entry will increase access and accelerate innovation.

What an exciting time to be alive! 

Thank you for reading! I really enjoyed making this for you!  
The Decoding ‚≠ï is a thoughtful weekly 5-minute email that keeps you in the loop about machine research and the data economy. [Click here to sign up](https://thedecoding.net/)!"
23,deeplearning,gpt,top,2022-11-03 23:55:15,BlogNLP: AI Writing Tool,britdev,False,1.0,36,ylj1ux,https://www.reddit.com/r/deeplearning/comments/ylj1ux/blognlp_ai_writing_tool/,9,1667519715.0,"Hey everyone,

I created this web app using Open AI's GPT-3 (Davinci model). The purpose here is to provide a free tool to allow people to generate blog content/outlines/headlines and help with writer's block. Will continue to improve it over time, but just a side project I figured would provide some value to you all. Hope you all enjoy and please share ‚ù§Ô∏è

[https://www.blognlp.com/](https://www.blognlp.com/)"
24,deeplearning,gpt,top,2021-06-14 06:34:33,"This Chinese Super Scale Intelligence Model, ‚ÄòWu Dao 2.0‚Äô, Claims To Be Trained Using 1.75 Trillion Parameters, Surpassing All Prior Models to Achieve a New Breakthrough in Deep Learning",ai-lover,False,0.92,36,nzgkj3,https://www.reddit.com/r/deeplearning/comments/nzgkj3/this_chinese_super_scale_intelligence_model_wu/,9,1623652473.0,"Deep learning is one area of technology where ambitiousness has no barriers. According to a recent announcement by¬†[The Beijing Academy of Artificial Intelligence (BAAI)](https://www.baai.ac.cn/), in China, yet another milestone has been achieved in the field with its ‚ÄúWu Dao‚Äù AI system. The¬†[GPT 3](https://www.marktechpost.com/2020/08/02/gpt-3-a-new-breakthrough-in-language-generator/)¬†brought in new interest for all the AI researchers, the super scale pre training models. By this approach and making use of 175 billion parameters, it managed to achieve exceptional performance results across the natural language processing tasks (NLP). However, the lacking component is its inability to have any form of cognitive abilities or common sense. Therefore, despite the size, even these models cannot indulge in tasks such as open dialogues, visual reasoning, and so on. With Wu Dao, the researchers plan to address this issue. This is China‚Äôs first attempt at a home-grown super-scale intelligent model system.¬†

Article: [https://www.marktechpost.com/2021/06/13/this-chinese-super-scale-intelligence-model-wu-dao-2-0-claims-to-be-trained-using-1-75-trillion-parameters-surpassing-all-prior-models-to-achieve-a-new-breakthrough-in-deep-learning/](https://www.marktechpost.com/2021/06/13/this-chinese-super-scale-intelligence-model-wu-dao-2-0-claims-to-be-trained-using-1-75-trillion-parameters-surpassing-all-prior-models-to-achieve-a-new-breakthrough-in-deep-learning/?_ga=2.13897584.636390090.1623335762-488125022.1618729090)

Reference: [https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/](https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/)"
25,deeplearning,gpt,top,2023-08-25 13:21:12,AI Meets AI: A Conversation Between GPT-4 and Google's Bard,Ubica123,False,0.79,33,160z5pp,https://www.youtube.com/watch?v=3H45IncZ7gs,3,1692969672.0,
26,deeplearning,gpt,top,2023-05-01 20:45:08,What are some small LLM models or free LLM APIs for tiny fun project?,silent_lantern,False,0.94,33,1350qtu,https://www.reddit.com/r/deeplearning/comments/1350qtu/what_are_some_small_llm_models_or_free_llm_apis/,19,1682973908.0,"Hi, I'm looking for a free/opensource api to build a small GPT webapp for fun. I want to deploy it on something like Heroku and use Flask in the backend. 


I'm also open to uploading a small-ish llm model on Heroku and use that to answer chat like queries from users.


Do you know of any such small foss models and/or free APIs?"
27,deeplearning,gpt,top,2022-03-12 04:56:16,Microsoft‚Äôs Latest Machine Learning Research Introduces ŒºTransfer: A New Technique That Can Tune The 6.7 Billion Parameter GPT-3 Model Using Only 7% Of The Pretraining Compute,No_Coffee_4638,False,1.0,32,tc8u6k,https://www.reddit.com/r/deeplearning/comments/tc8u6k/microsofts_latest_machine_learning_research/,0,1647060976.0,"Scientists conduct trial and error procedures which experimenting, that many times lear to freat scientific breakthroughs. Similarly, foundational research provides for developing large-scale AI systems theoretical insights that reduce the amount of trial and error required and can be very cost-effective.

Microsoft team tunes massive neural networks that are too expensive to train several times. For this, they employed a specific parameterization that maintains appropriate hyperparameters across varied model sizes. The used ¬µ-Parametrization (or ¬µP, pronounced ‚Äúmyu-P‚Äù) is a unique way to learn all features in the infinite-width limit. The researchers collaborated with the OpenAI team to test the method‚Äôs practical benefit on various realistic cases.

Studies have shown that training large neural networks because their behavior changes as they grow in size are uncertain. Many works suggest heuristics that attempt to maintain consistency in the activation scales at initialization. However, as training progresses, this uniformity breaks off at various model widths.

[**CONTINUE READING MY SUMMARY ON THIS RESEARCH**](https://www.marktechpost.com/2022/03/11/microsofts-latest-machine-learning-research-introduces-%ce%bctransfer-a-new-technique-that-can-tune-the-6-7-billion-parameter-gpt-3-model-using-only-7-of-the-pretraining-compute/)

Paper: https://www.microsoft.com/en-us/research/uploads/prod/2021/11/TP5.pdf

Github:https://github.com/microsoft/mup

https://i.redd.it/7jrt9r3awvm81.gif"
28,deeplearning,gpt,top,2020-04-21 23:00:56,How does the talktotransformer website work so fast?,parrot15,False,0.93,30,g5prf5,https://www.reddit.com/r/deeplearning/comments/g5prf5/how_does_the_talktotransformer_website_work_so/,5,1587510056.0,"At [https://talktotransformer.com/](https://talktotransformer.com/), you can type a prompt and the transformer will autogenerate the text for you using OpenAI's GPT-2 1.5 billion parameter model.

I'm not asking how GPT-2 works, I'm asking something else. When I ran the GPT-2 1.5B model on the free TPU in Google Colab, it took around 20 to 40ish seconds to generate around the same about of text as the website generates per prompt.

And yet, the website is somehow generating text from the prompt almost instantaneously using the 1.5B model. This is on top of all the X number of people who must be using the website at the same time I am, so it is doing text generation concurrently and near-instantaneously using a gigantic model.

I am very confused. Did the creator of the website just use a lot more TPUs/GPUs behind the scenes and is letting them run 24/7 (I don't think so because that would cost a shit ton of money), or am I missing something fundamental here?

This same question can be applied to this website too: [https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is](https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is)

Please keep in mind that I'm relatively new to all of this. Thanks in advance!"
29,deeplearning,gpt,top,2022-12-12 17:29:39,ChatGPT context length,Wild-Ad3931,False,0.97,31,zk5esp,https://www.reddit.com/r/deeplearning/comments/zk5esp/chatgpt_context_length/,10,1670866179.0,"How come ChatGPT can follow entire discussions whereas nowaday's LLM are limited (to the best of my knowledge) 4096 tokens ?

I asked it and it is not able to answer neither, and I found nothing on Google because no paper is published. I was also curious to understand how come Beamsearch was so fast with ChatGPT.

https://preview.redd.it/o6djsmpb5i5a1.png?width=1126&format=png&auto=webp&s=98baae5bf6fa294db408f0530214f8afa8a32a0b"
30,deeplearning,gpt,top,2020-05-20 14:40:55,Gpt-2 Generated South Park chats between Characters,askbrodown,False,0.97,31,gncjho,https://www.reddit.com/r/deeplearning/comments/gncjho/gpt2_generated_south_park_chats_between_characters/,10,1589985655.0,"[https://www.soulreplica.com/brodown](https://www.soulreplica.com/brodown)

discussion on HN: [https://news.ycombinator.com/item?id=23246418](https://news.ycombinator.com/item?id=23246418)

The chats between the characters are generated by Gpt-2 774M model trained over all season's southpark episodes. The characters sounds exactly like who they are in the show.  The conversations either starts from a random topic, or they can respond  to a random trending tweets (e.g. trump, elon musk, etc.) the results  could be very on point, and sometimes hilarious:

https://preview.redd.it/56zto0ivlxz41.png?width=2222&format=png&auto=webp&s=fbf7882ed5750171d76ba8ef4b59886bec9135d2

https://preview.redd.it/e583h0yulxz41.png?width=2548&format=png&auto=webp&s=86814f5de77377530a48e97ab8f2ddaddaf9fb5d"
31,deeplearning,gpt,top,2023-04-24 01:17:58,Can an average person learn how to build a LLM model?,sch1zoph_,False,0.71,26,12wxrrd,https://www.reddit.com/r/deeplearning/comments/12wxrrd/can_an_average_person_learn_how_to_build_a_llm/,29,1682299078.0,"Hello everyone. I am a 30-year-old Korean male.

To be honest, I have never really studied properly in my life. It's a little embarrassing, but that's the truth.

Recently, while using ChatGPT, I had a dream for the first time. I want to create a chatbot that can provide a light comfort to people who come for advice. I would like to create an LLM model using Transformer, and use our country's beginner's counseling manual as the basis for the database.

I am aware that there are clear limits to the level of comfort that can be provided. Therefore, if the problem is too complex or serious for this chatbot to handle, I would like to recommend the nearest mental hospital or counseling center based on the user's location. And, if the user can prove that they have visited the hospital (currently considering a direction where the hospital or counseling center can provide direct certification), I would like to create a program that provides simple benefits (such as a free Starbucks coffee coupon).

I also thought about collecting a database of categories related to people's problems (excluding personal information) and selling it to counseling or psychiatric societies. I think this could be a great help to these societies.

The problem is that I have never studied ""even once,"" and I feel scared and fearful of the unfamiliar sensation. I have never considered myself a smart person.

However, I really want to make this happen! Our country is now in a state of constant conflict, and people hate and despise each other due to strong propaganda.

As a result, the birth rate has dropped to less than 1%, leading to a decline in the population. Many people hide their pain inside and have no will to solve it. They just drink with their friends to relieve their pain. This is obviously not a solution. Therefore, Korea has a really serious suicide rate.

I may not be able to solve this problem, but I want to put one small brick to build a big barrier to stop hatred. Can an ordinary person who knows nothing learn the common sense and study needed to build an LLM model? And what direction should one take to study one by one?"
32,deeplearning,gpt,top,2023-05-18 08:41:54,Tutorial to improve GPT throughput 16 times with dynamic batching,Greedy-Cupcake-3694,False,0.93,26,13ksxc8,https://www.reddit.com/r/deeplearning/comments/13ksxc8/tutorial_to_improve_gpt_throughput_16_times_with/,6,1684399314.0,"I wrote a tutorial to improve GPT completion throughput with dynamic batching [https://microsoft.github.io/batch-inference/examples/gpt\_completion.html](https://microsoft.github.io/batch-inference/examples/gpt_completion.html). And I can achieve 16 times throughput on V100 comparing to baseline. We built a python dynamic batching library so you can apply it on your own models easily [https://github.com/microsoft/batch-inference](https://github.com/microsoft/batch-inference).  


Although the tutorial we built for GPT shows promising result on throughput, it doesn't use complex decoding algorithms like top-p or beam search, and we are aware of more advanced batching algorithms for GPT completion. So we're considering building a GPT specific inference library for production use, hope we get enough resource to do it in future"
33,deeplearning,gpt,top,2023-04-12 05:21:13,Is OpenAI‚Äôs Study On The Labor Market Impacts Of AI Flawed?,LesleyFair,False,0.96,26,12jb4xz,https://www.reddit.com/r/deeplearning/comments/12jb4xz/is_openais_study_on_the_labor_market_impacts_of/,1,1681276873.0,"[Example img\_name](https://preview.redd.it/f3hrmeet1eta1.png?width=1451&format=png&auto=webp&s=20e20b142a2f88c3d495177e540f34bc8ea4312b)

We all have heard an uncountable amount of predictions about how AI will¬†***terk err jerbs!***

However, here we have a proper study on the topic from OpenAI and the University of Pennsylvania. They investigate how Generative Pre-trained Transformers (GPTs) could automate tasks across different occupations \[1\].

Although I‚Äôm going to discuss how the study comes with a set of ‚Äúimperfections‚Äù, the findings still make me really excited. The findings suggest that machine learning is going to deliver some serious productivity gains.

People in the data science world fought tooth and nail for years to squeeze some value out of incomplete data sets from scattered sources while hand-holding people on their way toward a data-driven organization. At the same time, the media was flooded with predictions of omniscient AI right around the corner.

*Let‚Äôs dive in and take an*¬†exciting glimpse into the future of labor markets\*!\*

# What They Did

The study looks at all US occupations. It breaks them down into tasks and assesses the possible level of for each task. They use that to estimate how much automation is possible for a given occupation.

The researchers used the¬†[O\*NET database,](https://www.onetcenter.org/database.html)¬†which is an occupation database specifically for the U.S. market. It lists 1,016 occupations along with its standardized descriptions of tasks.

The researchers annotated each task once manually and once using GPT-4. Thereby, each task was labeled as either somewhat (<50%) or significantly (>50%) automatable through LLMs. In their judgment, they considered both the direct ‚Äúexposure‚Äù of a task to GPT as well as to a secondary GPT-powered system, e.g. LLMs integrated with image generation systems.

To reiterate, a higher ‚Äúexposure‚Äù means that an occupation is more likely to get automated.

Lastly, they enriched the occupation data with wages and demographic information. This was used to determine whether e. g. high or low-paying jobs are at higher risk to be automated.

So far so good. This all sounds pretty decent. Sure, there is a lot of qualitative judgment going into their data acquisition process. However, we gotta cut them some slag. These kinds of studies always struggle to get any hard data and so far they did a good job.

However, there are a few obvious things to criticize. But before we get to that let‚Äôs look at their results.

# Key Findings

The study finds that 80% of the US workforce, across all industries, could have at least some tasks affected. Even more significantly, 19% of occupations are expected to have at least half of their tasks significantly automated!

Furthermore, they find that higher levels of automation exposure are associated with:

* Programming and writing skills
* Higher wages (contrary to previous research!)
* Higher levels of education (Bachelor‚Äôs and up)

Lower levels of exposure are associated with:

* Science and critical thinking skills
* Manual work and tasks that might potentially be done using physical robots

This is somewhat unsurprising. We of course know that LLMs will likely not increase productivity in the plumbing business. However, their findings underline again how different this wave is. In the past, simple and repetitive tasks fell prey to automation.

*This time it‚Äôs the suits!*

If we took this study at face value, many of us could start thinking about life as full-time pensioners.

But not so fast! This, like all the other studies on the topic, has a number of flaws.

# Necessary Criticism

First, let‚Äôs address the elephant in the room!

OpenAI co-authored the study. They have a vested interest in the hype around AI, both for commercial and regulatory reasons. Even if the external researchers performed their work with the utmost thoroughness and integrity, which I am sure they did, the involvement of OpenAI could have introduced an unconscious bias.

*But there‚Äôs more!*

The occupation database contains over 1000 occupations broken down into tasks. Neither GPT-4 nor the human labelers can possibly have a complete understanding of all the tasks across all occupations. Hence, their judgment about how much a certain task can be automated has to be rather hand-wavy in many cases.

Flaws in the data also arise from the GPT-based labeling itself.

The internet is flooded with countless sensationalist articles about how AI will replace jobs. It is hard to gauge whether this actually causes GPT models to be more optimistic when it comes to their own impact on society. However, it is possible and should not be neglected.

The authors do also not really distinguish between labor-augmenting and labor-displacing effects and it is hard to know what ‚Äúaffected by‚Äù or ‚Äúexposed to LLMs‚Äù actually means. Will people be replaced or will they just be able to do more?

Last but not least, lists of tasks most likely do not capture all requirements in a given occupation. For instance ""making someone feel cared for"" can be an essential part of a job but might be neglected in such a list.

# Take-Away And Implications

GPT models have the world in a frenzy - rightfully so.

Nobody knows whether 19% of knowledge work gets heavily automated or if it is only 10%.

As the dust settles, we will begin to see how the ecosystem develops and how productivity in different industries can be increased. Time will tell whether foundational LLMs, specialized smaller models, or vertical tools built on top of APIs will be having the biggest impact.

In any case, these technologies have the potential to create unimaginable value for the world. At the same time, change rarely happens without pain. I strongly believe in human ingenuity and our ability to adapt to change. All in all, the study - flaws aside - represents an honest attempt at gauging the future.

Efforts like this and their scrutiny are our best shot at navigating the future. Well, or we all get chased out of the city by pitchforks.

Jokes aside!

What an exciting time for science and humanity!

As always, I really enjoyed making this for you and I sincerely hope you found value in it!

If you are not subscribed to the newsletter yet,¬†[click here to sign up](https://thedecoding.net/)! I send out a thoughtful 5-minute email every week to keep you in the loop about machine learning research and the data economy.

*Thank you for reading and I see you next week ‚≠ï!*

**References:**

\[1\]¬†[https://arxiv.org/abs/2303.10130](https://arxiv.org/abs/2303.10130)"
34,deeplearning,gpt,top,2021-05-02 16:45:08,GPT-1 - Annotated Paper + Paper Summary,shreyansh26,False,0.96,25,n3aeh5,https://www.reddit.com/r/deeplearning/comments/n3aeh5/gpt1_annotated_paper_paper_summary/,2,1619973908.0," GPT-2 and recently, GPT-3 created a lot of hype when they were launched. However, it all started with the ""Improving¬†Language¬†Understanding¬†by¬†Generative¬†Pre-Training"" paper which introduced the idea of GPT-1.

As a part of my Paper Notes series, I have gone through the paper and created a brief yet informative summary of the paper. It will take just take a few minutes to understand GPT-1 well. Check out the links below and happy reading!

Paper Summary - [Improving Language Understanding by Generative Pre-Training](https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/)

Annotated Paper - [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT1.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT1.pdf)"
35,deeplearning,gpt,top,2023-03-05 11:10:56,LLaMA model parallelization and server configuration,ChristmasInOct,False,0.97,24,11ium8l,https://www.reddit.com/r/deeplearning/comments/11ium8l/llama_model_parallelization_and_server/,8,1678014656.0,"Hey everyone,

First of all, tldr at bottom, typed more than expected here.  

Please excuse the rather naive perspective I have here.  I've followed along with great interest, but this is not my industry.

Regardless, I have spent the past 3-4 days falling down a brutally obsessive rabbit hole, and I cannot seem to find this information.  I'm assuming it's just that I am missing context of course, and regardless of whether there is a clear answer, I'm trying to get a better understanding of this topic so that I could better appraise the situation myself.

Really I suppose I have two questions.  **The first** is regarding model parallelization.

I'm assuming this is not generic whatsoever.  What is the typical process engineers go about for designing such a pipeline?  Specifically in regards to these new LLaMA models, is something like ALPA relevant?  Deepspeed?

More importantly, what information should I be seeking to determine this myself?

This roughly segues to my **second inquiry**.

The reason I'm curious about splitting the model pipeline etc., is that I am potentially in interested in standing a server up for this.  Although I don't have much of a budget for this build (\~$30-40K is the rough top-end, but I'd be a lot happier around $20-25K), the money is there if I can genuinely satisfy my use-case.

I work at a small, but borderline manic startup working on enterprise software; 90% of the work we're doing based in the react/node ecosystem, some low-level work for backend services, and some very interesting database work that I have very little to do with.  I am a fullstack engineer that grew up playing with C++ => C#, and somehow ended up spending all of my time r/w'ing javascript.  Lol.  Anyways.

Part of our roadmap since GPT-3 and the playground were made publicly accessible, involves usage of these transformer models, and their ability to interpret natural language inputs, whether from user inputs, or scraped input values generated somewhere in a chain of requests / operations.

Seeing GPT-3 in action made me specifically realize that my estimations on this technology had been wildly off.  Seeing ChatGPT in action and uptick, the API's becoming available, has me further panicked.

Running our inference through their API has never really been an option for us.  I haven't even really looked that far into it, but bottom line the data running through our platform is all back-office, highly sensitive business information, and many have agreements explicitly restricting the movement of data to or from any cloud services, with Microsoft, Amazon, and Google all specifically mentioned.

Regardless of the reasoning for these contracts, the LLaMA release has had me obsessed over this topic in more detail than before, and whether or not I would be able to get this setup privately, for our use-case.

**To get to the actual second inquiry**:

Say I want to throw a budget rig together for this in a server cabinet.  Am I able to effectively parallelize the LLaMA model, well enough to justify going with 24GB VRAM 4090's in the rig?  Say I do so with DeepSpeed, or some of the standard model parallelization libraries.

Is the performance cost low enough to justify taking the extra compute here over 1/3 - 1/2 as many RTX6000 ADA's?

Or should I be grabbing the 48GB ADA's?

Like I said, I apologize for the naivety, I'm really looking for more information so that I can start to put this picture together better on my own.  It really isn't the easiest topic to research with how quickly things seem to move, and the giant gap between conversation depths (gamer || phd in a lot of the most interesting or niche discussions, little between).

Thank you very much for your time.

TL;DR - Any information on LLaMA model parallelization at the moment?  Will it be compatible with things like zero or alpa?  How about for throwing a rig together right now for fine-tuning and then running inference on the LLaMA models?  48GB 6000 ADA's, or 24GB 4090's?

Planning on putting it in a mostly empty 42U cabinet that also houses our primary web server and networking hardware, so if there is a sales pitch for 4090's across multiple nodes here, I do have a massive bias as the kind of nerd that finds that kind of hardware borderline erotic.

Hydro and cooling are not an issue, just usage of the budget and understanding the requirements / approach given memory limitations, and how to avoid communication bottlenecks or even balance them against raw compute.

Thanks again everyone!"
36,deeplearning,gpt,top,2019-10-19 14:26:50,Benchmarking ü§ó/Transformers on both PyTorch and TensorFlow,jikkii,False,0.96,24,dk4g7d,https://www.reddit.com/r/deeplearning/comments/dk4g7d/benchmarking_transformers_on_both_pytorch_and/,1,1571495210.0,"Since our recent release of [Transformers](https://github.com/huggingface/transformers) (previously known as pytorch-pretrained-BERT and pytorch-transformers), we've been working on a comparison between the implementation of our models in PyTorch and in TensorFlow.

We've released a [detailed report](https://medium.com/huggingface/benchmarking-transformers-pytorch-and-tensorflow-e2917fb891c2) where we benchmark each of the architectures hosted on our repository (BERT, GPT-2, DistilBERT, ...) in PyTorch with and without TorchScript, and in TensorFlow with and without XLA. We benchmark them for inference and the results are visible in the [following spreadsheet](https://docs.google.com/spreadsheets/d/1sryqufw2D0XlUH4sq3e9Wnxu5EAQkaohzrJbd5HdQ_w/edit#gid=0).

We would love to hear your thoughts on the process."
37,deeplearning,gpt,top,2023-04-25 17:53:47,"Microsoft releases SynapseMl v0.11 with support for ChatGPT, GPT-4, causal learning, and more",mhamilton723,False,0.86,22,12yqpnp,https://www.reddit.com/r/deeplearning/comments/12yqpnp/microsoft_releases_synapseml_v011_with_support/,0,1682445227.0,"Today Microsoft launched SynapseML v0.11 with support for ChatGPT, GPT-4, distributed training of huggingface and torchvision models, an ONNX Model hub integration, Causal Learning with EconML, 10x memory reductions for LightGBM, and a newly refactored integration with Vowpal Wabbit. To learn more check out our release notes and please feel give us a star if you enjoy the project!

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.11.0](https://github.com/microsoft/SynapseML/releases/tag/v0.11.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919)

Thank you to all the contributors in the community who made the release possible!

&#x200B;

[What's new in SynapseML v0.11](https://preview.redd.it/9pqj1mowj2wa1.png?width=4125&format=png&auto=webp&s=a358e73760c847a09cc76f2ed17dc58e15aed5ed)"
38,deeplearning,gpt,top,2022-12-07 00:33:41,Are currently state of art model for logical/common-sense reasoning all based on NLP(LLM)?,Accomplished-Bill-45,False,0.97,22,zen8l4,https://www.reddit.com/r/deeplearning/comments/zen8l4/are_currently_state_of_art_model_for/,6,1670373221.0,"Not very familiar with NLP, but I'm playing around with OpenAI's ChatGPT; particularly impressed by its reasoning, and its thought-process.

Are all good reasoning models derived from NLP (LLM) models with RL training method at the moment?

What are some papers/research team to read/follow to understand this area better and stay on updated?

&#x200B;

&#x200B;

for ChatGPT. I've tested it with following cases

Social reasoning ( which does a good job; such as: if I'm going to attend meeting tonight. I have a suit, but its dirty and size doesn't fit. another option is just wear underwear, the underwear is clean and fit in size. Which one should I wear to attend the meeting. )

Psychological reasoning ( it did a bad job.I asked it to infer someone's intention given his behaviours, expression, talks etc.)

Solving math question ( it‚Äôs ok, better then Minerva)

Asking LSAT logic game questions ( it gives its thought process, but failed to give correct answers)

I also wrote up a short mystery novel, ( like 200 words, with context) ask if it can tell is the victim is murdered or committed suicide; if its murdered, does victim knows the killer etc. It actually did ok job on this one if the context is clearly given that everyone can deduce some conclusion using common sense."
39,deeplearning,gpt,top,2023-12-24 07:52:27,Is famous argument that people need much less data to train is always true?,imtaevi,False,0.85,24,18pqlzi,https://www.reddit.com/r/deeplearning/comments/18pqlzi/is_famous_argument_that_people_need_much_less/,61,1703404347.0,"Many people repeat that people need much less data to train than neural networks.
How about case when neural network already trained on many other similar tasks? 
Because for many cases people are already trained on many other similar tasks by 

1 evolution 

2 childhood 

You can look lectures about child psychology that give examples of what abilities people have just from evolution.

Developmental Psychology - Lecture 01 (PSYC 240)

Also about why pretty much shape us from evolution and genetics. Look at separated twins study.

Video about low data learning from Siraj.

How to Learn from Little Data - Intro to Deep Learning #17

People was training on more than billions of images if you add how much there was thru all evolution. Imagine how many images there was starting from fist animal with eyes.

People was training on pretty much text or speech data if you add how much there was thru all evolution. Imagine how many words there was starting from fist animal with speech signals. 

If you look at lectures Developmental Psychology - Lecture 01 (PSYC 240). It looks like people not only good at learning but they already learned some cases just from their setup at birth. So some case they do not need to learn. It will look like they already learned them at some age without any training after birth. It will sound like people at age X can do Y. But they was not trained for Y.

My claim is that training of neural networks is like evolution + childhood of people. After that step both variants ai and human can learn new information pretty quickly.

About Beyond training data. What if I will make a self created story about some non existing tribe of Amazon. Could gpt analyze it? Looks like yes. Was that info in it‚Äôs database? No. SAT already have examples of what was not in database.

So in that example. Ai could learn and understand pretty fast about this tribe. It does not need millions of pages about that tribe.

I am talking about advantage of humans in not only evolution but in evolution + childhood. So if we compare scores of AI and human at SAT test. Preparation of Ai is training. Preparation of humans is evolution + childhood.

"
40,deeplearning,gpt,top,2022-12-03 00:17:31,A GPT-3 based Chrome Extension that debugs your code!,VideoTo,False,1.0,22,zb2kkc,https://www.reddit.com/r/deeplearning/comments/zb2kkc/a_gpt3_based_chrome_extension_that_debugs_your/,0,1670026651.0,"Link - [https://chrome.google.com/webstore/detail/clerkie-ai/oenpmifpfnikheaolfpabffojfjakfnn](https://chrome.google.com/webstore/detail/clerkie-ai/oenpmifpfnikheaolfpabffojfjakfnn)

Built  a quick tool I thought would be interesting - it‚Äôs a chrome extension  that uses GPT-3 under the hood to help debug your programming errors  when you paste them into Google (‚Äúeg. TypeError:‚Ä¶‚Äù).

This is definitely early days, so **if   this is something you would find valuable and wouldn't mind testing a   couple iterations of, please feel free to join the discord** \-> [https://discord.gg/KvG3azf39U](https://discord.gg/KvG3azf39U)

&#x200B;

https://i.redd.it/tt6hcqn2tk3a1.gif"
41,deeplearning,gpt,top,2023-01-22 19:11:36,Apple M2 Max 96 GB unified memory for larger models vs multiple 24GB GPUs or 40GB A100s?,lol-its-funny,False,1.0,22,10irh5u,https://www.reddit.com/r/deeplearning/comments/10irh5u/apple_m2_max_96_gb_unified_memory_for_larger/,14,1674414696.0,"How feasible is it to use an Apple Silicon M2 Max, which has about [96 GB unified memory](https://www.apple.com/shop/buy-mac/macbook-pro/16-inch-space-gray-apple-m2-max-with-12-core-cpu-and-38-core-gpu-1tb) for ""large model"" deep learning? I'm inspired by the the [Chinchilla](https://arxiv.org/abs/2203.15556) paper that shows a lot of promise at 70B parameters. Outperforming ultra large models like Gopher (280B) or GPT-3 (175B) there is hope for working with < 70B parameters without needing a super computer. At least for fine tuning. I've been working with GPT-J but want to scale/tinker with larger open-sourced models.

However, I don't know how clearly the CompSci theory (M2 Max's 38-core GPU, 16 core Neural Engine accessing 96 GB unified memory) maps out to the IT reality (toolkits and libraries on macOS actually using it). My exposure is mostly around Jupyter books on Colab Pro+ (A100s) and nvidia 3080 GPUs (locally). 

I appreciate your guidance."
42,deeplearning,gpt,top,2020-09-16 16:02:55,"Practical Natural Language Processing Book [Interview + Giveaway] | NLP, ML & AI in the Industry | GPT-3 and more",mukulkhanna1,False,0.84,21,ityg5g,https://youtu.be/ptTlH-ma8rg,0,1600272175.0,
43,deeplearning,gpt,top,2020-09-25 15:04:23,[P] Quelling my fears about the future with fortune cookies and GPT-2,lilsmacky,False,0.93,21,izl2w3,https://www.reddit.com/r/deeplearning/comments/izl2w3/p_quelling_my_fears_about_the_future_with_fortune/,0,1601046263.0,"With all that is happening in the world I think many worry about the future. To get some clarity I trained a GPT-2 to tell fortune cookie fortunes. The results were very wholesome and made me feel better, so I thought I should share. :)

&#x200B;

[Github link](https://github.com/simon-larsson/fortune-cookie-gpt2)

&#x200B;

The project itself is nothing noteworthy, it is just [huggingface](https://github.com/huggingface/transformers). But I think GPT-2 really nails the pseudo profundity of fortune cookies.

&#x200B;

| Sample fortunes  |
|:-----------|
|It really does not matter which direction you are traveling from if you move ahead.|
|Life is short. Eat your cake! |
|A person who is not afraid of any, may be welcomed in safety.|
|An iron furnace will break if you ignore its own command.|
|A great change among people is upon us.|
|It's a beautiful day. Look around you.|
|Ask someone to hold your hand. You will feel happier today.|
|People can accomplish great things when they are given equal opportunities.|
|Do not hesitate to order a drink, it will fill you with high-quality energy.|
|Be not afraid to ask for more information. Stay alert! We've got to learn as much as we can today.|
|You will build strong friendships.|
|Stay aware of what you don't know.|
|In times when you are in the best position for what you dream, be open and honest.|
|Do not let fate judge us.|
|You will become a better man by not fighting past your obstacles.|
|All things must come about from below.|
|All will be right with you, no mistakes will happen. Everything will be fine.|
|You are the first to get the position you want to be.|
|The more you spend, the more you have.|
|Everyone who works for you seeks to achieve their own ends.|
|You will be surrounded by love.|
|Your loved ones will love you even more than they did before.|
|You will be happy in your lifetime.|
|The most beautiful thing in life is having a new adventure.|
|What is it like working for an entrepreneur?|
|If the best you can do is follow the rules of the market, what do you do then?|
|Try not to be taken by surprise.|
|You will receive a special medal for your efforts.|
|A happy new year brings a big reward for you.|
|A great idea may be waiting to become ingrained into your soul.|
|The moment your wish will come true should come true.|
|You will become a better person.|
|There are ways we can save others.|
|We need more people to fill the shoes of today.|
|You will become a better friend.|
|The great change taking place in the world will have a major impact on the way we look at ourselves.|
|Your life will change your life forever.|
|You can accomplish many things today.|
|Everything we do today will be great for you.|
|You will be welcomed into our world to become your personal hero.|
|It is you they are looking for.|
|It is a great day to be the happiest.|
|Your ability to be a better life will be honored by your accomplishments.|
|It is never too late to pursue the dream of a new career.|
|Today is a big day for you, but tomorrow will be a long day.|
|Good luck, you will be having a happy and successful day.|
|Try again tomorrow.|
|Now is your chance at success.|
|Tomorrow is about where you will be spending time today.|
|Don't worry about tomorrow just wait for it.|"
44,deeplearning,gpt,top,2023-02-05 16:44:56,Beat GPT-3 which has unlimited money using Open Source community,koyo4ever,False,0.8,21,10ugxmc,https://www.reddit.com/r/deeplearning/comments/10ugxmc/beat_gpt3_which_has_unlimited_money_using_open/,8,1675615496.0,"Is it technically possible to train some model using a lot of personal computers like a cluster.

Eg: an Algorithm to train tiny parts of some model using personal computer of volunteers. Like a community that makes your gpu capacity available, even if it's little.

The idea is train tiny parts of a model, with a lot of volunteers, then bring it together to make some powerful deepmind.

Can this model beat a lot of money spent in models like GPT-3?"
45,deeplearning,gpt,top,2023-10-24 15:34:49,MemGPT Explained!,CShorten,False,0.96,22,17ffmuu,https://www.reddit.com/r/deeplearning/comments/17ffmuu/memgpt_explained/,2,1698161689.0,"Hey everyone! I am SUPER excited to publish a new paper summary video of MemGPT from Packer et al. at UC Berkeley!

MemGPT is a massive step forward in the evolution from naive Retrieval-Augmented Generation (RAG) to creating an OPERATING SYSTEM for LLM applications!

This works by telling the LLM about its limited input window and giving it new ""tools"" / APIs to manage its own memory. For example, the LLM processes the conversation history in a chatbot or the next paragraph in document processing and determines what is important to add to its working context.

The authors design a operating system around this concept complete with events, functions, and of a virtual context management algorithm inspired by operating system concepts such as page replacement. When the LLM determines it needs more context to answer a question, it searches into it's external context (could be recall storage (complete history of events such as dialogue in a chatbot across 4 months), or its archival storage (information such as Wikipedia entries stored in a Vector DB) -- it then parses the search results to determine what is worth adding to its working context.

The authors test MemGPT on chatbots and the experiments from Lost in the Middle, finding that this explicit memory management overcomes the problems of losing relevant information in the middle of search results!

I think there are tons of exciting implications of this work such as the intersection with the Gorilla LLMs (trying to allocate as few tokens as possible in describing a tool to an LLM), as well as this general phenomenon of connecting LLMs to Operating Systems!

Here is my review of the paper in more detail, I hope you find it useful!

[https://www.youtube.com/watch?v=nQmZmFERmrg](https://www.youtube.com/watch?v=nQmZmFERmrg)"
46,deeplearning,gpt,top,2023-04-18 15:00:24,Uni project: a FOSS LLM comparison tool - would you find this useful?,copywriterpirate,False,1.0,20,12qq3mz,https://www.reddit.com/gallery/12qq3mz,3,1681830024.0,
47,deeplearning,gpt,top,2024-02-10 10:23:55,Home-trained transformer,prumf,False,0.92,20,1andcv7,https://www.reddit.com/r/deeplearning/comments/1andcv7/hometrained_transformer/,12,1707560635.0,"I am learning about the inner-workings of transformers, as well as GPT and BERT, but I don‚Äôt see the point of knowing about it if I can‚Äôt use my knowledge.

Training a full-blown transformer, even one with a few hundred millions parameters, is really expensive.

Do you guys have any ideas on what kind of small (and probably kind of artificial) task I could train a really small transformer, so that it would be relatively fast and inexpensive on a low-grade consumer GPU ?

thanks for your feedback ‚ù§Ô∏è"
48,deeplearning,gpt,top,2023-03-22 21:55:31,ChatLLaMA ‚Äì A ChatGPT style chatbot for Facebook's LLaMA,imgonnarelph,False,1.0,20,11yy5es,https://chatllama.baseten.co/,2,1679522131.0,
49,deeplearning,gpt,top,2023-02-01 09:00:18,"Python wrapper of OpenAI's New AI classifier tool, which detects whether the paragraph was generated by ChatGPT, GPT models, or written by humans",StoicBatman,False,0.88,18,10qouv9,https://www.reddit.com/r/deeplearning/comments/10qouv9/python_wrapper_of_openais_new_ai_classifier_tool/,3,1675242018.0,"OpenAI¬†has developed a new AI classifier tool which detects whether the content (paragraph, code, etc.) was generated by #ChatGPT, #GPT-based large language models or written by humans.

Here is a python wrapper of openai model to detect if a text is written by humans or generated by ChatGPT, GPT models

Github:¬†[https://github.com/promptslab/openai-detector](https://github.com/promptslab/openai-detector)  
Openai release:¬†[https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/](https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/)

If you are interested in #PromptEngineering, #LLMs, #ChatGPT and other latest research discussions, please consider joining our discord [discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)  


https://preview.redd.it/9d8ooeg2ljfa1.png?width=1358&format=png&auto=webp&s=0de7ccc5cd16d6bbad3dcfe7d8441547a6196fc8"
50,deeplearning,gpt,top,2019-10-23 12:25:09,I'm looking for success/failure stories applying unsupervised document embedding techniques,shaypal5,False,0.95,19,dlyjjk,https://www.reddit.com/r/deeplearning/comments/dlyjjk/im_looking_for_successfailure_stories_applying/,2,1571833509.0,"Hey everyone! :)

As the title says, I am looking for both success stories and disappointing failures of applications of **modern** unsupervised document embedding techniques on actual problems (as opposed to academic benchmarks, toy datasets, academic evaluation tasks, etc.). The main focus is naturally on industry uses for business/product problems, but I would also love to hear about cases from government bodies, non-profits, use in research (with empirical measurement and where document embedding is one of the tools, not the subject of research) and any other ""real life"" use. I would love to hear about your experience, but connecting me to people you know or even hinting me towards companies or projects you know used these techniques (or tried to) would also be of tremendous help.

What's in it for you? Well, I'm preparing a talk for [the data science track of the CodeteCON #KRK5 conference](https://codetecon.pl/en/#program) based on my [literature review-y blog post on document embedding techniques](https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d?source=friends_link&sk=158194696b5fe4cad9605f4648eb2a83), and while I feel I have a pretty good overview of the academic papers, benchmarks and SOTA status up until the most recent stuff to come out in the field at this point in time, I can't say the same for uses in the industry; I have a partial view from my experience in one ongoing project to actually use this, and experience shared by some of my data scientist friends (all in Israel, naturally) - most of it, so far, by the way, is that averaging (good) word embeddings is a very tough ""baseline"" to beat.

This is why I thought reaching out to get a better sense of things in the industry world-wide, and enriching my talk with the status of actual successes and industry applications will give people attending my talk more value, and will serve my attempt to make my talk a status report on the topic.

And (coming back to WIIFM) naturally (I think), I intend to share any (share-able) knowledge I accumulate not only in my talk, but also by adding a section dedicated to it to [the aforementioned blog post](https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d?source=friends_link&sk=158194696b5fe4cad9605f4648eb2a83), and maybe even by writing an extended post around it (if enough interesting trends and issues come up). So, hopefully, if you are (like me) interested in this, we might also end up getting, together, a nice overview of where the industry stands at the moment.

What **modern** techniques (so no variants of bag-of-words or topic modeling techniques) am I talking about? These are the ones that I know of (I'd love to hear about others!):

* n-gram embeddings
* Averaging word embeddings (including all variants, e.g. SIF)
* Sent2Vec
* Paragraph vectors (doc2vec)
* Doc2VecC
* Skip-thought vectors
* FastSent
* Quick-thought vectors
* Word Mover‚Äôs Embedding (WME)
* Sentence-BERT (SBERT)
* GPT/GPT2 (can also be supervised)
* Universal Sentence Encoder (can also be supervised)
* GenSen (can also be supervised)

Thank you and cheers,Shay :)"
51,deeplearning,gpt,top,2023-03-09 00:50:22,"AI generated video chapter titles (YouTube, Vimeo, etc)",happybirthday290,False,0.91,19,11mdvb9,https://i.redd.it/h6utxsxg2mma1.png,1,1678323022.0,
52,deeplearning,gpt,top,2023-12-28 16:22:37,Do Large Vision-language Models Understand Charts? We found that the answer is NO!,steeveHuang,False,0.96,17,18sxs1r,https://www.reddit.com/r/deeplearning/comments/18sxs1r/do_large_visionlanguage_models_understand_charts/,2,1703780557.0,"We've just wrapped up a collaborative study with Columbia University and the University of Macau that probes into the capabilities of Large Vision-Language Models (LVLMs) when it comes to understanding and describing charts. The findings are quite startling.

Despite advancements in LVLMs, our research reveals that even the most advanced LVLMs like GPT-4V and Bard fall short. A striking üö®**81.27%** (321/ 395) üö® of the captions they generated contained factual errors, misinterpreting data from charts. This suggests a significant gap in these models' ability to grasp the nuances and relationships between data points in visual representations.

üîç Explore our findings in detail with the full paper on [Arxiv](https://arxiv.org/abs/2312.10160).

üíª: Code and data are also available on [GitHub](https://github.com/khuangaf/CHOCOLATE)

&#x200B;

https://preview.redd.it/448ty01q929c1.png?width=1362&format=png&auto=webp&s=c6ce27262247ce6978ae7ff169f6fc844fda63de"
53,deeplearning,gpt,top,2023-09-29 14:02:33,This week in AI - all the Major AI developments in a nutshell,wyem,False,0.88,18,16vch0x,https://www.reddit.com/r/deeplearning/comments/16vch0x/this_week_in_ai_all_the_major_ai_developments_in/,2,1695996153.0,"1. **Meta AI** presents **Emu**, a quality-tuned latent diffusion model for generating highly aesthetic images. Emu significantly outperforms SDXLv1.0 on visual appeal.
2. **Meta AI** researchers present a series of long-context LLMs with context windows of up to 32,768 tokens. LLAMA 2 70B variant surpasses gpt-3.5-turbo-16k‚Äôs overall performance on a suite of long-context tasks.
3. **Abacus AI** released a larger 70B version of **Giraffe**. Giraffe is a family of models that are finetuned from base Llama 2 and have a larger context length of 32K tokens\].
4. **Cerebras** and **Opentensor** released Bittensor Language Model, ‚Äò**BTLM-3B-8K**‚Äô, a new 3 billion parameter open-source language model with an 8k context length trained on 627B tokens of SlimPajama. It outperforms models trained on hundreds of billions more tokens and achieves comparable performance to open 7B parameter models. The model needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models and is available with an Apache 2.0 license for commercial use.
5. **OpenAI** is rolling out, over the next two weeks, new voice and image capabilities in ChatGPT enabling ChatGPT to understand images, understand speech and speak. The new voice capability is powered by a new text-to-speech model, capable of generating human-like audio from just text and a few seconds of sample speech. .
6. **Mistral AI**, a French startup, released its first 7B-parameter model, **Mistral 7B**, which outperforms all currently available open models up to 13B parameters on all standard English and code benchmarks. Mistral 7B is released in Apache 2.0, making it usable without restrictions anywhere.
7. **Microsoft** has released **AutoGen** \- an open-source framework that enables development of LLM applications using multiple agents that can converse with each other to solve a task. Agents can operate in various modes that employ combinations of LLMs, human inputs and tools.
8. **LAION** released **LeoLM**, the first open and commercially available German foundation language model built on Llama-2
9. Researchers from **Google** and **Cornell University** present and release code for DynIBaR (Neural Dynamic Image-Based Rendering) - a novel approach that generates photorealistic renderings from complex, dynamic videos taken with mobile device cameras, overcoming fundamental limitations of prior methods and enabling new video effects.
10. **Cloudflare** launched **Workers AI** (an AI inference as a service platform), **Vectorize** (a vector Database) and **AI Gateway** with tools to cache, rate limit and observe AI deployments. Llama2 is available on Workers AI.
11. **Amazon** announced the general availability of **Bedrock**, its service that offers a choice of generative AI models from Amazon itself and third-party partners through an API.
12. **Spotify** has launched a pilot program for AI-powered voice translations of podcasts in other languages - in the podcaster‚Äôs voic. It uses OpenAI‚Äôs newly released voice generation model.
13. **Getty Images** has launched a generative AI image tool, ‚Äò**Generative AI by Getty Images**‚Äô, that is ‚Äòcommercially‚Äësafe‚Äô. It‚Äôs powered by Nvidia Picasso, a custom model trained exclusively using Getty‚Äôs images library.
14. **Optimus**, Tesla‚Äôs humanoid robot, can now sort objects autonomously and do yoga. Its neural network is trained fully end-to-end.
15. **Amazon** will invest up to $4 billion in Anthropic. Developers and engineers will be able to build on top of Anthropic‚Äôs models via Amazon Bedrock.
16. **Google Search** indexed shared Bard conversational links into its search results pages. Google says it is working on a fix.

&#x200B;

  
My plug: If you like this news format, you might find the [newsletter, AI Brews](https://aibrews.com/), helpful - it's free to join, sent only once a week with bite-sized news, learning resources and selected tools. I didn't add links to news sources here because of auto-mod, but they are included in the newsletter. Thanks"
54,deeplearning,gpt,top,2021-05-16 14:43:36,XLNet - Annotated Paper + Paper Summary,shreyansh26,False,0.84,17,ndpsv7,https://www.reddit.com/r/deeplearning/comments/ndpsv7/xlnet_annotated_paper_paper_summary/,2,1621176216.0,"Although BERT became really popular after its release, it did have some limitations. And there were certain limitations associated with autoregressive methods like ELMo and GPT as well. XLNet was introduced to get the best of both worlds while at the same time not include their weaknesses.

In continuation of my Paper Notes series, I have written an informative summary of the paper. Personally, reading the XLNet paper was a very fun experience. I was amazed at every step, how they were including stuff to make the whole model work so well. The paper contained many interesting concepts that I had to give time to understand. So don't worry if you don't get it on the first go. Check out the links below and happy reading!

Paper Summary - [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/)

Annotated Paper - [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/XLNet.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/XLNet.pdf)"
55,deeplearning,gpt,top,2023-04-16 04:52:51,"BERT Explorer - Analyzing the ""T"" of GPT",msahmad,False,0.92,19,12nvtm3,https://www.reddit.com/r/deeplearning/comments/12nvtm3/bert_explorer_analyzing_the_t_of_gpt/,0,1681620771.0,"If you want to dig deeper into NLP, LLM, Generative AI, you might consider starting with a model like BERT. This tool helps in exploring the inner working of Transformer-based model like BERT. It helped me understands some key concepts like word embedding, self-attention, multi-head attention, encoder, masked-language model, etc. Give it a try and explore BERT in a different way.

BERT == Bidirectional Encoder Representations from TransformersGPT == Generative Pre-trained Transformer

They both use the Transformer model, but BERT is relatively simpler because it only uses the encoder part of the Transformer.

BERT Explorer[https://www.101ai.net/text/bert](https://www.101ai.net/text/bert)

https://i.redd.it/bxxboyyuhaua1.gif"
56,deeplearning,gpt,top,2023-02-02 23:02:25,Why are FPGAs better than GPUs for deep learning?,Open-Dragonfly6825,False,0.96,19,10s3u1s,https://www.reddit.com/r/deeplearning/comments/10s3u1s/why_are_fpgas_better_than_gpus_for_deep_learning/,37,1675378945.0,"I've worked for some years developing scientific applications for GPUs. Recently we've been trying to integrate FPGAs into our technologies; and consequently I've been trying to understand what they are useful for.

I've found many posts here and there that claim that FPGAs are better suited than GPUs to accelerate Deep Learning/AI workloads (for example, [this one by Intel](https://www.intel.com/content/www/us/en/artificial-intelligence/programmable/fpga-gpu.html)). However, I don't understand why that would be the case. I think the problem is that all those posts try to explain what an FPGA is and what its differences are to a GPU, so that people that work on Deep Learning understand why they are better suited. Nevertheless, my position is exactly the opposite: I know quite well how a GPU works and what it is good for, I know well enough how an FPGA works and how it differs from a GPU, **but I do not know enough about Deep Learning** to understand why Deep Learning applicatios would benefit more from the special features of FPGAs rather than from the immense parallelism GPUs offers.

As far as I know, an FPGA will never beat a traditional GPU in terms of raw parallelism (or, if it does, it would be much less cost efficient). Thus, when it comes to matrix multiplications, i.e. the main operation in Deep Learning models, or convolutions, GPUs can parallelly work with much bigger matrices. The only explanation I can think of is that traditional Deep Learning applications don't necessarily use such big matrices, but rather smaller ones that can also be fully parallelized in FPGAs and benefit highly from custom-hardware optimizations (optimized matrix multiplications/tensor operations, working with reduced-bit values such as FP16, deep-pipeline parallelism, ...). However, given the recent increase in popularity of very complex models (GPT-3, dall-e, and the like) which boast using millions or even billions of parameters, it is hard to imagine that popular deep learning models work with small matrices of which fully parallel architectures can be synthesized in FPGAs.

What am I missing? Any insight will be greatly appreciated.

EDIT: I know TPUs are a thing and are regarded as ""the best option"" for deep learning acceleration. I will not be working with them, however, so I am not interested in knowing the details on how they compare with GPUs or FPGAs."
57,deeplearning,gpt,top,2022-05-06 01:54:02,Meta's open-source new model OPT is GPT-3's closest competitor!,OnlyProggingForFun,False,0.91,17,ujcra5,https://youtu.be/Ejg0OunCi9U,5,1651802042.0,
58,deeplearning,gpt,top,2023-01-28 06:34:28,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,1.0,18,10n8c80,https://www.reddit.com/r/deeplearning/comments/10n8c80/a_python_module_to_generate_optimized_prompts/,0,1674887668.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features üöÄ

* üßô‚Äç‚ôÄÔ∏è NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* üî® Easily add one-shot, two-shot, or few-shot examples to the prompt
* ‚úå Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* üí• Custom examples and samples can be easily added to the prompt
* üí∞ Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Example](https://preview.redd.it/sjvhtd8b3nea1.png?width=1236&format=png&auto=webp&s=e9a3a28f41f59cd25fe8e95bd1fca56b15f27a6e)

&#x200B;

https://preview.redd.it/fnb05bys3nea1.png?width=1398&format=png&auto=webp&s=096f4e2cbd0a71e795f30cc5e3720316b5e5caf6"
59,deeplearning,gpt,top,2023-03-10 05:22:26,[POC] ChatGPT Audio Bot (like Google Assistant and Alexa) - Opensource,JC1DA,False,0.82,17,11nfrhw,https://www.reddit.com/r/deeplearning/comments/11nfrhw/poc_chatgpt_audio_bot_like_google_assistant_and/,2,1678425746.0,"Hey guys, just wanna share this bot that I quickly built using ChatGPT API.

GitHub page: [https://github.com/LanyTek/ChatGPT\_Audio\_Bot](https://github.com/LanyTek/ChatGPT_Audio_Bot)

This service allows you to keep talking to the bot using your voice like you often do with Google Assistant or Alexa. It can be used in a lot of scenarios like teaching, gossiping, or quickly retrieving information without the need of typing. 

I have a quick video demo here if you wanna check [https://www.youtube.com/watch?v=e9n0BJfMyKw](https://www.youtube.com/watch?v=e9n0BJfMyKw)

It's open source so feel free to use it for anything you like :)"
60,deeplearning,gpt,top,2023-02-14 15:35:09,A Comprehensive Guide & Hand-Curated Resource List for Prompt Engineering and LLMs on Github,StoicBatman,False,1.0,18,11289nd,https://www.reddit.com/r/deeplearning/comments/11289nd/a_comprehensive_guide_handcurated_resource_list/,1,1676388909.0,"Greetings,

Excited to share with all those interested in Prompt Engineering and Large Language Models (LLMs)!

We've hand-curated a comprehensive, Free & Open Source resource list on Github that includes everything related to Prompt Engineering, LLMs, and all related topics. We've covered most things, from papers and articles to tools and code!

Here you will find:

* üìÑ Papers in different categories such as Prompt Engineering Techniques, Text to Image Generation, Text Music/Sound Generation, Text Video Generation etc.
* üîß Tools & code to build different GPT-based applications
* üíª Open-Source & Paid APIs
* üíæ Datasets
* üß† Prompt-Based Models
* üìö Tutorials from Beginner to Advanced level
* üé• Videos
* ü§ù Prompt-Engineering Communities and Groups for discussion

**Resource list**: [https://github.com/promptslab/Awesome-Prompt-Engineering](https://github.com/promptslab/Awesome-Prompt-Engineering)

We hope it will help you to get started & learn more about Prompt-Engineering. If you have questions, Join our discord for Prompt-Engineering, LLMs and other latest research discussions

[https://discord.com/invite/m88xfYMbK6](https://discord.com/invite/m88xfYMbK6)

Thank you :)  


https://preview.redd.it/4l453lkr76ia1.png?width=1770&format=png&auto=webp&s=c76dc9e01c40f2a845a1518401d12f21bfe13575"
61,deeplearning,gpt,top,2023-04-07 10:58:54,Text-to-image Diffusion Models in Generative AI: A Survey,Learningforeverrrrr,False,0.85,15,12ehc2m,https://www.reddit.com/r/deeplearning/comments/12ehc2m/texttoimage_diffusion_models_in_generative_ai_a/,0,1680865134.0,"Diffusion models have become a SOTA generative modeling method for numerous content types, such as images, audio, graph, etc. As the number of articles on diffusion models has grown exponentially over the past few years, there is an increasing need for survey works to summarize them. Recognizing the existence of such works, our team has completed multiple field-specific surveys on diffusion models. We promote our works here and hope they can be helpful to researchers in relative fields: text-to-image diffusion models [\[a survey\]](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey), audio diffusion models [\[a survey\]](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI), and graph diffusion models [\[a survey\]](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material) .

In the following, we briefly summarize our survey on text-to-image diffusion models.

[Text-to-image Diffusion Models in Generative AI: A Survey](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)

As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis, i.e., text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.

Moreover, we have also completed two survey works on generative AI (AIGC) [\[a survey\]](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need) and ChatGPT [\[a survey\]](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era), respectively. Interested readers may give it a look."
62,deeplearning,gpt,top,2020-12-15 23:04:59,[R] NeurIPS 2020 | Teaching Transformers New Tricks,Yuqing7,False,0.94,15,kdws3z,https://www.reddit.com/r/deeplearning/comments/kdws3z/r_neurips_2020_teaching_transformers_new_tricks/,0,1608073499.0,"Transformers are a class of attention-based neural architectures that have enabled advanced pretrained language models such as Google‚Äôs BERT and OpenAI‚Äôs GPT series and produced numerous breakthroughs in speech recognition and other natural language processing (NLP) tasks since their debut in 2017. Transformers perform exceptionally well on problems with sequential data, and have more recently been extended to reinforcement learning, computer vision and symbolic mathematics.

This year, 22 Transformer-related research papers were accepted by NeurIPS, the world‚Äôs most prestigious machine learning conference. *Synced* has selected ten of these works to showcase the latest Transformer trends ‚Äî from extended use of the neural architecture to innovative advancements in technique, architectural design changes and more.

Here is a quick read: [NeurIPS 2020 | Teaching Transformers New Tricks](https://syncedreview.com/2020/12/15/neurips-2020-teaching-transformers-new-tricks/)"
63,deeplearning,gpt,top,2023-11-15 10:30:36,"GPT-4 Turbo: Die Zukunft der K√ºnstlichen Intelligenz, entwickelt von OpenAI",Webglobic_tech,False,0.86,15,17vqtlk,https://webglobic.com/magazine/,0,1700044236.0,
64,deeplearning,gpt,top,2020-02-12 16:27:26,GPT Explained!,HenryAILabs,False,0.84,14,f2tqky,https://www.reddit.com/r/deeplearning/comments/f2tqky/gpt_explained/,0,1581524846.0,[https://youtu.be/9ebPNEHRwXU](https://youtu.be/9ebPNEHRwXU)
65,deeplearning,gpt,top,2019-08-24 10:59:28,Gpt-2 online version!,susmit410,False,0.84,15,cus11v,https://www.reddit.com/r/deeplearning/comments/cus11v/gpt2_online_version/,10,1566644368.0,https://talktotransformer.com/
66,deeplearning,gpt,top,2020-03-15 04:38:51,Making an Omelette with AI (GPT-2),jerry0822,False,0.72,16,fivz40,https://youtu.be/EV8LK49f3D4,0,1584247131.0,
67,deeplearning,gpt,top,2023-04-24 22:41:22,AbridgIt - a browser extension that uses GPT to summarize any article you find on the web with a single click,nick313,False,0.77,14,12xzadf,https://www.reddit.com/r/deeplearning/comments/12xzadf/abridgit_a_browser_extension_that_uses_gpt_to/,4,1682376082.0,"Hi everyone,

I‚Äôd love your feedback on a new project I‚Äôm working on called [AbridgIt](http://www.abridgit.com/). When playing with GPT, one of my favorite things to ask it is to summarize long text. So, I built a simple Chrome browser extension that will automatically summarize any article you find on the web with a single click. This is version 1 so it‚Äôs pretty simple, but I would love to get some people to try it (it‚Äôs free) and give some feedback.

Example of how it works:

&#x200B;

https://preview.redd.it/m1ryu2u9uwva1.png?width=640&format=png&auto=webp&s=4626472cfaed0b1cedbb3492f1a1209491a8a265

 Check it out and let me know what you think."
68,deeplearning,gpt,top,2023-03-13 12:50:10,Learning logical relationships with neural networks with differential ILP,Neurosymbolic,False,1.0,13,11q8tir,https://www.reddit.com/r/deeplearning/comments/11q8tir/learning_logical_relationships_with_neural/,3,1678711810.0,"Since last week‚Äôs post on my lab‚Äôs software package [PyReason](https://neurosymoblic.asu.edu/pyreason/), I got a lot of questions on if it would be possible for a neural network to learn logical relationships from data. After all, ChatGPT seems to be able to generate Python code, and Meta released a NeurIPS paper to show you can learn math equations from data using a transformer-based model, so why not logic? In this article, we will one line of research in this area ‚Äì differential ILP.

The research in this area really kicked off with a 2018 [paper from DeepMind](https://www.reddit.com/r/deeplearning/jair.org/index.php/jair/article/view/11172) where Richard Evans and Edward Grefenstette showed that you could adapt techniques from ‚Äúinductive logic programming‚Äù to use gradient descent, and learn logical rules from data. Previous (non-neural) work on inductive logic programming was generally not designed to work with noisy data and instead fit the historical examples in a precise manner. Evans and Grefenstette utilized a neural architecture and a loss function ‚Äì and they showed they could handle noisy data and even do some level of integration with CNN‚Äôs. Their neural architecture mimicked a set of candidate logical rules ‚Äì and the rules assigned higher weights by gradient descent would be thought to best fit the data. However, a downside to this approach is that the neural network was [quintic in the size of the input](https://www.youtube.com/watch?v=SOnAE0EyX8c&list=PLpqh-PUKX-i7URwnkTqpAkSchJHvbxZHB&index=5). This is why they only applied their approach on very small problems ‚Äì it did not see very wide adoption.

That said, in the last two years, there have been some notable follow-ons to this work. Researchers out of Kyoto University and NTT introduced a manner to learn rules that are more expressive in a different manner by allowing function symbols in the logical language ([Shindo et al., AAAI 2021](https://ojs.aaai.org/index.php/AAAI/article/view/16637/16444)). They leverage a clause search and refinement process to limit the number of candidate rules ‚Äì hence limiting the size of the neural network. A student team from ASU created a presentation on their work for our recent seminar course on neuro symbolic AI. We released a three part video series from their talk:

[Part 1: Review of differentiable inductive logic programming](https://www.youtube.com/watch?v=JIS78a40q8U&t=270s)

[Part 2: Clause search and refinement In our recent video series](https://www.youtube.com/watch?v=nzfbxlHUwuE&t=345s)

[Part 3: Experiments](https://www.youtube.com/watch?v=-fKWNtHUIN0&t=27s)

[Slides](https://labs.engineering.asu.edu/labv2/wp-content/uploads/sites/82/2022/10/Shindo_dILP.pdf)

Some think that the ability to learn such relationships will represent a significant advancement in ML, specifically addressing shortcomings in areas such as knowledge graph completion and reasoning about scene graphs. However, the gap still remains wide, and ILP techniques, including differentiable ILP still have a ways to go. Really interested in what your thoughts are, feel free to comment below."
69,deeplearning,gpt,top,2022-12-23 14:35:17,How to change career trajectory to NLP engineer,Creative-Milk-8266,False,0.84,12,zth8rl,https://www.reddit.com/r/deeplearning/comments/zth8rl/how_to_change_career_trajectory_to_nlp_engineer/,3,1671806117.0," A little of my background - 5 years experience in data science. Mostly related to prototyping statistical models and optimization problems, bringing them into production. Some experience in building pipeline and orchestration flow with AWS services.

I have basic understanding on Transformers, BERT, GPT. Did my first NLP Kaggle competition the first time recently.

I'd like my next job to be a NLP engineer. How should I prepare myself for it?

Here's some of the items I'm thinking

&#x200B;

1. More hands on projects I can put on resume, including integration with cloud services. Any recommendations on what kinds of projects I should pick?  
 
2. Tryout techniques of speeding up models like distilled model, dynamic shape, quantization. Anything else that would be helpful?  
 
3. Understand lower level of GPU programming knowledges. Not sure if this is helpful for me finding a NLP job. If so, what kind of things I can do to go deeper on this subject. I'm currently taking¬†[Intro to Parallel Programming](https://classroom.udacity.com/courses/cs344)¬†CS344 course on Udemy (highly recommend btw).  
 
4. Grind leetcode :/  
 

Please point out other important directions I missed."
70,deeplearning,gpt,top,2023-06-29 19:49:38,"Open Orca, an open sourced replication of Microsofts Orca is in development! Heres the dataset!",Alignment-Lab-AI,False,1.0,12,14mejzk,https://www.reddit.com/r/deeplearning/comments/14mejzk/open_orca_an_open_sourced_replication_of/,2,1688068178.0,"Today we are releasing a dataset that lets open source models learn to think like GPT-4!

We call this Open Orca, as a tribute to the team who has released the Orca paper describing the data collection methods we have attempted to replicate in an open-source manner for the benefit of humanity.

With this data, we expect new open source models to be developed which are smaller, faster, and smarter than ever before because were going to be the ones doing the developing!

[https://huggingface.co/datasets/Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)

We'd like to give special recognition to the following contributors for their significant efforts and dedication:

caseus

Eric Hartford

NanoBit

Pankaj

winddude

Rohan

[http://alignmentlab.ai/:](http://alignmentlab.ai/:)

Entropi

neverendingtoast

AtlasUnified

AutoMeta

lightningRalf

NanoBit

caseus

The Orca paper has been replicated to as fine of a degree of precision as a motley crew of ML nerds toiling for weeks could pull off (a very high degree).

We will be releasing trained Orca models as the training currently in progress completes.

The dataset is still in final cleanup, and we will continue with further augmentations beyond the base Orca data in due time.

Right now, we are testing our fifth iteration of Orca on a subset of the final data, and are just about to jump into the final stages!

Many thanks to NanoBit and Caseus, makers of Axolotl \[[https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)\] for lending us their expertise on the platform that developed and trained manticore, minotaur, and many others!

If you want to follow along, meet the devs, ask us questions, get involved, or check out our other projects, such as:

Landmark Attention

[https://twitter.com/Yampeleg's](https://twitter.com/Yampeleg's) recently announced context extension method, which outperforms rope (were going to push this one later today)

EDIT: We've been made aware that Eric Hartford, a team member who chose to depart our team yesterday after some internal discussion of our grievances, has made claims to be the sole originator of the Open Orca project and to claim the work as his own. We wish to clarify that this was a team effort from the outset, and he was one of over a dozen data scientists, machine learning engineers, and other specialists who have been involved in this project from the outset.

Eric joined the team with the mutual understanding that we were all to be treated as equals and get our due credit for involvement, as well as say in group decisions.

He made snap decisions on behalf of the team contrary to long term plans, including announcing the project publicly on his blog, and implying that he was the sole originator and project lead.

We attempted to reconcile this internally, but he chose to depart from the team.

As such, we elected to release the data publicly in advance of original plans.

We have appropriately attributed he and all other contributors, as was originally planned.

We thank Eric for his contributions to the project and wish him well on his individual endeavors.

This repo is the original repo from which the entire team had agreed to work out of and publish out of from the outset.

Eric's repo represents his duplication and augmentation of the team's collective effort, initiated after he had chosen to depart the team."
71,deeplearning,gpt,top,2022-10-08 14:29:00,Unsupervised training objective for auto-regressive models,Expert-Departure-236,False,0.94,13,xyu69q,https://www.reddit.com/r/deeplearning/comments/xyu69q/unsupervised_training_objective_for/,0,1665239340.0,"What are some unsupervised training objectives for auto-regressive models like GPT etc? Apart from CLM  
For example -  Bert, we have NSP(next sentence prediction) other than MLM"
72,deeplearning,gpt,top,2023-10-11 16:40:27,Is it possible to train a GPT-2 model on free google colab?,JastorJ,False,0.93,11,175ikm3,https://www.reddit.com/r/deeplearning/comments/175ikm3/is_it_possible_to_train_a_gpt2_model_on_free/,6,1697042427.0, My course has an assignment where we have to implement a research paper  and I was thinking about implementing GPT2 model but I am worried that  it could take enormous resources to train it properly. Is it possible to  train it on google colab using a small amount of text data to get good  results from it.  I don't have access to GPU so I have to use colab.
73,deeplearning,gpt,top,2022-04-09 09:04:28,what are the Exact Hardware Requirements for GPT-2 1.5 B,Siyam_fahad,False,0.93,13,tzpcoo,https://www.reddit.com/r/deeplearning/comments/tzpcoo/what_are_the_exact_hardware_requirements_for_gpt2/,11,1649495068.0," I want to train Gpt-2 over a very large amount of data (Terabytes of text data), what hardware I will require for it, there is no issue with the budget, I want to train it in the best and fastest way possible, please help me with your best advice :)"
74,deeplearning,gpt,top,2022-12-03 19:29:01,BlogNLP: AI Blog Writing Tool,britdev,False,0.88,12,zboc8w,https://www.reddit.com/r/deeplearning/comments/zboc8w/blognlp_ai_blog_writing_tool/,7,1670095741.0,"Hey everyone,

I developed this web app with Open AI's GPT-3 to provide a free, helpful resource for generating blog content, outlines, and more - so you can beat writer's block! I'm sure you'll find it useful and I'd really appreciate it if you shared it with others ‚ù§Ô∏è.

[https://www.blognlp.com/](https://www.blognlp.com/)"
75,deeplearning,gpt,top,2023-04-02 18:10:37,Should we draw inspiration from Deep learning/Computer vision world for fine-tuning LLMs?,Vegetable-Skill-9700,False,0.78,12,129t3tl,https://www.reddit.com/r/deeplearning/comments/129t3tl/should_we_draw_inspiration_from_deep/,8,1680459037.0,"With HuggingGPT, BloombergGPT, and OpenAI's chatGPT store, it looks like the world is moving towards specialized GPTs for specialized tasks. What do you think are the best tips & tricks when it comes to fine-tuning and refining these task-specific GPTs?

Over the last decade, I have built many computer vision models (for human pose estimation, action classification, etc.), and our general approach was always based on Transfer learning. Take a state-of-the-art public model and fine-tune it by collecting data for the given use case.

Do you think that paradigm still holds true for LLMs?

Based on my experience, I believe observing the model's performance as it interacts with real-world data, identifying failure cases (where the model's outputs are wrong), and using them to create a high-quality retraining dataset will be the key.

&#x200B;

P.S. I am building an open-source project UpTrain ([https://github.com/uptrain-ai/uptrain](https://github.com/uptrain-ai/uptrain)), which helps data scientists to do so. We just wrote a blog on how this principle can be applied to fine-tune an LLM for a conversation summarization task. Check it out here: [https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation\_summarization](https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation_summarization)"
76,deeplearning,gpt,top,2021-07-28 14:19:34,AI Email Generator Web App with GPT-3,thelazyaz,False,0.87,11,otaun5,https://www.youtube.com/watch?v=oJWBQKrF4uM&feature=youtu.be,1,1627481974.0,
77,deeplearning,gpt,top,2023-11-15 20:27:19,"Nvidia introduces the H200, an AI-crunching monster GPU that may speed up ChatGPT",keghn,False,0.88,12,17w2vgj,https://arstechnica.com/information-technology/2023/11/nvidia-introduces-its-most-powerful-gpu-yet-designed-for-accelerating-ai/,0,1700080039.0,
78,deeplearning,gpt,top,2023-05-15 15:10:52,[P] ts-tok: Time-Series Forecasting with Classification,arpytanshu,False,0.92,11,13ib22w,https://www.reddit.com/r/deeplearning/comments/13ib22w/p_tstok_timeseries_forecasting_with_classification/,3,1684163452.0,"Hey everyone!  
I wanted to share with you a weekend project I've been working on called **ts-tok**. It's an experimental approach to time-series forecasting that uses classification instead of regression.  
Essentially, we take a range of time-series values and transform them into a fixed vocabulary of tokens. This allows for a seamless training of GPT like models without changing the architecture or loss function.  
There are some subtleties required for data preparation for training, and I've outlined these in the README, so feel free to check it out!  
While this approach 'may' not have practical applications in the real world, it's been a fun experiment to explore.  
I've included some forecasting results in the output/ folder, so feel free to check those out! Open to feedback from the community about potential use cases and limitations of this approach.  
Thanks for taking the time to read about this project!  
[https://github.com/arpytanshu1/ts-tok](https://github.com/arpytanshu1/ts-tok)"
79,deeplearning,gpt,top,2020-05-21 18:38:22,Understanding encoder and decoder structures within transformers,de1pher,False,0.94,12,go2ha2,https://www.reddit.com/r/deeplearning/comments/go2ha2/understanding_encoder_and_decoder_structures/,6,1590086302.0,"Hi all,

I'm learning about sequence-to-sequence transformers and I'm having a hard time understanding the encoder-decoder pattern.

As far as I understand it encoders are effectively designed to extract features that would enable a decoder to make sense of them. The most notable example would be an autoencoder which condenses the original input into a dense lower-dimensional space that can later be used for a variety of tasks. I believe that a decoder, on the other hand, is meant to translate an input signal (raw or encoded data) and generate some useful predictions that humans should be able to make sense of.

If my understanding is correct, then there are two problems that bother me:

1. Aren't all neural networks with at least 1 hidden layer transformers then? We can think of the hidden layer as the encoder and the output as the decoder. If we have more than 1 hidden layer, then it might become difficult to work out where the encoder ends and the decoder begins. The ""encoder-decoder"" characterisation initially led me to think that it's some kind of a multi-agent setup akin to GANs
2. BERT is considered to be an encoder-only transformer and GPT is a decoder-only transformer -- why is that? First, if neither of them contains both an encoder and a decoder, then why are they even considered to be transformers? And what is it that makes BERT and encoder model and GPT a decoder model when both ultimately output token probabilities for a given input string?

I feel like I'm definitely missing something here and I would appreciate if you guys could help out :)

Many thanks!

&#x200B;

EDIT:

Hey guys, I'd like to thank those who attempted to help me :)

I **think** I'm beginning to develop a better understanding of the transformer model, but if you think that I'm still missing something then please correct me. First, I'd like to point out that there is *the* Transformer model proposed by the [""Attention Is All You Need""](https://arxiv.org/abs/1706.03762) paper, whereas I was originally referring to transformers as a general class of models consisting of arbitrary encoders and decoders (which do not have to use attention or positional encoding or anything like that by definition).

The encoder-only (e.g. BERT) and decoder-only (e.g. GPT) transformers effectively borrow the encoders and decoders from *the* transformer model and modify them which explains their names ""encoder-only transformer"" and ""decoder-only transformer"".

The general idea of encoders is to contextualize the **input** sequence A into a rich representation of itself. The general idea of a decoder is to parse the **output** sequence B together with the contextual information from the encoder and effectively find the relationship between the enoded input and the required output. This explains the difference between an arbitrary NN with a hidden layer and a true encoder-decoder architecture.

I'd also like to recommend [this video](https://www.youtube.com/watch?v=TQQlZhbC5ps) from CodeEmporium which provides an excellent explanation of the transformer model (thanks to u/adikhad for suggesting it)."
80,deeplearning,gpt,top,2024-01-10 06:21:19,Are NLP based AI skills useless?,SnooBeans7516,False,0.7,11,1931m9b,https://www.reddit.com/r/deeplearning/comments/1931m9b/are_nlp_based_ai_skills_useless/,9,1704867679.0,"So something's been on my mind recently and I wanted to get Reddit's thoughts.

The thing that is troubling me as I get deeper, is it seems that for a lot of language based NLP tasks, ChatGPT or these other foundation models seem SOTA for 99% of tasks. In terms of developing valuable skills and products, it feels like the two options are: 

1. Research and training foundation models at large research labs 

2. Using GPT APIs in some sort of system (RAG or just raw prompts)

I feel like this issue is rooted in language itself being a somewhat static medium that can be mostly mastered by a single model. As opposed to images where we see a lot of cool projects from people that have to employ many different techniques to complete a specific task.

\## Example

Unlike images, the use cases seem much more straightforward, and don't require any special work other than throwing GPT at it. 

For example, I wanted to create a BERT like model to replace ingredients in a recipe, conditioned on a user's requirements or desires. The problem is, the more I worked on this and trained some models, the more it seemed that ChatGPT could just do it better with some prompt engineering...

==================

All this to say, I was previously motivated to be a part-time NLP researcher, implementing research into products in cool ways. Now it kind of feels like the only actually valuable investment is into ""AI engineering"", building systems using a GPT API:/"
81,deeplearning,gpt,top,2023-02-17 02:54:52,How likely is ChatGPT to be weaponized as an information pollution tool? What are the possible implementation paths? How to prevent possible attacks?,zcwang0702,False,0.71,10,1148t20,https://www.reddit.com/r/deeplearning/comments/1148t20/how_likely_is_chatgpt_to_be_weaponized_as_an/,14,1676602492.0,
82,deeplearning,gpt,top,2020-11-25 12:55:02,This AI Can Generate the Other Half of a Picture Using a GPT Model,OnlyProggingForFun,False,0.78,10,k0robn,https://youtu.be/FwXQ568_io0,2,1606308902.0,
83,deeplearning,gpt,top,2023-08-02 11:33:32,Using PDFs with GPT Models,Disastrous_Look_1745,False,0.76,11,15g6i4x,https://www.reddit.com/r/deeplearning/comments/15g6i4x/using_pdfs_with_gpt_models/,2,1690976012.0,Found a blog talking about how we can interact with PDFs in Python by using GPT API & Langchain. It talks about some pretty cool automations you can build involving PDFs - [https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/](https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/)
84,deeplearning,gpt,top,2023-03-15 00:07:51,GPTMinusOne - AI that hides the use of ChatGPT and GPT4,tomd_96,False,0.81,9,11rfgbs,https://github.com/tom-doerr/gpt_minus_one,3,1678838871.0,
85,deeplearning,gpt,top,2023-04-02 12:37:38,[N] Software 3.0 Blog Post Release üî•,DragonLord9,False,0.73,10,129k24i,https://www.reddit.com/r/deeplearning/comments/129k24i/n_software_30_blog_post_release/,3,1680439058.0,"Hi all, excited to share my blog post on [**Software 3.0**](https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm_campaign=post&utm_medium=web)

https://preview.redd.it/9b4hjkkhugra1.png?width=1500&format=png&auto=webp&s=e341f3ab4c3c8abb206df8daa17428a297ff61e2

The blog post offers an insightful read on the new GPT-powered programming paradigm where the new programming language is simply ""*English*"", as well as recent developments in AI.

The post was originally written before GPT-4 release, and the predictions seem to have held surprisingly well. Knowledge cutoff date 28 Feb 2023.

Please read and share!! Happy to answer any follow-ups here or on DM üòä

Tweet: [https://twitter.com/DivGarg9/status/1642229948185280521?s=20](https://twitter.com/DivGarg9/status/1642229948185280521?s=20)

Blog:¬†[https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm\_campaign=post&utm\_medium=web](https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm_campaign=post&utm_medium=web)"
86,deeplearning,gpt,top,2023-11-08 15:37:08,Start with Large Language Models (LLMs) in 2023,OnlyProggingForFun,False,0.69,10,17qo9lt,https://www.reddit.com/r/deeplearning/comments/17qo9lt/start_with_large_language_models_llms_in_2023/,11,1699457828.0,"This is a complete guide to start and improve your LLM skills in 2023 without an advanced background in the field and stay up-to-date with the latest news and state-of-the-art techniques!

The complete article: https://www.louisbouchard.ai/from-zero-to-hero-with-llms/

All the links on GitHub: https://github.com/louisfb01/start-llms 

Artificial is a fantastic field, and so are language models like GPT-4, Claude..., but it goes extremely fast. Don't miss out on the most important and exciting news by joining great communities, people, newsletters, and more you can all find in this guide!

This guide is intended for anyone with a small background in programming and machine learning. Simple python knowledge is enough to get you started. There is no specific order to follow, but a classic path would be from top to bottom. If you don't like reading books, skip it, if you don't want to follow an online course, you can skip it as well. There is not a single way to become a ""LLM expert"" and with motivation, you can absolutely achieve it."
87,deeplearning,gpt,top,2023-05-25 15:49:21,New Weaviate Podcast - Neurosymbolic Search!,CShorten,False,1.0,9,13rla6o,https://www.reddit.com/r/deeplearning/comments/13rla6o/new_weaviate_podcast_neurosymbolic_search/,1,1685029761.0,"Hey everyone! I am super excited to publish Weaviate Podcast #49 with Professor Laura Dietz!

There are two main sections in this podcast: (1) Neurosymbolic AI methods in Search and (2) Using LLMs for Relevance Judgements. We also conclude with reflecting on broader societal discussions around AI advancements such as ChatGPT.  

Neurosymbolic AI broadly describes the combination of Neural-inspired computing technologies such as Deep Learning with symbolic algorithms such as tree search or say Knowledge Graph data structures! Dr. Dietz explained many interesting ideas particularly around Entity Linking and Entity Ranking. I think the intersection of Vector Search with Knowledge Graph technologies is quite exciting -- of course we are also seeing more combination in how LLMs can use query languages, or say the manifestation of MCTS in the Tree-of-Thoughts paper.  

Using LLMs for Relevance Judgement is another absolutely massive emerging area of search technology! There are quite a few dimensions to this -- Professor Dietz and collaborators have recently published ""Perspectives on Large Language Models for Relevance Judgement"" on the Human-Machine Collaboration spectrum for annotating relevance judgements. I personally think this will be very impactful for people looking to build search functionality but don't yet have user data on queries and want to generate synthetic queries to test different models and ranking systems. Dr. Dietz also explained how this is broadly related to the judgement of say abstractive summarization and question answering -- quite similar in spirit to the ChatArena Weaviate Podcast we also recently published.

I learned so much from speaking with Dr. Dietz, I hope you enjoy the podcast!

https://www.youtube.com/watch?v=2s\_GGMZ\_Zgs"
88,deeplearning,gpt,top,2024-02-19 15:46:54,"When will the advancement in things like GPT, Stable Diffusion and Sora plateau?",yasserius,False,0.76,9,1auqe2g,https://www.reddit.com/r/deeplearning/comments/1auqe2g/when_will_the_advancement_in_things_like_gpt/,13,1708357614.0,"I know deep learning based models are seeing the fastest advancements ever, thanks to huge compute and data. 

But people keep extrapolating and people talk about accelerating / exponential growth and the singularity being reached.

But I would argue most of these models are based off novel neural architectures and then they hyperparameter tune and train on more data, and voila, you see huge advancements and it makes it look like accelerating progress.

But once we've exhausted the different architectures and optimized the hyperparameters, shouldn't we plateau? I'm not saying we will plateau anytime soon, but can we really expect more emergent behavior as we keep scaling up?

Please don't attack me for being a noob, constructive explanations based on logic will be appreciated, thanks!"
89,deeplearning,gpt,top,2021-09-30 14:07:00,New to NLP (but not machine learning) - questions about Huggingface and NLP model development with additional text/non-text features,jsxgd,False,0.91,9,pykia3,https://www.reddit.com/r/deeplearning/comments/pykia3/new_to_nlp_but_not_machine_learning_questions/,4,1633010820.0,"Hi everyone,

&#x200B;

My work is almost always focused on structured, tabular data. Recently, though, I have been working on some tasks that are more centered on NLP for personal enrichment. I've generally been understanding well how some of the model architectures work like BERT or GPT. And I understand the difference between common NLP tasks like fill-mask and text generation. I've learned a lot from the Huggingface docs.

&#x200B;

I have two questions that are more about actually engineering something with these models:

1) I can see in Huggingface that models are marked for a specific task, like fill-mask. However, in tutorials I find, I can see that these models are being used for other tasks with seemingly good performance. For example, I found a tutorial that uses \`distilbert-base-multilingual-cased\` for a novel text classification model (classifying article text as one of several news categories). But in Huggingface, this model is labeled as a fill-mask model. What gives? Is it mislabeled? Or can I use any model for any task, just with varying degrees of success?

&#x200B;

2) I'm having a hard time finding any tutorials that mix text data with additional features which may ALSO be text or just numeric/categorical. For example, if my task is classifying a sent email as ""opened"" or ""not opened"", my main feature might be the email subject text. I might also (optionally) have a pre-header text, which in some email clients appears right below the subject. Then, I also have some additional potential features like the date the email was sent, the domain of the recipient email, etc. These features may also have a variable relationship with the text, e.g. ""Happy Christmas"" as an email subject may fare differently in December vs. January. Are there any good resources to learn how to incorporate these kinds of features into the same model?

3) More generally about deep learning (particularly if you're using tensorflow/keras) - but also with respect to the questions above - what's the best way to utilize aggregate data for classification? If I'm again looking at email data, I can of course look at this recipient-by-recipient with a 0/1 binary target field for ""opened\_email"". But this data set is huge, and in this format would be repetitive as subjects would be the same for recipients getting the same email. I can instead aggregate to a per-subject data set with two fields called ""Opens"" and ""NonOpens"" containing the counts for each type of event. Or I can do ""OpenRate"" and ""TotalRecipients"" containing the percent of recipients who opened the email and the denominator of the rate. In more classical models/packages (xgboost, GLMs, etc) it's pretty easy to make use of data in this format for binary classification. Is it similarly just as easy in a NN built with tensorflow/keras?

&#x200B;

Thanks!"
90,deeplearning,gpt,top,2021-12-17 16:25:47,Transformer assimilates syntax perfectly,jssmith42,False,0.84,8,ril1wx,https://www.reddit.com/r/deeplearning/comments/ril1wx/transformer_assimilates_syntax_perfectly/,4,1639758347.0,"Has anyone analysed why GPT-3 seems to master the syntax of languages nearly perfectly as opposed to not having a perfect understanding of higher-level aspects of cognition?

It could be a simple answer, that syntax is less of a complex system/pattern/structure than conceptual understanding of the world.

But I feel like there is something more interesting to be said.

For example, it seems like the bigger the model, the smarter it becomes.

Is AI as simple as, we have a structure (a neural network) that can intuitively understand any system or phenomenon because it finds some kind of model for it, a layered series of weights corresponding to some conceptual hierarchy. It just depends what order the phenomenon is. A hyper-complex phenomenon needs 100 layers, or whatever. A simple one only needs 3. In either case, there is conceivably nothing a neural network cannot eventually understand.

Is this true? If so, it‚Äôs a pretty wild notion to contemplate."
91,deeplearning,gpt,top,2023-04-07 10:28:52,"Series of Surveys on ChatGPT, Generative AI (AIGC), and Diffusion Models",Learningforeverrrrr,False,0.86,9,12egmab,https://www.reddit.com/r/deeplearning/comments/12egmab/series_of_surveys_on_chatgpt_generative_ai_aigc/,0,1680863332.0,"* **A survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)
* **A survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)
* **A survey on Text-to-image diffusion models:** [**Text-to-image Diffusion Models in Generative AI: A Survey**](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)
* **A survey on Audio diffusion models:** [**A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI**](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI)
* **A survey on Graph diffusion models:** [**A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material**](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material)

**ChatGPT goes viral.** Launched by OpenAI on November 30, 2022, ChatGPT has attracted unprecedented attention due to its powerful abilities all over the world.  It took only 5 days \[1\] and 2 months \[2\] for ChatGPT to have 1 million users and 100 million monthly users after launch, making it the fastest-growing consumer application in history. ChatGPT can be seen as the milestone for the GPT family to go viral. In academia, ChatGPT has also inspired a large number of works discussing its applications in multiple fields, with **more than 500 papers within four months** after release and **the number is still increasing rapidly.**  This brings a huge challenge for a researcher who hopes to have an overview of ChatGPT applications or hopes to start his or her journey with ChatGPT in their own field.  **To help more people keep up with the latest progress of the GPT family,** we‚Äôre glad to share a self-contained survey that not only summarizes **the recent applications** of ChatGPT and other GPT variants like GPT-4, but also introduces the **underlying techniques** and **challenges.** Please refer to the following link for the paper: [One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era).

&#x200B;

**From ChatGPT to Generative AI.**  One highlighting ability of the GPT family is that it can generate natural languages, which falls into the area of Generative AI. Apart from text, Generative AI can also generate content in other modalities, such as image, audio, and graph. More excitingly, Generative AI is able to convert data from one modality to another one, such as the text-to-image task (generating images from text). **To help readers have a better overview of Generative AI,** we provide a complete survey on underlying **techniques,** summary and development of **typical tasks in academia**, and also **industrial applications.** Please refer to the following link for the paper.  [A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

&#x200B;

**From Generative AI to Diffusion Models.** The prosperity of a field is always driven by the development of technology, and so is Generative AI.  Different from ChatGPT which generates text based on the transformer, **diffuson models** have greatly accelerated the development of other fields in Generative AI, such as image synthesis.  Although we provide a summary of diffusion models and typical tasks in the Generative AI survey, we cannot include detailed discussions due to paper length limitations. **For those who are interested in the technical details of diffusion models and the recent progress of their applications in Generative AI,** we provide three self-contained surveys on **how diffusion models are applied in three typical areas: Text-to-image diffusion models** (also includes related tasks such as image editing)**, Audio diffusion models** (including text to speech synthesis and enhancement), and **Graph diffusion models** (including molecule, protein and material areas). Please refer to the following links for the paper.

* [Text-to-image Diffusion Models in Generative AI: A Survey](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)
* [A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI)
* [A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material)

We hope our survey series will help people for a better understanding of ChatGPT and Generative AI, and we will update the survey regularly to include the latest progress. Please refer to the personal pages of the authors for the latest updates on surveys. If you have any suggestions or problems, please feel free to contact us.

\[1\] Greg Brockman, co-founder of OpenAI, [https://twitter.com/gdb/status/1599683104142430208?lang=en](https://twitter.com/gdb/status/1599683104142430208?lang=en)

\[2\] Reuters, [https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/)"
92,deeplearning,gpt,top,2023-10-26 17:59:49,Long text summarization tool how-to (700+ pages),Old_Swan8945,False,0.84,8,17h2fbk,https://www.reddit.com/r/deeplearning/comments/17h2fbk/long_text_summarization_tool_howto_700_pages/,8,1698343189.0,"Hey all I've seen a bunch of posts about summarization of long texts and seems like there's been a lot of challenges, so wanted to spread some knowledge out there about some things I've discovered as I launched my tool here ([summarize-article.co](https://summarize-article.co)) (longest text was a psych book from one of my users at 700+ pages).

The most basic problem in the summarization process is the GPT context window length, so the basic strategy I follow is the following:

1. Chunk the text into chunks that fit inside the context window
2. Recursively summarize the summaries until it becomes manageable
3. Use a long context-window model to generate the final summary using a prompt that takes the recursively-generated summaries and re-restructures the output
4. Additional prompt magic to optimize the outputs (DM me for more details :D)

Anyway, would appreciate any feedback on the results or anything you think could be improved, otherwise feel free to check it out or msg me if you want to learn more about how it works!"
93,deeplearning,gpt,top,2020-02-13 15:38:23,GPT-2 Explained!,HenryAILabs,False,0.86,10,f3bqlv,https://www.reddit.com/r/deeplearning/comments/f3bqlv/gpt2_explained/,0,1581608303.0,https://youtu.be/UULqu7LQoHs
94,deeplearning,gpt,top,2022-06-15 16:07:22,Join us for the OpenAI GPT-3 Deep Learning Labs Hackathon!,zakrzzz,False,0.9,8,vcxzp4,https://www.reddit.com/r/deeplearning/comments/vcxzp4/join_us_for_the_openai_gpt3_deep_learning_labs/,0,1655309242.0,"We are waiting for all of you, AI enthusiasts with coding experience and without, on the 24th - 26th of June to help you turn your ground-breaking ideas into reality!

Register here - [https://lablab.ai/event/gpt3-online](https://lablab.ai/event/gpt3-online)  


https://preview.redd.it/mhr93wgo6t591.png?width=1600&format=png&auto=webp&s=07e23c79830db8061eb300f76b64588b01219ebc"
95,deeplearning,gpt,top,2020-06-18 01:54:32,[Video Analysis] ImageGPT,HenryAILabs,False,0.82,7,hb5bit,https://www.reddit.com/r/deeplearning/comments/hb5bit/video_analysis_imagegpt/,0,1592445272.0,https://youtu.be/7rFLnQdl22c
96,deeplearning,gpt,top,2023-02-03 19:38:56,GPT-2 small model (124M params) hw requirements,honzovy-klipy,False,0.9,7,10st418,https://www.reddit.com/r/deeplearning/comments/10st418/gpt2_small_model_124m_params_hw_requirements/,1,1675453136.0,"Hey, I was wandering how much VRAM and RAM do I need for running (inference only) gpt2-small model from hugging face, but was not able to find anything. Can somebody help please?"
97,deeplearning,gpt,top,2023-01-14 14:48:43,Scaling Language Models Shines Light On The Future Of AI ‚≠ï,LesleyFair,False,0.71,7,10bq685,https://www.reddit.com/r/deeplearning/comments/10bq685/scaling_language_models_shines_light_on_the/,1,1673707723.0,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs might allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* reduce hallucination and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example is words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (a proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/jlh726eku0ca1.png?width=800&format=png&auto=webp&s=55d170251a967f31b36f01864af6bb7e2dbda253)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.At **TheDecoding** ‚≠ï, I send out a thoughtful newsletter about ML research and the data economy once a week.No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)"
98,deeplearning,gpt,top,2023-03-11 00:38:10,Generate READMEs Using ChatGPT,tomd_96,False,0.92,9,11o5zyl,https://www.reddit.com/r/deeplearning/comments/11o5zyl/generate_readmes_using_chatgpt/,0,1678495090.0,"&#x200B;

https://i.redd.it/k375our2a0na1.gif

&#x200B;

You can use this program I wrote to generate readmes: [https://github.com/tom-doerr/codex-readme](https://github.com/tom-doerr/codex-readme)

&#x200B;

It's far from perfect, but I now added ChatGPT and it is surprisingly good at inferring what the project is about. It often generates interesting usage examples and explains the available command line options.

&#x200B;

You probably won't yet use this for larger projects, but I think this can make sense for small projects or single scripts. Many small scripts are very useful but might never be published because of the work that is required to document and explain it. Using this AI might assist you with that.

&#x200B;

Reportedly GPT-4 is coming out next week, which probably would make it even better.

&#x200B;

What do you think?"
99,deeplearning,gpt,top,2020-06-10 20:36:33,"GPT-3: The $4,600,000 Language model",mippie_moe,False,0.76,6,h0jm54,https://lambdalabs.com/blog/demystifying-gpt-3/,4,1591821393.0,
100,deeplearning,gpt,comments,2023-12-24 07:52:27,Is famous argument that people need much less data to train is always true?,imtaevi,False,0.83,23,18pqlzi,https://www.reddit.com/r/deeplearning/comments/18pqlzi/is_famous_argument_that_people_need_much_less/,61,1703404347.0,"Many people repeat that people need much less data to train than neural networks.
How about case when neural network already trained on many other similar tasks? 
Because for many cases people are already trained on many other similar tasks by 

1 evolution 

2 childhood 

You can look lectures about child psychology that give examples of what abilities people have just from evolution.

Developmental Psychology - Lecture 01 (PSYC 240)

Also about why pretty much shape us from evolution and genetics. Look at separated twins study.

Video about low data learning from Siraj.

How to Learn from Little Data - Intro to Deep Learning #17

People was training on more than billions of images if you add how much there was thru all evolution. Imagine how many images there was starting from fist animal with eyes.

People was training on pretty much text or speech data if you add how much there was thru all evolution. Imagine how many words there was starting from fist animal with speech signals. 

If you look at lectures Developmental Psychology - Lecture 01 (PSYC 240). It looks like people not only good at learning but they already learned some cases just from their setup at birth. So some case they do not need to learn. It will look like they already learned them at some age without any training after birth. It will sound like people at age X can do Y. But they was not trained for Y.

My claim is that training of neural networks is like evolution + childhood of people. After that step both variants ai and human can learn new information pretty quickly.

About Beyond training data. What if I will make a self created story about some non existing tribe of Amazon. Could gpt analyze it? Looks like yes. Was that info in it‚Äôs database? No. SAT already have examples of what was not in database.

So in that example. Ai could learn and understand pretty fast about this tribe. It does not need millions of pages about that tribe.

I am talking about advantage of humans in not only evolution but in evolution + childhood. So if we compare scores of AI and human at SAT test. Preparation of Ai is training. Preparation of humans is evolution + childhood.

"
101,deeplearning,gpt,comments,2023-03-25 04:24:49,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.91,46,121agx4,https://www.reddit.com/r/deeplearning/comments/121agx4/do_we_really_need_100b_parameters_in_a_large/,54,1679718289.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
102,deeplearning,gpt,comments,2023-04-05 19:44:10,AI vs Humans: Can You Tell the Difference?,YoutubeStruggle,False,0.62,3,12cvkvu,https://www.reddit.com/r/deeplearning/comments/12cvkvu/ai_vs_humans_can_you_tell_the_difference/,29,1680723850.0,"We would greatly appreciate your feedback on our[AI Content Detector](https://ai-content-detector.online/) that detects text generated by ChatGPT, a large language model trained by OpenAI. Our aim is to provide a reliable tool for distinguishing between human-written text and machine-generated text, and we would love to hear your thoughts on how effective the tool is in achieving this goal. Specifically, we would like to know if you found the site easy to navigate if the results provided were accurate, and if there are any additional features you would like to see implemented. Your feedback will help us to continue improving the site and provide the best possible experience for our users. Thank you in advance for your¬†valuable¬†input!"
103,deeplearning,gpt,comments,2023-02-02 23:02:25,Why are FPGAs better than GPUs for deep learning?,Open-Dragonfly6825,False,0.92,16,10s3u1s,https://www.reddit.com/r/deeplearning/comments/10s3u1s/why_are_fpgas_better_than_gpus_for_deep_learning/,37,1675378945.0,"I've worked for some years developing scientific applications for GPUs. Recently we've been trying to integrate FPGAs into our technologies; and consequently I've been trying to understand what they are useful for.

I've found many posts here and there that claim that FPGAs are better suited than GPUs to accelerate Deep Learning/AI workloads (for example, [this one by Intel](https://www.intel.com/content/www/us/en/artificial-intelligence/programmable/fpga-gpu.html)). However, I don't understand why that would be the case. I think the problem is that all those posts try to explain what an FPGA is and what its differences are to a GPU, so that people that work on Deep Learning understand why they are better suited. Nevertheless, my position is exactly the opposite: I know quite well how a GPU works and what it is good for, I know well enough how an FPGA works and how it differs from a GPU, **but I do not know enough about Deep Learning** to understand why Deep Learning applicatios would benefit more from the special features of FPGAs rather than from the immense parallelism GPUs offers.

As far as I know, an FPGA will never beat a traditional GPU in terms of raw parallelism (or, if it does, it would be much less cost efficient). Thus, when it comes to matrix multiplications, i.e. the main operation in Deep Learning models, or convolutions, GPUs can parallelly work with much bigger matrices. The only explanation I can think of is that traditional Deep Learning applications don't necessarily use such big matrices, but rather smaller ones that can also be fully parallelized in FPGAs and benefit highly from custom-hardware optimizations (optimized matrix multiplications/tensor operations, working with reduced-bit values such as FP16, deep-pipeline parallelism, ...). However, given the recent increase in popularity of very complex models (GPT-3, dall-e, and the like) which boast using millions or even billions of parameters, it is hard to imagine that popular deep learning models work with small matrices of which fully parallel architectures can be synthesized in FPGAs.

What am I missing? Any insight will be greatly appreciated.

EDIT: I know TPUs are a thing and are regarded as ""the best option"" for deep learning acceleration. I will not be working with them, however, so I am not interested in knowing the details on how they compare with GPUs or FPGAs."
104,deeplearning,gpt,comments,2023-04-24 01:17:58,Can an average person learn how to build a LLM model?,sch1zoph_,False,0.72,27,12wxrrd,https://www.reddit.com/r/deeplearning/comments/12wxrrd/can_an_average_person_learn_how_to_build_a_llm/,29,1682299078.0,"Hello everyone. I am a 30-year-old Korean male.

To be honest, I have never really studied properly in my life. It's a little embarrassing, but that's the truth.

Recently, while using ChatGPT, I had a dream for the first time. I want to create a chatbot that can provide a light comfort to people who come for advice. I would like to create an LLM model using Transformer, and use our country's beginner's counseling manual as the basis for the database.

I am aware that there are clear limits to the level of comfort that can be provided. Therefore, if the problem is too complex or serious for this chatbot to handle, I would like to recommend the nearest mental hospital or counseling center based on the user's location. And, if the user can prove that they have visited the hospital (currently considering a direction where the hospital or counseling center can provide direct certification), I would like to create a program that provides simple benefits (such as a free Starbucks coffee coupon).

I also thought about collecting a database of categories related to people's problems (excluding personal information) and selling it to counseling or psychiatric societies. I think this could be a great help to these societies.

The problem is that I have never studied ""even once,"" and I feel scared and fearful of the unfamiliar sensation. I have never considered myself a smart person.

However, I really want to make this happen! Our country is now in a state of constant conflict, and people hate and despise each other due to strong propaganda.

As a result, the birth rate has dropped to less than 1%, leading to a decline in the population. Many people hide their pain inside and have no will to solve it. They just drink with their friends to relieve their pain. This is obviously not a solution. Therefore, Korea has a really serious suicide rate.

I may not be able to solve this problem, but I want to put one small brick to build a big barrier to stop hatred. Can an ordinary person who knows nothing learn the common sense and study needed to build an LLM model? And what direction should one take to study one by one?"
105,deeplearning,gpt,comments,2020-08-17 19:53:20,Personal GPT-3 project üöÄ: Guess the movie! You can't recall the name of that movie you watched? You know what the movie's about but you just can't remember its name? I used the GPT-3 model to solve this problem! Just feed it a small description of the movie/tv show and it will do the rest.,CallmeMehdi25,False,0.96,115,iblhzl,https://i.redd.it/z12t847vamh51.gif,29,1597694000.0,
106,deeplearning,gpt,comments,2024-01-22 19:37:33,Wanna develop my own LLM model like GPT and Gemini,missionseeker,False,0.2,0,19d47b7,https://www.reddit.com/r/deeplearning/comments/19d47b7/wanna_develop_my_own_llm_model_like_gpt_and_gemini/,24,1705952253.0,"I'm web developer and have a good understanding of data structures. I'm now interested to create my own LLM model but I didn't know from where I could start. So, if you guys help me to get some useful resources or information which would help me. So to get my own did I've to enroll myself into PhD as I didn't have no basic understanding of it. 
I request you guys to help me."
107,deeplearning,gpt,comments,2023-04-28 16:55:02,Tokenization of numerical series,Turbulent-Bet-6326,False,0.75,4,1321qjc,https://www.reddit.com/r/deeplearning/comments/1321qjc/tokenization_of_numerical_series/,24,1682700902.0,"Hello,

im trying to use GPT architecture on numerical data and i need to tokenize the input sequence of floats and then process it using GPT model. Any ideas how i could do that ? I tried to search the internet for it but with no luck.

&#x200B;

Much thanks"
108,deeplearning,gpt,comments,2024-02-17 08:34:08,Question about LLM's proficiency in advanced mathematics,WinExcellent381,False,0.6,1,1asxab6,https://www.reddit.com/r/deeplearning/comments/1asxab6/question_about_llms_proficiency_in_advanced/,21,1708158848.0,"The most cutting-edge LLMs like GPT 4 Turbo and Gemini Ultra 1.0 are great, but when it comes to mathematics, they are really limited. When will we start to have LLMs that will get a perfect score in IMO or the William Lowell Putnam Mathematical Competition every single time, and can solve master's or PhD questions about differential geometry or quantum field theory better and faster than any physicist or mathematician alive? Is AGI necessary for such capabilities or is it that researchers just haven't trained the models specifically on those tasks?"
109,deeplearning,gpt,comments,2021-08-19 07:03:53,Dual 3090 vs A6000 + Intel vs AMD?,xKaiz3n,False,0.77,7,p79uhm,https://www.reddit.com/r/deeplearning/comments/p79uhm/dual_3090_vs_a6000_intel_vs_amd/,21,1629356633.0,"Hello,

I've been asked to spec out a machine for a range of DL tasks (inc. GPT-3/4 & classification etc.). Looking at prices here (AUS) it seems the price for 2x 3090s (AUD$3000 - 4000) is around the same price as 1x A6000 (AUD$7500 - 8500). 

I've gone into this with a fairly rudimentary understanding of both hardware at this level and deep learning (read: I'm a student & interning), so apologies if I've said something particularly silly.  I'm also looking to see if there are any recommendations for CPU's:

\- do DL packages have a preference for AMD vs Intel like they do with GPU's?

\- which CPU would you guys choose that won't bottleneck the GPUs?

&#x200B;

Thank you!"
110,deeplearning,gpt,comments,2023-05-01 20:45:08,What are some small LLM models or free LLM APIs for tiny fun project?,silent_lantern,False,0.96,34,1350qtu,https://www.reddit.com/r/deeplearning/comments/1350qtu/what_are_some_small_llm_models_or_free_llm_apis/,19,1682973908.0,"Hi, I'm looking for a free/opensource api to build a small GPT webapp for fun. I want to deploy it on something like Heroku and use Flask in the backend. 


I'm also open to uploading a small-ish llm model on Heroku and use that to answer chat like queries from users.


Do you know of any such small foss models and/or free APIs?"
111,deeplearning,gpt,comments,2023-04-05 01:36:40,Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT,Time_Key8052,False,0.95,86,12c43uu,https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/,19,1680658600.0,"Vicuna : ChatGPT Alternative, Open-Source, High Quality and Low Cost 

&#x200B;

[ Relative Response Quality Assessed by GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a072b91)

Vicuna-13B has demonstrated competitive performance against other open-source models, such as Stanford Alpaca, by fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.

Evaluation using GPT-4 as a judge shows that Vicuna-13B achieves more than 90% of the quality of OpenAI ChatGPT and Google Bard AI, while outperforming other models such as Meta LLaMA (Large Language Model Meta AI) and Stanford Alpaca in more than 90% of cases.

The cost of training Vicuna-13B is approximately $300.

The training and serving code, along with an online demo, are publicly available for non-commercial use.

&#x200B;

More Information : [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT](https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)

Discord Server : [https://discord.gg/h6kCZb72G7](https://discord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysorg](https://twitter.com/lmsysorg)"
112,deeplearning,gpt,comments,2022-12-02 01:35:02,GPT-3 Generated Rap Battle between Yann LeCun & Gary Marcus,hayAbhay,False,0.99,140,za73dc,https://i.redd.it/ybfcfvez1e3a1.png,16,1669944902.0,
113,deeplearning,gpt,comments,2023-01-27 10:45:48,‚≠ï What People Are Missing About Microsoft‚Äôs $10B Investment In OpenAI,LesleyFair,False,0.95,118,10mhyek,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI‚Äôs models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let‚Äôs first look at Sam Altman‚Äôs backstory.

*Let‚Äôs go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI ‚Äî one of the most exciting and impactful organizations in all of tech.

However, OpenAI ‚Äî the rocket ship of AI innovation ‚Äî is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn‚Äôt take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can‚Äôt they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT‚Äôs outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That‚Äôs gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI‚Äôs profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI‚Äôs API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don‚Äôt have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman‚Äôs foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft‚Äôs sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI ‚Äî whatever that looks like ‚Äî the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ‚≠ï.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)‚Äã

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)‚Äã

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)‚Äã

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)‚Äã

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)‚Äã

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
114,deeplearning,gpt,comments,2021-10-09 12:58:48,Research proposal feedback,KAKA7861111,False,0.39,0,q4ktyi,https://www.reddit.com/r/deeplearning/comments/q4ktyi/research_proposal_feedback/,15,1633784328.0," 

Hi everyone.

I need your feedback on this. I am writing a research proposal. The topic is Coding AI:

1. I am proposing a solution to train a GPT-3 for code optimization. like input would be code and output would be optimized code in terms of latency and big o notation.

Any related literate. feedback on approach"
115,deeplearning,gpt,comments,2020-09-18 10:45:02,GPT-3: new AI can write like a human but don't mistake that for thinking ‚Äì neuroscientist,PowerOfLove1985,False,0.84,39,iv3rnz,https://theconversation.com/gpt-3-new-ai-can-write-like-a-human-but-dont-mistake-that-for-thinking-neuroscientist-146082,14,1600425902.0,
116,deeplearning,gpt,comments,2023-03-29 14:13:46,AI Startup Cerebras releases open source ChatGPT-like alternative models,Time_Key8052,False,0.93,45,125pbbf,https://gpt4chatgpt.tistory.com/entry/Cerebras-releases-open-source-ChatGPT-like-alternative-models,14,1680099226.0,
117,deeplearning,gpt,comments,2023-01-22 19:11:36,Apple M2 Max 96 GB unified memory for larger models vs multiple 24GB GPUs or 40GB A100s?,lol-its-funny,False,0.93,20,10irh5u,https://www.reddit.com/r/deeplearning/comments/10irh5u/apple_m2_max_96_gb_unified_memory_for_larger/,14,1674414696.0,"How feasible is it to use an Apple Silicon M2 Max, which has about [96 GB unified memory](https://www.apple.com/shop/buy-mac/macbook-pro/16-inch-space-gray-apple-m2-max-with-12-core-cpu-and-38-core-gpu-1tb) for ""large model"" deep learning? I'm inspired by the the [Chinchilla](https://arxiv.org/abs/2203.15556) paper that shows a lot of promise at 70B parameters. Outperforming ultra large models like Gopher (280B) or GPT-3 (175B) there is hope for working with < 70B parameters without needing a super computer. At least for fine tuning. I've been working with GPT-J but want to scale/tinker with larger open-sourced models.

However, I don't know how clearly the CompSci theory (M2 Max's 38-core GPU, 16 core Neural Engine accessing 96 GB unified memory) maps out to the IT reality (toolkits and libraries on macOS actually using it). My exposure is mostly around Jupyter books on Colab Pro+ (A100s) and nvidia 3080 GPUs (locally). 

I appreciate your guidance."
118,deeplearning,gpt,comments,2023-02-17 02:54:52,How likely is ChatGPT to be weaponized as an information pollution tool? What are the possible implementation paths? How to prevent possible attacks?,zcwang0702,False,0.75,12,1148t20,https://www.reddit.com/r/deeplearning/comments/1148t20/how_likely_is_chatgpt_to_be_weaponized_as_an/,14,1676602492.0,
119,deeplearning,gpt,comments,2023-04-30 03:53:26,Why do neural networks like ChatGPT use memory and processing power while at rest?,Will_Tomos_Edwards,False,0.33,0,133f4m4,https://www.reddit.com/r/deeplearning/comments/133f4m4/why_do_neural_networks_like_chatgpt_use_memory/,13,1682826806.0,"It is surprising to me that the default behavior of many neural networks is to run processes and use resources (RAM, CPU or their equivalent) . Why are they not just stored in memory by default? Obviously they must take up a lot of memory, but I don't understand why the default behaviour is to be active."
120,deeplearning,gpt,comments,2024-02-19 15:46:54,"When will the advancement in things like GPT, Stable Diffusion and Sora plateau?",yasserius,False,0.74,9,1auqe2g,https://www.reddit.com/r/deeplearning/comments/1auqe2g/when_will_the_advancement_in_things_like_gpt/,13,1708357614.0,"I know deep learning based models are seeing the fastest advancements ever, thanks to huge compute and data. 

But people keep extrapolating and people talk about accelerating / exponential growth and the singularity being reached.

But I would argue most of these models are based off novel neural architectures and then they hyperparameter tune and train on more data, and voila, you see huge advancements and it makes it look like accelerating progress.

But once we've exhausted the different architectures and optimized the hyperparameters, shouldn't we plateau? I'm not saying we will plateau anytime soon, but can we really expect more emergent behavior as we keep scaling up?

Please don't attack me for being a noob, constructive explanations based on logic will be appreciated, thanks!"
121,deeplearning,gpt,comments,2023-04-19 22:19:12,Would it be possible to help transformer models avoid lying by having the RLHF stage include 'invalid' statements?,brainhack3r,False,0.69,6,12sccd7,https://www.reddit.com/r/deeplearning/comments/12sccd7/would_it_be_possible_to_help_transformer_models/,13,1681942752.0,"I'm trying to understand the transformer/GPT models and one of the things I've been curious about is the tendency for LLMs to lie.

My background is search + big data and I'm pivoting into AI so still trying to understand a lot of this stuff.

My understanding is that GPT4 was trained with a base model, then it was aligned via RLHF,.

My thinking is that you could train GPT4 to not lie by generating a number of 'invalid' statements.

Such as:

Mickey Mouse was elected President of the United States in [invalid] 

The idea here would be to have predictions for things that are generally not true so that the model can realize when it's 'lying'"
122,deeplearning,gpt,comments,2023-12-13 14:32:50,How much should score large language model in best learning scenario?,imtaevi,False,0.67,1,18hi3kq,https://www.reddit.com/r/deeplearning/comments/18hi3kq/how_much_should_score_large_language_model_in/,12,1702477970.0,I just read that chat gpt scores as average person on sat test. Also I heard from Eliezer Yudkowsky that in perfect scenario chat gpt should answer as average person and it simulates average person that made all text that it used to learn. So in best learning scenario large language model will answer and score on test as average person because it simulates average person (like SAT exam score of 1000) or other scenario that in best learning case large language model will answer much better than average person and score like best 1 in 10 000 people because by learning it becomes more and more smart?
123,deeplearning,gpt,comments,2024-02-10 10:23:55,Home-trained transformer,prumf,False,0.92,20,1andcv7,https://www.reddit.com/r/deeplearning/comments/1andcv7/hometrained_transformer/,12,1707560635.0,"I am learning about the inner-workings of transformers, as well as GPT and BERT, but I don‚Äôt see the point of knowing about it if I can‚Äôt use my knowledge.

Training a full-blown transformer, even one with a few hundred millions parameters, is really expensive.

Do you guys have any ideas on what kind of small (and probably kind of artificial) task I could train a really small transformer, so that it would be relatively fast and inexpensive on a low-grade consumer GPU ?

thanks for your feedback ‚ù§Ô∏è"
124,deeplearning,gpt,comments,2023-12-26 02:41:39,"Are there Ai animal or vision, sound intelligence examples? Will that ai learn language faster?",imtaevi,False,0.46,0,18qxolt,https://www.reddit.com/r/deeplearning/comments/18qxolt/are_there_ai_animal_or_vision_sound_intelligence/,11,1703558499.0,"Basic idea here is to at first get most smart possible visual AI. After that add language to visual intelligence and compare how fast it learn language with existing Ai versions. Or how lower is amount of text data that it needs to achieve some score on some verbal test.

Are there examples of neural networks which are pretty smart as LLM are smart? Networks that trained 

Case 1
Only on visual information. Like animal.

Case 2
Only on sound information. Like blind animal.

So here is analogy => if chat gpt is looking like human then that visual, sound Ai will be like other animals. Like monkey or cat. Or blind cat.

Smart I mean it could answer some questions visually or make some behavior. Because it looks like some animals are more smart than other animals. So it is possible to make some test that will detect that intelligence. Test How smart is that visual or sound based ai. Some animals could solve some puzzles for example. Navigate maze could be that kind of test. 

Here are question answer examples for testing intelligence.

Question about similar things. You show to this ai  picture of cat and it will answer with other pictures of cats. 

Questions about parts of image. You show it picture of man and it will answer like showing pictures of all parts of that separately. So it will answer with many pictures => man, each clothes of man like hat, t-shirt. Also it will show many pictures for each part of human body like arm, leg, neck, hair cut.

Question about what this can do or purpose. You show it a picture of scissors and it will show picture of scissors cutting paper. 

So in that test Ai could answer with many pictures to a question that is in 1 picture. 


Also if it will be possible to make this Ai as smart as monkey or dog or cat. If we will add language information to that Ai. For example if it will learn all TikTok, Vimeo videos visually and understand something about them. Will it learn human language more fast than current version of neural networks? So maybe it will need less language data than current version of AI to achieve score of 100 iq on SAT test. It is possible that it will learn language faster because it will already understand some concepts from its visual, auditory intelligence. It will understand what is car or what is trees visually. On top of that after time language data can be added."
125,deeplearning,gpt,comments,2022-04-09 09:04:28,what are the Exact Hardware Requirements for GPT-2 1.5 B,Siyam_fahad,False,0.87,11,tzpcoo,https://www.reddit.com/r/deeplearning/comments/tzpcoo/what_are_the_exact_hardware_requirements_for_gpt2/,11,1649495068.0," I want to train Gpt-2 over a very large amount of data (Terabytes of text data), what hardware I will require for it, there is no issue with the budget, I want to train it in the best and fastest way possible, please help me with your best advice :)"
126,deeplearning,gpt,comments,2023-01-19 07:55:49,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.89,68,10fw22o,https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/,11,1674114949.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=webp&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ‚≠ï, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
127,deeplearning,gpt,comments,2023-11-08 15:37:08,Start with Large Language Models (LLMs) in 2023,OnlyProggingForFun,False,0.68,9,17qo9lt,https://www.reddit.com/r/deeplearning/comments/17qo9lt/start_with_large_language_models_llms_in_2023/,11,1699457828.0,"This is a complete guide to start and improve your LLM skills in 2023 without an advanced background in the field and stay up-to-date with the latest news and state-of-the-art techniques!

The complete article: https://www.louisbouchard.ai/from-zero-to-hero-with-llms/

All the links on GitHub: https://github.com/louisfb01/start-llms 

Artificial is a fantastic field, and so are language models like GPT-4, Claude..., but it goes extremely fast. Don't miss out on the most important and exciting news by joining great communities, people, newsletters, and more you can all find in this guide!

This guide is intended for anyone with a small background in programming and machine learning. Simple python knowledge is enough to get you started. There is no specific order to follow, but a classic path would be from top to bottom. If you don't like reading books, skip it, if you don't want to follow an online course, you can skip it as well. There is not a single way to become a ""LLM expert"" and with motivation, you can absolutely achieve it."
128,deeplearning,gpt,comments,2024-02-13 14:38:25,"This is my foundation block or sandwich stack AI model... Criticisms and inputs are welcome, formulated by openAI scholarGPT - thanks in advance.",AskACapperDOTcom,False,0.25,0,1apuyv4,/r/OpenAI/comments/1apf8jy/this_is_my_foundation_block_or_sandwich_stack_ai/,10,1707835105.0,
129,deeplearning,gpt,comments,2019-08-24 10:59:28,Gpt-2 online version!,susmit410,False,0.9,17,cus11v,https://www.reddit.com/r/deeplearning/comments/cus11v/gpt2_online_version/,10,1566644368.0,https://talktotransformer.com/
130,deeplearning,gpt,comments,2022-12-12 17:29:39,ChatGPT context length,Wild-Ad3931,False,0.95,29,zk5esp,https://www.reddit.com/r/deeplearning/comments/zk5esp/chatgpt_context_length/,10,1670866179.0,"How come ChatGPT can follow entire discussions whereas nowaday's LLM are limited (to the best of my knowledge) 4096 tokens ?

I asked it and it is not able to answer neither, and I found nothing on Google because no paper is published. I was also curious to understand how come Beamsearch was so fast with ChatGPT.

https://preview.redd.it/o6djsmpb5i5a1.png?width=1126&format=png&auto=webp&s=98baae5bf6fa294db408f0530214f8afa8a32a0b"
131,deeplearning,gpt,comments,2020-05-20 14:40:55,Gpt-2 Generated South Park chats between Characters,askbrodown,False,0.97,30,gncjho,https://www.reddit.com/r/deeplearning/comments/gncjho/gpt2_generated_south_park_chats_between_characters/,10,1589985655.0,"[https://www.soulreplica.com/brodown](https://www.soulreplica.com/brodown)

discussion on HN: [https://news.ycombinator.com/item?id=23246418](https://news.ycombinator.com/item?id=23246418)

The chats between the characters are generated by Gpt-2 774M model trained over all season's southpark episodes. The characters sounds exactly like who they are in the show.  The conversations either starts from a random topic, or they can respond  to a random trending tweets (e.g. trump, elon musk, etc.) the results  could be very on point, and sometimes hilarious:

https://preview.redd.it/56zto0ivlxz41.png?width=2222&format=png&auto=webp&s=fbf7882ed5750171d76ba8ef4b59886bec9135d2

https://preview.redd.it/e583h0yulxz41.png?width=2548&format=png&auto=webp&s=86814f5de77377530a48e97ab8f2ddaddaf9fb5d"
132,deeplearning,gpt,comments,2022-11-03 23:55:15,BlogNLP: AI Writing Tool,britdev,False,0.98,37,ylj1ux,https://www.reddit.com/r/deeplearning/comments/ylj1ux/blognlp_ai_writing_tool/,9,1667519715.0,"Hey everyone,

I created this web app using Open AI's GPT-3 (Davinci model). The purpose here is to provide a free tool to allow people to generate blog content/outlines/headlines and help with writer's block. Will continue to improve it over time, but just a side project I figured would provide some value to you all. Hope you all enjoy and please share ‚ù§Ô∏è

[https://www.blognlp.com/](https://www.blognlp.com/)"
133,deeplearning,gpt,comments,2021-06-14 06:34:33,"This Chinese Super Scale Intelligence Model, ‚ÄòWu Dao 2.0‚Äô, Claims To Be Trained Using 1.75 Trillion Parameters, Surpassing All Prior Models to Achieve a New Breakthrough in Deep Learning",ai-lover,False,0.91,38,nzgkj3,https://www.reddit.com/r/deeplearning/comments/nzgkj3/this_chinese_super_scale_intelligence_model_wu/,9,1623652473.0,"Deep learning is one area of technology where ambitiousness has no barriers. According to a recent announcement by¬†[The Beijing Academy of Artificial Intelligence (BAAI)](https://www.baai.ac.cn/), in China, yet another milestone has been achieved in the field with its ‚ÄúWu Dao‚Äù AI system. The¬†[GPT 3](https://www.marktechpost.com/2020/08/02/gpt-3-a-new-breakthrough-in-language-generator/)¬†brought in new interest for all the AI researchers, the super scale pre training models. By this approach and making use of 175 billion parameters, it managed to achieve exceptional performance results across the natural language processing tasks (NLP). However, the lacking component is its inability to have any form of cognitive abilities or common sense. Therefore, despite the size, even these models cannot indulge in tasks such as open dialogues, visual reasoning, and so on. With Wu Dao, the researchers plan to address this issue. This is China‚Äôs first attempt at a home-grown super-scale intelligent model system.¬†

Article: [https://www.marktechpost.com/2021/06/13/this-chinese-super-scale-intelligence-model-wu-dao-2-0-claims-to-be-trained-using-1-75-trillion-parameters-surpassing-all-prior-models-to-achieve-a-new-breakthrough-in-deep-learning/](https://www.marktechpost.com/2021/06/13/this-chinese-super-scale-intelligence-model-wu-dao-2-0-claims-to-be-trained-using-1-75-trillion-parameters-surpassing-all-prior-models-to-achieve-a-new-breakthrough-in-deep-learning/?_ga=2.13897584.636390090.1623335762-488125022.1618729090)

Reference: [https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/](https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/)"
134,deeplearning,gpt,comments,2023-01-29 22:39:07,[P] We built a browser extension that unlocks browser mode capabilities using ChatGPT: MULTI¬∑ON: AI Web Co-Pilot powered by ChatGPT,DragonLord9,False,0.95,65,10okyg3,https://v.redd.it/2hw47h0b82fa1,8,1675031947.0,
135,deeplearning,gpt,comments,2024-01-10 06:21:19,Are NLP based AI skills useless?,SnooBeans7516,False,0.69,11,1931m9b,https://www.reddit.com/r/deeplearning/comments/1931m9b/are_nlp_based_ai_skills_useless/,9,1704867679.0,"So something's been on my mind recently and I wanted to get Reddit's thoughts.

The thing that is troubling me as I get deeper, is it seems that for a lot of language based NLP tasks, ChatGPT or these other foundation models seem SOTA for 99% of tasks. In terms of developing valuable skills and products, it feels like the two options are: 

1. Research and training foundation models at large research labs 

2. Using GPT APIs in some sort of system (RAG or just raw prompts)

I feel like this issue is rooted in language itself being a somewhat static medium that can be mostly mastered by a single model. As opposed to images where we see a lot of cool projects from people that have to employ many different techniques to complete a specific task.

\## Example

Unlike images, the use cases seem much more straightforward, and don't require any special work other than throwing GPT at it. 

For example, I wanted to create a BERT like model to replace ingredients in a recipe, conditioned on a user's requirements or desires. The problem is, the more I worked on this and trained some models, the more it seemed that ChatGPT could just do it better with some prompt engineering...

==================

All this to say, I was previously motivated to be a part-time NLP researcher, implementing research into products in cool ways. Now it kind of feels like the only actually valuable investment is into ""AI engineering"", building systems using a GPT API:/"
136,deeplearning,gpt,comments,2023-10-31 04:46:09,How to ask questions about multiple PDFs,sam_from_NZ,False,0.33,0,17kd753,https://www.reddit.com/r/deeplearning/comments/17kd753/how_to_ask_questions_about_multiple_pdfs/,9,1698727569.0,"Hi guys and girls, what is a good way to be able to find answers to my own custom questions about a directory of PDFs. I‚Äôm a climate researcher and want to speed up my literature review process.  I‚Äôm good at using python and I pay for a chat GPT subscription but I don‚Äôt understand about paying for tokens, is there a completely separate thing? Anyways, is there an easy way to code this up and python? Surely? Or do I need to be looking at software that has already been developed? Cheers!"
137,deeplearning,gpt,comments,2023-02-21 11:06:33,I created a Search Engine For Books using GPT-3 üîéüìò. Here's how you can create it too:,Pritish-Mishra,False,0.67,5,1180x0e,https://youtu.be/SXFP4nHAWN8,8,1676977593.0,
138,deeplearning,gpt,comments,2023-04-02 18:10:37,Should we draw inspiration from Deep learning/Computer vision world for fine-tuning LLMs?,Vegetable-Skill-9700,False,0.77,11,129t3tl,https://www.reddit.com/r/deeplearning/comments/129t3tl/should_we_draw_inspiration_from_deep/,8,1680459037.0,"With HuggingGPT, BloombergGPT, and OpenAI's chatGPT store, it looks like the world is moving towards specialized GPTs for specialized tasks. What do you think are the best tips & tricks when it comes to fine-tuning and refining these task-specific GPTs?

Over the last decade, I have built many computer vision models (for human pose estimation, action classification, etc.), and our general approach was always based on Transfer learning. Take a state-of-the-art public model and fine-tune it by collecting data for the given use case.

Do you think that paradigm still holds true for LLMs?

Based on my experience, I believe observing the model's performance as it interacts with real-world data, identifying failure cases (where the model's outputs are wrong), and using them to create a high-quality retraining dataset will be the key.

&#x200B;

P.S. I am building an open-source project UpTrain ([https://github.com/uptrain-ai/uptrain](https://github.com/uptrain-ai/uptrain)), which helps data scientists to do so. We just wrote a blog on how this principle can be applied to fine-tune an LLM for a conversation summarization task. Check it out here: [https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation\_summarization](https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation_summarization)"
139,deeplearning,gpt,comments,2023-05-13 22:42:26,Domain specific chatbot. Semantic search isn't enough.,mldlbr,False,0.9,8,13gv1zj,https://www.reddit.com/r/deeplearning/comments/13gv1zj/domain_specific_chatbot_semantic_search_isnt/,8,1684017746.0,"Hi guys, I'm struggling to find a reliable solution to this specific problem.

I  have a huge dataset with chat conversations, about several topics. I  want to ask questions and retrieve information about these conversations in a chatbot way.

I have tried  semantic search with chatGPT to answer questions about these  conversations. The problem is that semantic search only returns top  similar sentences, and doesn't ‚Äòread‚Äô all conversations, that‚Äôs not  enough to answer generic questions, just very specific ones. For  example, if I ask ‚ÄúWhat are these people talking about person X?‚Äù it  will return only the top sentences (through semantic similarity) and  that will not tell the whole story. The LLM‚Äôs models have a limit of  tokens, so I can‚Äôt send the whole dataset as context.

Is there any approach to giving a reliable answer based on reading all the messages?

Any ideas on how to approach this problem?"
140,deeplearning,gpt,comments,2023-04-08 07:55:07,need help. GPT-3.5 can't solve it.,ryanultralifeio,False,0.25,0,12ff87f,https://www.reddit.com/r/deeplearning/comments/12ff87f/need_help_gpt35_cant_solve_it/,8,1680940507.0,"Trying to make a schedule for the league, here are the constraints.  I think it should be tailormade for AI.


Schedule May 2023 Games.

ÔøºÔøº

I need you to schedule games between 7 teams, on 4 fields, beginning on Monday May 1st for the whole month of May 2023. 

The 4 fields are; Quincy, Portola, Chester and Loyalton. Fields in Quincy, Portola and Loyalton are available beginning May 1st. The field in Chester is available beginning May 8th.

 Saturdays can have 3 games per day at either 10am, 1pm, or 4 pm. 

No games on Sunday. 

Monday, Tuesday, Wednesday, Thursday, and Friday games are at 5:00. 

Mondays, Tuesdays, Wednesdays, Thursdays, and Fridays can have games played on 3 different fields at the same time. 

There are 7 teams. Quincy Red, Quincy Blue, Quincy Grey, Portola Padres, Portola Dodgers, Chester Giants and Loyalton. 

All teams can only play each other 2 times in May with the exceptions of Quincy Grey and Quincy Red, Quincy Grey and Quincy Blue, and Quincy Grey and Chester Giants, who can only play each other 1 time in May. 

Only Loyalton cannot play on May 3,4, or 5 for Sierra Nevada Journeys. 

All teams are unavailable to play May 26,27,29 for Memorial Day Weekend. 

All teams are unavailable to play May 17,18,19 for 6th grade field trip. 

Each team will play one home game against each other, except for the teams only playing one game. 

Quincy Blue only plays home games on Quincy field on Mondays, and Thursdays. 

Quincy Grey only plays home games on Quincy field on Wednesdays, and Fridays. 

Quincy Red only plays home games on Quincy Field on Tuesdays, Thursdays, and Fridays. 

Loyalton only plays home games on Loyalton field. 

Chester Giants only play Home Games on Chester field. 

Portola Padres only play home games on Portola field. 

Portola Dodgers only play home games on Portola field. 

Each team can play a maximum of two games per week. 

A team cannot play without two calenders days between games. 

A team cannot play two games on consecutive days.

A team cannot play two games on the same day. 

Teams must have at least 9 games.

Put the total number of games played per team at the bottom of the whole months schedule.

2+ hours a no good results........."
141,deeplearning,gpt,comments,2023-10-26 17:59:49,Long text summarization tool how-to (700+ pages),Old_Swan8945,False,0.84,8,17h2fbk,https://www.reddit.com/r/deeplearning/comments/17h2fbk/long_text_summarization_tool_howto_700_pages/,8,1698343189.0,"Hey all I've seen a bunch of posts about summarization of long texts and seems like there's been a lot of challenges, so wanted to spread some knowledge out there about some things I've discovered as I launched my tool here ([summarize-article.co](https://summarize-article.co)) (longest text was a psych book from one of my users at 700+ pages).

The most basic problem in the summarization process is the GPT context window length, so the basic strategy I follow is the following:

1. Chunk the text into chunks that fit inside the context window
2. Recursively summarize the summaries until it becomes manageable
3. Use a long context-window model to generate the final summary using a prompt that takes the recursively-generated summaries and re-restructures the output
4. Additional prompt magic to optimize the outputs (DM me for more details :D)

Anyway, would appreciate any feedback on the results or anything you think could be improved, otherwise feel free to check it out or msg me if you want to learn more about how it works!"
142,deeplearning,gpt,comments,2022-12-07 00:33:41,Are currently state of art model for logical/common-sense reasoning all based on NLP(LLM)?,Accomplished-Bill-45,False,1.0,25,zen8l4,https://www.reddit.com/r/deeplearning/comments/zen8l4/are_currently_state_of_art_model_for/,6,1670373221.0,"Not very familiar with NLP, but I'm playing around with OpenAI's ChatGPT; particularly impressed by its reasoning, and its thought-process.

Are all good reasoning models derived from NLP (LLM) models with RL training method at the moment?

What are some papers/research team to read/follow to understand this area better and stay on updated?

&#x200B;

&#x200B;

for ChatGPT. I've tested it with following cases

Social reasoning ( which does a good job; such as: if I'm going to attend meeting tonight. I have a suit, but its dirty and size doesn't fit. another option is just wear underwear, the underwear is clean and fit in size. Which one should I wear to attend the meeting. )

Psychological reasoning ( it did a bad job.I asked it to infer someone's intention given his behaviours, expression, talks etc.)

Solving math question ( it‚Äôs ok, better then Minerva)

Asking LSAT logic game questions ( it gives its thought process, but failed to give correct answers)

I also wrote up a short mystery novel, ( like 200 words, with context) ask if it can tell is the victim is murdered or committed suicide; if its murdered, does victim knows the killer etc. It actually did ok job on this one if the context is clearly given that everyone can deduce some conclusion using common sense."
143,deeplearning,gpt,comments,2023-02-05 16:44:56,Beat GPT-3 which has unlimited money using Open Source community,koyo4ever,False,0.78,21,10ugxmc,https://www.reddit.com/r/deeplearning/comments/10ugxmc/beat_gpt3_which_has_unlimited_money_using_open/,8,1675615496.0,"Is it technically possible to train some model using a lot of personal computers like a cluster.

Eg: an Algorithm to train tiny parts of some model using personal computer of volunteers. Like a community that makes your gpu capacity available, even if it's little.

The idea is train tiny parts of a model, with a lot of volunteers, then bring it together to make some powerful deepmind.

Can this model beat a lot of money spent in models like GPT-3?"
144,deeplearning,gpt,comments,2024-01-05 08:27:31,6 ways AI can make your life easier in 2024,PoetryOne4804,False,0.33,0,18z212l,https://www.reddit.com/r/deeplearning/comments/18z212l/6_ways_ai_can_make_your_life_easier_in_2024/,8,1704443251.0,"Artificial intelligence is developing every day. ChatGPT was a game changer for millions of people, but it is not the only one. Advances in AI are coming, and they're coming FAST. Very fast. There‚Äôre so many tasks AI can help with and make this year less stressful. Let me show you these ways:

**1) Chatbots for answering questions and brainstorming**

Except ChatGPT, you can use Google Bard, SpinBot, and YouChat.

**2) AI Essay writers**

Many people use [essay writing services](https://www.reddit.com/r/deeplearning/comments/16gnuwy/best_essay_writing_services_top_5/) but not all think that AI can also help in academic writing. AI essay writers like [Textero.ai](https://Textero.ai) can be faster and generate ideas or find sources for your topic.

**3) Daily life tools**

There‚Äôre AI planners to schedule meetings and integrate with your calendars. You can also keep track of finances using PocketGuard, Wally, or Cleo.

**4) Tools for social networks**

There‚Äôre various AI tools tailored for social networks, such as Postwise for Twitter posts and Steve.ai for YouTube.

**5) Tools to improve health and fitness goals**

AI tools like Apple Watches and Fitbits can monitor your fitness and health. They can even track your sleep and offer suggestions to improve sleep quality.

**6) Tools for academic needs**

Even though some professors are against using AI while studying, students look for ways to make academic life easier. Useful tools for school life you can find here:  [ai tools for students](https://www.reddit.com/r/artificial/comments/1716t0y/ai_tools_for_students_from_ai_essay_generators_to/)

Any other tools to share? Feel free to write about them, I‚Äôm ready to try more new services."
145,deeplearning,gpt,comments,2023-03-05 11:10:56,LLaMA model parallelization and server configuration,ChristmasInOct,False,1.0,25,11ium8l,https://www.reddit.com/r/deeplearning/comments/11ium8l/llama_model_parallelization_and_server/,8,1678014656.0,"Hey everyone,

First of all, tldr at bottom, typed more than expected here.  

Please excuse the rather naive perspective I have here.  I've followed along with great interest, but this is not my industry.

Regardless, I have spent the past 3-4 days falling down a brutally obsessive rabbit hole, and I cannot seem to find this information.  I'm assuming it's just that I am missing context of course, and regardless of whether there is a clear answer, I'm trying to get a better understanding of this topic so that I could better appraise the situation myself.

Really I suppose I have two questions.  **The first** is regarding model parallelization.

I'm assuming this is not generic whatsoever.  What is the typical process engineers go about for designing such a pipeline?  Specifically in regards to these new LLaMA models, is something like ALPA relevant?  Deepspeed?

More importantly, what information should I be seeking to determine this myself?

This roughly segues to my **second inquiry**.

The reason I'm curious about splitting the model pipeline etc., is that I am potentially in interested in standing a server up for this.  Although I don't have much of a budget for this build (\~$30-40K is the rough top-end, but I'd be a lot happier around $20-25K), the money is there if I can genuinely satisfy my use-case.

I work at a small, but borderline manic startup working on enterprise software; 90% of the work we're doing based in the react/node ecosystem, some low-level work for backend services, and some very interesting database work that I have very little to do with.  I am a fullstack engineer that grew up playing with C++ => C#, and somehow ended up spending all of my time r/w'ing javascript.  Lol.  Anyways.

Part of our roadmap since GPT-3 and the playground were made publicly accessible, involves usage of these transformer models, and their ability to interpret natural language inputs, whether from user inputs, or scraped input values generated somewhere in a chain of requests / operations.

Seeing GPT-3 in action made me specifically realize that my estimations on this technology had been wildly off.  Seeing ChatGPT in action and uptick, the API's becoming available, has me further panicked.

Running our inference through their API has never really been an option for us.  I haven't even really looked that far into it, but bottom line the data running through our platform is all back-office, highly sensitive business information, and many have agreements explicitly restricting the movement of data to or from any cloud services, with Microsoft, Amazon, and Google all specifically mentioned.

Regardless of the reasoning for these contracts, the LLaMA release has had me obsessed over this topic in more detail than before, and whether or not I would be able to get this setup privately, for our use-case.

**To get to the actual second inquiry**:

Say I want to throw a budget rig together for this in a server cabinet.  Am I able to effectively parallelize the LLaMA model, well enough to justify going with 24GB VRAM 4090's in the rig?  Say I do so with DeepSpeed, or some of the standard model parallelization libraries.

Is the performance cost low enough to justify taking the extra compute here over 1/3 - 1/2 as many RTX6000 ADA's?

Or should I be grabbing the 48GB ADA's?

Like I said, I apologize for the naivety, I'm really looking for more information so that I can start to put this picture together better on my own.  It really isn't the easiest topic to research with how quickly things seem to move, and the giant gap between conversation depths (gamer || phd in a lot of the most interesting or niche discussions, little between).

Thank you very much for your time.

TL;DR - Any information on LLaMA model parallelization at the moment?  Will it be compatible with things like zero or alpa?  How about for throwing a rig together right now for fine-tuning and then running inference on the LLaMA models?  48GB 6000 ADA's, or 24GB 4090's?

Planning on putting it in a mostly empty 42U cabinet that also houses our primary web server and networking hardware, so if there is a sales pitch for 4090's across multiple nodes here, I do have a massive bias as the kind of nerd that finds that kind of hardware borderline erotic.

Hydro and cooling are not an issue, just usage of the budget and understanding the requirements / approach given memory limitations, and how to avoid communication bottlenecks or even balance them against raw compute.

Thanks again everyone!"
146,deeplearning,gpt,comments,2023-11-16 08:28:46,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,0.84,71,17whz6e,https://v.redd.it/qx68wbuf7o0c1,7,1700123326.0,
147,deeplearning,gpt,comments,2022-12-03 19:29:01,BlogNLP: AI Blog Writing Tool,britdev,False,0.88,13,zboc8w,https://www.reddit.com/r/deeplearning/comments/zboc8w/blognlp_ai_blog_writing_tool/,7,1670095741.0,"Hey everyone,

I developed this web app with Open AI's GPT-3 to provide a free, helpful resource for generating blog content, outlines, and more - so you can beat writer's block! I'm sure you'll find it useful and I'd really appreciate it if you shared it with others ‚ù§Ô∏è.

[https://www.blognlp.com/](https://www.blognlp.com/)"
148,deeplearning,gpt,comments,2023-02-15 09:25:30,[P] From ‚Äúiron manual‚Äù to ‚ÄúIron Man‚Äù ‚Äî Augmenting GPT for fast editable memory to enable context aware question & answering,skeltzyboiii,False,1.0,43,112u10p,https://i.redd.it/yujf2enambia1.gif,7,1676453130.0,
149,deeplearning,gpt,comments,2021-12-08 08:20:19,"Can a seq2seq problem be posed as a generative modelling problem? To clarify, can i pose machine translation as a generative problem, so i can use a Dall-e or CLIP, or VQGAN-style architecture, thereby using only a decoder from gpt-2?",Comfortable-You1776,False,1.0,1,rbmin2,https://www.reddit.com/r/deeplearning/comments/rbmin2/can_a_seq2seq_problem_be_posed_as_a_generative/,7,1638951619.0,"This way, I can reuse many pretrained transformers from huggingface.  Please comment."
150,deeplearning,gpt,comments,2021-02-10 09:56:17,Secondary GPU for Display out?,jnfinity,False,0.8,3,lgqh7u,https://www.reddit.com/r/deeplearning/comments/lgqh7u/secondary_gpu_for_display_out/,7,1612950977.0,"I am getting my RTX 3090 today (finally) and I am wondering if it would make sense to get a cheap GPU for display out on a system that will mostly run headless? Is there much of a benefit or should I just use it without a second GPU and expect not much of a penalty? 

I am mostly working on NLP and ASR tasks right now (both using transformer NNs (think Wav2Vec 2.0 and minGPT)"
151,deeplearning,gpt,comments,2023-12-06 21:16:44,Platform with algorithm that creates posts,gate-app,False,0.83,4,18cehg4,https://www.reddit.com/r/deeplearning/comments/18cehg4/platform_with_algorithm_that_creates_posts/,7,1701897404.0,"So i made this thing it'll keep growing and growing.

i published my [notes](https://ablaze-mine-be9.notion.site/Algorithm-566bcebb669f49c2aedb63ffd04df3bc?pvs=4) if someones interested im looking for more serious people who believe in this, also for opinions of credible people.

&#x200B;

&#x200B;

one if the ideas:

Tiktok has a huge algorithm but the only thing it does is recommends user created content to people.  what it has is millions of users metrics and how they interact with the content which is what makes its algorithms good.  there can be a platform that collects all that useful metrics too, but uses them not only for recommender model, but also for post creation.  you can take a llm (gpt) today and make it generate posts, then collect millions of peoples interactions and how they respond to them, all the metrics and train the post creator model with it. you can easily make an actual quality content creation bot thats better than any copywriter and understands the relevant details better than anyone.  the reason the other platforms do so well is because of the insane amounts of data they monitor.  the post creation is 2 parts:  one that finds relevant stuff on the internet, tracks events, and just figures out best content to post about.  the other one is llm model that takes any piece of information and converts it into a post with title and all the other fields  both can be trained with data from users.  i am working on this idea further theres a demo with a feed of posts and a chatbot [https://gate-app.com/](https://gate-app.com/) [https://gate-app.com/posts/170145283354301509](https://gate-app.com/posts/170145283354301509) "
152,deeplearning,gpt,comments,2023-04-28 18:10:08,The Little Book of Deep Learning is a 140 page (phone-formatted!) technical introduction of the necessary background for denoising diffusion and GPT models. BY-NC-SA.,FrancoisFleuret,False,0.98,82,1325a0j,https://fleuret.org/public/lbdl.pdf,6,1682705408.0,
153,deeplearning,gpt,comments,2023-05-18 08:41:54,Tutorial to improve GPT throughput 16 times with dynamic batching,Greedy-Cupcake-3694,False,0.92,27,13ksxc8,https://www.reddit.com/r/deeplearning/comments/13ksxc8/tutorial_to_improve_gpt_throughput_16_times_with/,6,1684399314.0,"I wrote a tutorial to improve GPT completion throughput with dynamic batching [https://microsoft.github.io/batch-inference/examples/gpt\_completion.html](https://microsoft.github.io/batch-inference/examples/gpt_completion.html). And I can achieve 16 times throughput on V100 comparing to baseline. We built a python dynamic batching library so you can apply it on your own models easily [https://github.com/microsoft/batch-inference](https://github.com/microsoft/batch-inference).  


Although the tutorial we built for GPT shows promising result on throughput, it doesn't use complex decoding algorithms like top-p or beam search, and we are aware of more advanced batching algorithms for GPT completion. So we're considering building a GPT specific inference library for production use, hope we get enough resource to do it in future"
154,deeplearning,gpt,comments,2023-11-06 02:57:04,"If a conversation is not deleted, can ChatGPT-4 continuously learn and maintain the conversation state?",Turbulent_Dot_5216,False,0.56,1,17ot4d4,https://www.reddit.com/r/deeplearning/comments/17ot4d4/if_a_conversation_is_not_deleted_can_chatgpt4/,6,1699239424.0," As a beginner, I have a question for everyone: Does ChatGPT-4 forget the context if the conversation is closed or left idle for a long period, meaning it can't maintain the state of the conversation? I want ChatGPT-4 to learn legal knowledge, and in one conversation, provide it with a vast amount of legal material over a long period. Can ChatGPT-4 remember the previous legal material every time I open it, i.e., maintain the conversation state? If not, how can I make ChatGPT-4 remember previous conversations? 

 Additionally, if I do not delete a conversation and continuously feed ChatGPT-4 a large amount of legal information within that same conversation, can ChatGPT-4 achieve self-learning? That is, can it become increasingly proficient in legal matters, or regardless of how much information I provide, will ChatGPT-4 not improve? "
155,deeplearning,gpt,comments,2020-05-21 18:38:22,Understanding encoder and decoder structures within transformers,de1pher,False,0.83,10,go2ha2,https://www.reddit.com/r/deeplearning/comments/go2ha2/understanding_encoder_and_decoder_structures/,6,1590086302.0,"Hi all,

I'm learning about sequence-to-sequence transformers and I'm having a hard time understanding the encoder-decoder pattern.

As far as I understand it encoders are effectively designed to extract features that would enable a decoder to make sense of them. The most notable example would be an autoencoder which condenses the original input into a dense lower-dimensional space that can later be used for a variety of tasks. I believe that a decoder, on the other hand, is meant to translate an input signal (raw or encoded data) and generate some useful predictions that humans should be able to make sense of.

If my understanding is correct, then there are two problems that bother me:

1. Aren't all neural networks with at least 1 hidden layer transformers then? We can think of the hidden layer as the encoder and the output as the decoder. If we have more than 1 hidden layer, then it might become difficult to work out where the encoder ends and the decoder begins. The ""encoder-decoder"" characterisation initially led me to think that it's some kind of a multi-agent setup akin to GANs
2. BERT is considered to be an encoder-only transformer and GPT is a decoder-only transformer -- why is that? First, if neither of them contains both an encoder and a decoder, then why are they even considered to be transformers? And what is it that makes BERT and encoder model and GPT a decoder model when both ultimately output token probabilities for a given input string?

I feel like I'm definitely missing something here and I would appreciate if you guys could help out :)

Many thanks!

&#x200B;

EDIT:

Hey guys, I'd like to thank those who attempted to help me :)

I **think** I'm beginning to develop a better understanding of the transformer model, but if you think that I'm still missing something then please correct me. First, I'd like to point out that there is *the* Transformer model proposed by the [""Attention Is All You Need""](https://arxiv.org/abs/1706.03762) paper, whereas I was originally referring to transformers as a general class of models consisting of arbitrary encoders and decoders (which do not have to use attention or positional encoding or anything like that by definition).

The encoder-only (e.g. BERT) and decoder-only (e.g. GPT) transformers effectively borrow the encoders and decoders from *the* transformer model and modify them which explains their names ""encoder-only transformer"" and ""decoder-only transformer"".

The general idea of encoders is to contextualize the **input** sequence A into a rich representation of itself. The general idea of a decoder is to parse the **output** sequence B together with the contextual information from the encoder and effectively find the relationship between the enoded input and the required output. This explains the difference between an arbitrary NN with a hidden layer and a true encoder-decoder architecture.

I'd also like to recommend [this video](https://www.youtube.com/watch?v=TQQlZhbC5ps) from CodeEmporium which provides an excellent explanation of the transformer model (thanks to u/adikhad for suggesting it)."
156,deeplearning,gpt,comments,2023-10-11 16:40:27,Is it possible to train a GPT-2 model on free google colab?,JastorJ,False,1.0,13,175ikm3,https://www.reddit.com/r/deeplearning/comments/175ikm3/is_it_possible_to_train_a_gpt2_model_on_free/,6,1697042427.0, My course has an assignment where we have to implement a research paper  and I was thinking about implementing GPT2 model but I am worried that  it could take enormous resources to train it properly. Is it possible to  train it on google colab using a small amount of text data to get good  results from it.  I don't have access to GPU so I have to use colab.
157,deeplearning,gpt,comments,2023-07-29 20:02:45,"Promptify 2.0: More Structured, More Powerful LLMs with Prompt-Optimization, Prompt-Engineering, and Structured Json Parsing with GPT-n Models! üöÄ",StoicBatman,False,0.71,3,15d1fs8,https://www.reddit.com/r/deeplearning/comments/15d1fs8/promptify_20_more_structured_more_powerful_llms/,6,1690660965.0,"Hello fellow coders and AI enthusiasts!

First up, a huge Thank You for making Promptify a hit with **over** [**2.3k+ stars on Github**](https://github.com/promptslab/Promptify) ! üåü

Back in 2022, we were the first one to tackle the common challenge of uncontrolled, unstructured outputs from large language models like GPT-3. , and your support has pushed us to keep improving.Today, we're thrilled to share some major updates that make Promptify even more powerful

&#x200B;

https://preview.redd.it/29ajik9xmyeb1.png?width=1510&format=png&auto=webp&s=3c3bfeebd6ba5e878885b079510a8972cc72c3b8

&#x200B;

* **Unified Architecture üß≠**: Introducing Prompter, Model & Pipeline Solution
* **Detailed Output Logs üìî**: Comprehensive structured JSON format output within the log folder.
* **Wider Model Support ü§ù**: Supporting models from OpenAI, Azure, Cohere, Anthropic, Huggingface and more - think of it as your universal language model adapter.
* **Robust Parser ü¶∏‚Äç‚ôÇÔ∏è**: Parser to handle incomplete or unstructured JSON outputs from any LLMs.
* **Ready-Made Jinja Templates üìù**: Jinja prompt templates for NER, Text Classification, QA, Relation-Extraction, Tabular data, etc.
* **Database Integration üîó**: Soon, Promptify directly to Mongodb integration. Stay tuned!
* **Effortless Embedding Generation üß¨**: Generate embeddings from various LLMs effortlessly with the new update.

&#x200B;

https://preview.redd.it/k50gmbxymyeb1.png?width=2160&format=png&auto=webp&s=ef063a7a0594eccac5674bd60d7adce193eecc3f

Check out the examples and take Promptify for a spin on GitHub. If you like what you see, we'd be honored if you gave us a star!

* **Github**: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* **Colab:** [Try Now on Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)
* **Explore Other Cool Open Source LLM Tools:** [https://github.com/promptslab](https://github.com/promptslab)

Join **1.6k+ Promptify users on Discord** to dive deep into prompt engineering, discuss the latest with LLMs, and advance NLP research together: [https://discord.com/invite/m88xfYMbK6](https://discord.com/invite/m88xfYMbK6)Thank you again for your support - here's to more structured AI!

&#x200B;"
158,deeplearning,gpt,comments,2023-02-01 15:20:25,Launching my first-ever open-source project and it might make your ChatGPT answers better,Vegetable-Skill-9700,False,0.75,2,10qx9po,https://www.reddit.com/r/deeplearning/comments/10qx9po/launching_my_firstever_opensource_project_and_it/,6,1675264825.0,"I am building UpTrain - an open-source ML diagnostic toolkit that recently got investment from YCombinator.

As you know no ML model is 100% accurate, and, further, their accuracy deteriorates over time üò£. Additionally, due to the black boxiness ‚¨õ nature of Large Language models, it's challenging to identify and fix their problems.

The tool helps ML practitioners to:
1. Understand how their models are performing in production
2. Catch edge cases and outliers to help them refine their models
3. Allow them to define custom monitors to catch under-performing data-points
4. Retrain the model on them to improve its accuracy

You can check out the project here: https://github.com/uptrain-ai/uptrain. Would love to hear feedback from the community!"
159,deeplearning,gpt,comments,2020-08-05 10:58:06,image-GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model,OnlyProggingForFun,False,0.94,96,i437pt,https://www.youtube.com/watch?v=FwXQ568_io0,6,1596625086.0,
160,deeplearning,gpt,comments,2023-02-20 21:27:58,Fine tuning a GPT for text generation,nashcaps2724,False,0.91,9,117l2vf,https://www.reddit.com/r/deeplearning/comments/117l2vf/fine_tuning_a_gpt_for_text_generation/,6,1676928478.0,"Hi all, let me lay out my problem‚Ä¶

Imagine there are two corpora, Corpus A (100,000~) and Corpus B (20,000,000~). 

Individuals create reports for corpus A based on the information in corpus B. 

My idea was to pretrain a GPT on corpus A, and fine tune it to take documents from corpus B as an input, and output text in the style of corpus A (essentially a mix of text generation and summarization). 

Is this something folks think is even feasible? Should I be pretaining the GPT on both corpora or just corpus A? I thought of both fine tuning an OpenAI GPT and training from scratch. 

Any advice would be welcome!"
161,deeplearning,gpt,comments,2023-04-25 23:50:32,Research on the political biases of ChatGPT,Mysterious_Potato132,False,0.75,4,12z08ni,/r/ChatGPT/comments/12yz49i/research_on_the_political_biases_of_chatgpt/,6,1682466632.0,
162,deeplearning,gpt,comments,2023-01-20 08:53:58,Gotcha,actual_rocketman,False,0.94,64,10gs1ik,https://i.redd.it/yyh41pnje7da1.jpg,5,1674204838.0,
163,deeplearning,gpt,comments,2023-03-01 05:52:05,Career pivot for a SWE considering chatGPT disruption?,Which-Distance1384,False,0.6,1,11evrik,https://www.reddit.com/r/deeplearning/comments/11evrik/career_pivot_for_a_swe_considering_chatgpt/,5,1677649925.0,"tl;dr : what to do to board GPT ship?

&#x200B;

I really dont think ChatGPT will replace jobs very soon. Not until many software and apps are developed to replace workers, e.g., something that can do HR job, read internal docs and summarize, code from a design, etc etc. And all that needs experts. Given the field is pretty new and there are not many experts in this field, at least for the scale needed, I think the process of job loss takes some time. For near future probably we all SWEs can enjoy the more efficient coding, documentation readintg, etc. But for sure in the future there will be changes.

&#x200B;

I have been bored with my job as a SWE for a while. I have worked mostly in Cloud all my career (AWS and GCP) and always wanted to try something new.

&#x200B;

I find ChatGPT disruptive and interesting and want to be a part of it. I wonder what is the best way for me to join Large Language Model efforts? Which ways have best ROI?

\- Going to a PhD program and becoming a researcher?

\- Getting an MSC and become some sort of NLP AI engineer?

\- Finding internal teams that are willing to give me a try on their research/product?

&#x200B;

&#x200B;"
164,deeplearning,gpt,comments,2023-03-21 02:06:28,CoDev- A GPT 4.0 Virtual Developer To Generate Apps,aisaint,False,0.75,8,11x3p2u,https://www.reddit.com/r/deeplearning/comments/11x3p2u/codev_a_gpt_40_virtual_developer_to_generate_apps/,5,1679364388.0,"&#x200B;

&#x200B;

CoDev is a GPT 4.0 virtual developer prompt to help you create and refine boilerplates/apps. You can get the prompt from my GitHub link below, paste it in a new Chat session, and issue the commands (see How To Use CoDev). In this article, we will use CoDev to create a React/Typescript/MUI dashboard boiler plate

[https://medium.com/@etherlegend/codev-a-gpt-4-0-virtual-developer-to-build-app-boilerplates-34a431e779c7](https://medium.com/@etherlegend/codev-a-gpt-4-0-virtual-developer-to-build-app-boilerplates-34a431e779c7)"
165,deeplearning,gpt,comments,2022-05-06 01:54:02,Meta's open-source new model OPT is GPT-3's closest competitor!,OnlyProggingForFun,False,0.95,19,ujcra5,https://youtu.be/Ejg0OunCi9U,5,1651802042.0,
166,deeplearning,gpt,comments,2023-04-24 13:14:39,Applications of GPT,AcornWizard,False,0.3,0,12xfegq,https://www.reddit.com/r/deeplearning/comments/12xfegq/applications_of_gpt/,5,1682342079.0,"Hello. Is ChatGPT currently the only implemented application that uses GPT? Looking on the internet I see a lot of flashy but often vague talk about potential applications, yet I have not found more implemented uses."
167,deeplearning,gpt,comments,2020-04-21 23:00:56,How does the talktotransformer website work so fast?,parrot15,False,0.97,34,g5prf5,https://www.reddit.com/r/deeplearning/comments/g5prf5/how_does_the_talktotransformer_website_work_so/,5,1587510056.0,"At [https://talktotransformer.com/](https://talktotransformer.com/), you can type a prompt and the transformer will autogenerate the text for you using OpenAI's GPT-2 1.5 billion parameter model.

I'm not asking how GPT-2 works, I'm asking something else. When I ran the GPT-2 1.5B model on the free TPU in Google Colab, it took around 20 to 40ish seconds to generate around the same about of text as the website generates per prompt.

And yet, the website is somehow generating text from the prompt almost instantaneously using the 1.5B model. This is on top of all the X number of people who must be using the website at the same time I am, so it is doing text generation concurrently and near-instantaneously using a gigantic model.

I am very confused. Did the creator of the website just use a lot more TPUs/GPUs behind the scenes and is letting them run 24/7 (I don't think so because that would cost a shit ton of money), or am I missing something fundamental here?

This same question can be applied to this website too: [https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is](https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is)

Please keep in mind that I'm relatively new to all of this. Thanks in advance!"
168,deeplearning,gpt,comments,2023-11-16 05:05:18,Is there any way to pipe the results from GPT or any LLM to some generative AI like Dall e or Stable Diffusion ?,Sanjuej,False,0.67,2,17wep4d,https://www.reddit.com/r/deeplearning/comments/17wep4d/is_there_any_way_to_pipe_the_results_from_gpt_or/,5,1700111118.0,I'm trying to create a specific type of design using Generative AI. So I'm trying to curate the prompt and make it hyperdetailed and then take that prompt to generate the Image. Is there any way I can do this if yes could you share some resources I could see?
169,deeplearning,gpt,comments,2021-07-15 17:06:55,"EleutherAI Researchers Open-Source GPT-J, A Six-Billion Parameter Natural Language Processing (NLP) AI Model Based On GPT-3",techsucker,False,1.0,59,okx5hm,https://www.reddit.com/r/deeplearning/comments/okx5hm/eleutherai_researchers_opensource_gptj_a/,5,1626368815.0,"[GPT-J](https://www.eleuther.ai/), a six-billion-parameter natural language processing (NLP) AI model based on GPT-3, has been open-sourced by a team of EleutherAI researchers. The model was trained on an open-source text [dataset of 800GB](https://pile.eleuther.ai/) and was comparable with a GPT-3 model of similar size.

The model was trained using Google Cloud‚Äôs v3-256 TPUs using EleutherAI‚Äôs Pile dataset, which took about five weeks. GPT-J achieves accuracy similar to OpenAI‚Äôs reported findings for their 6.7B parameter version of GPT-3 on standard NLP benchmark workloads. The model code, pre-trained weight files, a Colab notebook, and a sample web page are included in EleutherAI‚Äôs release.

Story: [https://www.marktechpost.com/2021/07/15/eleutherai-researchers-open-source-gpt-j-a-six-billion-parameter-natural-language-processing-nlp-ai-model-based-on-gpt-3/](https://www.marktechpost.com/2021/07/15/eleutherai-researchers-open-source-gpt-j-a-six-billion-parameter-natural-language-processing-nlp-ai-model-based-on-gpt-3/) 

Github repository for GPT-J: https://github.com/kingoflolz/mesh-transformer-jax

Colab Notebook: https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab\_demo.ipynb

Web Demo: https://6b.eleuther.ai/"
170,deeplearning,gpt,comments,2020-07-28 12:31:26,GPT-3 writes my SQL queries for me,Independent-Square32,False,0.88,56,hzdthe,https://youtu.be/WlMHYEFt2uA,5,1595939486.0,
171,deeplearning,gpt,comments,2024-01-13 04:24:32,"Idea / Proposal for someone smarter than I am. Devs/Coders, please hear me out.",DriestBum,False,0.2,0,195ff66,https://www.reddit.com/r/deeplearning/comments/195ff66/idea_proposal_for_someone_smarter_than_i_am/,5,1705119872.0,"Disclaimer: you're probably more knowledgeable than me, have more experience, and are far better at coding than me. Keep that in mind.

Here's the TLDR: create a new program (perhaps using Transformers/Tiny LLM, perhaps not needing LLMs at all) that scans, details, and analyzes the users exact hardware setup, and ultimately determines the optimal LLM/quant/settings/config for that specific hardware configuration (and perhaps use case, like general chat/role play/instruct/coding...).

Why? 

Because of the multitude of posts we see of new people trying to get into open source LLMs, but having no understanding of where they should start, what is possible on their machine, and how to configure the model for their needs. 

I was one of these people, and only after long periods of reading guides/tutorials/wikis/model cards/etc was I finally able to get a working model on my machine with decent speed and quality. For a person who is tech minded, but not a coder, and not familiar with anything other than straight forward download .exe and go, I had to start at square 1 and figure out GitHub, Linux, and all the backend llamma.ccp and whatnot. It took forever, I'm just a regular tech consumer, I don't know how to build shit. Mind you, it was a valuable experience, but I guarentee many people would have given up without pressing on and figuring it all out.

If we want greater adoption in the open source space, an easier on boarding process would be a God send for people like me. I would have paid for it! Easily! I put out offers for people to create a docker container so I could just click ""run"". The offers to build it were way out of my budget, so I was forced to grind it out and stumble my way through the steep learning curve. 

I'm not a unique case. I'm someone who has used ChatGPT, played with Spaces on Hugging Face, and really wanted a local LLM to use with sensitive local data that I couldn't trust OpenAI or anyone with. I understood the value, but I didn't have the ability to spin up a model without massive amounts of homework. 

So anyway, that's the idea. A stand alone program, or utility, that analyzes a user's hardware capability, suggests appropriate specs/config for a model, and gives a bullet point list of what to do to get that specific model working in a numbered list of steps. 

I would have paid $100 for that shortcut. Easily. I almost paid 20x that for a container of an old model. Just to get it working. 

To reiterate, the problem is:

New people asking ""what do I need to run x?"" or ""can my x machine run this model?"" Or ""what's the beat coding model I can use with x machine"". 

Solution:

A utility that's only purpose is to examine the user's hardware and recommend optimal models, quants, and settings profile (for webui / lm studio...) 

Just an idea. 

Please DM if anyone is interested in a collaboration. I'm a corporate finance person, with a marketing degree/background, I'm not a developer. I can't build it, but I could hype it, and potentially sell it. I know the target market, and it's large and narrow enough to be worthwhile. 

Thanks for coming to my lunch and learn presentation. And I'm totally aware of how I pitched a close source business idea to an open source sub, I know, but the people who could/want to do this are here. I'm not going to lose sleep over offending a commie or two."
172,deeplearning,gpt,comments,2023-09-29 21:35:07,Question about training and fine tuning a GPT model,mojo_no_jojo,False,1.0,1,16vo3wy,https://www.reddit.com/r/deeplearning/comments/16vo3wy/question_about_training_and_fine_tuning_a_gpt/,5,1696023307.0,"Not sure if this is the right place to ask for help, and if it isn't, I apologise in advance. Please #nohate

I have created an LLM that performs miserably and terribly. I mean that'd be an understatement. I followed a incredibly poor YouTube video of a guy doing this, but he never really showed how to fine tune it so I can make it work better. My code and everything works, because I know I have a poorly functional model. FYI, I used the openwebtext corpus (about 45gb) for training and validation. 

So, here's my question: How may I fine tune it so I can make it work? Specifically, for my use case. 

My use case: I'd like to be able to give the model some text (preferably in paragraphs), and make it generate some questions about it.

I know there are libraries on HuggingFace that I can use but I'd very much like to be able to do it myself, if possible. 

Resources at my disposal: My own basic laptop, and Google Colab with T4 GPU and a TPU (among other things it offers). To train my (poorly functional) model I used a T4 GPU. 

Time: less than two weeks.

Premise: I'm doing my Masters and as part of my final sem project, I've decided to create my own model where you give it some text, like from a PDF document, and it asks you questions. I would like to include boolean type questions, like True or False types, and/or MCQ types. 

So please can you share how may I fine tune the model so it is able to do exactly that? I could share the Github project if you'd like, but I'm not sure if this the right place to ask this. I've not 100% grasped the Transformer model, but I'm trying. I'm pretty OK with deep learning though. 

If I sounded boastful or rude, I am sorry. I'm not trying to be it.

Edit 1: Changed the type of model I had created. It's an LLM."
173,deeplearning,gpt,comments,2023-04-14 04:09:58,AgentGPT and AutoGPT with Self-planning Capabilities,deeplearningperson,False,0.6,2,12ljbt8,https://youtu.be/1ohmpaA_IWo,5,1681445398.0,
174,deeplearning,gpt,comments,2023-12-28 21:36:23,"The best current models (Dolphin, Mixtral, Solar, Noromaid) and where to try them",Horror_Echo6243,False,0.88,6,18t59yu,https://www.reddit.com/r/deeplearning/comments/18t59yu/the_best_current_models_dolphin_mixtral_solar/,5,1703799383.0," 

I just saw a lot of people talking about this models so if you want to test them i found this websites that have all of them

\- [infermatic.ai](https://infermatic.ai/) (all of them)

\- [https://replicate.com/tomasmcm/solar-10.7b-instruct-v1.0](https://replicate.com/tomasmcm/solar-10.7b-instruct-v1.0) (for solar)

\- [https://huggingface.co/chat](https://huggingface.co/chat) (for mixtral)

Let me know if you find more, I'd like to know

And heres a little resume if you don't know what each model is for

Dolphin : An uncensored model derived from an open-source dataset, it uses instructions from FLANv2 enhanced with GPT-4 and GPT-3.5 completions‚Äã‚Äã.

Mixtral : An advanced text generation model using a Mix of Experts architecture

Solar : domain specialization and optimization. It's recognized for its high performance and efficiency

Noromaid: Storywriting and roleplay"
175,deeplearning,gpt,comments,2023-08-03 23:38:39,What would be the initial costs of developing a text-to-video AI? How would be the quality of this AI?,Claud1ao,False,0.67,1,15hjv2y,https://www.reddit.com/r/deeplearning/comments/15hjv2y/what_would_be_the_initial_costs_of_developing_a/,4,1691105919.0,"I was wondering if this would be super expensive or not.

The cost to develop GPT-3 was about $4 millions according to some resources online. 

Would the cost to develop the first version of a text-to-video AI the same? Around $5M? Is in this value included the salaries of the employees or $5M is just the amount used to train the AI?

Any answer is appreciated.

Thanks in advance."
176,deeplearning,gpt,comments,2023-06-17 20:54:57,LLMs for small projects,KrazedRook,False,0.8,3,14c1hgq,https://www.reddit.com/r/deeplearning/comments/14c1hgq/llms_for_small_projects/,3,1687035297.0,"I am looking fo a LLM to use for an app that I'm working on (for myself, it wont be published). I was originally goin to use ChatGPT but I have "" exceeded my current quota "" and I don't want to have to pay for this, so thats out of the question. I want to see if there are any others that I could use for my project, I'm using VS Code. If you have any I can use please explain it to me if you can in summary."
177,deeplearning,gpt,comments,2021-11-24 20:45:14,Current best accessible solution to isolating sounds in an audio file?,MonmusuAficionado,False,1.0,1,r1erh9,https://www.reddit.com/r/deeplearning/comments/r1erh9/current_best_accessible_solution_to_isolating/,4,1637786714.0,"I have an audio file with a voice and other background sounds, I would like remove the voice from the audio, so I need a way detect it and isolate from everything else (other sounds share similar frequencies and I was told there is no easy traditional solution to this). Does anyone know of any models created for this purpose? What I mean by accessible is something I can either train myself (so something like GPT-3 would not be an option), or a pre-trained model available through some online service."
178,deeplearning,gpt,comments,2020-09-12 12:27:55,Can GPT-3 really help you and your company? What can it really do? Real-World Applications Demo,OnlyProggingForFun,False,0.54,1,irbp5k,https://www.youtube.com/watch?v=Gm4AMjV8ErM,4,1599913675.0,
179,deeplearning,gpt,comments,2020-06-10 20:36:33,"GPT-3: The $4,600,000 Language model",mippie_moe,False,0.82,7,h0jm54,https://lambdalabs.com/blog/demystifying-gpt-3/,4,1591821393.0,
180,deeplearning,gpt,comments,2021-09-30 14:07:00,New to NLP (but not machine learning) - questions about Huggingface and NLP model development with additional text/non-text features,jsxgd,False,0.84,8,pykia3,https://www.reddit.com/r/deeplearning/comments/pykia3/new_to_nlp_but_not_machine_learning_questions/,4,1633010820.0,"Hi everyone,

&#x200B;

My work is almost always focused on structured, tabular data. Recently, though, I have been working on some tasks that are more centered on NLP for personal enrichment. I've generally been understanding well how some of the model architectures work like BERT or GPT. And I understand the difference between common NLP tasks like fill-mask and text generation. I've learned a lot from the Huggingface docs.

&#x200B;

I have two questions that are more about actually engineering something with these models:

1) I can see in Huggingface that models are marked for a specific task, like fill-mask. However, in tutorials I find, I can see that these models are being used for other tasks with seemingly good performance. For example, I found a tutorial that uses \`distilbert-base-multilingual-cased\` for a novel text classification model (classifying article text as one of several news categories). But in Huggingface, this model is labeled as a fill-mask model. What gives? Is it mislabeled? Or can I use any model for any task, just with varying degrees of success?

&#x200B;

2) I'm having a hard time finding any tutorials that mix text data with additional features which may ALSO be text or just numeric/categorical. For example, if my task is classifying a sent email as ""opened"" or ""not opened"", my main feature might be the email subject text. I might also (optionally) have a pre-header text, which in some email clients appears right below the subject. Then, I also have some additional potential features like the date the email was sent, the domain of the recipient email, etc. These features may also have a variable relationship with the text, e.g. ""Happy Christmas"" as an email subject may fare differently in December vs. January. Are there any good resources to learn how to incorporate these kinds of features into the same model?

3) More generally about deep learning (particularly if you're using tensorflow/keras) - but also with respect to the questions above - what's the best way to utilize aggregate data for classification? If I'm again looking at email data, I can of course look at this recipient-by-recipient with a 0/1 binary target field for ""opened\_email"". But this data set is huge, and in this format would be repetitive as subjects would be the same for recipients getting the same email. I can instead aggregate to a per-subject data set with two fields called ""Opens"" and ""NonOpens"" containing the counts for each type of event. Or I can do ""OpenRate"" and ""TotalRecipients"" containing the percent of recipients who opened the email and the denominator of the rate. In more classical models/packages (xgboost, GLMs, etc) it's pretty easy to make use of data in this format for binary classification. Is it similarly just as easy in a NN built with tensorflow/keras?

&#x200B;

Thanks!"
181,deeplearning,gpt,comments,2023-05-27 19:32:26,Can GPT generate GPS data?,Dangerous-Soft899,False,0.6,1,13tg26x,https://www.reddit.com/r/deeplearning/comments/13tg26x/can_gpt_generate_gps_data/,4,1685215946.0,"I am currently working on an orbit determination software. Everything is going well. The software seems to be working fine on a real dataset we got from the satellites that our customers own. Yet, we are not entirely sure if the software is doing a great job because of how little amount of data that we tested our software on. I was searching for any kind of open source GPS satellite data, but I wasn't able to. 

So, I have come up with this idea. How about training a pre-trained GPT on the GPS data that we currently have and having the model generate GPS data that are reasonable.

I am not really knowledgeable in LLM or deep learning in general. Can Generative Pre-trained Transformers generate GPS data? or can you train one to generate GPS data?

Thanks!"
182,deeplearning,gpt,comments,2023-11-06 05:28:48,"I want to create a continuously improving legal AI. My idea is to constantly feed ChatGPT-4 legal knowledge so that it keeps learning. Is this possible? If it can't be done with ChatGPT-4, is there another way to achieve this?",Turbulent_Dot_5216,False,0.27,0,17ovpmz,https://www.reddit.com/r/deeplearning/comments/17ovpmz/i_want_to_create_a_continuously_improving_legal/,3,1699248528.0," I want to create a continuously improving legal AI. My idea is to constantly feed ChatGPT-4 legal knowledge so that it keeps learning. Is this possible? If it can't be done with ChatGPT-4, is there another way to achieve this? "
183,deeplearning,gpt,comments,2023-04-24 22:41:22,AbridgIt - a browser extension that uses GPT to summarize any article you find on the web with a single click,nick313,False,0.78,15,12xzadf,https://www.reddit.com/r/deeplearning/comments/12xzadf/abridgit_a_browser_extension_that_uses_gpt_to/,4,1682376082.0,"Hi everyone,

I‚Äôd love your feedback on a new project I‚Äôm working on called [AbridgIt](http://www.abridgit.com/). When playing with GPT, one of my favorite things to ask it is to summarize long text. So, I built a simple Chrome browser extension that will automatically summarize any article you find on the web with a single click. This is version 1 so it‚Äôs pretty simple, but I would love to get some people to try it (it‚Äôs free) and give some feedback.

Example of how it works:

&#x200B;

https://preview.redd.it/m1ryu2u9uwva1.png?width=640&format=png&auto=webp&s=4626472cfaed0b1cedbb3492f1a1209491a8a265

 Check it out and let me know what you think."
184,deeplearning,gpt,comments,2023-11-15 18:18:23,Exploring the Frontiers of AI with Taskade: Introducing AI Agents for Deep Learning Enthusiasts üöÄ,taskade,False,0.78,5,17vzwl5,https://www.reddit.com/r/deeplearning/comments/17vzwl5/exploring_the_frontiers_of_ai_with_taskade/,4,1700072303.0," 
Hey r/deeplearning,

I'm John from [Taskade](https://taskade.com), and I'm thrilled to introduce you to our latest endeavor in the realm of AI: Taskade AI Agents. This feature is a blend of practicality and deep learning innovation, and we're eager to dive into discussions with enthusiasts like you.

**Taskade AI Agents - What's Under the Hood?**

- Taskade AI Agents is all about creating, training, and deploying custom AI agents to automate and enhance productivity tasks.
- Powered by GPT-4 Turbo, it's designed for those who appreciate the intricacies of AI and deep learning technologies.

**Why It Matters for Deep Learning:**

- Our AI Agents are more than just productivity tools; they're a testament to the advancements in neural networks and AI capabilities.
- We're pushing the boundaries of how AI can be utilized in everyday task management and collaboration environments.

**We're Keen on Your Insights:**

- As deep learning enthusiasts, your perspectives on AI implementation, performance, and potential improvements are invaluable.
- How do you see AI Agents like ours fitting into the broader landscape of AI and deep learning?
- We're especially interested in your thoughts on our use of GPT-4 Turbo and how it could evolve.

**Join the Conversation:**

- Learn more about Taskade AI Agents on our [Product Hunt page](https://www.producthunt.com/posts/taskade-ai-agents).
- Dive deeper into our feature on our [Blog](https://www.taskade.com/blog/custom-ai-agents-gpts/).
- Try it out and experiment with it [here](https://www.taskade.com/ai).

Your feedback, critiques, and ideas are not just welcomed, they're needed. Help us understand the impact of Taskade AI Agents from a deep learning perspective and how we can continue to innovate in this space.

Looking forward to some insightful discussions!

Cheers,
John & the /r/Taskade Team ü§ñ‚ú®"
185,deeplearning,gpt,comments,2021-12-17 16:25:47,Transformer assimilates syntax perfectly,jssmith42,False,0.91,9,ril1wx,https://www.reddit.com/r/deeplearning/comments/ril1wx/transformer_assimilates_syntax_perfectly/,4,1639758347.0,"Has anyone analysed why GPT-3 seems to master the syntax of languages nearly perfectly as opposed to not having a perfect understanding of higher-level aspects of cognition?

It could be a simple answer, that syntax is less of a complex system/pattern/structure than conceptual understanding of the world.

But I feel like there is something more interesting to be said.

For example, it seems like the bigger the model, the smarter it becomes.

Is AI as simple as, we have a structure (a neural network) that can intuitively understand any system or phenomenon because it finds some kind of model for it, a layered series of weights corresponding to some conceptual hierarchy. It just depends what order the phenomenon is. A hyper-complex phenomenon needs 100 layers, or whatever. A simple one only needs 3. In either case, there is conceivably nothing a neural network cannot eventually understand.

Is this true? If so, it‚Äôs a pretty wild notion to contemplate."
186,deeplearning,gpt,comments,2021-11-26 18:59:22,Music generation toolbox,wingedsheep38,False,0.67,3,r2u8oi,https://www.reddit.com/r/deeplearning/comments/r2u8oi/music_generation_toolbox/,4,1637953162.0,"This year I joined the team ""Lovelace and the machines"" for the AI Song Contest 2021. With the goal of using algorithms / machine learning to generate music, and then team up with musicians to create an actual song. We used a combination of GPT-3 for the lyrics and a music transformer implementation for the notes, and a bunch of other techniques for analyzing and rating the results or generating variations. It was a really cool challenge and our team ended up in second place with our song ""[Quantum trap](https://www.youtube.com/watch?v=YSn5pBdFjS4)"". I wrote a [blogpost](https://wingedsheep.com/music-generation-creating-a-song-for-the-ai-song-contest-2021/) about it for those interested in the details.

I created a project for the music generation tools that we used, so other people who are interested can experiment with it. You can find it here: [https://github.com/wingedsheep/music-generation-toolbox](https://github.com/wingedsheep/music-generation-toolbox). The goal of this project is to implement new techniques of music generation so they can be compared and tested.

Some samples created so far:

* Pop909 dataset with a compound word transformer [https://soundcloud.com/user-419192262-663004693/sets/compound-word-transformer-pop909](https://soundcloud.com/user-419192262-663004693/sets/compound-word-transformer-pop909)
* Pop909 dataset with a routing transformer [https://soundcloud.com/user-419192262-663004693/sets/routing-transformer-pop909](https://soundcloud.com/user-419192262-663004693/sets/routing-transformer-pop909)
* Lakh midi dataset (multi instrument) with music transformer [https://soundcloud.com/user-419192262-663004693/sets/generated-by-music-transformer-from-scratch](https://soundcloud.com/user-419192262-663004693/sets/generated-by-music-transformer-from-scratch)

I'm always interested to hear new ideas on how to improve or which new techniques to add!

Also I'm looking for a way to host the models, so people can try it in Colab without having to train a model from scratch. Any good ideas on where to put my models?"
187,deeplearning,gpt,comments,2023-12-14 02:03:03,[D] Constructing an Efficient Knowledge Graph RAG Pipeline with LlamaIndex,Fit_Maintenance_2455,False,1.0,2,18hxo48,https://www.reddit.com/r/deeplearning/comments/18hxo48/d_constructing_an_efficient_knowledge_graph_rag/,4,1702519383.0,"Large Language Models (LLMs) such as ChatGPT and Bard exhibit remarkable abilities within their specialized areas of training. However, their constraints in handling new or private data inquiries are widely recognized.

Retrieval Augmented Generation (RAG) emerges as a solution to bridge this gap, allowing LLMs to access external knowledge sources. This article delves into RAG, examines its elements, and constructs a usable RAG workflow that harnesses the potential of LlamaIndex, a knowledge graph.

&#x200B;

Link: [https://medium.com/ai-advances/constructing-an-efficient-knowledge-graph-rag-pipeline-with-llamaindex-81a0a0b105b7](https://medium.com/ai-advances/constructing-an-efficient-knowledge-graph-rag-pipeline-with-llamaindex-81a0a0b105b7)  "
188,deeplearning,gpt,comments,2023-07-31 17:01:30,Where can I keep on top of LLM developments?,gonidphoe7,False,0.55,1,15elov0,https://www.reddit.com/r/deeplearning/comments/15elov0/where_can_i_keep_on_top_of_llm_developments/,3,1690822890.0,"I'm currently attempting to broaden my knowledge of AI and ML, particularly in relation to large language models. My understanding so far is that a significant limitation of these models is their restricted context window, which appears to hinder their ability to maintain continuity of information and reason effectively about complex topics. I see models like GPT-4, Anthropic's Claude, and Mosaic ML implementing larger windows (currently 32k, 100k and 82k tokens respectively).

Can anyone confirm whether my comprehension of the context window is accurate? If not, could you explain the primary challenges that impede the reasoning and problem-solving abilities of LLMs? Additionally, what are the proposed solutions currently being explored to overcome these challenges? Finally, could anyone recommend the best way to stay on top of developments in the LLM and AI agent space?"
189,deeplearning,gpt,comments,2023-01-11 14:41:25,What do you all think about these ‚ÄúSEO is Dead‚Äù articles?,Aggressive-Twist-252,False,0.91,65,1096byl,https://www.reddit.com/r/deeplearning/comments/1096byl/what_do_you_all_think_about_these_seo_is_dead/,3,1673448085.0,"I keep seeing [articles](https://jina.ai/news/seo-is-dead-long-live-llmo/) like this over the years and it made me wonder. Is SEO really dead? Or will it evolve? Back then I kept wondering if it‚Äôs true or not. Some believe SEO is dead, some don‚Äôt. But now with tools like Chat GPT and Midjourney, I think it‚Äôs time to take a look back and see how this might change SEO or if it will ‚Äúkill‚Äù SEO.

I keep seeing threads and discussions seeing how people are excited and worried at the same time with how AI might be able to do a better job. But the way I see it, AI content still needs a person to tell it what to do and make the writing look nice. And also I think that the internet will have a lot of writing that was made by AI and that might change how we find things online. You might also see a ton of content being written by AI and trigger some plagiarism detectors and have a lot of websites get penalized. Hopefully the internet won‚Äôt be filled with boilerplate copy/pasted content coming from Chat GPT.

Well we have Google to filter out trash content anyway. But I know Google has some issues lately that they need to fix. One is that they also have AI that can help people find things on the internet with their search engine, and they need to make sure they are still the best in terms of search. 

The second is that Google needs to find a way to tell if something is really good or not, like how some websites that show art do. Google wants to show the best thing first, but it's hard because sometimes the thing that is the best is also something that Google's customers want people to see. It‚Äôs possible that some AI generated contentSo it's kind of tricky.

I have a feeling companies that already make SEO-writing and checking bots are gonna roll out some fresh new models soon. They're gonna be even better than before. These bots are going to write some good articles and product descriptions that are almost perfect. It almost looks like a human wrote the article or description. And all a human will do is quickly check for any false claims and write a headline that doesn't sound like a robot wrote it. 

We can only really tell 5-10 years from now. In the meantime, I‚Äôll probably go back practicing some handyman skills and also go back teaching people how to drive and also be a service driver. These jobs I had in the past were way different from what I am earning now but if the worst comes to worst, at least I have these physical skills ready."
190,deeplearning,gpt,comments,2023-02-03 19:38:56,GPT-2 small model (124M params) hw requirements,honzovy-klipy,False,0.9,7,10st418,https://www.reddit.com/r/deeplearning/comments/10st418/gpt2_small_model_124m_params_hw_requirements/,1,1675453136.0,"Hey, I was wandering how much VRAM and RAM do I need for running (inference only) gpt2-small model from hugging face, but was not able to find anything. Can somebody help please?"
191,deeplearning,gpt,comments,2023-04-18 15:00:24,Uni project: a FOSS LLM comparison tool - would you find this useful?,copywriterpirate,False,0.97,21,12qq3mz,https://www.reddit.com/gallery/12qq3mz,3,1681830024.0,
192,deeplearning,gpt,comments,2023-04-02 12:37:38,[N] Software 3.0 Blog Post Release üî•,DragonLord9,False,0.74,11,129k24i,https://www.reddit.com/r/deeplearning/comments/129k24i/n_software_30_blog_post_release/,3,1680439058.0,"Hi all, excited to share my blog post on [**Software 3.0**](https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm_campaign=post&utm_medium=web)

https://preview.redd.it/9b4hjkkhugra1.png?width=1500&format=png&auto=webp&s=e341f3ab4c3c8abb206df8daa17428a297ff61e2

The blog post offers an insightful read on the new GPT-powered programming paradigm where the new programming language is simply ""*English*"", as well as recent developments in AI.

The post was originally written before GPT-4 release, and the predictions seem to have held surprisingly well. Knowledge cutoff date 28 Feb 2023.

Please read and share!! Happy to answer any follow-ups here or on DM üòä

Tweet: [https://twitter.com/DivGarg9/status/1642229948185280521?s=20](https://twitter.com/DivGarg9/status/1642229948185280521?s=20)

Blog:¬†[https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm\_campaign=post&utm\_medium=web](https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm_campaign=post&utm_medium=web)"
193,deeplearning,gpt,comments,2023-01-17 16:06:37,What nobody tells you about chatGPT and GPT-4,thomas999999,False,0.33,0,10efwno,https://www.reddit.com/r/deeplearning/comments/10efwno/what_nobody_tells_you_about_chatgpt_and_gpt4/,3,1673971597.0,i wanted to share a nice writeup my friend made about chatGPT [https://medium.com/@christian.bernhard97/what-nobody-tells-you-about-chatgpt-and-gpt-4-c8d97ae9f92d](https://medium.com/@christian.bernhard97/what-nobody-tells-you-about-chatgpt-and-gpt-4-c8d97ae9f92d)
194,deeplearning,gpt,comments,2022-06-09 06:57:22,Fine-tuning model for long contexts,Expert-Departure-236,False,1.0,1,v8av9v,https://www.reddit.com/r/deeplearning/comments/v8av9v/finetuning_model_for_long_contexts/,3,1654757842.0,"How to train GPT or BERT for large context where context length is more than 1024 tokens. Truncating the context is not an option as the complete context is important. 

One approach that I can think of is breaking/dividing the context into multiple chunks.

What are my other options?t"
195,deeplearning,gpt,comments,2023-12-12 13:23:57,LLM,BV98-_-,False,0.29,0,18glry5,https://www.reddit.com/r/deeplearning/comments/18glry5/llm/,3,1702387437.0,"Hi guys, i'm working ti create a LLM with a model as gpt. I have a text dataset. Where can i find gpt documentation to create my model? Then do you suggest to me to do a finetuning for an model already existent?"
196,deeplearning,gpt,comments,2019-09-05 21:34:30,Write With Transformer: A web app to compare generative NLP transformer-based models.,jikkii,False,1.0,4,d077mt,https://www.reddit.com/r/deeplearning/comments/d077mt/write_with_transformer_a_web_app_to_compare/,3,1567719270.0,"Sharing with you a project we've been working on at Hugging Face: [Write With Transformer](https://transformer.huggingface.co/). It is a web app that hosts most state-of-the-art transformer-based NLP generative models like **GPT-2**, **GPT** or **XLNet.**

You can write a context and trigger completions from the generative model you choose, in a Google Doc-like interface. It also includes one of our fine-tuned models, using GPT-2 as a pretrained model and fine-tuning it on Arxiv papers to get NLP/Deep Learning completions.

It's built on top of our library [pytorch-transformers](https://github.com/huggingface/pytorch-transformers). Let us know what you think!"
197,deeplearning,gpt,comments,2023-05-15 15:10:52,[P] ts-tok: Time-Series Forecasting with Classification,arpytanshu,False,0.93,12,13ib22w,https://www.reddit.com/r/deeplearning/comments/13ib22w/p_tstok_timeseries_forecasting_with_classification/,3,1684163452.0,"Hey everyone!  
I wanted to share with you a weekend project I've been working on called **ts-tok**. It's an experimental approach to time-series forecasting that uses classification instead of regression.  
Essentially, we take a range of time-series values and transform them into a fixed vocabulary of tokens. This allows for a seamless training of GPT like models without changing the architecture or loss function.  
There are some subtleties required for data preparation for training, and I've outlined these in the README, so feel free to check it out!  
While this approach 'may' not have practical applications in the real world, it's been a fun experiment to explore.  
I've included some forecasting results in the output/ folder, so feel free to check those out! Open to feedback from the community about potential use cases and limitations of this approach.  
Thanks for taking the time to read about this project!  
[https://github.com/arpytanshu1/ts-tok](https://github.com/arpytanshu1/ts-tok)"
198,deeplearning,gpt,comments,2023-03-08 09:56:09,"Weaviate Vector DB adds support for Product Quantization, Bitmap Filters, Filtered Hybrid Search, Tunable Consistency, and more in the v1.18 release.",hootenanny1,False,0.78,5,11lsal6,https://www.reddit.com/r/deeplearning/comments/11lsal6/weaviate_vector_db_adds_support_for_product/,2,1678269369.0,"Ever since Chat-GPT has hit the masses, the interest in vector search has gone through the roof. Weaviate takes an end2end approach to vector search because it also stores the data object, and builds inverted indexes besides the vector indexes.  


Yesterday, version `v1.18.0` was released, with the following features that were in high demand by the community:

# Product Quantization

Weaviate v1.18 allows compressing vector embeddings using Product Quantization in combination with HNSW vector indexing (HNSW-PQ). This allows for a lower memory footprint while keeping low latency and high recall

# Bitmap Filtering

Weaviate's inverted index is now built natively on top of roaring bitmaps. This allows for very fast filtered vector search even at the 100M or billion scale. In some extreme cases, search latencies went down from 5s to 5ms.

# Filtered Hybrid Search

Weaviate v1.17 added support for Hybrid (BM25 sparse + Vector Dense) search. However, it did not (yet) allow for setting filters on Hybrid Search queries. This is now possible with v1.18

# BM25 WAND Scoring

Weak-AND (""WAND"") is a BM25 scoring algorithm that avoids scoring documents that cannot reach a high enough score to be contained in the result set. This speeds up BM25 ‚Äì¬†and in turn ‚Äì hybrid search

# Tunable Consistency and Automatic Repairs

A previous Weaviate release added support for High-Availability through Replication. However, the desired level of consistency when reading and writing was set by Weaviate. Now, the user can set these settings according to their preferences. In addition, if Weaviate detects an inconsistency (e.g. after a temporary node failure) it can now be repaired automatically when reading the ""corrupt"" object.

# Cursor API

In previous Weaviate releases, it was impossible to export all objects from Weaviate because of the increasing cost of each page on pagination. The new cursor API provides a constant-cost way to extract all objects (and their vector embeddings) from Weaviate.

# Azure Backup Module

In addition to Google Cloud Storage, and Amazon S3, Weaviate now supports Azure Blob storage for seamless backups and restores.

\---

More information:

* [Release blog post](https://weaviate.io/blog/weaviate-1-18-release)
* [Release on GitHub](https://github.com/weaviate/weaviate/releases/tag/v1.18.0)

Disclaimer: I am a co-founder of Weaviate."
199,deeplearning,gpt,comments,2023-08-25 13:21:12,AI Meets AI: A Conversation Between GPT-4 and Google's Bard,Ubica123,False,0.82,35,160z5pp,https://www.youtube.com/watch?v=3H45IncZ7gs,3,1692969672.0,
200,deeplearning,gpt,relevance,2024-01-01 05:48:19,"VerificationGPT (open-source verification for GPT-4 using Brave Search, arXiv, and other APIs)",contextfund,False,1.0,2,18vq5vb,/r/contextfund/comments/18vp9hv/verificationgpt/,0,1704088099.0,
201,deeplearning,gpt,relevance,2023-04-05 01:36:40,Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT,Time_Key8052,False,0.94,83,12c43uu,https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/,19,1680658600.0,"Vicuna : ChatGPT Alternative, Open-Source, High Quality and Low Cost 

&#x200B;

[ Relative Response Quality Assessed by GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a072b91)

Vicuna-13B has demonstrated competitive performance against other open-source models, such as Stanford Alpaca, by fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.

Evaluation using GPT-4 as a judge shows that Vicuna-13B achieves more than 90% of the quality of OpenAI ChatGPT and Google Bard AI, while outperforming other models such as Meta LLaMA (Large Language Model Meta AI) and Stanford Alpaca in more than 90% of cases.

The cost of training Vicuna-13B is approximately $300.

The training and serving code, along with an online demo, are publicly available for non-commercial use.

&#x200B;

More Information : [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT](https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)

Discord Server : [https://discord.gg/h6kCZb72G7](https://discord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysorg](https://twitter.com/lmsysorg)"
202,deeplearning,gpt,relevance,2024-01-22 19:37:33,Wanna develop my own LLM model like GPT and Gemini,missionseeker,False,0.2,0,19d47b7,https://www.reddit.com/r/deeplearning/comments/19d47b7/wanna_develop_my_own_llm_model_like_gpt_and_gemini/,24,1705952253.0,"I'm web developer and have a good understanding of data structures. I'm now interested to create my own LLM model but I didn't know from where I could start. So, if you guys help me to get some useful resources or information which would help me. So to get my own did I've to enroll myself into PhD as I didn't have no basic understanding of it. 
I request you guys to help me."
203,deeplearning,gpt,relevance,2023-10-24 15:34:49,MemGPT Explained!,CShorten,False,0.93,21,17ffmuu,https://www.reddit.com/r/deeplearning/comments/17ffmuu/memgpt_explained/,2,1698161689.0,"Hey everyone! I am SUPER excited to publish a new paper summary video of MemGPT from Packer et al. at UC Berkeley!

MemGPT is a massive step forward in the evolution from naive Retrieval-Augmented Generation (RAG) to creating an OPERATING SYSTEM for LLM applications!

This works by telling the LLM about its limited input window and giving it new ""tools"" / APIs to manage its own memory. For example, the LLM processes the conversation history in a chatbot or the next paragraph in document processing and determines what is important to add to its working context.

The authors design a operating system around this concept complete with events, functions, and of a virtual context management algorithm inspired by operating system concepts such as page replacement. When the LLM determines it needs more context to answer a question, it searches into it's external context (could be recall storage (complete history of events such as dialogue in a chatbot across 4 months), or its archival storage (information such as Wikipedia entries stored in a Vector DB) -- it then parses the search results to determine what is worth adding to its working context.

The authors test MemGPT on chatbots and the experiments from Lost in the Middle, finding that this explicit memory management overcomes the problems of losing relevant information in the middle of search results!

I think there are tons of exciting implications of this work such as the intersection with the Gorilla LLMs (trying to allocate as few tokens as possible in describing a tool to an LLM), as well as this general phenomenon of connecting LLMs to Operating Systems!

Here is my review of the paper in more detail, I hope you find it useful!

[https://www.youtube.com/watch?v=nQmZmFERmrg](https://www.youtube.com/watch?v=nQmZmFERmrg)"
204,deeplearning,gpt,relevance,2023-04-14 04:09:58,AgentGPT and AutoGPT with Self-planning Capabilities,deeplearningperson,False,0.6,2,12ljbt8,https://youtu.be/1ohmpaA_IWo,5,1681445398.0,
205,deeplearning,gpt,relevance,2023-04-06 07:43:06,A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?,Learningforeverrrrr,False,0.56,1,12dcnrm,https://www.reddit.com/r/deeplearning/comments/12dcnrm/a_complete_survey_on_generative_ai_aigc_is/,0,1680766986.0,"We recently completed two surveys: one on generative AI and the other on ChatGPT. Generative AI and ChatGPT are two fast-evolving research fields, and we will update the content soon, for which your feedback is appreciated (you can reach out to us through emails on the paper).

The title of this post refers to the first one, however, we put both links below.

**Link to a survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

**Link to a survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)

The following is the **abstract** of the **survey on generative AI** with a summary **figure**.

As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible to miss the opportunity to glimpse AIGC from a certain angle.  In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? To answer this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its **techniques** to **applications**. Modern generative AI relies on various technical foundations, ranging from **model architecture** and **self-supervised pretraining** to **generative modeling** methods (like GAN and diffusion models). After introducing the fundamental techniques, this work focuses on the technological development of various AIGC tasks based on their output type, including **text**, **images**, **videos**, **3D content**, **etc**., which depicts the full potential of ChatGPT's future. Moreover, we summarize their significant applications in some mainstream **industries**, such as **education** and **creativity** content. Finally, we discuss the **challenges** currently faced and present an **outlook** on how generative AI might evolve in the near future.

&#x200B;

https://preview.redd.it/scbpeabnx7sa1.png?width=1356&format=png&auto=webp&s=445da6a707ceb6af75e5305137ad30dcd06c32fe

**Link to a survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

**Link to a survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)"
206,deeplearning,gpt,relevance,2024-02-19 15:46:54,"When will the advancement in things like GPT, Stable Diffusion and Sora plateau?",yasserius,False,0.76,9,1auqe2g,https://www.reddit.com/r/deeplearning/comments/1auqe2g/when_will_the_advancement_in_things_like_gpt/,13,1708357614.0,"I know deep learning based models are seeing the fastest advancements ever, thanks to huge compute and data. 

But people keep extrapolating and people talk about accelerating / exponential growth and the singularity being reached.

But I would argue most of these models are based off novel neural architectures and then they hyperparameter tune and train on more data, and voila, you see huge advancements and it makes it look like accelerating progress.

But once we've exhausted the different architectures and optimized the hyperparameters, shouldn't we plateau? I'm not saying we will plateau anytime soon, but can we really expect more emergent behavior as we keep scaling up?

Please don't attack me for being a noob, constructive explanations based on logic will be appreciated, thanks!"
207,deeplearning,gpt,relevance,2023-03-15 01:53:32,How good is GPT-4 compared to ChatGPT?,OnlyProggingForFun,False,0.18,0,11rihli,https://youtu.be/GroMQETFXLc,1,1678845212.0,
208,deeplearning,gpt,relevance,2023-12-17 22:15:54,Any idea of GPT-4 Vision architecture?,AfraidAd4094,False,0.71,3,18kstjs,https://www.reddit.com/r/deeplearning/comments/18kstjs/any_idea_of_gpt4_vision_architecture/,1,1702851354.0,"Is it a big Vision Transformer, or maybe extra feature engineering step to adapt images as an input gpt-4? Like transforming an image to a vector embedding of same dimensions as text input  


 "
209,deeplearning,gpt,relevance,2023-11-07 01:54:32,A GPT to generate NFL Plays,bougsamm,False,0.8,3,17pjgg9,https://www.reddit.com/r/deeplearning/comments/17pjgg9/a_gpt_to_generate_nfl_plays/,0,1699322072.0,"Hi everyone, coming ack here after few months of work.I have created an AI able to generate NFL plays. The idea would be later to use it to simulate a high number of scenarios and predict plays' outcomes. You can play with a beta of an app here : [https://huggingface.co/spaces/samchain/QBGPT](https://huggingface.co/spaces/samchain/QBGPT)

I did spend some time on improving it but I would be very interested to have your feedbacks on when it fails hard to generate realistic plays, when not etc. Any relevant comment is welcome. I will update it soon in order to have game information and which teams are facing each others.

If interested by the methodology, a link of the article is available in the second page of the app.

More generally, I am interested to hear your opinions on AI in sports and especially NFL. Do you think this game can be simulated and if yes what are the challenges ?

https://i.redd.it/9vjlhu7p0uyb1.gif"
210,deeplearning,gpt,relevance,2023-01-17 16:06:37,What nobody tells you about chatGPT and GPT-4,thomas999999,False,0.25,0,10efwno,https://www.reddit.com/r/deeplearning/comments/10efwno/what_nobody_tells_you_about_chatgpt_and_gpt4/,3,1673971597.0,i wanted to share a nice writeup my friend made about chatGPT [https://medium.com/@christian.bernhard97/what-nobody-tells-you-about-chatgpt-and-gpt-4-c8d97ae9f92d](https://medium.com/@christian.bernhard97/what-nobody-tells-you-about-chatgpt-and-gpt-4-c8d97ae9f92d)
211,deeplearning,gpt,relevance,2023-02-03 13:21:37,Implementing DetectGPT from scratch - Open-sourcing DetectGPT,BurhanUlTayyab,False,1.0,4,10sk6dl,https://www.reddit.com/r/deeplearning/comments/10sk6dl/implementing_detectgpt_from_scratch_opensourcing/,0,1675430497.0,"We've implemented DetectGPT paper in Pytorch. Our implementation can be found below

Github: [https://github.com/BurhanUlTayyab/DetectGPT](https://github.com/BurhanUlTayyab/DetectGPT)

Website: [https://gptzero.sg](https://gptzero.sg)

Discord: [https://discord.com/invite/F3kFan28vH](https://discord.com/invite/F3kFan28vH)

We're also working on a GPTZerov2 (inspired by LLM based transformers and GANs), which would be more accurate, and can detect lines changed by humans.

Please give some feedback on our work.

Thanks"
212,deeplearning,gpt,relevance,2023-04-21 14:29:14,Is there any nano-gpt/pico-gpt like implementation available for stable-diffusion models?,Blue_Dude3,False,0.67,1,12u3j5x,https://www.reddit.com/r/deeplearning/comments/12u3j5x/is_there_any_nanogptpicogpt_like_implementation/,1,1682087354.0,"The original paper skips some implementation details like -

* how exactly does the attention mechanism work? What are the query, key, value pairs?
* The loss function of the auto encoder is not clear at all.

and many other small details where the authors have just referenced some other papers.

The implementations available on Github (mainly from stability AI and CompVis) is too complicated to understand since it is written for different architectures, tasks. And the code base does not have comments which is also not helpful.

I would like to have a simple implementation of stable-diffusion model for any one particular task like (text to image or image to image). Understand the purpose of each module / block with reference to the paper.

Can anyone suggest such implementation of stable-diffusion that achieves some reasonable results (like nano-gpt)?"
213,deeplearning,gpt,relevance,2023-04-24 13:14:39,Applications of GPT,AcornWizard,False,0.27,0,12xfegq,https://www.reddit.com/r/deeplearning/comments/12xfegq/applications_of_gpt/,5,1682342079.0,"Hello. Is ChatGPT currently the only implemented application that uses GPT? Looking on the internet I see a lot of flashy but often vague talk about potential applications, yet I have not found more implemented uses."
214,deeplearning,gpt,relevance,2023-10-05 15:06:05,Generating production-level streaming microservices using GPT,davorrunje,False,0.9,8,170kbnm,https://www.reddit.com/r/deeplearning/comments/170kbnm/generating_productionlevel_streaming/,2,1696518365.0,"[faststream-gen](https://github.com/airtai/faststream-gen/) uses GPT models to automatically generate microservices using the [FastStream](https://github.com/airtai/faststream) framework for Apache Kafka, RabbitMQ and NATS. Simply describe your microservice in plain English, and it will generate a production-level FastStream application ready to deploy in a few minutes and under $1 cost, together with unit and integration tests, documentation and Docker images.

See the full (video and detailed step-by-step textual) tutorial üëâ [here](https://faststream-gen.airt.ai/Tutorial/Cryptocurrency_Tutorial/) üëà"
215,deeplearning,gpt,relevance,2023-08-02 11:33:32,Using PDFs with GPT Models,Disastrous_Look_1745,False,0.73,10,15g6i4x,https://www.reddit.com/r/deeplearning/comments/15g6i4x/using_pdfs_with_gpt_models/,2,1690976012.0,Found a blog talking about how we can interact with PDFs in Python by using GPT API & Langchain. It talks about some pretty cool automations you can build involving PDFs - [https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/](https://nanonets.com/blog/chat-with-pdfs-using-chatgpt-and-openai-gpt-api/)
216,deeplearning,gpt,relevance,2023-09-28 13:34:24,First Impressions with GPT-4V(ision),zerojames_,False,0.67,3,16ug8gc,https://www.reddit.com/r/deeplearning/comments/16ug8gc/first_impressions_with_gpt4vision/,0,1695908064.0,"My colleague Piotr and I have been testing GPT-4V(ision) over the last day. We wrote up our findings, covering how GPT-4V performs on:

1. Visual question answering (VQA) across a range of domains (locations, movies, plants)
2. OCR
3. Math OCR
4. Object detection
5. And more

TL;DR: GPT-4V performed well for VQA and document OCR but struggled with OCR on real-world images and object detection (where we asked for bounding boxes).

[https://blog.roboflow.com/gpt-4-vision/](https://blog.roboflow.com/gpt-4-vision/)

I would love to hear what other people have found working with GPT-4V."
217,deeplearning,gpt,relevance,2023-04-25 17:53:47,"Microsoft releases SynapseMl v0.11 with support for ChatGPT, GPT-4, causal learning, and more",mhamilton723,False,0.88,24,12yqpnp,https://www.reddit.com/r/deeplearning/comments/12yqpnp/microsoft_releases_synapseml_v011_with_support/,0,1682445227.0,"Today Microsoft launched SynapseML v0.11 with support for ChatGPT, GPT-4, distributed training of huggingface and torchvision models, an ONNX Model hub integration, Causal Learning with EconML, 10x memory reductions for LightGBM, and a newly refactored integration with Vowpal Wabbit. To learn more check out our release notes and please feel give us a star if you enjoy the project!

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.11.0](https://github.com/microsoft/SynapseML/releases/tag/v0.11.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919)

Thank you to all the contributors in the community who made the release possible!

&#x200B;

[What's new in SynapseML v0.11](https://preview.redd.it/9pqj1mowj2wa1.png?width=4125&format=png&auto=webp&s=a358e73760c847a09cc76f2ed17dc58e15aed5ed)"
218,deeplearning,gpt,relevance,2022-12-12 17:29:39,ChatGPT context length,Wild-Ad3931,False,0.97,30,zk5esp,https://www.reddit.com/r/deeplearning/comments/zk5esp/chatgpt_context_length/,10,1670866179.0,"How come ChatGPT can follow entire discussions whereas nowaday's LLM are limited (to the best of my knowledge) 4096 tokens ?

I asked it and it is not able to answer neither, and I found nothing on Google because no paper is published. I was also curious to understand how come Beamsearch was so fast with ChatGPT.

https://preview.redd.it/o6djsmpb5i5a1.png?width=1126&format=png&auto=webp&s=98baae5bf6fa294db408f0530214f8afa8a32a0b"
219,deeplearning,gpt,relevance,2023-09-29 21:35:07,Question about training and fine tuning a GPT model,mojo_no_jojo,False,1.0,1,16vo3wy,https://www.reddit.com/r/deeplearning/comments/16vo3wy/question_about_training_and_fine_tuning_a_gpt/,5,1696023307.0,"Not sure if this is the right place to ask for help, and if it isn't, I apologise in advance. Please #nohate

I have created an LLM that performs miserably and terribly. I mean that'd be an understatement. I followed a incredibly poor YouTube video of a guy doing this, but he never really showed how to fine tune it so I can make it work better. My code and everything works, because I know I have a poorly functional model. FYI, I used the openwebtext corpus (about 45gb) for training and validation. 

So, here's my question: How may I fine tune it so I can make it work? Specifically, for my use case. 

My use case: I'd like to be able to give the model some text (preferably in paragraphs), and make it generate some questions about it.

I know there are libraries on HuggingFace that I can use but I'd very much like to be able to do it myself, if possible. 

Resources at my disposal: My own basic laptop, and Google Colab with T4 GPU and a TPU (among other things it offers). To train my (poorly functional) model I used a T4 GPU. 

Time: less than two weeks.

Premise: I'm doing my Masters and as part of my final sem project, I've decided to create my own model where you give it some text, like from a PDF document, and it asks you questions. I would like to include boolean type questions, like True or False types, and/or MCQ types. 

So please can you share how may I fine tune the model so it is able to do exactly that? I could share the Github project if you'd like, but I'm not sure if this the right place to ask this. I've not 100% grasped the Transformer model, but I'm trying. I'm pretty OK with deep learning though. 

If I sounded boastful or rude, I am sorry. I'm not trying to be it.

Edit 1: Changed the type of model I had created. It's an LLM."
220,deeplearning,gpt,relevance,2023-05-27 19:32:26,Can GPT generate GPS data?,Dangerous-Soft899,False,0.6,1,13tg26x,https://www.reddit.com/r/deeplearning/comments/13tg26x/can_gpt_generate_gps_data/,4,1685215946.0,"I am currently working on an orbit determination software. Everything is going well. The software seems to be working fine on a real dataset we got from the satellites that our customers own. Yet, we are not entirely sure if the software is doing a great job because of how little amount of data that we tested our software on. I was searching for any kind of open source GPS satellite data, but I wasn't able to. 

So, I have come up with this idea. How about training a pre-trained GPT on the GPS data that we currently have and having the model generate GPS data that are reasonable.

I am not really knowledgeable in LLM or deep learning in general. Can Generative Pre-trained Transformers generate GPS data? or can you train one to generate GPS data?

Thanks!"
221,deeplearning,gpt,relevance,2023-03-20 06:27:48,GPT-4,Genius_feed,False,0.4,0,11wat6c,https://i.redd.it/h1ov2l5p8uoa1.jpg,0,1679293668.0,
222,deeplearning,gpt,relevance,2023-08-11 17:29:49,GPT Sequence Classification explainability or interpretability,how_the_turn_tablez,False,0.67,1,15of290,https://www.reddit.com/r/deeplearning/comments/15of290/gpt_sequence_classification_explainability_or/,0,1691774989.0,"I‚Äôm using GPT-2 for Sequence Classification. I want to understand the words or sequences that lead to the predictions. Can you point me towards any papers, repos or libraries?"
223,deeplearning,gpt,relevance,2023-12-07 05:25:34,Gemini vs. GPT-4: Google's AI Takes the Lead in Benchmarks,Damanjain,False,0.44,0,18cofmx,https://thebuzz.news/article/gemini-vs-gpt-4-in-benchmarks/11594/,2,1701926734.0,
224,deeplearning,gpt,relevance,2023-01-29 22:39:07,[P] We built a browser extension that unlocks browser mode capabilities using ChatGPT: MULTI¬∑ON: AI Web Co-Pilot powered by ChatGPT,DragonLord9,False,0.95,64,10okyg3,https://v.redd.it/2hw47h0b82fa1,8,1675031947.0,
225,deeplearning,gpt,relevance,2023-11-02 13:41:49,MemGPT with Charles Packer - Weaviate Podcast #73!,CShorten,False,1.0,1,17m3r5l,https://www.reddit.com/r/deeplearning/comments/17m3r5l/memgpt_with_charles_packer_weaviate_podcast_73/,0,1698932509.0,"Hey everyone! I am SUPER excited to publish our 73rd Weaviate Podcast with Charles Packer from UC Berkeley, the lead author of MemGPT!!

MemGPT is the Operating System for LLMs! This is one of the most exciting recent ideas in Large Language Model Applications that draws connections between Virtual Context Management in Operating Systems and the limited context windows of LLMs! One of the key ideas in MemGPT is to explicitly prompt the model that it has a limited token window and give it tools to self-edit its own context!!

This was such a fun discussion covering all things from the origin of the research, connections between operating systems and LLMs, details of the paper such as inference cost, explicit context annotation, recall and archival storage, and many more!

We also concluded the podcast with discussions around Creativity in AI and what directions from Multi-Agent Role Playing or Evolutionary methods will achieve truly creative, novel outputs from Generative AI systems!

I hope you enjoy the podcast! As always, more than happy to answer any questions or discuss any ideas about the content in the podcast!

[https://www.youtube.com/watch?v=rxjsbUiuOFo](https://www.youtube.com/watch?v=rxjsbUiuOFo)"
226,deeplearning,gpt,relevance,2023-11-16 08:28:46,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,0.84,69,17whz6e,https://v.redd.it/qx68wbuf7o0c1,7,1700123326.0,
227,deeplearning,gpt,relevance,2023-12-12 08:32:31,[D] Revolutionizing 3D Asset Modeling in the Metaverse Era with 3D-GPT,Fit_Maintenance_2455,False,0.5,0,18ghcme,https://www.reddit.com/r/deeplearning/comments/18ghcme/d_revolutionizing_3d_asset_modeling_in_the/,1,1702369951.0,"The metaverse, a digital realm that promises to blend virtual and real-world experiences, has ignited a revolution in the world of 3D asset modeling. As virtual environments become more immersive and interactive, the demand for realistic 3D models has never been higher. Traditionally, 3D modeling has been a labor-intensive process, involving complex design, meticulous refinement, and extensive client communication.

&#x200B;

link: [https://medium.com/@andysingal/revolutionizing-3d-asset-modeling-in-the-metaverse-era-with-3d-gpt-e8bab53ec13b](https://medium.com/@andysingal/revolutionizing-3d-asset-modeling-in-the-metaverse-era-with-3d-gpt-e8bab53ec13b) "
228,deeplearning,gpt,relevance,2023-06-11 15:56:18,ChatGPT interrogating bugs and errors!!,Available-Bass-7575,False,0.58,2,146xgyf,https://youtu.be/zfIyIScu0oo,1,1686498978.0,
229,deeplearning,gpt,relevance,2023-01-28 06:34:28,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,1.0,18,10n8c80,https://www.reddit.com/r/deeplearning/comments/10n8c80/a_python_module_to_generate_optimized_prompts/,0,1674887668.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features üöÄ

* üßô‚Äç‚ôÄÔ∏è NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* üî® Easily add one-shot, two-shot, or few-shot examples to the prompt
* ‚úå Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* üí• Custom examples and samples can be easily added to the prompt
* üí∞ Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Example](https://preview.redd.it/sjvhtd8b3nea1.png?width=1236&format=png&auto=webp&s=e9a3a28f41f59cd25fe8e95bd1fca56b15f27a6e)

&#x200B;

https://preview.redd.it/fnb05bys3nea1.png?width=1398&format=png&auto=webp&s=096f4e2cbd0a71e795f30cc5e3720316b5e5caf6"
230,deeplearning,gpt,relevance,2022-12-10 22:44:46,InstructGPT,MRMohebian,False,0.8,6,zi62fr,https://www.reddit.com/r/deeplearning/comments/zi62fr/instructgpt/,1,1670712286.0,"Recently, ChatGPT became trendy. It was adopting an algorithm by the name of InstructGPT. 
We go through the paper ""Training Language Models to Follow Instructions with Human Feedback"" in this video and go into extensive detail about how InstructGPT works. 

Please subscribe, leave a comment and share with your friends.

[R]
https://youtu.be/lYRWzCPGM2Q"
231,deeplearning,gpt,relevance,2023-08-05 21:29:02,Auto-GPT (or other) based Web Crawler/Scraper,venuv,False,0.67,1,15j7ait,https://www.reddit.com/r/deeplearning/comments/15j7ait/autogpt_or_other_based_web_crawlerscraper/,1,1691270942.0,"Are there any AI utilities (GenAI or 'vanilla' Deep Learning ha) that take a URL as input, fire up your browser, set it to full screen mode, take a snapshot of it, identify the part of the screen that is the 'body' of the browser, OCR the results and return?

I figure someone in the Auto-GPT/Gorilla/other GPT with API/ world has thought of this. Has anyone acted on it? Any pointers? It doesn't have to be perfect, just a time saver."
232,deeplearning,gpt,relevance,2023-11-15 10:30:36,"GPT-4 Turbo: Die Zukunft der K√ºnstlichen Intelligenz, entwickelt von OpenAI",Webglobic_tech,False,0.82,14,17vqtlk,https://webglobic.com/magazine/,0,1700044236.0,
233,deeplearning,gpt,relevance,2023-04-19 13:51:18,Alpaca Electron: ChatGPT Locally!,oridnary_artist,False,0.69,6,12rtzak,https://youtu.be/0oz3RaLlTlM,0,1681912278.0,
234,deeplearning,gpt,relevance,2023-08-25 13:21:12,AI Meets AI: A Conversation Between GPT-4 and Google's Bard,Ubica123,False,0.82,36,160z5pp,https://www.youtube.com/watch?v=3H45IncZ7gs,3,1692969672.0,
235,deeplearning,gpt,relevance,2023-06-18 03:01:59,Did Microsoft take down the BioGPT?,Excellent-Screen-836,False,0.76,4,14c9c8b,https://www.reddit.com/r/deeplearning/comments/14c9c8b/did_microsoft_take_down_the_biogpt/,1,1687057319.0,"I am trying to download the BioGPT model and I get the public access denied error. Does anyone have an idea? It was working fine till a couple of weeks ago. 

https://preview.redd.it/a0araikgzo6b1.png?width=720&format=png&auto=webp&s=b6b9451a9443e63ea76cd3073da8c72d485978e5"
236,deeplearning,gpt,relevance,2023-10-11 16:40:27,Is it possible to train a GPT-2 model on free google colab?,JastorJ,False,1.0,13,175ikm3,https://www.reddit.com/r/deeplearning/comments/175ikm3/is_it_possible_to_train_a_gpt2_model_on_free/,6,1697042427.0, My course has an assignment where we have to implement a research paper  and I was thinking about implementing GPT2 model but I am worried that  it could take enormous resources to train it properly. Is it possible to  train it on google colab using a small amount of text data to get good  results from it.  I don't have access to GPU so I have to use colab.
237,deeplearning,gpt,relevance,2023-10-26 14:51:06,5 Game-Changing Applications of GPT-4: No Coding Skills Required!,OnlyProggingForFun,False,0.25,0,17gy9w8,https://youtu.be/lwNy4lgDpjY,0,1698331866.0,
238,deeplearning,gpt,relevance,2023-03-29 14:13:46,AI Startup Cerebras releases open source ChatGPT-like alternative models,Time_Key8052,False,0.95,46,125pbbf,https://gpt4chatgpt.tistory.com/entry/Cerebras-releases-open-source-ChatGPT-like-alternative-models,14,1680099226.0,
239,deeplearning,gpt,relevance,2023-08-27 15:18:38,MetaGPT: Redefining Multi-Agent Collaboration for Complex Tasks.,OnlyProggingForFun,False,0.6,1,162t4o7,https://youtu.be/YtxMderNrzU,0,1693149518.0,
240,deeplearning,gpt,relevance,2023-03-09 09:27:13,ChatGPT vs Other Chatbots!!,Genius_feed,False,0.23,0,11mnvcr,https://i.redd.it/7eqx02homoma1.png,2,1678354033.0,
241,deeplearning,gpt,relevance,2022-12-02 01:35:02,GPT-3 Generated Rap Battle between Yann LeCun & Gary Marcus,hayAbhay,False,0.99,139,za73dc,https://i.redd.it/ybfcfvez1e3a1.png,16,1669944902.0,
242,deeplearning,gpt,relevance,2023-04-25 23:50:32,Research on the political biases of ChatGPT,Mysterious_Potato132,False,0.87,6,12z08ni,/r/ChatGPT/comments/12yz49i/research_on_the_political_biases_of_chatgpt/,6,1682466632.0,
243,deeplearning,gpt,relevance,2023-12-22 21:52:34,NeuralFlash - a flashcard-making GPT specializing in AI to help you study.,MachineScholar,False,0.67,1,18opxcs,https://www.reddit.com/r/deeplearning/comments/18opxcs/neuralflash_a_flashcardmaking_gpt_specializing_in/,0,1703281954.0,"Hey everyone. I'm a computer science student and I've been searching for the most efficient way to study ML concepts via Quizlet flashcards so I came up with a ""pipeline"" by making this custom GPT and feeding it my Markdown notes. Here's a little guide:

1. Take lecture/book notes in Markdown (I use obsidian to do this since it's free, fast, and open source)
2. Open up NeuralFlash and choose the ""Generate flashcards from my AI notes"" action.
3. Copy your entire Markdown note, paste it into NeuralFlash.
4. Copy the csv it outputs and paste it into the ""import"" area of your Quizlet flashcard set (make sure you select comma instead of tab).
5. Learn and succeed.

**Here the link to the GPT:** [**https://chat.openai.com/g/g-m4nFBaKA8-neuralflash**](https://chat.openai.com/g/g-m4nFBaKA8-neuralflash)"
244,deeplearning,gpt,relevance,2023-04-08 07:55:07,need help. GPT-3.5 can't solve it.,ryanultralifeio,False,0.23,0,12ff87f,https://www.reddit.com/r/deeplearning/comments/12ff87f/need_help_gpt35_cant_solve_it/,8,1680940507.0,"Trying to make a schedule for the league, here are the constraints.  I think it should be tailormade for AI.


Schedule May 2023 Games.

ÔøºÔøº

I need you to schedule games between 7 teams, on 4 fields, beginning on Monday May 1st for the whole month of May 2023. 

The 4 fields are; Quincy, Portola, Chester and Loyalton. Fields in Quincy, Portola and Loyalton are available beginning May 1st. The field in Chester is available beginning May 8th.

 Saturdays can have 3 games per day at either 10am, 1pm, or 4 pm. 

No games on Sunday. 

Monday, Tuesday, Wednesday, Thursday, and Friday games are at 5:00. 

Mondays, Tuesdays, Wednesdays, Thursdays, and Fridays can have games played on 3 different fields at the same time. 

There are 7 teams. Quincy Red, Quincy Blue, Quincy Grey, Portola Padres, Portola Dodgers, Chester Giants and Loyalton. 

All teams can only play each other 2 times in May with the exceptions of Quincy Grey and Quincy Red, Quincy Grey and Quincy Blue, and Quincy Grey and Chester Giants, who can only play each other 1 time in May. 

Only Loyalton cannot play on May 3,4, or 5 for Sierra Nevada Journeys. 

All teams are unavailable to play May 26,27,29 for Memorial Day Weekend. 

All teams are unavailable to play May 17,18,19 for 6th grade field trip. 

Each team will play one home game against each other, except for the teams only playing one game. 

Quincy Blue only plays home games on Quincy field on Mondays, and Thursdays. 

Quincy Grey only plays home games on Quincy field on Wednesdays, and Fridays. 

Quincy Red only plays home games on Quincy Field on Tuesdays, Thursdays, and Fridays. 

Loyalton only plays home games on Loyalton field. 

Chester Giants only play Home Games on Chester field. 

Portola Padres only play home games on Portola field. 

Portola Dodgers only play home games on Portola field. 

Each team can play a maximum of two games per week. 

A team cannot play without two calenders days between games. 

A team cannot play two games on consecutive days.

A team cannot play two games on the same day. 

Teams must have at least 9 games.

Put the total number of games played per team at the bottom of the whole months schedule.

2+ hours a no good results........."
245,deeplearning,gpt,relevance,2023-03-11 00:38:10,Generate READMEs Using ChatGPT,tomd_96,False,0.91,8,11o5zyl,https://www.reddit.com/r/deeplearning/comments/11o5zyl/generate_readmes_using_chatgpt/,0,1678495090.0,"&#x200B;

https://i.redd.it/k375our2a0na1.gif

&#x200B;

You can use this program I wrote to generate readmes: [https://github.com/tom-doerr/codex-readme](https://github.com/tom-doerr/codex-readme)

&#x200B;

It's far from perfect, but I now added ChatGPT and it is surprisingly good at inferring what the project is about. It often generates interesting usage examples and explains the available command line options.

&#x200B;

You probably won't yet use this for larger projects, but I think this can make sense for small projects or single scripts. Many small scripts are very useful but might never be published because of the work that is required to document and explain it. Using this AI might assist you with that.

&#x200B;

Reportedly GPT-4 is coming out next week, which probably would make it even better.

&#x200B;

What do you think?"
246,deeplearning,gpt,relevance,2023-12-13 10:06:50,Durchbruch in der KI mit Gemini: ChatGPT 4.0 in Benchmark-Tests √ºbertroffen!,Webglobic_tech,False,0.25,0,18hdkrs,https://webglobic.com/2023/12/12/gemini-uebertrifft-chatgpt4-0-in-benchmark-tests-ein-neuer-meilenstein-in-der-ki-entwicklung/,0,1702462010.0,
247,deeplearning,gpt,relevance,2023-12-12 12:10:06,[D] Crafting Visually Stunning Slides with Assistants API (GPT-4) and DALL¬∑E-3,Fit_Maintenance_2455,False,0.5,0,18gkhbu,https://www.reddit.com/r/deeplearning/comments/18gkhbu/d_crafting_visually_stunning_slides_with/,0,1702383006.0,"In the realm of presentations, creating visually compelling slides that effectively communicate data insights is a skill coveted by professionals across diverse industries. However, the traditional process of manually constructing these slides can be a time-consuming endeavor. Enter the new Assistants API (GPT-4) and DALL¬∑E-3, revolutionary tools that streamline the slide creation process, offering efficiency and visual finesse.

Crafting slides that capture the essence of complex data sets while maintaining audience engagement is a multifaceted challenge. It demands a blend of data interpretation, storytelling finesse, and an eye for design. Traditionally, this process involves laborious manual work, from structuring information to selecting images and formatting layouts.

&#x200B;

link: [https://medium.com/ai-advances/crafting-visually-stunning-slides-with-assistants-api-gpt-4-and-dall-e-3-f862368cec44](https://medium.com/ai-advances/crafting-visually-stunning-slides-with-assistants-api-gpt-4-and-dall-e-3-f862368cec44) "
248,deeplearning,gpt,relevance,2023-05-18 08:41:54,Tutorial to improve GPT throughput 16 times with dynamic batching,Greedy-Cupcake-3694,False,0.93,26,13ksxc8,https://www.reddit.com/r/deeplearning/comments/13ksxc8/tutorial_to_improve_gpt_throughput_16_times_with/,6,1684399314.0,"I wrote a tutorial to improve GPT completion throughput with dynamic batching [https://microsoft.github.io/batch-inference/examples/gpt\_completion.html](https://microsoft.github.io/batch-inference/examples/gpt_completion.html). And I can achieve 16 times throughput on V100 comparing to baseline. We built a python dynamic batching library so you can apply it on your own models easily [https://github.com/microsoft/batch-inference](https://github.com/microsoft/batch-inference).  


Although the tutorial we built for GPT shows promising result on throughput, it doesn't use complex decoding algorithms like top-p or beam search, and we are aware of more advanced batching algorithms for GPT completion. So we're considering building a GPT specific inference library for production use, hope we get enough resource to do it in future"
249,deeplearning,gpt,relevance,2023-02-20 21:27:58,Fine tuning a GPT for text generation,nashcaps2724,False,0.84,8,117l2vf,https://www.reddit.com/r/deeplearning/comments/117l2vf/fine_tuning_a_gpt_for_text_generation/,6,1676928478.0,"Hi all, let me lay out my problem‚Ä¶

Imagine there are two corpora, Corpus A (100,000~) and Corpus B (20,000,000~). 

Individuals create reports for corpus A based on the information in corpus B. 

My idea was to pretrain a GPT on corpus A, and fine tune it to take documents from corpus B as an input, and output text in the style of corpus A (essentially a mix of text generation and summarization). 

Is this something folks think is even feasible? Should I be pretaining the GPT on both corpora or just corpus A? I thought of both fine tuning an OpenAI GPT and training from scratch. 

Any advice would be welcome!"
250,deeplearning,gpt,relevance,2023-09-26 18:24:05,"OpenAI‚Äôs GPT-4 with vision still has flaws, paper reveals",Nalix01,False,0.33,0,16svoeg,https://www.reddit.com/r/deeplearning/comments/16svoeg/openais_gpt4_with_vision_still_has_flaws_paper/,1,1695752645.0,"OpenAI initially promoted GPT-4's ability to analyze and interpret images alongside text, but has since limited these features due to concerns about misuse and privacy. A recent paper sheds light on the efforts to mitigate these issues and the ongoing challenges GPT-4 faces in interpreting images accurately and responsibly.

If you want to stay ahead of the curve in AI and tech,¬†[look here first](https://dupple.com/techpresso?utm_source=reddit&utm_medium=social&utm_campaign=post).

**Image Analysis Concerns**

* **Abuse and Privacy Issues:** OpenAI limited GPT-4's image features due to potential misuse and privacy violations.
* **Mitigation Efforts:** The company is working on safeguards to prevent malicious use and bias in GPT-4‚Äôs image analysis.

**Performance Issues**

* **Inaccurate Inferences:** GPT-4V can make incorrect inferences, combining text strings wrongly and missing details.
* **Identification Issues:** Struggles with identifying dangerous substances or chemicals and gives wrong medical imaging responses.

**Discrimination and Bias**

* **Misunderstood Symbols:** GPT-4V doesn't grasp the nuances of certain hate symbols.
* **Discrimination:** Shows bias against certain sexes and body types, relating responses mainly to body weight and body positivity.

[Source (Tech Crunch)](https://techcrunch.com/2023/09/26/openais-gpt-4-with-vision-still-has-flaws-paper-reveals/#:~:text=The%20paper%20reveals%20that%20GPT,facts%20in%20an%20authoritative%20tone)

**PS:** **If you enjoyed this post**, you‚Äôll love my¬†[ML-powered newsletter](https://dupple.com/techpresso?utm_source=reddit&utm_medium=social&utm_campaign=post)¬†that summarizes the best AI/tech news from¬†50+ media. It‚Äôs already being read by¬†**7,000+** **professionals** from¬†**OpenAI, Google, Meta**‚Ä¶"
251,deeplearning,gpt,relevance,2023-12-15 04:50:13,OpenAI's Next Move: ChatGPT 4.5 Upgrade in the Works? Sam Altman Clarifies,Damanjain,False,0.13,0,18is4sm,https://thebuzz.news/article/openai-chatgpt-4-5-leak/11787/,1,1702615813.0,
252,deeplearning,gpt,relevance,2023-08-28 07:12:44,OpenAI introduces fine-tuning capabilities for GPT-3.5 Turbo,intengineering,False,1.0,4,163f3fp,https://interestingengineering.com/innovation/openai-introduces-fine-tuning-capabilities-for-gpt-35-turbo,1,1693206764.0,
253,deeplearning,gpt,relevance,2023-06-28 15:07:16,Measure the utility and quality of GPT-generated text,Synthesize2023,False,0.86,5,14lc04p,https://gretel.ai/blog/measure-the-utility-and-quality-of-gpt-generated-text-using-gretels-new-text-report,0,1687964836.0,
254,deeplearning,gpt,relevance,2023-02-01 09:00:18,"Python wrapper of OpenAI's New AI classifier tool, which detects whether the paragraph was generated by ChatGPT, GPT models, or written by humans",StoicBatman,False,0.92,21,10qouv9,https://www.reddit.com/r/deeplearning/comments/10qouv9/python_wrapper_of_openais_new_ai_classifier_tool/,3,1675242018.0,"OpenAI¬†has developed a new AI classifier tool which detects whether the content (paragraph, code, etc.) was generated by #ChatGPT, #GPT-based large language models or written by humans.

Here is a python wrapper of openai model to detect if a text is written by humans or generated by ChatGPT, GPT models

Github:¬†[https://github.com/promptslab/openai-detector](https://github.com/promptslab/openai-detector)  
Openai release:¬†[https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/](https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/)

If you are interested in #PromptEngineering, #LLMs, #ChatGPT and other latest research discussions, please consider joining our discord [discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)  


https://preview.redd.it/9d8ooeg2ljfa1.png?width=1358&format=png&auto=webp&s=0de7ccc5cd16d6bbad3dcfe7d8441547a6196fc8"
255,deeplearning,gpt,relevance,2023-06-01 14:21:15,How Does ChatGPT Learn: Reinforcement Learning Explained,OnlyProggingForFun,False,0.25,0,13xiv73,https://youtu.be/lWK9T56t-YM,0,1685629275.0,
256,deeplearning,gpt,relevance,2023-12-31 22:37:16,[P] Ported nanoGPT to Apple's new MLX framework: Early Results on Macbook M3 Pro GPU,brownmamba94,False,1.0,1,18vixnd,/r/MachineLearning/comments/18vhvl1/p_ported_nanogpt_to_apples_new_mlx_framework/,0,1704062236.0,
257,deeplearning,gpt,relevance,2023-02-11 06:59:00,‚≠ï New Open-Source Version Of ChatGPT,LesleyFair,False,0.87,39,10zepkt,https://www.reddit.com/r/deeplearning/comments/10zepkt/new_opensource_version_of_chatgpt/,3,1676098740.0,"GPT is getting competition from open-source.

A group of researchers, around the YouTuber [Yannic Kilcher](https://www.ykilcher.com/), have announced that they are working on [Open Assistant](https://github.com/LAION-AI/Open-Assistant). The goal is to produce a chat-based language model that is much smaller than GPT-3 while maintaining similar performance.

If you want to support them, they are crowd-sourcing training data [here](https://open-assistant.io/).

**What Does This Mean?**

Current language models are too big.

They require millions of dollars of hardware to train and use. Hence, access to this technology is limited to big organizations. Smaller firms and universities are effectively shut out from the developments.

Shrinking and open-sourcing models will facilitate academic research and niche applications.

Projects such as Open Assistant will help to make language models a commodity. Lowering the barrier to entry will increase access and accelerate innovation.

What an exciting time to be alive! 

Thank you for reading! I really enjoyed making this for you!  
The Decoding ‚≠ï is a thoughtful weekly 5-minute email that keeps you in the loop about machine research and the data economy. [Click here to sign up](https://thedecoding.net/)!"
258,deeplearning,gpt,relevance,2023-04-16 04:52:51,"BERT Explorer - Analyzing the ""T"" of GPT",msahmad,False,0.88,17,12nvtm3,https://www.reddit.com/r/deeplearning/comments/12nvtm3/bert_explorer_analyzing_the_t_of_gpt/,0,1681620771.0,"If you want to dig deeper into NLP, LLM, Generative AI, you might consider starting with a model like BERT. This tool helps in exploring the inner working of Transformer-based model like BERT. It helped me understands some key concepts like word embedding, self-attention, multi-head attention, encoder, masked-language model, etc. Give it a try and explore BERT in a different way.

BERT == Bidirectional Encoder Representations from TransformersGPT == Generative Pre-trained Transformer

They both use the Transformer model, but BERT is relatively simpler because it only uses the encoder part of the Transformer.

BERT Explorer[https://www.101ai.net/text/bert](https://www.101ai.net/text/bert)

https://i.redd.it/bxxboyyuhaua1.gif"
259,deeplearning,gpt,relevance,2023-08-19 05:39:27,NanoChatGPT - turning nanogpt into a chat model/LLM,vatsadev,False,0.67,1,15v7gvl,https://github.com/VatsaDev/nanoChatGPT,0,1692423567.0,
260,deeplearning,gpt,relevance,2023-11-15 20:27:19,"Nvidia introduces the H200, an AI-crunching monster GPU that may speed up ChatGPT",keghn,False,0.81,9,17w2vgj,https://arstechnica.com/information-technology/2023/11/nvidia-introduces-its-most-powerful-gpu-yet-designed-for-accelerating-ai/,0,1700080039.0,
261,deeplearning,gpt,relevance,2023-11-16 08:28:39,Elon Musk's xAI Unveils Grok: The New AI Challenger to OpenAI's ChatGPT,Webglobic_tech,False,1.0,1,17whz4d,https://v.redd.it/qx68wbuf7o0c1,0,1700123319.0,
262,deeplearning,gpt,relevance,2023-02-06 00:07:43,ChatGPT: From Nowhere to Knowledgeable,crawfa,False,0.61,3,10urypd,https://www.reddit.com/r/deeplearning/comments/10urypd/chatgpt_from_nowhere_to_knowledgeable/,0,1675642063.0,"ChatGPT is taking the world by storm, and is now the fastest growing software application ever, eclipsing TikTok, which may soon be the fastest shrinking software application ever if it gets banned in the US. This article explains at a high level what ChatGPT is, how it works at a high level, what you can do with it, as well as some developer choices they have made and identifies some things that ChatGPT does not do well.

[https://open.substack.com/pub/stayblog/p/chatgpt-from-nowhere-to-knowledgeable?r=1qxkwn&utm\_campaign=post&utm\_medium=web](https://open.substack.com/pub/stayblog/p/chatgpt-from-nowhere-to-knowledgeable?r=1qxkwn&utm_campaign=post&utm_medium=web)"
263,deeplearning,gpt,relevance,2023-11-06 02:57:04,"If a conversation is not deleted, can ChatGPT-4 continuously learn and maintain the conversation state?",Turbulent_Dot_5216,False,0.57,1,17ot4d4,https://www.reddit.com/r/deeplearning/comments/17ot4d4/if_a_conversation_is_not_deleted_can_chatgpt4/,6,1699239424.0," As a beginner, I have a question for everyone: Does ChatGPT-4 forget the context if the conversation is closed or left idle for a long period, meaning it can't maintain the state of the conversation? I want ChatGPT-4 to learn legal knowledge, and in one conversation, provide it with a vast amount of legal material over a long period. Can ChatGPT-4 remember the previous legal material every time I open it, i.e., maintain the conversation state? If not, how can I make ChatGPT-4 remember previous conversations? 

 Additionally, if I do not delete a conversation and continuously feed ChatGPT-4 a large amount of legal information within that same conversation, can ChatGPT-4 achieve self-learning? That is, can it become increasingly proficient in legal matters, or regardless of how much information I provide, will ChatGPT-4 not improve? "
264,deeplearning,gpt,relevance,2023-04-10 08:13:15,Summarize documents with ChatGPT via Python scripts,rottoneuro,False,0.5,0,12hbq89,https://levelup.gitconnected.com/summarize-documents-with-chatgpt-a43456841cc4,1,1681114395.0,
265,deeplearning,gpt,relevance,2023-11-06 05:28:48,"I want to create a continuously improving legal AI. My idea is to constantly feed ChatGPT-4 legal knowledge so that it keeps learning. Is this possible? If it can't be done with ChatGPT-4, is there another way to achieve this?",Turbulent_Dot_5216,False,0.27,0,17ovpmz,https://www.reddit.com/r/deeplearning/comments/17ovpmz/i_want_to_create_a_continuously_improving_legal/,3,1699248528.0," I want to create a continuously improving legal AI. My idea is to constantly feed ChatGPT-4 legal knowledge so that it keeps learning. Is this possible? If it can't be done with ChatGPT-4, is there another way to achieve this? "
266,deeplearning,gpt,relevance,2023-07-22 18:37:01,"LLAMA 2 - Model Explained, Demo and Comparison to ChatGPT",Combination-Fun,False,0.8,3,156roiv,https://www.reddit.com/r/deeplearning/comments/156roiv/llama_2_model_explained_demo_and_comparison_to/,0,1690051021.0,"LLAMA 2 is the largest and best opensource LLM every released free for commercial use. There have been several improvements that make LLAMA 2 better than LLAMA 1. Here is a video explaining the LLAMA 2 Model, a quick Demo of the model along with a Comparison to ChatGPT:

[https://youtu.be/TiloR3qRogs](https://youtu.be/TiloR3qRogs)

Hope its useful!"
267,deeplearning,gpt,relevance,2023-06-14 07:24:34,ZBrain- Build ChatGPT-like apps with your private data,rituraj2406,False,1.0,7,1491d52,https://www.reddit.com/r/deeplearning/comments/1491d52/zbrain_build_chatgptlike_apps_with_your_private/,0,1686727474.0," Hello Community,

We at ZBrain have built a platform to create ChatGPT-like apps with your private data, You can import your data from multiple sources and DBs and integrate the app into any of your workflows.

We have also added AI risk governance to mitigate the confidential data leak and now working on Flow a no-code tool to give you the freedom to create your own business logic.

You can try the tool now at [https://zbrain.ai/](https://zbrain.ai/). We would love to hear your thoughts and feedback to improve the tool."
268,deeplearning,gpt,relevance,2023-04-16 18:03:39,ChatGPT Math Problem Challenge! (AAAI-MAKE 2023),Neurosymbolic,False,0.33,0,12oj6bi,https://youtube.com/watch?v=iRhbOE9U_Tk&feature=share,0,1681668219.0,
269,deeplearning,gpt,relevance,2023-03-01 05:52:05,Career pivot for a SWE considering chatGPT disruption?,Which-Distance1384,False,0.6,1,11evrik,https://www.reddit.com/r/deeplearning/comments/11evrik/career_pivot_for_a_swe_considering_chatgpt/,5,1677649925.0,"tl;dr : what to do to board GPT ship?

&#x200B;

I really dont think ChatGPT will replace jobs very soon. Not until many software and apps are developed to replace workers, e.g., something that can do HR job, read internal docs and summarize, code from a design, etc etc. And all that needs experts. Given the field is pretty new and there are not many experts in this field, at least for the scale needed, I think the process of job loss takes some time. For near future probably we all SWEs can enjoy the more efficient coding, documentation readintg, etc. But for sure in the future there will be changes.

&#x200B;

I have been bored with my job as a SWE for a while. I have worked mostly in Cloud all my career (AWS and GCP) and always wanted to try something new.

&#x200B;

I find ChatGPT disruptive and interesting and want to be a part of it. I wonder what is the best way for me to join Large Language Model efforts? Which ways have best ROI?

\- Going to a PhD program and becoming a researcher?

\- Getting an MSC and become some sort of NLP AI engineer?

\- Finding internal teams that are willing to give me a try on their research/product?

&#x200B;

&#x200B;"
270,deeplearning,gpt,relevance,2023-09-02 17:47:32,LLaVA: Bridging the Gap Between Visual and Language AI with GPT-4,OnlyProggingForFun,False,0.6,1,1688v3c,https://youtu.be/Pn1B_L_zAwI,1,1693676852.0,
271,deeplearning,gpt,relevance,2023-02-03 19:38:56,GPT-2 small model (124M params) hw requirements,honzovy-klipy,False,1.0,9,10st418,https://www.reddit.com/r/deeplearning/comments/10st418/gpt2_small_model_124m_params_hw_requirements/,1,1675453136.0,"Hey, I was wandering how much VRAM and RAM do I need for running (inference only) gpt2-small model from hugging face, but was not able to find anything. Can somebody help please?"
272,deeplearning,gpt,relevance,2023-04-01 17:50:06,Fine-tune GPT on sketch data (stroke-3),mellamo_maria,False,1.0,1,128tfvc,https://www.reddit.com/r/deeplearning/comments/128tfvc/finetune_gpt_on_sketch_data_stroke3/,0,1680371406.0," These past days I have started a personal project where I would like to build a model that, given an uncompleted sketch, it can finish it. I was planning on using some pretrained models that are available in HuggingFace and fine-tune them with my sketch data for my task. The sketch data I have is in stoke-3 format, like the following example:  
\[  
\[10, 20, 1\],  
\[20, 30, 1\],  
\[30, 40, 1\],  
\[40, 50, 0\],  
\[50, 60, 1\],  
\[60, 70, 0\]  
\]  
The first value of each triple is the X-coordinate, the second value the Y-coordinate and the last value is a binary value indicating whether the pen is down (1) or up (0). I was wondering if you guys could give me some instruction/tips about how should I approach this problem? How should I prepare/preprocess the data so I can fit it into the pre-trained models like BERT, GPT, etc. Since it's stroke-3 data and not text or a sequence of numbers, I don't really know how should I treat/process the data.

Thanks a lot! :)"
273,deeplearning,gpt,relevance,2023-08-09 16:05:13,Atai Barkai on PodcastGPT by TawkitAI - Weaviate Podcast #62!,CShorten,False,1.0,1,15mjhan,https://www.reddit.com/r/deeplearning/comments/15mjhan/atai_barkai_on_podcastgpt_by_tawkitai_weaviate/,0,1691597113.0,"I am SUPER excited to publish Weaviate Podcast #62 with Atai Barkai, discussing PodcastGPT by tawkitAI!

I love thinking about the future of podcasting, particularly with the use of Whisper, Vector Databases, LLMs, and Retrieval-Augmented Generation (RAG)! Atai and I discussed a ton of interesting ideas on the podcast, such as the future of the podcast listening experience, strategies for clip extraction and recommendation, and many interesting emerging ideas like ChatArena, Tree-of-Thoughts, and of course, LLM Fine-Tuning and RAG!

I hope you enjoy the podcast! As always I am more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

[https://www.youtube.com/watch?v=DgDbtuGugqA](https://www.youtube.com/watch?v=DgDbtuGugqA&fbclid=IwAR3-D6j2mxOmSel5r_xveY4EqbwtTfp1XWWeLaTnfBPRQNliIDcScb8bBRA)"
274,deeplearning,gpt,relevance,2023-08-05 17:09:44,The Quest to Have Endless Conversations with Llama and ChatGPT üó£Ô∏èüí¨,JClub,False,0.67,2,15j1117,https://medium.com/@joaolages/the-quest-to-have-endless-conversations-with-llama-and-chatgpt-%EF%B8%8F-81360b9b34b2,0,1691255384.0,
275,deeplearning,gpt,relevance,2023-01-19 07:55:49,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.9,71,10fw22o,https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/,11,1674114949.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=webp&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ‚≠ï, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
276,deeplearning,gpt,relevance,2023-02-05 16:44:56,Beat GPT-3 which has unlimited money using Open Source community,koyo4ever,False,0.77,19,10ugxmc,https://www.reddit.com/r/deeplearning/comments/10ugxmc/beat_gpt3_which_has_unlimited_money_using_open/,8,1675615496.0,"Is it technically possible to train some model using a lot of personal computers like a cluster.

Eg: an Algorithm to train tiny parts of some model using personal computer of volunteers. Like a community that makes your gpu capacity available, even if it's little.

The idea is train tiny parts of a model, with a lot of volunteers, then bring it together to make some powerful deepmind.

Can this model beat a lot of money spent in models like GPT-3?"
277,deeplearning,gpt,relevance,2022-10-24 19:29:56,Huggingface GPT Seq2Seq input format,_Arsenie_Boca_,False,1.0,8,yck9wp,https://www.reddit.com/r/deeplearning/comments/yck9wp/huggingface_gpt_seq2seq_input_format/,0,1666639796.0,"I want to finetune a GPT-like model on a Seq2Seq task. Even though I have been using Huggingface for quite some time, I am very confused about how to pass the input to the model.

Some examples simply concatenate prompt and target and pass them via `input_ids`. But how are you supposed to tell the model where the prompt ends and the target begins? 

The 2 most intuitive candidates from reading the docs seem to be `token_type_ids` and `labels`, but the docs dont seem to be explicit about that.

Any help appreciated! This is probably an easy one for everyone that has done Seq2Seq with HF before :)"
278,deeplearning,gpt,relevance,2023-07-24 17:54:54,AI Digests: GPT-4 generated Newsletter on ArXiv Deep Learning Papers,CommercialLynx7233,False,1.0,1,158hu6c,https://www.reddit.com/r/deeplearning/comments/158hu6c/ai_digests_gpt4_generated_newsletter_on_arxiv/,0,1690221294.0,"Hey y'all,

I built a quick site called [AI Digests](https://aidigest.dev/), that uses GPT-4 to generate a newsletter summarizing the key themes/concepts discussed, in ArXiv Deep Learning (cs.LG) papers, on a daily basis. Here is last Friday's Edition: [https://aidigest.dev/edition/2023-07-22](https://aidigest.dev/edition/2023-07-22)

If you are interested, please do subscribe by submitting your email! Let me know what you guys think!"
279,deeplearning,gpt,relevance,2024-02-13 14:38:25,"This is my foundation block or sandwich stack AI model... Criticisms and inputs are welcome, formulated by openAI scholarGPT - thanks in advance.",AskACapperDOTcom,False,0.25,0,1apuyv4,/r/OpenAI/comments/1apf8jy/this_is_my_foundation_block_or_sandwich_stack_ai/,10,1707835105.0,
280,deeplearning,gpt,relevance,2023-04-30 03:53:26,Why do neural networks like ChatGPT use memory and processing power while at rest?,Will_Tomos_Edwards,False,0.38,0,133f4m4,https://www.reddit.com/r/deeplearning/comments/133f4m4/why_do_neural_networks_like_chatgpt_use_memory/,13,1682826806.0,"It is surprising to me that the default behavior of many neural networks is to run processes and use resources (RAM, CPU or their equivalent) . Why are they not just stored in memory by default? Obviously they must take up a lot of memory, but I don't understand why the default behaviour is to be active."
281,deeplearning,gpt,relevance,2022-12-06 01:38:42,ChatGPT explained in 5 minutes,OnlyProggingForFun,False,0.43,0,zdr6l7,https://youtu.be/AsFgn8vU-tQ,0,1670290722.0,
282,deeplearning,gpt,relevance,2023-03-21 02:06:28,CoDev- A GPT 4.0 Virtual Developer To Generate Apps,aisaint,False,0.71,7,11x3p2u,https://www.reddit.com/r/deeplearning/comments/11x3p2u/codev_a_gpt_40_virtual_developer_to_generate_apps/,5,1679364388.0,"&#x200B;

&#x200B;

CoDev is a GPT 4.0 virtual developer prompt to help you create and refine boilerplates/apps. You can get the prompt from my GitHub link below, paste it in a new Chat session, and issue the commands (see How To Use CoDev). In this article, we will use CoDev to create a React/Typescript/MUI dashboard boiler plate

[https://medium.com/@etherlegend/codev-a-gpt-4-0-virtual-developer-to-build-app-boilerplates-34a431e779c7](https://medium.com/@etherlegend/codev-a-gpt-4-0-virtual-developer-to-build-app-boilerplates-34a431e779c7)"
283,deeplearning,gpt,relevance,2023-04-11 20:07:47,What‚Äôs the difference between AutoGPT and BabyAGI?,naed900,False,0.13,0,12ivvad,https://www.reddit.com/r/deeplearning/comments/12ivvad/whats_the_difference_between_autogpt_and_babyagi/,2,1681243667.0,"Read tons of stuff about this, but still can‚Äôt see the differences. Help :)?"
284,deeplearning,gpt,relevance,2023-05-03 18:21:05,Has any one used sentence embeddings of chat gpt? [D],hippier579,False,0.67,2,136v5n3,https://www.reddit.com/r/deeplearning/comments/136v5n3/has_any_one_used_sentence_embeddings_of_chat_gpt_d/,0,1683138065.0,
285,deeplearning,gpt,relevance,2023-02-17 12:18:21,"ChatGPT - model, alignment and training explained",Combination-Fun,False,0.67,2,114j09j,/r/ChatGPT/comments/114izlj/chatgpt_model_alignment_and_training_explained/,0,1676636301.0,
286,deeplearning,gpt,relevance,2023-04-05 15:23:45,Lifeline - Arxiv Conversational Search Assistant Demo (using ChatGPT),CommercialLynx7233,False,0.71,3,12cnu4c,https://www.reddit.com/r/deeplearning/comments/12cnu4c/lifeline_arxiv_conversational_search_assistant/,1,1680708225.0,"Hey guys,

I wanted to share a quick side project I built called [Lifeline](https://www.lifeline.dev/). [Lifeline](https://www.lifeline.dev/) is a search assistant on Arxiv Computer Science papers, leveraging ChatGPT. You can use it to find papers on specific topics, get summaries, ask questions about particular CS topics, find datasets or get similar papers. **Essentially, think of it as a conversational assistant that has knowledge about every CS paper published on Arxiv on or after 2022.**

Here are some sample questions: (Here's a [video](https://www.youtube.com/watch?v=VpFRkbKprLE) where I go through some examples)

* Are there any papers examining consciousness in recent AI systems, specifically large language models?
* What is the difference between chain of thought and augmenting language models with API calls?
* Summarize the new GPT-4 model
* Is GPT-4 better than lawyers on the bar exam? (lol...)
* What are some recent approaches for 3D object construction, from natural language?

If you want to contribute or have any questions, email me at: [rahul@lifeline.dev](mailto:rahul@lifeline.dev) .

Thank you!"
287,deeplearning,gpt,relevance,2023-04-09 19:34:12,"ChatGPT for free now , GPT4ALL is now here",oridnary_artist,False,0.75,8,12gt77m,https://www.youtube.com/watch?v=WiCYfi3SUTE&t=1s,0,1681068852.0,
288,deeplearning,gpt,relevance,2023-02-02 20:16:45,1-click deploy for your GPT-3 App,VideoTo,False,0.67,1,10rzn7z,https://www.reddit.com/r/deeplearning/comments/10rzn7z/1click_deploy_for_your_gpt3_app/,0,1675369005.0,"Link - [https://github.com/ClerkieAI/berri\_ai](https://github.com/ClerkieAI/berri_ai)

We  made a package that makes it easy for developers to quickly deploy  their LLM Agent from Google Colab to production (Web App and API  Endpoint).

**How it works?**

Just install the package, import the function, and run deploy.

At the end of the deploy (\~10-15mins), you will get:

1. A web app to interact with your agent üëâ  [https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-4524...](https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-45247-8aqi.zeet-team-ishaan-jaff.zeet.app/)
2. An endpoint you can query üëâ  [https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-4524...](https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-45247-8aqi.zeet-team-ishaan-jaff.zeet.app/langchain_agent?query=%22who) is obama?""

Want a more detailed walkthrough? Check out our loom - [https://www.loom.com/share/fd4375b4a77f4ea7802369cb06a16d43](https://www.loom.com/share/fd4375b4a77f4ea7802369cb06a16d43)

We‚Äôre still early so would love your feedback and opinions. Feel free to try   us out for free ‚Äì and if you need help building an agent / want a   specific integration, just let us know!

https://i.redd.it/s53l400o2ufa1.gif"
289,deeplearning,gpt,relevance,2023-10-25 01:25:27,How we Built an Open-Source RAG-based ChatGPT Web App: Meet Our new AI Tutor!,OnlyProggingForFun,False,0.6,1,17ft66e,https://youtu.be/7ytyK6u3aAk,0,1698197127.0,
290,deeplearning,gpt,relevance,2023-04-29 09:34:51,Connecting assistants to ChatGPT is nuts! JARVIS is ever closer!,Lewenhart87,False,0.35,0,132ogn4,https://v.redd.it/4257us79jswa1,1,1682760891.0,
291,deeplearning,gpt,relevance,2023-03-16 12:48:36,Alpaca - Train Your GPT-4 for Less Than $100,deeplearningperson,False,0.4,0,11st80q,https://youtu.be/6qdzsDSduww,2,1678970916.0,
292,deeplearning,gpt,relevance,2022-12-05 02:22:37,Building A Virtual Machine Inside ChatGPT,x_abyss,False,0.81,3,zctzmf,https://www.engraved.blog/building-a-virtual-machine-inside/,0,1670206957.0,
293,deeplearning,gpt,relevance,2023-09-11 21:06:33,"Meta sets GPT-4 as the bar for its next AI model, says a new report",Nalix01,False,0.5,0,16g7dh3,https://www.reddit.com/r/deeplearning/comments/16g7dh3/meta_sets_gpt4_as_the_bar_for_its_next_ai_model/,2,1694466393.0,"Meta is reportedly planning to train a new model that it hopes will be as powerful as OpenAI‚Äôs GPT-4, by heavily investing in data centers and H100 chips. They hope the AI model will be way more powerful than Llama 2.

If you want to stay ahead of the curve in AI and tech,¬†[look here first](https://dupple.com/techpresso).

**Meta's AI Ambitions**

* **New AI Development**: Meta is working on an AI model, which they hope to be several times more powerful than their recent model, Llama 2.
* **Accelerating Generative AI**: This initiative is spearheaded by a group established by Mark Zuckerberg earlier this year, focusing on AI tools that produce human-like expressions.
* **Expected Timeline**: Meta anticipates the commencement of training for this AI system in early 2024.

**Strategic Positioning in the AI Race**

* **Behind Rivals**: This new model is part of Zuckerberg's strategy to reposition Meta as a leading entity in the AI domain after falling behind competitors.
* **Infrastructure Development**: Meta is investing in data centers and acquiring advanced Nvidia chips (H100s) for AI training.
* **Shift from Microsoft**: While Meta's Llama 2 was integrated with Microsoft's cloud platform, Azure, the new model is intended to be trained on Meta's infrastructure.

**Open-source Approach and Implications**

* **Advocating Open-Source**: Zuckerberg's plan is to make the new AI model open-source, making it freely accessible for companies to build AI-driven tools.
* **Benefits and Risks**: Open-source AI models are favored due to their cost-effectiveness and flexibility. However, they also come with potential downsides, including legal risks and misuse for disseminating false information.
* **Concerns from Experts**: There are raised apprehensions about the unpredictability of the system and its potential vulnerabilities, emphasizing the need for transparency and control.

Sources [(WSJ](https://www.wsj.com/tech/ai/meta-is-developing-a-new-more-powerful-ai-system-as-technology-race-escalates-decf9451) and [TheVerge](https://www.theverge.com/2023/9/10/23867323/meta-new-ai-model-gpt-4-openai-chatbot-google-apple))

**PS:** **If you enjoyed this post**, you‚Äôll love my¬†[ML-powered newsletter](https://dupple.com/techpresso)¬†that summarizes the best AI/tech news from¬†50+ media. It‚Äôs already being read by¬†**6,000+** **professionals** from¬†**OpenAI, Google, Meta**‚Ä¶"
294,deeplearning,gpt,relevance,2022-12-04 20:11:36,5 ChatGPT Tutorial for Total Beginners,dulldata,False,0.4,0,zck620,https://www.youtube.com/watch?v=gMb4iYHaONQ,0,1670184696.0,
295,deeplearning,gpt,relevance,2023-04-21 15:38:04,StableLM: The New Best Open Source Base Models For GPT Apps!,l33thaxman,False,0.86,5,12u89zf,https://www.reddit.com/r/deeplearning/comments/12u89zf/stablelm_the_new_best_open_source_base_models_for/,2,1682091484.0,"Stability AI recently release 3B and 7B of what they are calling StableLM.  If the early metrics are anything to go by these models will be the best models to build from for your generative AI applications. StableLM trains on more data like the LLama models, has the largest open source context window of 4096, and is under a permission license! 

[https://youtu.be/z1sFnzgKw\_Q](https://youtu.be/z1sFnzgKw_Q)"
296,deeplearning,gpt,relevance,2023-02-07 19:42:25,New Weaviate Podcast - Adding ChatGPT to Weaviate!,HenryAILabs,False,1.0,1,10wb0ed,https://www.reddit.com/r/deeplearning/comments/10wb0ed/new_weaviate_podcast_adding_chatgpt_to_weaviate/,0,1675798945.0,"This podcast debuts the Weaviate generate module! The generate module is a new API in Weaviate that facilitates passing data from the Weaviate database to ChatGPT. Here is a snippet from Bob around the 43 minute mark I really enjoyed, describing how this kind of LLM technology is changing the world of database technology, ""Yeah so, what I‚Äôm really excited about and this is something that it‚Äôs just so funny right because if you see it, you have this huge epiphany. I‚Äôve always been thinking of working with these models on input. Right so that they we can solve the problem of not having 100% keyword based search, so that we can have semantic search, image search, and those kind of things. I saw that as this beautiful uniqueness coming from a vector search engine or vector search database. So now what we‚Äôre adding is not only the input in the database but the output. So we‚Äôre basically saying we‚Äôre going to give you relevant information coming from the database, but that‚Äôs not per se stored inside the database. That‚Äôs new! I mean, just think about the most used databases in the world, Postgres, or MySQL, those kind of databases. It only outputs what‚Äôs in there. It makes sense. Because that‚Äôs how you use it. But now what we‚Äôre saying, is that‚Äôs fine you can do that, but also it can give you information, give you data that‚Äôs generated based on a task or prompt that you‚Äôre giving it. Having databases that make sense of it at input and generate new relevant content if that‚Äôs something you want as a user is amazing, and it‚Äôs just getting started. We should do this podcast like a half a year from now again and see how it's evolved because this is just too exciting man."". I really hope you enjoy the podcast, we are more than happy to answer any questions or help you get started with Weaviate!  


[https://www.youtube.com/watch?v=ro3ln4A9N8w](https://www.youtube.com/watch?v=ro3ln4A9N8w)"
297,deeplearning,gpt,relevance,2023-01-19 18:46:25,Fine-tuning GPT Models With Docker and WandB,l33thaxman,False,1.0,1,10g9ntd,https://www.reddit.com/r/deeplearning/comments/10g9ntd/finetuning_gpt_models_with_docker_and_wandb/,0,1674153985.0,"GPT models are very powerful.  What makes them even more powerful is fine-tuning the models on your own data.  However, installing all the needed packages can be a large headache if you want to fine-tune the larger variants.

This video goes over a repo that allows one to use a docker image and wandb to easily fine-tune models without headaches.

[https://youtu.be/usz8JOxgQFs](https://youtu.be/usz8JOxgQFs)"
298,deeplearning,gpt,relevance,2023-03-10 05:22:26,[POC] ChatGPT Audio Bot (like Google Assistant and Alexa) - Opensource,JC1DA,False,0.85,19,11nfrhw,https://www.reddit.com/r/deeplearning/comments/11nfrhw/poc_chatgpt_audio_bot_like_google_assistant_and/,2,1678425746.0,"Hey guys, just wanna share this bot that I quickly built using ChatGPT API.

GitHub page: [https://github.com/LanyTek/ChatGPT\_Audio\_Bot](https://github.com/LanyTek/ChatGPT_Audio_Bot)

This service allows you to keep talking to the bot using your voice like you often do with Google Assistant or Alexa. It can be used in a lot of scenarios like teaching, gossiping, or quickly retrieving information without the need of typing. 

I have a quick video demo here if you wanna check [https://www.youtube.com/watch?v=e9n0BJfMyKw](https://www.youtube.com/watch?v=e9n0BJfMyKw)

It's open source so feel free to use it for anything you like :)"
299,deeplearning,gpt,relevance,2023-03-28 16:41:49,Cerebras Open Sources Seven GPT models and Introduces New Scaling Law,CS-fan-101,False,0.72,3,124uq0t,/r/mlscaling/comments/124t0hz/cerebras_open_sources_seven_gpt_models_and/,0,1680021709.0,
