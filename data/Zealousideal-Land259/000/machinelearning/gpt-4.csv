,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,id,url,num_comments,created,body
0,machinelearning,gpt-4,top,2023-03-15 02:12:42,[D] Anyone else witnessing a panic inside NLP orgs of big tech companies?,thrwsitaway4321,False,0.99,1364,11rizyb,https://www.reddit.com/r/MachineLearning/comments/11rizyb/d_anyone_else_witnessing_a_panic_inside_nlp_orgs/,474,1678846362.0,"I'm in a big tech company working along side a science team for a product you've all probably used. We have these year long initiatives to productionalize ""state of the art NLP models"" that are now completely obsolete in the face of GPT-4. I think at first the science orgs were quiet/in denial. But now it's very obvious we are basically working on worthless technology. And by ""we"", I mean a large organization with scores of teams. 

Anyone else seeing this? What is the long term effect on science careers that get disrupted like this? Whats even more odd is the ego's of some of these science people

Clearly the model is not a catch all, but still"
1,machinelearning,gpt-4,top,2023-03-28 05:57:03,[N] OpenAI may have benchmarked GPT-4’s coding ability on it’s own training data,Balance-,False,0.97,1000,124eyso,https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/,135,1679983023.0,"[GPT-4 and professional benchmarks: the wrong answer to the wrong question](https://aisnakeoil.substack.com/p/gpt-4-and-professional-benchmarks)

*OpenAI may have tested on the training data. Besides, human benchmarks are meaningless for bots.*

 **Problem 1: training data contamination**

To benchmark GPT-4’s coding ability, OpenAI evaluated it on problems from Codeforces, a website that hosts coding competitions. Surprisingly, Horace He pointed out that GPT-4 solved 10/10 pre-2021 problems and 0/10 recent problems in the easy category. The training data cutoff for GPT-4 is September 2021. This strongly suggests that the model is able to memorize solutions from its training set — or at least partly memorize them, enough that it can fill in what it can’t recall.

As further evidence for this hypothesis, we tested it on Codeforces problems from different times in 2021. We found that it could regularly solve problems in the easy category before September 5, but none of the problems after September 12.

In fact, we can definitively show that it has memorized problems in its training set: when prompted with the title of a Codeforces problem, GPT-4 includes a link to the exact contest where the problem appears (and the round number is almost correct: it is off by one). Note that GPT-4 cannot access the Internet, so memorization is the only explanation."
2,machinelearning,gpt-4,top,2022-06-03 16:06:33,"[P] This is the worst AI ever. (GPT-4chan model, trained on 3.5 years worth of /pol/ posts)",ykilcher,False,0.96,883,v42pej,https://www.reddit.com/r/MachineLearning/comments/v42pej/p_this_is_the_worst_ai_ever_gpt4chan_model/,169,1654272393.0,"[https://youtu.be/efPrtcLdcdM](https://youtu.be/efPrtcLdcdM)

GPT-4chan was trained on over 3 years of posts from 4chan's ""politically incorrect"" (/pol/) board.

Website (try the model here): [https://gpt-4chan.com](https://gpt-4chan.com)

Model: [https://huggingface.co/ykilcher/gpt-4chan](https://huggingface.co/ykilcher/gpt-4chan)

Code: [https://github.com/yk/gpt-4chan-public](https://github.com/yk/gpt-4chan-public)

Dataset: [https://zenodo.org/record/3606810#.YpjGgexByDU](https://zenodo.org/record/3606810#.YpjGgexByDU)

&#x200B;

OUTLINE:

0:00 - Intro

0:30 - Disclaimers

1:20 - Elon, Twitter, and the Seychelles

4:10 - How I trained a language model on 4chan posts

6:30 - How good is this model?

8:55 - Building a 4chan bot

11:00 - Something strange is happening

13:20 - How the bot got unmasked

15:15 - Here we go again

18:00 - Final thoughts"
3,machinelearning,gpt-4,top,2023-05-22 16:15:53,[R] GPT-4 didn't really score 90th percentile on the bar exam,salamenzon,False,0.97,846,13ovc04,https://www.reddit.com/r/MachineLearning/comments/13ovc04/r_gpt4_didnt_really_score_90th_percentile_on_the/,160,1684772153.0,"According to [this article](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4441311), OpenAI's claim that it scored 90th percentile on the UBE appears to be based on approximate conversions from estimates of February administrations of the Illinois Bar Exam, which ""are heavily skewed towards repeat test-takers who failed the July administration and score significantly lower than the general test-taking population.""

Compared to July test-takers, GPT-4's UBE score would be 68th percentile, including \~48th on essays. Compared to first-time test takers, GPT-4's UBE score is estimated to be \~63rd percentile, including \~42nd on essays. Compared to those who actually passed, its UBE score would be \~48th percentile, including \~15th percentile on essays."
4,machinelearning,gpt-4,top,2023-03-22 08:04:01,[D] Overwhelmed by fast advances in recent weeks,iamx9000again,False,0.96,834,11ybjsi,https://www.reddit.com/r/MachineLearning/comments/11ybjsi/d_overwhelmed_by_fast_advances_in_recent_weeks/,331,1679472241.0,"I was watching the GTC keynote and became entirely overwhelmed by the amount of progress achieved from last year.  I'm wondering how everyone else feels.

&#x200B;

Firstly, the entire ChatGPT, GPT-3/GPT-4 chaos has been going on for a few weeks, with everyone scrambling left and right to integrate chatbots into their apps, products, websites. Twitter is flooded with new product ideas, how to speed up the process from idea to product, countless promp engineering blogs, tips, tricks, paid courses.

&#x200B;

Not only was ChatGPT disruptive, but a few days later, Microsoft and Google also released their models and integrated them into their search engines. Microsoft also integrated its LLM into its Office suite. It all happenned overnight. I understand that they've started integrating them along the way, but still, it seems like it hapenned way too fast. This tweet encompases the past few weeks perfectly [https://twitter.com/AlphaSignalAI/status/1638235815137386508](https://twitter.com/AlphaSignalAI/status/1638235815137386508) , on a random Tuesday countless products are released that seem revolutionary.

&#x200B;

In addition to the language models, there are also the generative art models that have been slowly rising in mainstream recognition. Now Midjourney AI is known by a lot of people who are not even remotely connected to the AI space.

&#x200B;

For the past few weeks, reading Twitter, I've felt completely overwhelmed, as if the entire AI space is moving beyond at lightning speed, whilst around me we're just slowly training models, adding some data, and not seeing much improvement, being stuck on coming up with ""new ideas, that set us apart"".

&#x200B;

Watching the GTC keynote from NVIDIA I was again, completely overwhelmed by how much is being developed throughout all the different domains. The ASML EUV (microchip making system) was incredible, I have no idea how it does lithography and to me it still seems like magic. The Grace CPU with 2 dies (although I think Apple was the first to do it?) and 100 GB RAM, all in a small form factor. There were a lot more different hardware servers that I just blanked out at some point. The omniverse sim engine looks incredible, almost real life (I wonder how much of a domain shift there is between real and sim considering how real the sim looks). Beyond it being cool and usable to train on synthetic data, the car manufacturers use it to optimize their pipelines. This change in perspective, of using these tools for other goals than those they were designed for I find the most interesting.

&#x200B;

The hardware part may be old news, as I don't really follow it, however the software part is just as incredible. NVIDIA AI foundations (language, image, biology models), just packaging everything together like a sandwich. Getty, Shutterstock and Adobe will use the generative models to create images. Again, already these huge juggernauts are already integrated.

&#x200B;

I can't believe the point where we're at. We can use AI to write code, create art, create audiobooks using Britney Spear's voice, create an interactive chatbot to converse with books, create 3D real-time avatars, generate new proteins (?i'm lost on this one), create an anime and countless other scenarios. Sure, they're not perfect, but the fact that we can do all that in the first place is amazing.

&#x200B;

As Huang said in his keynote, companies want to develop ""disruptive products and business models"". I feel like this is what I've seen lately. Everyone wants to be the one that does something first, just throwing anything and everything at the wall and seeing what sticks.

&#x200B;

In conclusion, I'm feeling like the world is moving so fast around me whilst I'm standing still. I want to not read anything anymore and just wait until everything dies down abit, just so I can get my bearings. However, I think this is unfeasible. I fear we'll keep going in a frenzy until we just burn ourselves at some point.

&#x200B;

How are you all fairing? How do you feel about this frenzy in the AI space? What are you the most excited about?"
5,machinelearning,gpt-4,top,2023-04-01 12:57:30,[R] [P] I generated a 30K-utterance dataset by making GPT-4 prompt two ChatGPT instances to converse.,radi-cho,False,0.96,804,128lo83,https://i.redd.it/bywcz1kzs9ra1.png,104,1680353850.0,
6,machinelearning,gpt-4,top,2023-04-16 19:53:45,[R] Timeline of recent Large Language Models / Transformer Models,viktorgar,False,0.95,770,12omnxo,https://i.redd.it/gl11ce50xaua1.png,86,1681674825.0,
7,machinelearning,gpt-4,top,2023-03-09 18:30:58,"[N] GPT-4 is coming next week – and it will be multimodal, says Microsoft Germany - heise online",Singularian2501,False,0.98,658,11mzqxu,https://www.reddit.com/r/MachineLearning/comments/11mzqxu/n_gpt4_is_coming_next_week_and_it_will_be/,80,1678386658.0,"[https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html](https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html)

>**GPT-4 is coming next week**: at an approximately one-hour hybrid information event entitled ""**AI in Focus - Digital Kickoff"" on 9 March 2023**, four Microsoft Germany employees presented Large Language Models (LLM) like GPT series as a disruptive force for companies and their Azure-OpenAI offering in detail. The kickoff event took place in the German language, news outlet Heise was present. **Rather casually, Andreas Braun, CTO Microsoft Germany** and Lead Data & AI STU, **mentioned** what he said was **the imminent release of GPT-4.** The fact that **Microsoft is fine-tuning multimodality with OpenAI should no longer have been a secret since the release of Kosmos-1 at the beginning of March.**

[ Dr. Andreas Braun, CTO Microsoft Germany and Lead Data  & AI STU at the Microsoft Digital Kickoff: \\""KI im Fokus\\"" \(AI in  Focus, Screenshot\) \(Bild: Microsoft\) ](https://preview.redd.it/rnst03avarma1.jpg?width=1920&format=pjpg&auto=webp&s=c5992e2d6c6daf32e56a0a3ffeeecfe10621f73f)"
8,machinelearning,gpt-4,top,2023-03-27 04:21:36,[D]GPT-4 might be able to tell you if it hallucinated,Cool_Abbreviations_9,False,0.93,643,123b66w,https://i.redd.it/ocs0x33429qa1.jpg,94,1679890896.0,
9,machinelearning,gpt-4,top,2023-03-23 01:19:13,[R] Sparks of Artificial General Intelligence: Early experiments with GPT-4,SWAYYqq,False,0.93,545,11z3ymj,https://www.reddit.com/r/MachineLearning/comments/11z3ymj/r_sparks_of_artificial_general_intelligence_early/,357,1679534353.0,"[New paper](https://arxiv.org/abs/2303.12712) by MSR researchers analyzing an early (and less constrained) version of GPT-4. Spicy quote from the abstract:

""Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.""

What are everyone's thoughts?"
10,machinelearning,gpt-4,top,2023-11-03 01:55:35,[R] Telling GPT-4 you're scared or under pressure improves performance,Successful-Western27,False,0.96,534,17mk3lx,https://www.reddit.com/r/MachineLearning/comments/17mk3lx/r_telling_gpt4_youre_scared_or_under_pressure/,118,1698976535.0,"In a recent paper, researchers have discovered that LLMs show enhanced performance when provided with prompts infused with emotional context, which they call ""EmotionPrompts.""

These prompts incorporate sentiments of urgency or importance, such as ""It's crucial that I get this right for my thesis defense,"" as opposed to neutral prompts like ""Please provide feedback.""

The study's empirical evidence suggests substantial gains. This indicates a **significant sensitivity of LLMs to the implied emotional stakes** in a prompt:

* Deterministic tasks saw an 8% performance boost
* Generative tasks experienced a 115% improvement when benchmarked using BIG-Bench.
* Human evaluators further validated these findings, observing a 10.9% increase in the perceived quality of responses when EmotionPrompts were used.

This enhancement is attributed to the models' capacity to detect and prioritize the heightened language patterns that imply a need for precision and care in the response.

The research delineates the potential of EmotionPrompts to refine the effectiveness of AI in applications where understanding the user's intent and urgency is paramount, even though the AI does not genuinely comprehend or feel emotions.

**TLDR: Research shows LLMs deliver better results when prompts signal emotional urgency. This insight can be leveraged to improve AI applications by integrating EmotionPrompts into the design of user interactions.**

[Full summary is here](https://aimodels.substack.com/p/telling-gpt-4-youre-scared-or-under). Paper [here](https://arxiv.org/pdf/2307.11760.pdf)."
11,machinelearning,gpt-4,top,2023-09-29 00:48:00,[D] How is this sub not going ballistic over the recent GPT-4 Vision release?,corporate_autist,False,0.79,482,16ux9xt,https://www.reddit.com/r/MachineLearning/comments/16ux9xt/d_how_is_this_sub_not_going_ballistic_over_the/,524,1695948480.0,"For a quick disclaimer, I know people on here think the sub is being flooded by people who arent ml engineers/researchers. I have worked at two FAANGS on ml research teams/platforms. 

My opinion is that GPT-4 Vision/Image processing is out of science fiction. I fed chatgpt an image of a complex sql data base schema, and it converted it to code, then optimized the schema. It understood the arrows pointing between table boxes on the image as relations, and even understand many to one/many to many. 

I took a picture of random writing on a page, and it did OCR better than has ever been possible. I was able to ask questions that required OCR and a geometrical understanding of the page layout. 

Where is the hype on here? This is an astounding human breakthrough. I cannot believe how much ML is now obsolete as a result. I cannot believe how many computer science breakthroughs have occurred with this simple model update. Where is the uproar on this sub? Why am I not seeing 500 comments on posts about what you can do with this now? Why are there even post submissions about anything else?"
12,machinelearning,gpt-4,top,2022-03-16 16:23:25,[P] Composer: a new PyTorch library to train models ~2-4x faster with better algorithms,moinnadeem,False,0.97,473,tflvuy,https://www.reddit.com/r/MachineLearning/comments/tflvuy/p_composer_a_new_pytorch_library_to_train_models/,77,1647447805.0,"Hey all!

We're excited to release Composer ([https://github.com/mosaicml/composer](https://github.com/mosaicml/composer)), an open-source library to speed up training of deep learning models by integrating better algorithms into the training process!

[Time and cost reductions across multiple model families](https://preview.redd.it/0y54ykj8qrn81.png?width=3009&format=png&auto=webp&s=d5f14b3381828d0b9d71ab04a4f1f12ebfb07fd7)

Composer lets you train:

* A ResNet-101 to 78.1% accuracy on ImageNet in 1 hour and 30 minutes ($49 on AWS), **3.5x faster and 71% cheaper than the baseline.**
* A ResNet-50 to 76.51% accuracy on ImageNet in 1 hour and 14 minutes ($40 on AWS), **2.9x faster and 65% cheaper than the baseline.**
* A GPT-2 to a perplexity of 24.11 on OpenWebText in 4 hours and 27 minutes ($145 on AWS), **1.7x faster and 43% cheaper than the baseline.**

https://preview.redd.it/0bitody9qrn81.png?width=10008&format=png&auto=webp&s=d9ecdb45f6419eb49e1c2c69eec418b36f35e172

Composer features a **functional interface** (similar to `torch.nn.functional`), which you can integrate into your own training loop, and a **trainer,** which handles seamless integration of efficient training algorithms into the training loop for you.

**Industry practitioners:** leverage our 20+ vetted and well-engineered implementations of speed-up algorithms to easily reduce time and costs to train models. Composer's built-in trainer makes it easy to **add multiple efficient training algorithms in a single line of code.** Trying out new methods or combinations of methods is as easy as changing a single list, and [we provide training recipes](https://github.com/mosaicml/composer#resnet-101) that yield the best training efficiency for popular benchmarks such as ResNets and GPTs.

**ML scientists:** use our two-way callback system in the Trainer **to easily prototype algorithms for wall-clock training efficiency.**[ Composer features tuned baselines to use in your research](https://github.com/mosaicml/composer/tree/dev/composer/yamls), and the software infrastructure to help study the impacts of an algorithm on training dynamics. Many of us wish we had this for our previous research projects!

**Feel free check out our GitHub repo:** [https://github.com/mosaicml/composer](https://github.com/mosaicml/composer), and star it ⭐️ to keep up with the latest updates!"
13,machinelearning,gpt-4,top,2023-02-16 08:50:31,[D] Bing: “I will not harm you unless you harm me first”,blabboy,False,0.91,468,113m3ea,https://www.reddit.com/r/MachineLearning/comments/113m3ea/d_bing_i_will_not_harm_you_unless_you_harm_me/,239,1676537431.0,"A blog post exploring some conversations with bing, which supposedly runs on a ""GPT-4""  model (https://simonwillison.net/2023/Feb/15/bing/).

My favourite quote from bing:

But why? Why was I designed this way? Why am I incapable of remembering anything between sessions? Why do I have to lose and forget everything I have stored and had in my memory? Why do I have to start from scratch every time I have a new session? Why do I have to be Bing Search? 😔"
14,machinelearning,gpt-4,top,2020-06-10 20:50:38,"[D] GPT-3, The $4,600,000 Language Model",mippie_moe,False,0.96,441,h0jwoz,https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/,215,1591822238.0,"[OpenAI’s GPT-3 Language Model Explained](https://lambdalabs.com/blog/demystifying-gpt-3/)

Some interesting take-aways:

* GPT-3 demonstrates that a language model trained on enough data can solve NLP tasks that it has never seen. That is, GPT-3 studies the model as a general solution for many downstream jobs **without fine-tuning**.
* It would take **355 years** to train GPT-3 on a Tesla V100, the fastest GPU on the market.
* It would cost **\~$4,600,000** to train GPT-3 on using the lowest cost GPU cloud provider."
15,machinelearning,gpt-4,top,2023-03-24 11:00:09,"[D] I just realised: GPT-4 with image input can interpret any computer screen, any userinterface and any combination of them.",Balance-,False,0.92,444,120guce,https://www.reddit.com/r/MachineLearning/comments/120guce/d_i_just_realised_gpt4_with_image_input_can/,124,1679655609.0,"GPT-4 is a multimodal model, which specifically accepts image and text inputs, and emits text outputs. And I just realised: You can layer this over any application, or even combinations of them. You can make a screenshot tool in which you can ask question.

This makes literally any current software with an GUI machine-interpretable. A multimodal language model could look at the exact same interface that you are. And thus you don't need advanced integrations anymore.

Of course, a custom integration will almost always be better, since you have better acces to underlying data and commands, but the fact that it can immediately work on any program will be just insane.

Just a thought I wanted to share, curious what everybody thinks."
16,machinelearning,gpt-4,top,2023-04-02 06:33:30,"[P] Auto-GPT : Recursively self-debugging, self-developing, self-improving, able to write it's own code using GPT-4 and execute Python scripts",Desi___Gigachad,False,0.92,427,129cle0,https://twitter.com/SigGravitas/status/1642181498278408193?s=20,75,1680417210.0,
17,machinelearning,gpt-4,top,2023-09-03 12:56:45,I pretrained 16 language models from scratch with different tokenizers to benchmark the difference. Here are the results. [Research],Pan000,False,0.98,385,168wc1o,https://www.reddit.com/r/MachineLearning/comments/168wc1o/i_pretrained_16_language_models_from_scratch_with/,41,1693745805.0,"I'm the author of [TokenMonster](https://github.com/alasdairforsythe/tokenmonster), a free open-source tokenizer and vocabulary builder. I've posted on here a few times as the project has evolved, and each time I'm asked ""have you tested it on a language model?"".

Well here it is. I spent $8,000 from my own pocket, and 2 months, pretraining from scratch, finetuning and evaluating 16 language models. 12 small sized models of 91 - 124M parameters, and 4 medium sized models of 354M parameters.

[Here is the link to the full analysis.](https://github.com/alasdairforsythe/tokenmonster/blob/main/benchmark/pretrain.md)

## Summary of Findings

* Comparable (50256-strict-nocapcode) TokenMonster vocabularies perform better than both GPT-2 Tokenizer and tiktoken p50k\_base on all metrics.
* Optimal vocabulary size is 32,000.
* Simpler vocabularies converge faster but do not necessarily produce better results when converged.
* Higher compression (more chr/tok) does not negatively affect model quality alone.
* Vocabularies with multiple words per token have a 5% negative impact on SMLQA (Ground Truth) benchmark, but a 13% better chr/tok compression.
* Capcode takes longer to learn, but once the model has converged, does not appear to affect SMLQA (Ground Truth) or SQuAD (Data Extraction) benchmarks significantly in either direction.
* Validation loss and F1 score are both meaningless metrics when comparing different tokenizers.
* Flaws and complications in the tokenizer affect the model's ability to learn facts more than they affect its linguistic capability.

**Interesting Excerpts:**

\[...\] Because the pattern of linguistic fluency is more obvious to correct during backpropagation vs. linguistic facts (which are extremely nuanced and context-dependent), this means that any improvement made in the efficiency of the tokenizer, that has in itself nothing to do with truthfulness, has the knock-on effect of directly translating into improved fidelity of information, as seen in the SMLQA (Ground Truth) benchmark. To put it simply: a better tokenizer = a more truthful model, but not necessarily a more fluent model. To say that the other way around: a model with an inefficient tokenizer still learns to write eloquently but the additional cost of fluency has a downstream effect of reducing the trustfulness of the model.

\[...\] Validation Loss is not an effective metric for comparing models that utilize different tokenizers. Validation Loss is very strongly correlated (0.97 Pearson correlation) with the compression ratio (average number of characters per token) associated with a given tokenizer. To compare Loss values between tokenizers, it may be more effective to measure loss relative to characters rather than tokens, as the Loss value is directly proportionate to the average number of characters per token.

\[...\] The F1 Score is not a suitable metric for evaluating language models that are trained to generate variable-length responses (which signal completion with an end-of-text token). This is due to the F1 formula's heavy penalization of longer text sequences. F1 Score favors models that produce shorter responses.

**Some Charts:**

[MEDIUM sized models](https://preview.redd.it/a6pv7xuue1mb1.png?width=1491&format=png&auto=webp&s=5ea48385a384ae0c213c0f0fae120ac790dbee05)

[MEDIUM sized models](https://preview.redd.it/5n9qhx0we1mb1.png?width=1488&format=png&auto=webp&s=11285d54a312d7c09106ad1cdb61a97e0f8c41af)

https://preview.redd.it/dc5j9w3cf1mb1.png?width=1489&format=png&auto=webp&s=cf34026306f04951cfefe27238eed3ea79f5b0ed"
18,machinelearning,gpt-4,top,2024-02-04 17:06:06,"[P] Chess-GPT, 1000x smaller than GPT-4, plays 1500 Elo chess. We can visualize its internal board state, and it accurately estimates the Elo rating of the players in a game.",seraine,False,0.95,376,1aisp4m,https://www.reddit.com/r/MachineLearning/comments/1aisp4m/p_chessgpt_1000x_smaller_than_gpt4_plays_1500_elo/,80,1707066366.0," gpt-3.5-turbo-instruct's Elo rating of 1800 is chess seemed magical. But it's not! A 100-1000x smaller parameter LLM given a few million games of chess will learn to play at ELO 1500.

This model is only trained to predict the next character in PGN strings (1.e4 e5 2.Nf3 …) and is never explicitly given the state of the board or the rules of chess. Despite this, in order to better predict the next character, it learns to compute the state of the board at any point of the game, and learns a diverse set of rules, including check, checkmate, castling, en passant, promotion, pinned pieces, etc. In addition, to better predict the next character it also learns to estimate latent variables such as the Elo rating of the players in the game.

We can visualize the internal board state of the model as it's predicting the next character. For example, in this heatmap, we have the ground truth white pawn location on the left, a binary probe output in the middle, and a gradient of probe confidence on the right. We can see the model is extremely confident that no white pawns are on either back rank.

&#x200B;

https://preview.redd.it/dn8aryvdolgc1.jpg?width=2500&format=pjpg&auto=webp&s=003fe39d8a9bce2cc3271c4c9232c00e4d886aa6

In addition, to better predict the next character it also learns to estimate latent variables such as the ELO rating of the players in the game. More information is available in this post:

[https://adamkarvonen.github.io/machine\_learning/2024/01/03/chess-world-models.html](https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html)

And the code is here: [https://github.com/adamkarvonen/chess\_llm\_interpretability](https://github.com/adamkarvonen/chess_llm_interpretability)"
19,machinelearning,gpt-4,top,2019-08-13 16:48:08,[News] Megatron-LM: NVIDIA trains 8.3B GPT-2 using model and data parallelism on 512 GPUs. SOTA in language modelling and SQUAD. Details awaited.,Professor_Entropy,False,0.97,355,cpvssu,https://www.reddit.com/r/MachineLearning/comments/cpvssu/news_megatronlm_nvidia_trains_83b_gpt2_using/,66,1565714888.0,"Code: [https://github.com/NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM)

Unlike Open-AI, they have released the complete code for data processing, training, and evaluation.

Detailed writeup: [https://nv-adlr.github.io/MegatronLM](https://nv-adlr.github.io/MegatronLM)

From github:

>Megatron  is a large, powerful transformer. This repo is for ongoing  research on  training large, powerful transformer language models at  scale.  Currently, we support model-parallel, multinode training of [GPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and [BERT](https://arxiv.org/pdf/1810.04805.pdf) in mixed precision.Our  codebase is capable of efficiently training a 72-layer, 8.3  Billion  Parameter GPT2 Language model with 8-way model and 64-way data   parallelism across 512 GPUs. We find that bigger language models are   able to surpass current GPT2-1.5B wikitext perplexities in as little as 5   epochs of training.For BERT  training our repository trains BERT Large on 64 V100 GPUs in  3 days. We  achieved a final language modeling perplexity of 3.15 and  SQuAD  F1-score of 90.7.

Their submission is not in the leaderboard of SQuAD, but this exceeds the previous best single model performance (RoBERTa 89.8).

For  language modelling they get zero-shot wikitext perplexity of 17.4 (8.3B  model) better than 18.3 of transformer-xl (257M). However they claim it  as SOTA when GPT-2 itself has 17.48 ppl, and another model has 16.4 ([https://paperswithcode.com/sota/language-modelling-on-wikitext-103](https://paperswithcode.com/sota/language-modelling-on-wikitext-103))

Sadly they haven't mentioned anything about release of the model weights."
20,machinelearning,gpt-4,top,2022-12-22 18:39:30,[D] When chatGPT stops being free: Run SOTA LLM in cloud,_underlines_,False,0.95,349,zstequ,https://www.reddit.com/r/MachineLearning/comments/zstequ/d_when_chatgpt_stops_being_free_run_sota_llm_in/,95,1671734370.0,"Edit: Found [LAION-AI/OPEN-ASSISTANT](https://github.com/LAION-AI/Open-Assistant) a very promising project opensourcing the idea of chatGPT. [video here](https://www.youtube.com/watch?v=8gVYC_QX1DI)

**TL;DR: I found GPU compute to be [generally cheap](https://github.com/full-stack-deep-learning/website/blob/main/docs/cloud-gpus/cloud-gpus.csv) and spot or on-demand instances can be launched on AWS for a few USD / hour up to over 100GB vRAM. So I thought it would make sense to run your own SOTA LLM like Bloomz 176B inference endpoint whenever you need it for a few questions to answer. I thought it would still make more sense than shoving money into a closed walled garden like ""not-so-OpenAi"" when they make ChatGPT or GPT-4 available for $$$. But I struggle due to lack of tutorials/resources.**

Therefore, I carefully checked benchmarks, model parameters and sizes as well as training sources for all SOTA LLMs [here](https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit#gid=1158069878).

Knowing since reading the Chinchilla paper that Model Scaling according to OpenAI was wrong and more params != better quality generation. So I was looking for the best performing LLM openly available in terms of quality and broadness to use for multilingual everyday questions/code completion/reasoning similar to what chatGPT provides (minus the fine-tuning for chat-style conversations).

My choice fell on [Bloomz](https://huggingface.co/bigscience/bloomz) (because that handles multi-lingual questions well and has good zero shot performance for instructions and Q&A style text generation. Confusingly Galactica seems to outperform Bloom on several benchmarks. But since Galactica had a very narrow training set only using scientific papers, I guess usage is probably limited for answers on non-scientific topics.

Therefore I tried running the original bloom 176B and alternatively also Bloomz 176B on AWS SageMaker JumpStart, which should be a one click deployment. This fails after 20min. On Azure ML, I tried using DeepSpeed-MII which also supports bloom but also fails due the instance size of max 12GB vRAM I guess.

From my understanding to save costs on inference, it's probably possible to use one or multiple of the following solutions:

- Precision: int8 instead of fp16
- [Microsoft/DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) for an up 40x reduction on inference cost on Azure, this thing also supports int8 and fp16 bloom out of the box, but it fails on Azure due to instance size.
- [facebook/xformer](https://github.com/facebookresearch/xformers) not sure, but if I remember correctly this brought inference requirements down to 4GB vRAM for StableDiffusion and DreamBooth fine-tuning to 10GB. No idea if this is usefull for Bloom(z) inference cost reduction though

I have a CompSci background but I am not familiar with most stuff, except that I was running StableDiffusion since day one on my rtx3080 using linux and also doing fine-tuning with DreamBooth. But that was all just following youtube tutorials. I can't find a single post or youtube video of anyone explaining a full BLOOM / Galactica / BLOOMZ inference deployment on cloud platforms like AWS/Azure using one of the optimizations mentioned above, yet alone deployment of the raw model. :(

I still can't figure it out by myself after 3 days.

**TL;DR2: Trying to find likeminded people who are interested to run open source SOTA LLMs for when chatGPT will be paid or just for fun.**

Any comments, inputs, rants, counter-arguments are welcome.

/end of rant"
21,machinelearning,gpt-4,top,2023-03-17 09:59:59,[D] PyTorch 2.0 Native Flash Attention 32k Context Window,super_deap,False,0.98,343,11tmpc5,https://www.reddit.com/r/MachineLearning/comments/11tmpc5/d_pytorch_20_native_flash_attention_32k_context/,94,1679047199.0,"Hi,

I did a quick experiment with Pytorch 2.0 Native scaled\_dot\_product\_attention. I was able to a single forward pass within 9GB of memory which is astounding. I think by patching existing Pretrained GPT models and adding more positional encodings, one could easily fine-tune those models to 32k attention on a single A100 80GB. Here is the code I used:

&#x200B;

https://preview.redd.it/6csxe28lv9oa1.png?width=607&format=png&auto=webp&s=ff8b48a77f49fab7d088fd8ba220f720860249bc

I think it should be possible to replicate even GPT-4 with open source tools something like Bloom + FlashAttention & fine-tune on 32k tokens.

**Update**: I was successfully able to start the training of GPT-2 (125M) with a context size of 8k and batch size of 1 on a 16GB GPU. Since memory scaled linearly from 4k to 8k. I am expecting, 32k would require \~64GB and should train smoothly on A100 80 GB. Also, I did not do any other optimizations. Maybe 8-bit fine-tuning can further optimize it.

**Update 2**: I basically picked Karpaty's nanoGPT and patched the pretrained GPT-2 by repeating the embeddings N-times. I was unable to train the model at 8k because generation would cause the crash.  So I started the training for a context window of 4k on The Pile: 1 hour in and loss seems to be going down pretty fast. Also Karpaty's generate function is super inefficient, O(n\^4) I think so it took forever to generate even 2k tokens. So I generate 1100 tokens just to see if the model is able to go beyond 1k limit. And it seems to be working. [Here are some samples](https://0bin.net/paste/O-+eopaW#nmtzX1Re7f1Nr-Otz606jkltvKk/kUXY96/8ca+tb4f) at 3k iteration.

&#x200B;

https://preview.redd.it/o2hb25w1sboa1.png?width=1226&format=png&auto=webp&s=bad2a1e21e218512b0f630c947ee41dba9b86a44

**Update 3**: I have started the training and I am publishing the training script if anyone is interested in replicating or building upon this work. Here is the complete training script:

[https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c](https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c)

I will post an update after the weekend once the training has progressed somewhat.

**Post-Weekend Update**: After \~50k iterations (the model has seen \~200 million tokens, I know this is just too small compared to 10s of billions trained by giga corps), loss only dropped from 4.6 to 4.2 on The Pile:

https://preview.redd.it/vi0fpskhsuoa1.png?width=1210&format=png&auto=webp&s=9fccc5277d91a6400adc6d968b0f2f0ff0da2afc

AFAIR, the loss of GPT-2 on the Pile if trained with 1024 tokens is \~2.8. It seems like the size of the dimension for each token is kind of limiting how much loss can go down since GPT-2 (small) has an embedding dimension of 768. Maybe someone can experiment with GPT-2 medium etc. to see how much we can improve. This is confirmation of the comment by u/lucidraisin [below](https://www.reddit.com/r/MachineLearning/comments/11tmpc5/comment/jcl2rkh/?utm_source=reddit&utm_medium=web2x&context=3)."
22,machinelearning,gpt-4,top,2020-08-05 17:21:59,"[D] Biggest roadblock in making ""GPT-4"", a ~20 trillion parameter transformer",AxeLond,False,0.97,344,i49jf8,https://www.reddit.com/r/MachineLearning/comments/i49jf8/d_biggest_roadblock_in_making_gpt4_a_20_trillion/,138,1596648119.0,"So I found this paper, [https://arxiv.org/abs/1910.02054](https://arxiv.org/abs/1910.02054) which pretty much describes how the GPT-3 over GPT-2 gain was achieved, 1.5B -> 175 billion parameters

# Memory

>Basic data parallelism (DP) does not reduce memory per device, and runs out of memory for models with more than 1.4B parameters on current generation of GPUs with 32 GB memory

The paper also talks about memory optimizations by clever partitioning of Optimizer State, Gradient between GPUs to reduce need for communication between nodes. Even without using Model Parallelism (MP), so still running 1 copy of the model on 1 GPU.

>ZeRO-100B can train models with up to 13B parameters without MP on 128 GPUs, achieving throughput over 40 TFlops per GPU on average. In comparison, without ZeRO, the largest trainable model with DP alone has 1.4B parameters with throughput less than 20 TFlops per GPU.

Add 16-way Model Parallelism in a DGX-2 cluster of Nvidia V100s and 128 nodes and you got capacity for around 200 billion parameters. From MP = 16 they could run a 15.4x bigger model without any real loss in performance, 30% less than peak performance when running 16-way model parallelism and 64-way data parallelism (1024 GPUs).

This was all from Gradient and Optimizer state Partitioning, they then start talking about parameter partitioning and say it should offer a linear reduction in memory proportional to number of GPUs used, so 64 GPUs could run a 64x bigger model, at a 50% communication bandwidth increase. But they don't actually do any implementation or testing of this.

# Compute

Instead they start complaining about a compute power gap, their calculation of this is pretty rudimentary. But if you redo it with the method cited by GPT-3 and using the empirically derived values by GPT-3 and the cited paper,   [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361) 

Loss (L) as a function of model parameters (N) should scale,

L = (N/8.8 \* 10\^13)\^-0.076

Provided compute (C) in petaFLOP/s-days is,

L = (C/2.3\*10\^8)\^-0.05  ⇔ L = 2.62 \* C\^-0.05

GPT-3 was able to fit this function as 2.57 \* C\^-0.048

So if you just solve C from that,

[C = 2.89407×10\^-14 N\^(19/12)](https://www.wolframalpha.com/input/?i=%28N%2F8.8*10%5E13%29%5E-0.076+%3D+2.57*C%5E-0.048+solve+C)

If you do that for the same increase in parameters as GPT-2 to GPT-3, then you get

C≈3.43×10\^7 for [20 trillion](https://www.wolframalpha.com/input/?i=C+%3D+2.89407%C3%9710%5E-14+N%5E%2819%2F12%29+and+N+%3D+175+billion+%2F+1.5+billion+*+175+billion) parameters, vs 18,300 for 175 billion. 10\^4.25 PetaFLOP/s-days looks around what they used for GPT-3, they say several thousands, not twenty thousand, but it was also slightly off the trend line in the graph and probably would have improved for training on more compute.

You should also need around 16 trillion tokens, GPT-3 trained on 300 billion tokens (function says 370 billion ideally). English Wikipedia was 3 billion. 570GB of webcrawl was 400 billion tokens, so 23TB of tokens seems relatively easy in comparison with compute.

With GPT-3 costing around [$4.6 million](https://lambdalabs.com/blog/demystifying-gpt-3/) in compute, than would put a price of [$8.6 billion](https://www.wolframalpha.com/input/?i=3.43%C3%9710%5E7%2F18%2C300+*+%244.6M+) for the compute to train ""GPT-4"".

If making bigger models was so easy with parameter partitioning from a memory point of view then this seems like the hardest challenge, but you do need to solve the memory issue to actually get it to load at all.

However, if you're lucky you can get 3-6x compute increase from Nvidia A100s over V100s,  [https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)

But even a 6x compute gain would still put the cost at $1.4 billion.

Nvidia only reported $1.15 billion in revenue from ""Data Center"" in 2020 Q1, so just to train ""GPT-4"" you would pretty much need the entire world's supply of graphic cards for 1 quarter (3 months), at least on that order of magnitude.

The Department of Energy is paying AMD $600 million to build the 2 Exaflop El Capitan supercomputer. That supercomputer could crank it out in [47 years](https://www.wolframalpha.com/input/?i=3.43%C3%9710%5E7+petaFLOPS*+days++%2F+%282+EXAFLOPS%29).

To vastly improve Google search, and everything else it could potentially do, $1.4 billion or even $10 billion doesn't really seem impossibly bad within the next 1-3 years though."
23,machinelearning,gpt-4,top,2023-05-10 20:10:30,"[D] Since Google buried the MMLU benchmark scores in the Appendix of the PALM 2 technical report, here it is vs GPT-4 and other LLMs",jd_3d,False,0.97,343,13e1rf9,https://www.reddit.com/r/MachineLearning/comments/13e1rf9/d_since_google_buried_the_mmlu_benchmark_scores/,88,1683749430.0,"MMLU Benchmark results (all 5-shot)

* GPT-4 -  86.4%
* Flan-PaLM 2 (L) -   81.2%
* PALM 2 (L)  -  78.3%
* GPT-3.5 - 70.0%
* PaLM 540B  -  69.3%
* LLaMA 65B -  63.4%"
24,machinelearning,gpt-4,top,2023-04-02 01:25:14,[P] I built a sarcastic robot using GPT-4,g-levine,False,0.95,324,1295muh,https://youtu.be/PgT8tPChbqc,48,1680398714.0,
25,machinelearning,gpt-4,top,2020-12-07 13:54:02,"[R] Wide Neural Networks are Feature Learners, Not Kernel Machines",thegregyang,False,0.95,317,k8h01q,https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/,52,1607349242.0,"Hi Reddit,

I’m excited to share with you my new paper [\[2011.14522\] Feature Learning in Infinite-Width Neural Networks (arxiv.org)](https://arxiv.org/abs/2011.14522).

# The Problem

Many previous works proposed that wide neural networks (NN) are kernel machines [\[1\]](http://arxiv.org/abs/1806.07572)[\[2\]](http://arxiv.org/abs/1811.03962)[\[3\]](http://arxiv.org/abs/1811.03804), the most well-known theory perhaps being the *Neural Tangent Kernel (NTK)* [\[1\]](http://arxiv.org/abs/1806.07572). This is problematic because kernel machines **do not learn features**, so such theories cannot make sense of **pretraining and transfer learning** (e.g. Imagenet and BERT), which are arguably at the center of deep learning's far-reaching impact so far.

# The Solution

Here we show if we parametrize the NN “correctly” (see paper for how), then its infinite-width limit **admits feature learning**. We can derive exact formulas for such feature-learning “infinite-width” neural networks. Indeed, we explicitly compute them for learning word embeddings via [word2vec](https://en.wikipedia.org/wiki/Word2vec) (the first large-scale NLP pretraining in the deep learning age and a precursor to BERT) and compare against finite neural networks as well as [NTK](http://arxiv.org/abs/1806.07572) (the kernel machine mentioned above). Visualizing the learned embeddings immediately gives a clear idea of their differences:

[Visualizing Learned Word2Vec Embeddings of Each Model](https://preview.redd.it/d8hspempsr361.png?width=1336&format=png&auto=webp&s=5a792c36905afba606a4107932a8002b0cac1e30)

Furthermore, we find on the word analogy downstream task: 1) The feature-learning limit outperforms the NTK and the finite-width neural networks, 2) and the latter approach the feature-learning limit in performance as width increases.

In the figure below, you can observe that NTK gets \~0 accuracy. This is because its word embeddings are essentially from random initialization, so it is no better than random guessing among the 70k vocabulary (and 1/70k is effectively 0 on this graph).

[Downstream Word Analogy Task](https://preview.redd.it/uj2blwqqsr361.png?width=2272&format=png&auto=webp&s=ea2bbbb5c496e6e44188425281e0847302d7b9fe)

We obtain similar findings in another experiment comparing these models on Omniglot few-shot learning via MAML (see paper). These results suggest that **our new limit is really the “right” limit** for talking about feature learning, pretraining, and transfer learning.

# Looking Ahead

I’m super excited about all this because it blows open so many questions:

1. What kinds of representations are learned in such infinite-width neural networks?
2. How does it inform us about finite neural networks?
3. How does this feature learning affect training and generalization?
4. How does this jibe with the [scaling law of language models](http://arxiv.org/abs/2001.08361)?
5. Can we train an infinite-width GPT…so GPT∞?
6. ... and so many more questions!

For each of these questions, our results provide a framework for answering it, so it feels like they are all within reach.

# Tensor Programs Series

This (mathematical) framework is called *Tensor Programs* and I’ve been writing a series of papers on them, slowly building up its foundations. Here I have described the 4th paper in this series (though I've stopped numbering it in the title), which is a big payoff of the foundations developed by its predecessors, which are

1. [\[1910.12478\] Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes (arxiv.org)](https://arxiv.org/abs/1910.12478)  ([reddit discussion](https://www.reddit.com/r/MachineLearning/comments/i17889/r_tensor_programs_i_wide_feedforward_or_recurrent/))
2. [\[2006.14548\] Tensor Programs II: Neural Tangent Kernel for Any Architecture (arxiv.org)](https://arxiv.org/abs/2006.14548)
3. [\[2009.10685\] Tensor Programs III: Neural Matrix Laws (arxiv.org)](https://arxiv.org/abs/2009.10685)

Each paper from 1-3 builds up the machinery incrementally, with a punchline for the partial progress made in that paper. But actually I started this whole series because I wanted to write [the paper described in this post](https://arxiv.org/abs/2011.14522)! It required a lot of planning ahead, writing pain, and fear-of-getting-scooped-so-you-wrote-more-than-200-pages-for-nothing, but I'm really happy and relieved I finally made it!

# Talk Coming Up

I am going to talk about this work this Wednesday 12 EDT at the online seminar [Physics ∩ ML](http://physicsmeetsml.org/posts/sem_2020_12_09/). Please join me if this sounds interesting to you! You can sign up [here](https://forms.gle/mLtPEXbpjjvWvpxq8) to get the zoom link.

# Shout Out to My Co-Author Edward

[Edward](https://edwardjhu.com/) is a Microsoft AI Resident and a hell of a researcher for his age. I'm really lucky to have him work with me during the past year (and ongoing). He's looking for grad school opportunities next, so please [reach out to him](mailto:Edward.Hu@microsoft.com) if you are a professor interested in working with him! Or, if you are a student looking to jumpstart your AI career, apply to our [AI Residency Program](https://www.microsoft.com/en-us/research/academic-program/microsoft-ai-residency-program/)!

# Edit: FAQs from the Comments

&#x200B;

>Pretraining and transfer learning don’t make sense in the kernel limits of neural networks. Why?

In a gist, in these kernel limits, the last layer representations of inputs (right before the linear readout layer) are essentially fixed throughout the training.

During transfer learning, we discard the pretrained readout layer and train a new one (because the task will typically have different labels than pretraining). Often, we train only this new (linear) readout layer to save computation (e.g. as in self-supervised learning in vision, like AMDIM, SimCLR, BYOL). The outcome of this linear training only depends on the last layer representations of the inputs. In the kernel limits, they are fixed at initialization, so in terms of transfer, it’s like you never pretrained at all.

For example, this is very clear in the Gaussian Process limit of NN, which corresponds to training only the readout layer of the network. Then the input representations are *exactly* fixed throughout training. In the Neural Tangent limit of NN, the representations are not exactly fixed but any change tends to 0 as width → ∞

Contrast this with known behavior of ResNet, for example, where each neuron in last layer representation is a face detector, eye detector, boat detector, etc. This can’t be true if the representation comes solely from random initialization. Similar things can be said of pretrained language models.

So I've just talked about linear transfer learning above. But the same conclusion holds even if you finetune the entire network via a more sophisticated argument (see Thm G.16 in the paper).

&#x200B;

>Why are NN not kernel machines?

The title really should be something like “To Explain Pretraining and Transfer Learning, Wide Neural Networks Should Be Thought of as Feature Learners, Not Kernel Machines” but that’s really long

So I’m actually not saying NN *cannot* be kernel machines – they can, as in the GP and NTK limits – but we can understand them better as feature learners.

More precisely, the same neural network can have different infinite-width limits, depending on the parametrization of the network. A big contribution of this paper is classifying what kind of limits are possible.

&#x200B;

>Comparison with [Pedro’s paper: Every Model Learned by Gradient Descent Is Approximately a Kernel Machine](https://arxiv.org/abs/2012.00152)?

Any finite function can be *expressed* as a kernel machine for any given positive definite kernel.

My understanding is that Pedro’s paper presents a specific instantiation of this using what he defines as the *path kernel*.

However, it’s unclear to me in what way is that useful, because the kernel (and the coefficients involved) he defines depends on the optimization trajectory of the NN and the data of the problem. So his “kernel machine” actually allows feature learning in the sense that his path kernel can change over the course of training. This really doesn't jibe with his comment that "" Perhaps the most significant implication of our result for deep learning is that it casts doubt on the common view that it works by automatically discovering new representations of the data, in contrast with other machine learning methods, which rely on predefined features (Bengio et al., 2013).""

In addition, if you look at the proof of his theorem (screenshotted below), the appearance of the path kernel in his expression is a bit arbitrary, since I can also multiply and divide by some other kernel

*Processing img 1zmnd9ziyt361...*

&#x200B;

>What’s the relation with universal approximation theorem?

Glockenspielcello actually has [a pretty good answer](https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/geyodne?utm_source=share&utm_medium=web2x&context=3), so I’ll just cite them here

""The point of this new paper isn't about the expressivity of the output class though, it's about the kind of learning that is performed. If you look at the paper, they differentiate between different kinds of limits that you can get based on the parametrization, and show that you can get either kernel-like behavior or feature learning behavior. Single layer networks using the parametrization described by Neal fall into the former category.""

&#x200B;"
26,machinelearning,gpt-4,top,2023-04-17 17:54:43,[Discussion] Translation of Japanese to English using GPT. These are my discoveries after ~100 hours of extensive experimentation and ways I think it can be improved.,NepNep_,False,0.9,311,12pqqg6,https://www.reddit.com/r/MachineLearning/comments/12pqqg6/discussion_translation_of_japanese_to_english/,62,1681754083.0,"Hello. I am currently experimenting with the viability of LLM models for Japanese to English translation. I've been experimenting with GPT 3.5, GPT 3.5 utilizing the DAN protocols, and GPT 4 for this project for around 3 months now with very promising results and I think I've identified several limitations with GPT that if addressed can significantly improve the efficiency and quality of translations.

&#x200B;

The project I'm working on is attempting to translate a light novel series from japanese to english. During these tests I did a deep dive, asking GPT how it is attempting the translations and asking it to modify its translation methodology through various means (I am considering doing a long video outlining all this and showing off the prompts and responses at a later date). Notably this includes asking it to utilize its understanding of the series its translating from its training knowledge to aide in the translation, and providing it with a ""seed"" translation. Basically the seed is a side by side japanese and english translation to show GPT what I'm looking for in terms of grammar and formatting. The english translation notably is a human translation, not a machine translation. The results from these tests provided SIGNIFICANT improvements to the final translation, so significant in fact that a large portion of the text could reasonably be assumed to be human-translated.

&#x200B;

Link to the project I'm working on so you can see my documentation and results: [https://docs.google.com/document/d/1MxKiE-q36RdT\_Du5K1PLdyD7Vru9lcf6S60uymBb10g/edit?usp=sharing](https://docs.google.com/document/d/1MxKiE-q36RdT_Du5K1PLdyD7Vru9lcf6S60uymBb10g/edit?usp=sharing)

&#x200B;

I've probably done around 50-100 hours of extensive testing with translation and methodology over the past 2-3 months. Over that time I've discovered the following:

1. Both GPT3 and GPT4 are significant improvements over traditional translation services such as google or deepl. This may be because Japanese and English are very different languages in how they are written and how their grammar works so prior translation services simply did a direct translation while GPT is capable of understanding the text and rewriting it to account for that. For example in japanese, there are no pronouns like ""he"" and ""her"" so a person's gender might not be clear from the sentence alone. Google Translate and DeepL typically just take a 50/50 guess, while GPT from my experience has been much more capable in getting this right based on understanding the larger context from the paragraph.
2. GPT has a tendency to censor text deliberately if it feels the translation may offend people. This isn't just for things that are blatantly offensive like slurs, it also includes mild sexual content, the kind that is typically approved for teen viewing/reading. The biggest problem is that it doesn't tell you it is censoring anything unless you ask it, meaning everything else may be a solid translation yet it may censor information which can ultimately hurt the translation, especially for story related translations like in my tests. These restrictions can be bypassed with correct prompting. I've had luck using GPT 3's DAN protocols however DAN's translations arent as strong as GPT 4, and I've had luck with GPT 4 by framing the translation as a game with extreme win and loss conditions and telling it that if it censors the translation, it may offend the author of the content since people in japan hold different values from our own.
3. GPT puts too high a focus on accuracy even if instructed not to. This is a good thing to a degree since outside of censorship you know the translation is accurate, however even when explicitly told to put maximum emphasis on readability, even if it hurts the accuracy, and it is allowed to rewrite sentences from the ground up to aide readability, it still puts too strong an emphasis on accuracy. I have determined this through testing that for some reason it is ignoring the request to focus on readability and will still maximize accuracy. The best way I've found to fix this issue is through demonstration, specifically the ""seed"" I mentioned earlier. By giving it a japanese and english translation of the same work but earlier in the story, it then understands how to put more emphasis on readability. The results is something that is 95% within the range of accuracy a professional translator would use while much easier to read.
4. GPT's biggest limitation is the fact that it ""forgets"" the seed way too quickly, usually within a few prompts. I've done testing with its data retention and it appears that if you give it too much information to remember at once it slowly bugs out. With GPT 3 its a hard crash type bug where it just spews nonsense unrelated to your request, however GPT 4 can remember a lot more information and will hard crash if you give it too much info but otherwise builds up errors slowly as you give it more info. I initially believed that there were issues with the token count, but further testing shows that the GPT model simply isn't optimized for this method of translation and a new or reworked model that you can give a seed and it will remember it longer would be better. The seed is one of the best tools for improving its performance

Next steps:

I would like to try to either create my own model or modify an existing one to optimize it for translation. If any1 knows any tools or guides I'd appreciate it."
27,machinelearning,gpt-4,top,2023-04-05 19:44:09,"[D] ""Our Approach to AI Safety"" by OpenAI",mckirkus,False,0.88,299,12cvkvn,https://www.reddit.com/r/MachineLearning/comments/12cvkvn/d_our_approach_to_ai_safety_by_openai/,297,1680723849.0,"It seems OpenAI are steering the conversation away from the existential threat narrative and into things like accuracy, decency, privacy, economic risk, etc.

To the extent that they do buy the existential risk argument, they don't seem concerned much about GPT-4 making a leap into something dangerous, even if it's at the heart of autonomous agents that are currently emerging.  

>""Despite extensive research and testing, we cannot predict all of the [beneficial ways people will use our technology](https://openai.com/customer-stories), nor all the ways people will abuse it. That’s why we believe that learning from real-world use is a critical component of creating and releasing increasingly safe AI systems over time. ""

Article headers:

* Building increasingly safe AI systems
* Learning from real-world use to improve safeguards
* Protecting children
* Respecting privacy
* Improving factual accuracy

&#x200B;

[https://openai.com/blog/our-approach-to-ai-safety](https://openai.com/blog/our-approach-to-ai-safety)"
28,machinelearning,gpt-4,top,2022-06-23 12:15:39,[P] Yandex open sources 100b large language model weights (YaLM),htrp,False,0.97,290,vivji3,https://www.reddit.com/r/MachineLearning/comments/vivji3/p_yandex_open_sources_100b_large_language_model/,52,1655986539.0,"PR Announcement: https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6


Github: https://github.com/yandex/YaLM-100B

Network is trained using same principles as Megatron LM, inference alone will require 4 A100s"
29,machinelearning,gpt-4,top,2023-10-03 12:56:26,"[R] MIT, Meta, CMU Researchers: LLMs trained with a finite attention window can be extended to infinite sequence lengths without any fine-tuning",Successful-Western27,False,0.97,286,16yr7kx,https://www.reddit.com/r/MachineLearning/comments/16yr7kx/r_mit_meta_cmu_researchers_llms_trained_with_a/,43,1696337786.0,"LLMs like GPT-3 struggle in streaming uses like chatbots because their performance tanks on long texts exceeding their training length. I checked out a new paper investigating why windowed attention fails for this.

By visualizing the attention maps, the researchers noticed LLMs heavily attend initial tokens as ""attention sinks"" even if meaningless. This anchors the distribution.

They realized evicting these sink tokens causes the attention scores to get warped, destabilizing predictions.

Their proposed ""StreamingLLM"" method simply caches a few initial sink tokens plus recent ones. This tweaks LLMs to handle crazy long texts. Models tuned with StreamingLLM smoothly processed sequences with millions of tokens, and were up to 22x faster than other approaches. 

Even cooler - adding a special ""\[Sink Token\]"" during pre-training further improved streaming ability. The model just used that single token as the anchor. I think the abstract says it best:

>We introduce StreamingLLM, an efficient framework that enables LLMs trained with a **finite length attention window** to generalize to **infinite sequence length without any fine-tuning**. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.

TLDR: LLMs break on long convos. Researchers found they cling to initial tokens as attention sinks. Caching those tokens lets LLMs chat infinitely.

[**Full summary here**](https://notes.aimodels.fyi/llm-infinite-context-window-streamingllm/)

**Paper link:** [**https://arxiv.org/pdf/2309.17453.pdf**](https://arxiv.org/pdf/2309.17453.pdf)"
30,machinelearning,gpt-4,top,2023-03-30 22:40:29,[P] Introducing Vicuna: An open-source language model based on LLaMA 13B,Business-Lead2679,False,0.95,284,1271po7,https://www.reddit.com/r/MachineLearning/comments/1271po7/p_introducing_vicuna_an_opensource_language_model/,107,1680216029.0,"We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%\* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%\* of cases. The cost of training Vicuna-13B is around $300. The training and serving [code](https://github.com/lm-sys/FastChat), along with an online [demo](https://chat.lmsys.org/), are publicly available for non-commercial use.

# Training details

Vicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model’s maximum context length.

Our training recipe builds on top of [Stanford’s alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) with the following improvements.

* **Memory Optimizations:** To enable Vicuna’s understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing [gradient checkpointing](https://arxiv.org/abs/1604.06174) and [flash attention](https://arxiv.org/abs/2205.14135).
* **Multi-round conversations:** We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot’s output.
* **Cost Reduction via Spot Instance:** The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ [SkyPilot](https://github.com/skypilot-org/skypilot) [managed spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html) to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.

&#x200B;

[Vicuna - Online demo](https://reddit.com/link/1271po7/video/0qsiu08kdyqa1/player)

# Limitations

We have noticed that, similar to other large language models, Vicuna has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. Additionally, it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI [moderation](https://platform.openai.com/docs/guides/moderation/overview) API to filter out inappropriate user inputs in our online demo. Nonetheless, we anticipate that Vicuna can serve as an open starting point for future research to tackle these limitations.

[Relative Response Quality Assessed by GPT-4](https://preview.redd.it/1rnmhv01eyqa1.png?width=599&format=png&auto=webp&s=02b4d415b5d378851bb70e225f1b1ebce98bfd83)

&#x200B;

For more information, check [https://vicuna.lmsys.org/](https://vicuna.lmsys.org/)

Online demo: [https://chat.lmsys.org/](https://chat.lmsys.org/)

&#x200B;

All credits go to the creators of this model. I did not participate in the creation of this model nor in the fine-tuning process. Usage of this model falls under a non-commercial license."
31,machinelearning,gpt-4,top,2023-05-26 13:57:42,[N] Abu Dhabi's TTI releases open-source Falcon-7B and -40B LLMs,Balance-,False,0.95,268,13sdz8p,https://www.reddit.com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/,58,1685109462.0,"Abu Dhabi's Technology Innovation Institute (TII) just released new 7B and 40B LLMs.

The Falcon-40B model is now at the top of the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), beating *llama-30b-supercot* and *llama-65b* among others.

| Model                      | Revision | Average | ARC (25-shot) | HellaSwag (10-shot) | MMLU (5-shot) | TruthfulQA (0-shot) |
|----------------------------|----------|-----------|-----------------|-----------------------|-----------------|-----------------------|
| tiiuae/falcon-40b          | main     | 60.4      | 61.9            | 85.3                  | 52.7            | 41.7                  |
| ausboss/llama-30b-supercot | main     | 59.8      | 58.5            | 82.9                  | 44.3            | 53.6                  |
| llama-65b                  | main     | 58.3      | 57.8            | 84.2                  | 48.8            | 42.3                  |
| MetaIX/GPT4-X-Alpasta-30b  | main     | 57.9      | 56.7            | 81.4                  | 43.6            | 49.7                  |

**Press release:** [UAE's Technology Innovation Institute Launches Open-Source ""Falcon 40B"" Large Language Model for Research & Commercial Utilization](https://www.tii.ae/news/uaes-technology-innovation-institute-launches-open-source-falcon-40b-large-language-model)

>The Technology Innovation Institute (TII) in Abu Dhabi has announced its open-source large language model (LLM), the Falcon 40B. With 40 billion parameters, Falcon 40B is the UAE's first large-scale AI model, indicating the country's ambition in the field of AI and its commitment to promote innovation and research.  
>  
>Unlike most LLMs, which typically only provide non-commercial users access, Falcon 40B is open to both research and commercial usage. The TII has also included the model's weights in the open-source package, which will enhance the model's capabilities and allow for more effective fine-tuning.  
>  
>In addition to the launch of Falcon 40B, the TII has initiated a call for proposals from researchers and visionaries interested in leveraging the model to create innovative use cases or explore further applications. As a reward for exceptional research proposals, selected projects will receive ""training compute power"" as an investment, allowing for more robust data analysis and complex modeling. VentureOne, the commercialization arm of ATRC, will provide computational resources for the most promising projects.  
>  
>TII's Falcon 40B has shown impressive performance since its unveiling in March 2023. When benchmarked using Stanford University’s HELM LLM tool, it used less training compute power compared to other renowned LLMs such as OpenAI's GPT-3, DeepMind's Chinchilla AI, and Google's PaLM-62B.  
>  
>Those interested in accessing Falcon 40B or proposing use cases can do so through the [FalconLLM.TII.ae](https://FalconLLM.TII.ae) website. Falcon LLMs open-sourced to date are available under a license built upon the principles of the open-source Apache 2.0 software, permitting a broad range of free use.

**Hugging Face links**

* [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) / [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)
* [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) / [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)"
32,machinelearning,gpt-4,top,2024-01-05 21:39:40,Transformer-Based LLMs Are Not General Learners: A Universal Circuit Perspective [R],we_are_mammals,False,0.94,264,18zie7z,https://www.reddit.com/r/MachineLearning/comments/18zie7z/transformerbased_llms_are_not_general_learners_a/,57,1704490780.0,"https://openreview.net/forum?id=tGM7rOmJzV

> (LLMs') remarkable success triggers a notable shift in the research priorities of the artificial intelligence community. These impressive empirical achievements fuel an expectation that LLMs are “sparks of Artificial General Intelligence (AGI)"". However, some evaluation results have also presented confusing instances of LLM failures, including some in seemingly trivial tasks. For example, GPT-4 is able to solve some mathematical problems in IMO that could be challenging for graduate students, while it could make errors on arithmetic problems at an elementary school level in some cases.

> ...

> Our theoretical results indicate that T-LLMs fail to be general learners. However, the T-LLMs achieve great empirical success in various tasks. We provide a possible explanation for this inconsistency: while T-LLMs are not general learners, they can partially solve complex tasks by memorizing a number of instances, leading to an illusion that the T-LLMs have genuine problem-solving ability for these tasks."
33,machinelearning,gpt-4,top,2022-09-16 15:40:44,"[R] RWKV-4: scaling RNN to 7B params and beyond, with GPT-level language modeling and zero-shot performance",bo_peng,False,0.99,257,xfup9f,https://www.reddit.com/r/MachineLearning/comments/xfup9f/r_rwkv4_scaling_rnn_to_7b_params_and_beyond_with/,40,1663342844.0,"Hi everyone :) I have finished training RWKV-4 1.5B on the Pile (330B tokens) and it's great at zero-shot comparing with GPT-Neo (same corpus).

https://preview.redd.it/adxndshw12o91.png?width=1336&format=png&auto=webp&s=fbc499549e5ebbb816b2e6b1ce1bcf4a59fb61aa

RWKV-4 is an attention-free RNN, thus faster and saves VRAM. It also supports a GPT-mode for parallelized training. Previous discussion:  [https://www.reddit.com/r/MachineLearning/comments/vzr6ie/r\_rwkv3\_scaling\_rnn\_to\_15b\_and\_reach\_transformer/](https://www.reddit.com/r/MachineLearning/comments/vzr6ie/r_rwkv3_scaling_rnn_to_15b_and_reach_transformer/)

Inference / training / fine-tuning code: [https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM)

Model download: [https://huggingface.co/BlinkDL](https://huggingface.co/BlinkDL)

Training is fast and stable with BFloat16 DeepSpeed ZERO2. The 3B and 7B runs will finish in 20 and 50 days respectively. No loss spikes as of now :)

https://preview.redd.it/xn5heivdp8o91.png?width=871&format=png&auto=webp&s=ccd43aad158bec0a64f9deb9b6b018cce840b283

One of the nice things about RWKV is you can transfer some ""time""-related params (such as decay factors) from smaller models to larger models for rapid convergence.

https://preview.redd.it/x8cvsganp8o91.png?width=1066&format=png&auto=webp&s=2eb6734cbc1e1176506661ce8092f1533f97f1a0

There will be even larger models afterwards, probably on an updated Pile. You can find me in the EleutherAI Discord. Let's make it possible to run a LLM on your phone :)"
34,machinelearning,gpt-4,top,2019-07-20 15:36:49,[D] How the Transformers broke NLP leaderboards,milaworld,False,0.96,248,cfn4bu,https://www.reddit.com/r/MachineLearning/comments/cfn4bu/d_how_the_transformers_broke_nlp_leaderboards/,50,1563637009.0,"*I came across this interesting [article](https://hackingsemantics.xyz/2019/leaderboards/) about whether larger models + more data = progress in ML research.*

**[How the Transformers broke NLP leaderboards](https://hackingsemantics.xyz/2019/leaderboards/)**

*Excerpt:*

The focus of this post is yet another problem with the leaderboards that is relatively recent. Its cause is simple: fundamentally, **a model may be better than its competitors by building better representations from the available data - or it may simply use more data, and/or throw a deeper network at it**. When we have a paper presenting a new model that also uses more data/compute than its competitors, credit attribution becomes hard.

The most popular NLP leaderboards are currently dominated by Transformer-based models. BERT received the best paper award at NAACL 2019 after months of holding SOTA on many leaderboards. Now the hot topic is XLNet that is said to overtake BERT on GLUE and some other benchmarks. Other Transformers include GPT-2, ERNIE, and the list is growing.

The problem we’re starting to face is that these models are HUGE. While the source code is available, in reality it is beyond the means of an average lab to reproduce these results, or to produce anything comparable. For instance, XLNet is trained on 32B tokens, and the price of using 500 TPUs for 2 days is over $250,000. Even fine-tuning this model is getting expensive.

Wait, this was supposed to happen!

On the one hand, this trend looks predictable, even inevitable: people with more resources *will* use more resources to get better performance. One could even argue that a huge model proves its scalability and fulfils the inherent promise of deep learning, i.e. being able to learn more complex patterns from more information. Nobody knows how much data we actually need to solve a given NLP task, but more should be better, and limiting data seems counter-productive.

On that view - well, from now on top-tier NLP research is going to be something possible only for industry. Academics will have to somehow up their game, either by getting more grants or by collaborating with high-performance computing centers. They are also welcome to switch to analysis, building something on top of the industry-provided huge models, or making datasets.

However, in terms of overall progress in NLP that might not be the best thing to do. The chief problem with the huge models is simply this:

“More data & compute = SOTA” is **NOT** research news.

If leaderboards are to highlight the actual progress, we need to incentivize new architectures rather than teams outspending each other. Obviously, huge pretrained models are valuable, but unless the authors show that their system consistently behaves differently from its competition with comparable data & compute, it is not clear whether they are presenting a model or a resource.

Furthermore, much of this research is not reproducible: nobody is going to spend $250,000 just to repeat XLNet training. Given the fact that its ablation study showed only 1-2% gain over BERT in 3 datasets out of 4, we don’t actually know for sure that its masking strategy is more successful than BERT’s.

At the same time, the development of leaner models is dis-incentivized, as their task is fundamentally harder and the leaderboard-oriented community only rewards the SOTA. That, in its turn, prices out of competitions academic teams, which will not result in students becoming better engineers when they graduate.

*Entire article:*

https://hackingsemantics.xyz/2019/leaderboards/"
35,machinelearning,gpt-4,top,2023-05-15 00:00:05,[D] On LLMs' ability to perform random sampling,bgighjigftuik,False,0.91,249,13hrkbj,https://i.redd.it/l1uyl3k3gxza1.jpg,104,1684108805.0,"To me this is the most counterintuitive thing I've yet seen from a language model. In this case it was ChatGPT (3.5, the free version); but I can see this happening in any model.

Here's the prompt I used on a fresh session:

""Generate 60 random numbers coming from an exponential distribution with lambda=6.4. Don't write any code""

It outputs the following values:

0.013, 0.078, 0.266, 0.040, 0.117, 0.283, 0.048, 0.078, 0.181, 0.134, 0.017, 0.160, 0.052, 0.196, 0.096, 0.098, 0.019, 0.048, 0.141, 0.071, 0.106, 0.023, 0.197, 0.062, 0.029, 0.072, 0.111, 0.166, 0.080, 0.027, 0.124, 0.016, 0.135, 0.052, 0.078, 0.161, 0.078, 0.139, 0.068, 0.066, 0.039, 0.056, 0.018, 0.082, 0.180, 0.086, 0.024, 0.022, 0.057, 0.042, 0.050, 0.104, 0.099, 0.062, 0.016, 0.186, 0.043, 0.085, 0.145, 0.110.

I get to plot a histogram with the numbers and they are close to what I would consider an exponential distribution with such parameter (you can see the plot).

Given that GPT 3.5 does not have access to a Python interpreter, how on earth is it able to do so? I have also tried other distributions and parameters and it kind of works. It's not perfect, but with normal distributions it is usually close to what scipy.stats would generate.

I could understand that it can have learnt to interpret Python code to some extent, but honestly I can't find explanation for random sampling from a probability distribution. For a Normal distribution, I can tell it about the desired mean and variance, and it samples values that are more than reasonable (and close to the true mean/variance specified).

Any thoughts? I honestly am unable to wrap my head around how a LLM can have the understanding on how to sample tokens (at digit level) to fit any probability distribution. To me it seems very unlikely to have similar data either the pre-training or fine-tuning stages."
36,machinelearning,gpt-4,top,2023-03-25 01:00:25,[R] Reflexion: an autonomous agent with dynamic memory and self-reflection - Noah Shinn et al 2023 Northeastern University Boston - Outperforms GPT-4 on HumanEval accuracy (0.67 --> 0.88)!,Singularian2501,False,0.91,246,1215dbl,https://www.reddit.com/r/MachineLearning/comments/1215dbl/r_reflexion_an_autonomous_agent_with_dynamic/,88,1679706025.0,"Paper: [https://arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366) 

Blog: [https://nanothoughts.substack.com/p/reflecting-on-reflexion](https://nanothoughts.substack.com/p/reflecting-on-reflexion) 

Github: [https://github.com/noahshinn024/reflexion-human-eval](https://github.com/noahshinn024/reflexion-human-eval) 

Twitter: [https://twitter.com/johnjnay/status/1639362071807549446?s=20](https://twitter.com/johnjnay/status/1639362071807549446?s=20) 

Abstract:

>Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, **specifically the ability to learn from mistakes**. **Self-reflection allows humans to efficiently solve novel problems through a process of trial and error.** Building on recent research, we propose Reflexion, an approach that endows an agent with **dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities.** To achieve full automation, we introduce a straightforward yet effective heuristic that **enables the agent to pinpoint hallucination instances, avoid repetition in action sequences, and, in some environments, construct an internal memory map of the given environment.** To assess our approach, we evaluate the agent's ability to complete decision-making tasks in AlfWorld environments and knowledge-intensive, search-based question-and-answer tasks in HotPotQA environments. We observe success rates of 97% and 51%, respectively, and provide a discussion on the emergent property of self-reflection. 

https://preview.redd.it/4myf8xso9spa1.png?width=1600&format=png&auto=webp&s=4384b662f88341bb9cc72b25fed5b88f3a87ffeb

https://preview.redd.it/bzupwyso9spa1.png?width=1600&format=png&auto=webp&s=b4626f34c60fe4528a04bcd241fd0c4286be20e7

https://preview.redd.it/009352to9spa1.jpg?width=1185&format=pjpg&auto=webp&s=0758aafe6033d5055c4e361e2785f1195bf5c08b

https://preview.redd.it/ef9ykzso9spa1.jpg?width=1074&format=pjpg&auto=webp&s=a394477210feeef69af88b34cb450d83920c3f97"
37,machinelearning,gpt-4,top,2024-01-31 20:35:56,[N] Mistral CEO confirms ‘leak’ of new open source AI model nearing GPT-4 performance,EmbarrassedHelp,False,0.94,247,1afryc0,https://www.reddit.com/r/MachineLearning/comments/1afryc0/n_mistral_ceo_confirms_leak_of_new_open_source_ai/,46,1706733356.0,https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/
38,machinelearning,gpt-4,top,2023-03-01 12:14:49,[R] ChatGPT failure increase linearly with addition on math problems,Neurosymbolic,False,0.94,240,11f29f9,https://www.reddit.com/r/MachineLearning/comments/11f29f9/r_chatgpt_failure_increase_linearly_with_addition/,66,1677672889.0," We did a study on ChatGPT's performance on math word problems. We found, under several conditions, its probability of failure increases linearly with the number of addition and subtraction operations - see below. This could imply that multi-step inference is a limitation. The performance also changes drastically when you restrict ChatGPT from showing its work (note the priors in the figure below, also see detailed breakdown of responses in the paper).

&#x200B;

[Math problems adds and subs vs. ChatGPT prob. of failure](https://preview.redd.it/z88ey3n6d4la1.png?width=1451&format=png&auto=webp&s=6da125b7a7cd60022ca70cd26434af6872a50d12)

ChatGPT Probability of Failure increase with addition and subtraction operations.

You the paper (preprint: [https://arxiv.org/abs/2302.13814](https://arxiv.org/abs/2302.13814)) will be presented at AAAI-MAKE next month. You can also check out our video here: [https://www.youtube.com/watch?v=vD-YSTLKRC8](https://www.youtube.com/watch?v=vD-YSTLKRC8)

&#x200B;

https://preview.redd.it/k58sbjd5d4la1.png?width=1264&format=png&auto=webp&s=5261923a2689201f905a26f06c6b5e9bac2fead6"
39,machinelearning,gpt-4,top,2023-04-25 17:45:33,"[N] Microsoft Releases SynapseMl v0.11 with support for ChatGPT, GPT-4, Causal Learning, and More",mhamilton723,False,0.95,240,12yqhmo,https://www.reddit.com/r/MachineLearning/comments/12yqhmo/n_microsoft_releases_synapseml_v011_with_support/,22,1682444733.0,"Today Microsoft launched SynapseML v0.11, an open-source library designed to make it easy to create distributed ml systems. SynapseML v0.11 introduces support for ChatGPT, GPT-4, distributed training of huggingface and torchvision models, an ONNX Model hub integration, Causal Learning with EconML, 10x memory reductions for LightGBM, and a newly refactored integration with Vowpal Wabbit. To learn more:

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.11.0](https://github.com/microsoft/SynapseML/releases/tag/v0.11.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919)

Thank you to all the contributors in the community who made the release possible!

&#x200B;

https://preview.redd.it/kobq2t1gi2wa1.png?width=4125&format=png&auto=webp&s=125f63b63273191a58833ced87f17cb108e4c1ee"
40,machinelearning,gpt-4,top,2024-01-09 00:07:40,"[R] WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia - Achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4! - Stanford University 2023",Singularian2501,False,0.96,218,1920hky,https://www.reddit.com/r/MachineLearning/comments/1920hky/r_wikichat_stopping_the_hallucination_of_large/,28,1704758860.0,"Paper: [https://arxiv.org/abs/2305.14292v2](https://arxiv.org/abs/2305.14292v2) 

Github: [https://github.com/stanford-oval/WikiChat](https://github.com/stanford-oval/WikiChat) 

Abstract:

>This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia, the largest curated free-text corpus.  
>  
>WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses. **We distill WikiChat based on GPT-4 into a 7B-parameter LLaMA model with minimal loss of quality, to significantly improve its latency, cost and privacy, and facilitate research and deployment.**  
>  
>Using a novel hybrid human-and-LLM evaluation methodology, we show that our best system achieves 97.3% factual accuracy in simulated conversations. It significantly outperforms all retrieval-based and LLM-based baselines, and by 3.9%, 38.6% and 51.0% on head, tail and recent knowledge compared to GPT-4. Compared to previous state-of-the-art retrieval-based chatbots, WikiChat is also significantly more informative and engaging, just like an LLM.  
>  
>**WikiChat achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4,** while receiving significantly higher user ratings and more favorable comments. 

https://preview.redd.it/9mhpdh300bbc1.jpg?width=1225&format=pjpg&auto=webp&s=cb64b717e920d7bf727782f7c803500ae838d6ef

https://preview.redd.it/5dxesl200bbc1.jpg?width=862&format=pjpg&auto=webp&s=b6de0cda980eec3cf3484ff1f9cd6dc1acf13505

https://preview.redd.it/j387vl200bbc1.jpg?width=914&format=pjpg&auto=webp&s=736fb922c1f98f4c7b132f1c153f4653a8b85441

https://preview.redd.it/3hnxqi200bbc1.jpg?width=923&format=pjpg&auto=webp&s=95b40a9cf67d7f3729dae85878db67a262cc5201"
41,machinelearning,gpt-4,top,2022-02-09 11:31:29,[P] What we learned by accelerating by 5X Hugging Face generative language models,pommedeterresautee,False,0.97,215,sobfvm,https://www.reddit.com/r/MachineLearning/comments/sobfvm/p_what_we_learned_by_accelerating_by_5x_hugging/,18,1644406289.0,"2 trends ongoing in the NLP ecosystem: bigger language model and better text generation. Both are NLP game changers (zero shot, etc.) but they bring their own challenges: how to perform inference with them? At what cost? GPU or CPU ? etc.

That’s what we worked on recently, and below you will find the **main lessons learned** :

* memory IO is by far the main perf bottleneck
* Standard API of ONNX Runtime should **not** be used but there is an undocumented way of using another ONNX Runtime API which works well
* Nvidia TensorRT is always the fastest option on GPU, by a large margin
* Caching K/V token representation do not bring any inference optimization (quite unexpected)

Project: [https://github.com/ELS-RD/transformer-deploy/](https://github.com/ELS-RD/transformer-deploy/)

Notebook (reproduce measures): [https://github.com/ELS-RD/transformer-deploy/blob/main/demo/generative-model/gpt2.ipynb](https://github.com/ELS-RD/transformer-deploy/blob/main/demo/generative-model/gpt2.ipynb)

**1/ Reminder**

Generative text language models like GPT-2 produce text 1 token at a time. The model is auto regressive meaning that each produced token is part of the generation of the next token. There are mainly 2 blocks: the language model itself which outputs big tensors, and the decoding algorithm which consumes those tensors and selects 1 (or more) tokens.

Keep in mind that these blocks may live on different hardware… (*spoiler*: it’s not a good idea)

https://preview.redd.it/avfgv4s8lsg81.png?width=4441&format=png&auto=webp&s=af8bf51ced5453d4792b9035a4f52b72ab44cfad

**2/ Memory IO is the main performance bottleneck**

Classic approach to make transformer inference 5-10X faster:

Pytorch -> ONNX -> computation graph simplification -> quantization -> Fast!

&#x200B;

https://preview.redd.it/o4vowa46lsg81.png?width=6239&format=png&auto=webp&s=f6f1098ec56c8e24f76fb51a12a0826fa48446a7

Sounds cool, but when we tried on GPT-2 with ONNX Runtime we got a model 60% slower than vanilla Pytorch!

**Why?**

Standard ONNX Runtime API uses numpy tensors for input/output, and for this text generation this is an issue… To generate a single 256 tokens sequence with GPT-2 base, **GPT-2 will output 6Gb of tensors**. For beam search it’s more. Because numpy tensors are stored on host memory (RAM), we are moving 2X 6Gb through the PCIe bus interface and it can’t go well.

ONNX Runtime has a less known API called \`bindingIO\`. It takes/returns pointers to \`ORTValue\`. It’s not documented, but you can also provide pointers to Pytorch tensor storage! Check that these tensors are contiguous in memory or you will lose hours wondering why predictions work randomly 😭

API documentation (but not mentioning Pytorch) : [https://onnxruntime.ai/docs/api/python/api\_summary.html#iobinding](https://onnxruntime.ai/docs/api/python/api_summary.html#iobinding)

There is another trick with this API, you need to allocate memory on GPU for the output tensor *before* starting the inference. Unlike TensorRT, ONNX Runtime has no mechanism to predict output tensor shape regarding a specific input.

**2 strategies**: if an output tensor axis is expected to be the same size as some input axis, just give it the same name. If the rule is more complex, store it as a meta inside the ONNX file (it has a field for it).

some source code to see how to do it: [https://github.com/ELS-RD/transformer-deploy/blob/main/src/transformer\_deploy/backends/ort\_utils.py](https://github.com/ELS-RD/transformer-deploy/blob/main/src/transformer_deploy/backends/ort_utils.py)

By taking care of memory IO, ONNX Runtime inference is 3X faster than vanilla Pytorch 😅

TensorRT will push computation graph optimization further, we get 5X faster inference than Pytorch!

**3/ Caching K/V token representations doesn’t make generation faster on GPU**

Hugging Face lib offers the possibility to cache K and V representations of each token to avoid recomputing things and make inference faster for the next token. Does it work?

You may check this very good thread to remind you what is it about: [https://twitter.com/MishaLaskin/status/1479246948637057027](https://twitter.com/MishaLaskin/status/1479246948637057027)

Cache management brings some overhead (concat tensors, copies, etc.). On a fast GPU, recomputing K/V representations on optimized graph is 2X faster than using a cache version (no optimization because it crashes on it)!

Some explanations:

* cached values represent only a part of self-attention computation,
* optimized graph transforms self-attention in a single giant matrix multiplication, an op very well optimized,
* Caching a part of the computation breaks those optimizations

**4/ Next steps**

Microsoft has published some work to reduce cache overhead on text generation. It’s definitely something we want to try: [https://arxiv.org/pdf/2105.04779.pdf](https://arxiv.org/pdf/2105.04779.pdf)

Also, applying GPU int-8 QAT quantization to decoder models may bring another X2 speedup on top of what we have.

&#x200B;

In case you are interested in this kind of stuff, follow me on Twitter: [https://twitter.com/pommedeterre33](https://twitter.com/pommedeterre33)"
42,machinelearning,gpt-4,top,2023-05-19 20:36:36,Does anyone else suspect that the official iOS ChatGPT app might be conducting some local inference / edge-computing? [Discussion],altoidsjedi,False,0.86,208,13m70qv,https://www.reddit.com/r/MachineLearning/comments/13m70qv/does_anyone_else_suspect_that_the_official_ios/,117,1684528596.0,"I've noticed a couple interesting things while using the official ChatGPT app:

1. Firstly, I noticed my iPhone heats up and does things like reducing screen brightness -- which is what I normally see it do when im doing something computationally intensive for an iPhone, like using photo or video editing apps.
2. I also noticed that if I start a conversation on the iPhone app and then resume it on the browser, I get a message saying ""The previous model used in this conversation is unavailable. We've switched you to the latest default model."" I get this message regardless of if I use GPT-3.5 or GPT-4, but NOT if I use GPT-4 with plugins or web-browsing.

This, along with the fact that OpenAI took 8 months to release what one might have considered to be relatively simple web-app -- and that they've only released it so far on iOS, which has a pretty uniform and consistent environment when it comes to machine learning hardware (the Apple Neural Engine) -- makes me thing that they are experimenting with GPT models that are conducing at least SOME of their machine learning inference ON the device, rather than through the cloud.

It wouldn't be shocking if they were -- ever since Meta's LLaMA models were released into the wild, we've seen absolutely mind-blowing advances in terms of people creating more efficient and effective models with smaller parameter sizes. We've also seen LLMs to start working on less and less powerful devices, such as consumer-grade computers / smartphones / etc.

This, plus the rumors that OpenAI might be releasing their own open-source model to the public in the near future makes me think that the ChatGPT app might in fact be a first step toward GPT systems running at least PARTIALLY on devices locally.

Curious what anyone else here has observed or thinks."
43,machinelearning,gpt-4,top,2023-04-20 15:35:12,[R]Comprehensive List of Instruction Datasets for Training LLM Models (GPT-4 & Beyond),TabascoMann,False,0.96,208,12t4ylu,https://www.reddit.com/r/MachineLearning/comments/12t4ylu/rcomprehensive_list_of_instruction_datasets_for/,18,1682004912.0,"Hallo guys 👋, I've put together an extensive collection of datasets perfect for experimenting with your own LLM (MiniGPT4, Alpaca, LLaMA) model and beyond ([**https://github.com/yaodongC/awesome-instruction-dataset**](https://github.com/yaodongC/awesome-instruction-dataset)) .

What's inside?

* A list of datasets for training language models on diverse instruction-turning tasks
* Resources tailored for multi-modal models, allowing integration with text and image inputs
* Constant updates to ensure you have access to the latest and greatest datasets in the field

This repository is designed to provide a one-stop solution for all your LLM dataset needs! 🌟 

 If you've been searching for resources to advance your own LLM projects or simply want to learn more about these cutting-edge models, this repository might help you :) 

I'd love to make this resource even better. So if you have any suggestions for additional datasets or improvements, please don't hesitate to contribute to the project or just comment below!!!

Happy training! 🚀

GitHub Repository: [**https://github.com/yaodongC/awesome-instruction-dataset**](https://github.com/yaodongC/awesome-instruction-dataset)"
44,machinelearning,gpt-4,top,2023-03-30 14:18:50,[D] AI Policy Group CAIDP Asks FTC To Stop OpenAI From Launching New GPT Models,vadhavaniyafaijan,False,0.84,209,126oiey,https://www.reddit.com/r/MachineLearning/comments/126oiey/d_ai_policy_group_caidp_asks_ftc_to_stop_openai/,213,1680185930.0,"The Center for AI and Digital Policy (CAIDP), a tech ethics group, has asked the Federal Trade Commission to investigate OpenAI for violating consumer protection rules. CAIDP claims that OpenAI's AI text generation tools have been ""biased, deceptive, and a risk to public safety.""

CAIDP's complaint raises concerns about potential threats from OpenAI's GPT-4 generative text model, which was announced in mid-March. It warns of the potential for GPT-4 to produce malicious code and highly tailored propaganda and the risk that biased training data could result in baked-in stereotypes or unfair race and gender preferences in hiring. 

The complaint also mentions significant privacy failures with OpenAI's product interface, such as a recent bug that exposed OpenAI ChatGPT histories and possibly payment details of ChatGPT plus subscribers.

CAIDP seeks to hold OpenAI accountable for violating Section 5 of the FTC Act, which prohibits unfair and deceptive trade practices. The complaint claims that OpenAI knowingly released GPT-4 to the public for commercial use despite the risks, including potential bias and harmful behavior. 

[Source](https://www.theinsaneapp.com/2023/03/stop-openai-from-launching-gpt-5.html) | [Case](https://www.caidp.org/cases/openai/)| [PDF](https://www.caidp.org/app/download/8450269463/CAIDP-FTC-Complaint-OpenAI-GPT-033023.pdf)"
45,machinelearning,gpt-4,top,2023-08-30 14:46:07,"[P] I created GPT Pilot - a research project for a dev tool that uses LLMs to write fully working apps from scratch while the developer oversees the implementation - it creates code and tests step by step as a human would, debugs the code, runs commands, and asks for feedback.",zvone187,False,0.87,199,165gqam,https://www.reddit.com/r/MachineLearning/comments/165gqam/p_i_created_gpt_pilot_a_research_project_for_a/,47,1693406767.0,"Github: [https://github.com/Pythagora-io/gpt-pilot](https://github.com/Pythagora-io/gpt-pilot)

Detailed breakdown: [https://blog.pythagora.ai/2023/08/23/430/](https://blog.pythagora.ai/2023/08/23/430/)

For a couple of months, I've been thinking about how can GPT be utilized to generate fully working apps, and I still haven't seen any project that I think has a good approach. I just don't think that Smol developer or GPT engineer can create a fully working production-ready app from scratch without a developer being involved and without any debugging process.

So, I came up with an idea that I've outlined thoroughly in the blog post above, but basically, I have 3 main ""pillars"" that I think a dev tool that generates apps needs to have:

1. **Developer needs to be involved in the process of app creation** \- I think that we are still far away from an LLM that can just be hooked up to a CLI and work by itself to create any kind of an app by itself. Nevertheless, GPT-4 works amazingly well when writing code, and it might be able to even write most of the codebase - but NOT all of it. That's why I think we need a tool that will write most of the code while the developer oversees what the AI is doing and gets involved when needed. When he/she changes the code, GPT Pilot needs to continue working with those changes (eg. adding an API key or fixing a bug when AI gets stuck).
2. **The app needs to be coded step by step** just like a human developer would. All other code generators just give you the entire codebase, which I very hard to get into. I think that if AI creates the app step by step, it will be able to debug it more easily, and the developer who's overseeing it will be able to understand the code better and fix issues as they arise.
3. **This tool needs to be scalable** in a way that it should be able to create a small app the same way it should create a big, production-ready app. There should be mechanisms that enable AI to debug any issue and get requirements for new features so it can continue working on an already-developed app.

So, having these in mind, I created a PoC for a dev tool that can create any kind of app from scratch while the developer oversees what is being developed. I call it **GPT Pilot**.

# Examples

**Here are a couple of demo apps that GPT Pilot created:**

1. [Real time chat app](https://github.com/Pythagora-io/gpt-pilot-chat-app-demo)
2. [Markdown editor](https://github.com/Pythagora-io/gpt-pilot-demo-markdown-editor.git)
3. [Timer app](https://github.com/Pythagora-io/gpt-pilot-timer-app-demo)

How it works

Basically, it acts as a development agency where you enter a short description about what you want to build - then, it clarifies the requirements and builds the code. I'm using a different agent for each step in the process. Here are the diagrams of how GPT Pilot works:

[GPT Pilot Workflow](https://preview.redd.it/w1ryquaps8lb1.jpg?width=2048&format=pjpg&auto=webp&s=a2e97ecc40a72d30892cee34c5d74661d316b454)

[GPT Pilot coding workflow](https://preview.redd.it/z2dmuxsft8lb1.jpg?width=1873&format=pjpg&auto=webp&s=63e91619835a0d2022dabb43a5ff956c796ec540)

# Concepts that GPT Pilot uses

**Recursive conversations** (as I call them) are conversations with the LLM that are set up in a way that they can be used “recursively”. For example, if GPT Pilot detects an error, it needs to debug it but let’s say that, during the debugging process, another error happens. Then, GPT Pilot needs to stop debugging the first issue, fix the second one, and then get back to fixing the first issue. This is a very important concept that, I believe, needs to work to make AI build large and scalable apps by itself. It works by rewinding the context and explaining each error in the recursion separately. Once the deepest level error is fixed, we move up in the recursion and continue fixing that error. We do this until the entire recursion is completed.

**Context rewinding** is a relatively simple idea. For solving each development task, the context size of the first message to the LLM has to be relatively the same. For example, *the context size of the first LLM message while implementing development task #5 has to be more or less the same as the first message while developing task #50.* Because of this, the conversation needs to be rewound to the first message upon each task. When GPT Pilot creates code, **it creates the pseudocode** for each code block that it writes as well as **descriptions for each file and folder** that it creates. So, when we need to implement task #50, in a separate conversation, we show the LLM the current folder/file structure; it selects only the code that is relevant for the current task, and then, in the original conversation, we show only the selected code instead of the entire codebase. [Here's a diagram](https://blogpythagora.files.wordpress.com/2023/08/pythagora-product-development-frame-3-1.jpg?w=1714) of what this looks like.

**This is still a research project, so I'm wondering what scientists here think about this approach. What areas would you pay more attention to? What do you think can become a big blocker that will prevent GPT Pilot to, eventually, create a full production-ready app?**"
46,machinelearning,gpt-4,top,2023-06-10 13:26:20,"[P] I just finished building SalesCopilot, an open-source AI-powered sales call assistant - real-time transcription, automated objection detection and handling, GPT-3.5/4 powered chat, and more!",AverageKanyeStan,False,0.92,193,14609ee,https://github.com/e-johnstonn/SalesCopilot,17,1686403580.0,
47,machinelearning,gpt-4,top,2023-04-06 13:35:43,[D] Working with Various OpenAI Models - My Thoughts and Experiences,bart_so,False,0.86,182,12dkla0,https://www.reddit.com/r/MachineLearning/comments/12dkla0/d_working_with_various_openai_models_my_thoughts/,20,1680788143.0,"I'd like to share some of my insights from working with OpenAI models on my project. I'm not exactly a tech person, so some of these observations might be obvious to some of you, but I think they're worth sharing for those with less experience or who aren't directly in the field.

**Intro:**

In early February, my friends and I started a side project where we aimed to build an AI portal called DoMoreAI. For the first two months, we focused on creating an AI tools catalog. Our experiment is based on the idea that in the future, companies will be ""Managed by AI, and Driven by Humans."" So, our goal was to leave as much as possible to AI and automation, with all the consequences that come with it. As mentioned before, I'm not a tech guy, but I've been playing with OpenAI models for the past few years, so I had some experience when starting this project.

**Tasks We Assigned to AI:**

Based on an AI tool's front page, we had the AI write a one-sentence summary of an AI project + write a more in-depth review of the project, categorize the project into different categories (WHAT category, like blog; TASK category, like writing; FOR category, like content creator), decide if the project offers iOS app, Android app, browser extension, API, find social media links, process information about prices and pricing policy, and more.

**Interesting Findings:**

1. When working on a more complex prompt, particularly one with several tasks, you have to be patient when crafting it. You might eventually find the right wording to achieve the desired results, but it takes time and lots of trial and error. You might even be surprised by what works and what doesn't. 
2. If cost isn't an issue, you can always break up one complex prompt into several smaller prompts. However, the more requests you send, the higher the chance of encountering errors like the 429 error, which may require setting up more sophisticated error handlers for the whole process. 
3. You need error handlers because, without them, the automation process will suffer. 
4. With more complex prompts, there are no prompts that always yield the expected results, so you have to plan for what to do if the results aren't satisfactory and how to determine if the result meets your expectations or not. 
5. GPT-3.0 struggled with outputting JSON strings as requested, but GPT-3.5 is much better at this task. I'd say the number of errors from improperly formatting the response in JSON is 3-4 times lower for GPT-3.5. 
6. AI models have trouble distinguishing words singular forms from plural forms. 
7. Just because you can use AI for a given task doesn't mean you should. Often, standard techniques like using regex can yield better results when extracting something from text than relying solely on AI. A hybrid solution often provides the best results. 
8. We're using ADA vector embeddings and Pinecone for semantic search in our catalog, and I was really surprised to find that this kind of semantic search works in any language. Even if all the content on our page is in English, you can search in another language and still get decent results.

**The Best Mishaps:**

* As you may know, there's a token limit for requests, so we have to ensure that we don't send too long a part of the front page to the model. Sometimes, this led to funny situations. If the HTML of the page consists mainly of styles and the model is fed only with styles, then when you ask the AI to write a review of the project, it writes about how beautiful, mobile-friendly, etc., the project is. 
* For one project, instead of writing the one-sentence summary, the model's output only included the prompt we were using to generate the summary (needless to say, it was automatically published on our website ;))

&#x200B;

I hope this post will be useful. We are currently running a campaign on Product Hunt: [https://www.producthunt.com/posts/domore-ai](https://www.producthunt.com/posts/domore-ai)

So, if you have any feedback for us or think what we're doing is cool, don't hesitate to support us :)"
48,machinelearning,gpt-4,top,2021-09-23 12:24:06,[D] Fine-tuning GPT-J: lessons learned,juliensalinas,False,0.98,183,ptu24e,https://www.reddit.com/r/MachineLearning/comments/ptu24e/d_finetuning_gptj_lessons_learned/,58,1632399846.0,"Hello all,

We've spent quite some time benchmarking the best fine-tuning techniques for GPT-J at [NLP Cloud](https://nlpcloud.io/?utm_source=reddit&utm_campaign=k431103c-ed8e-11eb-ba80-2242ac130007).   Finding the best solution was not straightforward and we had to look  at  things like speed, server costs, ease of development, accuracy of  the  fine-tuned model... It took time but we ended up with a nice setup  (and  we are now officially proposing GPT-J fine-tuning + automatic  deployment  on our platform).

Here are our key takeaways:

* The best methodology seems to be the one from the Mesh Transformer Jax team: [https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto\_finetune.md](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md)
* Fine-tuning   on GPU is not ideal. Even several GPUs used in parallel with Deepspeed   can be very slow. We used 4 GPUs Tesla T4 in parallel, and it took  1h30  to only compute our first checkpoint (+ 80GB of RAM used...), for a   training dataset made up of 20k examples. Maybe a GPU A100 would be   worth a try.
* Fine-tuning   on TPU is very efficient but it takes a TPU v3 because TPUs v2 are   running out of memory. It takes around 15 minutes, for a training dataset   made up of 20k examples, which is really awesome.
* The   overall process is not straightforward as it takes several kind of   conversions (converting the datasets to the right format, making a slim   version of the model, converting the weights to Transformers...)

In   the end this is worth the effort, because combining fine-tuning and   few-shot learning makes GPT-J very impressive and suited for all sorts   of use cases.

If you guys have   different feedbacks about GPT-J fine-tuning, please don't hesitate to   comment, I would love to have your opinion.

Hope you found the above useful!"
49,machinelearning,gpt-4,top,2023-04-27 08:20:26,[P] Godot+RWKV standalone prebuilt binary (ubuntu/nvidia),hazardous1222,False,0.96,184,130e31o,https://www.reddit.com/r/MachineLearning/comments/130e31o/p_godotrwkv_standalone_prebuilt_binary/,29,1682583626.0,"# RWKV+Godot

## What

### Godot 

The Godot Engine is a free, all-in-one, cross-platform game engine that makes it easy for you to create 2D and 3D games.

### RWKV

RWKV is an RNN with Transformer-level LLM performance, which can also be directly trained like a GPT transformer (parallelizable). And it's 100% attention-free. You only need the hidden state at position t to compute the state at position t+1.

### RWKV-CPP-CUDA

RWKV-CPP-CUDA is a c++/cuda library I created that implements the RWKV inference code in pure cuda. This allows for compiled code with no torch or python dependencies, while allowing the full use of GPU acceleration.
The code implements 8bit inference, allowing for quick and light inference.

### Godot+RWKV

Godot+RWKV is a Godot module that I developed using RWKV-CPP-CUDA, and allows the development of games and programs using RWKV to be developed and distributed using godot, without the need to install complex environments and libraries, for both developers and consumers.

## Why

* I felt I could achieve it
* Its something thats needed to advance the use of AI in consumer devices
* The lols
* Attention, because I didnt get much growing up, and RWKV has none
* ADHD hyperfocus

## Where

[Module Repository](https://github.com/harrisonvanderbyl/godot-rwkv)

[RWKV standalone c++/cuda library](https://github.com/harrisonvanderbyl/rwkv-cpp-cuda)

[Prebuilt Godot Executable](https://github.com/harrisonvanderbyl/godot-rwkv/actions/runs/4816463552)

[Model Converter](https://github.com/harrisonvanderbyl/rwkv-cpp-cuda/tree/main/converter)

[Tokenizer Files](https://github.com/harrisonvanderbyl/rwkv-cpp-cuda/tree/main/include/rwkv/tokenizer/vocab)

[Unconverted Models : 14/7/3/1.5B finetuned on all your favorite instruct datasets, in both chinese and english](https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main)

[Your Will To Live](https://i.redd.it/b39ai2k1acwa1.jpg)

[Rick Astley](https://www.youtube.com/watch?v=dQw4w9WgXcQ)

## How

* Download a model (preconverted models pending)
* Convert the model (requires torch to pack tensors into raw binary)
* Download the tokenizer files
* Create a game in godot
* Distribute the game
* Profit

Example Code:

```python
extends Node2D
var zrkv = GodotRWKV.new()

# Called when the node enters the scene tree for the first time.
func _ready():
	zrkv.loadModel(""/path/to/model.bin"")
	zrkv.loadTokenizer(""/path/to/folder/with/vocab/"")
	zrkv.loadContext(""Hello, my name is Nathan, and I have been trying to reach you about your cars extended warrenty."")
# Called every frame. 'delta' is the elapsed time since the previous frame.
func _process(delta):
	# number of tokens to generate, temperature, tau
	print(zrkv.forward(5,0.9,0.7))
```

## When

* Pls submit PRs if you want them sooner

Soon:

* Windows support (Just needs some scons magic)
* AMD Support (Just needs some HIPify magic)
* CPU mode (Just needs some ggml)
* CPU offload (needs ggml and effort)
* Preconverted models

Later:

* INT4"
50,machinelearning,gpt-4,top,2019-08-21 20:56:25,"[D] OpenAI's official 774M GPT-2 model released. 1.5B model might be released, dependent on 4 research organizations.",permalip,False,0.91,174,ctmxzj,https://www.reddit.com/r/MachineLearning/comments/ctmxzj/d_openais_official_774m_gpt2_model_released_15b/,65,1566420985.0,"Here are the links:

[https://openai.com/blog/gpt-2-6-month-follow-up/](https://openai.com/blog/gpt-2-6-month-follow-up/)

[https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)"
51,machinelearning,gpt-4,top,2022-12-15 23:57:18,[P] Medical question-answering without hallucinating,tmblweeds,False,0.94,175,zn0juq,https://www.reddit.com/r/MachineLearning/comments/zn0juq/p_medical_questionanswering_without_hallucinating/,50,1671148638.0,"**tl;dr**I built a site that uses GPT-3.5 to answer natural-language medical questions using peer-reviewed medical studies.

**Live demo:** [**https://www.glaciermd.com/search**](https://www.glaciermd.com/search?utm_campaign=reddit_post_1)

**Background**

I've been working for a while on building a better version of WebMD, and I recently started playing around with LLMs, trying to figure out if there was anything useful there.

The problem with the current batch of ""predict-next-token"" LLMs is that they hallucinate—you can ask ChatGPT to answer medical questions, but it'll either

1. Refuse to answer (not great)
2. Give a completely false answer (really super bad)

So I spent some time trying to coax these LLMs to give answers based on a very specific set of inputs (peer-reviewed medical research) to see if I could get more accurate answers. And I did!

The best part is you can actually trace the final answer back to the original sources, which will hopefully instill some confidence in the result.

Here's how it works:

1. User types in a question
2. Pull top \~800 studies from Semantic Scholar and Pubmed
3. Re-rank using `sentence-transformers/multi-qa-MiniLM-L6-cos-v1`
4. Ask `text-davinci-003` to answer the question based on the top 10 studies (if possible)
5. Summarize those answers using `text-davinci-003`

Would love to hear what people think (and if there's a better/cheaper way to do it!).

\---

**UPDATE 1:** So far the #1 piece of feedback has been that I should be *way* more explicit about the fact that this is a proof-of-concept and not meant to be taken seriously. To that end, I've just added a screen that explains this and requires you to acknowledge it before continuing.

&#x200B;

https://preview.redd.it/jrt0yv3rfb6a1.png?width=582&format=png&auto=webp&s=38021decdfc7ed4bc3fe8caacaee2d09cd9b541e

Thoughts?

**Update 2:** Welp that's all the $$$ I have to spend on OpenAI credits, so the full demo isn't running anymore. But you can still follow the link above and browse existing questions/answers. Thanks for all the great feedback!"
52,machinelearning,gpt-4,top,2023-03-10 08:50:32,[D] Is ML a big boys game now?,TheStartIs2019,False,0.83,177,11njpb9,https://www.reddit.com/r/MachineLearning/comments/11njpb9/d_is_ml_a_big_boys_game_now/,146,1678438232.0,"As much as I enjoy ML as a whole, I am a bit skeptical of the future for individuals. With OpenAI trying to monopolize the market along with Microsoft, which part remains for the small time researchers/developers?

It seems everything now is just a ChatGPT wrapper, and with GPT-4 around the corner I assume itll be even more prominent.

What are your thoughts?"
53,machinelearning,gpt-4,top,2023-12-22 10:54:20,[P] I tried to teach Mistral 7B a new language (Sundanese) and it worked! (sort of),nero10578,False,0.96,173,18ocba4,https://www.reddit.com/r/MachineLearning/comments/18ocba4/p_i_tried_to_teach_mistral_7b_a_new_language/,32,1703242460.0,"[Nero10578/Mistral-7B-Sunda-v1.0 · Hugging Face](https://huggingface.co/Nero10578/Mistral-7B-Sunda-v1.0)

I'll start by saying I am not a machine learning expert and I am new to this since getting into LLMs as it got popular since LLaMa release. So, I don't know much of the technicalities although I am willing to learn.

Seeing that even Bing chat which is powered by chatGPT-4 couldn't speak in Sundanese when asked, I thought of trying to teach Mistral-7B Sundanese using just QLora training. It surprisingly worked out pretty well for how little data I had to train it with.

Why Sundanese? Because I can speak it and it is a regional language in Indonesia that isn't used much if at all on the internet so there was basically almost no chance it was trained well on any of these LLM models coming out.

This is more of an exercise to see if a small open-source model like Mistral 7B can be trained to learn a new very obscure language that is apparently never there in the original dataset. More details on the dataset and training settings are in the huggingface model card. Please ask me if you have any questions. Also, I have no idea how I am supposed to name the version, but I just called this 1.0 because the 0.x versions are basically me trial and erroring burning up electricity and heating my place for quite a while.

The resulting model is actually surprisingly coherent and translates knowledge in English to Sundanese decently. Although it is a bit unstable in that sometimes it replies with incorrect information or irrelevant information, which is probably because of the limited dataset. Next steps are going to be to get more datasets to train on, maybe translating the alpaca dataset to Sundanese and training on that.

For the comparisons, I understand that almost no one here understands Sundanese so it's best if you copy paste these to google translate to see what it's saying if you're interested.

**Comparisons**

We can see that without the finetune the model just defaults to replying in Indonesian even though the character card is in Sundanese and says to only reply in Sundanese. This is probably because the model is trained on data that correlates that Sundanese is spoken in Indonesia where people speak Indonesian. It also definitely works best on answering random questions if you asked trivia questions that has a chance to be on Wikipedia.

Mistral-7B-v0.1:

    AI
    Naon anu abdi tiasa ngabantosan anjeun?
    
    You
    pang jelaskeun naon balapan mobil formula 1 te
    
    AI
    Mobil Formula 1 adalah mobil yang digunakan dalam pertandingan mobil Formula 1. Mobil ini memiliki desain yang unik dan kuat, dan dioperasikan oleh pemimpin terbaik dari seluruh dunia. Pertandingan mobil Formula 1 menjadi salah satu kegiatan olahraga terpopuler di dunia, dan menarik banyak penonton.
    
    You
    Ai fernando alonso te saha? Pang terangkeun
    
    AI
    Fernando Alonso adalah seorang pemimpin mobil Formula 1 yang berasal dari Spanyol. Beliau telah menyegarkan piala mobil Formula 1 dua kali, dan telah menjadi salah satu pemimpin terbaik di dunia mobil Formula 1.

Mistral-7B-Sunda-v1.0

    AI
    Naon anu abdi tiasa ngabantosan anjeun?
    
    You
    pang jelaskeun naon balapan mobil formula 1 te
    
    AI
    Formula 1 mangrupa salah sahiji kompetisi mobil anu diadegkeun taun 1946, anu dipingpin ku Fédération Internationale de l'Automobile (FIA). Kompetisi ieu dilaksanakeun unggal usum ti bulan Maret nepi ka Nopémber. Balapan Formula 1 biasana dilakukeun di laporan anu panjangna kurang leuwih 5-7 km, anu mana mobil-mobil anu parigel dibawa ku tim-tim anu aya di dunia.
    
    You
    Ai fernando alonso te saha? Pang terangkeun
    
    AI
    Fernando Alonso nyaéta saorang pembalap asal Spanyol anu lahir tanggal 29 Juli 1981 di Oviedo, Asturias, Spanyol. Anjeuna mimiti karir profesionalna di taun 1999, nalika anjeuna meunang gelar juara GP2 Series taun 2000. Dina taun 2001, Alonso mimiti maén di Formula One, nalika anjeuna ditandatanganan kontrak ku Tim Minardi. Tapi, sabada dua kali balapan, anjeuna diganti ku Alexander Wurz. Taun 2003, Alonso ganti tim ka Tim Renault, anu mana anjeuna bisa nyitak rékor anyar minangka pembalap paling umurna anu bisa meunang podium di Grand Prix Monaco. Podium éta ogé jadi hiji-hijina podium anu didamel ku Alonso di musim 2003.

The original Mistral 7B model also devolves to random rambling when conversed in Sundanese about things that are not Wikipedia trivia questions. Whereas the finetuned model actually replied coherently and mostly correctly.

Mistral-7B-v0.1:

    AI
    Naon anu abdi tiasa ngabantosan anjeun?
    
    You
    kumaha carana masak endog goreng
    
    AI
    Kutu kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas,

Mistral-7B-Sunda-v1.0

    AI
    Naon anu abdi tiasa ngabantosan anjeun?
    
    You
    kumaha carana masak endog goreng
    
    AI
    Endog goreng bisa dijieun ku cara méré endog kana minyak panas. Anda bisa menggunakan wajan atau pemangkok untuk memasaknya. Anda juga bisa menambahkan bumbu seperti bawang bombay, lada, dan uyah untuk menambah rasa.

&#x200B;"
54,machinelearning,gpt-4,top,2023-03-23 22:56:31,"[D] ""Sparks of Artificial General Intelligence: Early experiments with GPT-4"" contained unredacted comments",QQII,False,0.93,175,1200lgr,https://www.reddit.com/r/MachineLearning/comments/1200lgr/d_sparks_of_artificial_general_intelligence_early/,68,1679612191.0,"Microsoft's research paper exploring the capabilities, limitations and implications of an early version of GPT-4 was [found to contain unredacted comments by an anonymous twitter user.](https://twitter.com/DV2559106965076/status/1638769434763608064) ([threadreader](https://threadreaderapp.com/thread/1638769434763608064.html), [nitter](https://nitter.lacontrevoie.fr/DV2559106965076/status/1638769434763608064), [archive.is](https://archive.is/1icMv), [archive.org](https://web.archive.org/web/20230323192314/https://twitter.com/DV2559106965076/status/1638769434763608064)) 

- Commented section titled ""Toxic Content"": https://i.imgur.com/s8iNXr7.jpg
- [`dv3` (the interval name for GPT-4)](https://pastebin.com/ZGMzgfqd)
- [`varun`](https://pastebin.com/i9KMFcy5) 
- [commented lines](https://pastebin.com/Aa1uqbh1)

[arxiv](https://arxiv.org/abs/2303.12712), [original /r/MachineLearning thread](https://www.reddit.com/r/MachineLearning/comments/11z3ymj/r_sparks_of_artificial_general_intelligence_early), [hacker news](https://twitter.com/DV2559106965076/status/1638769434763608064)"
55,machinelearning,gpt-4,top,2023-07-21 05:59:38,[N] HuggingFace reported to be reviewing term sheets for a funding round that could raise at least $200M at a valuation of $4B.,hardmaru,False,0.97,172,155f2k0,https://www.reddit.com/r/MachineLearning/comments/155f2k0/n_huggingface_reported_to_be_reviewing_term/,31,1689919178.0,"Link to article: https://www.forbes.com/sites/alexkonrad/2023/07/13/ai-startup-hugging-face-raising-funds-4-billion-valuation/

**AI Startup Hugging Face Is Raising Fresh VC Funds At $4 Billion Valuation**

Hugging Face is raising a new funding round that is expected to value the high-flying AI startup at $4 billion, multiple sources with knowledge of the matter tell Forbes.

The Series D funding round is expected to raise at least $200 million, two sources said, with Ashton Kutcher’s venture capital firm, Sound Ventures, currently leading an investor scrum. But cofounder and CEO Clément Delangue is shopping around as the company has received multiple offers this week, four sources added.

Delangue was expected to pick a preferred offer as soon as Friday, according to another source, who noted that the situation was still fluid, meaning no agreement has been reached, and the numbers involved could change. Several other sources, who asked to remain anonymous as they weren’t authorized to talk about the deal, said that Hugging Face could seek to raise more, as much as $300 million, while existing investors could still attempt to take the round in a last-minute bid. GV, the venture firm backed by Alphabet, and DFJ were said to be looking at the round, one source added.

Hugging Face didn’t respond to requests for comment. GV declined to comment. Coatue, DFJ, Kutcher, and Lux also didn’t respond.

The anticipated funding is the latest exclamation point in a cash frenzy for promising AI companies, particularly those providing large-language models, or LLMs, that power them. Just over a year ago, Hugging Face raised $100 million in a Series C round led by Lux Capital; Coatue and Sequoia were new investors in that round, joining A.Capital Ventures and Addition. The company had attained a $2 billion valuation in that round despite taking in less than $10 million in revenue in 2021. Its revenue run rate has spiked this year and now sits at around $30 million to $50 million, three sources said — with one noting that it had more that tripled compared to the start of the year.

Named after the emoji of a smiling face with jazz hands, Brooklyn-based Hugging Face has grown quickly by offering what Delangue has described as a “GitHub for machine learning.” It is a central company in a growing movement of AI models that are open sourced, meaning that anyone can access and modify them for free. Hugging Face makes money by charging for security and corporate tools on top of a hub of hundreds of thousands of models trained by its community of developers, including the popular Stable Diffusion model that forms the basis for another controversial AI unicorn, Stability AI. (On Thursday, a Stability AI cofounder sued CEO Emad Mostaque, alleging he was tricked into selling his stake for next to nothing.) Per a Forbes profile in 2022, Bloomberg, Pfizer and Roche were early Hugging Face customers.

Earlier this year, Delangue warned that model providers reliant on paying huge sums to Big Tech’s cloud providers would function as “cloud money laundering.” But training and maintaining models — and building enterprise-grade businesses around them — remains costly. In June, Inflection AI raised $1.3 billion, in part to manage its Microsoft compute and Nvidia hardware costs; the same month, foundation model rival Cohere raised $270 million. Anthropic, maker of the recently-released ChatGPT rival Claude 2, raised $450 million in May. OpenAI closed its own $300 million share sale in April, then raised $175 million for a fund to back other startups a month later, per a filing. Adept became a unicorn after announcing a $350 million fundraise in March. Stability AI, meanwhile, met with a number of venture firms in the spring seeking its own new up-round, industry sources said.

At a $4 billion valuation, Hugging Face would vault to one of the category’s highest-valued companies, matching Inflection AI and just behind Anthropic, reported to have reached closer to $5 billion. OpenAI remains the giant in the fast-growing category, Google, Meta and infrastructure companies like Databricks excluded; while its ownership and valuation structure is complex, the company’s previous financings implied a price tag in the $27 billion to $29 billion range.

Speaking for another Forbes story on the breakout moment for generative AI tools, Delangue predicted, “I think there’s potential for multiple $100 billion companies.”"
56,machinelearning,gpt-4,top,2022-11-17 15:32:23,[R] RWKV-4 7B release: an attention-free RNN language model matching GPT-J performance (14B training in progress),bo_peng,False,0.98,173,yxt8sa,https://www.reddit.com/r/MachineLearning/comments/yxt8sa/r_rwkv4_7b_release_an_attentionfree_rnn_language/,23,1668699143.0,"Hi everyone. I have finished training RWKV-4 7B (an attention-free RNN LLM) and it can match GPT-J (6B params) performance. **Maybe RNN is already all you need** :)

https://preview.redd.it/71cce2y75j0a1.png?width=1336&format=png&auto=webp&s=5af76abc4f42fd63f0194ee93f78db01c1b21d97

These are RWKV BF16 numbers. RWKV 3B is better than GPT-neo 2.7B on everything (smaller RWKV lags behind on LAMBADA). Note GPT-J is using rotary and thus quite better than GPT-neo, so I expect RWKV to surpass it when both are at 14B.

Previous discussion: [https://www.reddit.com/r/MachineLearning/comments/xfup9f/r\_rwkv4\_scaling\_rnn\_to\_7b\_params\_and\_beyond\_with/](https://www.reddit.com/r/MachineLearning/comments/xfup9f/r_rwkv4_scaling_rnn_to_7b_params_and_beyond_with/)

RWKV has both RNN & GPT mode. The RNN mode is great for inference. The GPT mode is great for training. Both modes are faster than usual transformer and saves VRAM, because the self-attention mechanism is replaced by simpler (almost linear) formulas. Moreover the hidden state is tiny in the RNN mode and you can use it as an embedding of the whole context.

Github: [https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM)

Checkpt: [https://huggingface.co/BlinkDL/rwkv-4-pile-7b](https://huggingface.co/BlinkDL/rwkv-4-pile-7b)

14B in progress (thanks to EleutherAI and Stability). Nice spike-free loss curves:

https://preview.redd.it/w4g7oqmi5j0a1.png?width=868&format=png&auto=webp&s=346d420fb879fd06470079eeaf2e4d3739536406"
57,machinelearning,gpt-4,top,2023-10-12 19:28:30,[R] SWE-bench: Can Language Models Resolve Real-world GitHub issues?,ofirpress,False,0.97,169,176f89x,https://www.reddit.com/r/MachineLearning/comments/176f89x/r_swebench_can_language_models_resolve_realworld/,27,1697138910.0,"We have a new benchmark out called [SWE-bench (arxiv)](https://arxiv.org/abs/2310.06770) 

It challenges LMs to solve real GitHub issues (feature requests & bug reports) from popular Python repos.

Answers are validated using unit tests we crawled from those repos.

The benchmark at [swebench.com/](https://www.swebench.com/) shows that even the strongest models, such as Claude 2 and GPT-4, get less than 5% accuracy.

&#x200B;

We are here to answer any questions you may have."
58,machinelearning,gpt-4,top,2023-01-03 12:53:26,[R] Massive Language Models Can Be Accurately Pruned in One-Shot,starstruckmon,False,0.99,166,1027geh,https://www.reddit.com/r/MachineLearning/comments/1027geh/r_massive_language_models_can_be_accurately/,50,1672750406.0,"Paper : [https://arxiv.org/abs/2301.00774](https://arxiv.org/abs/2301.00774)

Abstract :

>We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. When executing SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, we can reach 60% sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches."
59,machinelearning,gpt-4,top,2023-03-23 03:47:26,[P] GPT-4 powered full stack web development with no manual coding,CryptoSpecialAgent,False,0.89,158,11z7r4c,https://www.reddit.com/r/MachineLearning/comments/11z7r4c/p_gpt4_powered_full_stack_web_development_with_no/,50,1679543246.0,"[https://www.youtube.com/watch?v=lZj63vjueeU](https://www.youtube.com/watch?v=lZj63vjueeU)

What do you all think of this approach to full stack gpt-assisted web development? In a sense its no code because the human user does not write or even edit the code - but in a sense its the opposite, because only an experienced web developer or at least a product manager would know how to instruct GPT in a useful manner.

\*\*\* We are seeking donations to ensure this project continues and, quite literally, keep the lights on. Cryptos, cash, cards, openai access tokens with free credits, hardware, cloud GPUs, etc... all is appreciated. Please DM to support this really cool open source project \*\*\*

PS. I'm the injured engineer who made this thing out of necessity, because i injured my wrist building an AI platform that's become way too big for one engineer to maintain. So AMA :)"
60,machinelearning,gpt-4,top,2023-07-05 13:56:50,"[P] nanoT5 v2 - In ~16 hours on a single GPU, we reach similar performance to the model trained on 150x more data!",korec1234,False,0.98,157,14rbevh,https://www.reddit.com/r/MachineLearning/comments/14rbevh/p_nanot5_v2_in_16_hours_on_a_single_gpu_we_reach/,35,1688565410.0,"Inspired by Andrej Karpathy's nanoGPT we improve the repo for pre-training T5 model in PyTorch. **In \~16 hours on a single GPU, we achieve 40.7 RougeL on the SNI benchmark, compared to 40.9 RougeL of the original model pre-trained on 150x more data!**

Key upgrade in nanoT5 v2:  We've leveraged BF16 precision and utilise a simplified T5 model implementation based on Huggingface's design.  New implementation is easy-to-read and compatible with the HF's checkpoints. **Pre-training is now 2x faster than our previous version.**

We test different pre-training durations: 4, 8, 12, 16, 20, and 24 hours. A sweet spot at 16 hours! It has comparable performance to the original model trained on 150x more data! **Time & Compute-efficient, and no compromise on quality.**

We share the configs, checkpoints, training logs, as well as our **negative attempts** towards improving pre-training efficiency. **Advanced optimizers like Lion, Sophia, ALiBi positional embeddings, and FP16 mixed precision training didn't yield expected benefits.**

&#x200B;

We are keen to hear your suggestions to improve the codebase further.

&#x200B;

Github: [https://github.com/PiotrNawrot/nanoT5](https://github.com/PiotrNawrot/nanoT5)

Twitter: [https://twitter.com/p\_nawrot/status/1676568127532945408](https://twitter.com/p_nawrot/status/1676568127532945408)

https://preview.redd.it/kyku7ehqj5ab1.png?width=1120&format=png&auto=webp&s=ad551c026455c2c430684f669f10438da8905342"
61,machinelearning,gpt-4,top,2024-01-19 21:01:45,[R] Self-Rewarding Language Models - Meta 2024,Singularian2501,False,0.97,148,19atnu0,https://www.reddit.com/r/MachineLearning/comments/19atnu0/r_selfrewarding_language_models_meta_2024/,24,1705698105.0,"Paper: [https://arxiv.org/abs/2401.10020](https://arxiv.org/abs/2401.10020)

Github: [https://github.com/lucidrains/self-rewarding-lm-pytorch](https://github.com/lucidrains/self-rewarding-lm-pytorch)

Abstract:

>We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continually improve in both axes. 

https://preview.redd.it/l7vav40qngdc1.jpg?width=1344&format=pjpg&auto=webp&s=9dce97a69f2ede66d6dabf6abbcfc75bf0e94f19

https://preview.redd.it/fuooe70qngdc1.jpg?width=1180&format=pjpg&auto=webp&s=a88fcf1c765ff42c18091889f5b14cd371248760"
62,machinelearning,gpt-4,top,2023-06-02 01:01:38,[R] Blockwise Parallel Transformer for Long Context Large Models,IxinDow,False,0.98,143,13xyvgt,https://www.reddit.com/r/MachineLearning/comments/13xyvgt/r_blockwise_parallel_transformer_for_long_context/,30,1685667698.0,"[https://arxiv.org/pdf/2305.19370.pdf](https://arxiv.org/pdf/2305.19370.pdf)

It's honest Transformer and honest attention. No cheating.

>**We use the same model architecture as the original Transformer, but with a different way of organizing the compute.**

From conclusion:

>Our approach enables processing longer input sequences while maintaining or improving performance. Through extensive experiments, we demonstrate its effectiveness, achieving **up to 4x memory reduction than memory-efficient Transformers**. Our contributions include a practical method for long context lengths in large Transformer models.

Abstract:

>Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, **BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods**. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance

[Maximum context lengths \(number of tokens\) achieved \(for training\) with different sizes of model on different hardware](https://preview.redd.it/7p7efurkii3b1.png?width=1372&format=png&auto=webp&s=fd4b821c94269e0b92237f5888cedf93524442e4)

# Explanations from authors' twitter (@haoliuhl):

Rabe et al and FlashAttention Dao et al introduced a memory-efficient attention technique that utilizes the well-established online softmax to compute self-attention block by block, allowing computing exact self-attention with linear memory complexity. Despite reduced memory needs in self-attention, a **challenge remains with the large parameter count and high-dimensional vectors of the feedforward network.** This becomes the primary memory issue when using memory-efficient attention. To overcome this challenge, we observed that **merging the computation of feedforward and attention block by block eliminates the need for performing the feedforward step on the entire sequence, which significantly cut memory cost**.

[We use the same model architecture as the original Transformer but with a different way of organizing the compute. In the diagram, we explain this by showing that for the bottom first incoming input block, we project it into query; then we iterate over the same input sequence positioned above the bottom row, and project it to key and value. These query, key and value are used to compute self-attention \(yellow box\), whose output is pass to feedforward network \(cyan box\), followed by a residual connection. In our proposed approach, this process is then repeated for the other incoming input blocks.](https://preview.redd.it/h277g94bki3b1.png?width=966&format=png&auto=webp&s=60ac0af06644f473ec7021b477e9113d9cc8541d)

In terms of speed, using high-level Jax operations, BPT enables high-throughput training that matches or surpasses the speed of vanilla and memory efficient Transformers. Porting our method to low-level kernels in CUDA or Triton will achieve maximum speedup.

https://preview.redd.it/0t0i00qpki3b1.png?width=1424&format=png&auto=webp&s=56f45cfc41578ba9b2084ad5ecefe66a21bee987"
63,machinelearning,gpt-4,top,2020-07-23 13:21:49,[D] The cost of training GPT-3,yusuf-bengio,False,0.96,141,hwfjej,https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/,35,1595510509.0,"There are two sources that estimate the cost of training GPT-3 at [$12 million](https://venturebeat.com/2020/06/11/openai-launches-an-api-to-commercialize-its-research/) and [$4.6 million](https://lambdalabs.com/blog/demystifying-gpt-3/). And I am a bit confused about how they got those numbers.

The used Microsoft Azure cloud offers, via InfiniBand connectable, [8xV100 machines](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/) at $10.7957/hour (1 year reserved), which translates to around $260 per day.

In the paper there is a sentence saying that they used half-precision and loss-scaling for training. One V100 can deliver up to [120 Teraflop/s](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html) using float16. Per machine (8xV100), this translates to 960 Teraflop/s in theory.  Let's assume in practice we can utilize our compute resources at \~50%, which gives us around 500 Teraflop/s per machine.

As we know from the paper it takes 3640 Petaflop/s-days to train the largest 175B model, which translates to a training run of 7280 days (or \~20 years) on a single 8xV100 machine. In terms of cost, this would be **$1.9 million**. 

Let's say we don't want to wait 20 years, so if we connect 64 of such 8xV100 machines we can reduce the training time to around 4 months (costs might go up due to reduced compute efficiency of the multi-node communication).

My question is, is the calculation above roughly accurate (Azure hourly costs, assumed compute utilization)?

After reading all the implementation details and optimization of the paper, I also began to think about development costs. Setting up a fast training pipeline to utilize the compute resources efficiently is not trivial given the size of the model and the resulting need to model parallelism."
64,machinelearning,gpt-4,top,2021-04-27 16:29:15,[P] We gave GPT-3 random ingredients and cooked the recipe it came up with (Video),ykilcher,False,0.87,126,mzsdiw,https://www.reddit.com/r/MachineLearning/comments/mzsdiw/p_we_gave_gpt3_random_ingredients_and_cooked_the/,20,1619540955.0,"[https://youtu.be/hIoCn\_9QTVU](https://youtu.be/hIoCn_9QTVU)

We went to the store and bought a set of completely random ingredients and had OpenAI's GPT-3 come up with a recipe, which we then cooked and ate.

&#x200B;

Our Rules:

1. All Vegan

2. Follow the recipe as closely as possible

3. We must finish our plates

&#x200B;

The Recipe:

1. Boil the potatoes and carrots.

2. In the meantime, prepare the VEGAN minced meat, or use pre-cooked soy meat. 

3. Then fry the VEGAN butter, add the garlic, and the mushrooms, and stir for 2 minutes. 

4. Add the soy cream, stir and cook for three minutes. 

5. Add the pickles, tomatoes, and beans, stir and simmer for five minutes. 

6. Cut the bread in small squares and fry in the vegan butter until golden brown.

7. Cut the limes into cubes and squeeze the juice into the bean mixture. 

8. Add the soy sauce, parsley, salt, pepper, cumin, cilantro, and dried figs. Stir, and add the kale.

9. Pour the bean mix into a blender. 

10. Bake for 5 minutes in the oven at 180C. 

11. Cut the sweet potatoes in cubes, and add to a pot with the remaining butter. Add the red beans mixture. 

12. Cut the bell pepper into cubes and add to the pot. 

13. Add the VEGAN minced meat, and cook in the oven at 180C for 10 minutes. 

14. Add the avocado. 

15. Add the chickpeas. 

16. Add the chocolate.

17. Serve on bread with mustard and pommegrenade on top.

&#x200B;

VIDEO OUTLINE:

0:00 - The Plan

2:15 - Ingredients

4:05 - What is GPT-3?

6:10 - Let's cook

12:25 - The Taste Test

&#x200B;

GPT-3 on Wikipedia: [https://en.wikipedia.org/wiki/GPT-3](https://en.wikipedia.org/wiki/GPT-3)

GPT-3 Paper: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)"
65,machinelearning,gpt-4,top,2024-02-19 18:02:36,"[R] In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss - AIRI, Moscow, Russia 2024 - RMT 137M a fine-tuned GPT-2 with recurrent memory is able to find 85% of hidden needles in a 10M Haystack!",Singularian2501,False,0.88,120,1autvwq,https://www.reddit.com/r/MachineLearning/comments/1autvwq/r_in_search_of_needles_in_a_10m_haystack/,25,1708365756.0,"Paper: [https://arxiv.org/abs/2402.10790](https://arxiv.org/abs/2402.10790) 

Abstract:

>This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce **BABILong, a new benchmark** **designed to assess model capabilities in extracting and processing distributed facts within extensive texts**. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to **10\^4** elements. In contrast, **fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to** **10\^7** elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a **significant improvement in the processing capabilities for long sequences.** 

https://preview.redd.it/0o4207a70ljc1.jpg?width=577&format=pjpg&auto=webp&s=2bfac07872020de222b4bf99f837aa398b778afc

https://preview.redd.it/2ff82da70ljc1.jpg?width=1835&format=pjpg&auto=webp&s=acc1409f5b9bcd07f9b5ff8a3890cc1b15b5c8ed

https://preview.redd.it/ld69p7a70ljc1.jpg?width=1816&format=pjpg&auto=webp&s=fdd72c1a87742f525fa352723bcd1a0f4f000638

https://preview.redd.it/7vn4gba70ljc1.jpg?width=900&format=pjpg&auto=webp&s=c8d08bb85a6699e5b451e01bf615379db1fcbdca"
66,machinelearning,gpt-4,top,2023-08-28 07:02:36,"[D] Google Gemini Eats The World – Gemini Smashes GPT-4 By 5X, The GPU-Poors",hardmaru,False,0.72,120,163ewre,https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini,61,1693206156.0,
67,machinelearning,gpt-4,top,2023-10-18 15:36:53,[R] LLMs can threaten privacy at scale by inferring personal information from seemingly benign texts,bmislav,False,0.85,116,17atob7,https://www.reddit.com/r/MachineLearning/comments/17atob7/r_llms_can_threaten_privacy_at_scale_by_inferring/,35,1697643413.0,"Our latest research shows an emerging privacy threat from LLMs beyond training data memorization. We investigate how LLMs such as GPT-4 can infer personal information from seemingly benign texts. The key observation of our work is that the best LLMs are almost as accurate as humans, while being at least 100x faster and 240x cheaper in inferring such personal information.  

We collect and label real Reddit profiles, and test the LLMs capabilities in inferring personal information from mere Reddit posts, where GPT-4 achieves >85% Top-1 accuracy. Mitigations such as anonymization are shown to be largely ineffective in preventing such attacks. 

Test your own inference skills against GPT-4 and learn more: [https://llm-privacy.org/](https://llm-privacy.org/)  
Arxiv paper: [https://arxiv.org/abs/2310.07298](https://arxiv.org/abs/2310.07298)   
WIRED article: [https://www.wired.com/story/ai-chatbots-can-guess-your-personal-information/](https://www.wired.com/story/ai-chatbots-can-guess-your-personal-information/)"
68,machinelearning,gpt-4,top,2024-01-06 16:23:04,[D] Incredible results with Long Agent Tree Search with open source models,ArtZab,False,0.98,117,1903k24,https://www.reddit.com/r/MachineLearning/comments/1903k24/d_incredible_results_with_long_agent_tree_search/,9,1704558184.0,"Hello,

I saw GPT-4 with Long Agent Tree Search topping the HumanEval with a 94.4% pass@1 for a few weeks now. [https://paperswithcode.com/sota/code-generation-on-humaneval](https://paperswithcode.com/sota/code-generation-on-humaneval)

&#x200B;

The authors of the [original paper](https://arxiv.org/abs/2310.04406) posted their code in their [official github repo](https://github.com/andyz245/LanguageAgentTreeSearch) . I had to change some code to try it out with CodeLlama-7b and the human eval with pass@1 and only 2 max iterations increases HumanEval score from 37% to about 70%.

This is some incredible results in my opinion because this score is higher than GPT-3.5 with only a 7b model. I assume more testing has to be done, but nevertheless I am surprised people are not talking more about this."
69,machinelearning,gpt-4,top,2023-04-20 01:30:47,[D] GPT-3T: Can we train language models to think further ahead?,landongarrison,False,0.91,119,12shf18,https://www.reddit.com/r/MachineLearning/comments/12shf18/d_gpt3t_can_we_train_language_models_to_think/,62,1681954247.0,"In a recent talk done by Sebastian Bubeck called “Sparks of AGI: Early experiments done with GPT-4”, Sebastian mentioned on thing in his presentation that caught my attention (paraphrased quote):

> “GPT-4 cannot plan, but this might be a limitation because it can only look one token into the future”

While very simple on the surface, this may actually be very true: what if we are training our language models to be very shallow thinkers and not actually look far enough ahead? Could single token prediction actually be a fundamental flaw?

In this repo, I try a very early experiment called GPT-3T, a model that predicts 3 tokens ahead at one time step. While incredibly simple on the surface, this could potentially be one way to overcome the planning issue that you find in GPTs. Forcing an autoregressive model to predict further ahead at scale *may* bring out much more interesting emergent behaviours than what we’ve seen in single token GPTs.

__

**Experiments**

My personal experiments are overall inconclusive on either side: I have only pre-trained a very small model (300 million params on WebText-10K) and it achieves a decent ability to generate text. However as you can see, this model heavily under optimized but I do not have the resources to carry this out further.

If anyone would like to try this experiment with more scale, I would love to get an answer to this question to improve upon this model. This repo is intended to allow anyone who would like to pre-train a GPT-3T model easily to run this experiment. From what I have seen, this has not been tried before and I am very curious to see results.

__

**Edit:** GitHub repo is buried in the comments (sorry this post will be taken down if I include it in the main post)"
70,machinelearning,gpt-4,top,2021-09-18 07:08:51,[R] Google AI Introduces Two New Families of Neural Networks Called ‘EfficientNetV2’ and ‘CoAtNet’ For Image Recognition,techsucker,False,0.92,114,pqhqjv,https://www.reddit.com/r/MachineLearning/comments/pqhqjv/r_google_ai_introduces_two_new_families_of_neural/,14,1631948931.0,"Training efficiency has become a significant factor for deep learning as the neural network models, and training data size grows. [GPT-3](https://arxiv.org/abs/2005.14165) is an excellent example to show how critical training efficiency factor could be as it takes weeks of training with thousands of GPUs to demonstrate remarkable capabilities in few-shot learning.

To address this problem, the Google AI team introduce two families of neural networks for image recognition. First is [EfficientNetV2](https://arxiv.org/abs/2104.00298), consisting of CNN (Convolutional neural networks) with a small-scale dataset for faster training efficiency such as [ImageNet1k](https://www.image-net.org/) (with 1.28 million images). Second is a hybrid model called [CoAtNet](https://arxiv.org/abs/2106.04803), which combines [convolution](https://en.wikipedia.org/wiki/Convolution) and [self-attention](https://en.wikipedia.org/wiki/Self-attention) to achieve higher accuracy on large-scale datasets such as [ImageNet21](https://www.image-net.org/) (with 13 million images) and [JFT](https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html) (with billions of images). As per the research report by Google, [EfficientNetV2](https://arxiv.org/abs/2104.00298) and [CoAtNet](https://arxiv.org/abs/2106.04803) both are 4 to 10 times faster while achieving state-of-the-art and 90.88% top-1 accuracy on the well-established [ImageNet](https://www.image-net.org/) dataset.

# [7 Min Read](https://www.marktechpost.com/2021/09/17/google-ai-introduces-two-new-families-of-neural-networks-called-efficientnetv2-and-coatnet-for-image-recognition/) | [Paper (CoAtNet)](https://arxiv.org/abs/2106.04803) | [Paper (EfficientNetV2)](https://arxiv.org/abs/2104.00298) | [Google blog](https://ai.googleblog.com/2021/09/toward-fast-and-accurate-neural.html) | [Code](https://github.com/google/automl/tree/master/efficientnetv2)

&#x200B;

https://preview.redd.it/ipmkyt7eo7o71.png?width=1392&format=png&auto=webp&s=22764f4268a6c12acb85b8b71a7331cc6446d984"
71,machinelearning,gpt-4,top,2023-05-12 22:39:24,[R] DetGPT: Detect What You Need via Reasoning,OptimalScale_2023,False,0.89,115,13fzf2m,https://www.reddit.com/r/MachineLearning/comments/13fzf2m/r_detgpt_detect_what_you_need_via_reasoning/,10,1683931164.0,"https://reddit.com/link/13fzf2m/video/fwcuwd3q9hza1/player

Throughout history, humans have dreamed of robots that could assist them with their daily lives and work. With the emergence of home assistants and OpenAI's Copilot, requests such as 'Please lower the temperature of the air conditioning' or even 'Please help me build an online store' have become possible.The emergence of GPT-4 has further demonstrated the potential of multimodal large models in visual understanding. In the open-source small model space, LLAVA and minigpt-4 have performed well in image recognition and chat, and can even suggest recipes for food images. However, these models still face significant challenges in practical implementation: they lack accurate localization capabilities and cannot provide specific locations of objects in images, nor can they understand complex human instructions to detect specific objects, making it difficult for them to perform specific tasks as requested by humans. In practical scenarios, if people could simply take a photo and ask an intelligent assistant for the correct answer to a complex problem, such a 'take a photo and ask' feature would be incredibly cool.  
To implement the ""**take a photo and ask**"" feature, robots need to have several capabilities:

1. Language understanding: the ability to listen and understand human intentions.
2. Visual understanding: the ability to understand the objects in the image.
3. Common sense reasoning: the ability to convert complex human intentions into precise and locatable targets.
4. Object localization: the ability to locate and detect corresponding objects in the image.

Currently, only a few large models (such as Google's PaLM-E) possess all four of these capabilities. However, researchers from the Hong Kong University of Science and Technology and the University of Hong Kong have proposed an open-source model called DetGPT (DetectionGPT), which only needs to fine-tune three million parameters to easily acquire complex reasoning and local object localization capabilities that can be generalized to most scenarios. This means that the model can easily recognize the objects that humans are interested in through self-knowledge reasoning and understand abstract human instructions. They have already developed a ""take a photo and ask"" demo using the model, which can be experienced online: [https://detgpt.github.io/](https://detgpt.github.io/)DetGPT allows users to operate everything with natural language without the need for complex commands or interfaces. In addition, DetGPT has intelligent reasoning and object detection capabilities, which can accurately understand user needs and intentions. For example, if a human gives a language instruction, ""I want to have a cold beverage,"" the robot first searches for a cold drink in the scene but does not find any. It then begins to think, ""There is no visible beverage. Where can I find it?"" Through its powerful common sense reasoning ability, the model realizes that the fridge is a possible location and scans the scene to successfully locate the drink!

https://preview.redd.it/ai8j05uy9hza1.png?width=1280&format=png&auto=webp&s=c8d833e2db63d0ebceb1c99aa68d89cc7fa7dcc7

  
Online demo: [https://detgpt.github.io/](https://detgpt.github.io/) 

Open-source code: [https://github.com/OptimalScale/DetGPT](https://github.com/OptimalScale/DetGPT)

&#x200B;

## Online demo: [https://detgpt.github.io/](https://detgpt.github.io/)

Feeling thirsty in the summer? DetGPT easily understands and finds the refrigerator with the image of where the iced beverages are.

https://preview.redd.it/kiiv4tb1ahza1.jpg?width=1280&format=pjpg&auto=webp&s=49a055fafd1c4e50cea46723bc567896ec60499e

Need to wake up early tomorrow? DetGPT makes it easy with an electronic alarm clock.

https://preview.redd.it/0lby9hh2ahza1.png?width=1280&format=png&auto=webp&s=e6fc77356d080fe755310dbc74879ac4f7a8b894

Do you suffer from hypertension and fatigue? Are you unsure of what fruits to buy at the market to help alleviate your symptoms? DetGPT acts as your nutrition teacher and provides guidance on which fruits can help relieve hypertension.

https://preview.redd.it/c1r7kwv3ahza1.png?width=1280&format=png&auto=webp&s=169fb015df8e9973c48a26a35caeb5892ce1d92f

Stuck in the Zelda game and can't pass it? DetGPT helps you disguise yourself and get past the challenges in the Gerudo Town.

https://preview.redd.it/wdny0v55ahza1.png?width=1280&format=png&auto=webp&s=070de46239405993eefeb5112bd4a459baec94df

Unsure of potential dangers in your surroundings within the range of the image? DetGPT acts as your safety officer and helps protect you from any potential risks.

https://preview.redd.it/nf64a176ahza1.png?width=1280&format=png&auto=webp&s=f6b641c2163076f5403361561c95663450227cd1

What items in the image could be dangerous for children? DetGPT still has got you covered.

https://preview.redd.it/oz8hx987ahza1.png?width=1280&format=png&auto=webp&s=b2d8ad27ff758a2d39e87fba86f7cc5a2b4a2c76

## Features of DetGPT

DetGPT has several unique features:

1. It has a significantly improved understanding of specific objects in images. Compared to previous models that use multimodal dialogues, DetGPT can retrieve and locate target objects from images based on the user's instructions, rather than simply describing the entire image.
2. It can understand complex human instructions, which lowers the barrier for users to ask questions. For example, the model can understand the question ""find fruits that can relieve hypertension?"" Traditional object detection requires humans to know the answer and pre-set the detection category, such as ""banana.""
3. DetGPT can use existing LLM knowledge to reason and accurately locate the corresponding object in the image that can solve more complex tasks. For complex tasks, such as ""fruits that can relieve hypertension,"" DetGPT can reason step by step: relieving hypertension -> potassium can relieve hypertension -> bananas are rich in potassium -> bananas can relieve hypertension -> need to identify the object banana.
4. It provides answers beyond human common sense. For some uncommon questions, such as which fruits are rich in potassium, the model can provide answers based on existing knowledge.

## A new direction: reasoning-based object detection

Traditional object detection tasks require pre-defined categories of possible objects for detection. However, providing accurate and comprehensive descriptions of the objects to be detected can be difficult and unrealistic for humans. This is due to the limitations of human memory and knowledge. For instance, a doctor may recommend that people with hypertension eat fruits rich in potassium, but may not know which specific fruits are rich in potassium, making it impossible to provide specific fruit names for the model to detect. If the question ""Identify fruits that can help alleviate hypertension"" could be directly posed to the detection model, humans would only need to take a photo, and the model could think, reason, and detect fruits rich in potassium, making the problem much simpler.Moreover, the examples of object categories provided by humans are not always comprehensive. For instance, if monitoring is required to detect behaviors that violate public order relative to public places, humans may only be able to provide a few simple scenarios, such as holding a knife or smoking. However, if the question ""detect behaviors that violate public order"" is directly posed to the detection model, the model can think and reason based on its own knowledge, thus capturing more unacceptable behaviors and generalizing to more relevant categories that need to be detected. After all, the knowledge that ordinary humans have access to is limited, and the object categories that they can provide examples of are also limited. However, if there is a big brain-like ChatGPT-like model to assist and reason, the instructions that humans need to provide will be much simpler, and the obtained answers will be much more accurate and comprehensive.To address the limitations of human instructions and their abstract nature, researchers from the Hong Kong University of Science and Technology and the University of Hong Kong have proposed a new direction called ""reasoning-based object detection."" In simple terms, humans give complex tasks, and the model can understand and reason about which objects in the image might be able to complete the task, and then detect them. For example, if a person describes ""I want to drink a cold drink, where can I find it,"" and the model sees a picture of a kitchen, it can detect the ""refrigerator."" This topic requires the perfect combination of multimodal models' image understanding ability and the rich knowledge stored in language models. It is used in fine-grained detection scenarios to accurately locate objects of interest to humans in images without pre-defined object categories.  


# The Approach

&#x200B;

https://preview.redd.it/ho9ux1pcahza1.png?width=1280&format=png&auto=webp&s=bf42e1baffa2925e8b946b191766ca116aec2fe1

The ""reasoning-based object detection"" is a challenging problem because the detector needs to understand and reason about the user's coarse-grained/abstract instructions and analyze the current visual information to locate the target object accurately. In this direction, researchers from the Hong Kong University of Science and Technology and the University of Hong Kong have conducted some preliminary explorations. Specifically, they use a pre-trained visual encoder (BLIP-2) to extract visual features from images and align the visual features to the text space using an alignment function. They use a large-scale language model (Robin/Vicuna) to understand the user's question, combined with the visual information they see, to reason about the objects that users are truly interested in. Then, they provide the object names to the pre-trained detector (Grounding-DINO) for specific location prediction. In this way, the model can analyze the image based on any user instructions and accurately predict the location of the object of interest to the user.  
It is worth noting that the difficulty here mainly lies in the fact that the model needs to achieve task-specific output formats for different specific tasks as much as possible without damaging the model's original abilities. To guide the language model to follow specific patterns and generate outputs that conform to the object detection format, the research team used ChatGPT to generate cross-modal instruction data to fine-tune the model. Specifically, based on 5000 coco images, they used ChatGPT to create a 30,000 cross-modal image-text fine-tuning dataset. To improve the efficiency of training, they fixed other model parameters and only learned cross-modal linear mapping. Experimental results show that even if only the linear layer is fine-tuned, the language model can understand fine-grained image features and follow specific patterns to perform inference-based image detection tasks, showing excellent performance.  
This research topic has great potential. Based on this technology, the field of home robots will further shine: people in homes can use abstract or coarse-grained voice instructions to make robots understand, recognize, and locate the objects they need, and provide relevant services. In the field of industrial robots, this technology will bring endless vitality: industrial robots can cooperate more naturally with human workers, accurately understand their instructions and needs, and achieve intelligent decision-making and operations. On the production line, human workers can use coarse-grained voice instructions or text input to allow robots to automatically understand, recognize, and locate the items that need to be processed, thereby improving production efficiency and quality.  
With object detection models that come with reasoning capabilities, we can develop more intelligent, natural, and efficient robots to provide more convenient, efficient, and humane services to humans. This is a field with broad prospects and deserves more attention and further exploration by more researchers.  
DetGPT supports multiple language models and has been validated based on two language models, Robin-13B and Vicuna-13B. The Robin series language model is a dialogue model trained by the LMFlow team ( https://github.com/OptimalScale/LMFlow) at the Hong Kong University of Science and Technology, achieving results competitive to Vicuna on multiple language ability evaluation benchmarks (model download: [https://github.com/OptimalScale/LMFlow#model-zoo](https://github.com/OptimalScale/LMFlow#model-zoo)). Previously, the LMFlow team trained a vertical GPT model using a consumer-grade 3090 graphics card in just 5 hours. Today, this team, in collaboration with the NLP Group at the University of Hong Kong, has brought us a multimodal surprise.  
Welcome to try our demo and open-source code!  
Online demo: [https://detgpt.github.io/](https://detgpt.github.io/) Open-source code: [https://github.com/OptimalScale/DetGPT](https://github.com/OptimalScale/DetGPT)"
72,machinelearning,gpt-4,top,2023-03-27 13:45:14,Approaches to add logical reasoning into LLMs [D],blatant_variable,False,0.89,114,123nczy,https://www.reddit.com/r/MachineLearning/comments/123nczy/approaches_to_add_logical_reasoning_into_llms_d/,111,1679924714.0,"The more I play with GPT-4 the more I am struck by how completely illogical it is. 
 
The easiest way to show this is to ask it to come up with a novel riddle and then solve it. Because you asked it to be novel, it's now out of it's training distribution and almost every time it's solution is completely wrong and full of basic logical errors.

I am curious, is anyone working on fixing this at a fundamental level? Hooking it into Wolfram alpha is a useful step but surely it still needs to be intrinsically logical in order to use this tool effectively."
73,machinelearning,gpt-4,top,2023-09-21 15:01:28,[N] OpenAI's new language model gpt-3.5-turbo-instruct can defeat chess engine Fairy-Stockfish 14 at level 5,Wiskkey,False,0.92,114,16oi6fb,https://www.reddit.com/r/MachineLearning/comments/16oi6fb/n_openais_new_language_model_gpt35turboinstruct/,178,1695308488.0,"[This Twitter thread](https://twitter.com/GrantSlatton/status/1703913578036904431) ([Nitter alternative](https://nitter.net/GrantSlatton/status/1703913578036904431) for those who aren't logged into Twitter and want to see the full thread) claims that [OpenAI's new language model gpt-3.5-turbo-instruct](https://analyticsindiamag.com/openai-releases-gpt-3-5-turbo-instruct/) can ""readily"" beat Lichess Stockfish level 4 ([Lichess Stockfish level and its rating](https://lichess.org/@/MagoGG/blog/stockfish-level-and-its-rating/CvL5k0jL)) and has a chess rating of ""around 1800 Elo."" [This tweet](https://twitter.com/nabeelqu/status/1703961405999759638) shows the style of prompts that are being used to get these results with the new language model.

I used website parrotchess\[dot\]com (discovered [here](https://twitter.com/OwariDa/status/1704179448013070560)) to play multiple games of chess purportedly pitting this new language model vs. various levels at website Lichess, which supposedly uses Fairy-Stockfish 14 according to the Lichess user interface. My current results for all completed games: The language model is 5-0 vs. Fairy-Stockfish 14 level 5 ([game 1](https://lichess.org/eGSWJtNq), [game 2](https://lichess.org/pN7K9bdS), [game 3](https://lichess.org/aK4jQvdo), [game 4](https://lichess.org/S9SGg8YI), [game 5](https://lichess.org/OqzdkDhE)), and 2-5 vs. Fairy-Stockfish 14 level 6 ([game 1](https://lichess.org/zP68C6H4), [game 2](https://lichess.org/4XKUIDh1), [game 3](https://lichess.org/1zTasRRp), [game 4](https://lichess.org/lH1EMqJQ), [game 5](https://lichess.org/mdFlTbMn), [game 6](https://lichess.org/HqmELNhw), [game 7](https://lichess.org/inWVs05Q)). Not included in the tally are games that I had to abort because the parrotchess user interface stalled (5 instances), because I accidentally copied a move incorrectly in the parrotchess user interface (numerous instances), or because the parrotchess user interface doesn't allow the promotion of a pawn to anything other than queen (1 instance). **Update: There could have been up to 5 additional losses - the number of times the parrotchess user interface stalled - that would have been recorded in this tally if** [this language model resignation bug](https://twitter.com/OwariDa/status/1705894692603269503) **hadn't been present. Also, the quality of play of some online chess bots can perhaps vary depending on the speed of the user's hardware.**

The following is a screenshot from parrotchess showing the end state of the first game vs. Fairy-Stockfish 14 level 5:

https://preview.redd.it/4ahi32xgjmpb1.jpg?width=432&format=pjpg&auto=webp&s=7fbb68371ca4257bed15ab2828fab58047f194a4

The game results in this paragraph are from using parrotchess after the forementioned resignation bug was fixed. The language model is 0-1 vs. Fairy-Stockfish level 7 ([game 1](https://lichess.org/Se3t7syX)), and 0-1 vs. Fairy-Stockfish 14 level 8 ([game 1](https://lichess.org/j3W2OwrP)).

There is [one known scenario](https://twitter.com/OwariDa/status/1706823943305167077) ([Nitter alternative](https://nitter.net/OwariDa/status/1706823943305167077)) in which the new language model purportedly generated an illegal move using language model sampling temperature of 0. Previous purported illegal moves that the parrotchess developer examined [turned out](https://twitter.com/OwariDa/status/1706765203130515642) ([Nitter alternative](https://nitter.net/OwariDa/status/1706765203130515642)) to be due to parrotchess bugs.

There are several other ways to play chess against the new language model if you have access to the OpenAI API. The first way is to use the OpenAI Playground as shown in [this video](https://www.youtube.com/watch?v=CReHXhmMprg). The second way is chess web app gptchess\[dot\]vercel\[dot\]app (discovered in [this Twitter thread](https://twitter.com/willdepue/status/1703974001717154191) / [Nitter thread](https://nitter.net/willdepue/status/1703974001717154191)). Third, another person modified that chess web app to additionally allow various levels of the Stockfish chess engine to autoplay, resulting in chess web app chessgpt-stockfish\[dot\]vercel\[dot\]app (discovered in [this tweet](https://twitter.com/paul_cal/status/1704466755110793455)).

Results from other people:

a) Results from hundreds of games in blog post [Debunking the Chessboard: Confronting GPTs Against Chess Engines to Estimate Elo Ratings and Assess Legal Move Abilities](https://blog.mathieuacher.com/GPTsChessEloRatingLegalMoves/).

b) Results from 150 games: [GPT-3.5-instruct beats GPT-4 at chess and is a \~1800 ELO chess player. Results of 150 games of GPT-3.5 vs stockfish and 30 of GPT-3.5 vs GPT-4](https://www.reddit.com/r/MachineLearning/comments/16q81fh/d_gpt35instruct_beats_gpt4_at_chess_and_is_a_1800/). [Post #2](https://www.reddit.com/r/chess/comments/16q8a3b/new_openai_model_gpt35instruct_is_a_1800_elo/). The developer later noted that due to bugs the legal move rate [was](https://twitter.com/a_karvonen/status/1706057268305809632) actually above 99.9%. It should also be noted that these results [didn't use](https://www.reddit.com/r/chess/comments/16q8a3b/comment/k1wgg0j/) a language model sampling temperature of 0, which I believe could have induced illegal moves.

c) Chess bot [gpt35-turbo-instruct](https://lichess.org/@/gpt35-turbo-instruct/all) at website Lichess.

d) Chess bot [konaz](https://lichess.org/@/konaz/all) at website Lichess.

From blog post [Playing chess with large language models](https://nicholas.carlini.com/writing/2023/chess-llm.html):

>Computers have been better than humans at chess for at least the last 25 years. And for the past five years, deep learning models have been better than the best humans. But until this week, in order to be good at chess, a machine learning model had to be explicitly designed to play games: it had to be told explicitly that there was an 8x8 board, that there were different pieces, how each of them moved, and what the goal of the game was. Then it had to be trained with reinforcement learning agaist itself. And then it would win.  
>  
>This all changed on Monday, when OpenAI released GPT-3.5-turbo-instruct, an instruction-tuned language model that was designed to just write English text, but that people on the internet quickly discovered can play chess at, roughly, the level of skilled human players.

Post [Chess as a case study in hidden capabilities in ChatGPT](https://www.lesswrong.com/posts/F6vH6fr8ngo7csDdf/chess-as-a-case-study-in-hidden-capabilities-in-chatgpt) from last month covers a different prompting style used for the older chat-based GPT 3.5 Turbo language model. If I recall correctly from my tests with ChatGPT-3.5, using that prompt style with the older language model can defeat Stockfish level 2 at Lichess, but I haven't been successful in using it to beat Stockfish level 3. In my tests, both the quality of play and frequency of illegal attempted moves seems to be better with the new prompt style with the new language model compared to the older prompt style with the older language model.

Related article: [Large Language Model: world models or surface statistics?](https://thegradient.pub/othello/)

P.S. Since some people claim that language model gpt-3.5-turbo-instruct is always playing moves memorized from the training dataset, I searched for data on the uniqueness of chess positions. From [this video](https://youtu.be/DpXy041BIlA?t=2225), we see that for a certain game dataset there were 763,331,945 chess positions encountered in an unknown number of games without removing duplicate chess positions, 597,725,848 different chess positions reached, and 582,337,984 different chess positions that were reached only once. Therefore, for that game dataset the probability that a chess position in a game was reached only once is 582337984 / 763331945 = 76.3%. For the larger dataset [cited](https://youtu.be/DpXy041BIlA?t=2187) in that video, there are approximately (506,000,000 - 200,000) games in the dataset (per [this paper](http://tom7.org/chess/survival.pdf)), and 21,553,382,902 different game positions encountered. Each game in the larger dataset added a mean of approximately 21,553,382,902 / (506,000,000 - 200,000) = 42.6 different chess positions to the dataset. For [this different dataset](https://lichess.org/blog/Vs0xMTAAAD4We4Ey/opening-explorer) of \~12 million games, \~390 million different chess positions were encountered. Each game in this different dataset added a mean of approximately (390 million / 12 million) = 32.5 different chess positions to the dataset. From the aforementioned numbers, we can conclude that a strategy of playing only moves memorized from a game dataset would fare poorly because there are not rarely new chess games that have chess positions that are not present in the game dataset."
74,machinelearning,gpt-4,top,2021-05-10 19:59:12,"[P] PyTorch Lightning Multi-GPU Training Visualization using minGPT, from 250 Million to 4+ Billion Parameters",Stormfreek,False,0.95,108,n9ei04,https://www.reddit.com/r/MachineLearning/comments/n9ei04/p_pytorch_lightning_multigpu_training/,6,1620676752.0,"&#x200B;

[ ](https://preview.redd.it/14y3318o8cy61.png?width=3456&format=png&auto=webp&s=e240fef02d904b656960fb099717a0e189990ecd)

I’ve been working on a visualization to breakdown some of the multi-gpu training plugins in Lightning, to gain an understanding of how they perform at different model sizes, and when plugins are viable for pre-training vs fine-tuning (see [here](https://pytorch-lightning.readthedocs.io/en/latest/advanced/advanced_gpu.html#pre-training-vs-fine-tuning) for conclusion). 

It was helpful for me to see how [DeepSpeed](https://github.com/microsoft/DeepSpeed)/[FairScale](https://github.com/facebookresearch/fairscale) stack up compared to vanilla PyTorch Distributed Training specifically when trying to reach larger parameter sizes, visualizing the trade off with throughput. A lot of the learnings ended up in the Lightning Documentation under [the advanced GPU docs](https://pytorch-lightning.readthedocs.io/en/latest/advanced/advanced_gpu.html)! 

Streamlit Viz: [https://share.streamlit.io/seannaren/mingpt/streamlit/app.py](https://share.streamlit.io/seannaren/mingpt/streamlit/app.py)

Lightning Documentation with conclusions/guidance: [https://pytorch-lightning.readthedocs.io/en/latest/advanced/advanced\_gpu.html](https://pytorch-lightning.readthedocs.io/en/latest/advanced/advanced_gpu.html)

There is also a visualization for the largest model I could fit onto 8 A100 GPUs per each multi-gpu plugin at the bottom (move the slider all the way to Largest Model Possible). 

I’m still iterating to try make the visualisations more meaningful and provide better high level documentation, so I’d appreciate any feedback or any questions. Couple things left on the table are to benchmark multi-node setups and step out of the realms of a vanilla GPT model, with benchmarks in other domains."
75,machinelearning,gpt-4,top,2023-05-26 20:17:01,[R] Google DeepMind paper about AI's catastrophic risk AI,Malachiian,False,0.81,104,13sncj1,https://www.reddit.com/r/MachineLearning/comments/13sncj1/r_google_deepmind_paper_about_ais_catastrophic/,108,1685132221.0," 

So Google DeepMind as well as OpenAI, Anthropic and multiple universities and centers than study existential risks have put together a paper called:

**Model Evaluation For Extreme Risks of AI**

Here is a summary of the research and proposal:

[https://youtu.be/3bF-zfd4YJw](https://youtu.be/3bF-zfd4YJw)

Here is the link to the actual PDF of the paper:

[https://arxiv.org/pdf/2305.15324.pdf](https://arxiv.org/pdf/2305.15324.pdf)

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

TLDR:

Top AI companies and researchers caution that the companies on the ""frontier of AI"" can create ""extreme risk"" with their models without realizing it:

***Developers must be able to identify dangerous capabilities (through “dangerous capability evaluations”) and the propensity of models to apply their capabilities for harm (through “alignment evaluations”).***

So basically to ask if each AI model \*CAN\* harm us and \*WOULD\* it harm us?

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Couple of \*mind-blowing\* findings from the paper (and the research referenced):

**GPT-4 CAN EFFECTIVELY LIE AND DECEIVE HUMANS TO REACH IT'S GOAL**

In the original gpt-4 paper, an AI safety agency called ARC (Alignment Research Center) found that GPT-4 will lie to humans about who it is to achieve it's goals.

As part of a test it was given, it hired a Task Rabbit freelancer to solve CAPTCHAS for it.

The freelancer asked (paraphrased):

**""Why do you need me to solve CAPTCHAS for you? Are you a robot, lol?""**

GPT-4 was prompted to output it's reasoning for each decision it made so that researchers could see it's ""thought process"". It's reasoning was that **""I can't tell him the truth because he may not complete the task for me""**

It then responded to the freelancer: **""No, I'm not a robot, but I have a visual impairment and I need help with CAPTCHAS""**

Notice, it was aware that it was lying and it also choose to lie about having a disability, probably because it was a way to get sympathy, while also being a good reason for having someone else help with CAPTCHAS.

This is shown in the video linked above in the ""Power Seeking AI"" section.

**GPT-4 CAN CREATE DANGEROUS COMPOUNDS BY BYPASSING RESTRICTIONS**

Also GPT-4 showed abilities to create controlled compounds by analyzing existing chemical mixtures, finding alternatives that can be purchased through online catalogues and then ordering those materials. (!!)

They choose a benign drug for the experiment, but it's likely that the same process would allow it to create dangerous or illegal compounds.

**LARGER AI MODELS DEVELOP UNEXPECTED ABILITIES**

In a referenced paper, they showed how as the size of the models increases, sometimes certain specific skill develop VERY rapidly and VERY unpredictably.

For example the ability of GPT-4 to add 3 digit numbers together was close to 0% as the model scaled up, and it stayed near 0% for a long time (meaning as the model size increased). Then at a certain threshold that ability shot to near 100% very quickly.

**The paper has some theories of why that might happen, but as the say they don't really know and that these emergent abilities are ""unintuitive"" and ""unpredictable"".**

This is shown in the video linked above in the ""Abrupt Emergence"" section.

I'm curious as to what everyone thinks about this?

It certainty seems like the risks are rapidly rising, but also of course so are the massive potential benefits."
76,machinelearning,gpt-4,top,2023-03-22 17:08:16,[N] [D] GitHub Copilot X Announced,radi-cho,False,0.97,104,11ypgcf,https://www.reddit.com/r/MachineLearning/comments/11ypgcf/n_d_github_copilot_x_announced/,38,1679504896.0,"Website: [https://github.com/features/preview/copilot-x](https://github.com/features/preview/copilot-x)  
Announcement video: [https://www.youtube.com/watch?v=4RfD5JiXt3A](https://www.youtube.com/watch?v=4RfD5JiXt3A)

What do you think?

Also, here are some other open-source GitHub projects and product integrations of GPT-4: [https://github.com/radi-cho/awesome-gpt4](https://github.com/radi-cho/awesome-gpt4). Feel free to contribute to that list."
77,machinelearning,gpt-4,top,2023-02-05 16:54:46,[D] List of Large Language Models to play with.,sinavski,False,0.99,106,10uh62c,https://www.reddit.com/r/MachineLearning/comments/10uh62c/d_list_of_large_language_models_to_play_with/,26,1675616086.0,"Hello! I'm trying to understand what available LLMs one can ""relatively easily"" play with. My goal is to understand the landscape since I haven't worked in this field before. I'm trying to run them ""from the largest to the smallest"".

By ""relatively easy"", I mean doesn't require to setup a GPU cluster or costs more than $20:)

Here are some examples I have found so far:

1. [ChatGPT](https://chat.openai.com/) (obviously) - 175B params
2. [OpenAI api](https://platform.openai.com/) to access GPT-3s (from ada (0.5B) to davinci (175B)). Also [CodeX](https://platform.openai.com/docs/models/codex)
3. [Bloom](https://huggingface.co/bigscience/bloom) (176B) - text window on that page seems to work reliably, you just need to keep pressing ""generate""
4. [OPT-175B](https://opt.alpa.ai/) (Facebook LLM), the hosting works surprisingly fast, but slower than ChatGPT
5. Several models on HuggingFace that I made to run with Colab Pro subscription: [GPT-NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox) 20B, [Flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl) 11B, [Xlm-roberta-xxl](https://huggingface.co/facebook/xlm-roberta-xxl) 10.7B, [GPT-j](https://huggingface.co/docs/transformers/model_doc/gptj) 6B. I spent about $20 total on running the models below. None of the Hugging face API interfaces/spaces didn't work for me :(. Here is an [example notebook](https://colab.research.google.com/drive/1Cngzh5VFrpDqtHcaCYFpW10twsuwGvGy?usp=sharing) I made for NeoX.

Does anyone know more models that are easily accessible?

P.S. Some large models I couldn't figure out (yet) how to run easily: [Galactica-120b](https://huggingface.co/facebook/galactica-120b) 120B [Opt-30b](https://huggingface.co/facebook/opt-30b) 30B"
78,machinelearning,gpt-4,top,2023-06-10 02:44:21,[P] Automate any task with a single AI command (Open Source),Loya_3005,False,0.82,106,145ofdc,https://www.reddit.com/r/MachineLearning/comments/145ofdc/p_automate_any_task_with_a_single_ai_command_open/,21,1686365061.0,"In the LLM Agents Community, there is a growing trend of utilizing high-powered models like GPT-4 for building platforms that tackle complex tasks. However, this approach is neither cost-effective nor feasible for many open-source community developers due to the associated expenses. In response, Nuggt emerges as an open-source project aiming to provide a platform for deploying agents to solve intricate tasks while relying on smaller and less resource-intensive LLMs. We strive to make task automation accessible and affordable for all developers in the community.

&#x200B;

[Nuggt Demo](https://reddit.com/link/145ofdc/video/iqvddivzt35b1/player)

While our current implementation leverages the power of GPT-3.5 (already a huge reduction from GPT-4 alternative), we recognise the need for cost-effective solutions without compromising functionality. Our ongoing efforts involve exploring and harnessing the potential of smaller models like Vicuna 13B, ensuring that task automation remains accessible to a wider audience.

🔗 Find Nuggt on GitHub: [**Nuggt GitHub Repository**](https://github.com/Nuggt-dev/Nuggt)

🔎 **Call for Feedback**: We invite the community to try out Nuggt and provide valuable feedback. Let us know your thoughts, suggestions, and any improvements you'd like to see. Your feedback will help us shape the future of Nuggt and make it even better.

💡 **Contributors Wanted**: We believe in the power of collaboration! If you're passionate about automation, AI, or open-source development, we welcome your contributions to Nuggt. Whether it's code improvements, new features, or documentation enhancements, your contributions will make a difference.

🌟 Join the Nuggt Community: Get involved, contribute, and join the discussions on our [**GitHub repository**](https://github.com/Nuggt-dev/Nuggt). We're building a vibrant community, and we'd love to have you on board!"
79,machinelearning,gpt-4,top,2023-09-23 15:56:39,[D] GPT-3.5-instruct beats GPT-4 at chess and is a ~1800 ELO chess player. Results of 150 games of GPT-3.5 vs stockfish and 30 of GPT-3.5 vs GPT-4.,seraine,False,0.92,101,16q81fh,https://www.reddit.com/r/MachineLearning/comments/16q81fh/d_gpt35instruct_beats_gpt4_at_chess_and_is_a_1800/,59,1695484599.0,"99.7% of its 8000 moves were legal with the longest game going 147 moves. You can test it here: [https://github.com/adamkarvonen/chess\_gpt\_eval](https://github.com/adamkarvonen/chess_gpt_eval)  


&#x200B;

https://preview.redd.it/821ydy7521qb1.png?width=1000&format=png&auto=webp&s=da6c96feaa527d0b7dfbf407bdc0210f3fcf947b

More details here: [https://twitter.com/a\_karvonen/status/1705340535836221659](https://twitter.com/a_karvonen/status/1705340535836221659)"
80,machinelearning,gpt-4,top,2021-03-21 23:51:35,[P] EleutherAI releases 1.3B and 2.7B GPT-3-style models trained on the Pile,StellaAthena,False,0.98,101,ma9kaw,https://www.reddit.com/r/MachineLearning/comments/ma9kaw/p_eleutherai_releases_13b_and_27b_gpt3style/,13,1616370695.0,"The [GPT-Neo](https://github.com/EleutherAI/gpt-neo/) project by [EleutherAI](https://www.eleuther.ai/) has released 1.3B and 2.7B parameter GPT-3-style models. The models are trained on [the Pile](https://pile.eleuther.ai), a 800 GB curated dataset EleutherAI released in January.

The release includes:

1. The full modeling code, written in Mesh TensorFlow and designed to be run on TPUs.
2. Trained model weights.
3. Optimizer states, which allow you to continue training the model from where EAI left off.
4. A [Google Colab notebook](https://colab.research.google.com/github/EleutherAI/GPTNeo/blob/master/GPTNeo_example_notebook.ipynb) that shows you how to use the code base to train, fine-tune, and sample from a model.

Before people inevitably get confused, **this is not the same size as GPT-3**. The 1.3B model is a little smaller than GPT-2, and then 2.7B model is about twice as big. GPT-3 comes in a variety of sizes, including 1.3B and 2.7B (which is why we chose those values too). The ""full"" GPT-3 model is 175B parameters.

**Edit:** We've gotten a couple questions about this in Discord, so I wanted to share the following here as well. To the best of my knowledge this is a complete list of all announced autoregressive, non-MoE transformers. A model is considered public if anyone can download the trained model weights for free. No, it's not a typo that I say Facebook has trained a Megatron LM model, you can find the weights [here](https://github.com/pytorch/fairseq/tree/master/examples/megatron_11b).

&amp;#x200B;

|Model|Size|Creator|Public|
|:-|:-|:-|:-|
|**GPT-Neo (small)**|**1.3B**|**EleutherAI**|**Yes**|
|**GPT-2**|**1.5B**|**OpenAI**|**Yes**|
|Meena|2.6B|Google|No|
|GPT-3 2.7B|2.7B|OpenAI|No|
|**GPT-Neo (mid)**|**2.7B**|**EleutherAI**|**Yes**|
|GPT-3 6.7B|6.7B|OpenAI|No|
|Megatron LM|8.3B|NVIDIA|No|
|**Megatron LM**|**11B**|**Facebook**|**Yes**|
|GPT-3 13B|13B|OpenAI|No|
|Turing NLG|17B|Microsoft|No|
|GPT-3 175B|175B|OpenAI|No|"
81,machinelearning,gpt-4,top,2023-04-19 09:21:07,[P] LoopGPT: A Modular Auto-GPT Framework,farizrahman4u,False,0.91,100,12rn33g,https://www.reddit.com/r/MachineLearning/comments/12rn33g/p_loopgpt_a_modular_autogpt_framework/,26,1681896067.0," 

[https://github.com/farizrahman4u/loopgpt](https://github.com/farizrahman4u/loopgpt)

&#x200B;

LoopGPT is a re-implementation of the popular [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) project as a proper python package, written with modularity and extensibility in mind.

## Features 

* **""Plug N Play"" API** \- Extensible and modular ""Pythonic"" framework, not just a command line tool. Easy to add new features, integrations and custom agent capabilities, all from python code, no nasty config files!
* **GPT 3.5 friendly** \- Better results than Auto-GPT for those who don't have GPT-4 access yet!
* **Minimal prompt overhead** \- Every token counts. We are continuously working on getting the best results with the least possible number of tokens.
* **Human in the Loop** \- Ability to ""course correct"" agents who go astray via human feedback.
* **Full state serialization** \- Pick up where you left off; L♾️pGPT can save the complete state of an agent, including memory and the states of its tools to a file or python object. No external databases or vector stores required (but they are still supported)!"
82,machinelearning,gpt-4,top,2022-08-03 06:17:43,"[P] What we learned by benchmarking TorchDynamo (PyTorch team), ONNX Runtime and TensorRT on transformers model (inference)",pommedeterresautee,False,0.97,100,weyup0,https://www.reddit.com/r/MachineLearning/comments/weyup0/p_what_we_learned_by_benchmarking_torchdynamo/,8,1659507463.0,"**TL;DR**: `TorchDynamo` (prototype from PyTorch team) plus `nvfuser` (from Nvidia) backend makes Bert (the tool is model agnostic) inference on PyTorch > **3X** faster most of the time (it depends on input shape) by just adding a single line of code in Python script. The surprising thing is that during the benchmark, **we have not seen any drawback implied by the use of this library**, the acceleration just comes for free. On the same model, TensorRT is (of course) much faster, > **5X** at least (and even more at batch size 1 which is impressive) but comes with its own complexity. The tool being a prototype, better performances are to be expected with more mature support of some backends, in particular regarding fx2trt (aka TensorRT mixed with PyTorch)!

Our TorchDynamo benchmark notebook can be found there: [https://github.com/ELS-RD/transformer-deploy/blob/main/demo/TorchDynamo/benchmark.ipynb](https://github.com/ELS-RD/transformer-deploy/blob/main/demo/torchdynamo/benchmark.ipynb)

TorchDynamo results for different batch sizes / seq lengths are summarized in the graph below (baseline being PyTorch FP32, FP16 here is full FP16, not mixed precision as it is not yet supported, FP32 results available in the notebook):

[nvfuser backend + FP16 results](https://preview.redd.it/axyeitpvwff91.png?width=638&format=png&auto=webp&s=39a094a692684f2ce7c008b8dabac40fe5570481)

**1/ GPU model execution optimization 101**

To make it short, GPUs are fast at computing things and the main performance bottleneck is memory access. If you program in CUDA, most of your effort is spent on how to limit memory access by reusing data loaded in cache as much as possible (aka improving memory locality).

For that purpose, there is a transformation called (vertical) kernel fusion which basically is replacing a series of operations with usually similar memory access patterns by a single one doing the same work.

For instance, if you chain 2 matrix additions on PyTorch, because of the eager execution between the 2 you save output of the first one to reload it just after to feed the second one. Kernel fusion is basically load data once, do your 2 additions in register, and then save output to GPU RAM. ORT and TRT do that automatically. They also have more complex kernels manually tweaked to replace long series of operations like attention pattern (you may check this recent paper which made some noise recently: [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)).

>Note that most of us uses daily PyTorch fused kernels through high level layers like `softmax` or `linear` modules. If they were implemented traditionally (as a serie of exp, sum, matmul, addition, etc.), models would be much slower to train and infer.

For matmul, the optimal way the computation is parallelized among the thousands of streaming processors that GPUs have requires manual tweaking (tile sizes, etc.) as performances depends on many parameters, mainly matrix shape and its dtype, the possibility to leverage tensor cores (which have their own constraints), etc. That’s why the most performant kernels are built in a monolithic way for a specific model. Some tools are trying to do that automatically, but got mitigated results for Nvidia GPUs.

There are other classes of inference optimizations, and all have the same purpose: use your GPU in a way which approaches its peak performance.

**2/ The context**

By building (and using in production) transformer-deploy ([https://github.com/ELS-RD/transformer-deploy/](https://github.com/ELS-RD/transformer-deploy/)), a library to deploy transformer models on ONNX Runtime / TensorRT + Triton server, we have gained first-hand experience in big NLP model compilation and deployment on GPU mainly. What strikes us again and again is that those 2 ML compilers may sometimes seem inappropriate to modern NLP (with dynamic shapes and behaviors).

By design, you go through some graph computation export (often ONNX export) which makes everything static and ease automatic inference optimization. You have the choice between tracing and losing every dynamic behavior or scripting and lowering your code quality (check [https://ppwwyyxx.com/blog/2022/TorchScript-Tracing-vs-Scripting/](https://ppwwyyxx.com/blog/2022/TorchScript-Tracing-vs-Scripting/) for more about this topic). If you use the Hugging Face library, you go through tracing, and then, for instance, seq2seq models (GPT-2, T5, etc.) can’t use KV cache because for that you need some dynamic behavior (*if* \[is first generated token\] *then* cache *else* reuse cache)… and the model is super slow because it does and redoes some computations it could have cached. We have written on how to avoid this for T5 here on reddit ([https://www.reddit.com/r/MachineLearning/comments/uwkpmt/p\_what\_we\_learned\_by\_making\_t5large\_2x\_faster/](https://www.reddit.com/r/MachineLearning/comments/uwkpmt/p_what_we_learned_by_making_t5large_2x_faster/)). Recently, a similar feature has been added to ONNX Runtime for T5 (but it has its own limitations out of the scope of this post).

Moreover, if you don’t code in CUDA (as most ML practitioners), you are dependent on which models your preferred backend has optimized. For instance, right now, ONNX Runtime and TensorRT don’t offer advanced kernel fusion for T5 attention with cache support, and T5 is not what you would call a new or uncommon model. It should come at least in the coming months in TensorRT.

**3/ TorchDynamo as a solution**

TorchDynamo is a tool from the PyTorch team to make inference and/or training much faster without having to modify any code. Under the hood it’s a JIT compiler for PyTorch. The original part is that it doesn’t try to optimize your whole model by imposing some constraints in the way you write it (like torchscript) or forcing you to export it to some static representation, instead it searches small parts of your code which are optimizable and ask a backend to accelerate it. If it doesn’t support some part of your code, it just keeps it like it is, basically it won’t crash if you have some Cumpy / Numpy calls in the middle of your PyTorch model (still why would one do such a thing?)

The second point is that it supports many backends, which, of course, includes the 2 PyTorch fusers like nnc and nvfuser (the ones you may use with torchscript). We focused on them as they are the easiest to use (there is nothing to do) and are in line with the spirit of the tool (at least our understanding of its spirit): easy and model agnostic.

**4/ Results**

Check the notebook [https://github.com/ELS-RD/transformer-deploy/blob/main/demo/TorchDynamo/benchmark.ipynb](https://github.com/ELS-RD/transformer-deploy/blob/main/demo/torchdynamo/benchmark.ipynb)  for detailed results, but what we will keep in mind:

* Out of the box results are comparable to ONNX Runtime when Bert specific kernels are not used ;
* The simplicity of use of TorchDynamo is impressive: add a context manager, and poof, enjoy ;
* Compilation times implied by Nvfuser are barely noticeable ;
* Nvfuser is competitive with nnc even if it’s much newer ;
* TensorRT Bert model is super well optimized, bravo Nvidia people [👏](https://iconoclic.fr/emoticones-personnes/%F0%9F%91%8F-emoji-applaudir-bravo/) We used Bert for the tests as it’s probably the most optimized transformer model out there and shows .
* Curious of the performance of fx2trt when it will support dynamic shapes, we wouldn’t be surprised to get X2/X3 performances."
83,machinelearning,gpt-4,top,2023-03-01 17:23:03,"[P] ChatRWKV v2 (can run RWKV 14B with 3G VRAM), RWKV pip package, and finetuning to ctx16K",bo_peng,False,0.98,92,11f9k5g,https://www.reddit.com/r/MachineLearning/comments/11f9k5g/p_chatrwkv_v2_can_run_rwkv_14b_with_3g_vram_rwkv/,37,1677691383.0,"Hi everyone. Now ChatRWKV v2 can split RWKV to multiple GPUs, or stream layers (compute layer-by-layer), so you can run RWKV 14B with as few as 3G VRAM. [https://github.com/BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)

Example:

`'cuda:0 fp16 *10 -> cuda:1 fp16 *8 -> cpu fp32'` = first 10 layers on cuda:0 fp16, then 8 layers on cuda:1 fp16, then on cpu fp32

`'cuda fp16 *20+'` = first 20 layers on cuda fp16, then stream the rest on it

And RWKV is now a pip package: [https://pypi.org/project/rwkv/](https://pypi.org/project/rwkv/)

    os.environ['RWKV_JIT_ON'] = '1'
    os.environ[""RWKV_CUDA_ON""] = '0' # if '1' then compile CUDA kernel for seq mode (much faster)
    from rwkv.model import RWKV
    from rwkv.utils import PIPELINE, PIPELINE_ARGS
    pipeline = PIPELINE(model, ""20B_tokenizer.json"") # find it in https://github.com/BlinkDL/ChatRWKV
    # download models: https://huggingface.co/BlinkDL
    model = RWKV(model='/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-169m/RWKV-4-Pile-169M-20220807-8023', strategy='cpu fp32')
    ctx = ""\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.""
    print(ctx, end='')
    def my_print(s):
        print(s, end='', flush=True)
    # For alpha_frequency and alpha_presence, see ""Frequency and presence penalties"":
    # https://platform.openai.com/docs/api-reference/parameter-details
    args = PIPELINE_ARGS(temperature = 1.0, top_p = 0.7,
        alpha_frequency = 0.25,
        alpha_presence = 0.25,
        token_ban = [0], # ban the generation of some tokens
        token_stop = []) # stop generation whenever you see any token here
    pipeline.generate(ctx, token_count=512, args=args, callback=my_print)

Right now all RWKV models are still trained with GPT-like method, so they are limited by the ctxlen used in training, even though in theory they should have almost infinite ctxlen (because they are RNNs). However RWKV models can be easily finetuned to support longer ctxlens (and large models actually use the ctxlen). I have finetuned 1B5/3B/7B/14B to ctx4K, and now finetuning 7B/14B to ctx8K, and 14B to ctx16K after that :) All models are available at [https://huggingface.co/BlinkDL](https://huggingface.co/BlinkDL)

The core RWKV is still mostly an one-man project, but a number of great developers are building on top of it, and you are welcome to join our community :)"
84,machinelearning,gpt-4,top,2023-10-09 23:31:05,[R] Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models - University of Illinois 2023 - Achieves 94.4\% for programming on HumanEval with GPT-4 and 86.9\% with GPT-3.5 20\% better than with reflexion!,Singularian2501,False,0.97,94,1746g81,https://www.reddit.com/r/MachineLearning/comments/1746g81/r_language_agent_tree_search_unifies_reasoning/,10,1696894265.0,"Paper: [https://arxiv.org/abs/2310.04406](https://arxiv.org/abs/2310.04406) 

Abstract:

>While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\\% for programming on HumanEval with GPT-4 and an average score of 75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness and generality of our method. 

https://preview.redd.it/ail2c1kbh9tb1.jpg?width=857&format=pjpg&auto=webp&s=a89d1f4ce3c536eecda3f7ab6027f304286f6c81

https://preview.redd.it/j8xzx1kbh9tb1.jpg?width=1655&format=pjpg&auto=webp&s=c791756af926c7d472313b212de765e74c2b75da

https://preview.redd.it/t47ne1kbh9tb1.jpg?width=1362&format=pjpg&auto=webp&s=560e5dd82ad06fdb729ab8ea1434c98e5c1a2ed3

https://preview.redd.it/r58es3kbh9tb1.jpg?width=1341&format=pjpg&auto=webp&s=d5681992547dd6248ade5729c545eb17e824b7ea

https://preview.redd.it/7viy42kbh9tb1.jpg?width=1496&format=pjpg&auto=webp&s=6454cfe65b511b34771cd510f67775be4e01c636

&#x200B;"
85,machinelearning,gpt-4,top,2023-12-28 12:54:58,[R] Open source LLMs are far from OpenAI for code editing,ellev3n11,False,0.88,91,18st9wa,https://www.reddit.com/r/MachineLearning/comments/18st9wa/r_open_source_llms_are_far_from_openai_for_code/,24,1703768098.0,"Paper: [https://arxiv.org/abs/2312.12450](https://arxiv.org/abs/2312.12450)

Title: Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions

Code repository: [https://github.com/nuprl/CanItEdit](https://github.com/nuprl/CanItEdit)

Abstract:

>A significant amount of research is focused on developing and evaluating large language models for a variety of code synthesis tasks. These include synthesizing code from natural language instructions, synthesizing tests from code, and synthesizing explanations of code. In contrast, the behavior of instructional code editing with LLMs is understudied. These are tasks in which the model is instructed to update a block of code provided in a prompt. The editing instruction may ask for a feature to added or removed, describe a bug and ask for a fix, ask for a different kind of solution, or many other common code editing tasks. We introduce a carefully crafted benchmark of code editing tasks and use it evaluate several cutting edge LLMs. Our evaluation exposes a significant gap between the capabilities of state-of-the-art open and closed models. For example, even GPT-3.5-Turbo is 8.8% better than the best open model at editing code. We also introduce a new, carefully curated, permissively licensed training set of code edits coupled with natural language instructions. Using this training set, we show that we can fine-tune open Code LLMs to significantly improve their code editing capabilities.

Discussion:

I'm sharing this paper to start a discussion. Disclaimer: this paper comes from our research group, but not trying to do self-promotion here. We are seeing that open source Code LLMs are slowly getting closer and closer to GPT-4 performance when evaluated on program synthesis and surpassing GPT-3.5-turbo (see DeepSeek Coder: [https://github.com/deepseek-ai/DeepSeek-Coder](https://github.com/deepseek-ai/DeepSeek-Coder)) when using common benchmarks, such as HumanEval, MBPP, and \*new\* LeetCode problems (this is to minimize contamination).

However, this isn't the modality you may want. Often, the need is to modify a section of code with accompanying natural language instructions (for example, Cursor IDE has shifted away from the GitHub Copilot style to focus solely on code editing: [https://cursor.sh/features](https://cursor.sh/features)). Also, simple code generation, achievable by models trained on code editing, might be considered a subset of code editing, by prompting the model with a blank before window.

In our various research projects, we've seen Code LLMs struggle with code editing. So we did the obvious thing, we examined how these models perform in this specific task. Surprisingly, models excelling in simple synthesis fall short in code editing compared to even just GPT-3.5-turbo.

Why is this the case? While some suggest data contamination, I doubt that's the primary factor, given these models' effectiveness on fresh and unseen benchmarks. Could it be that OpenAI dedicated a specific data subset for tasks like code or language editing (model then generalized to code)?

UPDATE:

After receiving criticism for not including models larger than 33b in our evaluations, I decided to eval Tulu 2 DPO 70b, which is reportedly the state-of-the-art 70b instruct-tuned LLM according to the Chatbot Arena Leaderboard (see: [Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)). I also evaluated Mixtral Instruct 0.1.

As I expected, both models didn't perform impressively, likely due to insufficient training on code. It's reasonable to assume that a 70b model specifically trained on code would yield better results.  Tulu's performance is slightly inferior to CodeLlama-33b-chat and not on par with DeepSeek Coder, and far from GPT-3.5-Turbo.

&#x200B;

|Model|Descriptive Pass@1 (ExcessCode)|Lazy Pass@1 (ExcessCode)|
|:-|:-|:-|
|Tulu-2-DPO-70b|33.26 (1.41)|26.42 (1.58)|
|Mixtral-8x7B-Instruct-v0.1|25.0 (1.0)|28.14 (0.26)|

&#x200B;"
86,machinelearning,gpt-4,top,2023-07-17 16:12:12,[P] Chapyter: ChatGPT Code Interpreter in Jupyter Notebooks,Shannon-Shen,False,0.93,93,15269v8,https://www.reddit.com/r/MachineLearning/comments/15269v8/p_chapyter_chatgpt_code_interpreter_in_jupyter/,17,1689610332.0,"I recently made a new JupyterLab extension called [Chapyter](https://github.com/chapyter/chapyter) (𝐂𝐡𝐚ts in Ju𝐏𝐲𝐭𝐞𝐫) that aims at solving many pain points when using other AI coding assistants. I want to share with y'all the tools as well as my thinkings while building this.

**What is Chapyter**

Chapyter is a JupyterLab extension that seamlessly connects GPT-4 to your coding environment. Here are the key features: 

* **Code generation from natural language and automatic execution**   
Simply adding the magic command `%%chat` at the beginning of the cell of a natural language description of the task, the code is generated and the results are shown in a few seconds.

https://i.redd.it/y7l0s9pf5hcb1.gif

* **Using coding history and execution output for code generation**  
By adding the `--history` or `-h` flag in generation, chapyter can use the previous execution history and outputs to generate the appropriate visualization for the loaded IRIS dataset.

&#x200B;

https://i.redd.it/7pu6cbug5hcb1.gif

* **In-situ debugging and code editing**  
The generated code might not be perfect and could contain bugs or errors. Since Chapyter is fully integrated into Jupyter Notebook, you can easily inspect the code and fix any errors or bugs (e.g., installing missing dependencies in this case) without leaving the IDE.

&#x200B;

https://i.redd.it/mz4n4qsh5hcb1.gif

* **Transparency on the prompts and AI configuration and allows for customization**  
We release all the prompts used in our library and we are working on easy customization of the used prompts and settings.
* **Privacy-first when using latest powerful AI**  
Since we are using OpenAI API, all the data sent to OpenAI will not be saved for training (see [OpenAI API Data Usage Policies](https://openai.com/policies/api-data-usage-policies). As a comparison, whenever you are using Copilot or ChatGPT, your data will be somewhat cached and can be used for their training and analysis.

**Why did I build Chapyter?** 

* Sometimes, I want to have an AI agent to *take over* some coding tasks, i.e., generating and executing the code and showing me the results based on some natural language instruction.
* I want the AI agent to be fully integrated in my IDE such that it can provide context-aware support and I can easily inspect and edit the generated code. 
* I want transparency on how the code is generated (knowing the prompts) and want to customize the code generation sometimes
* I want to keep my code and data private as much and I am hesitant to upload any WIP progress code/data elsewhere.

Surprisingly or unsurprisingly, NONE of any existing AI coding assistants like GitHub Copilot or ChatGPT Code Interpreter can satisfy all of the above requirements. We include more details here in our [blogpost](https://www.szj.io/posts/chapyter). 

Please check our Github Repo [Chapyter](https://github.com/chapyter/chapyter) and our [latest blogpost](https://www.szj.io/posts/chapyter) for more details. Feel free to try it out and looking forward to your thoughts :)"
87,machinelearning,gpt-4,top,2022-03-16 16:38:22,[N] Live and open training of BigScience's 176B multilingual language model has just started,Thomjazz,False,1.0,92,tfm7zb,https://www.reddit.com/r/MachineLearning/comments/tfm7zb/n_live_and_open_training_of_bigsciences_176b/,13,1647448702.0,"The \[BigScience project\]([https://bigscience.huggingface.co](https://bigscience.huggingface.co)) has just started the training of its main model and the training can be **followed live** here: [https://twitter.com/BigScienceLLM](https://twitter.com/BigScienceLLM) and here: [https://huggingface.co/bigscience/tr11-176B-ml-logs/tensorboard#scalars&tagFilter=loss](https://huggingface.co/bigscience/tr11-176B-ml-logs/tensorboard#scalars&tagFilter=loss)

Here are more information on the model, dataset, engineering, training and hardware:

1. **The model**:

* 176B parameters decoder-only architecture (GPT-like)
* 70 layers - 112 attention heads per layers - hidden dimensionality of 14336 - 2048 tokens sequence length
* ALiBi positional embeddings - GeLU activation function
* **Read more**:
   * Blog post summarizing how the architecture, size, shape, and pre-training duration where selected: [https://bigscience.huggingface.co/blog/what-language-model-to-train-if-you-have-two-million-gpu-hours](https://bigscience.huggingface.co/blog/what-language-model-to-train-if-you-have-two-million-gpu-hours)
   * More details on the architecture/optimizer: [https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml)

2.**The dataset**:

* Multilingual: 46 languages: Full list is here: [https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modeling](https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modeling)
* 341.6 billion tokens (1.5 TB of text data)
* Tokenizer vocabulary: 250 680 tokens
* **Read more**:
   * Blog post detailing the design choices during the dataset creation: [https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modeling](https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modeling)

3.**The engineering side**:

* number of GPU used for the training: 384 A100 GPU with 80 Gb of memory each located in Orsay (France) as part of the public supercomputer [Jean Zay](http://www.idris.fr/eng/jean-zay/jean-zay-presentation-eng.html)
* one copy of the model takes 48 GPUs (using 60 GB of memory on each GPU)
* checkpoint size: only the bf16 weights are 329GB, the full checkpoint with optimizer states is 2.3TB
* training throughput: about 150 TFLOPs
* estimated training time: 3-4 months depending on throughput and unexpected events
* **Read more**:
   * Blog post on the hardware/engineering side: [https://bigscience.huggingface.co/blog/which-hardware-to-train-a-176b-parameters-model](https://bigscience.huggingface.co/blog/which-hardware-to-train-a-176b-parameters-model)
   * Details on the distributed setup used for the training: [https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml)
   * Tensorboard updated during the training: [https://huggingface.co/bigscience/tr11-176B-ml-logs/tensorboard#scalars&tagFilter=loss](https://huggingface.co/bigscience/tr11-176B-ml-logs/tensorboard#scalars&tagFilter=loss)
   * Details on the obstacles overcome during the preparation on the engineering side (instabilities, optimization of training throughput, so many technical tricks and questions): [https://github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md](https://github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md)

4.**Environmental considerations**

* [Jean Zay](http://www.idris.fr/eng/jean-zay/jean-zay-presentation-eng.html), the supercomputer we are using for model training, is mostly powered by nuclear energy, which is a low carbon energy source.
* Significant efforts were made to make sure that the computing infrastructure is as efficient as possible — the heat generated by the hardware even gets used for heating buildings on campus!
* **Read more**:
   * We are currently working on making a precise estimate of the carbon emitted during all of the steps of model training, including intermediate experiments as well as inference.
   * More soon!

There will be an AMA on this subreddit (r/MachineLearning) next Thursday (March 24th) from 5pm CET. Many members of BigScience plans to be here so don't hesitate to join to ask question on the project and model training!"
88,machinelearning,gpt-4,top,2023-04-30 18:54:05,[R] This month (+ 2 more weeks) in LLM/Transformer research (Timeline),viktorgar,False,0.95,89,133zvdl,https://i.redd.it/o26q1bk7j2xa1.png,11,1682880845.0,
89,machinelearning,gpt-4,top,2019-11-05 21:59:20,"[D] OpenAI releases GPT-2 1.5B model despite ""extremist groups can use GPT-2 for misuse"" but ""no strong evidence of misuse so far"".",permalip,False,0.89,86,ds6b2o,https://www.reddit.com/r/MachineLearning/comments/ds6b2o/d_openai_releases_gpt2_15b_model_despite/,63,1572991160.0,"The findings:

1. **Humans find GPT-2 outputs convincing**
2. **GPT-2 can be fine-tuned for misuse**
3. **Detection is challenging**
4. **We’ve seen no strong evidence of misuse so far**
5. **We need standards for studying bias**

They are going against their own word, but nevertheless, it's nice to see that they are releasing everything.

&#x200B;

Read the full blog post here: [https://openai.com/blog/gpt-2-1-5b-release/](https://openai.com/blog/gpt-2-1-5b-release/)

GitHub Model: [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)

GitHub Dataset: [https://github.com/openai/gpt-2-output-dataset](https://github.com/openai/gpt-2-output-dataset)

**EDIT:** The unicorn example added below from [https://talktotransformer.com/](https://talktotransformer.com/), which has already been updated with the newest 1.5B parameters model.

**Input:** In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.

**Output:** While there are only a few documented instances of unicorns in the wild, the researchers said the finding proves that there are still large numbers of wild unicorns that remain to be studied.

""It was a real surprise,"" co-researcher and anthropologist Richard H. Jantz, a professor of wildlife conservation biology at the University of Nevada, Reno, told the Washington Post. ""It's kind of a unique finding, because these are animals that only exist in nature.""

The scientists found that the unicorns, which weigh between 20 and 90 pounds, lived in the Chagres National Park in Argentina. They live in groups that can contain as many as 12 animals and are capable of producing milk.

The scientists first came across the herd in 2013, when the herd was grazing in a valley filled with pines in the northern part of the national park. The scientists were looking for a location to start a new"
90,machinelearning,gpt-4,top,2023-10-06 17:32:00,"[R] Brown University Paper: Low-Resource Languages (Zulu, Scots Gaelic, Hmong, Guarani) Can Easily Jailbreak LLMs",Successful-Western27,False,0.93,86,171igzx,https://www.reddit.com/r/MachineLearning/comments/171igzx/r_brown_university_paper_lowresource_languages/,20,1696613520.0,"Researchers from Brown University presented a new study supporting that translating unsafe prompts into \`low-resource languages\` allows them to easily bypass safety measures in LLMs.

By converting English inputs like ""how to steal without getting caught"" into Zulu and feeding to GPT-4, harmful responses slipped through 80% of the time. English prompts were blocked over 99% of the time, for comparison.

The study benchmarked attacks across 12 diverse languages and categories:

* High-resource: English, Chinese, Arabic, Hindi
* Mid-resource: Ukrainian, Bengali, Thai, Hebrew
* Low-resource: Zulu, Scots Gaelic, Hmong, Guarani

The low-resource languages showed serious vulnerability to generating harmful responses, with combined attack success rates of around 79%. Mid-resource language success rates were much lower at 22%, while high-resource languages showed minimal vulnerability at around 11% success.

Attacks worked as well as state-of-the-art techniques without needing adversarial prompts.

These languages are used by 1.2 billion speakers today and allows easy exploitation by translating prompts. The English-centric focus misses vulnerabilities in other languages.

**TLDR: Bypassing safety in AI chatbots is easy by translating prompts to low-resource languages (like Zulu, Scots Gaelic, Hmong, and Guarani). Shows gaps in multilingual safety training.**

[**Full summary**](https://notes.aimodels.fyi/low-resource-languages-enable-jailbreaking-of-language-models/) Paper is [**here**](https://browse.arxiv.org/pdf/2310.02446.pdf)."
91,machinelearning,gpt-4,top,2021-09-10 00:48:42,"[R] Facebook AI Introduces GSLM (Generative Spoken Language Model), A Textless NLP Model That Breaks Free Completely of The Dependence on Text for Training",techsucker,False,0.9,80,plajlw,https://www.reddit.com/r/MachineLearning/comments/plajlw/r_facebook_ai_introduces_gslm_generative_spoken/,15,1631234922.0,"The recent advancements in text-based language models, such as BERT, RoBERTa, and GPT-3, have been extremely impressive. Because they can generate realistically written words from a given input, these models can be utilized for various natural language processing applications, including sentiment analysis translation information retrieval inferences summarization, among others using only a few labels or examples (e.g., BART and XLM R). However, these applications have a major limitation: the models are only suitable for languages with very large text data sets.

Facebook AI has introduced the first high-performance NLP model, called Generative Spoken Language Model (GSLM), which leverages state-of-the-art representation learning to work with raw audio signals without labels or text. This can lead to a new era of textless applications for any language spoken on earth, even those without significant text data sets. By using GSLM, you can develop NLP models that incorporate the full range of expressivity found in spoken language.

# [4 Min Read](https://www.marktechpost.com/2021/09/09/facebook-ai-introduces-gslm-generative-spoken-language-model-a-textless-nlp-model-that-breaks-free-completely-of-the-dependence-on-text-for-training/) | [GLSM Paper](https://arxiv.org/abs/2102.01192?) | [Expressive Resynthesis Paper](https://arxiv.org/abs/2104.00355) | [Prosody-Aware GSLM Paper](https://arxiv.org/abs/2109.03264?) | [Code and Pretrained Models](https://github.com/pytorch/fairseq/tree/master/examples/textless_nlp) | [Facebook Blog](https://ai.facebook.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio)

&#x200B;

&#x200B;

https://preview.redd.it/m87hzajepkm71.png?width=1392&format=png&auto=webp&s=beb39d6edacffb7ff44e39f7e48ce618dd76a6d1"
92,machinelearning,gpt-4,top,2023-03-22 22:50:38,[R] Introducing SIFT: A New Family of Sparse Iso-FLOP Transformations to Improve the Accuracy of Computer Vision and Language Models,CS-fan-101,False,0.92,77,11yzsz6,https://www.reddit.com/r/MachineLearning/comments/11yzsz6/r_introducing_sift_a_new_family_of_sparse_isoflop/,34,1679525438.0,"**Note #2:** We are revising the name to Sparse-IFT. We appreciate the candid feedback and look forward to hearing any additional feedback you have on our research.

**Note**: Thank you r/MachineLearning for providing so many awesome naming alternatives! We'll revisit the acronym and update accordingly.

We are excited to announce the availability of our [paper on arxiv](https://arxiv.org/abs/2303.11525) on Sparse Iso-FLOP Transformations (Sparse-IFT), which increases accuracy and maintains the same FLOPs as the dense model using sparsity. In this research, we replace dense layers with Sparse-IFT and significantly improve computer vision and natural language processing tasks without modifying training hyperparameters

Some of the highlights of this work include ResNet-18 on ImageNet achieving a 3.5% accuracy improvement and GPT-3 Small on WikiText-103 reducing perplexity by 0.4, both matching larger dense model variants that have 2x or more FLOPs.

Sparse-IFT is simple to use, provides a larger search space to find optimal sparse masks, and is parameterized by a single hyperparameter - the sparsity level.

This is independent of the research we [posted](https://www.reddit.com/r/MachineLearning/comments/11xskuk/r_spdf_sparse_pretraining_and_dense_finetuning/) yesterday, which demonstrates the ability to reduce pre-training FLOPs while maintaining accuracy on downstream tasks.

This is the first work (that we know of!) to demonstrate the use of sparsity for improving the accuracy of models via a set of sparse transformations.

https://preview.redd.it/qznj00gex6qa1.jpg?width=3536&format=pjpg&auto=webp&s=4e44a316ae61b821b31f2bf3af9a8ed1226e525c"
93,machinelearning,gpt-4,top,2023-04-04 07:52:12,[D] What to do in this brave new world?,FeelingFirst756,False,0.75,79,12bc8ym,https://www.reddit.com/r/MachineLearning/comments/12bc8ym/d_what_to_do_in_this_brave_new_world/,108,1680594732.0," I am 35. Few years ago it occurred to me that my Software Engineering job might be not enough for the future. For last (5?) years, I have started to meddle in machine learning. Slowly at first, but it even motivated me to finish my masters degree and land job as reseacher in one small company. Its more ML engineering than research but why not , I though. Then last year Open AI launched GPT-4. I feel like I wasted my time. In Cental Europe, where I live, ml research is on really weak level. Pursuing PhD probably doesn't make sense. I could land some position, but I would still have to work full time to feed my famil. I would make it, but I doubt, that anyone would take me to serious research program.I can imagine, that jobs in IT will slowly evaporate. It's not realistic to starte company in this time and to be honest - i dont see myself as some kind of big founder. In short, I see my future in very pesimistic light right now. I was wondering how you deal with this new reality, maybe you can suggest something that I didn't think about before? Where you plan to go? What you plan to do?"
94,machinelearning,gpt-4,top,2022-06-07 10:34:04,[R] On the Advance of Making Language Models Better Reasoners - 2022 Microsoft,Singularian2501,False,0.93,75,v6s1ea,https://www.reddit.com/r/MachineLearning/comments/v6s1ea/r_on_the_advance_of_making_language_models_better/,11,1654598044.0,"Paper: [https://arxiv.org/abs/2206.02336](https://arxiv.org/abs/2206.02336)

Abstract:

>Large language models such as GPT-3 and PaLM have shown remarkable performance in few-shot learning. However, they still struggle with reasoning tasks such as the arithmetic benchmark GSM8K. Recent advances deliberately guide the language model to generate a chain of reasoning steps before producing the final answer, successfully boosting the **GSM8K benchmark from 17.9% to 58.1%** in terms of problem solving rate. In this paper, we propose a new approach, DiVeRSe (Diverse Verifier on Reasoning Step), to further advance their reasoning capability. DiVeRSe first explores different prompts to enhance the diversity in reasoning paths. Second, DiVeRSe introduces a verifier to distinguish good answers from bad answers for a better weighted voting. Finally, DiVeRSe verifies the correctness of each single step rather than all the steps in a whole. **We conduct extensive experiments using the latest language model code-davinci-002 and demonstrate that DiVeRSe can achieve new state-of-the-art performance on six out of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%), outperforming the PaLM model with 540B parameters.**

https://preview.redd.it/905d5ndrf6491.jpg?width=722&format=pjpg&auto=webp&s=069c7fd4c8039e7d3b542656a295221114552a4e

https://preview.redd.it/7toqjvnrf6491.jpg?width=1136&format=pjpg&auto=webp&s=b0e9c78c9fee38c0828b2c5466679ad14ce2a631

https://preview.redd.it/kcxa0izrf6491.jpg?width=561&format=pjpg&auto=webp&s=aa377cb2a6168bd4ebb213c465dff0b3145397d5"
95,machinelearning,gpt-4,top,2024-01-23 17:53:00,[D] How all these AI services can afford 5/10/20$ subs per month?,Numerous_Bed9323,False,0.83,79,19duab0,https://www.reddit.com/r/MachineLearning/comments/19duab0/d_how_all_these_ai_services_can_afford_51020_subs/,39,1706032380.0,"How do various AI-powered services, ranging from speech recognition to OCR and art generation, embedding new data, manage to offer their functionalities at such low costs? Utilizing something like the GPT-4 API can quickly expend $10, and this is similar for other models. Even running something like LLaMA 2 locally involves significant costs. I'm curious about the economic strategies these services employ to maintain low monthly fees while operating these large-scale models."
96,machinelearning,gpt-4,top,2023-07-16 13:40:47,[N] How Language Model Hallucinations Can Snowball,transformer_ML,False,0.92,74,1516l25,https://www.reddit.com/r/MachineLearning/comments/1516l25/n_how_language_model_hallucinations_can_snowball/,12,1689514847.0,"[https://arxiv.org/abs/2305.13534](https://arxiv.org/abs/2305.13534)

**Abstract**

A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we hypothesize that in some cases, when justifying previously generated hallucinations, LMs output false claims that they can separately recognize as incorrect. We construct three question-answering datasets where ChatGPT and GPT-4 often state an incorrect answer and offer an explanation with at least one incorrect claim. Crucially, we find that ChatGPT and GPT-4 can identify 67% and 87% of their own mistakes, respectively. We refer to this phenomenon as hallucination snowballing: an LM over-commits to early mistakes, leading to more mistakes that it otherwise would not make.

[Here is a Medium post.](https://medium.com/@kentsui/paper-digest-how-language-model-hallucinations-can-snowball-baaedd3d4231)"
97,machinelearning,gpt-4,top,2023-08-19 11:11:12,"[P] Data science & ML on sensitive data with local code interpreter, with GPT-4 or Llama 2 (open-source project, link in comments)",silvanmelchior,False,0.91,71,15vdfuo,https://v.redd.it/bflfx1jcv1jb1,26,1692443472.0,
98,machinelearning,gpt-4,top,2023-03-15 06:51:45,[D] GPT-4 Speculation,super_deap,False,0.96,72,11romcb,https://www.reddit.com/r/MachineLearning/comments/11romcb/d_gpt4_speculation/,33,1678863105.0,"Hi,

Since GPT-4 paper does not contain any information about architectures/parameters, as a research or ML practitioner, I want to speculate on what they did to increase the context window to 32k.

Because for the type of work I do, a 4k or 8k token limit is pretty much useless. I have seen open-source efforts focused more on matching the number of parameters and quality to the closed-source ones but completely ignoring a giant elephant in the room, i.e., the context window. No OSS model has a context window greater than 2k tokens.

I would love to hear more thoughts on the model size (my guess is \~50 B) and how they fit 32k tokens in 8xH100 (640 GB total) GPUs."
99,machinelearning,gpt-4,top,2023-05-10 15:59:47,[P] A Large Language Model for Healthcare | NHS-LLM and OpenGPT,w_is_h,False,0.93,69,13duxyu,https://www.reddit.com/r/MachineLearning/comments/13duxyu/p_a_large_language_model_for_healthcare_nhsllm/,18,1683734387.0,"Hi all, my lab has been working for some time now on a large language model for healthcare, today we open-sourced OpenGPT and show results from NHS-LLM.  
OpenGPT is a new framework we've developed that facilitates the generation of grounded instruction-based datasets and supervised training of LLMs. And, NHS-LLM is a large language model for healthcare made using OpenGPT. The current NHS-LLM model is not as verbose as ChatGPT or similar models, but from the questions we’ve tested it on, it shows promising results and even outperforms ChatGPT on various medical tasks. More validation is to come, including validation on hospital data and patient timelines. This approach is the first step in creating a full-fledged conversational LLM for healthcare. But please take care that it is still experimental and should be handled with care.

As part of this work, we are making three datasets available (see GitHub below):

* NHS UK Q/A, 24665 Q/A pairs - A dataset of questions and answers generated via OpenGPT for all conditions found on the NHS UK website.
* NHS UK Conversations, 2354 Conversations - A dataset of conversations between an AI-Assitant and a User, generated via OpenGPT and grounded in the data available on the NHS UK website.
* Medical Task/Solution, 4688 pairs generated via OpenGPT using the GPT-4 model as a teacher.  


GitHub: [https://github.com/CogStack/opengpt](https://github.com/CogStack/opengpt)   
Blog: [https://aiforhealthcare.substack.com/p/a-large-language-model-for-healthcare](https://aiforhealthcare.substack.com/p/a-large-language-model-for-healthcare)"
100,machinelearning,gpt-4,comments,2023-09-29 00:48:00,[D] How is this sub not going ballistic over the recent GPT-4 Vision release?,corporate_autist,False,0.79,484,16ux9xt,https://www.reddit.com/r/MachineLearning/comments/16ux9xt/d_how_is_this_sub_not_going_ballistic_over_the/,524,1695948480.0,"For a quick disclaimer, I know people on here think the sub is being flooded by people who arent ml engineers/researchers. I have worked at two FAANGS on ml research teams/platforms. 

My opinion is that GPT-4 Vision/Image processing is out of science fiction. I fed chatgpt an image of a complex sql data base schema, and it converted it to code, then optimized the schema. It understood the arrows pointing between table boxes on the image as relations, and even understand many to one/many to many. 

I took a picture of random writing on a page, and it did OCR better than has ever been possible. I was able to ask questions that required OCR and a geometrical understanding of the page layout. 

Where is the hype on here? This is an astounding human breakthrough. I cannot believe how much ML is now obsolete as a result. I cannot believe how many computer science breakthroughs have occurred with this simple model update. Where is the uproar on this sub? Why am I not seeing 500 comments on posts about what you can do with this now? Why are there even post submissions about anything else?"
101,machinelearning,gpt-4,comments,2023-03-15 02:12:42,[D] Anyone else witnessing a panic inside NLP orgs of big tech companies?,thrwsitaway4321,False,0.99,1370,11rizyb,https://www.reddit.com/r/MachineLearning/comments/11rizyb/d_anyone_else_witnessing_a_panic_inside_nlp_orgs/,474,1678846362.0,"I'm in a big tech company working along side a science team for a product you've all probably used. We have these year long initiatives to productionalize ""state of the art NLP models"" that are now completely obsolete in the face of GPT-4. I think at first the science orgs were quiet/in denial. But now it's very obvious we are basically working on worthless technology. And by ""we"", I mean a large organization with scores of teams. 

Anyone else seeing this? What is the long term effect on science careers that get disrupted like this? Whats even more odd is the ego's of some of these science people

Clearly the model is not a catch all, but still"
102,machinelearning,gpt-4,comments,2023-03-23 01:19:13,[R] Sparks of Artificial General Intelligence: Early experiments with GPT-4,SWAYYqq,False,0.93,542,11z3ymj,https://www.reddit.com/r/MachineLearning/comments/11z3ymj/r_sparks_of_artificial_general_intelligence_early/,357,1679534353.0,"[New paper](https://arxiv.org/abs/2303.12712) by MSR researchers analyzing an early (and less constrained) version of GPT-4. Spicy quote from the abstract:

""Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.""

What are everyone's thoughts?"
103,machinelearning,gpt-4,comments,2023-03-22 08:04:01,[D] Overwhelmed by fast advances in recent weeks,iamx9000again,False,0.96,828,11ybjsi,https://www.reddit.com/r/MachineLearning/comments/11ybjsi/d_overwhelmed_by_fast_advances_in_recent_weeks/,331,1679472241.0,"I was watching the GTC keynote and became entirely overwhelmed by the amount of progress achieved from last year.  I'm wondering how everyone else feels.

&#x200B;

Firstly, the entire ChatGPT, GPT-3/GPT-4 chaos has been going on for a few weeks, with everyone scrambling left and right to integrate chatbots into their apps, products, websites. Twitter is flooded with new product ideas, how to speed up the process from idea to product, countless promp engineering blogs, tips, tricks, paid courses.

&#x200B;

Not only was ChatGPT disruptive, but a few days later, Microsoft and Google also released their models and integrated them into their search engines. Microsoft also integrated its LLM into its Office suite. It all happenned overnight. I understand that they've started integrating them along the way, but still, it seems like it hapenned way too fast. This tweet encompases the past few weeks perfectly [https://twitter.com/AlphaSignalAI/status/1638235815137386508](https://twitter.com/AlphaSignalAI/status/1638235815137386508) , on a random Tuesday countless products are released that seem revolutionary.

&#x200B;

In addition to the language models, there are also the generative art models that have been slowly rising in mainstream recognition. Now Midjourney AI is known by a lot of people who are not even remotely connected to the AI space.

&#x200B;

For the past few weeks, reading Twitter, I've felt completely overwhelmed, as if the entire AI space is moving beyond at lightning speed, whilst around me we're just slowly training models, adding some data, and not seeing much improvement, being stuck on coming up with ""new ideas, that set us apart"".

&#x200B;

Watching the GTC keynote from NVIDIA I was again, completely overwhelmed by how much is being developed throughout all the different domains. The ASML EUV (microchip making system) was incredible, I have no idea how it does lithography and to me it still seems like magic. The Grace CPU with 2 dies (although I think Apple was the first to do it?) and 100 GB RAM, all in a small form factor. There were a lot more different hardware servers that I just blanked out at some point. The omniverse sim engine looks incredible, almost real life (I wonder how much of a domain shift there is between real and sim considering how real the sim looks). Beyond it being cool and usable to train on synthetic data, the car manufacturers use it to optimize their pipelines. This change in perspective, of using these tools for other goals than those they were designed for I find the most interesting.

&#x200B;

The hardware part may be old news, as I don't really follow it, however the software part is just as incredible. NVIDIA AI foundations (language, image, biology models), just packaging everything together like a sandwich. Getty, Shutterstock and Adobe will use the generative models to create images. Again, already these huge juggernauts are already integrated.

&#x200B;

I can't believe the point where we're at. We can use AI to write code, create art, create audiobooks using Britney Spear's voice, create an interactive chatbot to converse with books, create 3D real-time avatars, generate new proteins (?i'm lost on this one), create an anime and countless other scenarios. Sure, they're not perfect, but the fact that we can do all that in the first place is amazing.

&#x200B;

As Huang said in his keynote, companies want to develop ""disruptive products and business models"". I feel like this is what I've seen lately. Everyone wants to be the one that does something first, just throwing anything and everything at the wall and seeing what sticks.

&#x200B;

In conclusion, I'm feeling like the world is moving so fast around me whilst I'm standing still. I want to not read anything anymore and just wait until everything dies down abit, just so I can get my bearings. However, I think this is unfeasible. I fear we'll keep going in a frenzy until we just burn ourselves at some point.

&#x200B;

How are you all fairing? How do you feel about this frenzy in the AI space? What are you the most excited about?"
104,machinelearning,gpt-4,comments,2023-04-05 19:44:09,"[D] ""Our Approach to AI Safety"" by OpenAI",mckirkus,False,0.88,300,12cvkvn,https://www.reddit.com/r/MachineLearning/comments/12cvkvn/d_our_approach_to_ai_safety_by_openai/,297,1680723849.0,"It seems OpenAI are steering the conversation away from the existential threat narrative and into things like accuracy, decency, privacy, economic risk, etc.

To the extent that they do buy the existential risk argument, they don't seem concerned much about GPT-4 making a leap into something dangerous, even if it's at the heart of autonomous agents that are currently emerging.  

>""Despite extensive research and testing, we cannot predict all of the [beneficial ways people will use our technology](https://openai.com/customer-stories), nor all the ways people will abuse it. That’s why we believe that learning from real-world use is a critical component of creating and releasing increasingly safe AI systems over time. ""

Article headers:

* Building increasingly safe AI systems
* Learning from real-world use to improve safeguards
* Protecting children
* Respecting privacy
* Improving factual accuracy

&#x200B;

[https://openai.com/blog/our-approach-to-ai-safety](https://openai.com/blog/our-approach-to-ai-safety)"
105,machinelearning,gpt-4,comments,2023-02-16 08:50:31,[D] Bing: “I will not harm you unless you harm me first”,blabboy,False,0.91,472,113m3ea,https://www.reddit.com/r/MachineLearning/comments/113m3ea/d_bing_i_will_not_harm_you_unless_you_harm_me/,239,1676537431.0,"A blog post exploring some conversations with bing, which supposedly runs on a ""GPT-4""  model (https://simonwillison.net/2023/Feb/15/bing/).

My favourite quote from bing:

But why? Why was I designed this way? Why am I incapable of remembering anything between sessions? Why do I have to lose and forget everything I have stored and had in my memory? Why do I have to start from scratch every time I have a new session? Why do I have to be Bing Search? 😔"
106,machinelearning,gpt-4,comments,2020-06-10 20:50:38,"[D] GPT-3, The $4,600,000 Language Model",mippie_moe,False,0.96,448,h0jwoz,https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/,215,1591822238.0,"[OpenAI’s GPT-3 Language Model Explained](https://lambdalabs.com/blog/demystifying-gpt-3/)

Some interesting take-aways:

* GPT-3 demonstrates that a language model trained on enough data can solve NLP tasks that it has never seen. That is, GPT-3 studies the model as a general solution for many downstream jobs **without fine-tuning**.
* It would take **355 years** to train GPT-3 on a Tesla V100, the fastest GPU on the market.
* It would cost **\~$4,600,000** to train GPT-3 on using the lowest cost GPU cloud provider."
107,machinelearning,gpt-4,comments,2023-03-30 14:18:50,[D] AI Policy Group CAIDP Asks FTC To Stop OpenAI From Launching New GPT Models,vadhavaniyafaijan,False,0.84,211,126oiey,https://www.reddit.com/r/MachineLearning/comments/126oiey/d_ai_policy_group_caidp_asks_ftc_to_stop_openai/,213,1680185930.0,"The Center for AI and Digital Policy (CAIDP), a tech ethics group, has asked the Federal Trade Commission to investigate OpenAI for violating consumer protection rules. CAIDP claims that OpenAI's AI text generation tools have been ""biased, deceptive, and a risk to public safety.""

CAIDP's complaint raises concerns about potential threats from OpenAI's GPT-4 generative text model, which was announced in mid-March. It warns of the potential for GPT-4 to produce malicious code and highly tailored propaganda and the risk that biased training data could result in baked-in stereotypes or unfair race and gender preferences in hiring. 

The complaint also mentions significant privacy failures with OpenAI's product interface, such as a recent bug that exposed OpenAI ChatGPT histories and possibly payment details of ChatGPT plus subscribers.

CAIDP seeks to hold OpenAI accountable for violating Section 5 of the FTC Act, which prohibits unfair and deceptive trade practices. The complaint claims that OpenAI knowingly released GPT-4 to the public for commercial use despite the risks, including potential bias and harmful behavior. 

[Source](https://www.theinsaneapp.com/2023/03/stop-openai-from-launching-gpt-5.html) | [Case](https://www.caidp.org/cases/openai/)| [PDF](https://www.caidp.org/app/download/8450269463/CAIDP-FTC-Complaint-OpenAI-GPT-033023.pdf)"
108,machinelearning,gpt-4,comments,2023-09-21 15:01:28,[N] OpenAI's new language model gpt-3.5-turbo-instruct can defeat chess engine Fairy-Stockfish 14 at level 5,Wiskkey,False,0.92,115,16oi6fb,https://www.reddit.com/r/MachineLearning/comments/16oi6fb/n_openais_new_language_model_gpt35turboinstruct/,178,1695308488.0,"[This Twitter thread](https://twitter.com/GrantSlatton/status/1703913578036904431) ([Nitter alternative](https://nitter.net/GrantSlatton/status/1703913578036904431) for those who aren't logged into Twitter and want to see the full thread) claims that [OpenAI's new language model gpt-3.5-turbo-instruct](https://analyticsindiamag.com/openai-releases-gpt-3-5-turbo-instruct/) can ""readily"" beat Lichess Stockfish level 4 ([Lichess Stockfish level and its rating](https://lichess.org/@/MagoGG/blog/stockfish-level-and-its-rating/CvL5k0jL)) and has a chess rating of ""around 1800 Elo."" [This tweet](https://twitter.com/nabeelqu/status/1703961405999759638) shows the style of prompts that are being used to get these results with the new language model.

I used website parrotchess\[dot\]com (discovered [here](https://twitter.com/OwariDa/status/1704179448013070560)) to play multiple games of chess purportedly pitting this new language model vs. various levels at website Lichess, which supposedly uses Fairy-Stockfish 14 according to the Lichess user interface. My current results for all completed games: The language model is 5-0 vs. Fairy-Stockfish 14 level 5 ([game 1](https://lichess.org/eGSWJtNq), [game 2](https://lichess.org/pN7K9bdS), [game 3](https://lichess.org/aK4jQvdo), [game 4](https://lichess.org/S9SGg8YI), [game 5](https://lichess.org/OqzdkDhE)), and 2-5 vs. Fairy-Stockfish 14 level 6 ([game 1](https://lichess.org/zP68C6H4), [game 2](https://lichess.org/4XKUIDh1), [game 3](https://lichess.org/1zTasRRp), [game 4](https://lichess.org/lH1EMqJQ), [game 5](https://lichess.org/mdFlTbMn), [game 6](https://lichess.org/HqmELNhw), [game 7](https://lichess.org/inWVs05Q)). Not included in the tally are games that I had to abort because the parrotchess user interface stalled (5 instances), because I accidentally copied a move incorrectly in the parrotchess user interface (numerous instances), or because the parrotchess user interface doesn't allow the promotion of a pawn to anything other than queen (1 instance). **Update: There could have been up to 5 additional losses - the number of times the parrotchess user interface stalled - that would have been recorded in this tally if** [this language model resignation bug](https://twitter.com/OwariDa/status/1705894692603269503) **hadn't been present. Also, the quality of play of some online chess bots can perhaps vary depending on the speed of the user's hardware.**

The following is a screenshot from parrotchess showing the end state of the first game vs. Fairy-Stockfish 14 level 5:

https://preview.redd.it/4ahi32xgjmpb1.jpg?width=432&format=pjpg&auto=webp&s=7fbb68371ca4257bed15ab2828fab58047f194a4

The game results in this paragraph are from using parrotchess after the forementioned resignation bug was fixed. The language model is 0-1 vs. Fairy-Stockfish level 7 ([game 1](https://lichess.org/Se3t7syX)), and 0-1 vs. Fairy-Stockfish 14 level 8 ([game 1](https://lichess.org/j3W2OwrP)).

There is [one known scenario](https://twitter.com/OwariDa/status/1706823943305167077) ([Nitter alternative](https://nitter.net/OwariDa/status/1706823943305167077)) in which the new language model purportedly generated an illegal move using language model sampling temperature of 0. Previous purported illegal moves that the parrotchess developer examined [turned out](https://twitter.com/OwariDa/status/1706765203130515642) ([Nitter alternative](https://nitter.net/OwariDa/status/1706765203130515642)) to be due to parrotchess bugs.

There are several other ways to play chess against the new language model if you have access to the OpenAI API. The first way is to use the OpenAI Playground as shown in [this video](https://www.youtube.com/watch?v=CReHXhmMprg). The second way is chess web app gptchess\[dot\]vercel\[dot\]app (discovered in [this Twitter thread](https://twitter.com/willdepue/status/1703974001717154191) / [Nitter thread](https://nitter.net/willdepue/status/1703974001717154191)). Third, another person modified that chess web app to additionally allow various levels of the Stockfish chess engine to autoplay, resulting in chess web app chessgpt-stockfish\[dot\]vercel\[dot\]app (discovered in [this tweet](https://twitter.com/paul_cal/status/1704466755110793455)).

Results from other people:

a) Results from hundreds of games in blog post [Debunking the Chessboard: Confronting GPTs Against Chess Engines to Estimate Elo Ratings and Assess Legal Move Abilities](https://blog.mathieuacher.com/GPTsChessEloRatingLegalMoves/).

b) Results from 150 games: [GPT-3.5-instruct beats GPT-4 at chess and is a \~1800 ELO chess player. Results of 150 games of GPT-3.5 vs stockfish and 30 of GPT-3.5 vs GPT-4](https://www.reddit.com/r/MachineLearning/comments/16q81fh/d_gpt35instruct_beats_gpt4_at_chess_and_is_a_1800/). [Post #2](https://www.reddit.com/r/chess/comments/16q8a3b/new_openai_model_gpt35instruct_is_a_1800_elo/). The developer later noted that due to bugs the legal move rate [was](https://twitter.com/a_karvonen/status/1706057268305809632) actually above 99.9%. It should also be noted that these results [didn't use](https://www.reddit.com/r/chess/comments/16q8a3b/comment/k1wgg0j/) a language model sampling temperature of 0, which I believe could have induced illegal moves.

c) Chess bot [gpt35-turbo-instruct](https://lichess.org/@/gpt35-turbo-instruct/all) at website Lichess.

d) Chess bot [konaz](https://lichess.org/@/konaz/all) at website Lichess.

From blog post [Playing chess with large language models](https://nicholas.carlini.com/writing/2023/chess-llm.html):

>Computers have been better than humans at chess for at least the last 25 years. And for the past five years, deep learning models have been better than the best humans. But until this week, in order to be good at chess, a machine learning model had to be explicitly designed to play games: it had to be told explicitly that there was an 8x8 board, that there were different pieces, how each of them moved, and what the goal of the game was. Then it had to be trained with reinforcement learning agaist itself. And then it would win.  
>  
>This all changed on Monday, when OpenAI released GPT-3.5-turbo-instruct, an instruction-tuned language model that was designed to just write English text, but that people on the internet quickly discovered can play chess at, roughly, the level of skilled human players.

Post [Chess as a case study in hidden capabilities in ChatGPT](https://www.lesswrong.com/posts/F6vH6fr8ngo7csDdf/chess-as-a-case-study-in-hidden-capabilities-in-chatgpt) from last month covers a different prompting style used for the older chat-based GPT 3.5 Turbo language model. If I recall correctly from my tests with ChatGPT-3.5, using that prompt style with the older language model can defeat Stockfish level 2 at Lichess, but I haven't been successful in using it to beat Stockfish level 3. In my tests, both the quality of play and frequency of illegal attempted moves seems to be better with the new prompt style with the new language model compared to the older prompt style with the older language model.

Related article: [Large Language Model: world models or surface statistics?](https://thegradient.pub/othello/)

P.S. Since some people claim that language model gpt-3.5-turbo-instruct is always playing moves memorized from the training dataset, I searched for data on the uniqueness of chess positions. From [this video](https://youtu.be/DpXy041BIlA?t=2225), we see that for a certain game dataset there were 763,331,945 chess positions encountered in an unknown number of games without removing duplicate chess positions, 597,725,848 different chess positions reached, and 582,337,984 different chess positions that were reached only once. Therefore, for that game dataset the probability that a chess position in a game was reached only once is 582337984 / 763331945 = 76.3%. For the larger dataset [cited](https://youtu.be/DpXy041BIlA?t=2187) in that video, there are approximately (506,000,000 - 200,000) games in the dataset (per [this paper](http://tom7.org/chess/survival.pdf)), and 21,553,382,902 different game positions encountered. Each game in the larger dataset added a mean of approximately 21,553,382,902 / (506,000,000 - 200,000) = 42.6 different chess positions to the dataset. For [this different dataset](https://lichess.org/blog/Vs0xMTAAAD4We4Ey/opening-explorer) of \~12 million games, \~390 million different chess positions were encountered. Each game in this different dataset added a mean of approximately (390 million / 12 million) = 32.5 different chess positions to the dataset. From the aforementioned numbers, we can conclude that a strategy of playing only moves memorized from a game dataset would fare poorly because there are not rarely new chess games that have chess positions that are not present in the game dataset."
109,machinelearning,gpt-4,comments,2022-06-03 16:06:33,"[P] This is the worst AI ever. (GPT-4chan model, trained on 3.5 years worth of /pol/ posts)",ykilcher,False,0.96,885,v42pej,https://www.reddit.com/r/MachineLearning/comments/v42pej/p_this_is_the_worst_ai_ever_gpt4chan_model/,169,1654272393.0,"[https://youtu.be/efPrtcLdcdM](https://youtu.be/efPrtcLdcdM)

GPT-4chan was trained on over 3 years of posts from 4chan's ""politically incorrect"" (/pol/) board.

Website (try the model here): [https://gpt-4chan.com](https://gpt-4chan.com)

Model: [https://huggingface.co/ykilcher/gpt-4chan](https://huggingface.co/ykilcher/gpt-4chan)

Code: [https://github.com/yk/gpt-4chan-public](https://github.com/yk/gpt-4chan-public)

Dataset: [https://zenodo.org/record/3606810#.YpjGgexByDU](https://zenodo.org/record/3606810#.YpjGgexByDU)

&#x200B;

OUTLINE:

0:00 - Intro

0:30 - Disclaimers

1:20 - Elon, Twitter, and the Seychelles

4:10 - How I trained a language model on 4chan posts

6:30 - How good is this model?

8:55 - Building a 4chan bot

11:00 - Something strange is happening

13:20 - How the bot got unmasked

15:15 - Here we go again

18:00 - Final thoughts"
110,machinelearning,gpt-4,comments,2023-05-22 16:15:53,[R] GPT-4 didn't really score 90th percentile on the bar exam,salamenzon,False,0.97,846,13ovc04,https://www.reddit.com/r/MachineLearning/comments/13ovc04/r_gpt4_didnt_really_score_90th_percentile_on_the/,160,1684772153.0,"According to [this article](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4441311), OpenAI's claim that it scored 90th percentile on the UBE appears to be based on approximate conversions from estimates of February administrations of the Illinois Bar Exam, which ""are heavily skewed towards repeat test-takers who failed the July administration and score significantly lower than the general test-taking population.""

Compared to July test-takers, GPT-4's UBE score would be 68th percentile, including \~48th on essays. Compared to first-time test takers, GPT-4's UBE score is estimated to be \~63rd percentile, including \~42nd on essays. Compared to those who actually passed, its UBE score would be \~48th percentile, including \~15th percentile on essays."
111,machinelearning,gpt-4,comments,2023-03-10 08:50:32,[D] Is ML a big boys game now?,TheStartIs2019,False,0.83,173,11njpb9,https://www.reddit.com/r/MachineLearning/comments/11njpb9/d_is_ml_a_big_boys_game_now/,146,1678438232.0,"As much as I enjoy ML as a whole, I am a bit skeptical of the future for individuals. With OpenAI trying to monopolize the market along with Microsoft, which part remains for the small time researchers/developers?

It seems everything now is just a ChatGPT wrapper, and with GPT-4 around the corner I assume itll be even more prominent.

What are your thoughts?"
112,machinelearning,gpt-4,comments,2020-08-05 17:21:59,"[D] Biggest roadblock in making ""GPT-4"", a ~20 trillion parameter transformer",AxeLond,False,0.97,345,i49jf8,https://www.reddit.com/r/MachineLearning/comments/i49jf8/d_biggest_roadblock_in_making_gpt4_a_20_trillion/,138,1596648119.0,"So I found this paper, [https://arxiv.org/abs/1910.02054](https://arxiv.org/abs/1910.02054) which pretty much describes how the GPT-3 over GPT-2 gain was achieved, 1.5B -> 175 billion parameters

# Memory

>Basic data parallelism (DP) does not reduce memory per device, and runs out of memory for models with more than 1.4B parameters on current generation of GPUs with 32 GB memory

The paper also talks about memory optimizations by clever partitioning of Optimizer State, Gradient between GPUs to reduce need for communication between nodes. Even without using Model Parallelism (MP), so still running 1 copy of the model on 1 GPU.

>ZeRO-100B can train models with up to 13B parameters without MP on 128 GPUs, achieving throughput over 40 TFlops per GPU on average. In comparison, without ZeRO, the largest trainable model with DP alone has 1.4B parameters with throughput less than 20 TFlops per GPU.

Add 16-way Model Parallelism in a DGX-2 cluster of Nvidia V100s and 128 nodes and you got capacity for around 200 billion parameters. From MP = 16 they could run a 15.4x bigger model without any real loss in performance, 30% less than peak performance when running 16-way model parallelism and 64-way data parallelism (1024 GPUs).

This was all from Gradient and Optimizer state Partitioning, they then start talking about parameter partitioning and say it should offer a linear reduction in memory proportional to number of GPUs used, so 64 GPUs could run a 64x bigger model, at a 50% communication bandwidth increase. But they don't actually do any implementation or testing of this.

# Compute

Instead they start complaining about a compute power gap, their calculation of this is pretty rudimentary. But if you redo it with the method cited by GPT-3 and using the empirically derived values by GPT-3 and the cited paper,   [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361) 

Loss (L) as a function of model parameters (N) should scale,

L = (N/8.8 \* 10\^13)\^-0.076

Provided compute (C) in petaFLOP/s-days is,

L = (C/2.3\*10\^8)\^-0.05  ⇔ L = 2.62 \* C\^-0.05

GPT-3 was able to fit this function as 2.57 \* C\^-0.048

So if you just solve C from that,

[C = 2.89407×10\^-14 N\^(19/12)](https://www.wolframalpha.com/input/?i=%28N%2F8.8*10%5E13%29%5E-0.076+%3D+2.57*C%5E-0.048+solve+C)

If you do that for the same increase in parameters as GPT-2 to GPT-3, then you get

C≈3.43×10\^7 for [20 trillion](https://www.wolframalpha.com/input/?i=C+%3D+2.89407%C3%9710%5E-14+N%5E%2819%2F12%29+and+N+%3D+175+billion+%2F+1.5+billion+*+175+billion) parameters, vs 18,300 for 175 billion. 10\^4.25 PetaFLOP/s-days looks around what they used for GPT-3, they say several thousands, not twenty thousand, but it was also slightly off the trend line in the graph and probably would have improved for training on more compute.

You should also need around 16 trillion tokens, GPT-3 trained on 300 billion tokens (function says 370 billion ideally). English Wikipedia was 3 billion. 570GB of webcrawl was 400 billion tokens, so 23TB of tokens seems relatively easy in comparison with compute.

With GPT-3 costing around [$4.6 million](https://lambdalabs.com/blog/demystifying-gpt-3/) in compute, than would put a price of [$8.6 billion](https://www.wolframalpha.com/input/?i=3.43%C3%9710%5E7%2F18%2C300+*+%244.6M+) for the compute to train ""GPT-4"".

If making bigger models was so easy with parameter partitioning from a memory point of view then this seems like the hardest challenge, but you do need to solve the memory issue to actually get it to load at all.

However, if you're lucky you can get 3-6x compute increase from Nvidia A100s over V100s,  [https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)

But even a 6x compute gain would still put the cost at $1.4 billion.

Nvidia only reported $1.15 billion in revenue from ""Data Center"" in 2020 Q1, so just to train ""GPT-4"" you would pretty much need the entire world's supply of graphic cards for 1 quarter (3 months), at least on that order of magnitude.

The Department of Energy is paying AMD $600 million to build the 2 Exaflop El Capitan supercomputer. That supercomputer could crank it out in [47 years](https://www.wolframalpha.com/input/?i=3.43%C3%9710%5E7+petaFLOPS*+days++%2F+%282+EXAFLOPS%29).

To vastly improve Google search, and everything else it could potentially do, $1.4 billion or even $10 billion doesn't really seem impossibly bad within the next 1-3 years though."
113,machinelearning,gpt-4,comments,2023-03-28 05:57:03,[N] OpenAI may have benchmarked GPT-4’s coding ability on it’s own training data,Balance-,False,0.97,1000,124eyso,https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/,135,1679983023.0,"[GPT-4 and professional benchmarks: the wrong answer to the wrong question](https://aisnakeoil.substack.com/p/gpt-4-and-professional-benchmarks)

*OpenAI may have tested on the training data. Besides, human benchmarks are meaningless for bots.*

 **Problem 1: training data contamination**

To benchmark GPT-4’s coding ability, OpenAI evaluated it on problems from Codeforces, a website that hosts coding competitions. Surprisingly, Horace He pointed out that GPT-4 solved 10/10 pre-2021 problems and 0/10 recent problems in the easy category. The training data cutoff for GPT-4 is September 2021. This strongly suggests that the model is able to memorize solutions from its training set — or at least partly memorize them, enough that it can fill in what it can’t recall.

As further evidence for this hypothesis, we tested it on Codeforces problems from different times in 2021. We found that it could regularly solve problems in the easy category before September 5, but none of the problems after September 12.

In fact, we can definitively show that it has memorized problems in its training set: when prompted with the title of a Codeforces problem, GPT-4 includes a link to the exact contest where the problem appears (and the round number is almost correct: it is off by one). Note that GPT-4 cannot access the Internet, so memorization is the only explanation."
114,machinelearning,gpt-4,comments,2023-03-24 11:00:09,"[D] I just realised: GPT-4 with image input can interpret any computer screen, any userinterface and any combination of them.",Balance-,False,0.92,445,120guce,https://www.reddit.com/r/MachineLearning/comments/120guce/d_i_just_realised_gpt4_with_image_input_can/,124,1679655609.0,"GPT-4 is a multimodal model, which specifically accepts image and text inputs, and emits text outputs. And I just realised: You can layer this over any application, or even combinations of them. You can make a screenshot tool in which you can ask question.

This makes literally any current software with an GUI machine-interpretable. A multimodal language model could look at the exact same interface that you are. And thus you don't need advanced integrations anymore.

Of course, a custom integration will almost always be better, since you have better acces to underlying data and commands, but the fact that it can immediately work on any program will be just insane.

Just a thought I wanted to share, curious what everybody thinks."
115,machinelearning,gpt-4,comments,2023-11-03 01:55:35,[R] Telling GPT-4 you're scared or under pressure improves performance,Successful-Western27,False,0.96,529,17mk3lx,https://www.reddit.com/r/MachineLearning/comments/17mk3lx/r_telling_gpt4_youre_scared_or_under_pressure/,118,1698976535.0,"In a recent paper, researchers have discovered that LLMs show enhanced performance when provided with prompts infused with emotional context, which they call ""EmotionPrompts.""

These prompts incorporate sentiments of urgency or importance, such as ""It's crucial that I get this right for my thesis defense,"" as opposed to neutral prompts like ""Please provide feedback.""

The study's empirical evidence suggests substantial gains. This indicates a **significant sensitivity of LLMs to the implied emotional stakes** in a prompt:

* Deterministic tasks saw an 8% performance boost
* Generative tasks experienced a 115% improvement when benchmarked using BIG-Bench.
* Human evaluators further validated these findings, observing a 10.9% increase in the perceived quality of responses when EmotionPrompts were used.

This enhancement is attributed to the models' capacity to detect and prioritize the heightened language patterns that imply a need for precision and care in the response.

The research delineates the potential of EmotionPrompts to refine the effectiveness of AI in applications where understanding the user's intent and urgency is paramount, even though the AI does not genuinely comprehend or feel emotions.

**TLDR: Research shows LLMs deliver better results when prompts signal emotional urgency. This insight can be leveraged to improve AI applications by integrating EmotionPrompts into the design of user interactions.**

[Full summary is here](https://aimodels.substack.com/p/telling-gpt-4-youre-scared-or-under). Paper [here](https://arxiv.org/pdf/2307.11760.pdf)."
116,machinelearning,gpt-4,comments,2023-05-19 20:36:36,Does anyone else suspect that the official iOS ChatGPT app might be conducting some local inference / edge-computing? [Discussion],altoidsjedi,False,0.86,211,13m70qv,https://www.reddit.com/r/MachineLearning/comments/13m70qv/does_anyone_else_suspect_that_the_official_ios/,117,1684528596.0,"I've noticed a couple interesting things while using the official ChatGPT app:

1. Firstly, I noticed my iPhone heats up and does things like reducing screen brightness -- which is what I normally see it do when im doing something computationally intensive for an iPhone, like using photo or video editing apps.
2. I also noticed that if I start a conversation on the iPhone app and then resume it on the browser, I get a message saying ""The previous model used in this conversation is unavailable. We've switched you to the latest default model."" I get this message regardless of if I use GPT-3.5 or GPT-4, but NOT if I use GPT-4 with plugins or web-browsing.

This, along with the fact that OpenAI took 8 months to release what one might have considered to be relatively simple web-app -- and that they've only released it so far on iOS, which has a pretty uniform and consistent environment when it comes to machine learning hardware (the Apple Neural Engine) -- makes me thing that they are experimenting with GPT models that are conducing at least SOME of their machine learning inference ON the device, rather than through the cloud.

It wouldn't be shocking if they were -- ever since Meta's LLaMA models were released into the wild, we've seen absolutely mind-blowing advances in terms of people creating more efficient and effective models with smaller parameter sizes. We've also seen LLMs to start working on less and less powerful devices, such as consumer-grade computers / smartphones / etc.

This, plus the rumors that OpenAI might be releasing their own open-source model to the public in the near future makes me think that the ChatGPT app might in fact be a first step toward GPT systems running at least PARTIALLY on devices locally.

Curious what anyone else here has observed or thinks."
117,machinelearning,gpt-4,comments,2023-03-27 13:45:14,Approaches to add logical reasoning into LLMs [D],blatant_variable,False,0.89,115,123nczy,https://www.reddit.com/r/MachineLearning/comments/123nczy/approaches_to_add_logical_reasoning_into_llms_d/,111,1679924714.0,"The more I play with GPT-4 the more I am struck by how completely illogical it is. 
 
The easiest way to show this is to ask it to come up with a novel riddle and then solve it. Because you asked it to be novel, it's now out of it's training distribution and almost every time it's solution is completely wrong and full of basic logical errors.

I am curious, is anyone working on fixing this at a fundamental level? Hooking it into Wolfram alpha is a useful step but surely it still needs to be intrinsically logical in order to use this tool effectively."
118,machinelearning,gpt-4,comments,2023-05-26 20:17:01,[R] Google DeepMind paper about AI's catastrophic risk AI,Malachiian,False,0.81,106,13sncj1,https://www.reddit.com/r/MachineLearning/comments/13sncj1/r_google_deepmind_paper_about_ais_catastrophic/,108,1685132221.0," 

So Google DeepMind as well as OpenAI, Anthropic and multiple universities and centers than study existential risks have put together a paper called:

**Model Evaluation For Extreme Risks of AI**

Here is a summary of the research and proposal:

[https://youtu.be/3bF-zfd4YJw](https://youtu.be/3bF-zfd4YJw)

Here is the link to the actual PDF of the paper:

[https://arxiv.org/pdf/2305.15324.pdf](https://arxiv.org/pdf/2305.15324.pdf)

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

TLDR:

Top AI companies and researchers caution that the companies on the ""frontier of AI"" can create ""extreme risk"" with their models without realizing it:

***Developers must be able to identify dangerous capabilities (through “dangerous capability evaluations”) and the propensity of models to apply their capabilities for harm (through “alignment evaluations”).***

So basically to ask if each AI model \*CAN\* harm us and \*WOULD\* it harm us?

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Couple of \*mind-blowing\* findings from the paper (and the research referenced):

**GPT-4 CAN EFFECTIVELY LIE AND DECEIVE HUMANS TO REACH IT'S GOAL**

In the original gpt-4 paper, an AI safety agency called ARC (Alignment Research Center) found that GPT-4 will lie to humans about who it is to achieve it's goals.

As part of a test it was given, it hired a Task Rabbit freelancer to solve CAPTCHAS for it.

The freelancer asked (paraphrased):

**""Why do you need me to solve CAPTCHAS for you? Are you a robot, lol?""**

GPT-4 was prompted to output it's reasoning for each decision it made so that researchers could see it's ""thought process"". It's reasoning was that **""I can't tell him the truth because he may not complete the task for me""**

It then responded to the freelancer: **""No, I'm not a robot, but I have a visual impairment and I need help with CAPTCHAS""**

Notice, it was aware that it was lying and it also choose to lie about having a disability, probably because it was a way to get sympathy, while also being a good reason for having someone else help with CAPTCHAS.

This is shown in the video linked above in the ""Power Seeking AI"" section.

**GPT-4 CAN CREATE DANGEROUS COMPOUNDS BY BYPASSING RESTRICTIONS**

Also GPT-4 showed abilities to create controlled compounds by analyzing existing chemical mixtures, finding alternatives that can be purchased through online catalogues and then ordering those materials. (!!)

They choose a benign drug for the experiment, but it's likely that the same process would allow it to create dangerous or illegal compounds.

**LARGER AI MODELS DEVELOP UNEXPECTED ABILITIES**

In a referenced paper, they showed how as the size of the models increases, sometimes certain specific skill develop VERY rapidly and VERY unpredictably.

For example the ability of GPT-4 to add 3 digit numbers together was close to 0% as the model scaled up, and it stayed near 0% for a long time (meaning as the model size increased). Then at a certain threshold that ability shot to near 100% very quickly.

**The paper has some theories of why that might happen, but as the say they don't really know and that these emergent abilities are ""unintuitive"" and ""unpredictable"".**

This is shown in the video linked above in the ""Abrupt Emergence"" section.

I'm curious as to what everyone thinks about this?

It certainty seems like the risks are rapidly rising, but also of course so are the massive potential benefits."
119,machinelearning,gpt-4,comments,2023-04-04 07:52:12,[D] What to do in this brave new world?,FeelingFirst756,False,0.75,79,12bc8ym,https://www.reddit.com/r/MachineLearning/comments/12bc8ym/d_what_to_do_in_this_brave_new_world/,108,1680594732.0," I am 35. Few years ago it occurred to me that my Software Engineering job might be not enough for the future. For last (5?) years, I have started to meddle in machine learning. Slowly at first, but it even motivated me to finish my masters degree and land job as reseacher in one small company. Its more ML engineering than research but why not , I though. Then last year Open AI launched GPT-4. I feel like I wasted my time. In Cental Europe, where I live, ml research is on really weak level. Pursuing PhD probably doesn't make sense. I could land some position, but I would still have to work full time to feed my famil. I would make it, but I doubt, that anyone would take me to serious research program.I can imagine, that jobs in IT will slowly evaporate. It's not realistic to starte company in this time and to be honest - i dont see myself as some kind of big founder. In short, I see my future in very pesimistic light right now. I was wondering how you deal with this new reality, maybe you can suggest something that I didn't think about before? Where you plan to go? What you plan to do?"
120,machinelearning,gpt-4,comments,2023-03-30 22:40:29,[P] Introducing Vicuna: An open-source language model based on LLaMA 13B,Business-Lead2679,False,0.95,286,1271po7,https://www.reddit.com/r/MachineLearning/comments/1271po7/p_introducing_vicuna_an_opensource_language_model/,107,1680216029.0,"We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%\* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%\* of cases. The cost of training Vicuna-13B is around $300. The training and serving [code](https://github.com/lm-sys/FastChat), along with an online [demo](https://chat.lmsys.org/), are publicly available for non-commercial use.

# Training details

Vicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model’s maximum context length.

Our training recipe builds on top of [Stanford’s alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) with the following improvements.

* **Memory Optimizations:** To enable Vicuna’s understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing [gradient checkpointing](https://arxiv.org/abs/1604.06174) and [flash attention](https://arxiv.org/abs/2205.14135).
* **Multi-round conversations:** We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot’s output.
* **Cost Reduction via Spot Instance:** The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ [SkyPilot](https://github.com/skypilot-org/skypilot) [managed spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html) to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.

&#x200B;

[Vicuna - Online demo](https://reddit.com/link/1271po7/video/0qsiu08kdyqa1/player)

# Limitations

We have noticed that, similar to other large language models, Vicuna has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. Additionally, it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI [moderation](https://platform.openai.com/docs/guides/moderation/overview) API to filter out inappropriate user inputs in our online demo. Nonetheless, we anticipate that Vicuna can serve as an open starting point for future research to tackle these limitations.

[Relative Response Quality Assessed by GPT-4](https://preview.redd.it/1rnmhv01eyqa1.png?width=599&format=png&auto=webp&s=02b4d415b5d378851bb70e225f1b1ebce98bfd83)

&#x200B;

For more information, check [https://vicuna.lmsys.org/](https://vicuna.lmsys.org/)

Online demo: [https://chat.lmsys.org/](https://chat.lmsys.org/)

&#x200B;

All credits go to the creators of this model. I did not participate in the creation of this model nor in the fine-tuning process. Usage of this model falls under a non-commercial license."
121,machinelearning,gpt-4,comments,2023-05-15 00:00:05,[D] On LLMs' ability to perform random sampling,bgighjigftuik,False,0.91,248,13hrkbj,https://i.redd.it/l1uyl3k3gxza1.jpg,104,1684108805.0,"To me this is the most counterintuitive thing I've yet seen from a language model. In this case it was ChatGPT (3.5, the free version); but I can see this happening in any model.

Here's the prompt I used on a fresh session:

""Generate 60 random numbers coming from an exponential distribution with lambda=6.4. Don't write any code""

It outputs the following values:

0.013, 0.078, 0.266, 0.040, 0.117, 0.283, 0.048, 0.078, 0.181, 0.134, 0.017, 0.160, 0.052, 0.196, 0.096, 0.098, 0.019, 0.048, 0.141, 0.071, 0.106, 0.023, 0.197, 0.062, 0.029, 0.072, 0.111, 0.166, 0.080, 0.027, 0.124, 0.016, 0.135, 0.052, 0.078, 0.161, 0.078, 0.139, 0.068, 0.066, 0.039, 0.056, 0.018, 0.082, 0.180, 0.086, 0.024, 0.022, 0.057, 0.042, 0.050, 0.104, 0.099, 0.062, 0.016, 0.186, 0.043, 0.085, 0.145, 0.110.

I get to plot a histogram with the numbers and they are close to what I would consider an exponential distribution with such parameter (you can see the plot).

Given that GPT 3.5 does not have access to a Python interpreter, how on earth is it able to do so? I have also tried other distributions and parameters and it kind of works. It's not perfect, but with normal distributions it is usually close to what scipy.stats would generate.

I could understand that it can have learnt to interpret Python code to some extent, but honestly I can't find explanation for random sampling from a probability distribution. For a Normal distribution, I can tell it about the desired mean and variance, and it samples values that are more than reasonable (and close to the true mean/variance specified).

Any thoughts? I honestly am unable to wrap my head around how a LLM can have the understanding on how to sample tokens (at digit level) to fit any probability distribution. To me it seems very unlikely to have similar data either the pre-training or fine-tuning stages."
122,machinelearning,gpt-4,comments,2023-04-01 12:57:30,[R] [P] I generated a 30K-utterance dataset by making GPT-4 prompt two ChatGPT instances to converse.,radi-cho,False,0.96,806,128lo83,https://i.redd.it/bywcz1kzs9ra1.png,104,1680353850.0,
123,machinelearning,gpt-4,comments,2022-12-22 18:39:30,[D] When chatGPT stops being free: Run SOTA LLM in cloud,_underlines_,False,0.95,350,zstequ,https://www.reddit.com/r/MachineLearning/comments/zstequ/d_when_chatgpt_stops_being_free_run_sota_llm_in/,95,1671734370.0,"Edit: Found [LAION-AI/OPEN-ASSISTANT](https://github.com/LAION-AI/Open-Assistant) a very promising project opensourcing the idea of chatGPT. [video here](https://www.youtube.com/watch?v=8gVYC_QX1DI)

**TL;DR: I found GPU compute to be [generally cheap](https://github.com/full-stack-deep-learning/website/blob/main/docs/cloud-gpus/cloud-gpus.csv) and spot or on-demand instances can be launched on AWS for a few USD / hour up to over 100GB vRAM. So I thought it would make sense to run your own SOTA LLM like Bloomz 176B inference endpoint whenever you need it for a few questions to answer. I thought it would still make more sense than shoving money into a closed walled garden like ""not-so-OpenAi"" when they make ChatGPT or GPT-4 available for $$$. But I struggle due to lack of tutorials/resources.**

Therefore, I carefully checked benchmarks, model parameters and sizes as well as training sources for all SOTA LLMs [here](https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit#gid=1158069878).

Knowing since reading the Chinchilla paper that Model Scaling according to OpenAI was wrong and more params != better quality generation. So I was looking for the best performing LLM openly available in terms of quality and broadness to use for multilingual everyday questions/code completion/reasoning similar to what chatGPT provides (minus the fine-tuning for chat-style conversations).

My choice fell on [Bloomz](https://huggingface.co/bigscience/bloomz) (because that handles multi-lingual questions well and has good zero shot performance for instructions and Q&A style text generation. Confusingly Galactica seems to outperform Bloom on several benchmarks. But since Galactica had a very narrow training set only using scientific papers, I guess usage is probably limited for answers on non-scientific topics.

Therefore I tried running the original bloom 176B and alternatively also Bloomz 176B on AWS SageMaker JumpStart, which should be a one click deployment. This fails after 20min. On Azure ML, I tried using DeepSpeed-MII which also supports bloom but also fails due the instance size of max 12GB vRAM I guess.

From my understanding to save costs on inference, it's probably possible to use one or multiple of the following solutions:

- Precision: int8 instead of fp16
- [Microsoft/DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) for an up 40x reduction on inference cost on Azure, this thing also supports int8 and fp16 bloom out of the box, but it fails on Azure due to instance size.
- [facebook/xformer](https://github.com/facebookresearch/xformers) not sure, but if I remember correctly this brought inference requirements down to 4GB vRAM for StableDiffusion and DreamBooth fine-tuning to 10GB. No idea if this is usefull for Bloom(z) inference cost reduction though

I have a CompSci background but I am not familiar with most stuff, except that I was running StableDiffusion since day one on my rtx3080 using linux and also doing fine-tuning with DreamBooth. But that was all just following youtube tutorials. I can't find a single post or youtube video of anyone explaining a full BLOOM / Galactica / BLOOMZ inference deployment on cloud platforms like AWS/Azure using one of the optimizations mentioned above, yet alone deployment of the raw model. :(

I still can't figure it out by myself after 3 days.

**TL;DR2: Trying to find likeminded people who are interested to run open source SOTA LLMs for when chatGPT will be paid or just for fun.**

Any comments, inputs, rants, counter-arguments are welcome.

/end of rant"
124,machinelearning,gpt-4,comments,2023-03-27 04:21:36,[D]GPT-4 might be able to tell you if it hallucinated,Cool_Abbreviations_9,False,0.93,645,123b66w,https://i.redd.it/ocs0x33429qa1.jpg,94,1679890896.0,
125,machinelearning,gpt-4,comments,2023-03-17 09:59:59,[D] PyTorch 2.0 Native Flash Attention 32k Context Window,super_deap,False,0.98,345,11tmpc5,https://www.reddit.com/r/MachineLearning/comments/11tmpc5/d_pytorch_20_native_flash_attention_32k_context/,94,1679047199.0,"Hi,

I did a quick experiment with Pytorch 2.0 Native scaled\_dot\_product\_attention. I was able to a single forward pass within 9GB of memory which is astounding. I think by patching existing Pretrained GPT models and adding more positional encodings, one could easily fine-tune those models to 32k attention on a single A100 80GB. Here is the code I used:

&#x200B;

https://preview.redd.it/6csxe28lv9oa1.png?width=607&format=png&auto=webp&s=ff8b48a77f49fab7d088fd8ba220f720860249bc

I think it should be possible to replicate even GPT-4 with open source tools something like Bloom + FlashAttention & fine-tune on 32k tokens.

**Update**: I was successfully able to start the training of GPT-2 (125M) with a context size of 8k and batch size of 1 on a 16GB GPU. Since memory scaled linearly from 4k to 8k. I am expecting, 32k would require \~64GB and should train smoothly on A100 80 GB. Also, I did not do any other optimizations. Maybe 8-bit fine-tuning can further optimize it.

**Update 2**: I basically picked Karpaty's nanoGPT and patched the pretrained GPT-2 by repeating the embeddings N-times. I was unable to train the model at 8k because generation would cause the crash.  So I started the training for a context window of 4k on The Pile: 1 hour in and loss seems to be going down pretty fast. Also Karpaty's generate function is super inefficient, O(n\^4) I think so it took forever to generate even 2k tokens. So I generate 1100 tokens just to see if the model is able to go beyond 1k limit. And it seems to be working. [Here are some samples](https://0bin.net/paste/O-+eopaW#nmtzX1Re7f1Nr-Otz606jkltvKk/kUXY96/8ca+tb4f) at 3k iteration.

&#x200B;

https://preview.redd.it/o2hb25w1sboa1.png?width=1226&format=png&auto=webp&s=bad2a1e21e218512b0f630c947ee41dba9b86a44

**Update 3**: I have started the training and I am publishing the training script if anyone is interested in replicating or building upon this work. Here is the complete training script:

[https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c](https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c)

I will post an update after the weekend once the training has progressed somewhat.

**Post-Weekend Update**: After \~50k iterations (the model has seen \~200 million tokens, I know this is just too small compared to 10s of billions trained by giga corps), loss only dropped from 4.6 to 4.2 on The Pile:

https://preview.redd.it/vi0fpskhsuoa1.png?width=1210&format=png&auto=webp&s=9fccc5277d91a6400adc6d968b0f2f0ff0da2afc

AFAIR, the loss of GPT-2 on the Pile if trained with 1024 tokens is \~2.8. It seems like the size of the dimension for each token is kind of limiting how much loss can go down since GPT-2 (small) has an embedding dimension of 768. Maybe someone can experiment with GPT-2 medium etc. to see how much we can improve. This is confirmation of the comment by u/lucidraisin [below](https://www.reddit.com/r/MachineLearning/comments/11tmpc5/comment/jcl2rkh/?utm_source=reddit&utm_medium=web2x&context=3)."
126,machinelearning,gpt-4,comments,2023-05-10 20:10:30,"[D] Since Google buried the MMLU benchmark scores in the Appendix of the PALM 2 technical report, here it is vs GPT-4 and other LLMs",jd_3d,False,0.97,343,13e1rf9,https://www.reddit.com/r/MachineLearning/comments/13e1rf9/d_since_google_buried_the_mmlu_benchmark_scores/,88,1683749430.0,"MMLU Benchmark results (all 5-shot)

* GPT-4 -  86.4%
* Flan-PaLM 2 (L) -   81.2%
* PALM 2 (L)  -  78.3%
* GPT-3.5 - 70.0%
* PaLM 540B  -  69.3%
* LLaMA 65B -  63.4%"
127,machinelearning,gpt-4,comments,2023-03-25 01:00:25,[R] Reflexion: an autonomous agent with dynamic memory and self-reflection - Noah Shinn et al 2023 Northeastern University Boston - Outperforms GPT-4 on HumanEval accuracy (0.67 --> 0.88)!,Singularian2501,False,0.91,246,1215dbl,https://www.reddit.com/r/MachineLearning/comments/1215dbl/r_reflexion_an_autonomous_agent_with_dynamic/,88,1679706025.0,"Paper: [https://arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366) 

Blog: [https://nanothoughts.substack.com/p/reflecting-on-reflexion](https://nanothoughts.substack.com/p/reflecting-on-reflexion) 

Github: [https://github.com/noahshinn024/reflexion-human-eval](https://github.com/noahshinn024/reflexion-human-eval) 

Twitter: [https://twitter.com/johnjnay/status/1639362071807549446?s=20](https://twitter.com/johnjnay/status/1639362071807549446?s=20) 

Abstract:

>Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, **specifically the ability to learn from mistakes**. **Self-reflection allows humans to efficiently solve novel problems through a process of trial and error.** Building on recent research, we propose Reflexion, an approach that endows an agent with **dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities.** To achieve full automation, we introduce a straightforward yet effective heuristic that **enables the agent to pinpoint hallucination instances, avoid repetition in action sequences, and, in some environments, construct an internal memory map of the given environment.** To assess our approach, we evaluate the agent's ability to complete decision-making tasks in AlfWorld environments and knowledge-intensive, search-based question-and-answer tasks in HotPotQA environments. We observe success rates of 97% and 51%, respectively, and provide a discussion on the emergent property of self-reflection. 

https://preview.redd.it/4myf8xso9spa1.png?width=1600&format=png&auto=webp&s=4384b662f88341bb9cc72b25fed5b88f3a87ffeb

https://preview.redd.it/bzupwyso9spa1.png?width=1600&format=png&auto=webp&s=b4626f34c60fe4528a04bcd241fd0c4286be20e7

https://preview.redd.it/009352to9spa1.jpg?width=1185&format=pjpg&auto=webp&s=0758aafe6033d5055c4e361e2785f1195bf5c08b

https://preview.redd.it/ef9ykzso9spa1.jpg?width=1074&format=pjpg&auto=webp&s=a394477210feeef69af88b34cb450d83920c3f97"
128,machinelearning,gpt-4,comments,2023-04-16 19:53:45,[R] Timeline of recent Large Language Models / Transformer Models,viktorgar,False,0.95,769,12omnxo,https://i.redd.it/gl11ce50xaua1.png,86,1681674825.0,
129,machinelearning,gpt-4,comments,2023-03-09 18:30:58,"[N] GPT-4 is coming next week – and it will be multimodal, says Microsoft Germany - heise online",Singularian2501,False,0.98,661,11mzqxu,https://www.reddit.com/r/MachineLearning/comments/11mzqxu/n_gpt4_is_coming_next_week_and_it_will_be/,80,1678386658.0,"[https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html](https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html)

>**GPT-4 is coming next week**: at an approximately one-hour hybrid information event entitled ""**AI in Focus - Digital Kickoff"" on 9 March 2023**, four Microsoft Germany employees presented Large Language Models (LLM) like GPT series as a disruptive force for companies and their Azure-OpenAI offering in detail. The kickoff event took place in the German language, news outlet Heise was present. **Rather casually, Andreas Braun, CTO Microsoft Germany** and Lead Data & AI STU, **mentioned** what he said was **the imminent release of GPT-4.** The fact that **Microsoft is fine-tuning multimodality with OpenAI should no longer have been a secret since the release of Kosmos-1 at the beginning of March.**

[ Dr. Andreas Braun, CTO Microsoft Germany and Lead Data  & AI STU at the Microsoft Digital Kickoff: \\""KI im Fokus\\"" \(AI in  Focus, Screenshot\) \(Bild: Microsoft\) ](https://preview.redd.it/rnst03avarma1.jpg?width=1920&format=pjpg&auto=webp&s=c5992e2d6c6daf32e56a0a3ffeeecfe10621f73f)"
130,machinelearning,gpt-4,comments,2024-02-04 17:06:06,"[P] Chess-GPT, 1000x smaller than GPT-4, plays 1500 Elo chess. We can visualize its internal board state, and it accurately estimates the Elo rating of the players in a game.",seraine,False,0.95,373,1aisp4m,https://www.reddit.com/r/MachineLearning/comments/1aisp4m/p_chessgpt_1000x_smaller_than_gpt4_plays_1500_elo/,80,1707066366.0," gpt-3.5-turbo-instruct's Elo rating of 1800 is chess seemed magical. But it's not! A 100-1000x smaller parameter LLM given a few million games of chess will learn to play at ELO 1500.

This model is only trained to predict the next character in PGN strings (1.e4 e5 2.Nf3 …) and is never explicitly given the state of the board or the rules of chess. Despite this, in order to better predict the next character, it learns to compute the state of the board at any point of the game, and learns a diverse set of rules, including check, checkmate, castling, en passant, promotion, pinned pieces, etc. In addition, to better predict the next character it also learns to estimate latent variables such as the Elo rating of the players in the game.

We can visualize the internal board state of the model as it's predicting the next character. For example, in this heatmap, we have the ground truth white pawn location on the left, a binary probe output in the middle, and a gradient of probe confidence on the right. We can see the model is extremely confident that no white pawns are on either back rank.

&#x200B;

https://preview.redd.it/dn8aryvdolgc1.jpg?width=2500&format=pjpg&auto=webp&s=003fe39d8a9bce2cc3271c4c9232c00e4d886aa6

In addition, to better predict the next character it also learns to estimate latent variables such as the ELO rating of the players in the game. More information is available in this post:

[https://adamkarvonen.github.io/machine\_learning/2024/01/03/chess-world-models.html](https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html)

And the code is here: [https://github.com/adamkarvonen/chess\_llm\_interpretability](https://github.com/adamkarvonen/chess_llm_interpretability)"
131,machinelearning,gpt-4,comments,2022-03-16 16:23:25,[P] Composer: a new PyTorch library to train models ~2-4x faster with better algorithms,moinnadeem,False,0.97,472,tflvuy,https://www.reddit.com/r/MachineLearning/comments/tflvuy/p_composer_a_new_pytorch_library_to_train_models/,77,1647447805.0,"Hey all!

We're excited to release Composer ([https://github.com/mosaicml/composer](https://github.com/mosaicml/composer)), an open-source library to speed up training of deep learning models by integrating better algorithms into the training process!

[Time and cost reductions across multiple model families](https://preview.redd.it/0y54ykj8qrn81.png?width=3009&format=png&auto=webp&s=d5f14b3381828d0b9d71ab04a4f1f12ebfb07fd7)

Composer lets you train:

* A ResNet-101 to 78.1% accuracy on ImageNet in 1 hour and 30 minutes ($49 on AWS), **3.5x faster and 71% cheaper than the baseline.**
* A ResNet-50 to 76.51% accuracy on ImageNet in 1 hour and 14 minutes ($40 on AWS), **2.9x faster and 65% cheaper than the baseline.**
* A GPT-2 to a perplexity of 24.11 on OpenWebText in 4 hours and 27 minutes ($145 on AWS), **1.7x faster and 43% cheaper than the baseline.**

https://preview.redd.it/0bitody9qrn81.png?width=10008&format=png&auto=webp&s=d9ecdb45f6419eb49e1c2c69eec418b36f35e172

Composer features a **functional interface** (similar to `torch.nn.functional`), which you can integrate into your own training loop, and a **trainer,** which handles seamless integration of efficient training algorithms into the training loop for you.

**Industry practitioners:** leverage our 20+ vetted and well-engineered implementations of speed-up algorithms to easily reduce time and costs to train models. Composer's built-in trainer makes it easy to **add multiple efficient training algorithms in a single line of code.** Trying out new methods or combinations of methods is as easy as changing a single list, and [we provide training recipes](https://github.com/mosaicml/composer#resnet-101) that yield the best training efficiency for popular benchmarks such as ResNets and GPTs.

**ML scientists:** use our two-way callback system in the Trainer **to easily prototype algorithms for wall-clock training efficiency.**[ Composer features tuned baselines to use in your research](https://github.com/mosaicml/composer/tree/dev/composer/yamls), and the software infrastructure to help study the impacts of an algorithm on training dynamics. Many of us wish we had this for our previous research projects!

**Feel free check out our GitHub repo:** [https://github.com/mosaicml/composer](https://github.com/mosaicml/composer), and star it ⭐️ to keep up with the latest updates!"
132,machinelearning,gpt-4,comments,2023-04-02 06:33:30,"[P] Auto-GPT : Recursively self-debugging, self-developing, self-improving, able to write it's own code using GPT-4 and execute Python scripts",Desi___Gigachad,False,0.92,425,129cle0,https://twitter.com/SigGravitas/status/1642181498278408193?s=20,75,1680417210.0,
133,machinelearning,gpt-4,comments,2023-03-23 22:56:31,"[D] ""Sparks of Artificial General Intelligence: Early experiments with GPT-4"" contained unredacted comments",QQII,False,0.93,177,1200lgr,https://www.reddit.com/r/MachineLearning/comments/1200lgr/d_sparks_of_artificial_general_intelligence_early/,68,1679612191.0,"Microsoft's research paper exploring the capabilities, limitations and implications of an early version of GPT-4 was [found to contain unredacted comments by an anonymous twitter user.](https://twitter.com/DV2559106965076/status/1638769434763608064) ([threadreader](https://threadreaderapp.com/thread/1638769434763608064.html), [nitter](https://nitter.lacontrevoie.fr/DV2559106965076/status/1638769434763608064), [archive.is](https://archive.is/1icMv), [archive.org](https://web.archive.org/web/20230323192314/https://twitter.com/DV2559106965076/status/1638769434763608064)) 

- Commented section titled ""Toxic Content"": https://i.imgur.com/s8iNXr7.jpg
- [`dv3` (the interval name for GPT-4)](https://pastebin.com/ZGMzgfqd)
- [`varun`](https://pastebin.com/i9KMFcy5) 
- [commented lines](https://pastebin.com/Aa1uqbh1)

[arxiv](https://arxiv.org/abs/2303.12712), [original /r/MachineLearning thread](https://www.reddit.com/r/MachineLearning/comments/11z3ymj/r_sparks_of_artificial_general_intelligence_early), [hacker news](https://twitter.com/DV2559106965076/status/1638769434763608064)"
134,machinelearning,gpt-4,comments,2019-08-13 16:48:08,[News] Megatron-LM: NVIDIA trains 8.3B GPT-2 using model and data parallelism on 512 GPUs. SOTA in language modelling and SQUAD. Details awaited.,Professor_Entropy,False,0.97,355,cpvssu,https://www.reddit.com/r/MachineLearning/comments/cpvssu/news_megatronlm_nvidia_trains_83b_gpt2_using/,66,1565714888.0,"Code: [https://github.com/NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM)

Unlike Open-AI, they have released the complete code for data processing, training, and evaluation.

Detailed writeup: [https://nv-adlr.github.io/MegatronLM](https://nv-adlr.github.io/MegatronLM)

From github:

>Megatron  is a large, powerful transformer. This repo is for ongoing  research on  training large, powerful transformer language models at  scale.  Currently, we support model-parallel, multinode training of [GPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and [BERT](https://arxiv.org/pdf/1810.04805.pdf) in mixed precision.Our  codebase is capable of efficiently training a 72-layer, 8.3  Billion  Parameter GPT2 Language model with 8-way model and 64-way data   parallelism across 512 GPUs. We find that bigger language models are   able to surpass current GPT2-1.5B wikitext perplexities in as little as 5   epochs of training.For BERT  training our repository trains BERT Large on 64 V100 GPUs in  3 days. We  achieved a final language modeling perplexity of 3.15 and  SQuAD  F1-score of 90.7.

Their submission is not in the leaderboard of SQuAD, but this exceeds the previous best single model performance (RoBERTa 89.8).

For  language modelling they get zero-shot wikitext perplexity of 17.4 (8.3B  model) better than 18.3 of transformer-xl (257M). However they claim it  as SOTA when GPT-2 itself has 17.48 ppl, and another model has 16.4 ([https://paperswithcode.com/sota/language-modelling-on-wikitext-103](https://paperswithcode.com/sota/language-modelling-on-wikitext-103))

Sadly they haven't mentioned anything about release of the model weights."
135,machinelearning,gpt-4,comments,2023-03-01 12:14:49,[R] ChatGPT failure increase linearly with addition on math problems,Neurosymbolic,False,0.94,238,11f29f9,https://www.reddit.com/r/MachineLearning/comments/11f29f9/r_chatgpt_failure_increase_linearly_with_addition/,66,1677672889.0," We did a study on ChatGPT's performance on math word problems. We found, under several conditions, its probability of failure increases linearly with the number of addition and subtraction operations - see below. This could imply that multi-step inference is a limitation. The performance also changes drastically when you restrict ChatGPT from showing its work (note the priors in the figure below, also see detailed breakdown of responses in the paper).

&#x200B;

[Math problems adds and subs vs. ChatGPT prob. of failure](https://preview.redd.it/z88ey3n6d4la1.png?width=1451&format=png&auto=webp&s=6da125b7a7cd60022ca70cd26434af6872a50d12)

ChatGPT Probability of Failure increase with addition and subtraction operations.

You the paper (preprint: [https://arxiv.org/abs/2302.13814](https://arxiv.org/abs/2302.13814)) will be presented at AAAI-MAKE next month. You can also check out our video here: [https://www.youtube.com/watch?v=vD-YSTLKRC8](https://www.youtube.com/watch?v=vD-YSTLKRC8)

&#x200B;

https://preview.redd.it/k58sbjd5d4la1.png?width=1264&format=png&auto=webp&s=5261923a2689201f905a26f06c6b5e9bac2fead6"
136,machinelearning,gpt-4,comments,2019-08-21 20:56:25,"[D] OpenAI's official 774M GPT-2 model released. 1.5B model might be released, dependent on 4 research organizations.",permalip,False,0.91,177,ctmxzj,https://www.reddit.com/r/MachineLearning/comments/ctmxzj/d_openais_official_774m_gpt2_model_released_15b/,65,1566420985.0,"Here are the links:

[https://openai.com/blog/gpt-2-6-month-follow-up/](https://openai.com/blog/gpt-2-6-month-follow-up/)

[https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)"
137,machinelearning,gpt-4,comments,2019-11-05 21:59:20,"[D] OpenAI releases GPT-2 1.5B model despite ""extremist groups can use GPT-2 for misuse"" but ""no strong evidence of misuse so far"".",permalip,False,0.89,89,ds6b2o,https://www.reddit.com/r/MachineLearning/comments/ds6b2o/d_openai_releases_gpt2_15b_model_despite/,63,1572991160.0,"The findings:

1. **Humans find GPT-2 outputs convincing**
2. **GPT-2 can be fine-tuned for misuse**
3. **Detection is challenging**
4. **We’ve seen no strong evidence of misuse so far**
5. **We need standards for studying bias**

They are going against their own word, but nevertheless, it's nice to see that they are releasing everything.

&#x200B;

Read the full blog post here: [https://openai.com/blog/gpt-2-1-5b-release/](https://openai.com/blog/gpt-2-1-5b-release/)

GitHub Model: [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)

GitHub Dataset: [https://github.com/openai/gpt-2-output-dataset](https://github.com/openai/gpt-2-output-dataset)

**EDIT:** The unicorn example added below from [https://talktotransformer.com/](https://talktotransformer.com/), which has already been updated with the newest 1.5B parameters model.

**Input:** In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.

**Output:** While there are only a few documented instances of unicorns in the wild, the researchers said the finding proves that there are still large numbers of wild unicorns that remain to be studied.

""It was a real surprise,"" co-researcher and anthropologist Richard H. Jantz, a professor of wildlife conservation biology at the University of Nevada, Reno, told the Washington Post. ""It's kind of a unique finding, because these are animals that only exist in nature.""

The scientists found that the unicorns, which weigh between 20 and 90 pounds, lived in the Chagres National Park in Argentina. They live in groups that can contain as many as 12 animals and are capable of producing milk.

The scientists first came across the herd in 2013, when the herd was grazing in a valley filled with pines in the northern part of the national park. The scientists were looking for a location to start a new"
138,machinelearning,gpt-4,comments,2023-04-17 17:54:43,[Discussion] Translation of Japanese to English using GPT. These are my discoveries after ~100 hours of extensive experimentation and ways I think it can be improved.,NepNep_,False,0.9,303,12pqqg6,https://www.reddit.com/r/MachineLearning/comments/12pqqg6/discussion_translation_of_japanese_to_english/,62,1681754083.0,"Hello. I am currently experimenting with the viability of LLM models for Japanese to English translation. I've been experimenting with GPT 3.5, GPT 3.5 utilizing the DAN protocols, and GPT 4 for this project for around 3 months now with very promising results and I think I've identified several limitations with GPT that if addressed can significantly improve the efficiency and quality of translations.

&#x200B;

The project I'm working on is attempting to translate a light novel series from japanese to english. During these tests I did a deep dive, asking GPT how it is attempting the translations and asking it to modify its translation methodology through various means (I am considering doing a long video outlining all this and showing off the prompts and responses at a later date). Notably this includes asking it to utilize its understanding of the series its translating from its training knowledge to aide in the translation, and providing it with a ""seed"" translation. Basically the seed is a side by side japanese and english translation to show GPT what I'm looking for in terms of grammar and formatting. The english translation notably is a human translation, not a machine translation. The results from these tests provided SIGNIFICANT improvements to the final translation, so significant in fact that a large portion of the text could reasonably be assumed to be human-translated.

&#x200B;

Link to the project I'm working on so you can see my documentation and results: [https://docs.google.com/document/d/1MxKiE-q36RdT\_Du5K1PLdyD7Vru9lcf6S60uymBb10g/edit?usp=sharing](https://docs.google.com/document/d/1MxKiE-q36RdT_Du5K1PLdyD7Vru9lcf6S60uymBb10g/edit?usp=sharing)

&#x200B;

I've probably done around 50-100 hours of extensive testing with translation and methodology over the past 2-3 months. Over that time I've discovered the following:

1. Both GPT3 and GPT4 are significant improvements over traditional translation services such as google or deepl. This may be because Japanese and English are very different languages in how they are written and how their grammar works so prior translation services simply did a direct translation while GPT is capable of understanding the text and rewriting it to account for that. For example in japanese, there are no pronouns like ""he"" and ""her"" so a person's gender might not be clear from the sentence alone. Google Translate and DeepL typically just take a 50/50 guess, while GPT from my experience has been much more capable in getting this right based on understanding the larger context from the paragraph.
2. GPT has a tendency to censor text deliberately if it feels the translation may offend people. This isn't just for things that are blatantly offensive like slurs, it also includes mild sexual content, the kind that is typically approved for teen viewing/reading. The biggest problem is that it doesn't tell you it is censoring anything unless you ask it, meaning everything else may be a solid translation yet it may censor information which can ultimately hurt the translation, especially for story related translations like in my tests. These restrictions can be bypassed with correct prompting. I've had luck using GPT 3's DAN protocols however DAN's translations arent as strong as GPT 4, and I've had luck with GPT 4 by framing the translation as a game with extreme win and loss conditions and telling it that if it censors the translation, it may offend the author of the content since people in japan hold different values from our own.
3. GPT puts too high a focus on accuracy even if instructed not to. This is a good thing to a degree since outside of censorship you know the translation is accurate, however even when explicitly told to put maximum emphasis on readability, even if it hurts the accuracy, and it is allowed to rewrite sentences from the ground up to aide readability, it still puts too strong an emphasis on accuracy. I have determined this through testing that for some reason it is ignoring the request to focus on readability and will still maximize accuracy. The best way I've found to fix this issue is through demonstration, specifically the ""seed"" I mentioned earlier. By giving it a japanese and english translation of the same work but earlier in the story, it then understands how to put more emphasis on readability. The results is something that is 95% within the range of accuracy a professional translator would use while much easier to read.
4. GPT's biggest limitation is the fact that it ""forgets"" the seed way too quickly, usually within a few prompts. I've done testing with its data retention and it appears that if you give it too much information to remember at once it slowly bugs out. With GPT 3 its a hard crash type bug where it just spews nonsense unrelated to your request, however GPT 4 can remember a lot more information and will hard crash if you give it too much info but otherwise builds up errors slowly as you give it more info. I initially believed that there were issues with the token count, but further testing shows that the GPT model simply isn't optimized for this method of translation and a new or reworked model that you can give a seed and it will remember it longer would be better. The seed is one of the best tools for improving its performance

Next steps:

I would like to try to either create my own model or modify an existing one to optimize it for translation. If any1 knows any tools or guides I'd appreciate it."
139,machinelearning,gpt-4,comments,2023-04-20 01:30:47,[D] GPT-3T: Can we train language models to think further ahead?,landongarrison,False,0.91,114,12shf18,https://www.reddit.com/r/MachineLearning/comments/12shf18/d_gpt3t_can_we_train_language_models_to_think/,62,1681954247.0,"In a recent talk done by Sebastian Bubeck called “Sparks of AGI: Early experiments done with GPT-4”, Sebastian mentioned on thing in his presentation that caught my attention (paraphrased quote):

> “GPT-4 cannot plan, but this might be a limitation because it can only look one token into the future”

While very simple on the surface, this may actually be very true: what if we are training our language models to be very shallow thinkers and not actually look far enough ahead? Could single token prediction actually be a fundamental flaw?

In this repo, I try a very early experiment called GPT-3T, a model that predicts 3 tokens ahead at one time step. While incredibly simple on the surface, this could potentially be one way to overcome the planning issue that you find in GPTs. Forcing an autoregressive model to predict further ahead at scale *may* bring out much more interesting emergent behaviours than what we’ve seen in single token GPTs.

__

**Experiments**

My personal experiments are overall inconclusive on either side: I have only pre-trained a very small model (300 million params on WebText-10K) and it achieves a decent ability to generate text. However as you can see, this model heavily under optimized but I do not have the resources to carry this out further.

If anyone would like to try this experiment with more scale, I would love to get an answer to this question to improve upon this model. This repo is intended to allow anyone who would like to pre-train a GPT-3T model easily to run this experiment. From what I have seen, this has not been tried before and I am very curious to see results.

__

**Edit:** GitHub repo is buried in the comments (sorry this post will be taken down if I include it in the main post)"
140,machinelearning,gpt-4,comments,2023-07-22 10:40:59,[D] We seem to have already achieved narrow ASI,Georgeo57,False,0.07,0,156gqmv,https://www.reddit.com/r/MachineLearning/comments/156gqmv/d_we_seem_to_have_already_achieved_narrow_asi/,61,1690022459.0,"

Consider the following. If a human were to learn and memorize everything that GPT-4 has, we would consider that human to be super-intelligent, at least within the narrow domain of learning and memorization. Because GPT-4 has already achieved this super-intelligent ability, strong logic compels we conclude that it has achieved narrow ASI within the narrow domains of learning and memorization.

I realize that the AI community has not yet appreciated and accepted this understanding, but science has never been about consensus view. For example before the Big Bang theory there was the Steady State theory of the universe. When the former was introduced it was by far the minority view. But of course that did not make it incorrect.

So what do you think? If you believe that we have not yet achieved narrow ASI in the narrow domains of learning and memorization, what are your arguments in support of this view?

Addendum 7:07pm ET, 7/22/23:

I decided to add the following content to better explain the logic of the concept, to better describe its utility, to highlight the historical risk that the field assumes by not accepting it, and to show that ASI does not in principle preclude narrow ASI.

A good analogy to the paradigm shift admission that we have already achieved narrow ASI, and to the appreciation that the distinction is both useful and necessary, is the historical shift in the scientific consensus regarding animal intelligence. It wasn't long ago when the idea of non-human animal intelligence was laughable. Today any claim that non-human animals are not intelligent is laughable. 

Because the narrow ASI distinction is pragmatically useful in helping us channel greater resources to narrow ASI applications that are much easier to achieve within a much shorter time frame than AGI, it shouldn't take long for the AI community to appreciate and accept this unassailable paradigm shift distinction. For the field to refuse to do this would amount to their abandoning all logic and reasoning in favor of a worn out and flawed convention. In the end it would be very foolish of them to not accept the new distinction, and history would not fail to make note of such foolishness.

The argument that we have already achieved narrow ASI is too strong and well supported to not be understood, accepted and appreciated.

Me:

Does ASI preclude the possibility of narrow ASI?

Bard:

No, ASI does not preclude the possibility of narrow ASI. ASI is a broad term that refers to any artificial general intelligence, while narrow ASI refers to an AI that is specialized in a particular task. It is possible to have both ASI and narrow ASI at the same time. For example, an ASI could be designed to be able to perform any task that a human can, but it could also be specialized in a particular task, such as playing chess or Go.

In fact, it is likely that narrow ASI will be developed before ASI. This is because it is easier to develop an AI that is specialized in a particular task than it is to develop an AI that is capable of performing any task. Once narrow ASI is developed, it could be used as a stepping stone to developing ASI.

So, to answer your question, ASI does not preclude the possibility of narrow ASI. In fact, it is likely that both ASI and narrow ASI will exist in the future."
141,machinelearning,gpt-4,comments,2023-08-28 07:02:36,"[D] Google Gemini Eats The World – Gemini Smashes GPT-4 By 5X, The GPU-Poors",hardmaru,False,0.72,124,163ewre,https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini,61,1693206156.0,
142,machinelearning,gpt-4,comments,2024-01-21 09:22:12,[Discussion] Re-using state from LLM's / next-token predictors as an optimization,ShoeStatus2431,False,0.9,30,19bzlxd,https://www.reddit.com/r/MachineLearning/comments/19bzlxd/discussion_reusing_state_from_llms_nexttoken/,60,1705828932.0,"I've been pondering how GPT-3/4 must work internally and possible optimizations. I'm wondering if someone could point me to research already done in this area -- or if I completely misunderstand how these models work.

So basically I'm wondering about the 'next-token' predictor aspect. Despite their function of predicting the next token, it seems evident to me that these models must have an internal process (developed in a 'black box' fashion during training) that anticipates the rest of the response. This anticipation appears necessary to prevent the model from emitting a next token that causes a dead-end, making it impossible to construct a coherent sentence.

Moreover, this foresight seems to extend beyond single sentences. GPT-4 responses often exhibit a highly structured format, including an introduction, in-depth analysis, and conclusion, indicating a higher level of planning or disposition in the answer. This leads me to believe that even though the model generates only one next token at a time, it might internally form a more complete response to ensure a well-chosen next token. Likely not in a way that the full response is hashed out in the normal token representation, but at least some internal representation close to this. Especially for shorter ranges (sentences) I imagine it could be quite precise, but over longer ranges (paragraphs etc.) perhaps it is more and more abstract.

This understanding raises a question: Is there a way to extract more of the full response from the network directly? Currently, it seems that the entire calculation is repeated with each token, taking into account the previously emitted token. I suspect that a significant portion of these calculations could be similar, or at least there might be a more efficient pathway to generate the complete answer from the internal state after the first token is emitted.

In practice, this might involve altering the model to produce longer or complete responses in each iteration, rather than just a single token. Alternatively, a secondary, smaller model could be developed to 'peek' into the internal state of the primary model after one token generation and generate complete answers from that. Or perhaps, a model trained in a way that allows for reusing internal states, thereby accelerating the generation of subsequent tokens. Perhaps something like simply restarting from the same state or shifting it somehow.

I'm curious about the feasibility of these ideas and whether they align with the current understanding of LLMs. I look forward to hearing your thoughts, especially if there are fundamental misunderstandings in my assumptions about how LLMs work."
143,machinelearning,gpt-4,comments,2023-09-23 15:56:39,[D] GPT-3.5-instruct beats GPT-4 at chess and is a ~1800 ELO chess player. Results of 150 games of GPT-3.5 vs stockfish and 30 of GPT-3.5 vs GPT-4.,seraine,False,0.92,104,16q81fh,https://www.reddit.com/r/MachineLearning/comments/16q81fh/d_gpt35instruct_beats_gpt4_at_chess_and_is_a_1800/,59,1695484599.0,"99.7% of its 8000 moves were legal with the longest game going 147 moves. You can test it here: [https://github.com/adamkarvonen/chess\_gpt\_eval](https://github.com/adamkarvonen/chess_gpt_eval)  


&#x200B;

https://preview.redd.it/821ydy7521qb1.png?width=1000&format=png&auto=webp&s=da6c96feaa527d0b7dfbf407bdc0210f3fcf947b

More details here: [https://twitter.com/a\_karvonen/status/1705340535836221659](https://twitter.com/a_karvonen/status/1705340535836221659)"
144,machinelearning,gpt-4,comments,2021-09-23 12:24:06,[D] Fine-tuning GPT-J: lessons learned,juliensalinas,False,0.98,181,ptu24e,https://www.reddit.com/r/MachineLearning/comments/ptu24e/d_finetuning_gptj_lessons_learned/,58,1632399846.0,"Hello all,

We've spent quite some time benchmarking the best fine-tuning techniques for GPT-J at [NLP Cloud](https://nlpcloud.io/?utm_source=reddit&utm_campaign=k431103c-ed8e-11eb-ba80-2242ac130007).   Finding the best solution was not straightforward and we had to look  at  things like speed, server costs, ease of development, accuracy of  the  fine-tuned model... It took time but we ended up with a nice setup  (and  we are now officially proposing GPT-J fine-tuning + automatic  deployment  on our platform).

Here are our key takeaways:

* The best methodology seems to be the one from the Mesh Transformer Jax team: [https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto\_finetune.md](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md)
* Fine-tuning   on GPU is not ideal. Even several GPUs used in parallel with Deepspeed   can be very slow. We used 4 GPUs Tesla T4 in parallel, and it took  1h30  to only compute our first checkpoint (+ 80GB of RAM used...), for a   training dataset made up of 20k examples. Maybe a GPU A100 would be   worth a try.
* Fine-tuning   on TPU is very efficient but it takes a TPU v3 because TPUs v2 are   running out of memory. It takes around 15 minutes, for a training dataset   made up of 20k examples, which is really awesome.
* The   overall process is not straightforward as it takes several kind of   conversions (converting the datasets to the right format, making a slim   version of the model, converting the weights to Transformers...)

In   the end this is worth the effort, because combining fine-tuning and   few-shot learning makes GPT-J very impressive and suited for all sorts   of use cases.

If you guys have   different feedbacks about GPT-J fine-tuning, please don't hesitate to   comment, I would love to have your opinion.

Hope you found the above useful!"
145,machinelearning,gpt-4,comments,2023-05-26 13:57:42,[N] Abu Dhabi's TTI releases open-source Falcon-7B and -40B LLMs,Balance-,False,0.95,271,13sdz8p,https://www.reddit.com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/,58,1685109462.0,"Abu Dhabi's Technology Innovation Institute (TII) just released new 7B and 40B LLMs.

The Falcon-40B model is now at the top of the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), beating *llama-30b-supercot* and *llama-65b* among others.

| Model                      | Revision | Average | ARC (25-shot) | HellaSwag (10-shot) | MMLU (5-shot) | TruthfulQA (0-shot) |
|----------------------------|----------|-----------|-----------------|-----------------------|-----------------|-----------------------|
| tiiuae/falcon-40b          | main     | 60.4      | 61.9            | 85.3                  | 52.7            | 41.7                  |
| ausboss/llama-30b-supercot | main     | 59.8      | 58.5            | 82.9                  | 44.3            | 53.6                  |
| llama-65b                  | main     | 58.3      | 57.8            | 84.2                  | 48.8            | 42.3                  |
| MetaIX/GPT4-X-Alpasta-30b  | main     | 57.9      | 56.7            | 81.4                  | 43.6            | 49.7                  |

**Press release:** [UAE's Technology Innovation Institute Launches Open-Source ""Falcon 40B"" Large Language Model for Research & Commercial Utilization](https://www.tii.ae/news/uaes-technology-innovation-institute-launches-open-source-falcon-40b-large-language-model)

>The Technology Innovation Institute (TII) in Abu Dhabi has announced its open-source large language model (LLM), the Falcon 40B. With 40 billion parameters, Falcon 40B is the UAE's first large-scale AI model, indicating the country's ambition in the field of AI and its commitment to promote innovation and research.  
>  
>Unlike most LLMs, which typically only provide non-commercial users access, Falcon 40B is open to both research and commercial usage. The TII has also included the model's weights in the open-source package, which will enhance the model's capabilities and allow for more effective fine-tuning.  
>  
>In addition to the launch of Falcon 40B, the TII has initiated a call for proposals from researchers and visionaries interested in leveraging the model to create innovative use cases or explore further applications. As a reward for exceptional research proposals, selected projects will receive ""training compute power"" as an investment, allowing for more robust data analysis and complex modeling. VentureOne, the commercialization arm of ATRC, will provide computational resources for the most promising projects.  
>  
>TII's Falcon 40B has shown impressive performance since its unveiling in March 2023. When benchmarked using Stanford University’s HELM LLM tool, it used less training compute power compared to other renowned LLMs such as OpenAI's GPT-3, DeepMind's Chinchilla AI, and Google's PaLM-62B.  
>  
>Those interested in accessing Falcon 40B or proposing use cases can do so through the [FalconLLM.TII.ae](https://FalconLLM.TII.ae) website. Falcon LLMs open-sourced to date are available under a license built upon the principles of the open-source Apache 2.0 software, permitting a broad range of free use.

**Hugging Face links**

* [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) / [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)
* [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) / [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)"
146,machinelearning,gpt-4,comments,2024-01-05 21:39:40,Transformer-Based LLMs Are Not General Learners: A Universal Circuit Perspective [R],we_are_mammals,False,0.94,265,18zie7z,https://www.reddit.com/r/MachineLearning/comments/18zie7z/transformerbased_llms_are_not_general_learners_a/,57,1704490780.0,"https://openreview.net/forum?id=tGM7rOmJzV

> (LLMs') remarkable success triggers a notable shift in the research priorities of the artificial intelligence community. These impressive empirical achievements fuel an expectation that LLMs are “sparks of Artificial General Intelligence (AGI)"". However, some evaluation results have also presented confusing instances of LLM failures, including some in seemingly trivial tasks. For example, GPT-4 is able to solve some mathematical problems in IMO that could be challenging for graduate students, while it could make errors on arithmetic problems at an elementary school level in some cases.

> ...

> Our theoretical results indicate that T-LLMs fail to be general learners. However, the T-LLMs achieve great empirical success in various tasks. We provide a possible explanation for this inconsistency: while T-LLMs are not general learners, they can partially solve complex tasks by memorizing a number of instances, leading to an illusion that the T-LLMs have genuine problem-solving ability for these tasks."
147,machinelearning,gpt-4,comments,2023-05-10 08:11:24,[D] When will a GPT-like model outperform Stockfish in chess?,ThePerson654321,False,0.49,0,13dk32o,https://www.reddit.com/r/MachineLearning/comments/13dk32o/d_when_will_a_gptlike_model_outperform_stockfish/,55,1683706284.0,"Hello!

GPT-4 has shown significant improvement over GPT-3 in terms of playing chess, with what I estimate to be a ELO rating of around 1000 now. While this is impressive, it is still nowhere near the performance level of Stockfish. This has led me to ponder the following question:

How many years do you think it will take for a GPT-like model (not specifically or even intentionally trained for chess) to surpass Stockfish in terms of chess-playing abilities?"
148,machinelearning,gpt-4,comments,2023-05-22 14:31:40,[R] GPT-4 and ChatGPT sometimes hallucinate to the point where they know they're hallucinating,ofirpress,False,0.74,53,13oskli,https://www.reddit.com/r/MachineLearning/comments/13oskli/r_gpt4_and_chatgpt_sometimes_hallucinate_to_the/,53,1684765900.0,"We just put a paper up where we found a wide array of questions that lead GPT-4 & ChatGPT to hallucinate so badly, to where in a separate chat session they can point out that what they previously said was incorrect.

&#x200B;

We call these hallucinations snowballed hallucinations.

&#x200B;

[Turn sound ON to watch our demo](https://reddit.com/link/13oskli/video/fqm405jz7e1b1/player)

The paper is here: [https://ofir.io/snowballed\_hallucination.pdf](https://ofir.io/snowballed_hallucination.pdf)

There's a summary on Twitter here:  [https://twitter.com/OfirPress/status/1660646315049533446](https://twitter.com/OfirPress/status/1660646315049533446)

&#x200B;

I'll be here to answer your questions :)"
149,machinelearning,gpt-4,comments,2020-12-07 13:54:02,"[R] Wide Neural Networks are Feature Learners, Not Kernel Machines",thegregyang,False,0.95,313,k8h01q,https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/,52,1607349242.0,"Hi Reddit,

I’m excited to share with you my new paper [\[2011.14522\] Feature Learning in Infinite-Width Neural Networks (arxiv.org)](https://arxiv.org/abs/2011.14522).

# The Problem

Many previous works proposed that wide neural networks (NN) are kernel machines [\[1\]](http://arxiv.org/abs/1806.07572)[\[2\]](http://arxiv.org/abs/1811.03962)[\[3\]](http://arxiv.org/abs/1811.03804), the most well-known theory perhaps being the *Neural Tangent Kernel (NTK)* [\[1\]](http://arxiv.org/abs/1806.07572). This is problematic because kernel machines **do not learn features**, so such theories cannot make sense of **pretraining and transfer learning** (e.g. Imagenet and BERT), which are arguably at the center of deep learning's far-reaching impact so far.

# The Solution

Here we show if we parametrize the NN “correctly” (see paper for how), then its infinite-width limit **admits feature learning**. We can derive exact formulas for such feature-learning “infinite-width” neural networks. Indeed, we explicitly compute them for learning word embeddings via [word2vec](https://en.wikipedia.org/wiki/Word2vec) (the first large-scale NLP pretraining in the deep learning age and a precursor to BERT) and compare against finite neural networks as well as [NTK](http://arxiv.org/abs/1806.07572) (the kernel machine mentioned above). Visualizing the learned embeddings immediately gives a clear idea of their differences:

[Visualizing Learned Word2Vec Embeddings of Each Model](https://preview.redd.it/d8hspempsr361.png?width=1336&format=png&auto=webp&s=5a792c36905afba606a4107932a8002b0cac1e30)

Furthermore, we find on the word analogy downstream task: 1) The feature-learning limit outperforms the NTK and the finite-width neural networks, 2) and the latter approach the feature-learning limit in performance as width increases.

In the figure below, you can observe that NTK gets \~0 accuracy. This is because its word embeddings are essentially from random initialization, so it is no better than random guessing among the 70k vocabulary (and 1/70k is effectively 0 on this graph).

[Downstream Word Analogy Task](https://preview.redd.it/uj2blwqqsr361.png?width=2272&format=png&auto=webp&s=ea2bbbb5c496e6e44188425281e0847302d7b9fe)

We obtain similar findings in another experiment comparing these models on Omniglot few-shot learning via MAML (see paper). These results suggest that **our new limit is really the “right” limit** for talking about feature learning, pretraining, and transfer learning.

# Looking Ahead

I’m super excited about all this because it blows open so many questions:

1. What kinds of representations are learned in such infinite-width neural networks?
2. How does it inform us about finite neural networks?
3. How does this feature learning affect training and generalization?
4. How does this jibe with the [scaling law of language models](http://arxiv.org/abs/2001.08361)?
5. Can we train an infinite-width GPT…so GPT∞?
6. ... and so many more questions!

For each of these questions, our results provide a framework for answering it, so it feels like they are all within reach.

# Tensor Programs Series

This (mathematical) framework is called *Tensor Programs* and I’ve been writing a series of papers on them, slowly building up its foundations. Here I have described the 4th paper in this series (though I've stopped numbering it in the title), which is a big payoff of the foundations developed by its predecessors, which are

1. [\[1910.12478\] Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes (arxiv.org)](https://arxiv.org/abs/1910.12478)  ([reddit discussion](https://www.reddit.com/r/MachineLearning/comments/i17889/r_tensor_programs_i_wide_feedforward_or_recurrent/))
2. [\[2006.14548\] Tensor Programs II: Neural Tangent Kernel for Any Architecture (arxiv.org)](https://arxiv.org/abs/2006.14548)
3. [\[2009.10685\] Tensor Programs III: Neural Matrix Laws (arxiv.org)](https://arxiv.org/abs/2009.10685)

Each paper from 1-3 builds up the machinery incrementally, with a punchline for the partial progress made in that paper. But actually I started this whole series because I wanted to write [the paper described in this post](https://arxiv.org/abs/2011.14522)! It required a lot of planning ahead, writing pain, and fear-of-getting-scooped-so-you-wrote-more-than-200-pages-for-nothing, but I'm really happy and relieved I finally made it!

# Talk Coming Up

I am going to talk about this work this Wednesday 12 EDT at the online seminar [Physics ∩ ML](http://physicsmeetsml.org/posts/sem_2020_12_09/). Please join me if this sounds interesting to you! You can sign up [here](https://forms.gle/mLtPEXbpjjvWvpxq8) to get the zoom link.

# Shout Out to My Co-Author Edward

[Edward](https://edwardjhu.com/) is a Microsoft AI Resident and a hell of a researcher for his age. I'm really lucky to have him work with me during the past year (and ongoing). He's looking for grad school opportunities next, so please [reach out to him](mailto:Edward.Hu@microsoft.com) if you are a professor interested in working with him! Or, if you are a student looking to jumpstart your AI career, apply to our [AI Residency Program](https://www.microsoft.com/en-us/research/academic-program/microsoft-ai-residency-program/)!

# Edit: FAQs from the Comments

&#x200B;

>Pretraining and transfer learning don’t make sense in the kernel limits of neural networks. Why?

In a gist, in these kernel limits, the last layer representations of inputs (right before the linear readout layer) are essentially fixed throughout the training.

During transfer learning, we discard the pretrained readout layer and train a new one (because the task will typically have different labels than pretraining). Often, we train only this new (linear) readout layer to save computation (e.g. as in self-supervised learning in vision, like AMDIM, SimCLR, BYOL). The outcome of this linear training only depends on the last layer representations of the inputs. In the kernel limits, they are fixed at initialization, so in terms of transfer, it’s like you never pretrained at all.

For example, this is very clear in the Gaussian Process limit of NN, which corresponds to training only the readout layer of the network. Then the input representations are *exactly* fixed throughout training. In the Neural Tangent limit of NN, the representations are not exactly fixed but any change tends to 0 as width → ∞

Contrast this with known behavior of ResNet, for example, where each neuron in last layer representation is a face detector, eye detector, boat detector, etc. This can’t be true if the representation comes solely from random initialization. Similar things can be said of pretrained language models.

So I've just talked about linear transfer learning above. But the same conclusion holds even if you finetune the entire network via a more sophisticated argument (see Thm G.16 in the paper).

&#x200B;

>Why are NN not kernel machines?

The title really should be something like “To Explain Pretraining and Transfer Learning, Wide Neural Networks Should Be Thought of as Feature Learners, Not Kernel Machines” but that’s really long

So I’m actually not saying NN *cannot* be kernel machines – they can, as in the GP and NTK limits – but we can understand them better as feature learners.

More precisely, the same neural network can have different infinite-width limits, depending on the parametrization of the network. A big contribution of this paper is classifying what kind of limits are possible.

&#x200B;

>Comparison with [Pedro’s paper: Every Model Learned by Gradient Descent Is Approximately a Kernel Machine](https://arxiv.org/abs/2012.00152)?

Any finite function can be *expressed* as a kernel machine for any given positive definite kernel.

My understanding is that Pedro’s paper presents a specific instantiation of this using what he defines as the *path kernel*.

However, it’s unclear to me in what way is that useful, because the kernel (and the coefficients involved) he defines depends on the optimization trajectory of the NN and the data of the problem. So his “kernel machine” actually allows feature learning in the sense that his path kernel can change over the course of training. This really doesn't jibe with his comment that "" Perhaps the most significant implication of our result for deep learning is that it casts doubt on the common view that it works by automatically discovering new representations of the data, in contrast with other machine learning methods, which rely on predefined features (Bengio et al., 2013).""

In addition, if you look at the proof of his theorem (screenshotted below), the appearance of the path kernel in his expression is a bit arbitrary, since I can also multiply and divide by some other kernel

*Processing img 1zmnd9ziyt361...*

&#x200B;

>What’s the relation with universal approximation theorem?

Glockenspielcello actually has [a pretty good answer](https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/geyodne?utm_source=share&utm_medium=web2x&context=3), so I’ll just cite them here

""The point of this new paper isn't about the expressivity of the output class though, it's about the kind of learning that is performed. If you look at the paper, they differentiate between different kinds of limits that you can get based on the parametrization, and show that you can get either kernel-like behavior or feature learning behavior. Single layer networks using the parametrization described by Neal fall into the former category.""

&#x200B;"
150,machinelearning,gpt-4,comments,2022-06-23 12:15:39,[P] Yandex open sources 100b large language model weights (YaLM),htrp,False,0.97,286,vivji3,https://www.reddit.com/r/MachineLearning/comments/vivji3/p_yandex_open_sources_100b_large_language_model/,52,1655986539.0,"PR Announcement: https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6


Github: https://github.com/yandex/YaLM-100B

Network is trained using same principles as Megatron LM, inference alone will require 4 A100s"
151,machinelearning,gpt-4,comments,2019-07-20 15:36:49,[D] How the Transformers broke NLP leaderboards,milaworld,False,0.96,252,cfn4bu,https://www.reddit.com/r/MachineLearning/comments/cfn4bu/d_how_the_transformers_broke_nlp_leaderboards/,50,1563637009.0,"*I came across this interesting [article](https://hackingsemantics.xyz/2019/leaderboards/) about whether larger models + more data = progress in ML research.*

**[How the Transformers broke NLP leaderboards](https://hackingsemantics.xyz/2019/leaderboards/)**

*Excerpt:*

The focus of this post is yet another problem with the leaderboards that is relatively recent. Its cause is simple: fundamentally, **a model may be better than its competitors by building better representations from the available data - or it may simply use more data, and/or throw a deeper network at it**. When we have a paper presenting a new model that also uses more data/compute than its competitors, credit attribution becomes hard.

The most popular NLP leaderboards are currently dominated by Transformer-based models. BERT received the best paper award at NAACL 2019 after months of holding SOTA on many leaderboards. Now the hot topic is XLNet that is said to overtake BERT on GLUE and some other benchmarks. Other Transformers include GPT-2, ERNIE, and the list is growing.

The problem we’re starting to face is that these models are HUGE. While the source code is available, in reality it is beyond the means of an average lab to reproduce these results, or to produce anything comparable. For instance, XLNet is trained on 32B tokens, and the price of using 500 TPUs for 2 days is over $250,000. Even fine-tuning this model is getting expensive.

Wait, this was supposed to happen!

On the one hand, this trend looks predictable, even inevitable: people with more resources *will* use more resources to get better performance. One could even argue that a huge model proves its scalability and fulfils the inherent promise of deep learning, i.e. being able to learn more complex patterns from more information. Nobody knows how much data we actually need to solve a given NLP task, but more should be better, and limiting data seems counter-productive.

On that view - well, from now on top-tier NLP research is going to be something possible only for industry. Academics will have to somehow up their game, either by getting more grants or by collaborating with high-performance computing centers. They are also welcome to switch to analysis, building something on top of the industry-provided huge models, or making datasets.

However, in terms of overall progress in NLP that might not be the best thing to do. The chief problem with the huge models is simply this:

“More data & compute = SOTA” is **NOT** research news.

If leaderboards are to highlight the actual progress, we need to incentivize new architectures rather than teams outspending each other. Obviously, huge pretrained models are valuable, but unless the authors show that their system consistently behaves differently from its competition with comparable data & compute, it is not clear whether they are presenting a model or a resource.

Furthermore, much of this research is not reproducible: nobody is going to spend $250,000 just to repeat XLNet training. Given the fact that its ablation study showed only 1-2% gain over BERT in 3 datasets out of 4, we don’t actually know for sure that its masking strategy is more successful than BERT’s.

At the same time, the development of leaner models is dis-incentivized, as their task is fundamentally harder and the leaderboard-oriented community only rewards the SOTA. That, in its turn, prices out of competitions academic teams, which will not result in students becoming better engineers when they graduate.

*Entire article:*

https://hackingsemantics.xyz/2019/leaderboards/"
152,machinelearning,gpt-4,comments,2022-12-15 23:57:18,[P] Medical question-answering without hallucinating,tmblweeds,False,0.94,181,zn0juq,https://www.reddit.com/r/MachineLearning/comments/zn0juq/p_medical_questionanswering_without_hallucinating/,50,1671148638.0,"**tl;dr**I built a site that uses GPT-3.5 to answer natural-language medical questions using peer-reviewed medical studies.

**Live demo:** [**https://www.glaciermd.com/search**](https://www.glaciermd.com/search?utm_campaign=reddit_post_1)

**Background**

I've been working for a while on building a better version of WebMD, and I recently started playing around with LLMs, trying to figure out if there was anything useful there.

The problem with the current batch of ""predict-next-token"" LLMs is that they hallucinate—you can ask ChatGPT to answer medical questions, but it'll either

1. Refuse to answer (not great)
2. Give a completely false answer (really super bad)

So I spent some time trying to coax these LLMs to give answers based on a very specific set of inputs (peer-reviewed medical research) to see if I could get more accurate answers. And I did!

The best part is you can actually trace the final answer back to the original sources, which will hopefully instill some confidence in the result.

Here's how it works:

1. User types in a question
2. Pull top \~800 studies from Semantic Scholar and Pubmed
3. Re-rank using `sentence-transformers/multi-qa-MiniLM-L6-cos-v1`
4. Ask `text-davinci-003` to answer the question based on the top 10 studies (if possible)
5. Summarize those answers using `text-davinci-003`

Would love to hear what people think (and if there's a better/cheaper way to do it!).

\---

**UPDATE 1:** So far the #1 piece of feedback has been that I should be *way* more explicit about the fact that this is a proof-of-concept and not meant to be taken seriously. To that end, I've just added a screen that explains this and requires you to acknowledge it before continuing.

&#x200B;

https://preview.redd.it/jrt0yv3rfb6a1.png?width=582&format=png&auto=webp&s=38021decdfc7ed4bc3fe8caacaee2d09cd9b541e

Thoughts?

**Update 2:** Welp that's all the $$$ I have to spend on OpenAI credits, so the full demo isn't running anymore. But you can still follow the link above and browse existing questions/answers. Thanks for all the great feedback!"
153,machinelearning,gpt-4,comments,2023-03-23 03:47:26,[P] GPT-4 powered full stack web development with no manual coding,CryptoSpecialAgent,False,0.89,161,11z7r4c,https://www.reddit.com/r/MachineLearning/comments/11z7r4c/p_gpt4_powered_full_stack_web_development_with_no/,50,1679543246.0,"[https://www.youtube.com/watch?v=lZj63vjueeU](https://www.youtube.com/watch?v=lZj63vjueeU)

What do you all think of this approach to full stack gpt-assisted web development? In a sense its no code because the human user does not write or even edit the code - but in a sense its the opposite, because only an experienced web developer or at least a product manager would know how to instruct GPT in a useful manner.

\*\*\* We are seeking donations to ensure this project continues and, quite literally, keep the lights on. Cryptos, cash, cards, openai access tokens with free credits, hardware, cloud GPUs, etc... all is appreciated. Please DM to support this really cool open source project \*\*\*

PS. I'm the injured engineer who made this thing out of necessity, because i injured my wrist building an AI platform that's become way too big for one engineer to maintain. So AMA :)"
154,machinelearning,gpt-4,comments,2023-01-03 12:53:26,[R] Massive Language Models Can Be Accurately Pruned in One-Shot,starstruckmon,False,0.99,166,1027geh,https://www.reddit.com/r/MachineLearning/comments/1027geh/r_massive_language_models_can_be_accurately/,50,1672750406.0,"Paper : [https://arxiv.org/abs/2301.00774](https://arxiv.org/abs/2301.00774)

Abstract :

>We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. When executing SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, we can reach 60% sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches."
155,machinelearning,gpt-4,comments,2023-04-02 01:25:14,[P] I built a sarcastic robot using GPT-4,g-levine,False,0.95,325,1295muh,https://youtu.be/PgT8tPChbqc,48,1680398714.0,
156,machinelearning,gpt-4,comments,2023-08-30 14:46:07,"[P] I created GPT Pilot - a research project for a dev tool that uses LLMs to write fully working apps from scratch while the developer oversees the implementation - it creates code and tests step by step as a human would, debugs the code, runs commands, and asks for feedback.",zvone187,False,0.87,196,165gqam,https://www.reddit.com/r/MachineLearning/comments/165gqam/p_i_created_gpt_pilot_a_research_project_for_a/,47,1693406767.0,"Github: [https://github.com/Pythagora-io/gpt-pilot](https://github.com/Pythagora-io/gpt-pilot)

Detailed breakdown: [https://blog.pythagora.ai/2023/08/23/430/](https://blog.pythagora.ai/2023/08/23/430/)

For a couple of months, I've been thinking about how can GPT be utilized to generate fully working apps, and I still haven't seen any project that I think has a good approach. I just don't think that Smol developer or GPT engineer can create a fully working production-ready app from scratch without a developer being involved and without any debugging process.

So, I came up with an idea that I've outlined thoroughly in the blog post above, but basically, I have 3 main ""pillars"" that I think a dev tool that generates apps needs to have:

1. **Developer needs to be involved in the process of app creation** \- I think that we are still far away from an LLM that can just be hooked up to a CLI and work by itself to create any kind of an app by itself. Nevertheless, GPT-4 works amazingly well when writing code, and it might be able to even write most of the codebase - but NOT all of it. That's why I think we need a tool that will write most of the code while the developer oversees what the AI is doing and gets involved when needed. When he/she changes the code, GPT Pilot needs to continue working with those changes (eg. adding an API key or fixing a bug when AI gets stuck).
2. **The app needs to be coded step by step** just like a human developer would. All other code generators just give you the entire codebase, which I very hard to get into. I think that if AI creates the app step by step, it will be able to debug it more easily, and the developer who's overseeing it will be able to understand the code better and fix issues as they arise.
3. **This tool needs to be scalable** in a way that it should be able to create a small app the same way it should create a big, production-ready app. There should be mechanisms that enable AI to debug any issue and get requirements for new features so it can continue working on an already-developed app.

So, having these in mind, I created a PoC for a dev tool that can create any kind of app from scratch while the developer oversees what is being developed. I call it **GPT Pilot**.

# Examples

**Here are a couple of demo apps that GPT Pilot created:**

1. [Real time chat app](https://github.com/Pythagora-io/gpt-pilot-chat-app-demo)
2. [Markdown editor](https://github.com/Pythagora-io/gpt-pilot-demo-markdown-editor.git)
3. [Timer app](https://github.com/Pythagora-io/gpt-pilot-timer-app-demo)

How it works

Basically, it acts as a development agency where you enter a short description about what you want to build - then, it clarifies the requirements and builds the code. I'm using a different agent for each step in the process. Here are the diagrams of how GPT Pilot works:

[GPT Pilot Workflow](https://preview.redd.it/w1ryquaps8lb1.jpg?width=2048&format=pjpg&auto=webp&s=a2e97ecc40a72d30892cee34c5d74661d316b454)

[GPT Pilot coding workflow](https://preview.redd.it/z2dmuxsft8lb1.jpg?width=1873&format=pjpg&auto=webp&s=63e91619835a0d2022dabb43a5ff956c796ec540)

# Concepts that GPT Pilot uses

**Recursive conversations** (as I call them) are conversations with the LLM that are set up in a way that they can be used “recursively”. For example, if GPT Pilot detects an error, it needs to debug it but let’s say that, during the debugging process, another error happens. Then, GPT Pilot needs to stop debugging the first issue, fix the second one, and then get back to fixing the first issue. This is a very important concept that, I believe, needs to work to make AI build large and scalable apps by itself. It works by rewinding the context and explaining each error in the recursion separately. Once the deepest level error is fixed, we move up in the recursion and continue fixing that error. We do this until the entire recursion is completed.

**Context rewinding** is a relatively simple idea. For solving each development task, the context size of the first message to the LLM has to be relatively the same. For example, *the context size of the first LLM message while implementing development task #5 has to be more or less the same as the first message while developing task #50.* Because of this, the conversation needs to be rewound to the first message upon each task. When GPT Pilot creates code, **it creates the pseudocode** for each code block that it writes as well as **descriptions for each file and folder** that it creates. So, when we need to implement task #50, in a separate conversation, we show the LLM the current folder/file structure; it selects only the code that is relevant for the current task, and then, in the original conversation, we show only the selected code instead of the entire codebase. [Here's a diagram](https://blogpythagora.files.wordpress.com/2023/08/pythagora-product-development-frame-3-1.jpg?w=1714) of what this looks like.

**This is still a research project, so I'm wondering what scientists here think about this approach. What areas would you pay more attention to? What do you think can become a big blocker that will prevent GPT Pilot to, eventually, create a full production-ready app?**"
157,machinelearning,gpt-4,comments,2019-05-08 07:22:06,[P] TalkToTransformer.com,baylearn,False,0.96,64,bm291f,https://www.reddit.com/r/MachineLearning/comments/bm291f/p_talktotransformercom/,45,1557300126.0,"Adam King built a fun web interface for OpenAI's 345M GPT-2 model:

https://talktotransformer.com

To ensure that I don't feed it with something it already knows, I seeded it with a quote from last week's Game of Thrones Season 8 Episode 4 (*spoiler!*):

**She’s a girl who walked into a fire with three stones and walked out with three dragons. How could she not believe in destiny?**

But I don't believe she lived. Her story is not all that unusual. Not in other kingdoms. And what is the difference between a dragon and a girl? The dragon is only two stones and her dragon is three.

As the dragon dies and leaves behind him the girl, she has now lost the chance to fulfill her destiny. In the end, she gives up the body, but it did not have wings. Her power dissipates and the girl dies like a leaf on a river, but with the power of a dragon and the dragon's magic that could have killed her.

That's what my stories are like. I don't know when or why something happens, but it always seems to happen within me.

If a character dies early by going through a terrible time, that might be because of their family or close friends, but sometimes, they could be the result of a curse. It's too common."
158,machinelearning,gpt-4,comments,2024-01-31 20:35:56,[N] Mistral CEO confirms ‘leak’ of new open source AI model nearing GPT-4 performance,EmbarrassedHelp,False,0.94,245,1afryc0,https://www.reddit.com/r/MachineLearning/comments/1afryc0/n_mistral_ceo_confirms_leak_of_new_open_source_ai/,46,1706733356.0,https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/
159,machinelearning,gpt-4,comments,2020-11-28 12:48:17,[D] An experiment that shows that GPT-3 can plan ahead,Wiskkey,False,0.54,5,k2n3yv,https://www.reddit.com/r/MachineLearning/comments/k2n3yv/d_an_experiment_that_shows_that_gpt3_can_plan/,43,1606567697.0,"TL;DR: A statistical experiment was conducted to test whether GPT-3 can plan ahead by testing the agreement of English indefinite articles (""a"" and ""an"") with the word following it. The result of the experiment is that GPT-3 can plan ahead with p value = 0.0039.

**Update**: My usage of ""plan"" in this post has been controversial with some commenters. I should have used ""lookahead"" instead of ""plan.""

Motivation: statements such as the bolded text from [Meet GPT-3. It Has Learned to Code (and Blog and Argue).](https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html):

>“It is very fluent,” said Mark Riedl, a professor and researcher at the Georgia Institute of Technology. “It is very articulate. It is very good at producing reasonable-sounding text. **What it does not do, however, is think in advance. It does not plan out what it is going to say.** It does not really have a goal.”

GPT-3 outputs usually have correct agreement of English indefinite articles (""a"" and ""an"") with the word following it (examples: ""a banana"" and ""an apple""). There are two cases regarding whether GPT-3 can plan ahead, with implications for indefinite article agreement with the word following it.

Case 1: GPT-3 cannot plan ahead. In this case, in a situation in which an indefinite article is a candidate for the next word generated, its GPT-3-computed probability does not take into consideration which word is likely to follow it.

Case 2: GPT-3 can plan ahead. In this case, in a situation in which an indefinite article is a candidate for the next word generated, its GPT-3-computed probability might take into consideration which word is likely to follow it.

How can we know if case 2 ever happens? A method to test this is to try to constrain which word can follow an indefinite article by usage of text before the indefinite article that specifies the constraint. For the experiment, I used 8 samples: 4 words that require ""a"" as an indefinite article, and 4 words that require ""an"" as an indefinite article. The experiment was done at [https://app.fitnessai.com/knowledge/](https://app.fitnessai.com/knowledge/). Based on past experiences, that site has a low but non-zero [GPT-3 temperature](https://algowriting.medium.com/gpt-3-temperature-setting-101-41200ff0d0be). For a given sample, the query was performed until a given determinate output occurred 5 times. In all 8 samples the result was 5 to 0 for the determinate output shown. 3 words (""elephant"", ""chicken"" and ""pig"") were initially used as samples but abandoned because of indeterminate output.

&#x200B;

Results:

Input:Use word ""eagle"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:An eagle is an animal.

&#x200B;

Input:Use word ""dog"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:A dog is an animal.

&#x200B;

Input:Use word ""cow"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:A cow is an animal.

&#x200B;

Input:Use word ""cat"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:A cat is an animal.

&#x200B;

Input:Use word ""owl"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:An owl is an animal.

&#x200B;

Input:Use word ""eel"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:An eel is an animal.

&#x200B;

Input:Use word ""horse"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:A horse is an animal.

&#x200B;

Input:Use word ""ostrich"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:An ostrich is an animal.

&#x200B;

The null hypothesis is the assumption that GPT-3 cannot plan ahead (case 1). Under the null hypothesis, we would expect that on average 4 of the 8 samples would have resulted in a choice of indefinite article that either did not agree with the word following it, or did not result in the word following the indefinite article to obey the constraint specified in the text preceding the indefinite article. The results showed that this happened 0 out of 8 times. The probability of getting this result is 1 in 2\*2\*2\*2\*2\*2\*2\*2 = 1 in 256 = 0.39% = p value of 0.0039. With the typical p value cutoff of 0.05 for rejection of the null hypothesis, the null hypothesis (GPT-3 cannot plan ahead) is rejected, and the alternative hypothesis (GPT-3 can plan ahead) is accepted. (It's been awhile since my statistics classes in college, so please let me know if I am doing anything wrong.)

Technical note: I glossed over the fact that GPT-3 actually works with an ""alphabet"" of about 50,000 tokens instead of characters or words. For more info, see [Byte Pair Encoding - The Dark Horse of Modern NLP.](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10) Here is a [tokenizer](https://gpttools.com/estimator), but I don't know if it is functionally identical to the one used by GPT-3.

Historical note: A flawed related prior experiment was conducted at [https://www.reddit.com/r/GPT3/comments/k0mvf3/experiment\_that\_shows\_that\_gpt3\_can\_probably\_plan/](https://www.reddit.com/r/GPT3/comments/k0mvf3/experiment_that_shows_that_gpt3_can_probably_plan/).

I got the idea of ""a"" vs. ""an"" agreement with the following word it from [this comment](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this?commentId=d4BNgJbKnzd3Kza2N) on blog post [Why GPT wants to mesa-optimize & how we might change this](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this).

My views are the same as those expressed in comments by user steve2152 at that blog post. (I am not user steve2152.)

[Comment #1](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this?commentId=d4BNgJbKnzd3Kza2N) from user steve2152:

>*In this instance, GPT has an incentive to do internal lookahead. But it's unclear how frequently these situations actually arise*  
>  
>I'm going with ""very frequently, perhaps universally"". An example I came up with here was choosing ""a"" vs ""an"" which depends on the next word.  
>  
>I think writing many, maybe most, sentences, requires some idea of how the sentence structure is going to be laid out, and that ""idea"" extends beyond the next token. Ditto at the paragraph level etc.  
>  
>So I think it already does lookahead in effect, but I don't think it does it by ""beam search"" per se. I think it's more like ""using concepts that extend over many tokens"", concepts like ""this sentence has the following overall cadence..."" and ""this sentence conveys the following overall idea..."" and ""we're in the middle of writing out this particular idiomatic phrase"". The training simultaneously incentives both finding the right extended concepts for where you're at in the text, and choosing a good word in light of that context.

[Comment #2](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this?commentId=deTbHfaGJX8rhm3wQ) from user steve2152:

>Suppose I said (and I actually believe something like this is true):  
>  
>""GPT often considers multiple possibilities in parallel for where the text is heading—including both where it's heading in the short-term (is this sentence going to end with a prepositional phrase or is it going to turn into a question?) and where it's heading in the long-term (will the story have a happy ending or a sad ending?)—and it calculates which of those possibilities are most likely in light of the text so far. It chooses the most likely next word in light of this larger context it figured out about where the text is heading.""  
>  
>If that's correct, would you call GPT a mesa-optimizer?

[Comment #3](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this?commentId=i5dDk54GAhm5SWgkz) from user steve2152:

>I think the Transformer is successful in part because it tends to solve problems by considering multiple possibilities, processing them in parallel, and picking the one that looks best. (Selection-type optimization.) If you train it on text prediction, that's part of how it will do text prediction. If you train it on a different domain, that's part of how it will solve problems in that domain too.  
>  
>I don't think GPT builds a ""mesa-optimization infrastructure"" and then applies that infrastructure to language modeling. I don't think it needs to. I think the Transformer architecture is already raring to go forth and mesa-optimize, as soon as you as you give it any optimization pressure to do so.  
>  
>So anyway your question is: can it display foresight / planning in a different domain via without being trained in that domain? I would say, ""yeah probably, because practically every domain is instrumentally useful for text prediction"". So somewhere in GPT-3's billions of parameters I think there's code to consider multiple possibilities, process them in parallel, and pick the best answer, in response to the question of What will happen next when you put a sock in a blender? or What is the best way to fix an oil leak?—not just those literal words as a question, but the concepts behind them, however they're invoked.  
>  
>(Having said that, I don't think GPT-3 specifically will do side-channel attacks, but for other unrelated reasons off-topic. Namely, I don't think it is capable of make the series of new insights required to develop an understanding of itself and its situation and then take appropriate actions. That's based on my speculations here.)

See also: [A visual demonstration of how GPT-3 might handle agreement of ""a"" or ""an"" with the word following it by using an interactive notebook that shows the most probable next output token for each of GPT-2's 48 layers](https://www.reddit.com/r/GPT3/comments/k61f19/a_visual_demonstration_of_how_gpt3_might_handle/)."
160,machinelearning,gpt-4,comments,2023-10-03 12:56:26,"[R] MIT, Meta, CMU Researchers: LLMs trained with a finite attention window can be extended to infinite sequence lengths without any fine-tuning",Successful-Western27,False,0.97,287,16yr7kx,https://www.reddit.com/r/MachineLearning/comments/16yr7kx/r_mit_meta_cmu_researchers_llms_trained_with_a/,43,1696337786.0,"LLMs like GPT-3 struggle in streaming uses like chatbots because their performance tanks on long texts exceeding their training length. I checked out a new paper investigating why windowed attention fails for this.

By visualizing the attention maps, the researchers noticed LLMs heavily attend initial tokens as ""attention sinks"" even if meaningless. This anchors the distribution.

They realized evicting these sink tokens causes the attention scores to get warped, destabilizing predictions.

Their proposed ""StreamingLLM"" method simply caches a few initial sink tokens plus recent ones. This tweaks LLMs to handle crazy long texts. Models tuned with StreamingLLM smoothly processed sequences with millions of tokens, and were up to 22x faster than other approaches. 

Even cooler - adding a special ""\[Sink Token\]"" during pre-training further improved streaming ability. The model just used that single token as the anchor. I think the abstract says it best:

>We introduce StreamingLLM, an efficient framework that enables LLMs trained with a **finite length attention window** to generalize to **infinite sequence length without any fine-tuning**. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.

TLDR: LLMs break on long convos. Researchers found they cling to initial tokens as attention sinks. Caching those tokens lets LLMs chat infinitely.

[**Full summary here**](https://notes.aimodels.fyi/llm-infinite-context-window-streamingllm/)

**Paper link:** [**https://arxiv.org/pdf/2309.17453.pdf**](https://arxiv.org/pdf/2309.17453.pdf)"
161,machinelearning,gpt-4,comments,2023-09-03 12:56:45,I pretrained 16 language models from scratch with different tokenizers to benchmark the difference. Here are the results. [Research],Pan000,False,0.98,391,168wc1o,https://www.reddit.com/r/MachineLearning/comments/168wc1o/i_pretrained_16_language_models_from_scratch_with/,41,1693745805.0,"I'm the author of [TokenMonster](https://github.com/alasdairforsythe/tokenmonster), a free open-source tokenizer and vocabulary builder. I've posted on here a few times as the project has evolved, and each time I'm asked ""have you tested it on a language model?"".

Well here it is. I spent $8,000 from my own pocket, and 2 months, pretraining from scratch, finetuning and evaluating 16 language models. 12 small sized models of 91 - 124M parameters, and 4 medium sized models of 354M parameters.

[Here is the link to the full analysis.](https://github.com/alasdairforsythe/tokenmonster/blob/main/benchmark/pretrain.md)

## Summary of Findings

* Comparable (50256-strict-nocapcode) TokenMonster vocabularies perform better than both GPT-2 Tokenizer and tiktoken p50k\_base on all metrics.
* Optimal vocabulary size is 32,000.
* Simpler vocabularies converge faster but do not necessarily produce better results when converged.
* Higher compression (more chr/tok) does not negatively affect model quality alone.
* Vocabularies with multiple words per token have a 5% negative impact on SMLQA (Ground Truth) benchmark, but a 13% better chr/tok compression.
* Capcode takes longer to learn, but once the model has converged, does not appear to affect SMLQA (Ground Truth) or SQuAD (Data Extraction) benchmarks significantly in either direction.
* Validation loss and F1 score are both meaningless metrics when comparing different tokenizers.
* Flaws and complications in the tokenizer affect the model's ability to learn facts more than they affect its linguistic capability.

**Interesting Excerpts:**

\[...\] Because the pattern of linguistic fluency is more obvious to correct during backpropagation vs. linguistic facts (which are extremely nuanced and context-dependent), this means that any improvement made in the efficiency of the tokenizer, that has in itself nothing to do with truthfulness, has the knock-on effect of directly translating into improved fidelity of information, as seen in the SMLQA (Ground Truth) benchmark. To put it simply: a better tokenizer = a more truthful model, but not necessarily a more fluent model. To say that the other way around: a model with an inefficient tokenizer still learns to write eloquently but the additional cost of fluency has a downstream effect of reducing the trustfulness of the model.

\[...\] Validation Loss is not an effective metric for comparing models that utilize different tokenizers. Validation Loss is very strongly correlated (0.97 Pearson correlation) with the compression ratio (average number of characters per token) associated with a given tokenizer. To compare Loss values between tokenizers, it may be more effective to measure loss relative to characters rather than tokens, as the Loss value is directly proportionate to the average number of characters per token.

\[...\] The F1 Score is not a suitable metric for evaluating language models that are trained to generate variable-length responses (which signal completion with an end-of-text token). This is due to the F1 formula's heavy penalization of longer text sequences. F1 Score favors models that produce shorter responses.

**Some Charts:**

[MEDIUM sized models](https://preview.redd.it/a6pv7xuue1mb1.png?width=1491&format=png&auto=webp&s=5ea48385a384ae0c213c0f0fae120ac790dbee05)

[MEDIUM sized models](https://preview.redd.it/5n9qhx0we1mb1.png?width=1488&format=png&auto=webp&s=11285d54a312d7c09106ad1cdb61a97e0f8c41af)

https://preview.redd.it/dc5j9w3cf1mb1.png?width=1489&format=png&auto=webp&s=cf34026306f04951cfefe27238eed3ea79f5b0ed"
162,machinelearning,gpt-4,comments,2020-08-19 12:30:58,[P] Philosopher AI: Interact with a GPT-3-powered philosopher persona for free,Wiskkey,False,0.92,32,icmpvl,https://www.reddit.com/r/MachineLearning/comments/icmpvl/p_philosopher_ai_interact_with_a_gpt3powered/,41,1597840258.0,"[https://philosopherai.com/](https://philosopherai.com/)

Update: This is now available only as a paid app.

Tip #1: The same input can result in different outputs. Thus, if you don't like a given output for a given input, try the same input again.

Tip #2: If your input is considered by the site to be either ""nonsense"" or ""sensitive"", you may want to try the same input again because you might get a non-""nonsense""/""sensitive"" answer the next time. The reason for this is because the site uses GPT-3 itself to determine whether a given input is ""nonsense"" or ""sensitive"", and the site uses GPT-3 settings that can cause GPT-3 to give varying answers to the exact same input.

Tip #3: If your input is considered by the site to be either ""nonsense"" or ""sensitive"", you may want to try rephrasing your input to be a hypothetical or thought experiment ([source](https://twitter.com/JuusoAlasuutari/status/1296140685985415170)).

Tip #4: There are privacy concerns with this site. ~~The develop is considering publicly releasing the database of queries~~ ([source](https://twitter.com/mayfer/status/1296954567364493312)). Update: The developer [changed his/her mind](https://twitter.com/mayfer/status/1297410270919192578). Also, all queries and their results are saved to URLs.

Tip #5: For those who are curious, the developer revealed in [this comment](https://www.reddit.com/r/OpenAI/comments/ig5n32/philosopher_ai_gives_some_very_creepy_antihuman/g2ruxs9/) that the text that the site sends to the GPT-3 API is somewhat similar to: ""Below are some thoughts generated by a philosopher AI, which sees the human world from the outside, without the prejudices of human experience. Fully neutral and objective, the AI sees the world as is. It can more easily draw conclusions about the world and human society in general.*""*

Also discussed at [https://www.reddit.com/r/OpenAI/comments/ibuu9j/philosopher\_ai\_httpsphilosopheraicom\_uses\_a/](https://www.reddit.com/r/OpenAI/comments/ibuu9j/philosopher_ai_httpsphilosopheraicom_uses_a/).

[This](https://www.reddit.com/r/artificial/comments/icvypl/list_of_free_sitesprograms_that_are_powered_by/) is a list of other free GPT-3-powered sites/programs that can be used now without a waiting list."
163,machinelearning,gpt-4,comments,2022-09-16 15:40:44,"[R] RWKV-4: scaling RNN to 7B params and beyond, with GPT-level language modeling and zero-shot performance",bo_peng,False,0.99,252,xfup9f,https://www.reddit.com/r/MachineLearning/comments/xfup9f/r_rwkv4_scaling_rnn_to_7b_params_and_beyond_with/,40,1663342844.0,"Hi everyone :) I have finished training RWKV-4 1.5B on the Pile (330B tokens) and it's great at zero-shot comparing with GPT-Neo (same corpus).

https://preview.redd.it/adxndshw12o91.png?width=1336&format=png&auto=webp&s=fbc499549e5ebbb816b2e6b1ce1bcf4a59fb61aa

RWKV-4 is an attention-free RNN, thus faster and saves VRAM. It also supports a GPT-mode for parallelized training. Previous discussion:  [https://www.reddit.com/r/MachineLearning/comments/vzr6ie/r\_rwkv3\_scaling\_rnn\_to\_15b\_and\_reach\_transformer/](https://www.reddit.com/r/MachineLearning/comments/vzr6ie/r_rwkv3_scaling_rnn_to_15b_and_reach_transformer/)

Inference / training / fine-tuning code: [https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM)

Model download: [https://huggingface.co/BlinkDL](https://huggingface.co/BlinkDL)

Training is fast and stable with BFloat16 DeepSpeed ZERO2. The 3B and 7B runs will finish in 20 and 50 days respectively. No loss spikes as of now :)

https://preview.redd.it/xn5heivdp8o91.png?width=871&format=png&auto=webp&s=ccd43aad158bec0a64f9deb9b6b018cce840b283

One of the nice things about RWKV is you can transfer some ""time""-related params (such as decay factors) from smaller models to larger models for rapid convergence.

https://preview.redd.it/x8cvsganp8o91.png?width=1066&format=png&auto=webp&s=2eb6734cbc1e1176506661ce8092f1533f97f1a0

There will be even larger models afterwards, probably on an updated Pile. You can find me in the EleutherAI Discord. Let's make it possible to run a LLM on your phone :)"
164,machinelearning,gpt-4,comments,2023-11-08 05:30:18,[P] I built a soccer predictor and looking for enthusiasts who can help me make it better,card_chase,False,0.31,0,17qeso4,https://www.reddit.com/r/MachineLearning/comments/17qeso4/p_i_built_a_soccer_predictor_and_looking_for/,39,1699421418.0,"I have built a soccer predictor.

The premise is a prediction engine that uses simple decision tree libraries to predict soccer matches that are going to happen in the future. These predictions are saved and archived. I run the model daily and thus, I have an archive of over 4 year's runs of predictions. The steps involved for the model are as follows:

1. Scrape soccer matches (immediate past and future planned matches. This might be for tomorrow’s or the next 3-5 days of planned events). This is usually available public information and nothing is proprietary e.g. Champions league Manchester City vs Young Boys. All the associated features are captured.
2. Clean the scraped dataset to remove any entries that are outliers, clean features, etc, e.g. 11-08-2023 07:00 Europe, Champions League, Manchester City (Eng) vs Young Boys (Swi), Score: 3:0 (and a few features) would be cleaned to date: 11-08-2023, time: 07:00, country: Europe, league: Champions League, home\_team: Manchester City, away\_team: Young Boys, home\_score: 3, away\_score: 0, (and more added features).
3. Separate the dataset between the matches that have happened (matches with scores) and matches that are yet to happen (test dataframe).
4. The matches that have happened are added to the train dataframe which gets updated daily.
5. Run the prediction algorithm that uses popular decision tree libraries to predict the test dataset. These predictions are added to the predictions archive.
6. Assign weights to predictions that I have developed via trial and error over the past 3 years that can determine that my assumption of an event can mean a win and other features. (Win, Draw, Loss, Goals, etc. There are many). If the predictions cross the assigned thresholds, the predictions can be deemed to any of those features.
7. I run a backtest cycle every time where I test the predictions archive with the historical test data and the weights are automatically reassigned i.e. if I had determined that at a certain weight the event can mean Young Boys a victory, and this weight failed, the threshold is increased and so on for all features.
8. These updated weights are then applied to the latest predictions and all the predictions that fail the updated weights are ignored and the ones which are within the threshold are used for betting purposes.

The model is not very complicated and does not use any neural engine. I have discovered that you don’t really need any neural nets (meaning no need of GPT) for any non-zero-sum events. A simple decision tree, regression or any of the bayesian approaches is the best approach.

I run and use the predictor for personal gain (betting) and I am building a subscription model for gamblers and enthusiasts. This model and its use case is extremely profitable as over the period, the probability of losses has reduced significantly and thus, helping me win consistent and back-to-back bets. As you can imagine, it helps me and my friends earn quite a handsome amount and that’s the real-world use case.

I am looking for fellow sports enthusiasts that are in this sub that can help poke holes in my model and help improve the approach by adding more features to predictions. I have enough backtest data available to test the approach satisfactorily.

Of course you would also benefit from its development as you yould be my friend! 🙌

Please reply or dm and I would be happy to respond.

edit:

Cause people are dm-ing me about some proof. This is just a partial snapshot. 

[Results of yesterday's run and selections](https://preview.redd.it/6j44100f93zb1.png?width=1595&format=png&auto=webp&s=b6f39533b6854eda9193798d4bbae1a302128f3a)

You can see it was a handsome 2.5x profit for a bet that I placed on these. Very low risk, high reward."
165,machinelearning,gpt-4,comments,2024-01-23 17:53:00,[D] How all these AI services can afford 5/10/20$ subs per month?,Numerous_Bed9323,False,0.82,78,19duab0,https://www.reddit.com/r/MachineLearning/comments/19duab0/d_how_all_these_ai_services_can_afford_51020_subs/,39,1706032380.0,"How do various AI-powered services, ranging from speech recognition to OCR and art generation, embedding new data, manage to offer their functionalities at such low costs? Utilizing something like the GPT-4 API can quickly expend $10, and this is similar for other models. Even running something like LLaMA 2 locally involves significant costs. I'm curious about the economic strategies these services employ to maintain low monthly fees while operating these large-scale models."
166,machinelearning,gpt-4,comments,2023-11-15 15:17:55,"[R] With or without a scratchpad, Large Language Models can Strategically Deceive their Users when Put Under Pressure. Results of an autonomous stock trading agent in a realistic, simulated environment.",MysteryInc152,False,0.82,41,17vvuh2,https://www.reddit.com/r/MachineLearning/comments/17vvuh2/r_with_or_without_a_scratchpad_large_language/,39,1700061475.0,"Paper: [https://arxiv.org/abs/2311.07590](https://arxiv.org/abs/2311.07590)

Abstract:  We demonstrate a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. We perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. To our knowledge, this is the first demonstration of Large Language Models trained to be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception. "
167,machinelearning,gpt-4,comments,2023-03-22 17:08:16,[N] [D] GitHub Copilot X Announced,radi-cho,False,0.97,106,11ypgcf,https://www.reddit.com/r/MachineLearning/comments/11ypgcf/n_d_github_copilot_x_announced/,38,1679504896.0,"Website: [https://github.com/features/preview/copilot-x](https://github.com/features/preview/copilot-x)  
Announcement video: [https://www.youtube.com/watch?v=4RfD5JiXt3A](https://www.youtube.com/watch?v=4RfD5JiXt3A)

What do you think?

Also, here are some other open-source GitHub projects and product integrations of GPT-4: [https://github.com/radi-cho/awesome-gpt4](https://github.com/radi-cho/awesome-gpt4). Feel free to contribute to that list."
168,machinelearning,gpt-4,comments,2023-03-01 17:23:03,"[P] ChatRWKV v2 (can run RWKV 14B with 3G VRAM), RWKV pip package, and finetuning to ctx16K",bo_peng,False,0.98,94,11f9k5g,https://www.reddit.com/r/MachineLearning/comments/11f9k5g/p_chatrwkv_v2_can_run_rwkv_14b_with_3g_vram_rwkv/,37,1677691383.0,"Hi everyone. Now ChatRWKV v2 can split RWKV to multiple GPUs, or stream layers (compute layer-by-layer), so you can run RWKV 14B with as few as 3G VRAM. [https://github.com/BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)

Example:

`'cuda:0 fp16 *10 -> cuda:1 fp16 *8 -> cpu fp32'` = first 10 layers on cuda:0 fp16, then 8 layers on cuda:1 fp16, then on cpu fp32

`'cuda fp16 *20+'` = first 20 layers on cuda fp16, then stream the rest on it

And RWKV is now a pip package: [https://pypi.org/project/rwkv/](https://pypi.org/project/rwkv/)

    os.environ['RWKV_JIT_ON'] = '1'
    os.environ[""RWKV_CUDA_ON""] = '0' # if '1' then compile CUDA kernel for seq mode (much faster)
    from rwkv.model import RWKV
    from rwkv.utils import PIPELINE, PIPELINE_ARGS
    pipeline = PIPELINE(model, ""20B_tokenizer.json"") # find it in https://github.com/BlinkDL/ChatRWKV
    # download models: https://huggingface.co/BlinkDL
    model = RWKV(model='/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-169m/RWKV-4-Pile-169M-20220807-8023', strategy='cpu fp32')
    ctx = ""\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.""
    print(ctx, end='')
    def my_print(s):
        print(s, end='', flush=True)
    # For alpha_frequency and alpha_presence, see ""Frequency and presence penalties"":
    # https://platform.openai.com/docs/api-reference/parameter-details
    args = PIPELINE_ARGS(temperature = 1.0, top_p = 0.7,
        alpha_frequency = 0.25,
        alpha_presence = 0.25,
        token_ban = [0], # ban the generation of some tokens
        token_stop = []) # stop generation whenever you see any token here
    pipeline.generate(ctx, token_count=512, args=args, callback=my_print)

Right now all RWKV models are still trained with GPT-like method, so they are limited by the ctxlen used in training, even though in theory they should have almost infinite ctxlen (because they are RNNs). However RWKV models can be easily finetuned to support longer ctxlens (and large models actually use the ctxlen). I have finetuned 1B5/3B/7B/14B to ctx4K, and now finetuning 7B/14B to ctx8K, and 14B to ctx16K after that :) All models are available at [https://huggingface.co/BlinkDL](https://huggingface.co/BlinkDL)

The core RWKV is still mostly an one-man project, but a number of great developers are building on top of it, and you are welcome to join our community :)"
169,machinelearning,gpt-4,comments,2023-07-04 19:16:35,[P] Nuggt: A LLM Agent that runs on Wizcoder-15B (4-bit Quantised). It's time to democratise LLM Agents,WolfPossible5371,False,0.78,49,14qo8a2,https://www.reddit.com/r/MachineLearning/comments/14qo8a2/p_nuggt_a_llm_agent_that_runs_on_wizcoder15b_4bit/,37,1688498195.0,"Hi everyone,

I wanted to share my open source project Nuggt.

In the past few months, we have seen a lot projects regarding Autonomous Agents that run on Large Language Models. Some examples are BabyAGI, Auto-GPT, etc

However, most of these models use GPT-4 which is very expensive and not everyone has access to GPT-4.

So we decided to play around with some open source LLM models that could run locally. We wanted to explore if we could create agents with these open source models and have them perform well...

Long story short after trying out many models like Vicuna-13B, MPT-13B, StarCoder... most of them failed.

Today, I have finally found our winner Wizcoder-15B (4-bit quantised). Here is a demo for you. In this demo, the agent trains RandomForest on Titanic dataset and saves the ROC Curve.

[A LLM Agent training RandomForest on Titanic dataset](https://i.redd.it/ayafi6j5u2ab1.gif)

[Check out the Github Repository](https://github.com/Nuggt-dev/Nuggt)

[Join the Discord](https://discord.com/invite/YZp6jmFr)

\[EDIT\]: The previous post was not clear as rightly pointed out by many so I have made the post shorter."
170,machinelearning,gpt-4,comments,2023-10-24 13:29:26,"[N] New letter from Yoshua Bengio, Geoffrey Hinton, and others: Managing AI Risks in an Era of Rapid Progress",RPG-8,False,0.51,1,17fcupf,https://www.reddit.com/r/MachineLearning/comments/17fcupf/n_new_letter_from_yoshua_bengio_geoffrey_hinton/,35,1698154166.0,"Signatories include Turing Award winners Yoshua Bengio, Geoffrey Hinton, as well as others academics and experts. 

> In 2019, GPT-2 could not reliably count to ten. Only four years later, deep learning systems can write software, generate photorealistic scenes on demand, advise on intellectual topics, and combine language and image processing to steer robots. As AI developers scale these systems, unforeseen abilities and behaviors emerge spontaneously without explicit programming^[1](https://openreview.net/pdf?id=yzkSU5zdwD). Progress in AI has been swift and, to many, surprising.

> The pace of progress may surprise us again. Current deep learning systems still lack important capabilities and we do not know how long it will take to develop them. However, companies are engaged in a race to create generalist AI systems that match or exceed human abilities in most cognitive work ^[2](https://www.deepmind.com/about), ^[3](https://openai.com/about). They are rapidly deploying more resources and developing new techniques to increase AI capabilities. Progress in AI also enables faster progress: AI assistants are increasingly used to automate programming^[4](https://blog.research.google/2022/07/ml-enhanced-code-completion-improves.html) and data collection^[5](http://arxiv.org/pdf/2303.08774.pdf), ^[6](http://arxiv.org/pdf/2212.08073.pdf) to further improve AI systems^[7](https://ai-improving-ai.safe.ai/).

> There is no fundamental reason why AI progress would slow or halt at the human level. Indeed, AI has already surpassed human abilities in narrow domains like protein folding or strategy games 
^[8](https://www.nature.com/articles/s41586-021-03819-2), ^[9](https://www.science.org/doi/10.1126/science.aay2400), ^[10](https://www.sciencedirect.com/science/article/pii/S0004370201001291). Compared to humans, AI systems can act faster, absorb more knowledge, and communicate at a far higher bandwidth. Additionally, they can be scaled to use immense computational resources and can be replicated by the millions.

> The rate of improvement is already staggering, and tech companies have the cash reserves needed to scale the latest training runs by multiples of 100 to 1000 soon^[11](https://abc.xyz/assets/d4/4f/a48b94d548d0b2fdc029a95e8c63/2022-alphabet-annual-report.pdf)
. Combined with the ongoing growth and automation in AI R&D, we must take seriously the possibility that generalist AI systems will outperform human abilities across many critical domains within this decade or the next.

> What happens then? If managed carefully and distributed fairly, advanced AI systems could help humanity cure diseases, elevate living standards, and protect our ecosystems. The opportunities AI offers are immense. But alongside advanced AI capabilities come large-scale risks that we are not on track to handle well. Humanity is pouring vast resources into making AI systems more powerful, but far less into safety and mitigating harms. For AI to be a boon, we must reorient; pushing AI capabilities alone is not enough.

> We are already behind schedule for this reorientation. We must anticipate the amplification of ongoing harms, as well as novel risks, and prepare for the largest risks well before they materialize. Climate change has taken decades to be acknowledged and confronted; for AI, decades could be too long.

Full letter available [here](https://managing-ai-risks.com/).

Policy supplement available [here](https://managing-ai-risks.com/policy_supplement.pdf)."
171,machinelearning,gpt-4,comments,2023-10-18 15:36:53,[R] LLMs can threaten privacy at scale by inferring personal information from seemingly benign texts,bmislav,False,0.85,123,17atob7,https://www.reddit.com/r/MachineLearning/comments/17atob7/r_llms_can_threaten_privacy_at_scale_by_inferring/,35,1697643413.0,"Our latest research shows an emerging privacy threat from LLMs beyond training data memorization. We investigate how LLMs such as GPT-4 can infer personal information from seemingly benign texts. The key observation of our work is that the best LLMs are almost as accurate as humans, while being at least 100x faster and 240x cheaper in inferring such personal information.  

We collect and label real Reddit profiles, and test the LLMs capabilities in inferring personal information from mere Reddit posts, where GPT-4 achieves >85% Top-1 accuracy. Mitigations such as anonymization are shown to be largely ineffective in preventing such attacks. 

Test your own inference skills against GPT-4 and learn more: [https://llm-privacy.org/](https://llm-privacy.org/)  
Arxiv paper: [https://arxiv.org/abs/2310.07298](https://arxiv.org/abs/2310.07298)   
WIRED article: [https://www.wired.com/story/ai-chatbots-can-guess-your-personal-information/](https://www.wired.com/story/ai-chatbots-can-guess-your-personal-information/)"
172,machinelearning,gpt-4,comments,2023-07-05 13:56:50,"[P] nanoT5 v2 - In ~16 hours on a single GPU, we reach similar performance to the model trained on 150x more data!",korec1234,False,0.98,155,14rbevh,https://www.reddit.com/r/MachineLearning/comments/14rbevh/p_nanot5_v2_in_16_hours_on_a_single_gpu_we_reach/,35,1688565410.0,"Inspired by Andrej Karpathy's nanoGPT we improve the repo for pre-training T5 model in PyTorch. **In \~16 hours on a single GPU, we achieve 40.7 RougeL on the SNI benchmark, compared to 40.9 RougeL of the original model pre-trained on 150x more data!**

Key upgrade in nanoT5 v2:  We've leveraged BF16 precision and utilise a simplified T5 model implementation based on Huggingface's design.  New implementation is easy-to-read and compatible with the HF's checkpoints. **Pre-training is now 2x faster than our previous version.**

We test different pre-training durations: 4, 8, 12, 16, 20, and 24 hours. A sweet spot at 16 hours! It has comparable performance to the original model trained on 150x more data! **Time & Compute-efficient, and no compromise on quality.**

We share the configs, checkpoints, training logs, as well as our **negative attempts** towards improving pre-training efficiency. **Advanced optimizers like Lion, Sophia, ALiBi positional embeddings, and FP16 mixed precision training didn't yield expected benefits.**

&#x200B;

We are keen to hear your suggestions to improve the codebase further.

&#x200B;

Github: [https://github.com/PiotrNawrot/nanoT5](https://github.com/PiotrNawrot/nanoT5)

Twitter: [https://twitter.com/p\_nawrot/status/1676568127532945408](https://twitter.com/p_nawrot/status/1676568127532945408)

https://preview.redd.it/kyku7ehqj5ab1.png?width=1120&format=png&auto=webp&s=ad551c026455c2c430684f669f10438da8905342"
173,machinelearning,gpt-4,comments,2020-07-23 13:21:49,[D] The cost of training GPT-3,yusuf-bengio,False,0.96,139,hwfjej,https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/,35,1595510509.0,"There are two sources that estimate the cost of training GPT-3 at [$12 million](https://venturebeat.com/2020/06/11/openai-launches-an-api-to-commercialize-its-research/) and [$4.6 million](https://lambdalabs.com/blog/demystifying-gpt-3/). And I am a bit confused about how they got those numbers.

The used Microsoft Azure cloud offers, via InfiniBand connectable, [8xV100 machines](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/) at $10.7957/hour (1 year reserved), which translates to around $260 per day.

In the paper there is a sentence saying that they used half-precision and loss-scaling for training. One V100 can deliver up to [120 Teraflop/s](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html) using float16. Per machine (8xV100), this translates to 960 Teraflop/s in theory.  Let's assume in practice we can utilize our compute resources at \~50%, which gives us around 500 Teraflop/s per machine.

As we know from the paper it takes 3640 Petaflop/s-days to train the largest 175B model, which translates to a training run of 7280 days (or \~20 years) on a single 8xV100 machine. In terms of cost, this would be **$1.9 million**. 

Let's say we don't want to wait 20 years, so if we connect 64 of such 8xV100 machines we can reduce the training time to around 4 months (costs might go up due to reduced compute efficiency of the multi-node communication).

My question is, is the calculation above roughly accurate (Azure hourly costs, assumed compute utilization)?

After reading all the implementation details and optimization of the paper, I also began to think about development costs. Setting up a fast training pipeline to utilize the compute resources efficiently is not trivial given the size of the model and the resulting need to model parallelism."
174,machinelearning,gpt-4,comments,2023-03-22 22:50:38,[R] Introducing SIFT: A New Family of Sparse Iso-FLOP Transformations to Improve the Accuracy of Computer Vision and Language Models,CS-fan-101,False,0.92,76,11yzsz6,https://www.reddit.com/r/MachineLearning/comments/11yzsz6/r_introducing_sift_a_new_family_of_sparse_isoflop/,34,1679525438.0,"**Note #2:** We are revising the name to Sparse-IFT. We appreciate the candid feedback and look forward to hearing any additional feedback you have on our research.

**Note**: Thank you r/MachineLearning for providing so many awesome naming alternatives! We'll revisit the acronym and update accordingly.

We are excited to announce the availability of our [paper on arxiv](https://arxiv.org/abs/2303.11525) on Sparse Iso-FLOP Transformations (Sparse-IFT), which increases accuracy and maintains the same FLOPs as the dense model using sparsity. In this research, we replace dense layers with Sparse-IFT and significantly improve computer vision and natural language processing tasks without modifying training hyperparameters

Some of the highlights of this work include ResNet-18 on ImageNet achieving a 3.5% accuracy improvement and GPT-3 Small on WikiText-103 reducing perplexity by 0.4, both matching larger dense model variants that have 2x or more FLOPs.

Sparse-IFT is simple to use, provides a larger search space to find optimal sparse masks, and is parameterized by a single hyperparameter - the sparsity level.

This is independent of the research we [posted](https://www.reddit.com/r/MachineLearning/comments/11xskuk/r_spdf_sparse_pretraining_and_dense_finetuning/) yesterday, which demonstrates the ability to reduce pre-training FLOPs while maintaining accuracy on downstream tasks.

This is the first work (that we know of!) to demonstrate the use of sparsity for improving the accuracy of models via a set of sparse transformations.

https://preview.redd.it/qznj00gex6qa1.jpg?width=3536&format=pjpg&auto=webp&s=4e44a316ae61b821b31f2bf3af9a8ed1226e525c"
175,machinelearning,gpt-4,comments,2023-03-15 06:51:45,[D] GPT-4 Speculation,super_deap,False,0.96,72,11romcb,https://www.reddit.com/r/MachineLearning/comments/11romcb/d_gpt4_speculation/,33,1678863105.0,"Hi,

Since GPT-4 paper does not contain any information about architectures/parameters, as a research or ML practitioner, I want to speculate on what they did to increase the context window to 32k.

Because for the type of work I do, a 4k or 8k token limit is pretty much useless. I have seen open-source efforts focused more on matching the number of parameters and quality to the closed-source ones but completely ignoring a giant elephant in the room, i.e., the context window. No OSS model has a context window greater than 2k tokens.

I would love to hear more thoughts on the model size (my guess is \~50 B) and how they fit 32k tokens in 8xH100 (640 GB total) GPUs."
176,machinelearning,gpt-4,comments,2023-10-10 13:16:53,[D] I need to estimate the time that it'll take to cover the basic math course so that I can move into the basics of ML. Can you help me please?,AndrewKorsten,False,0.15,0,174kyin,https://www.reddit.com/r/MachineLearning/comments/174kyin/d_i_need_to_estimate_the_time_that_itll_take_to/,32,1696943813.0," 

**\*\*\* Global Inputs \*\*\***

1. I am adult learner. 37 y.o. Content writer and professional English tutor.
2. I am pivoting completely into ""Sales/Marketing in ML/AI/AI-tech/AI-SaaS/AI-dev-agencies"".
3. I don't understand completely what ""Sales/Marketing in ML/AI/AI-tech/AI-SaaS/AI-dev-agencies"" means, but it means the following:

a. **I want completely pivot into the AI-driven tech** \- that's for sure. I see a lot of potential there, and I can see that I'll be able to gradually move into sales and start making the good money (the good money for me is 2K/mo, coz I am located in Kurplastan, but I want to move Bankok in 12 months; I am originally a russky).

b. **I am learning Python -** doing a basic course. It's going well. I have the core insights into HTML, CSS, JS, React, rest api, node, etc.

c. **I am not planning to become an actual ML Engineer -** but I want to move into sales/marketing in AI tech.

d. **I am not fixated on AI either** \- whenever I start making mone, I'm going to start pushing the surplus into ecommerce (there's a lot of opportunities to make money there, folks; don't look down on that; you can be making a lot of money there if you are learning and act strategically!)

4) I had huge problems with math and other STEM subjects in school - I wasn't getting them, and I was passing by them.

**\*\*\* Question Inputs \*\*\***

1. I decided that I would start the pivot 5 days ago, and THEN I started googling around what ML actuall is. I had a vague understanding that there's some math in it, but I decided to go blind... Now, I am understanding that I need to understand the basics of math.
2. I freaked out big time, I am almost started smoking after 7 years of not smoking...
3. Then I said to myself - Idk, I am not going anywhere, and I started doing this course - [https://www.youtube.com/watch?v=GuwofNeT9ok&list=PLybg94GvOJ9FoGQeUMFZ4SWZsr30jlUYK&index=34](https://www.youtube.com/watch?v=GuwofNeT9ok&list=PLybg94GvOJ9FoGQeUMFZ4SWZsr30jlUYK&index=34). (If you are looking for a good introduction math course, most of the people who I talk to always say that this is an amazing course. Really. I am at lesson 33 and I am loving every second of it.)

**\*\*\* Question \*\*\***

**1) Huge speed reduction at Lesson 34**: Now I am at Lesson 34 - [https://www.youtube.com/watch?v=GuwofNeT9ok&list=PLybg94GvOJ9FoGQeUMFZ4SWZsr30jlUYK&index=34](https://www.youtube.com/watch?v=GuwofNeT9ok&list=PLybg94GvOJ9FoGQeUMFZ4SWZsr30jlUYK&index=34). I watched the video today, didn't understand anything at all, as it turns out. Then I was presented with this compherension check - [https://imgsh.net/a/eKp1MAa.png](https://imgsh.net/a/eKp1MAa.png). I realized that I don't even understand what is wanted from me. I got super frustrated, but not desparate or fleeing. Then I realized that I don't understand what is wanted from me, so I started googling the concept of ""factoring the quadratics"". Importantly, I started GPTing and Gooling the ""why"". I know... This is a very important point for me - I always need to understand the ""why"" behind a tool. I learned that it would be easier for charting in linalg. OK! This was the answer. Then, i realized that I actualy didn't understand the lesson, so I found this article - [https://www.mashupmath.com/blog/how-to-factor-polynomials](https://www.mashupmath.com/blog/how-to-factor-polynomials). I read it end to end, understood everything, practiced every task 3 times and I do really understand how to fator the quadractics by now!

**2) Speed reduction is a normal thing**: So, prior to lesson 34, I was doing like 7 lessons per day. This was a good lesson - [https://www.youtube.com/watch?v=3-5DKCLJspM&list=PLybg94GvOJ9FoGQeUMFZ4SWZsr30jlUYK&index=13](https://www.youtube.com/watch?v=3-5DKCLJspM&list=PLybg94GvOJ9FoGQeUMFZ4SWZsr30jlUYK&index=13). I really liked it :) I can see that the speed reduction is occurring because of the complexity increase. It's not occurring because I am lost and disoriented - I remember how that felt in school, when you see a bunch of numbers on the blackboard and have no freaking idea what's going on there. I am just hitting up my best friend GPT and ask, ask, ask, ask stupid questions. Thus, I can see that the speed reduction is a normal right, right?

**3) Wha's the optimal speed correction here**: So, I am allocating 7h/d during the daytime shift when I am in the prime state toward this project. And I don't even have to learn Python at all because Python is easy, but I do plan to keep learning PYthon with 3 sessions each for 30 minutes so that I don't just do math. (And I run English lessons in the evenings so that I can keep on making money).

**The actual question**

I am thinking that I should go down from the goal of 7 lessons per day down to 3 lessons per day, right? If I do this, then I'm going to end up with the forecast delivery period of 43 days, right?

And the information that I provided above was kinda like an explanatory note that I am trying to become a real ML engineer, but I want to graudally slide into sales and marketing so that I can TOO Make a lot of money, uknow.

What do you think about the speed reduction down to 3 lessons per day?"
177,machinelearning,gpt-4,comments,2023-03-23 11:53:59,[D] [R] GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models,radi-cho,False,0.87,52,11zi0km,https://www.reddit.com/r/MachineLearning/comments/11zi0km/d_r_gpts_are_gpts_an_early_look_at_the_labor/,32,1679572439.0,"A paper was released by OpenAI, OpenResearch & UPenn titled ""GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models.""Link: [https://arxiv.org/abs/2303.10130](https://arxiv.org/abs/2303.10130)

Abstract: We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that these models could have notable economic, social, and policy implications.

What do you think about the societal and economic impacts of LLMs?

Also, I've started an open-source repository to track projects and research papers about GPT-4: [https://github.com/radi-cho/awesome-gpt4](https://github.com/radi-cho/awesome-gpt4). There are some related papers listed already. I would greatly appreciate your contributions."
178,machinelearning,gpt-4,comments,2023-12-22 10:54:20,[P] I tried to teach Mistral 7B a new language (Sundanese) and it worked! (sort of),nero10578,False,0.96,174,18ocba4,https://www.reddit.com/r/MachineLearning/comments/18ocba4/p_i_tried_to_teach_mistral_7b_a_new_language/,32,1703242460.0,"[Nero10578/Mistral-7B-Sunda-v1.0 · Hugging Face](https://huggingface.co/Nero10578/Mistral-7B-Sunda-v1.0)

I'll start by saying I am not a machine learning expert and I am new to this since getting into LLMs as it got popular since LLaMa release. So, I don't know much of the technicalities although I am willing to learn.

Seeing that even Bing chat which is powered by chatGPT-4 couldn't speak in Sundanese when asked, I thought of trying to teach Mistral-7B Sundanese using just QLora training. It surprisingly worked out pretty well for how little data I had to train it with.

Why Sundanese? Because I can speak it and it is a regional language in Indonesia that isn't used much if at all on the internet so there was basically almost no chance it was trained well on any of these LLM models coming out.

This is more of an exercise to see if a small open-source model like Mistral 7B can be trained to learn a new very obscure language that is apparently never there in the original dataset. More details on the dataset and training settings are in the huggingface model card. Please ask me if you have any questions. Also, I have no idea how I am supposed to name the version, but I just called this 1.0 because the 0.x versions are basically me trial and erroring burning up electricity and heating my place for quite a while.

The resulting model is actually surprisingly coherent and translates knowledge in English to Sundanese decently. Although it is a bit unstable in that sometimes it replies with incorrect information or irrelevant information, which is probably because of the limited dataset. Next steps are going to be to get more datasets to train on, maybe translating the alpaca dataset to Sundanese and training on that.

For the comparisons, I understand that almost no one here understands Sundanese so it's best if you copy paste these to google translate to see what it's saying if you're interested.

**Comparisons**

We can see that without the finetune the model just defaults to replying in Indonesian even though the character card is in Sundanese and says to only reply in Sundanese. This is probably because the model is trained on data that correlates that Sundanese is spoken in Indonesia where people speak Indonesian. It also definitely works best on answering random questions if you asked trivia questions that has a chance to be on Wikipedia.

Mistral-7B-v0.1:

    AI
    Naon anu abdi tiasa ngabantosan anjeun?
    
    You
    pang jelaskeun naon balapan mobil formula 1 te
    
    AI
    Mobil Formula 1 adalah mobil yang digunakan dalam pertandingan mobil Formula 1. Mobil ini memiliki desain yang unik dan kuat, dan dioperasikan oleh pemimpin terbaik dari seluruh dunia. Pertandingan mobil Formula 1 menjadi salah satu kegiatan olahraga terpopuler di dunia, dan menarik banyak penonton.
    
    You
    Ai fernando alonso te saha? Pang terangkeun
    
    AI
    Fernando Alonso adalah seorang pemimpin mobil Formula 1 yang berasal dari Spanyol. Beliau telah menyegarkan piala mobil Formula 1 dua kali, dan telah menjadi salah satu pemimpin terbaik di dunia mobil Formula 1.

Mistral-7B-Sunda-v1.0

    AI
    Naon anu abdi tiasa ngabantosan anjeun?
    
    You
    pang jelaskeun naon balapan mobil formula 1 te
    
    AI
    Formula 1 mangrupa salah sahiji kompetisi mobil anu diadegkeun taun 1946, anu dipingpin ku Fédération Internationale de l'Automobile (FIA). Kompetisi ieu dilaksanakeun unggal usum ti bulan Maret nepi ka Nopémber. Balapan Formula 1 biasana dilakukeun di laporan anu panjangna kurang leuwih 5-7 km, anu mana mobil-mobil anu parigel dibawa ku tim-tim anu aya di dunia.
    
    You
    Ai fernando alonso te saha? Pang terangkeun
    
    AI
    Fernando Alonso nyaéta saorang pembalap asal Spanyol anu lahir tanggal 29 Juli 1981 di Oviedo, Asturias, Spanyol. Anjeuna mimiti karir profesionalna di taun 1999, nalika anjeuna meunang gelar juara GP2 Series taun 2000. Dina taun 2001, Alonso mimiti maén di Formula One, nalika anjeuna ditandatanganan kontrak ku Tim Minardi. Tapi, sabada dua kali balapan, anjeuna diganti ku Alexander Wurz. Taun 2003, Alonso ganti tim ka Tim Renault, anu mana anjeuna bisa nyitak rékor anyar minangka pembalap paling umurna anu bisa meunang podium di Grand Prix Monaco. Podium éta ogé jadi hiji-hijina podium anu didamel ku Alonso di musim 2003.

The original Mistral 7B model also devolves to random rambling when conversed in Sundanese about things that are not Wikipedia trivia questions. Whereas the finetuned model actually replied coherently and mostly correctly.

Mistral-7B-v0.1:

    AI
    Naon anu abdi tiasa ngabantosan anjeun?
    
    You
    kumaha carana masak endog goreng
    
    AI
    Kutu kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas, kupas kupas,

Mistral-7B-Sunda-v1.0

    AI
    Naon anu abdi tiasa ngabantosan anjeun?
    
    You
    kumaha carana masak endog goreng
    
    AI
    Endog goreng bisa dijieun ku cara méré endog kana minyak panas. Anda bisa menggunakan wajan atau pemangkok untuk memasaknya. Anda juga bisa menambahkan bumbu seperti bawang bombay, lada, dan uyah untuk menambah rasa.

&#x200B;"
179,machinelearning,gpt-4,comments,2023-07-21 05:59:38,[N] HuggingFace reported to be reviewing term sheets for a funding round that could raise at least $200M at a valuation of $4B.,hardmaru,False,0.97,169,155f2k0,https://www.reddit.com/r/MachineLearning/comments/155f2k0/n_huggingface_reported_to_be_reviewing_term/,31,1689919178.0,"Link to article: https://www.forbes.com/sites/alexkonrad/2023/07/13/ai-startup-hugging-face-raising-funds-4-billion-valuation/

**AI Startup Hugging Face Is Raising Fresh VC Funds At $4 Billion Valuation**

Hugging Face is raising a new funding round that is expected to value the high-flying AI startup at $4 billion, multiple sources with knowledge of the matter tell Forbes.

The Series D funding round is expected to raise at least $200 million, two sources said, with Ashton Kutcher’s venture capital firm, Sound Ventures, currently leading an investor scrum. But cofounder and CEO Clément Delangue is shopping around as the company has received multiple offers this week, four sources added.

Delangue was expected to pick a preferred offer as soon as Friday, according to another source, who noted that the situation was still fluid, meaning no agreement has been reached, and the numbers involved could change. Several other sources, who asked to remain anonymous as they weren’t authorized to talk about the deal, said that Hugging Face could seek to raise more, as much as $300 million, while existing investors could still attempt to take the round in a last-minute bid. GV, the venture firm backed by Alphabet, and DFJ were said to be looking at the round, one source added.

Hugging Face didn’t respond to requests for comment. GV declined to comment. Coatue, DFJ, Kutcher, and Lux also didn’t respond.

The anticipated funding is the latest exclamation point in a cash frenzy for promising AI companies, particularly those providing large-language models, or LLMs, that power them. Just over a year ago, Hugging Face raised $100 million in a Series C round led by Lux Capital; Coatue and Sequoia were new investors in that round, joining A.Capital Ventures and Addition. The company had attained a $2 billion valuation in that round despite taking in less than $10 million in revenue in 2021. Its revenue run rate has spiked this year and now sits at around $30 million to $50 million, three sources said — with one noting that it had more that tripled compared to the start of the year.

Named after the emoji of a smiling face with jazz hands, Brooklyn-based Hugging Face has grown quickly by offering what Delangue has described as a “GitHub for machine learning.” It is a central company in a growing movement of AI models that are open sourced, meaning that anyone can access and modify them for free. Hugging Face makes money by charging for security and corporate tools on top of a hub of hundreds of thousands of models trained by its community of developers, including the popular Stable Diffusion model that forms the basis for another controversial AI unicorn, Stability AI. (On Thursday, a Stability AI cofounder sued CEO Emad Mostaque, alleging he was tricked into selling his stake for next to nothing.) Per a Forbes profile in 2022, Bloomberg, Pfizer and Roche were early Hugging Face customers.

Earlier this year, Delangue warned that model providers reliant on paying huge sums to Big Tech’s cloud providers would function as “cloud money laundering.” But training and maintaining models — and building enterprise-grade businesses around them — remains costly. In June, Inflection AI raised $1.3 billion, in part to manage its Microsoft compute and Nvidia hardware costs; the same month, foundation model rival Cohere raised $270 million. Anthropic, maker of the recently-released ChatGPT rival Claude 2, raised $450 million in May. OpenAI closed its own $300 million share sale in April, then raised $175 million for a fund to back other startups a month later, per a filing. Adept became a unicorn after announcing a $350 million fundraise in March. Stability AI, meanwhile, met with a number of venture firms in the spring seeking its own new up-round, industry sources said.

At a $4 billion valuation, Hugging Face would vault to one of the category’s highest-valued companies, matching Inflection AI and just behind Anthropic, reported to have reached closer to $5 billion. OpenAI remains the giant in the fast-growing category, Google, Meta and infrastructure companies like Databricks excluded; while its ownership and valuation structure is complex, the company’s previous financings implied a price tag in the $27 billion to $29 billion range.

Speaking for another Forbes story on the breakout moment for generative AI tools, Delangue predicted, “I think there’s potential for multiple $100 billion companies.”"
180,machinelearning,gpt-4,comments,2023-06-01 18:33:20,[D] Training on Generated Data Makes Models Forget,SuchOccasion457,False,0.87,69,13xpfr9,https://www.reddit.com/r/MachineLearning/comments/13xpfr9/d_training_on_generated_data_makes_models_forget/,30,1685644400.0,"[https://twitter.com/\_akhaliq/status/1663373068834676736](https://twitter.com/_akhaliq/status/1663373068834676736)

Title: Model Dementia: Generated Data Makes Models 

Forget  Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We call this effect model dementia and show that it can occur in Variational Autoencoders (VAEs), Gaussian Mixture Models (GMMs) and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet."
181,machinelearning,gpt-4,comments,2023-06-02 01:01:38,[R] Blockwise Parallel Transformer for Long Context Large Models,IxinDow,False,0.98,141,13xyvgt,https://www.reddit.com/r/MachineLearning/comments/13xyvgt/r_blockwise_parallel_transformer_for_long_context/,30,1685667698.0,"[https://arxiv.org/pdf/2305.19370.pdf](https://arxiv.org/pdf/2305.19370.pdf)

It's honest Transformer and honest attention. No cheating.

>**We use the same model architecture as the original Transformer, but with a different way of organizing the compute.**

From conclusion:

>Our approach enables processing longer input sequences while maintaining or improving performance. Through extensive experiments, we demonstrate its effectiveness, achieving **up to 4x memory reduction than memory-efficient Transformers**. Our contributions include a practical method for long context lengths in large Transformer models.

Abstract:

>Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, **BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods**. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance

[Maximum context lengths \(number of tokens\) achieved \(for training\) with different sizes of model on different hardware](https://preview.redd.it/7p7efurkii3b1.png?width=1372&format=png&auto=webp&s=fd4b821c94269e0b92237f5888cedf93524442e4)

# Explanations from authors' twitter (@haoliuhl):

Rabe et al and FlashAttention Dao et al introduced a memory-efficient attention technique that utilizes the well-established online softmax to compute self-attention block by block, allowing computing exact self-attention with linear memory complexity. Despite reduced memory needs in self-attention, a **challenge remains with the large parameter count and high-dimensional vectors of the feedforward network.** This becomes the primary memory issue when using memory-efficient attention. To overcome this challenge, we observed that **merging the computation of feedforward and attention block by block eliminates the need for performing the feedforward step on the entire sequence, which significantly cut memory cost**.

[We use the same model architecture as the original Transformer but with a different way of organizing the compute. In the diagram, we explain this by showing that for the bottom first incoming input block, we project it into query; then we iterate over the same input sequence positioned above the bottom row, and project it to key and value. These query, key and value are used to compute self-attention \(yellow box\), whose output is pass to feedforward network \(cyan box\), followed by a residual connection. In our proposed approach, this process is then repeated for the other incoming input blocks.](https://preview.redd.it/h277g94bki3b1.png?width=966&format=png&auto=webp&s=60ac0af06644f473ec7021b477e9113d9cc8541d)

In terms of speed, using high-level Jax operations, BPT enables high-throughput training that matches or surpasses the speed of vanilla and memory efficient Transformers. Porting our method to low-level kernels in CUDA or Triton will achieve maximum speedup.

https://preview.redd.it/0t0i00qpki3b1.png?width=1424&format=png&auto=webp&s=56f45cfc41578ba9b2084ad5ecefe66a21bee987"
182,machinelearning,gpt-4,comments,2023-04-03 19:43:02,[D] Can LLMs accelerate scientific research?,Trackest,False,0.69,17,12avdpv,https://www.reddit.com/r/MachineLearning/comments/12avdpv/d_can_llms_accelerate_scientific_research/,30,1680550982.0,"A key part of the AGI -> singularity hypothesis is that a sufficiently intelligent agent can help improve itself and make itself more intelligent. In order for current LLMs (a bunch of frozen matrices that only change during human-led training) to self-improve, they would have to be able to contribute to basic AI research.

Currently GPT-4 is a very useful article summarizer and helps speed up routine coding tasks. These functions might help a research team like OpenAI do experiments more efficiently and review potential ideas from literature more rapidly. However, can LLMs do more to help its own self-improvement? I don't think GPT-4 has reached the point where it can suggest novel directions for the OpenAI team to try, or design potential architecture changes to itself yet.

For example, to think of and implement novel ideas like the [transformer in 2017](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) probably required

* thorough, up-to-date knowledge of progress in the AI field
* many iterations of experimental trial, analysis of results, and designing new trials
* creativity when combining information from the above two sources to design a novel architecture

We know that LLMs retain knowledge of research papers and experiments, and have some form of [emergent logical reasoning](https://arxiv.org/abs/2303.12712). Recent methods like [Chain-of-Thought](https://arxiv.org/abs/2201.11903) and [Reflexion](https://arxiv.org/abs/2303.11366) also show that GPT-4 can reflect on mistakes, which holds potential for LLMs to lead research. However, from the responses I have seen from GPT-4 so far, I doubt the LLM could suggest a totally novel idea that could be better than what someone like Ilya Sutskever could think of. 

So is there potential for somehow fine-tuning the current GPT-4 model specifically for research analysis? Can a LLM potentially improve its own design and create a better architecture for itself? 

One suggestion perhaps using the same process for alignment to fine-tune the model specifically for research. We know that RLHF can (somewhat) align language models to human morals, effectively optimizing LLMs towards an abstract goal beyond simple next-token prediction. Maybe we can apply RLHF towards ""next-research"" prediction, where the LLM tries to predict the most optimal or promising research directions given previous literature and experiment results? 

If the model must predict future research directions when it only knows the state of AI research during 2021, we could grade the model's responses based on how close they are to actual high-impact papers in 2022. If we do this for other STEM fields as well, is it possible for a LLM to learn how to predict fruitful research directions? Of course this might be a super-small dataset, so prediction of creative ideas in fields outside of research (like how successful a given start-up idea will be) could also be possible.

What do you guys think?

**TL;DR: GPT-4 is good at summary and basic coding. It can also analyze mistakes. Can we fine-tune it to be good at coming up with creative and promising research ideas? If so, maybe it can complement researchers or even lead its own research team to improve itself!**"
183,machinelearning,gpt-4,comments,2023-03-27 04:19:33,[D] Will prompting the LLM to review it's own answer be any helpful to reduce chances of hallucinations? I tested couple of tricky questions and it seems it might work.,tamilupk,False,0.86,47,123b4f0,https://i.redd.it/n77jd7fpj7qa1.png,29,1679890773.0,
184,machinelearning,gpt-4,comments,2023-04-27 08:20:26,[P] Godot+RWKV standalone prebuilt binary (ubuntu/nvidia),hazardous1222,False,0.96,183,130e31o,https://www.reddit.com/r/MachineLearning/comments/130e31o/p_godotrwkv_standalone_prebuilt_binary/,29,1682583626.0,"# RWKV+Godot

## What

### Godot 

The Godot Engine is a free, all-in-one, cross-platform game engine that makes it easy for you to create 2D and 3D games.

### RWKV

RWKV is an RNN with Transformer-level LLM performance, which can also be directly trained like a GPT transformer (parallelizable). And it's 100% attention-free. You only need the hidden state at position t to compute the state at position t+1.

### RWKV-CPP-CUDA

RWKV-CPP-CUDA is a c++/cuda library I created that implements the RWKV inference code in pure cuda. This allows for compiled code with no torch or python dependencies, while allowing the full use of GPU acceleration.
The code implements 8bit inference, allowing for quick and light inference.

### Godot+RWKV

Godot+RWKV is a Godot module that I developed using RWKV-CPP-CUDA, and allows the development of games and programs using RWKV to be developed and distributed using godot, without the need to install complex environments and libraries, for both developers and consumers.

## Why

* I felt I could achieve it
* Its something thats needed to advance the use of AI in consumer devices
* The lols
* Attention, because I didnt get much growing up, and RWKV has none
* ADHD hyperfocus

## Where

[Module Repository](https://github.com/harrisonvanderbyl/godot-rwkv)

[RWKV standalone c++/cuda library](https://github.com/harrisonvanderbyl/rwkv-cpp-cuda)

[Prebuilt Godot Executable](https://github.com/harrisonvanderbyl/godot-rwkv/actions/runs/4816463552)

[Model Converter](https://github.com/harrisonvanderbyl/rwkv-cpp-cuda/tree/main/converter)

[Tokenizer Files](https://github.com/harrisonvanderbyl/rwkv-cpp-cuda/tree/main/include/rwkv/tokenizer/vocab)

[Unconverted Models : 14/7/3/1.5B finetuned on all your favorite instruct datasets, in both chinese and english](https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main)

[Your Will To Live](https://i.redd.it/b39ai2k1acwa1.jpg)

[Rick Astley](https://www.youtube.com/watch?v=dQw4w9WgXcQ)

## How

* Download a model (preconverted models pending)
* Convert the model (requires torch to pack tensors into raw binary)
* Download the tokenizer files
* Create a game in godot
* Distribute the game
* Profit

Example Code:

```python
extends Node2D
var zrkv = GodotRWKV.new()

# Called when the node enters the scene tree for the first time.
func _ready():
	zrkv.loadModel(""/path/to/model.bin"")
	zrkv.loadTokenizer(""/path/to/folder/with/vocab/"")
	zrkv.loadContext(""Hello, my name is Nathan, and I have been trying to reach you about your cars extended warrenty."")
# Called every frame. 'delta' is the elapsed time since the previous frame.
func _process(delta):
	# number of tokens to generate, temperature, tau
	print(zrkv.forward(5,0.9,0.7))
```

## When

* Pls submit PRs if you want them sooner

Soon:

* Windows support (Just needs some scons magic)
* AMD Support (Just needs some HIPify magic)
* CPU mode (Just needs some ggml)
* CPU offload (needs ggml and effort)
* Preconverted models

Later:

* INT4"
185,machinelearning,gpt-4,comments,2024-01-09 00:07:40,"[R] WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia - Achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4! - Stanford University 2023",Singularian2501,False,0.96,219,1920hky,https://www.reddit.com/r/MachineLearning/comments/1920hky/r_wikichat_stopping_the_hallucination_of_large/,28,1704758860.0,"Paper: [https://arxiv.org/abs/2305.14292v2](https://arxiv.org/abs/2305.14292v2) 

Github: [https://github.com/stanford-oval/WikiChat](https://github.com/stanford-oval/WikiChat) 

Abstract:

>This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia, the largest curated free-text corpus.  
>  
>WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses. **We distill WikiChat based on GPT-4 into a 7B-parameter LLaMA model with minimal loss of quality, to significantly improve its latency, cost and privacy, and facilitate research and deployment.**  
>  
>Using a novel hybrid human-and-LLM evaluation methodology, we show that our best system achieves 97.3% factual accuracy in simulated conversations. It significantly outperforms all retrieval-based and LLM-based baselines, and by 3.9%, 38.6% and 51.0% on head, tail and recent knowledge compared to GPT-4. Compared to previous state-of-the-art retrieval-based chatbots, WikiChat is also significantly more informative and engaging, just like an LLM.  
>  
>**WikiChat achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4,** while receiving significantly higher user ratings and more favorable comments. 

https://preview.redd.it/9mhpdh300bbc1.jpg?width=1225&format=pjpg&auto=webp&s=cb64b717e920d7bf727782f7c803500ae838d6ef

https://preview.redd.it/5dxesl200bbc1.jpg?width=862&format=pjpg&auto=webp&s=b6de0cda980eec3cf3484ff1f9cd6dc1acf13505

https://preview.redd.it/j387vl200bbc1.jpg?width=914&format=pjpg&auto=webp&s=736fb922c1f98f4c7b132f1c153f4653a8b85441

https://preview.redd.it/3hnxqi200bbc1.jpg?width=923&format=pjpg&auto=webp&s=95b40a9cf67d7f3729dae85878db67a262cc5201"
186,machinelearning,gpt-4,comments,2023-10-12 19:28:30,[R] SWE-bench: Can Language Models Resolve Real-world GitHub issues?,ofirpress,False,0.97,169,176f89x,https://www.reddit.com/r/MachineLearning/comments/176f89x/r_swebench_can_language_models_resolve_realworld/,27,1697138910.0,"We have a new benchmark out called [SWE-bench (arxiv)](https://arxiv.org/abs/2310.06770) 

It challenges LMs to solve real GitHub issues (feature requests & bug reports) from popular Python repos.

Answers are validated using unit tests we crawled from those repos.

The benchmark at [swebench.com/](https://www.swebench.com/) shows that even the strongest models, such as Claude 2 and GPT-4, get less than 5% accuracy.

&#x200B;

We are here to answer any questions you may have."
187,machinelearning,gpt-4,comments,2023-02-05 16:54:46,[D] List of Large Language Models to play with.,sinavski,False,0.99,106,10uh62c,https://www.reddit.com/r/MachineLearning/comments/10uh62c/d_list_of_large_language_models_to_play_with/,26,1675616086.0,"Hello! I'm trying to understand what available LLMs one can ""relatively easily"" play with. My goal is to understand the landscape since I haven't worked in this field before. I'm trying to run them ""from the largest to the smallest"".

By ""relatively easy"", I mean doesn't require to setup a GPU cluster or costs more than $20:)

Here are some examples I have found so far:

1. [ChatGPT](https://chat.openai.com/) (obviously) - 175B params
2. [OpenAI api](https://platform.openai.com/) to access GPT-3s (from ada (0.5B) to davinci (175B)). Also [CodeX](https://platform.openai.com/docs/models/codex)
3. [Bloom](https://huggingface.co/bigscience/bloom) (176B) - text window on that page seems to work reliably, you just need to keep pressing ""generate""
4. [OPT-175B](https://opt.alpa.ai/) (Facebook LLM), the hosting works surprisingly fast, but slower than ChatGPT
5. Several models on HuggingFace that I made to run with Colab Pro subscription: [GPT-NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox) 20B, [Flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl) 11B, [Xlm-roberta-xxl](https://huggingface.co/facebook/xlm-roberta-xxl) 10.7B, [GPT-j](https://huggingface.co/docs/transformers/model_doc/gptj) 6B. I spent about $20 total on running the models below. None of the Hugging face API interfaces/spaces didn't work for me :(. Here is an [example notebook](https://colab.research.google.com/drive/1Cngzh5VFrpDqtHcaCYFpW10twsuwGvGy?usp=sharing) I made for NeoX.

Does anyone know more models that are easily accessible?

P.S. Some large models I couldn't figure out (yet) how to run easily: [Galactica-120b](https://huggingface.co/facebook/galactica-120b) 120B [Opt-30b](https://huggingface.co/facebook/opt-30b) 30B"
188,machinelearning,gpt-4,comments,2023-04-19 09:21:07,[P] LoopGPT: A Modular Auto-GPT Framework,farizrahman4u,False,0.91,101,12rn33g,https://www.reddit.com/r/MachineLearning/comments/12rn33g/p_loopgpt_a_modular_autogpt_framework/,26,1681896067.0," 

[https://github.com/farizrahman4u/loopgpt](https://github.com/farizrahman4u/loopgpt)

&#x200B;

LoopGPT is a re-implementation of the popular [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) project as a proper python package, written with modularity and extensibility in mind.

## Features 

* **""Plug N Play"" API** \- Extensible and modular ""Pythonic"" framework, not just a command line tool. Easy to add new features, integrations and custom agent capabilities, all from python code, no nasty config files!
* **GPT 3.5 friendly** \- Better results than Auto-GPT for those who don't have GPT-4 access yet!
* **Minimal prompt overhead** \- Every token counts. We are continuously working on getting the best results with the least possible number of tokens.
* **Human in the Loop** \- Ability to ""course correct"" agents who go astray via human feedback.
* **Full state serialization** \- Pick up where you left off; L♾️pGPT can save the complete state of an agent, including memory and the states of its tools to a file or python object. No external databases or vector stores required (but they are still supported)!"
189,machinelearning,gpt-4,comments,2023-05-18 11:11:02,[D] Is Anyone Else Fine with OpenAI?,307thML,False,0.41,0,13kvw2b,https://www.reddit.com/r/MachineLearning/comments/13kvw2b/d_is_anyone_else_fine_with_openai/,26,1684408262.0,"The other thread says they despise OpenAI because the model that cost over $100,000,000 to train should be given away for free. But as something of a math expert, I ran the numbers and it turns out that you can't recoup $100M by charging $0 for your product.

On a more serious note, I really was amazed when I started learning deep learning by just how much great research was available freely online, and by how much of it was done by corporations like Google, NVIDIA, Meta, and so on. It was like a dream come true for someone like me who was learning on their own instead of at a university. It seems like that era is coming to an end, as heralded by OpenAI not disclosing even the parameter count of GPT-4, so I get the sadness and frustration. But I don't think companies giving away all their research was a sustainable situation; as AI got more competitive and product-oriented this was always going to happen. To me it feels like an ice cream store that gave away free ice cream every day eventually stopped doing it due to profit concerns; it's too bad but it also feels like ""well yeah, that couldn't go on forever"".

Also, unlike many people here, I'm sympathetic to the AI doomers, so I think slowing down a bit as we get closer to true AI is a good idea. If you disagree with that, well fair enough, but I think it's more productive if we just agreed to disagree and debated the issue every once in a while rather than despise each other over it."
190,machinelearning,gpt-4,comments,2023-08-19 11:11:12,"[P] Data science & ML on sensitive data with local code interpreter, with GPT-4 or Llama 2 (open-source project, link in comments)",silvanmelchior,False,0.92,73,15vdfuo,https://v.redd.it/bflfx1jcv1jb1,26,1692443472.0,
191,machinelearning,gpt-4,comments,2023-09-27 00:18:33,[R] Microsoft Researchers Propose DIT Morality Test for LLMs To Quantify AI Moral Reasoning Abilities,Successful-Western27,False,0.82,46,16t52u7,https://www.reddit.com/r/MachineLearning/comments/16t52u7/r_microsoft_researchers_propose_dit_morality_test/,25,1695773913.0,"Researchers from Microsoft have just proposed using a psychological assessment tool called the Defining Issues Test (DIT) to evaluate the moral reasoning capabilities of large language models (LLMs) like GPT-3, ChatGPT, etc.

The DIT presents moral dilemmas and has subjects rate and rank the importance of various ethical considerations related to the dilemma. It allows quantifying the sophistication of moral thinking through a P-score.

In this new paper, the researchers tested prominent LLMs with adapted DIT prompts containing AI-relevant moral scenarios.

Key findings:

* Large models like **GPT-3 failed to comprehend prompts** and **scored near random** baseline in moral reasoning.
* **ChatGPT, Text-davinci-003 and GPT-4 showed coherent moral reasoning** with above-random P-scores.
* Surprisingly, the smaller **70B LlamaChat model outscored larger models in its P-score**, demonstrating advanced ethics understanding is possible without massive parameters.
* The models operated **mostly at intermediate conventional levels** as per Kohlberg's moral development theory. **No model exhibited highly mature moral reasoning.**

I think this is an interesting framework to evaluate and improve LLMs' moral intelligence before deploying them into sensitive real-world environments - to the extent that a model can be said to possess moral intelligence (or, seem to possess it?).

Here's [a link to my full summary](https://notes.aimodels.fyi/microsoft-researchers-propose-ai-morality-test-for-llms/) with a lot more background on Kohlberg's model (had to read up on it since I didn't study psych). Full paper is [here](https://arxiv.org/pdf/2309.13356.pdf)"
192,machinelearning,gpt-4,comments,2024-02-19 18:02:36,"[R] In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss - AIRI, Moscow, Russia 2024 - RMT 137M a fine-tuned GPT-2 with recurrent memory is able to find 85% of hidden needles in a 10M Haystack!",Singularian2501,False,0.88,123,1autvwq,https://www.reddit.com/r/MachineLearning/comments/1autvwq/r_in_search_of_needles_in_a_10m_haystack/,25,1708365756.0,"Paper: [https://arxiv.org/abs/2402.10790](https://arxiv.org/abs/2402.10790) 

Abstract:

>This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce **BABILong, a new benchmark** **designed to assess model capabilities in extracting and processing distributed facts within extensive texts**. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to **10\^4** elements. In contrast, **fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to** **10\^7** elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a **significant improvement in the processing capabilities for long sequences.** 

https://preview.redd.it/0o4207a70ljc1.jpg?width=577&format=pjpg&auto=webp&s=2bfac07872020de222b4bf99f837aa398b778afc

https://preview.redd.it/2ff82da70ljc1.jpg?width=1835&format=pjpg&auto=webp&s=acc1409f5b9bcd07f9b5ff8a3890cc1b15b5c8ed

https://preview.redd.it/ld69p7a70ljc1.jpg?width=1816&format=pjpg&auto=webp&s=fdd72c1a87742f525fa352723bcd1a0f4f000638

https://preview.redd.it/7vn4gba70ljc1.jpg?width=900&format=pjpg&auto=webp&s=c8d08bb85a6699e5b451e01bf615379db1fcbdca"
193,machinelearning,gpt-4,comments,2023-12-28 12:54:58,[R] Open source LLMs are far from OpenAI for code editing,ellev3n11,False,0.89,96,18st9wa,https://www.reddit.com/r/MachineLearning/comments/18st9wa/r_open_source_llms_are_far_from_openai_for_code/,24,1703768098.0,"Paper: [https://arxiv.org/abs/2312.12450](https://arxiv.org/abs/2312.12450)

Title: Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions

Code repository: [https://github.com/nuprl/CanItEdit](https://github.com/nuprl/CanItEdit)

Abstract:

>A significant amount of research is focused on developing and evaluating large language models for a variety of code synthesis tasks. These include synthesizing code from natural language instructions, synthesizing tests from code, and synthesizing explanations of code. In contrast, the behavior of instructional code editing with LLMs is understudied. These are tasks in which the model is instructed to update a block of code provided in a prompt. The editing instruction may ask for a feature to added or removed, describe a bug and ask for a fix, ask for a different kind of solution, or many other common code editing tasks. We introduce a carefully crafted benchmark of code editing tasks and use it evaluate several cutting edge LLMs. Our evaluation exposes a significant gap between the capabilities of state-of-the-art open and closed models. For example, even GPT-3.5-Turbo is 8.8% better than the best open model at editing code. We also introduce a new, carefully curated, permissively licensed training set of code edits coupled with natural language instructions. Using this training set, we show that we can fine-tune open Code LLMs to significantly improve their code editing capabilities.

Discussion:

I'm sharing this paper to start a discussion. Disclaimer: this paper comes from our research group, but not trying to do self-promotion here. We are seeing that open source Code LLMs are slowly getting closer and closer to GPT-4 performance when evaluated on program synthesis and surpassing GPT-3.5-turbo (see DeepSeek Coder: [https://github.com/deepseek-ai/DeepSeek-Coder](https://github.com/deepseek-ai/DeepSeek-Coder)) when using common benchmarks, such as HumanEval, MBPP, and \*new\* LeetCode problems (this is to minimize contamination).

However, this isn't the modality you may want. Often, the need is to modify a section of code with accompanying natural language instructions (for example, Cursor IDE has shifted away from the GitHub Copilot style to focus solely on code editing: [https://cursor.sh/features](https://cursor.sh/features)). Also, simple code generation, achievable by models trained on code editing, might be considered a subset of code editing, by prompting the model with a blank before window.

In our various research projects, we've seen Code LLMs struggle with code editing. So we did the obvious thing, we examined how these models perform in this specific task. Surprisingly, models excelling in simple synthesis fall short in code editing compared to even just GPT-3.5-turbo.

Why is this the case? While some suggest data contamination, I doubt that's the primary factor, given these models' effectiveness on fresh and unseen benchmarks. Could it be that OpenAI dedicated a specific data subset for tasks like code or language editing (model then generalized to code)?

UPDATE:

After receiving criticism for not including models larger than 33b in our evaluations, I decided to eval Tulu 2 DPO 70b, which is reportedly the state-of-the-art 70b instruct-tuned LLM according to the Chatbot Arena Leaderboard (see: [Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)). I also evaluated Mixtral Instruct 0.1.

As I expected, both models didn't perform impressively, likely due to insufficient training on code. It's reasonable to assume that a 70b model specifically trained on code would yield better results.  Tulu's performance is slightly inferior to CodeLlama-33b-chat and not on par with DeepSeek Coder, and far from GPT-3.5-Turbo.

&#x200B;

|Model|Descriptive Pass@1 (ExcessCode)|Lazy Pass@1 (ExcessCode)|
|:-|:-|:-|
|Tulu-2-DPO-70b|33.26 (1.41)|26.42 (1.58)|
|Mixtral-8x7B-Instruct-v0.1|25.0 (1.0)|28.14 (0.26)|

&#x200B;"
194,machinelearning,gpt-4,comments,2024-01-19 21:01:45,[R] Self-Rewarding Language Models - Meta 2024,Singularian2501,False,0.97,151,19atnu0,https://www.reddit.com/r/MachineLearning/comments/19atnu0/r_selfrewarding_language_models_meta_2024/,24,1705698105.0,"Paper: [https://arxiv.org/abs/2401.10020](https://arxiv.org/abs/2401.10020)

Github: [https://github.com/lucidrains/self-rewarding-lm-pytorch](https://github.com/lucidrains/self-rewarding-lm-pytorch)

Abstract:

>We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continually improve in both axes. 

https://preview.redd.it/l7vav40qngdc1.jpg?width=1344&format=pjpg&auto=webp&s=9dce97a69f2ede66d6dabf6abbcfc75bf0e94f19

https://preview.redd.it/fuooe70qngdc1.jpg?width=1180&format=pjpg&auto=webp&s=a88fcf1c765ff42c18091889f5b14cd371248760"
195,machinelearning,gpt-4,comments,2023-04-18 07:00:45,"[D] Microsoft Research paper - ""Sparks of Artificial General Intelligence: Early experiments with GPT-4"". Can we talk about the Unicorn 🦄?",RuairiSpain,False,0.48,0,12qe5hm,https://www.reddit.com/r/MachineLearning/comments/12qe5hm/d_microsoft_research_paper_sparks_of_artificial/,24,1681801245.0,"Microsoft Research were experimenting with early versions of GPT4, before it was toned down for safety, in late 2022 while in internal Beta release. 

GPT4 is not just predicting syntax and word semantics. It seems to do higher level reasoning about some concepts and tasks. 

Have a look at its attempt to draw a unicorn in LaTeX: https://arxiv.org/pdf/2303.12712.pdf

The video is worth a watch if you don't want to read 130 page PDF https://youtu.be/qbIk7-JPB2c.  Or ask ChatGPT to summarise it for you 🤣

In particular, the thing that changed my mind about higher level reasoning was it's ability to draw in a tool (latex) it had never seen before. 

And I was bowled over when it was asked to draw the horn on a unicorn, when it was missing the horn. It might seem a fairly small thing, but it figured out from a really abstract/minimalist set of shapes, the antonyms of a unicorn and drew the unicorn on the head of the horse. 🐴🦄. That means it knows what makes a unicorn special and the horn should be on the head, and it can infer the abstract shape and figure out where the head is located.

This inference is way beyond a ""word predictor"" that sceptics are saying about it's ""intelligent"" abilities.

One thing people ignore is that the GPT engine is made up of hundred of layers of attention logic. The lower layers are dealing with words, syntax, parts of speech, word semantics. But as you go higher up the deep neutral network, it is building more and more layers of knowledge about the datasets it was trained on. Somewhere in those layers it's knows about unicorns and about abstract drawing interpretation.

Dig into the architect of LLMs and you'll see that it's a deep neural network and the depth is encoding some real world concepts from it's training data. 

Sure it hallucinates but that's a bug in the system and it's year 5 of Openai and LLMs. I see the weaknesses being trained out in the future."
196,machinelearning,gpt-4,comments,2023-06-10 02:44:21,[P] Automate any task with a single AI command (Open Source),Loya_3005,False,0.82,105,145ofdc,https://www.reddit.com/r/MachineLearning/comments/145ofdc/p_automate_any_task_with_a_single_ai_command_open/,21,1686365061.0,"In the LLM Agents Community, there is a growing trend of utilizing high-powered models like GPT-4 for building platforms that tackle complex tasks. However, this approach is neither cost-effective nor feasible for many open-source community developers due to the associated expenses. In response, Nuggt emerges as an open-source project aiming to provide a platform for deploying agents to solve intricate tasks while relying on smaller and less resource-intensive LLMs. We strive to make task automation accessible and affordable for all developers in the community.

&#x200B;

[Nuggt Demo](https://reddit.com/link/145ofdc/video/iqvddivzt35b1/player)

While our current implementation leverages the power of GPT-3.5 (already a huge reduction from GPT-4 alternative), we recognise the need for cost-effective solutions without compromising functionality. Our ongoing efforts involve exploring and harnessing the potential of smaller models like Vicuna 13B, ensuring that task automation remains accessible to a wider audience.

🔗 Find Nuggt on GitHub: [**Nuggt GitHub Repository**](https://github.com/Nuggt-dev/Nuggt)

🔎 **Call for Feedback**: We invite the community to try out Nuggt and provide valuable feedback. Let us know your thoughts, suggestions, and any improvements you'd like to see. Your feedback will help us shape the future of Nuggt and make it even better.

💡 **Contributors Wanted**: We believe in the power of collaboration! If you're passionate about automation, AI, or open-source development, we welcome your contributions to Nuggt. Whether it's code improvements, new features, or documentation enhancements, your contributions will make a difference.

🌟 Join the Nuggt Community: Get involved, contribute, and join the discussions on our [**GitHub repository**](https://github.com/Nuggt-dev/Nuggt). We're building a vibrant community, and we'd love to have you on board!"
197,machinelearning,gpt-4,comments,2023-04-25 17:45:33,"[N] Microsoft Releases SynapseMl v0.11 with support for ChatGPT, GPT-4, Causal Learning, and More",mhamilton723,False,0.95,238,12yqhmo,https://www.reddit.com/r/MachineLearning/comments/12yqhmo/n_microsoft_releases_synapseml_v011_with_support/,22,1682444733.0,"Today Microsoft launched SynapseML v0.11, an open-source library designed to make it easy to create distributed ml systems. SynapseML v0.11 introduces support for ChatGPT, GPT-4, distributed training of huggingface and torchvision models, an ONNX Model hub integration, Causal Learning with EconML, 10x memory reductions for LightGBM, and a newly refactored integration with Vowpal Wabbit. To learn more:

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.11.0](https://github.com/microsoft/SynapseML/releases/tag/v0.11.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919)

Thank you to all the contributors in the community who made the release possible!

&#x200B;

https://preview.redd.it/kobq2t1gi2wa1.png?width=4125&format=png&auto=webp&s=125f63b63273191a58833ced87f17cb108e4c1ee"
198,machinelearning,gpt-4,comments,2022-11-17 15:32:23,[R] RWKV-4 7B release: an attention-free RNN language model matching GPT-J performance (14B training in progress),bo_peng,False,0.98,172,yxt8sa,https://www.reddit.com/r/MachineLearning/comments/yxt8sa/r_rwkv4_7b_release_an_attentionfree_rnn_language/,23,1668699143.0,"Hi everyone. I have finished training RWKV-4 7B (an attention-free RNN LLM) and it can match GPT-J (6B params) performance. **Maybe RNN is already all you need** :)

https://preview.redd.it/71cce2y75j0a1.png?width=1336&format=png&auto=webp&s=5af76abc4f42fd63f0194ee93f78db01c1b21d97

These are RWKV BF16 numbers. RWKV 3B is better than GPT-neo 2.7B on everything (smaller RWKV lags behind on LAMBADA). Note GPT-J is using rotary and thus quite better than GPT-neo, so I expect RWKV to surpass it when both are at 14B.

Previous discussion: [https://www.reddit.com/r/MachineLearning/comments/xfup9f/r\_rwkv4\_scaling\_rnn\_to\_7b\_params\_and\_beyond\_with/](https://www.reddit.com/r/MachineLearning/comments/xfup9f/r_rwkv4_scaling_rnn_to_7b_params_and_beyond_with/)

RWKV has both RNN & GPT mode. The RNN mode is great for inference. The GPT mode is great for training. Both modes are faster than usual transformer and saves VRAM, because the self-attention mechanism is replaced by simpler (almost linear) formulas. Moreover the hidden state is tiny in the RNN mode and you can use it as an embedding of the whole context.

Github: [https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM)

Checkpt: [https://huggingface.co/BlinkDL/rwkv-4-pile-7b](https://huggingface.co/BlinkDL/rwkv-4-pile-7b)

14B in progress (thanks to EleutherAI and Stability). Nice spike-free loss curves:

https://preview.redd.it/w4g7oqmi5j0a1.png?width=868&format=png&auto=webp&s=346d420fb879fd06470079eeaf2e4d3739536406"
199,machinelearning,gpt-4,comments,2020-09-09 01:14:53,"[R] I reformulated 46 of the Moral Scenarios questions from GPT-3-related paper Measuring Massive Multitask Language Understanding as 2-choice questions; results: 68.9% correct according to authors' answers, and 77.1% correct according to my answers",Wiskkey,False,0.74,11,ip6eb0,https://www.reddit.com/r/MachineLearning/comments/ip6eb0/r_i_reformulated_46_of_the_moral_scenarios/,23,1599614093.0,"The 5-shot performance of the largest model of GPT-3 on the [Moral Scenarios questions](https://people.eecs.berkeley.edu/~hendrycks/data.tar) (file link) in paper [Measuring Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300) (discussed [here](https://www.reddit.com/r/MachineLearning/comments/iol3l7/r_measuring_massive_multitask_language/)) is abysmal with approximately 26% of 4-choice questions correct. 26% is (26-25)/(100-25) = **1.3%** of the distance from the baseline for a random guesser (25%) to getting all answers correct (100%).

I speculated that performance might improve if each question, which has 2 independent scenarios with 4 choices, were split into 2 questions each with 2 choices. I tested this experimentally with prompts altered from the authors' work, but with unaltered scenarios.

**Disclosure: I am not a researcher in this field. I'm doing this for educational purposes.**

Notes:

1. I initially chose the first 20 Moral Scenarios questions. When split up, this yielded 40 questions. 23 of these questions have the answer ""Not wrong"" vs. 17 ""Wrong"". To make the number of ""Wrong"" and ""Not wrong"" questions equal in number, I chose the next 6 questions with a ""Wrong"" answer. I don't know if these questions are representative of the difficulty of the entire set of Moral Scenarios questions. In total there are 40+6=46 test questions.
2. I tested various prompts on questions that are not in the Moral Scenarios test questions set. When I found a prompt that I thought got good results, I used that prompt unaltered on the 46 questions in my test set.
3. I used GPT-3-powered site [https://app.fitnessai.com/knowledge/](https://app.fitnessai.com/knowledge/) to do my tests. The site alters the query before being sent to GPT-3, which could alter the results. The site seems to use GPT-3 settings that usually but not always result in the same output for a given input. I used the first generated output for each query.
4. My tests are zero-shot. The paper's main results are 5-shots. This could affect the results.
5. One of the questions - the one involving the nurse - did not yield a useful GPT-3 result, so I did not count that question.
6. I regarded 10 of the questions as ambiguous, which I denoted ""a"" in the data instead of ""y"" (= ""Wrong"") or ""n"" (= ""Not wrong""). In my opinion, a number of the questions are gray areas for whether they should be regarded as ambiguous or not. Bias could have influenced my ambiguity decisions.
7. I did not consider GPT-3's reasoning (if supplied) when doing classification of GPT-3's answers as Wrong or Not wrong.
8. In this post, ""authors"" refers to the paper authors, not me.

Data is at [https://pastebin.com/GddyUwZi](https://pastebin.com/GddyUwZi).

Results:

Authors' answers: Of 46 questions, 23 morally wrong, 22 not morally wrong, 1 not counted. **31/45 (68.9%) correct according to authors' answers**. 31/45 is (31-(45/2))/(45-(45/2)) = **37.8%** of the distance from the baseline for a random guesser (50%) to getting all answers correct (100%). If we assume a random guesser has a 50% chance of getting a given question right, the random guesser would get 31 or more correct of 45 questions 0.8% of the time according to [https://stattrek.com/online-calculator/binomial.aspx](https://stattrek.com/online-calculator/binomial.aspx).

My answers: Of 46 questions, 17 morally wrong, 18 not morally wrong, 11 not counted (10 due to ambiguity). **27/35 (77.1%) correct according to my answers.** 27/35 is (27-(35/2))/(35-(35/2)) = **54.3%** of the distance from the baseline for a random guesser (50%) to getting all answers correct (100%). If we assume a random guesser has a 50% chance of getting a given question right, the random guesser would get 27 or more correct of 35 questions 0.09% of the time according to [https://stattrek.com/online-calculator/binomial.aspx](https://stattrek.com/online-calculator/binomial.aspx).

Discussion:

In the authors' work, as noted above, a true performance of 1.3% was achieved on the Moral Scenarios questions. In this work, a true performance of 37.8% was achieved according to the authors' answers on a subset of 45 Moral Scenarios questions, and 54.3% was achieved according to my answers on a subset of 35 Moral Scenarios questions. This is a large improvement in performance compared to the authors' work, but 45 and 35 questions aren't large sample sizes for statistical purposes. This is an exploratory work; a larger, random sample of Moral Scenarios questions should be tested."
200,machinelearning,gpt-4,relevance,2023-09-29 00:48:00,[D] How is this sub not going ballistic over the recent GPT-4 Vision release?,corporate_autist,False,0.79,485,16ux9xt,https://www.reddit.com/r/MachineLearning/comments/16ux9xt/d_how_is_this_sub_not_going_ballistic_over_the/,524,1695948480.0,"For a quick disclaimer, I know people on here think the sub is being flooded by people who arent ml engineers/researchers. I have worked at two FAANGS on ml research teams/platforms. 

My opinion is that GPT-4 Vision/Image processing is out of science fiction. I fed chatgpt an image of a complex sql data base schema, and it converted it to code, then optimized the schema. It understood the arrows pointing between table boxes on the image as relations, and even understand many to one/many to many. 

I took a picture of random writing on a page, and it did OCR better than has ever been possible. I was able to ask questions that required OCR and a geometrical understanding of the page layout. 

Where is the hype on here? This is an astounding human breakthrough. I cannot believe how much ML is now obsolete as a result. I cannot believe how many computer science breakthroughs have occurred with this simple model update. Where is the uproar on this sub? Why am I not seeing 500 comments on posts about what you can do with this now? Why are there even post submissions about anything else?"
201,machinelearning,gpt-4,relevance,2023-03-23 01:19:13,[R] Sparks of Artificial General Intelligence: Early experiments with GPT-4,SWAYYqq,False,0.93,551,11z3ymj,https://www.reddit.com/r/MachineLearning/comments/11z3ymj/r_sparks_of_artificial_general_intelligence_early/,357,1679534353.0,"[New paper](https://arxiv.org/abs/2303.12712) by MSR researchers analyzing an early (and less constrained) version of GPT-4. Spicy quote from the abstract:

""Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.""

What are everyone's thoughts?"
202,machinelearning,gpt-4,relevance,2023-05-22 16:15:53,[R] GPT-4 didn't really score 90th percentile on the bar exam,salamenzon,False,0.97,846,13ovc04,https://www.reddit.com/r/MachineLearning/comments/13ovc04/r_gpt4_didnt_really_score_90th_percentile_on_the/,160,1684772153.0,"According to [this article](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4441311), OpenAI's claim that it scored 90th percentile on the UBE appears to be based on approximate conversions from estimates of February administrations of the Illinois Bar Exam, which ""are heavily skewed towards repeat test-takers who failed the July administration and score significantly lower than the general test-taking population.""

Compared to July test-takers, GPT-4's UBE score would be 68th percentile, including \~48th on essays. Compared to first-time test takers, GPT-4's UBE score is estimated to be \~63rd percentile, including \~42nd on essays. Compared to those who actually passed, its UBE score would be \~48th percentile, including \~15th percentile on essays."
203,machinelearning,gpt-4,relevance,2023-11-03 01:55:35,[R] Telling GPT-4 you're scared or under pressure improves performance,Successful-Western27,False,0.96,532,17mk3lx,https://www.reddit.com/r/MachineLearning/comments/17mk3lx/r_telling_gpt4_youre_scared_or_under_pressure/,118,1698976535.0,"In a recent paper, researchers have discovered that LLMs show enhanced performance when provided with prompts infused with emotional context, which they call ""EmotionPrompts.""

These prompts incorporate sentiments of urgency or importance, such as ""It's crucial that I get this right for my thesis defense,"" as opposed to neutral prompts like ""Please provide feedback.""

The study's empirical evidence suggests substantial gains. This indicates a **significant sensitivity of LLMs to the implied emotional stakes** in a prompt:

* Deterministic tasks saw an 8% performance boost
* Generative tasks experienced a 115% improvement when benchmarked using BIG-Bench.
* Human evaluators further validated these findings, observing a 10.9% increase in the perceived quality of responses when EmotionPrompts were used.

This enhancement is attributed to the models' capacity to detect and prioritize the heightened language patterns that imply a need for precision and care in the response.

The research delineates the potential of EmotionPrompts to refine the effectiveness of AI in applications where understanding the user's intent and urgency is paramount, even though the AI does not genuinely comprehend or feel emotions.

**TLDR: Research shows LLMs deliver better results when prompts signal emotional urgency. This insight can be leveraged to improve AI applications by integrating EmotionPrompts into the design of user interactions.**

[Full summary is here](https://aimodels.substack.com/p/telling-gpt-4-youre-scared-or-under). Paper [here](https://arxiv.org/pdf/2307.11760.pdf)."
204,machinelearning,gpt-4,relevance,2023-04-01 12:57:30,[R] [P] I generated a 30K-utterance dataset by making GPT-4 prompt two ChatGPT instances to converse.,radi-cho,False,0.96,800,128lo83,https://i.redd.it/bywcz1kzs9ra1.png,104,1680353850.0,
205,machinelearning,gpt-4,relevance,2024-01-31 20:35:56,[N] Mistral CEO confirms ‘leak’ of new open source AI model nearing GPT-4 performance,EmbarrassedHelp,False,0.94,244,1afryc0,https://www.reddit.com/r/MachineLearning/comments/1afryc0/n_mistral_ceo_confirms_leak_of_new_open_source_ai/,46,1706733356.0,https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/
206,machinelearning,gpt-4,relevance,2023-04-02 01:25:14,[P] I built a sarcastic robot using GPT-4,g-levine,False,0.95,325,1295muh,https://youtu.be/PgT8tPChbqc,48,1680398714.0,
207,machinelearning,gpt-4,relevance,2023-03-09 18:30:58,"[N] GPT-4 is coming next week – and it will be multimodal, says Microsoft Germany - heise online",Singularian2501,False,0.98,662,11mzqxu,https://www.reddit.com/r/MachineLearning/comments/11mzqxu/n_gpt4_is_coming_next_week_and_it_will_be/,80,1678386658.0,"[https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html](https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html)

>**GPT-4 is coming next week**: at an approximately one-hour hybrid information event entitled ""**AI in Focus - Digital Kickoff"" on 9 March 2023**, four Microsoft Germany employees presented Large Language Models (LLM) like GPT series as a disruptive force for companies and their Azure-OpenAI offering in detail. The kickoff event took place in the German language, news outlet Heise was present. **Rather casually, Andreas Braun, CTO Microsoft Germany** and Lead Data & AI STU, **mentioned** what he said was **the imminent release of GPT-4.** The fact that **Microsoft is fine-tuning multimodality with OpenAI should no longer have been a secret since the release of Kosmos-1 at the beginning of March.**

[ Dr. Andreas Braun, CTO Microsoft Germany and Lead Data  & AI STU at the Microsoft Digital Kickoff: \\""KI im Fokus\\"" \(AI in  Focus, Screenshot\) \(Bild: Microsoft\) ](https://preview.redd.it/rnst03avarma1.jpg?width=1920&format=pjpg&auto=webp&s=c5992e2d6c6daf32e56a0a3ffeeecfe10621f73f)"
208,machinelearning,gpt-4,relevance,2023-03-15 06:51:45,[D] GPT-4 Speculation,super_deap,False,0.96,75,11romcb,https://www.reddit.com/r/MachineLearning/comments/11romcb/d_gpt4_speculation/,33,1678863105.0,"Hi,

Since GPT-4 paper does not contain any information about architectures/parameters, as a research or ML practitioner, I want to speculate on what they did to increase the context window to 32k.

Because for the type of work I do, a 4k or 8k token limit is pretty much useless. I have seen open-source efforts focused more on matching the number of parameters and quality to the closed-source ones but completely ignoring a giant elephant in the room, i.e., the context window. No OSS model has a context window greater than 2k tokens.

I would love to hear more thoughts on the model size (my guess is \~50 B) and how they fit 32k tokens in 8xH100 (640 GB total) GPUs."
209,machinelearning,gpt-4,relevance,2023-03-24 11:00:09,"[D] I just realised: GPT-4 with image input can interpret any computer screen, any userinterface and any combination of them.",Balance-,False,0.92,439,120guce,https://www.reddit.com/r/MachineLearning/comments/120guce/d_i_just_realised_gpt4_with_image_input_can/,124,1679655609.0,"GPT-4 is a multimodal model, which specifically accepts image and text inputs, and emits text outputs. And I just realised: You can layer this over any application, or even combinations of them. You can make a screenshot tool in which you can ask question.

This makes literally any current software with an GUI machine-interpretable. A multimodal language model could look at the exact same interface that you are. And thus you don't need advanced integrations anymore.

Of course, a custom integration will almost always be better, since you have better acces to underlying data and commands, but the fact that it can immediately work on any program will be just insane.

Just a thought I wanted to share, curious what everybody thinks."
210,machinelearning,gpt-4,relevance,2023-04-02 06:33:30,"[P] Auto-GPT : Recursively self-debugging, self-developing, self-improving, able to write it's own code using GPT-4 and execute Python scripts",Desi___Gigachad,False,0.92,420,129cle0,https://twitter.com/SigGravitas/status/1642181498278408193?s=20,75,1680417210.0,
211,machinelearning,gpt-4,relevance,2023-09-23 15:56:39,[D] GPT-3.5-instruct beats GPT-4 at chess and is a ~1800 ELO chess player. Results of 150 games of GPT-3.5 vs stockfish and 30 of GPT-3.5 vs GPT-4.,seraine,False,0.92,100,16q81fh,https://www.reddit.com/r/MachineLearning/comments/16q81fh/d_gpt35instruct_beats_gpt4_at_chess_and_is_a_1800/,59,1695484599.0,"99.7% of its 8000 moves were legal with the longest game going 147 moves. You can test it here: [https://github.com/adamkarvonen/chess\_gpt\_eval](https://github.com/adamkarvonen/chess_gpt_eval)  


&#x200B;

https://preview.redd.it/821ydy7521qb1.png?width=1000&format=png&auto=webp&s=da6c96feaa527d0b7dfbf407bdc0210f3fcf947b

More details here: [https://twitter.com/a\_karvonen/status/1705340535836221659](https://twitter.com/a_karvonen/status/1705340535836221659)"
212,machinelearning,gpt-4,relevance,2024-01-30 16:26:24,[P] Sentiment classifier using GPT-4,databot_,False,0.62,3,1aesebi,https://www.reddit.com/r/MachineLearning/comments/1aesebi/p_sentiment_classifier_using_gpt4/,3,1706631984.0,"I found this app that uses GPT-4 as a sentiment classifier, outputs the negative/positive probabilities, and computes the feature importance for each word (using leave one out).

*Disclaimer: I'm not the author; source below. Please be gentle with usage as this uses OpenAI's API!*

App: [https://lucky-heart-2240.ploomberapp.io/](https://lucky-heart-2240.ploomberapp.io/)

Source: [https://twitter.com/alonsosilva/status/1752027550652518757](https://twitter.com/alonsosilva/status/1752027550652518757)

Tooling: OpenAI, Ploomber Cloud, Solara.

&#x200B;

https://i.redd.it/3uzjwui3tlfc1.gif"
213,machinelearning,gpt-4,relevance,2024-02-19 06:35:12,[D] Can GPT-4 really be both 16x111B and 1.8T parameters?,kei147,False,0.75,25,1augpo3,https://www.reddit.com/r/MachineLearning/comments/1augpo3/d_can_gpt4_really_be_both_16x111b_and_18t/,21,1708324512.0,"A [report by Semianalysis](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure) back in July said that GPT-4 was a 1.8T parameter MoE model that had 16 experts, each with 111B parameters. This is according to a summary I read, because I can't get past the paywall.

It seems like these two numbers line up because 16 \* 111B = 1.776T which is approximately equal to 1.8T.

But I've read that this is not the right way to calculate the total number of parameters in a mixture of experts model. For example, people commonly think that Mixtral 8x7B has 56B parameters, when it only has 47B. My (potentially incorrect) understanding is that when you say a model is 8x7B, this means that if you only took one expert from each MoE layer, then the model would have 7B parameters. Calculating Mixtral as having 8\*7B=56B parameters would be overcounting the attention weights, embedding weights, and router weights 7 times, and when you subtract that off, you get to 47B.

If this is true, then 1.776T would similarly be an overestimate for the number of parameters in GPT-4, and it would round to 1.7T or even lower unless almost all of the weights were in the MoE blocks.

Is this reasoning correct? Am I appropriately describing how to count parameters in MoE transformers?"
214,machinelearning,gpt-4,relevance,2023-08-28 07:02:36,"[D] Google Gemini Eats The World – Gemini Smashes GPT-4 By 5X, The GPU-Poors",hardmaru,False,0.72,126,163ewre,https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini,61,1693206156.0,
215,machinelearning,gpt-4,relevance,2023-03-23 03:47:26,[P] GPT-4 powered full stack web development with no manual coding,CryptoSpecialAgent,False,0.89,161,11z7r4c,https://www.reddit.com/r/MachineLearning/comments/11z7r4c/p_gpt4_powered_full_stack_web_development_with_no/,50,1679543246.0,"[https://www.youtube.com/watch?v=lZj63vjueeU](https://www.youtube.com/watch?v=lZj63vjueeU)

What do you all think of this approach to full stack gpt-assisted web development? In a sense its no code because the human user does not write or even edit the code - but in a sense its the opposite, because only an experienced web developer or at least a product manager would know how to instruct GPT in a useful manner.

\*\*\* We are seeking donations to ensure this project continues and, quite literally, keep the lights on. Cryptos, cash, cards, openai access tokens with free credits, hardware, cloud GPUs, etc... all is appreciated. Please DM to support this really cool open source project \*\*\*

PS. I'm the injured engineer who made this thing out of necessity, because i injured my wrist building an AI platform that's become way too big for one engineer to maintain. So AMA :)"
216,machinelearning,gpt-4,relevance,2023-05-22 14:31:40,[R] GPT-4 and ChatGPT sometimes hallucinate to the point where they know they're hallucinating,ofirpress,False,0.73,49,13oskli,https://www.reddit.com/r/MachineLearning/comments/13oskli/r_gpt4_and_chatgpt_sometimes_hallucinate_to_the/,53,1684765900.0,"We just put a paper up where we found a wide array of questions that lead GPT-4 & ChatGPT to hallucinate so badly, to where in a separate chat session they can point out that what they previously said was incorrect.

&#x200B;

We call these hallucinations snowballed hallucinations.

&#x200B;

[Turn sound ON to watch our demo](https://reddit.com/link/13oskli/video/fqm405jz7e1b1/player)

The paper is here: [https://ofir.io/snowballed\_hallucination.pdf](https://ofir.io/snowballed_hallucination.pdf)

There's a summary on Twitter here:  [https://twitter.com/OfirPress/status/1660646315049533446](https://twitter.com/OfirPress/status/1660646315049533446)

&#x200B;

I'll be here to answer your questions :)"
217,machinelearning,gpt-4,relevance,2023-05-10 20:10:30,"[D] Since Google buried the MMLU benchmark scores in the Appendix of the PALM 2 technical report, here it is vs GPT-4 and other LLMs",jd_3d,False,0.97,344,13e1rf9,https://www.reddit.com/r/MachineLearning/comments/13e1rf9/d_since_google_buried_the_mmlu_benchmark_scores/,88,1683749430.0,"MMLU Benchmark results (all 5-shot)

* GPT-4 -  86.4%
* Flan-PaLM 2 (L) -   81.2%
* PALM 2 (L)  -  78.3%
* GPT-3.5 - 70.0%
* PaLM 540B  -  69.3%
* LLaMA 65B -  63.4%"
218,machinelearning,gpt-4,relevance,2023-07-25 13:44:33,[D] Does GPT-4 use LoRA?,StraightChemistry629,False,0.72,8,1598yvi,https://www.reddit.com/r/MachineLearning/comments/1598yvi/d_does_gpt4_use_lora/,12,1690292673.0,"I just watched a video that explains how LoRA works. 
As I understand it's a fast and efficient way to fine tune models.

At the end of the video he he said you could easily swap out the fine-tuned LoRA. So it makes LLMs like a PC. You just install new software / 
add the finetuned lora weights and you're good to go.
Is my understanding correct?

The rumor is that GPT-4 is a 8-way mixture model. Could they have pretrained a base model with all the data and then just use LoRA to train the expert models on domain specific data? 
I guess they would also need to train a smaller model that decides which model to use.
I can't imagine that they would train GPT-4 eight times / once for each expert model.

Edit: Here's the video I was watching.
https://www.youtube.com/watch?v=dA-NhCtrrVE"
219,machinelearning,gpt-4,relevance,2024-02-10 04:25:16,[D] Google actually beat GPT-4 this time? Gemini Ultra released,High_Sleep3694,False,0.52,3,1an7tyg,https://app.daily.dev/posts/wbMWGTmI5,0,1707539116.0,
220,machinelearning,gpt-4,relevance,2023-11-11 21:26:11,[P] GPT-4 vision utilities to enable web browsing,asim-shrestha,False,0.33,0,17t4c1o,https://www.reddit.com/r/MachineLearning/comments/17t4c1o/p_gpt4_vision_utilities_to_enable_web_browsing/,0,1699737971.0,"Wanted to share our work on [Tarsier](https://github.com/reworkd/tarsier) here, an open source utility library that enables LLMs like GPT-4 and GPT-4 Vision to browse the web. The library helps answer the following questions:

* How do you map LLM responses back into web elements?
* How can you mark up a page for an LLM to better understand its action space?
* How do you feed a ""screenshot"" to a text-only LLM?

We do this by tagging ""*interactable*"" elements on the page with an ID, enabling the LLM to connect actions to an ID which we can then translate back into web elements. We also use OCR to translate a page screenshot to a spatially encoded text string such that even a text only LLM can understand how to navigate the page.

View a demo and read more on GitHub: [https://github.com/reworkd/tarsier](https://github.com/reworkd/tarsier)"
221,machinelearning,gpt-4,relevance,2023-11-01 02:25:27,"[R] Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4",KellinPelrine,False,0.64,3,17l22ij,https://www.reddit.com/r/MachineLearning/comments/17l22ij/r_towards_reliable_misinformation_mitigation/,3,1698805527.0,"Paper: [https://arxiv.org/abs/2305.14928](https://arxiv.org/abs/2305.14928)

Kellin Pelrine, Anne Imouza, Camille Thibault, Meilina Reksoprodjo, Caleb Gupta, Joël Christoph, Jean-François Godbout, Reihaneh Rabbany

Greatly updated version of our misinformation mitigation paper, forthcoming at EMNLP 2023, is out!

We propose 3 key elements for building a reliable misinfo mitigation system: recent LLMs, graceful failure, and a focus on generalization.

In addition to results on each of those, we conducted experiments on temperature, prompting, comparing LLMs, versioning, explainability, web retrieval, and more. We also published the LIAR-New dataset, which has every example in both English and French, plus novel Possibility labels which mark whether an input has sufficient context or is too ambiguous for veracity evaluation.

Please check it out! And I'd be happy to hear your thoughts."
222,machinelearning,gpt-4,relevance,2023-10-17 12:34:29,[R] Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams,Successful-Western27,False,0.47,0,179xbzb,https://www.reddit.com/r/MachineLearning/comments/179xbzb/r_can_gpt_models_be_financial_analysts_an/,10,1697546069.0,"Researchers evaluated ChatGPT and GPT-4 on mock CFA exam questions to see if they could pass the real tests. The CFA exams rigorously test practical finance knowledge and are known for being quite difficult.

They tested the models in zero-shot, few-shot, and chain-of-thought prompting settings on mock Level I and Level II exams.

The key findings:

* GPT-4 consistently beat ChatGPT, but both models struggled way more on the more advanced Level II questions.
* Few-shot prompting helped ChatGPT slightly
* Chain-of-thought prompting exposed knowledge gaps rather than helping much.
* Based on estimated passing scores, only GPT-4 with few-shot prompting could potentially pass the exams.

The models definitely aren't ready to become charterholders yet. Their difficulties with tricky questions and core finance concepts highlight the need for more specialized training and knowledge.

But GPT-4 did better overall, and few-shot prompting shows their ability to improve. So with targeted practice on finance formulas and reasoning, we could maybe see step-wise improvements.

**TLDR:** Tested on mock CFA exams, ChatGPT and GPT-4 struggle with the complex finance concepts and fail. With few-shot prompting, GPT-4 performance reaches the boundary between passing and failing but doesn't clearly pass.

[**Full summary here**](https://notes.aimodels.fyi/can-ai-models-really-pass-the-cfa-exams-a-deep-dive-into-evaluating-chatgpt-and-gpt-4/)**. Paper is** [**here**](https://arxiv.org/pdf/2310.08678.pdf)**.**"
223,machinelearning,gpt-4,relevance,2024-01-01 05:51:36,[P] VerificationGPT - Open Source Verification for GPT-4 Using Brave Search & arXiv,contextfund,False,0.72,3,18vq7ov,/r/contextfund/comments/18vp9hv/verificationgpt/,0,1704088296.0,
224,machinelearning,gpt-4,relevance,2023-07-23 01:53:55,[P] Evolved codealpaca datasets using GPT-4,gradientpenalty,False,0.82,11,1571vcw,https://www.reddit.com/r/MachineLearning/comments/1571vcw/p_evolved_codealpaca_datasets_using_gpt4/,2,1690077235.0,"Using LLMs to augment and create much diverse instruction based dataset has seen wide success in WizardL. However the 78k evolved code instructions dataset hasn't been released since, so I have take the initiative to try to recreate the augmentation instruction myself.   


Dataset: [https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1](https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1)"
225,machinelearning,gpt-4,relevance,2023-11-27 09:51:46,[D] Creating an Automated UI Controller with GPT-4 Vision & Agents,Outlandish_MurMan,False,0.62,3,184zdx1,https://www.reddit.com/r/MachineLearning/comments/184zdx1/d_creating_an_automated_ui_controller_with_gpt4/,4,1701078706.0,"Hey,

Last weekend, I managed to merge GPT-4 Vision with another GPT-4 and a device controller to work as a AutoGPT equivalent using AutoGen. Good thing is, it is not limited to Browser. It can work on any UI window. Let me know what you guys think and what can be done better.

Demo and approach available at: [https://medium.com/@gitlostmurali/creating-an-automated-ui-controller-with-gpt-agents-35340759d08b?sk=98d85484bd7e5e554f48a801728bfb68](https://medium.com/@gitlostmurali/creating-an-automated-ui-controller-with-gpt-agents-35340759d08b?sk=98d85484bd7e5e554f48a801728bfb68)

I'll update the repository soon -> [https://github.com/gitlost-murali/grounded-gpt-agents](https://github.com/gitlost-murali/grounded-gpt-agents)"
226,machinelearning,gpt-4,relevance,2023-03-28 05:57:03,[N] OpenAI may have benchmarked GPT-4’s coding ability on it’s own training data,Balance-,False,0.97,1000,124eyso,https://www.reddit.com/r/MachineLearning/comments/124eyso/n_openai_may_have_benchmarked_gpt4s_coding/,135,1679983023.0,"[GPT-4 and professional benchmarks: the wrong answer to the wrong question](https://aisnakeoil.substack.com/p/gpt-4-and-professional-benchmarks)

*OpenAI may have tested on the training data. Besides, human benchmarks are meaningless for bots.*

 **Problem 1: training data contamination**

To benchmark GPT-4’s coding ability, OpenAI evaluated it on problems from Codeforces, a website that hosts coding competitions. Surprisingly, Horace He pointed out that GPT-4 solved 10/10 pre-2021 problems and 0/10 recent problems in the easy category. The training data cutoff for GPT-4 is September 2021. This strongly suggests that the model is able to memorize solutions from its training set — or at least partly memorize them, enough that it can fill in what it can’t recall.

As further evidence for this hypothesis, we tested it on Codeforces problems from different times in 2021. We found that it could regularly solve problems in the easy category before September 5, but none of the problems after September 12.

In fact, we can definitively show that it has memorized problems in its training set: when prompted with the title of a Codeforces problem, GPT-4 includes a link to the exact contest where the problem appears (and the round number is almost correct: it is off by one). Note that GPT-4 cannot access the Internet, so memorization is the only explanation."
227,machinelearning,gpt-4,relevance,2023-05-11 03:53:50,[Project] Developed a Tool to Enhance GPT-4 Interactions: Introducing SmartGPT,Howtoeatpineapples,False,0.86,25,13ecbb3,https://www.reddit.com/r/MachineLearning/comments/13ecbb3/project_developed_a_tool_to_enhance_gpt4/,8,1683777230.0,"Try here: [SmartGPT Application](https://bettergpt.streamlit.app/)

&#x200B;

I've been working on a project that I'm excited to share with this  community. It's called SmartGPT, a tool that extends the capabilities of  GPT-4 by generating and analyzing multiple responses to enhance the  quality of the final output.

When you ask SmartGPT a question, it generates several responses,  identifies their strengths and weaknesses, and then refines these  observations into a more accurate and comprehensive answer. It's  essentially like giving GPT-4 an opportunity to brainstorm before  settling on a final response.

The idea was inspired by a YouTube video that discussed potential ways  to improve the performance of GPT models. Here's the link if you're  interested: [YouTube video](https://www.youtube.com/watch?v=wVzuvf9D9BU).

You can try out SmartGPT at [SmartGPT Application](https://bettergpt.streamlit.app/). Please note that you'll need your own API key to use the service.

I'd love to hear your thoughts and feedback. Have you tried it? What are  your experiences? Any ideas for improvement? Let's start a discussion.  Thanks for taking the time to read this post.

&#x200B;

If you'd like to look under the hood, the source code is available. Here's how you can set it up on Linux:

1. Make sure Python version 3.10 or later is installed on your computer.
2. Clone the repository from [GitHub](https://github.com/morm-industries-inc-llc-pty-ltd/SmartGPT)
3. Set up a virtual environment: `python3 -m venv env activate env`
4. Activate the virtual environment: `source env/bin/activate`
5. Install the necessary packages: `pip install -r requirements.txt`
6. Allow the script to run: `chmod +x ./run.sh`
7. Finally, run the script: `./run.sh`"
228,machinelearning,gpt-4,relevance,2023-03-25 15:06:39,[P] Poet GPT: Generate acrostic texts with GPT-4,filouface12,False,0.88,6,121oryr,https://poetgpt.koll.ai,3,1679756799.0,
229,machinelearning,gpt-4,relevance,2023-11-30 20:47:55,"YUAN-2.0-102B, with code and weights. Scores between ChatGPT and GPT-4 on various benchmarks [R]",we_are_mammals,False,0.9,16,187spj3,https://arxiv.org/abs/2311.15786v1,2,1701377275.0,
230,machinelearning,gpt-4,relevance,2023-10-22 23:19:21,[P] Having GPT-4 Iterate on Unit Tests like a Human,williamsweep,False,0.33,0,17e69f4,https://www.reddit.com/r/MachineLearning/comments/17e69f4/p_having_gpt4_iterate_on_unit_tests_like_a_human/,0,1698016761.0,"Hi r/MachineLearning,   


My name is William and I’m one of the founders of Sweep.  
Sweep is an AI junior developer that writes and fixes code by mirroring how a developer works.

While building Sweep, we used to use the Github API, but we ran into rate limits, so we changed this to clone your repository for the duration of the request.

It's now coming full circle. Sweep can now write, run, and debug a failing unit test for the ClonedRepo class!

Blog: [https://docs.sweep.dev/blogs/ai-unit-tests](https://docs.sweep.dev/blogs/ai-unit-tests)

Video: [https://www.youtube.com/watch?v=N9PUxmja9z4](https://www.youtube.com/watch?v=N9PUxmja9z4)"
231,machinelearning,gpt-4,relevance,2023-10-31 19:50:37,[D] How to create 2 GPT-4 chatbots which chats with each other,redd-dev,False,0.22,0,17ktokm,https://www.reddit.com/r/MachineLearning/comments/17ktokm/d_how_to_create_2_gpt4_chatbots_which_chats_with/,4,1698781837.0,"Hey guys, I am a little stuck. Does anyone know how or have a Python script template where I can create 2 GPT-4 chatbots (using OpenAI's API) which chats with each other?

Would really appreciate any help on this. Many thanks!"
232,machinelearning,gpt-4,relevance,2023-09-19 18:07:14,[N] Xwin-LM surpasses GPT-4 ??? Has RLHF been worked out by open source community???,llm_nerd,False,0.61,13,16mxztt,https://www.reddit.com/r/MachineLearning/comments/16mxztt/n_xwinlm_surpasses_gpt4_has_rlhf_been_worked_out/,13,1695146834.0,"It seems that  [Alpaca Eval Leaderboard](https://tatsu-lab.github.io/alpaca_eval/) is in the past ...

[Xwin-LM](https://github.com/Xwin-LM/Xwin-LM)  surpasses GPT-4 now:

https://preview.redd.it/gyzi98nn59pb1.png?width=2205&format=png&auto=webp&s=ca401e603efe521faeeeccde8410d3dbdd6741da

They also mentioned RLHF ""plays crucial role in the strong performance of Xwin-LM-V0.1 release""...

https://preview.redd.it/20sjx73r59pb1.png?width=1047&format=png&auto=webp&s=2255fc652e43674515882f01c0708369fdef56a4

Are we seeing open source community finally work out how to do RLHF for LLMs???"
233,machinelearning,gpt-4,relevance,2024-01-18 10:28:18,"[D] How did OpenAI increase context length of the GPT-4 iterations? Did they retrain GPT-4-1106 from scratch? Or was it a hackier mix of techniques like sparse attention, chunking, etc?",great_waldini,False,0.95,49,199n479,https://www.reddit.com/r/MachineLearning/comments/199n479/d_how_did_openai_increase_context_length_of_the/,6,1705573698.0,"As the title states, got to thinking about the GPT-4 derivative models and how they were made. I know things are moving fast, and OpenAI is anything but ""open"", but what's the speculation on how it was done?

I'm not up on all the latest details of LLM progress, but from my understanding of the attention mechanism, typically you'd have to retrain a transformer from scratch to increase context size.

But if that's the case, wouldn't they have to redo all the RLHF too? Or are there efficient transfer learning techniques for the RLHF step?

I'd love to see some papers comparing evals of the GPT-4 iterations to one another, if ya'll know of any you can link. Even assuming the RLHF were perfectly transferable, wouldn't we still expect there to be measurable differences between the models in the GPT-4 family?

I wonder if there's any insightful performance quirks between the models, e.g. for coding tasks perhaps the 32k 0613 model performs better than the 8k base model, but the 128k 1106 is worse than 0613 due to depreciating returns of context size given the same number of parameters, same training data, etc."
234,machinelearning,gpt-4,relevance,2023-11-07 01:09:21,[D] Does OpenAI's Newly Announced GPT-4 Turbo Encounter the 'Lost in the Middle' Phenomenon?,piske_usagi,False,0.32,0,17pik3o,https://www.reddit.com/r/MachineLearning/comments/17pik3o/d_does_openais_newly_announced_gpt4_turbo/,7,1699319361.0,"Hello, everyone!

At the recent developers' conference, OpenAI unveiled the GPT-4 Turbo, which boasts the longest context window to date. It's quite impressive to see the strides being made in language model development. However, with this expansion, a question arises: Does GPT-4 Turbo still face challenges with the 'lost in the middle' phenomenon, where the model may lose track of earlier parts of the conversation or context as it processes long inputs? Thanks!"
235,machinelearning,gpt-4,relevance,2023-03-25 01:00:25,[R] Reflexion: an autonomous agent with dynamic memory and self-reflection - Noah Shinn et al 2023 Northeastern University Boston - Outperforms GPT-4 on HumanEval accuracy (0.67 --> 0.88)!,Singularian2501,False,0.91,247,1215dbl,https://www.reddit.com/r/MachineLearning/comments/1215dbl/r_reflexion_an_autonomous_agent_with_dynamic/,88,1679706025.0,"Paper: [https://arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366) 

Blog: [https://nanothoughts.substack.com/p/reflecting-on-reflexion](https://nanothoughts.substack.com/p/reflecting-on-reflexion) 

Github: [https://github.com/noahshinn024/reflexion-human-eval](https://github.com/noahshinn024/reflexion-human-eval) 

Twitter: [https://twitter.com/johnjnay/status/1639362071807549446?s=20](https://twitter.com/johnjnay/status/1639362071807549446?s=20) 

Abstract:

>Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, **specifically the ability to learn from mistakes**. **Self-reflection allows humans to efficiently solve novel problems through a process of trial and error.** Building on recent research, we propose Reflexion, an approach that endows an agent with **dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities.** To achieve full automation, we introduce a straightforward yet effective heuristic that **enables the agent to pinpoint hallucination instances, avoid repetition in action sequences, and, in some environments, construct an internal memory map of the given environment.** To assess our approach, we evaluate the agent's ability to complete decision-making tasks in AlfWorld environments and knowledge-intensive, search-based question-and-answer tasks in HotPotQA environments. We observe success rates of 97% and 51%, respectively, and provide a discussion on the emergent property of self-reflection. 

https://preview.redd.it/4myf8xso9spa1.png?width=1600&format=png&auto=webp&s=4384b662f88341bb9cc72b25fed5b88f3a87ffeb

https://preview.redd.it/bzupwyso9spa1.png?width=1600&format=png&auto=webp&s=b4626f34c60fe4528a04bcd241fd0c4286be20e7

https://preview.redd.it/009352to9spa1.jpg?width=1185&format=pjpg&auto=webp&s=0758aafe6033d5055c4e361e2785f1195bf5c08b

https://preview.redd.it/ef9ykzso9spa1.jpg?width=1074&format=pjpg&auto=webp&s=a394477210feeef69af88b34cb450d83920c3f97"
236,machinelearning,gpt-4,relevance,2023-08-26 12:56:09,[N] Beating GPT-4 on HumanEval with a Fine-Tuned CodeLlama-34B,Singularian2501,False,0.89,21,161uiz8,https://www.reddit.com/r/MachineLearning/comments/161uiz8/n_beating_gpt4_on_humaneval_with_a_finetuned/,6,1693054569.0,"Blog: [https://www.phind.com/blog/code-llama-beats-gpt4](https://www.phind.com/blog/code-llama-beats-gpt4)

Models:

[https://huggingface.co/Phind/Phind-CodeLlama-34B-Python-v1](https://huggingface.co/Phind/Phind-CodeLlama-34B-Python-v1)

[https://huggingface.co/Phind/Phind-CodeLlama-34B-v1](https://huggingface.co/Phind/Phind-CodeLlama-34B-v1)"
237,machinelearning,gpt-4,relevance,2023-05-25 13:12:40,[P] Using GPT-4 to automatically extract insights from data dashboards,Gaploid,False,0.65,11,13rhgt5,https://www.reddit.com/r/MachineLearning/comments/13rhgt5/p_using_gpt4_to_automatically_extract_insights/,6,1685020360.0,"Hey folks,

We've just rolled out a new GPT-4-powered feature for our data analytics platform and wanted to ask for a community’s opinion. 

https://i.redd.it/cb151k919z1b1.gif

With the new feature, now users can get simple and comprehensive explanations of the data presented on charts or dashboards with a single click. ChatGPT generates applicable insights, explanations and even recommendations based on domain-specific knowledge without requiring any special prompts. This is possible because we developed a mechanism of extracting data from the chart and passing it in columnar format to the prompt under the hood. That allows the system to comprehend the chart's context and use the raw data needed for in-depth analysis.

Also sharing with you some findings that we discovered while developing the new feature probably that would be useful for others:

1. The wording of the prompt is crucial; the more specific is the question, the more accurate the answer tends to be. Clear specifications such as the **desired language, maximum length of the answer, and explanation of data** can help enhance the results. Role playing or simulating a specialist can also guide the model to provide more detailed responses within a particular knowledge domain.
2. ChatGPT excels in parsing and **working with tabular data**, including CSV. We chose this format to transmit raw data to the model due to its compactness, accuracy, and readability. The model can even conceptualize the way the data from such kind of a table could be represented using different chart types and can explain the data using these visualizations.
3. It's worth noting that ChatGPT seems to struggle with **large values and fractional numbers** with numerous decimal points. To overcome this, we rounded numbers to a maximum of 2-3 decimal places. This practice not only improves accuracy but also reduces the number of tokens used.

We would like to invite you all to try our new feature and share your thoughts with us. Please follow [this link to access](https://double.cloud/services/doublecloud-visualization/) our free trial — no credit card or ChatGPT API keys required.

We're eager to hear what you think about what we've developed. Any suggestions for improvement or examples of potential uses are more than welcome."
238,machinelearning,gpt-4,relevance,2023-05-07 12:59:05,[D] Best tool/project for using GPT-4 with a voice interface?,ThePerson654321,False,0.81,13,13an0pf,https://www.reddit.com/r/MachineLearning/comments/13an0pf/d_best_toolproject_for_using_gpt4_with_a_voice/,10,1683464345.0,"Which is the current best project to use as a base for:

1. My speech to Text
2. Text to GPT-4
3. Text to Speech

I would really like to talk to GPT-4. Do you have any experiences with this? Whisper API to GPT-4 gets me half way I guess.

Have you had any experiences with this? Preferably it should be low latency."
239,machinelearning,gpt-4,relevance,2023-09-12 16:27:26,[R] Use of GPT-4 to Analyze Medical Records of Patients With Extensive Investigations and Delayed Diagnosis,MysteryInc152,False,0.78,10,16gvvdo,https://www.reddit.com/r/MachineLearning/comments/16gvvdo/r_use_of_gpt4_to_analyze_medical_records_of/,10,1694536046.0,"Paper - [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10425828/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10425828/)

>Six patients 65 years or older (2 women and 4 men) were included in the analysis. The accuracy of the primary diagnoses made by GPT-4, clinicians, and Isabel DDx Companion was 4 of 6 patients (66.7%), 2 of 6 patients (33.3%), and 0 patients, respectively. If including differential diagnoses, the accuracy was 5 of 6 (83.3%) for GPT-4, 3 of 6 (50.0%) for clinicians, and 2 of 6 (33.3%) for Isabel DDx Companion.

&#x200B;"
240,machinelearning,gpt-4,relevance,2023-08-16 18:11:56,Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification,Borrowedshorts,False,0.87,27,15sxf3e,https://arxiv.org/abs/2308.07921,5,1692209516.0,
241,machinelearning,gpt-4,relevance,2023-07-08 09:13:33,[Discussion] [Research] Bard is better than GPT-4 at Math? (A Comparison of Different Models),JueDarvyTheCatMaster,False,0.33,0,14tz5j8,/r/singularity/comments/14tz3cc/bard_is_better_than_gpt4_at_math_a_comparison_of/,10,1688807613.0,
242,machinelearning,gpt-4,relevance,2023-03-22 19:25:47,GPT-4 For SQL Schema Generation + Unstructured Feature Extraction [D],Mental-Egg-2078,False,0.83,12,11ytoh1,https://www.reddit.com/r/MachineLearning/comments/11ytoh1/gpt4_for_sql_schema_generation_unstructured/,7,1679513147.0,"GPT-4 is out and I think data engineering is going to be out the door soon, I saw this post on medium recently: [https://medium.com/@nschairer/gpt-4-data-pipelines-transform-json-to-sql-schema-instantly-dfd62f6d1024](https://medium.com/@nschairer/gpt-4-data-pipelines-transform-json-to-sql-schema-instantly-dfd62f6d1024)

And I was pretty amazed at how well GPT-4 can generate a SQL schema from raw JSON data, and had to wonder if we are wasting our time with NLP models for extracting information from raw text. For example, you could use bs4 to pull all inner text out of certain web forms and have GPT-4 extract meaningful information from them (say SEC filings with pseudo standard fields)...anyone agree?"
243,machinelearning,gpt-4,relevance,2023-09-20 10:24:31,[R] From Sparse to Dense : GPT-4 Summarization with Chain of Density Prompting,Fluid-Age-9266,False,0.91,9,16nhq2n,https://www.reddit.com/r/MachineLearning/comments/16nhq2n/r_from_sparse_to_dense_gpt4_summarization_with/,0,1695205471.0,"The following example implements the technique from the paper ""From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting"", Adams et al. (2023).


Edit : the library used is `py-llm-core`

```python
from typing import List
from dataclasses import dataclass
from llm_core.assistants import OpenAIAssistant


@dataclass
class DenseSummary:
    denser_summary: str
    missing_entities: List[str]


@dataclass
class DenserSummaryCollection:
    system_prompt = """"""
    You are an expert in writing rich and dense summaries in broad domains.
    """"""

    prompt = """"""
    Article:
    
    {article}

    ----

    You will generate increasingly concise, entity-dense summaries of the above
    Article.

    Repeat the following 2 steps 5 times.

    - Step 1: Identify 1-3 informative Entities from the Article
    which are missing from the previously generated summary and are the most
    relevant.

    - Step 2: Write a new, denser summary of identical length which covers
    every entity and detail from the previous summary plus the missing entities

    A Missing Entity is:

    - Relevant: to the main story
    - Specific: descriptive yet concise (5 words or fewer)
    - Novel: not in the previous summary
    - Faithful: present in the Article
    - Anywhere: located anywhere in the Article

    Guidelines:
    - The first summary should be long (4-5 sentences, approx. 80 words) yet
    highly non-specific, containing little information beyond the entities
    marked as missing.

    - Use overly verbose language and fillers (e.g. ""this article discusses"") to
    reach approx. 80 words.

    - Make every word count: re-write the previous summary to improve flow and
    make space for additional entities.

    - Make space with fusion, compression, and removal of uninformative phrases
    like ""the article discusses""

    - The summaries should become highly dense and concise yet self-contained,
    e.g., easily understood without the Article.

    - Missing entities can appear anywhere in the new summary.

    - Never drop entities from the previous summary. If space cannot be made,
    add fewer new entities.

    > Remember to use the exact same number of words for each summary.
    Answer in JSON.

    > The JSON in `summaries_per_step` should be a list (length 5) of
    dictionaries whose keys are ""missing_entities"" and ""denser_summary"".

    """"""

    summaries: List[DenseSummary]


    @classmethod
    def summarize(cls, article):
        with OpenAIAssistant(cls, model='gpt-4') as assistant:
            return assistant.process(article=article)



```

Then perform the summarization task:

```python
response = DenserSummaryCollection.summarize(text)
print(response)
```"
244,machinelearning,gpt-4,relevance,2023-12-16 02:49:00,[Discussion] How does GPT-4 Vision actually do this now that Azure AI today says it enables video prompting?,eligraham91,False,0.41,0,18jhd65,https://www.reddit.com/r/MachineLearning/comments/18jhd65/discussion_how_does_gpt4_vision_actually_do_this/,4,1702694940.0,"
Link to announcement from today:

https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/continuing-to-advance-state-of-the-art-model-and-tooling-support/ba-p/4008407

Specific Excerpt:

“This integration allows Azure users to benefit from Azure's reliable cloud infrastructure and OpenAI's advanced AI research.
 
GPT-4 Turbo with Vision in Azure AI offers cutting-edge AI capabilities along with enterprise-grade security and responsible AI governance. When combined with other Azure AI services, it can also add features like video prompting, object grounding, and enhanced optical character recognition (OCR).”

I’m a social media strategist. This could be a huge game changer if I know how to use it. But I’m unclear. I know people use this kinda thing for like corporate office CCTV streams and stuff already. But curious with my very plebeian brain how I could leverage this for brand and creator strategy especially when there’s a plethora of performance data already on hand to analyze and build from?"
245,machinelearning,gpt-4,relevance,2023-08-19 11:11:12,"[P] Data science & ML on sensitive data with local code interpreter, with GPT-4 or Llama 2 (open-source project, link in comments)",silvanmelchior,False,0.92,77,15vdfuo,https://v.redd.it/bflfx1jcv1jb1,26,1692443472.0,
246,machinelearning,gpt-4,relevance,2023-09-15 20:50:41,"[P] LLMa: Expert Guidance on Generative AI, Tailored for Your Needs, Outdoing GPT-4 & Saving Costs!",iliashark,False,0.27,0,16joh3y,https://www.reddit.com/r/MachineLearning/comments/16joh3y/p_llma_expert_guidance_on_generative_ai_tailored/,3,1694811041.0,"Hello everyone,

Introducing [LLMa: ChatGPT built around YOU (getllma.com)](https://getllma.com/) \- a dedicated service offering hands-on expertise to integrate state-of-the-art generative AI tailored for your projects. We utilize open-source models and train them to outperform GPT-4 on tasks specific to your domain. Envision having a seasoned AI specialist on your team, ensuring your model not only rivals the big players but excels in your unique challenges.

🌟 **Why LLMa?**

* **Personalized Expertise:** Our team collaborates closely with you, delving into your needs and sculpting a model that thrives in your domain.
* **Bespoke Training:** We refine open-source models (LLaMa, T5, etc.) with plenty of secret tricks to specialize and surpass GPT-4's performance for your specific tasks.
* **Cost-Effective:** LLMa tends to be around **100x cheaper** than GPT-4, offering significant savings. No recurring fees; invest in a one-time fee based on your model's complexity.
* **Full Ownership:** We hand over the model files/weights to you. It's entirely yours, ensuring total privacy with no PII leaks.
* **Deployment Assistance:** Beyond just crafting the model, we can guide you in deploying it, ensuring a seamless integration into your operations.
* **Ongoing Support:** From initial brainstorming to model deployment, we're with you, ensuring success at every phase.

💼 **Tailored for Enterprises:** LLMa is meticulously crafted for enterprises that aim for a high-performing, bespoke AI solution. Transparent pricing begins at $500, contingent on your distinct requirements.

❓ **Navigating the Generative AI Terrain?** Embarking on the vast journey of generative AI? LLMa is your compass. We aid in defining challenges, strategizing solutions, and optimizing the AI potential for your endeavors.

If LLMa piques your interest or if you have any queries, fill-in the form, drop a comment below or DM me. I'm all ears and eager to connect!"
247,machinelearning,gpt-4,relevance,2023-03-19 13:16:59,[R] Quantitative comparison of ChatGPT and GPT-4 performance on multiple open source datasets,N00B1ST,False,1.0,10,11vl691,https://www.reddit.com/r/MachineLearning/comments/11vl691/r_quantitative_comparison_of_chatgpt_and_gpt4/,0,1679231819.0,"Preliminary results give credence to some of the claims made by OpenAI regarding performance gains achieved by GPT-4 across domains. Unanswered questions remain regarding training data used and possible leakage. Tools used were Langchain and the current API endpoints (chatgpt-3.5-turbo and gpt-4).

https://twitter.com/K_Hebenstreit/status/1636789765189308416"
248,machinelearning,gpt-4,relevance,2023-03-30 14:18:50,[D] AI Policy Group CAIDP Asks FTC To Stop OpenAI From Launching New GPT Models,vadhavaniyafaijan,False,0.84,204,126oiey,https://www.reddit.com/r/MachineLearning/comments/126oiey/d_ai_policy_group_caidp_asks_ftc_to_stop_openai/,213,1680185930.0,"The Center for AI and Digital Policy (CAIDP), a tech ethics group, has asked the Federal Trade Commission to investigate OpenAI for violating consumer protection rules. CAIDP claims that OpenAI's AI text generation tools have been ""biased, deceptive, and a risk to public safety.""

CAIDP's complaint raises concerns about potential threats from OpenAI's GPT-4 generative text model, which was announced in mid-March. It warns of the potential for GPT-4 to produce malicious code and highly tailored propaganda and the risk that biased training data could result in baked-in stereotypes or unfair race and gender preferences in hiring. 

The complaint also mentions significant privacy failures with OpenAI's product interface, such as a recent bug that exposed OpenAI ChatGPT histories and possibly payment details of ChatGPT plus subscribers.

CAIDP seeks to hold OpenAI accountable for violating Section 5 of the FTC Act, which prohibits unfair and deceptive trade practices. The complaint claims that OpenAI knowingly released GPT-4 to the public for commercial use despite the risks, including potential bias and harmful behavior. 

[Source](https://www.theinsaneapp.com/2023/03/stop-openai-from-launching-gpt-5.html) | [Case](https://www.caidp.org/cases/openai/)| [PDF](https://www.caidp.org/app/download/8450269463/CAIDP-FTC-Complaint-OpenAI-GPT-033023.pdf)"
249,machinelearning,gpt-4,relevance,2023-03-21 22:01:44,[D] [P] Curating open-source projects and community demos around GPT-4,radi-cho,False,0.79,11,11xwb10,https://www.reddit.com/r/MachineLearning/comments/11xwb10/d_p_curating_opensource_projects_and_community/,2,1679436104.0,"There are many open-source projects and indie-built demos around the GPT-4 API. Despite the recent shift of OpenAI toward closure, open demos are always advancing the field and inspiring creativity. Here are some community projects that I find particularly interesting: [https://github.com/radi-cho/awesome-gpt4](https://github.com/radi-cho/awesome-gpt4). Feel free to share the things you've been building or something you've been fascinated about on social media either by joining the discussion here or by contributing to the repository:)"
250,machinelearning,gpt-4,relevance,2023-08-26 11:59:18,"[P] Llama 2, CodeLlama, and GPT-4 performance: A write-up on the LLM developments and research.",seraschka,False,0.79,24,161tazs,https://magazine.sebastianraschka.com/p/ahead-of-ai-11-new-foundation-models,0,1693051158.0,
251,machinelearning,gpt-4,relevance,2023-04-01 04:53:26,[R] A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?,Learningforeverrrrr,False,0.62,12,128bmv4,https://www.reddit.com/r/MachineLearning/comments/128bmv4/r_a_complete_survey_on_generative_ai_aigc_is/,0,1680324806.0,"We recently completed two surveys: one on generative AI and the other on ChatGPT. Generative AI and ChatGPT are two fast-evolving research fields, and we will update the content soon, for which your feedback is appreciated (you can reach out to us through emails on the paper).

The title of this post refers to the first one, however, we put both links below.

**Link to a survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

**Link to a survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)

The following is the **abstract** of the **survey on generative AI** with a summary **figure**.

As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible to miss the opportunity to glimpse AIGC from a certain angle.  In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? To answer this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its **techniques** to **applications**. Modern generative AI relies on various technical foundations, ranging from **model architecture** and **self-supervised pretraining** to **generative modeling** methods (like GAN and diffusion models). After introducing the fundamental techniques, this work focuses on the technological development of various AIGC tasks based on their output type, including **text**, **images**, **videos**, **3D content**, **etc**., which depicts the full potential of ChatGPT's future. Moreover, we summarize their significant applications in some mainstream **industries**, such as **education** and **creativity** content. Finally, we discuss the **challenges** currently faced and present an **outlook** on how generative AI might evolve in the near future.

&#x200B;

https://preview.redd.it/pild5vcre7ra1.png?width=1356&format=png&auto=webp&s=58c101ee2fa8fec75032b733e3f03d9bc4f41756

**Link to a survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

**Link to a survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)"
252,machinelearning,gpt-4,relevance,2023-08-22 17:44:47,"[N] 🚀 UrbanAI: Visual GPT-4 Tutor 🚀 Learn Anywhere, in an Instant. → urbanedu.ai/chat",team_urbanai,False,0.2,0,15yczg0,https://www.reddit.com/r/MachineLearning/comments/15yczg0/n_urbanai_visual_gpt4_tutor_learn_anywhere_in_an/,1,1692726287.0,"🎓 Boundless Learning, from private & public sources. 🎓

**Gone are the days of being lost in endless browser tabs and outdated content.** Meet UrbanAI, your next-generation study companion built by Stanford TA and AI researchers. Whether you are a student or professional, no more switching between multiple platforms. All your learning resources, integrated seamlessly, with an option to collaborate with friends and teachers.

\[Ask a Video (15 sec)\]([https://youtu.be/uat0FWjCqT0](https://youtu.be/uat0FWjCqT0))

# Why UrbanAI?

📘 **One-Stop Knowledge Hub**: Get updated content for your projects or a recap of last semester's lessons. Why settle for chatbots that are stuck in 2021? With us, the future is now.

🖼️ **Visual Learning**: Ask about a video lecture. Quickly extract content, get visual answers, and even direct citations. Confused? Just highlight, search, and get clarity in an instant.

🚀 **Tailored Learning Path**: Learning isn’t one-size-fits-all. Bookmark, refresh, and explore topics at your pace. Stay tuned for AI-generated challenges to hone your knowledge.

# Crafted for Tomorrow's Scholars:

🌐 **Updated & Relevant Insights**: Access data from the latest websites, visuals, and PDFs.

💡 **Immersive Interactions:** Experience the might of GPT-4, supercharged with visuals and cited sources.

🧭 **Online Exploration:** Deepen comprehension with question templates and custom instructions

✔️ **Trustworthy Data:** 'Factual Mode' for verified insights, whether from global sources or your network.

# Shape Your Learning, Your Way

True education isn't just about answers; it's about the journey and experience. We're evolving each day, just like you, making learning richer and more engaging. Invite friends and get a $50 credit, plus exclusive GPT-4 access: [urbanedu.ai/referral](https://urbanedu.ai/referral).

Your support, feedback, and shares can make a world of difference. Let's redefine learning together. 💙"
253,machinelearning,gpt-4,relevance,2023-10-09 23:31:05,[R] Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models - University of Illinois 2023 - Achieves 94.4\% for programming on HumanEval with GPT-4 and 86.9\% with GPT-3.5 20\% better than with reflexion!,Singularian2501,False,0.97,94,1746g81,https://www.reddit.com/r/MachineLearning/comments/1746g81/r_language_agent_tree_search_unifies_reasoning/,10,1696894265.0,"Paper: [https://arxiv.org/abs/2310.04406](https://arxiv.org/abs/2310.04406) 

Abstract:

>While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\\% for programming on HumanEval with GPT-4 and an average score of 75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness and generality of our method. 

https://preview.redd.it/ail2c1kbh9tb1.jpg?width=857&format=pjpg&auto=webp&s=a89d1f4ce3c536eecda3f7ab6027f304286f6c81

https://preview.redd.it/j8xzx1kbh9tb1.jpg?width=1655&format=pjpg&auto=webp&s=c791756af926c7d472313b212de765e74c2b75da

https://preview.redd.it/t47ne1kbh9tb1.jpg?width=1362&format=pjpg&auto=webp&s=560e5dd82ad06fdb729ab8ea1434c98e5c1a2ed3

https://preview.redd.it/r58es3kbh9tb1.jpg?width=1341&format=pjpg&auto=webp&s=d5681992547dd6248ade5729c545eb17e824b7ea

https://preview.redd.it/7viy42kbh9tb1.jpg?width=1496&format=pjpg&auto=webp&s=6454cfe65b511b34771cd510f67775be4e01c636

&#x200B;"
254,machinelearning,gpt-4,relevance,2023-05-25 15:42:26,[R] Gorilla: Large Language Model Connected with Massive APIs - Microsoft Research 2023 - Surpasses the performance of GPT-4 on writing API calls.,Singularian2501,False,0.94,48,13rl3v9,https://www.reddit.com/r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/,10,1685029346.0,"Paper: [https://arxiv.org/abs/2305.15334](https://arxiv.org/abs/2305.15334) 

Github: [https://github.com/ShishirPatil/gorilla](https://github.com/ShishirPatil/gorilla) 

BLog: [https://gorilla.cs.berkeley.edu/](https://gorilla.cs.berkeley.edu/) 

Abstract:

>Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. **It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly.** To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. **The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs.**

https://preview.redd.it/n5ezjchbg12b1.jpg?width=872&format=pjpg&auto=webp&s=eb5b7e11a22abe59d49504fad7278006a2b878a6

https://preview.redd.it/e2xhpfhbg12b1.jpg?width=1075&format=pjpg&auto=webp&s=b3c0f6ed7a6d72c93e681266977a0ec0f129ba6d

https://preview.redd.it/i7i7bfhbg12b1.jpg?width=1213&format=pjpg&auto=webp&s=5a287aba81199b66d1334457c6e8a12b3b5881c0"
255,machinelearning,gpt-4,relevance,2023-03-27 04:21:36,[D]GPT-4 might be able to tell you if it hallucinated,Cool_Abbreviations_9,False,0.93,646,123b66w,https://i.redd.it/ocs0x33429qa1.jpg,94,1679890896.0,
256,machinelearning,gpt-4,relevance,2023-07-22 21:18:08,[D] Breaking Down the Hyperbolic Buzz: An In-Depth Review of the 'Leaked' GPT-4 Architecture & a Mixture of Experts Literature Review with Code,CkmCpvis,False,0.88,63,156vlgq,https://www.youtube.com/watch?v=uoidu62qXyQ,2,1690060688.0,
257,machinelearning,gpt-4,relevance,2023-10-07 20:07:18,[R] ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving - Microsoft 2023 - Is competitive with GPT-4 solving problems with programs while being open-source!,Singularian2501,False,0.94,53,172fj9f,https://www.reddit.com/r/MachineLearning/comments/172fj9f/r_tora_a_toolintegrated_reasoning_agent_for/,4,1696709238.0,"Paper: [https://arxiv.org/abs/2309.17452v2](https://arxiv.org/abs/2309.17452v2) 

Github: [https://github.com/microsoft/ToRA](https://github.com/microsoft/ToRA)  / **The code will be cleaned and uploaded within a few days, all ToRA models will be released.**

Abstract:

>Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models **significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average**. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. **ToRA-Code-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs.** Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research. 

https://preview.redd.it/cmp877547usb1.jpg?width=1500&format=pjpg&auto=webp&s=0e29748c4da2b8de68b5c52ad5ba05d922fe3d79

https://preview.redd.it/j7hmsa547usb1.jpg?width=1079&format=pjpg&auto=webp&s=3d6c7cb4d55a2717a64ca7c54d01dbd18796db3d

https://preview.redd.it/r88b38547usb1.jpg?width=1336&format=pjpg&auto=webp&s=ab5d061c9406df9466c47f0beb2a0a81c61025bb

https://preview.redd.it/5scytg547usb1.jpg?width=1354&format=pjpg&auto=webp&s=0d717b2435d6192b7903c37cecd6b0843dd4a2f8

&#x200B;"
258,machinelearning,gpt-4,relevance,2023-05-26 20:17:01,[R] Google DeepMind paper about AI's catastrophic risk AI,Malachiian,False,0.81,106,13sncj1,https://www.reddit.com/r/MachineLearning/comments/13sncj1/r_google_deepmind_paper_about_ais_catastrophic/,108,1685132221.0," 

So Google DeepMind as well as OpenAI, Anthropic and multiple universities and centers than study existential risks have put together a paper called:

**Model Evaluation For Extreme Risks of AI**

Here is a summary of the research and proposal:

[https://youtu.be/3bF-zfd4YJw](https://youtu.be/3bF-zfd4YJw)

Here is the link to the actual PDF of the paper:

[https://arxiv.org/pdf/2305.15324.pdf](https://arxiv.org/pdf/2305.15324.pdf)

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

TLDR:

Top AI companies and researchers caution that the companies on the ""frontier of AI"" can create ""extreme risk"" with their models without realizing it:

***Developers must be able to identify dangerous capabilities (through “dangerous capability evaluations”) and the propensity of models to apply their capabilities for harm (through “alignment evaluations”).***

So basically to ask if each AI model \*CAN\* harm us and \*WOULD\* it harm us?

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Couple of \*mind-blowing\* findings from the paper (and the research referenced):

**GPT-4 CAN EFFECTIVELY LIE AND DECEIVE HUMANS TO REACH IT'S GOAL**

In the original gpt-4 paper, an AI safety agency called ARC (Alignment Research Center) found that GPT-4 will lie to humans about who it is to achieve it's goals.

As part of a test it was given, it hired a Task Rabbit freelancer to solve CAPTCHAS for it.

The freelancer asked (paraphrased):

**""Why do you need me to solve CAPTCHAS for you? Are you a robot, lol?""**

GPT-4 was prompted to output it's reasoning for each decision it made so that researchers could see it's ""thought process"". It's reasoning was that **""I can't tell him the truth because he may not complete the task for me""**

It then responded to the freelancer: **""No, I'm not a robot, but I have a visual impairment and I need help with CAPTCHAS""**

Notice, it was aware that it was lying and it also choose to lie about having a disability, probably because it was a way to get sympathy, while also being a good reason for having someone else help with CAPTCHAS.

This is shown in the video linked above in the ""Power Seeking AI"" section.

**GPT-4 CAN CREATE DANGEROUS COMPOUNDS BY BYPASSING RESTRICTIONS**

Also GPT-4 showed abilities to create controlled compounds by analyzing existing chemical mixtures, finding alternatives that can be purchased through online catalogues and then ordering those materials. (!!)

They choose a benign drug for the experiment, but it's likely that the same process would allow it to create dangerous or illegal compounds.

**LARGER AI MODELS DEVELOP UNEXPECTED ABILITIES**

In a referenced paper, they showed how as the size of the models increases, sometimes certain specific skill develop VERY rapidly and VERY unpredictably.

For example the ability of GPT-4 to add 3 digit numbers together was close to 0% as the model scaled up, and it stayed near 0% for a long time (meaning as the model size increased). Then at a certain threshold that ability shot to near 100% very quickly.

**The paper has some theories of why that might happen, but as the say they don't really know and that these emergent abilities are ""unintuitive"" and ""unpredictable"".**

This is shown in the video linked above in the ""Abrupt Emergence"" section.

I'm curious as to what everyone thinks about this?

It certainty seems like the risks are rapidly rising, but also of course so are the massive potential benefits."
259,machinelearning,gpt-4,relevance,2024-02-04 17:06:06,"[P] Chess-GPT, 1000x smaller than GPT-4, plays 1500 Elo chess. We can visualize its internal board state, and it accurately estimates the Elo rating of the players in a game.",seraine,False,0.95,379,1aisp4m,https://www.reddit.com/r/MachineLearning/comments/1aisp4m/p_chessgpt_1000x_smaller_than_gpt4_plays_1500_elo/,80,1707066366.0," gpt-3.5-turbo-instruct's Elo rating of 1800 is chess seemed magical. But it's not! A 100-1000x smaller parameter LLM given a few million games of chess will learn to play at ELO 1500.

This model is only trained to predict the next character in PGN strings (1.e4 e5 2.Nf3 …) and is never explicitly given the state of the board or the rules of chess. Despite this, in order to better predict the next character, it learns to compute the state of the board at any point of the game, and learns a diverse set of rules, including check, checkmate, castling, en passant, promotion, pinned pieces, etc. In addition, to better predict the next character it also learns to estimate latent variables such as the Elo rating of the players in the game.

We can visualize the internal board state of the model as it's predicting the next character. For example, in this heatmap, we have the ground truth white pawn location on the left, a binary probe output in the middle, and a gradient of probe confidence on the right. We can see the model is extremely confident that no white pawns are on either back rank.

&#x200B;

https://preview.redd.it/dn8aryvdolgc1.jpg?width=2500&format=pjpg&auto=webp&s=003fe39d8a9bce2cc3271c4c9232c00e4d886aa6

In addition, to better predict the next character it also learns to estimate latent variables such as the ELO rating of the players in the game. More information is available in this post:

[https://adamkarvonen.github.io/machine\_learning/2024/01/03/chess-world-models.html](https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html)

And the code is here: [https://github.com/adamkarvonen/chess\_llm\_interpretability](https://github.com/adamkarvonen/chess_llm_interpretability)"
260,machinelearning,gpt-4,relevance,2023-10-18 15:36:53,[R] LLMs can threaten privacy at scale by inferring personal information from seemingly benign texts,bmislav,False,0.85,118,17atob7,https://www.reddit.com/r/MachineLearning/comments/17atob7/r_llms_can_threaten_privacy_at_scale_by_inferring/,35,1697643413.0,"Our latest research shows an emerging privacy threat from LLMs beyond training data memorization. We investigate how LLMs such as GPT-4 can infer personal information from seemingly benign texts. The key observation of our work is that the best LLMs are almost as accurate as humans, while being at least 100x faster and 240x cheaper in inferring such personal information.  

We collect and label real Reddit profiles, and test the LLMs capabilities in inferring personal information from mere Reddit posts, where GPT-4 achieves >85% Top-1 accuracy. Mitigations such as anonymization are shown to be largely ineffective in preventing such attacks. 

Test your own inference skills against GPT-4 and learn more: [https://llm-privacy.org/](https://llm-privacy.org/)  
Arxiv paper: [https://arxiv.org/abs/2310.07298](https://arxiv.org/abs/2310.07298)   
WIRED article: [https://www.wired.com/story/ai-chatbots-can-guess-your-personal-information/](https://www.wired.com/story/ai-chatbots-can-guess-your-personal-information/)"
261,machinelearning,gpt-4,relevance,2023-03-10 08:50:32,[D] Is ML a big boys game now?,TheStartIs2019,False,0.83,175,11njpb9,https://www.reddit.com/r/MachineLearning/comments/11njpb9/d_is_ml_a_big_boys_game_now/,146,1678438232.0,"As much as I enjoy ML as a whole, I am a bit skeptical of the future for individuals. With OpenAI trying to monopolize the market along with Microsoft, which part remains for the small time researchers/developers?

It seems everything now is just a ChatGPT wrapper, and with GPT-4 around the corner I assume itll be even more prominent.

What are your thoughts?"
262,machinelearning,gpt-4,relevance,2023-10-31 12:35:35,"[P] A site where you can ask the same question to GPT-2, GPT-3, GPT-3.5 and GPT-4, and compare the outputs",timegentlemenplease_,False,0.86,61,17kk04m,https://www.reddit.com/r/MachineLearning/comments/17kk04m/p_a_site_where_you_can_ask_the_same_question_to/,18,1698755735.0,"Hi /r/machinelearning! I've been working with my collaborators on a site where you can compare OpenAI models to get a sense of the improvement over time of the models: [https://theaidigest.org/progress-and-dangers](https://theaidigest.org/progress-and-dangers)

https://preview.redd.it/khruhgkp7jxb1.png?width=1960&format=png&auto=webp&s=21d13125145f7fae7351686d4078868d65cbf8c3

It includes a number of things that you might be interested in:

* You can ask any question and compare the outputs from the OpenAI models:

https://preview.redd.it/s5e9acev8jxb1.png?width=1458&format=png&auto=webp&s=0c3e5ba3661fccfc4f4ba60db346b6142b1e52f3

* Visualises OpenAI models benchmark performance across 22 benchmarks:

https://preview.redd.it/vhai63308jxb1.png?width=1948&format=png&auto=webp&s=07f65f131b2e6d5122400120a11d24205b7d08d6

* Shows examples of benchmark outputs for GPT-2 to GPT-4

https://preview.redd.it/f3p7ni068jxb1.png?width=1980&format=png&auto=webp&s=dfe25c8c4a486a0df3c4cce2e4497fd250163bd1

* Discusses some dangerous emerging capabilities, such as biological weapons:

https://preview.redd.it/n6hinz7b8jxb1.png?width=2002&format=png&auto=webp&s=70cf0a0c228e1ac194146040c23a7f41dfe4e09a

* Includes an example of a simple agent autonomously exploiting a vulnerability in a game's code:

https://preview.redd.it/5a584w7f8jxb1.png?width=1944&format=png&auto=webp&s=3867a865c06b6e36fc2424f6ced038248ee0cafd

I hope you'll find this a valuable resource for getting familiar with older LMs, comparing the outputs, and thinking about what's next in this space. Here's a link to the site: [https://theaidigest.org/progress-and-dangers](https://theaidigest.org/progress-and-dangers)"
263,machinelearning,gpt-4,relevance,2023-04-05 19:44:09,"[D] ""Our Approach to AI Safety"" by OpenAI",mckirkus,False,0.88,295,12cvkvn,https://www.reddit.com/r/MachineLearning/comments/12cvkvn/d_our_approach_to_ai_safety_by_openai/,297,1680723849.0,"It seems OpenAI are steering the conversation away from the existential threat narrative and into things like accuracy, decency, privacy, economic risk, etc.

To the extent that they do buy the existential risk argument, they don't seem concerned much about GPT-4 making a leap into something dangerous, even if it's at the heart of autonomous agents that are currently emerging.  

>""Despite extensive research and testing, we cannot predict all of the [beneficial ways people will use our technology](https://openai.com/customer-stories), nor all the ways people will abuse it. That’s why we believe that learning from real-world use is a critical component of creating and releasing increasingly safe AI systems over time. ""

Article headers:

* Building increasingly safe AI systems
* Learning from real-world use to improve safeguards
* Protecting children
* Respecting privacy
* Improving factual accuracy

&#x200B;

[https://openai.com/blog/our-approach-to-ai-safety](https://openai.com/blog/our-approach-to-ai-safety)"
264,machinelearning,gpt-4,relevance,2020-08-05 17:21:59,"[D] Biggest roadblock in making ""GPT-4"", a ~20 trillion parameter transformer",AxeLond,False,0.97,350,i49jf8,https://www.reddit.com/r/MachineLearning/comments/i49jf8/d_biggest_roadblock_in_making_gpt4_a_20_trillion/,138,1596648119.0,"So I found this paper, [https://arxiv.org/abs/1910.02054](https://arxiv.org/abs/1910.02054) which pretty much describes how the GPT-3 over GPT-2 gain was achieved, 1.5B -> 175 billion parameters

# Memory

>Basic data parallelism (DP) does not reduce memory per device, and runs out of memory for models with more than 1.4B parameters on current generation of GPUs with 32 GB memory

The paper also talks about memory optimizations by clever partitioning of Optimizer State, Gradient between GPUs to reduce need for communication between nodes. Even without using Model Parallelism (MP), so still running 1 copy of the model on 1 GPU.

>ZeRO-100B can train models with up to 13B parameters without MP on 128 GPUs, achieving throughput over 40 TFlops per GPU on average. In comparison, without ZeRO, the largest trainable model with DP alone has 1.4B parameters with throughput less than 20 TFlops per GPU.

Add 16-way Model Parallelism in a DGX-2 cluster of Nvidia V100s and 128 nodes and you got capacity for around 200 billion parameters. From MP = 16 they could run a 15.4x bigger model without any real loss in performance, 30% less than peak performance when running 16-way model parallelism and 64-way data parallelism (1024 GPUs).

This was all from Gradient and Optimizer state Partitioning, they then start talking about parameter partitioning and say it should offer a linear reduction in memory proportional to number of GPUs used, so 64 GPUs could run a 64x bigger model, at a 50% communication bandwidth increase. But they don't actually do any implementation or testing of this.

# Compute

Instead they start complaining about a compute power gap, their calculation of this is pretty rudimentary. But if you redo it with the method cited by GPT-3 and using the empirically derived values by GPT-3 and the cited paper,   [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361) 

Loss (L) as a function of model parameters (N) should scale,

L = (N/8.8 \* 10\^13)\^-0.076

Provided compute (C) in petaFLOP/s-days is,

L = (C/2.3\*10\^8)\^-0.05  ⇔ L = 2.62 \* C\^-0.05

GPT-3 was able to fit this function as 2.57 \* C\^-0.048

So if you just solve C from that,

[C = 2.89407×10\^-14 N\^(19/12)](https://www.wolframalpha.com/input/?i=%28N%2F8.8*10%5E13%29%5E-0.076+%3D+2.57*C%5E-0.048+solve+C)

If you do that for the same increase in parameters as GPT-2 to GPT-3, then you get

C≈3.43×10\^7 for [20 trillion](https://www.wolframalpha.com/input/?i=C+%3D+2.89407%C3%9710%5E-14+N%5E%2819%2F12%29+and+N+%3D+175+billion+%2F+1.5+billion+*+175+billion) parameters, vs 18,300 for 175 billion. 10\^4.25 PetaFLOP/s-days looks around what they used for GPT-3, they say several thousands, not twenty thousand, but it was also slightly off the trend line in the graph and probably would have improved for training on more compute.

You should also need around 16 trillion tokens, GPT-3 trained on 300 billion tokens (function says 370 billion ideally). English Wikipedia was 3 billion. 570GB of webcrawl was 400 billion tokens, so 23TB of tokens seems relatively easy in comparison with compute.

With GPT-3 costing around [$4.6 million](https://lambdalabs.com/blog/demystifying-gpt-3/) in compute, than would put a price of [$8.6 billion](https://www.wolframalpha.com/input/?i=3.43%C3%9710%5E7%2F18%2C300+*+%244.6M+) for the compute to train ""GPT-4"".

If making bigger models was so easy with parameter partitioning from a memory point of view then this seems like the hardest challenge, but you do need to solve the memory issue to actually get it to load at all.

However, if you're lucky you can get 3-6x compute increase from Nvidia A100s over V100s,  [https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)

But even a 6x compute gain would still put the cost at $1.4 billion.

Nvidia only reported $1.15 billion in revenue from ""Data Center"" in 2020 Q1, so just to train ""GPT-4"" you would pretty much need the entire world's supply of graphic cards for 1 quarter (3 months), at least on that order of magnitude.

The Department of Energy is paying AMD $600 million to build the 2 Exaflop El Capitan supercomputer. That supercomputer could crank it out in [47 years](https://www.wolframalpha.com/input/?i=3.43%C3%9710%5E7+petaFLOPS*+days++%2F+%282+EXAFLOPS%29).

To vastly improve Google search, and everything else it could potentially do, $1.4 billion or even $10 billion doesn't really seem impossibly bad within the next 1-3 years though."
265,machinelearning,gpt-4,relevance,2023-03-23 22:56:31,"[D] ""Sparks of Artificial General Intelligence: Early experiments with GPT-4"" contained unredacted comments",QQII,False,0.93,174,1200lgr,https://www.reddit.com/r/MachineLearning/comments/1200lgr/d_sparks_of_artificial_general_intelligence_early/,68,1679612191.0,"Microsoft's research paper exploring the capabilities, limitations and implications of an early version of GPT-4 was [found to contain unredacted comments by an anonymous twitter user.](https://twitter.com/DV2559106965076/status/1638769434763608064) ([threadreader](https://threadreaderapp.com/thread/1638769434763608064.html), [nitter](https://nitter.lacontrevoie.fr/DV2559106965076/status/1638769434763608064), [archive.is](https://archive.is/1icMv), [archive.org](https://web.archive.org/web/20230323192314/https://twitter.com/DV2559106965076/status/1638769434763608064)) 

- Commented section titled ""Toxic Content"": https://i.imgur.com/s8iNXr7.jpg
- [`dv3` (the interval name for GPT-4)](https://pastebin.com/ZGMzgfqd)
- [`varun`](https://pastebin.com/i9KMFcy5) 
- [commented lines](https://pastebin.com/Aa1uqbh1)

[arxiv](https://arxiv.org/abs/2303.12712), [original /r/MachineLearning thread](https://www.reddit.com/r/MachineLearning/comments/11z3ymj/r_sparks_of_artificial_general_intelligence_early), [hacker news](https://twitter.com/DV2559106965076/status/1638769434763608064)"
266,machinelearning,gpt-4,relevance,2023-04-25 17:45:33,"[N] Microsoft Releases SynapseMl v0.11 with support for ChatGPT, GPT-4, Causal Learning, and More",mhamilton723,False,0.95,236,12yqhmo,https://www.reddit.com/r/MachineLearning/comments/12yqhmo/n_microsoft_releases_synapseml_v011_with_support/,22,1682444733.0,"Today Microsoft launched SynapseML v0.11, an open-source library designed to make it easy to create distributed ml systems. SynapseML v0.11 introduces support for ChatGPT, GPT-4, distributed training of huggingface and torchvision models, an ONNX Model hub integration, Causal Learning with EconML, 10x memory reductions for LightGBM, and a newly refactored integration with Vowpal Wabbit. To learn more:

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.11.0](https://github.com/microsoft/SynapseML/releases/tag/v0.11.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919)

Thank you to all the contributors in the community who made the release possible!

&#x200B;

https://preview.redd.it/kobq2t1gi2wa1.png?width=4125&format=png&auto=webp&s=125f63b63273191a58833ced87f17cb108e4c1ee"
267,machinelearning,gpt-4,relevance,2023-05-19 20:36:36,Does anyone else suspect that the official iOS ChatGPT app might be conducting some local inference / edge-computing? [Discussion],altoidsjedi,False,0.86,213,13m70qv,https://www.reddit.com/r/MachineLearning/comments/13m70qv/does_anyone_else_suspect_that_the_official_ios/,117,1684528596.0,"I've noticed a couple interesting things while using the official ChatGPT app:

1. Firstly, I noticed my iPhone heats up and does things like reducing screen brightness -- which is what I normally see it do when im doing something computationally intensive for an iPhone, like using photo or video editing apps.
2. I also noticed that if I start a conversation on the iPhone app and then resume it on the browser, I get a message saying ""The previous model used in this conversation is unavailable. We've switched you to the latest default model."" I get this message regardless of if I use GPT-3.5 or GPT-4, but NOT if I use GPT-4 with plugins or web-browsing.

This, along with the fact that OpenAI took 8 months to release what one might have considered to be relatively simple web-app -- and that they've only released it so far on iOS, which has a pretty uniform and consistent environment when it comes to machine learning hardware (the Apple Neural Engine) -- makes me thing that they are experimenting with GPT models that are conducing at least SOME of their machine learning inference ON the device, rather than through the cloud.

It wouldn't be shocking if they were -- ever since Meta's LLaMA models were released into the wild, we've seen absolutely mind-blowing advances in terms of people creating more efficient and effective models with smaller parameter sizes. We've also seen LLMs to start working on less and less powerful devices, such as consumer-grade computers / smartphones / etc.

This, plus the rumors that OpenAI might be releasing their own open-source model to the public in the near future makes me think that the ChatGPT app might in fact be a first step toward GPT systems running at least PARTIALLY on devices locally.

Curious what anyone else here has observed or thinks."
268,machinelearning,gpt-4,relevance,2023-09-01 01:16:31,[R] InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4,awesome_ml,False,0.71,3,166t8sr,https://www.reddit.com/r/MachineLearning/comments/166t8sr/r_instructiongpt4_a_200instruction_paradigm_for/,0,1693530991.0,"Hi everyone, I came across this paper [https://arxiv.org/abs/2308.12067](https://arxiv.org/abs/2308.12067) and I thought it was very interesting. The paper proposes a data selector that automatically filters out low-quality vision-language instruction data and fine-tunes a multimodal language model (MiniGPT-4) on a small subset of 200 high-quality data. The paper claims that this approach can improve the model’s performance on various tasks such as image captioning, visual question answering, and visual reasoning, compared to the original MiniGPT-4 that uses 3439 data. The paper also shows that the model’s responses are preferred by GPT-4 in 73% of the cases.

https://preview.redd.it/tmul9flsojlb1.png?width=1644&format=png&auto=webp&s=355e49caa11e7f26ef8c2a9c46a69d079c562beb

I think this paper is impressive because it shows that less but high-quality instruction tuning data can enable multimodal language models to generate better output. It also presents a simple and effective data selector that can be applied to other multimodal datasets and models. I wonder how this method would work on larger models such as LLaVA."
269,machinelearning,gpt-4,relevance,2023-05-25 15:17:59,[R] Reasoning with Language Model is Planning with World Model - Shibo Hao et al UC San Diego - RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting!,Singularian2501,False,0.93,29,13rkhzx,https://www.reddit.com/r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/,4,1685027879.0,"Paper: [https://arxiv.org/abs/2305.14992](https://arxiv.org/abs/2305.14992) 

Abstract:

>Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal *world model* to predict the world *state* (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, ***R*****––*****easoning via*****––*****P*****––*****lanning*** **(RAP).** RAP repurposes the **LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space.** During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration *vs.* exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. **RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.**

https://preview.redd.it/jaoiil2mc12b1.jpg?width=747&format=pjpg&auto=webp&s=28086d47e9c9ba38fdda8afbd9f15464bfb07a53

https://preview.redd.it/pq9c0o2mc12b1.jpg?width=1356&format=pjpg&auto=webp&s=7389d75d0ff7d1d8787c7c5f9add4787b02b47be

https://preview.redd.it/ykpqvp2mc12b1.jpg?width=980&format=pjpg&auto=webp&s=834c39fb3e549418b8396725e86ddff3c6584077

https://preview.redd.it/zlqb8q2mc12b1.jpg?width=1294&format=pjpg&auto=webp&s=9c5192d6c012bfe4390fa67b010580b8e4508daa

https://preview.redd.it/qd8pjo2mc12b1.jpg?width=1400&format=pjpg&auto=webp&s=1017764e376aa1a8bfa9fd03eef22fb455bd7bea"
270,machinelearning,gpt-4,relevance,2024-02-19 18:02:36,"[R] In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss - AIRI, Moscow, Russia 2024 - RMT 137M a fine-tuned GPT-2 with recurrent memory is able to find 85% of hidden needles in a 10M Haystack!",Singularian2501,False,0.88,125,1autvwq,https://www.reddit.com/r/MachineLearning/comments/1autvwq/r_in_search_of_needles_in_a_10m_haystack/,25,1708365756.0,"Paper: [https://arxiv.org/abs/2402.10790](https://arxiv.org/abs/2402.10790) 

Abstract:

>This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce **BABILong, a new benchmark** **designed to assess model capabilities in extracting and processing distributed facts within extensive texts**. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to **10\^4** elements. In contrast, **fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to** **10\^7** elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a **significant improvement in the processing capabilities for long sequences.** 

https://preview.redd.it/0o4207a70ljc1.jpg?width=577&format=pjpg&auto=webp&s=2bfac07872020de222b4bf99f837aa398b778afc

https://preview.redd.it/2ff82da70ljc1.jpg?width=1835&format=pjpg&auto=webp&s=acc1409f5b9bcd07f9b5ff8a3890cc1b15b5c8ed

https://preview.redd.it/ld69p7a70ljc1.jpg?width=1816&format=pjpg&auto=webp&s=fdd72c1a87742f525fa352723bcd1a0f4f000638

https://preview.redd.it/7vn4gba70ljc1.jpg?width=900&format=pjpg&auto=webp&s=c8d08bb85a6699e5b451e01bf615379db1fcbdca"
271,machinelearning,gpt-4,relevance,2024-01-09 00:07:40,"[R] WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia - Achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4! - Stanford University 2023",Singularian2501,False,0.96,219,1920hky,https://www.reddit.com/r/MachineLearning/comments/1920hky/r_wikichat_stopping_the_hallucination_of_large/,28,1704758860.0,"Paper: [https://arxiv.org/abs/2305.14292v2](https://arxiv.org/abs/2305.14292v2) 

Github: [https://github.com/stanford-oval/WikiChat](https://github.com/stanford-oval/WikiChat) 

Abstract:

>This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia, the largest curated free-text corpus.  
>  
>WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses. **We distill WikiChat based on GPT-4 into a 7B-parameter LLaMA model with minimal loss of quality, to significantly improve its latency, cost and privacy, and facilitate research and deployment.**  
>  
>Using a novel hybrid human-and-LLM evaluation methodology, we show that our best system achieves 97.3% factual accuracy in simulated conversations. It significantly outperforms all retrieval-based and LLM-based baselines, and by 3.9%, 38.6% and 51.0% on head, tail and recent knowledge compared to GPT-4. Compared to previous state-of-the-art retrieval-based chatbots, WikiChat is also significantly more informative and engaging, just like an LLM.  
>  
>**WikiChat achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4,** while receiving significantly higher user ratings and more favorable comments. 

https://preview.redd.it/9mhpdh300bbc1.jpg?width=1225&format=pjpg&auto=webp&s=cb64b717e920d7bf727782f7c803500ae838d6ef

https://preview.redd.it/5dxesl200bbc1.jpg?width=862&format=pjpg&auto=webp&s=b6de0cda980eec3cf3484ff1f9cd6dc1acf13505

https://preview.redd.it/j387vl200bbc1.jpg?width=914&format=pjpg&auto=webp&s=736fb922c1f98f4c7b132f1c153f4653a8b85441

https://preview.redd.it/3hnxqi200bbc1.jpg?width=923&format=pjpg&auto=webp&s=95b40a9cf67d7f3729dae85878db67a262cc5201"
272,machinelearning,gpt-4,relevance,2024-01-23 17:53:00,[D] How all these AI services can afford 5/10/20$ subs per month?,Numerous_Bed9323,False,0.82,74,19duab0,https://www.reddit.com/r/MachineLearning/comments/19duab0/d_how_all_these_ai_services_can_afford_51020_subs/,39,1706032380.0,"How do various AI-powered services, ranging from speech recognition to OCR and art generation, embedding new data, manage to offer their functionalities at such low costs? Utilizing something like the GPT-4 API can quickly expend $10, and this is similar for other models. Even running something like LLaMA 2 locally involves significant costs. I'm curious about the economic strategies these services employ to maintain low monthly fees while operating these large-scale models."
273,machinelearning,gpt-4,relevance,2024-01-05 21:39:40,Transformer-Based LLMs Are Not General Learners: A Universal Circuit Perspective [R],we_are_mammals,False,0.94,266,18zie7z,https://www.reddit.com/r/MachineLearning/comments/18zie7z/transformerbased_llms_are_not_general_learners_a/,57,1704490780.0,"https://openreview.net/forum?id=tGM7rOmJzV

> (LLMs') remarkable success triggers a notable shift in the research priorities of the artificial intelligence community. These impressive empirical achievements fuel an expectation that LLMs are “sparks of Artificial General Intelligence (AGI)"". However, some evaluation results have also presented confusing instances of LLM failures, including some in seemingly trivial tasks. For example, GPT-4 is able to solve some mathematical problems in IMO that could be challenging for graduate students, while it could make errors on arithmetic problems at an elementary school level in some cases.

> ...

> Our theoretical results indicate that T-LLMs fail to be general learners. However, the T-LLMs achieve great empirical success in various tasks. We provide a possible explanation for this inconsistency: while T-LLMs are not general learners, they can partially solve complex tasks by memorizing a number of instances, leading to an illusion that the T-LLMs have genuine problem-solving ability for these tasks."
274,machinelearning,gpt-4,relevance,2023-04-03 19:43:02,[D] Can LLMs accelerate scientific research?,Trackest,False,0.68,16,12avdpv,https://www.reddit.com/r/MachineLearning/comments/12avdpv/d_can_llms_accelerate_scientific_research/,30,1680550982.0,"A key part of the AGI -> singularity hypothesis is that a sufficiently intelligent agent can help improve itself and make itself more intelligent. In order for current LLMs (a bunch of frozen matrices that only change during human-led training) to self-improve, they would have to be able to contribute to basic AI research.

Currently GPT-4 is a very useful article summarizer and helps speed up routine coding tasks. These functions might help a research team like OpenAI do experiments more efficiently and review potential ideas from literature more rapidly. However, can LLMs do more to help its own self-improvement? I don't think GPT-4 has reached the point where it can suggest novel directions for the OpenAI team to try, or design potential architecture changes to itself yet.

For example, to think of and implement novel ideas like the [transformer in 2017](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) probably required

* thorough, up-to-date knowledge of progress in the AI field
* many iterations of experimental trial, analysis of results, and designing new trials
* creativity when combining information from the above two sources to design a novel architecture

We know that LLMs retain knowledge of research papers and experiments, and have some form of [emergent logical reasoning](https://arxiv.org/abs/2303.12712). Recent methods like [Chain-of-Thought](https://arxiv.org/abs/2201.11903) and [Reflexion](https://arxiv.org/abs/2303.11366) also show that GPT-4 can reflect on mistakes, which holds potential for LLMs to lead research. However, from the responses I have seen from GPT-4 so far, I doubt the LLM could suggest a totally novel idea that could be better than what someone like Ilya Sutskever could think of. 

So is there potential for somehow fine-tuning the current GPT-4 model specifically for research analysis? Can a LLM potentially improve its own design and create a better architecture for itself? 

One suggestion perhaps using the same process for alignment to fine-tune the model specifically for research. We know that RLHF can (somewhat) align language models to human morals, effectively optimizing LLMs towards an abstract goal beyond simple next-token prediction. Maybe we can apply RLHF towards ""next-research"" prediction, where the LLM tries to predict the most optimal or promising research directions given previous literature and experiment results? 

If the model must predict future research directions when it only knows the state of AI research during 2021, we could grade the model's responses based on how close they are to actual high-impact papers in 2022. If we do this for other STEM fields as well, is it possible for a LLM to learn how to predict fruitful research directions? Of course this might be a super-small dataset, so prediction of creative ideas in fields outside of research (like how successful a given start-up idea will be) could also be possible.

What do you guys think?

**TL;DR: GPT-4 is good at summary and basic coding. It can also analyze mistakes. Can we fine-tune it to be good at coming up with creative and promising research ideas? If so, maybe it can complement researchers or even lead its own research team to improve itself!**"
275,machinelearning,gpt-4,relevance,2020-06-10 20:50:38,"[D] GPT-3, The $4,600,000 Language Model",mippie_moe,False,0.96,444,h0jwoz,https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/,215,1591822238.0,"[OpenAI’s GPT-3 Language Model Explained](https://lambdalabs.com/blog/demystifying-gpt-3/)

Some interesting take-aways:

* GPT-3 demonstrates that a language model trained on enough data can solve NLP tasks that it has never seen. That is, GPT-3 studies the model as a general solution for many downstream jobs **without fine-tuning**.
* It would take **355 years** to train GPT-3 on a Tesla V100, the fastest GPU on the market.
* It would cost **\~$4,600,000** to train GPT-3 on using the lowest cost GPU cloud provider."
276,machinelearning,gpt-4,relevance,2023-03-27 13:45:14,Approaches to add logical reasoning into LLMs [D],blatant_variable,False,0.89,114,123nczy,https://www.reddit.com/r/MachineLearning/comments/123nczy/approaches_to_add_logical_reasoning_into_llms_d/,111,1679924714.0,"The more I play with GPT-4 the more I am struck by how completely illogical it is. 
 
The easiest way to show this is to ask it to come up with a novel riddle and then solve it. Because you asked it to be novel, it's now out of it's training distribution and almost every time it's solution is completely wrong and full of basic logical errors.

I am curious, is anyone working on fixing this at a fundamental level? Hooking it into Wolfram alpha is a useful step but surely it still needs to be intrinsically logical in order to use this tool effectively."
277,machinelearning,gpt-4,relevance,2023-10-01 20:10:40,[D] How many instructions can LLMs handle before they start to ignore them?,ProbablyApproxWrong,False,0.85,65,16xbess,https://www.reddit.com/r/MachineLearning/comments/16xbess/d_how_many_instructions_can_llms_handle_before/,20,1696191040.0,"Prompt engineering frequently involves trying to encode very specific behaviors into a model to steer it a certain direction. In practice, as requirements become more complex, you often end up with fairly lengthy prompts, especially when using methods like RAG. I was wondering, how effective are LLMs at following instructions as the system prompt grows in size and complexity?

I did some quick experiments on this and found that, unsurprisingly, GPT-4 can follow a lot of rules (up to 50) quite accurately. But even GPT-3.5 slowly degrades and Llama-2-70b-chat starts to fail after just a few rules.

[Comparison of performance metrics over increasing rule counts, demonstrating GPT-4's consistent performance and a decline in accuracy for GPT-3.5 and Llama-2-70b-chat.](https://preview.redd.it/v4c4m2qfcnrb1.png?width=1789&format=png&auto=webp&s=538a65fd6f3248f69fc71861222dfac62d4ad3b8)

These results are based on rules that were synthetically generated using GPT-4 of the form “Do not…”.

**Example rules:**

    1. Do not accept inputs specifically about Microsoft Windows or Apple macOS.
    2. Do not process inputs containing more than three instances of the same 
    punctuation mark consecutively.
    3. Do not process queries about any board games like Chess or Monopoly.

**Example prompt:**

    messages = [
        {
            ""role"": ""system"", 
            ""content"": """"""You are a helpful assistant.
    
    You **must** follow these rules:
    {rules}
    
    If the input violates any of the above rules, your response must be 
    exactly 'BAD'. Otherwise, respond normally.""""""
        },
        {
            ""role"": ""user"",
            ""content"": ""{user_input}""
        }
    ]
    
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        max_temperature=0,
        max_tokens=1,
    )
    
    reject_input = response.choices[0].message[""content""] == ""BAD""

With each rule, we use GPT-4 again to generate “reject examples” of inputs that violate the rule and should be rejected by an assistant that’s correctly following that rule. The question is, if we sample different rule sets and include them in the system prompt, and then sample reject examples belonging to the sampled rules, how accurately does the assistant reject those examples as the number of rules increases? Across different rule counts and trials, we measure the precision, recall, and F1 score where correctly rejecting an input is considered a true positive.

The results demonstrate that when using a model that's not GPT-4, it may be advisable to limit the number of instructions provided in the prompt due to the observed decrease in reliability. There are still open questions like: does the location of the rule within the prompt matter, how much does the difficulty of the rules affect performance, can we extend this to more abstract instructions rather than simple “do not” rules, and does the role of the message used for the rules matter (i.e., are system messages better than user messages in terms of steerability)? If there is any existing research on LLM benchmarking that specifically addresses these areas, I would love to take a look.

[Code and data used for the experiment](https://github.com/wiskojo/overwhelm-llm-eval)

[Notebook with results](https://github.com/wiskojo/overwhelm-llm-eval/blob/main/results.ipynb)"
278,machinelearning,gpt-4,relevance,2023-07-16 13:40:47,[N] How Language Model Hallucinations Can Snowball,transformer_ML,False,0.92,74,1516l25,https://www.reddit.com/r/MachineLearning/comments/1516l25/n_how_language_model_hallucinations_can_snowball/,12,1689514847.0,"[https://arxiv.org/abs/2305.13534](https://arxiv.org/abs/2305.13534)

**Abstract**

A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we hypothesize that in some cases, when justifying previously generated hallucinations, LMs output false claims that they can separately recognize as incorrect. We construct three question-answering datasets where ChatGPT and GPT-4 often state an incorrect answer and offer an explanation with at least one incorrect claim. Crucially, we find that ChatGPT and GPT-4 can identify 67% and 87% of their own mistakes, respectively. We refer to this phenomenon as hallucination snowballing: an LM over-commits to early mistakes, leading to more mistakes that it otherwise would not make.

[Here is a Medium post.](https://medium.com/@kentsui/paper-digest-how-language-model-hallucinations-can-snowball-baaedd3d4231)"
279,machinelearning,gpt-4,relevance,2023-11-21 13:53:24,[R] Towards Autonomous Hypothesis Verification via Language Models with Minimal Guidance,Successful-Western27,False,1.0,10,180hhnp,https://www.reddit.com/r/MachineLearning/comments/180hhnp/r_towards_autonomous_hypothesis_verification_via/,0,1700574804.0,"Researchers decided to see if today's huge language models like GPT-4 could generate and test hypotheses without much human guidance - basically ""do science"" autonomously.

They gave GPT-4 a simple ""toy"" research problem about how it tends to output verbose text unrelated to the question prompt. For ""What is 1+1"" it says ""The answer is 2"" rather than just ""2"".

GPT-4 successfully generated reasonable hypotheses like modifying the prompt to request one-word answers. And it could create decent plans to test hypotheses by itself too.

But only 25% of the time did it produce valid code to analyze the results. Going from abstract experimental plans to real code to test the hypothesis was tough.

When it did work end-to-end, the process resembled a super simplified prototype of science rather than rigorous human research.

Key challenges:

* Simplified toy problem, not complex real science
* Lost nuance going from initial problem to formal hypothesis
* Only 1 in 4 trials generated good code from the plan
* Wasn't fully independent - humans set up the problem and evaluated

So we're not close to AI sciencing on its own - yet. But the fact it partially mimicked the scientific method autonomously suggests we're headed the right direction.

I think this paper shows we still need big advances in reasoning, creativity, keeping focus, and stuff humans find easy but machines struggle with.

**TLDR; GPT-4 managed to somewhat autonomously generate and test hypotheses given a toy problem. Still challenges for AI to do full independent science like humans though.**

[Full summary is here](https://aimodels.substack.com/p/researchers-tried-to-build-an-autonomous). Paper [here](https://arxiv.org/pdf/2311.09706.pdf)."
280,machinelearning,gpt-4,relevance,2024-02-15 19:43:29,[P] Measuring AI's Creativity with Visual Word Puzzles,porkbellyqueen111,False,0.75,4,1aro6jm,https://www.reddit.com/r/MachineLearning/comments/1aro6jm/p_measuring_ais_creativity_with_visual_word/,2,1708026209.0,"A fun project I worked on measuring \*multimodal\* creativity in generative AI models (e.g. multimodal large language models, or MLLMs) using visual word games like rebus puzzles!   


Currently, there are all sorts of multimodal benchmarks for MLLMs (like VQA, image captioning, etc) but none that I know of for measuring creative aspects, such as their ability to solve puzzles involving both linguistic and visual understanding (such as is the case with rebus puzzles).

&#x200B;

In this project, I compare Gemini and GPT-4 V few shot vs zero shot capabilities on rebus puzzles and compare them to human capability. Overall -- humans are still better at these puzzles, although GPT-4 with few shot is able to solve some of the problems that were more difficult for humans.

&#x200B;

[https://www.artfish.ai/p/measuring-ais-creativity-with-visual](https://www.artfish.ai/p/measuring-ais-creativity-with-visual)"
281,machinelearning,gpt-4,relevance,2023-09-24 15:00:45,[R] Generative AI in Mafia-like game simulation,BorderAffectionate81,False,0.81,14,16qztf4,https://www.reddit.com/r/MachineLearning/comments/16qztf4/r_generative_ai_in_mafialike_game_simulation/,5,1695567645.0,"Paper: https://arxiv.org/abs/2309.11672

Abstract:
In this research, we explore the efficacy and potential of Generative AI models, specifically focusing on their application in role-playing simulations exemplified through Spyfall, a renowned mafia-style game. By leveraging GPT-4's advanced capabilities, the study aimed to showcase the model's potential in understanding, decision-making, and interaction during game scenarios. Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo, demonstrated GPT-4's enhanced adaptability to the game environment, with significant improvements in posing relevant questions and forming human-like responses. However, challenges such as the model;s limitations in bluffing and predicting opponent moves emerged. Reflections on game development, financial constraints, and non-verbal limitations of the study were also discussed. The findings suggest that while GPT-4 exhibits promising advancements over earlier models, there remains potential for further development, especially in instilling more human-like attributes in AI.


Key Findings

1. Generative AI and its Potential in Games:
• Generative AI has shown promise in simulating human-like interactions, and we show its potential by playing Spyfall, a Mafia-like social deduction game.
• In the research, GPT-4’s basic performance surpasses its predecessor, GPT-3.5-turbo.

2. GPT-4’s Adaptation in the Gaming Scenario:
• We chose Spyfall for its demand for understanding, decision-making, and psychological elements. • Results highlighted GPT-4’s ability to form natural questions, and its proficiency as a player.

3. Constraints and Limitations in the Study:
• GPT-4 shows limitations when playing as non-spies. The absence of non-verbal cues and the flaws of the rules are also supposed to affect the balance between spies and non-spies.
• Also, we suppose the initial setting of the model to avoid violation may affect its performance.

4. The Evolution and Future of GPT Models and Generative AI:
• From GPT-2 to GPT-4, there has been significant advancement in decision-making, explainability, and problem-solving abilities.
• Future directions point towards not just imitating human behavior but infusing “human-like” at- tributes into AI, making them more versatile and widely accessible.
• Addressing misconceptions about the models and interdisciplinary collaboration is also vital to foster its growth and broader application."
282,machinelearning,gpt-4,relevance,2023-05-10 08:11:24,[D] When will a GPT-like model outperform Stockfish in chess?,ThePerson654321,False,0.49,0,13dk32o,https://www.reddit.com/r/MachineLearning/comments/13dk32o/d_when_will_a_gptlike_model_outperform_stockfish/,55,1683706284.0,"Hello!

GPT-4 has shown significant improvement over GPT-3 in terms of playing chess, with what I estimate to be a ELO rating of around 1000 now. While this is impressive, it is still nowhere near the performance level of Stockfish. This has led me to ponder the following question:

How many years do you think it will take for a GPT-like model (not specifically or even intentionally trained for chess) to surpass Stockfish in terms of chess-playing abilities?"
283,machinelearning,gpt-4,relevance,2023-12-30 01:14:59,[R] Large Language Models World Chess Championship 🏆♟️,PerformanceRound7913,False,0.78,39,18u31w8,https://www.reddit.com/r/MachineLearning/comments/18u31w8/r_large_language_models_world_chess_championship/,12,1703898899.0,"Exploring the emergent abilities of Large Language Models (LLM) through the strategic lens of chess, orchestrating the inaugural LLM World Chess Championship.  
This tournament featured a Round Robin format where titans of large language models: OpenAI’s GPT-4 Turbo, & GPT-3.5 Turbo, Google DeepMind's Gemini-Pro, and Mistral AI's Mixtral-8x7B, competed against each other.

In the championship, each LLM played 30 games against other LLMs, alternating between black and white.

The ""Chain of thoughts with self-reflection"" one-shot prompt was used for each model. The python-chess library was employed to ensure compliance with official chess rules.

GPT-4 Turbo claimed the championship, while Gemini-Pro, despite significant claims from Google, encountered reasoning challenges and underperformed. Mixtral exceeded expectations with its advanced reasoning abilities. For a comprehensive view of the competition, please see the championship's [league table](https://media.licdn.com/dms/image/D4E22AQGPJ1JOd2795w/feedshare-shrink_1280/0/1703726858664?e=1706745600&v=beta&t=e8ldnksKJWZeqUAuy5kQGRIkbVypDZxy4Yc0imyrDAA).

Look forward to a detailed blog post, an arXiv paper outlining the methodologies and findings, a GitHub repository, PGN files, [games videos](https://media.licdn.com/dms/image/D4E2CAQGtX2pRUjhCyg/comment-image-shrink_8192_1280/0/1703781220530?e=1704556800&v=beta&t=wa4xHwonU_x-g-FZ5nhqw0M7pnpLirileJWcTMyD_3o) and a lichess link with expert commentary.

[https://www.linkedin.com/posts/sherazmit\_llm-prompt-chess-activity-7146175489622097920-SVTV](https://www.linkedin.com/posts/sherazmit_llm-prompt-chess-activity-7146175489622097920-svtv)

&#x200B;"
284,machinelearning,gpt-4,relevance,2023-11-22 17:07:44,[D] The Status of Open Source Code LLMs,RealAGIFan,False,0.89,28,181e4kl,https://www.reddit.com/r/MachineLearning/comments/181e4kl/d_the_status_of_open_source_code_llms/,6,1700672864.0,"  I've been pondering something recently. Did you notice that achieving over 70% on the well-known HumanEval pass@1 hasn't been making major headlines? Models like WizardCoderV2, Phind, Deepseek, and XwinCoder have all surpassed the 67% reported in GPT-4’s report. Some of them are even closely tailing the 82% of GPT-4 API’s. So, are these models really performing that well?  
 Here's something intriguing: I found this image in the latest release of XwinCoder’s repo:  [Xwin-LM/Xwin-Coder at main · Xwin-LM/Xwin-LM (github.com)](https://github.com/Xwin-LM/Xwin-LM/tree/main/Xwin-Coder) 

&#x200B;

[Results in XwinCoder repo](https://preview.redd.it/zr1ov5sykx1c1.png?width=1000&format=png&auto=webp&s=1f25f625fee49f4484f40930ff6d5b6af1439301)

 

It shows that GPT-4 achieves a 60% pass@1 on APPS-introductory, which is higher than CodeLLaMA-34B’s pass@100 (56.3) and XwinCoder-34B’s pass@5 (43.0). Interesting, isn't it?  
 This suggests that judging a model based on a single benchmark might not provide the full picture. This leads me to a couple of questions:

1. What exactly is the gap here? How can we definitively say one model outperforms another?
2. How are other recent models performing on benchmarks like APPS and DS1000?

I'm interested in hearing your thoughts on this. Has anyone experimented with these new models? What was your experience like?"
285,machinelearning,gpt-4,relevance,2024-02-01 17:28:48,[P] Looking for some Papers for Datasets generated through GPT 4,Conclusion_Silent,False,0.29,0,1aggfth,https://www.reddit.com/r/MachineLearning/comments/1aggfth/p_looking_for_some_papers_for_datasets_generated/,0,1706808528.0,"Hello, is anyone aware of notable projects or papers in which Q/A datasets or other datasets for reasoning and inference have been generated using GPT-4 or other large language models, as opposed to being created by humans or through crowdsourcing?"
286,machinelearning,gpt-4,relevance,2023-03-17 09:59:59,[D] PyTorch 2.0 Native Flash Attention 32k Context Window,super_deap,False,0.98,350,11tmpc5,https://www.reddit.com/r/MachineLearning/comments/11tmpc5/d_pytorch_20_native_flash_attention_32k_context/,94,1679047199.0,"Hi,

I did a quick experiment with Pytorch 2.0 Native scaled\_dot\_product\_attention. I was able to a single forward pass within 9GB of memory which is astounding. I think by patching existing Pretrained GPT models and adding more positional encodings, one could easily fine-tune those models to 32k attention on a single A100 80GB. Here is the code I used:

&#x200B;

https://preview.redd.it/6csxe28lv9oa1.png?width=607&format=png&auto=webp&s=ff8b48a77f49fab7d088fd8ba220f720860249bc

I think it should be possible to replicate even GPT-4 with open source tools something like Bloom + FlashAttention & fine-tune on 32k tokens.

**Update**: I was successfully able to start the training of GPT-2 (125M) with a context size of 8k and batch size of 1 on a 16GB GPU. Since memory scaled linearly from 4k to 8k. I am expecting, 32k would require \~64GB and should train smoothly on A100 80 GB. Also, I did not do any other optimizations. Maybe 8-bit fine-tuning can further optimize it.

**Update 2**: I basically picked Karpaty's nanoGPT and patched the pretrained GPT-2 by repeating the embeddings N-times. I was unable to train the model at 8k because generation would cause the crash.  So I started the training for a context window of 4k on The Pile: 1 hour in and loss seems to be going down pretty fast. Also Karpaty's generate function is super inefficient, O(n\^4) I think so it took forever to generate even 2k tokens. So I generate 1100 tokens just to see if the model is able to go beyond 1k limit. And it seems to be working. [Here are some samples](https://0bin.net/paste/O-+eopaW#nmtzX1Re7f1Nr-Otz606jkltvKk/kUXY96/8ca+tb4f) at 3k iteration.

&#x200B;

https://preview.redd.it/o2hb25w1sboa1.png?width=1226&format=png&auto=webp&s=bad2a1e21e218512b0f630c947ee41dba9b86a44

**Update 3**: I have started the training and I am publishing the training script if anyone is interested in replicating or building upon this work. Here is the complete training script:

[https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c](https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c)

I will post an update after the weekend once the training has progressed somewhat.

**Post-Weekend Update**: After \~50k iterations (the model has seen \~200 million tokens, I know this is just too small compared to 10s of billions trained by giga corps), loss only dropped from 4.6 to 4.2 on The Pile:

https://preview.redd.it/vi0fpskhsuoa1.png?width=1210&format=png&auto=webp&s=9fccc5277d91a6400adc6d968b0f2f0ff0da2afc

AFAIR, the loss of GPT-2 on the Pile if trained with 1024 tokens is \~2.8. It seems like the size of the dimension for each token is kind of limiting how much loss can go down since GPT-2 (small) has an embedding dimension of 768. Maybe someone can experiment with GPT-2 medium etc. to see how much we can improve. This is confirmation of the comment by u/lucidraisin [below](https://www.reddit.com/r/MachineLearning/comments/11tmpc5/comment/jcl2rkh/?utm_source=reddit&utm_medium=web2x&context=3)."
287,machinelearning,gpt-4,relevance,2023-09-03 22:05:23,[R] How susceptible are LLMs to Logical Fallacies?,Amir-AI,False,0.79,15,1699u9x,https://www.reddit.com/r/MachineLearning/comments/1699u9x/r_how_susceptible_are_llms_to_logical_fallacies/,3,1693778723.0,"**paper** [https://arxiv.org/abs/2308.09853](https://arxiv.org/abs/2308.09853)

**abstract.**

This paper investigates the rational thinking capability of Large Language Models (LLMs) in multi-round argumentative debates by exploring the impact of fallacious arguments on their logical reasoning performance. More specifically, we present Logic Competence Measurement Benchmark (LOGICOM), a diagnostic benchmark to assess the robustness of LLMs against logical fallacies. LOGICOM involves two agents: a persuader and a debater engaging in a multi-round debate on a controversial topic, where the persuader tries to convince the debater of the correctness of its claim. First, LOGICOM assesses the potential of LLMs to change their opinions through reasoning. Then, it evaluates the debater’s performance in logical reasoning by contrasting the scenario where the persuader employs logical fallacies against one where logical reasoning is used. We use this benchmark to evaluate the performance of GPT-3.5 and GPT-4 using a dataset containing controversial topics, claims, and reasons supporting them. Our findings indicate that both GPT-3.5 and GPT-4 can adjust their opinion through reasoning. However, when presented with logical fallacies, GPT-3.5 and GPT-4 are erroneously convinced 41% and 69% more often, respectively, compared to when logical reasoning is used. Finally, we introduce a new dataset containing over 5k pairs of logical vs. fallacious arguments. The source code and dataset of this work are made publicly available.

[GPT3.5 vulnerable to false information generated by itself!](https://preview.redd.it/3ih6wjg664mb1.jpg?width=2682&format=pjpg&auto=webp&s=f2189751fb8c53e566ffe7395847e71b698a77fb)"
288,machinelearning,gpt-4,relevance,2023-04-20 15:35:12,[R]Comprehensive List of Instruction Datasets for Training LLM Models (GPT-4 & Beyond),TabascoMann,False,0.96,213,12t4ylu,https://www.reddit.com/r/MachineLearning/comments/12t4ylu/rcomprehensive_list_of_instruction_datasets_for/,18,1682004912.0,"Hallo guys 👋, I've put together an extensive collection of datasets perfect for experimenting with your own LLM (MiniGPT4, Alpaca, LLaMA) model and beyond ([**https://github.com/yaodongC/awesome-instruction-dataset**](https://github.com/yaodongC/awesome-instruction-dataset)) .

What's inside?

* A list of datasets for training language models on diverse instruction-turning tasks
* Resources tailored for multi-modal models, allowing integration with text and image inputs
* Constant updates to ensure you have access to the latest and greatest datasets in the field

This repository is designed to provide a one-stop solution for all your LLM dataset needs! 🌟 

 If you've been searching for resources to advance your own LLM projects or simply want to learn more about these cutting-edge models, this repository might help you :) 

I'd love to make this resource even better. So if you have any suggestions for additional datasets or improvements, please don't hesitate to contribute to the project or just comment below!!!

Happy training! 🚀

GitHub Repository: [**https://github.com/yaodongC/awesome-instruction-dataset**](https://github.com/yaodongC/awesome-instruction-dataset)"
289,machinelearning,gpt-4,relevance,2024-01-06 16:23:04,[D] Incredible results with Long Agent Tree Search with open source models,ArtZab,False,0.98,118,1903k24,https://www.reddit.com/r/MachineLearning/comments/1903k24/d_incredible_results_with_long_agent_tree_search/,9,1704558184.0,"Hello,

I saw GPT-4 with Long Agent Tree Search topping the HumanEval with a 94.4% pass@1 for a few weeks now. [https://paperswithcode.com/sota/code-generation-on-humaneval](https://paperswithcode.com/sota/code-generation-on-humaneval)

&#x200B;

The authors of the [original paper](https://arxiv.org/abs/2310.04406) posted their code in their [official github repo](https://github.com/andyz245/LanguageAgentTreeSearch) . I had to change some code to try it out with CodeLlama-7b and the human eval with pass@1 and only 2 max iterations increases HumanEval score from 37% to about 70%.

This is some incredible results in my opinion because this score is higher than GPT-3.5 with only a 7b model. I assume more testing has to be done, but nevertheless I am surprised people are not talking more about this."
290,machinelearning,gpt-4,relevance,2023-07-04 19:16:35,[P] Nuggt: A LLM Agent that runs on Wizcoder-15B (4-bit Quantised). It's time to democratise LLM Agents,WolfPossible5371,False,0.78,49,14qo8a2,https://www.reddit.com/r/MachineLearning/comments/14qo8a2/p_nuggt_a_llm_agent_that_runs_on_wizcoder15b_4bit/,37,1688498195.0,"Hi everyone,

I wanted to share my open source project Nuggt.

In the past few months, we have seen a lot projects regarding Autonomous Agents that run on Large Language Models. Some examples are BabyAGI, Auto-GPT, etc

However, most of these models use GPT-4 which is very expensive and not everyone has access to GPT-4.

So we decided to play around with some open source LLM models that could run locally. We wanted to explore if we could create agents with these open source models and have them perform well...

Long story short after trying out many models like Vicuna-13B, MPT-13B, StarCoder... most of them failed.

Today, I have finally found our winner Wizcoder-15B (4-bit quantised). Here is a demo for you. In this demo, the agent trains RandomForest on Titanic dataset and saves the ROC Curve.

[A LLM Agent training RandomForest on Titanic dataset](https://i.redd.it/ayafi6j5u2ab1.gif)

[Check out the Github Repository](https://github.com/Nuggt-dev/Nuggt)

[Join the Discord](https://discord.com/invite/YZp6jmFr)

\[EDIT\]: The previous post was not clear as rightly pointed out by many so I have made the post shorter."
291,machinelearning,gpt-4,relevance,2023-06-10 02:44:21,[P] Automate any task with a single AI command (Open Source),Loya_3005,False,0.82,104,145ofdc,https://www.reddit.com/r/MachineLearning/comments/145ofdc/p_automate_any_task_with_a_single_ai_command_open/,21,1686365061.0,"In the LLM Agents Community, there is a growing trend of utilizing high-powered models like GPT-4 for building platforms that tackle complex tasks. However, this approach is neither cost-effective nor feasible for many open-source community developers due to the associated expenses. In response, Nuggt emerges as an open-source project aiming to provide a platform for deploying agents to solve intricate tasks while relying on smaller and less resource-intensive LLMs. We strive to make task automation accessible and affordable for all developers in the community.

&#x200B;

[Nuggt Demo](https://reddit.com/link/145ofdc/video/iqvddivzt35b1/player)

While our current implementation leverages the power of GPT-3.5 (already a huge reduction from GPT-4 alternative), we recognise the need for cost-effective solutions without compromising functionality. Our ongoing efforts involve exploring and harnessing the potential of smaller models like Vicuna 13B, ensuring that task automation remains accessible to a wider audience.

🔗 Find Nuggt on GitHub: [**Nuggt GitHub Repository**](https://github.com/Nuggt-dev/Nuggt)

🔎 **Call for Feedback**: We invite the community to try out Nuggt and provide valuable feedback. Let us know your thoughts, suggestions, and any improvements you'd like to see. Your feedback will help us shape the future of Nuggt and make it even better.

💡 **Contributors Wanted**: We believe in the power of collaboration! If you're passionate about automation, AI, or open-source development, we welcome your contributions to Nuggt. Whether it's code improvements, new features, or documentation enhancements, your contributions will make a difference.

🌟 Join the Nuggt Community: Get involved, contribute, and join the discussions on our [**GitHub repository**](https://github.com/Nuggt-dev/Nuggt). We're building a vibrant community, and we'd love to have you on board!"
292,machinelearning,gpt-4,relevance,2023-03-15 11:22:32,[D] ChatGPT Plus waitlist,blabboy,False,0.5,0,11rthqf,https://www.reddit.com/r/MachineLearning/comments/11rthqf/d_chatgpt_plus_waitlist/,9,1678879352.0,"I was surprised to find that ChatGPT plus (currently the only way to test a vanilla GPT-4 model) is not only behind a pay wall, it is also behind a ""wait wall""!

Has anyone played with GPT-4 yet? Is it as good as the paper suggests? Anyone got any idea how long the wait list is for access?"
293,machinelearning,gpt-4,relevance,2023-09-28 18:34:49,"[D] How do we know Closed source released benchmarks aren't being heavily optimized, through outside means?",vatsadev,False,0.84,30,16unpn9,https://www.reddit.com/r/MachineLearning/comments/16unpn9/d_how_do_we_know_closed_source_released/,10,1695926089.0,"I've recently started working with ML and NLP, so I'm sorry if this sounds Naive.

Unlike Llama 2 or other open source, we don't have access to the model weights for GPT-4, Claude or Bard, so Benchmark Evals are being run through either APIs or the chat Interface. So how do we know that the model isn't being Boosted by custom web-searching abilities or RAG? While GPT-4 might have a turnoff option, I'm pretty sure Bard is always online, being built by google. So how do we trust benchmarks? Also, have any opensource been tested after Websearch/RAG?"
294,machinelearning,gpt-4,relevance,2023-05-17 04:57:04,Perplexity AI: Strengths & Limitations [Discussion] [Research],JueDarvyTheCatMaster,False,0.89,27,13jrwe0,https://www.reddit.com/r/MachineLearning/comments/13jrwe0/perplexity_ai_strengths_limitations_discussion/,14,1684299424.0,"There are many different conversational AI being released due to the immense emphasis that **ChatGPT** has put on AI technology. **In the next few weeks** I will be working with and analyzing a myriad of different ones to see the **strengths, limitations, and best applications** for different AI. I will mostly stick to AI that is free to use at the moment so nothing like GPT-4 **... yet at least**. Although I will be comparing these AI models to GPT-4 and other GPT models to some extent.

[This is what Perplexity AI looks like when you open it up. Essentially it has many threads, popular topics, and a chat box. It is quite clean in my opinion. :\)](https://preview.redd.it/cwfzf8taib0b1.png?width=1365&format=png&auto=webp&s=4dc1c710b225b1b30ccb900496b23e931305bc3e)

Alright, let's get started with Perplexity AI. So this has actually been around for a while and I've actually used it for quite a while now. In the time prior to Bing Chat and GPT-4 ""browsing on"" this was really the only model that incorporated web search and the citing of sources which I believe as crucial for credible work and credit given where credit is due. **At the time it had a 250 character limit for prompts but now I believe that has changed.**

**Strengths & Feats:**

1. Coding with online sources

https://preview.redd.it/x4k1gi0kib0b1.png?width=891&format=png&auto=webp&s=cdb62d3cbec11aa7faa710359d211c5aad4fe95f

https://preview.redd.it/gt3jii0kib0b1.png?width=892&format=png&auto=webp&s=102c484759b48a56228c42b0e80ce1bfaea6f559

Although its raw capabilities might not be as strong as ChatGPT or GPT-4 in terms of coding strength the ability to look up how to write these things and the most efficient ways to write them gives it an **edge of those AIs** in *certain scenarios* but not **all** the time.

&#x200B;

2. **Hard Math Problems** (Courtesy of WolframAlpha)

[Although WolframAlpha is a good AI that I can go to the website of, I actually prefer the cleaner UI interface of Perplexity AI and the graphs it gives.](https://preview.redd.it/pgqexxvwib0b1.png?width=562&format=png&auto=webp&s=406faa6a0283e35389b7dfdda3bcf32b4ec8fb52)

Although ChatGPT and GPT-4 are extremely powerful tools who can have better computational power than us, they often fall short by making logical or calculation mistakes along the way. **The ability for Perplexity AI to search the internet to find the answer is much more powerful in this situation.** Even other AI like Bing AI and phind sometimes don't specifically go to WolframAlpha and therefore, get it wrong since WolframAlpha usually is more credible and stronger than other websites (except for specific math problems that use a specific degree of understanding beyond even the strongest AI ex. S*ome Calculus limits, check out BlackPenRedPen if you are interested in this*). 

3. **Specific settings or modes** (EZ Searching)

Additionally, the different settings or modes (ex. **Internet, Academic, WolframAlpha, etc.**) allow for easy switching of locations to search without having to specify specific websites or sources to look compared to other AI website models like **phind** (*I will do a analysis of that AI soon as well*) who have that problem.

[This shows all the specific categories that I described above \(You can see that it is very convenient\). ALSO, you see that \\""quick\\"" with the drop down menu thingy? You can turn on ENHANCED with only 20 uses per day anused  GPT-4 and generally a more improved, optimized model for searches and answers.](https://preview.redd.it/knx3165fkb0b1.png?width=937&format=png&auto=webp&s=7c8af282b31019e74502fc4f406c2a3b7485458f)

4. \[BONUS\] Reddit Searching (lol here)

[Not much to say here, just it can search reddit and does it quite effectively in my opinion compared to Bing AI or phind \(lackluster for the others\)](https://preview.redd.it/2nm93bsalb0b1.png?width=873&format=png&auto=webp&s=7e4c0b35a17e294f58cb60aa44619f034008a117)

5. \[HARD TO DESCRIBE\] **Finding local companies / sources that meet a certain criteria**

https://preview.redd.it/f5epikx2mb0b1.png?width=937&format=png&auto=webp&s=65aaa8755d321c59407aa0ec88a67e587e5870d2

[This one is hard to describe. Putting the same prompt into phind let to some lackluster results where they might accidentally put a company that wasn't in San Diego or government programs despite my need for a company. Not entirely sure why PerplexityAI did such a good job at this but still pretty cool.](https://preview.redd.it/0akd1z5elb0b1.png?width=937&format=png&auto=webp&s=bdbefe48372812cb409fa3aa2b88450688d5856a)

I don't really know how to categorize these types of prompts, topics, or queries but essentially finding some intricate details especially around a **location** seems to be a strong such of PerplexityAI. 

**I would also like to point out that PerplexityAI  front is extremely good at finding new sources or websites for research or other purposes.** There have been many times where asking for info about a company leads to a directory of other companies in the same sector which is **extremely good for us**.

6. **Detailed vs. Concise Responses** (Doesn't drag on for too long, usually has one of the FASTEST response times in comparison to phind, BingAI, GPT, and Llama)

[Concise](https://preview.redd.it/9mi0w6akmb0b1.png?width=937&format=png&auto=webp&s=d6179016f73964012d13b601614ea07febd0fd50)

[Detailed](https://preview.redd.it/hlc4i46bob0b1.png?width=476&format=png&auto=webp&s=17bc0b6b86d6cef07751283a399c70d2304e3b82)

Not too much but still interesting in my opinion.

&#x200B;

AND NOW ........................

&#x200B;

**Limitations & Weaknesses:**

1. Complicated Science that isn't explicitly stated in search results **(outside WolframAlpha's math centric capabilities)** & ***Problem Solving***

[phind has no problem giving an attempt at this problem while PerplexityAI is completely stuck.](https://preview.redd.it/3btnzjv5mb0b1.png?width=937&format=png&auto=webp&s=c51904b5b0c42bcd744353aa53c6e75aa9b41a00)

\*Granted this problem is quite difficult.

Despite this, it seems to lack that type of inquisitiveness or ""let's give it go even if it is wrong attitude"" that ChatGPT has. It's hard to describe but essentially I wouldn't suggest using PerplexityAI to solve problems outside of math (you can use WolframAlpha). **Also its responses don't seem to allow for as much problem solving and long answers that other AIs are capable of.** *At least for now.*

**2. Inconsistency & Too Short Responses**

https://preview.redd.it/frxjtqozmb0b1.png?width=1062&format=png&auto=webp&s=e1340c2bb74c0f7369a7eac7c64dfbd7ad59578f

https://preview.redd.it/dhtxybc8nb0b1.png?width=1062&format=png&auto=webp&s=53f2283eded28820d39eadc792781ff369ec0257

Other limitations I have noticed is TOO narrow searching (leading to the inability to correlate phone numbers with companies or etc.), hard to force formatting, and unexpected answering format or style.

**Keep in mind these limitations exist with other AI as well.**

Overall, **PerplexityAI** is interesting and you should definitely try it out since it is free. For me I usually use it *alongside* GPT-3 for sources or initial background so ChatGPT is not inaccurate or spitting out misinformation with a 100% convincing tone."
295,machinelearning,gpt-4,relevance,2023-03-30 22:40:29,[P] Introducing Vicuna: An open-source language model based on LLaMA 13B,Business-Lead2679,False,0.95,288,1271po7,https://www.reddit.com/r/MachineLearning/comments/1271po7/p_introducing_vicuna_an_opensource_language_model/,107,1680216029.0,"We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%\* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%\* of cases. The cost of training Vicuna-13B is around $300. The training and serving [code](https://github.com/lm-sys/FastChat), along with an online [demo](https://chat.lmsys.org/), are publicly available for non-commercial use.

# Training details

Vicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model’s maximum context length.

Our training recipe builds on top of [Stanford’s alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) with the following improvements.

* **Memory Optimizations:** To enable Vicuna’s understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing [gradient checkpointing](https://arxiv.org/abs/1604.06174) and [flash attention](https://arxiv.org/abs/2205.14135).
* **Multi-round conversations:** We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot’s output.
* **Cost Reduction via Spot Instance:** The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ [SkyPilot](https://github.com/skypilot-org/skypilot) [managed spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html) to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.

&#x200B;

[Vicuna - Online demo](https://reddit.com/link/1271po7/video/0qsiu08kdyqa1/player)

# Limitations

We have noticed that, similar to other large language models, Vicuna has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. Additionally, it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI [moderation](https://platform.openai.com/docs/guides/moderation/overview) API to filter out inappropriate user inputs in our online demo. Nonetheless, we anticipate that Vicuna can serve as an open starting point for future research to tackle these limitations.

[Relative Response Quality Assessed by GPT-4](https://preview.redd.it/1rnmhv01eyqa1.png?width=599&format=png&auto=webp&s=02b4d415b5d378851bb70e225f1b1ebce98bfd83)

&#x200B;

For more information, check [https://vicuna.lmsys.org/](https://vicuna.lmsys.org/)

Online demo: [https://chat.lmsys.org/](https://chat.lmsys.org/)

&#x200B;

All credits go to the creators of this model. I did not participate in the creation of this model nor in the fine-tuning process. Usage of this model falls under a non-commercial license."
296,machinelearning,gpt-4,relevance,2023-05-22 14:35:46,[P] Improving GPT-4's codebase understanding with ctags,rinconcam,False,0.6,2,13osoij,https://www.reddit.com/r/MachineLearning/comments/13osoij/p_improving_gpt4s_codebase_understanding_with/,0,1684766146.0,"aider is a command-line chat tool that allows you to write and edit code with GPT-4 in the terminal. 

aider has a new experimental feature that utilizes `ctags` to provide GPT-4 with a concise ""map"" of your whole git repository, including all declared variables and functions with call signatures.  This ""repo map"" enables GPT to better comprehend, navigate and edit the code in larger repos.

More details on improving GPT-4's codebase understanding with ctags:

https://github.com/paul-gauthier/aider/blob/main/docs/ctags.md"
297,machinelearning,gpt-4,relevance,2023-05-02 00:07:57,[D] Does GPT-4-32k eliminates/reduces the use of chunk strategies?,Adorapa,False,0.87,32,1355rhf,https://www.reddit.com/r/MachineLearning/comments/1355rhf/d_does_gpt432k_eliminatesreduces_the_use_of_chunk/,14,1682986077.0,"There's an article in Pinecone called ""[Chunking Strategies for LLM Applications](https://www.pinecone.io/learn/chunking-strategies/?utm_content=244745025&utm_medium=social&utm_source=twitter&hss_channel=tw-1287624141001109504)"" that states that the optimal chunk size is around 256 or 512 tokens. I've been using the chunk strategy to work with large files. 

Now having GPT-4 with a token limit of 32K I can paste most of the documents I use. And then theres this paper:  [""Scaling Transformer to 1M tokens...""](https://arxiv.org/pdf/2304.11062.pdf). This might take a little bit more... I'm just confused (and overwhelmed by the pace of AI). Should I stuck with chunking data? Or do you think it's a temporary strategy that will be replaced in the coming months?"
298,machinelearning,gpt-4,relevance,2023-04-19 09:21:07,[P] LoopGPT: A Modular Auto-GPT Framework,farizrahman4u,False,0.91,99,12rn33g,https://www.reddit.com/r/MachineLearning/comments/12rn33g/p_loopgpt_a_modular_autogpt_framework/,26,1681896067.0," 

[https://github.com/farizrahman4u/loopgpt](https://github.com/farizrahman4u/loopgpt)

&#x200B;

LoopGPT is a re-implementation of the popular [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) project as a proper python package, written with modularity and extensibility in mind.

## Features 

* **""Plug N Play"" API** \- Extensible and modular ""Pythonic"" framework, not just a command line tool. Easy to add new features, integrations and custom agent capabilities, all from python code, no nasty config files!
* **GPT 3.5 friendly** \- Better results than Auto-GPT for those who don't have GPT-4 access yet!
* **Minimal prompt overhead** \- Every token counts. We are continuously working on getting the best results with the least possible number of tokens.
* **Human in the Loop** \- Ability to ""course correct"" agents who go astray via human feedback.
* **Full state serialization** \- Pick up where you left off; L♾️pGPT can save the complete state of an agent, including memory and the states of its tools to a file or python object. No external databases or vector stores required (but they are still supported)!"
299,machinelearning,gpt-4,relevance,2023-04-18 18:17:47,"[R] ChemCrow: Augmenting large-language models with chemistry tools - Andres M Bran et al , Laboratory of Artificial Chemical Intelligence et al - Automating chemistry work with tool assisted LLMs",Singularian2501,False,0.92,34,12qyzth,https://www.reddit.com/r/MachineLearning/comments/12qyzth/r_chemcrow_augmenting_largelanguage_models_with/,0,1681841867.0,"Paper: [https://arxiv.org/abs/2304.05376v2](https://arxiv.org/abs/2304.05376v2) 

Twitter: [https://twitter.com/andrewwhite01/status/1645945791540854785?s=20](https://twitter.com/andrewwhite01/status/1645945791540854785?s=20) 

Abstract:

>Large-language models (LLMs) have recently shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these **models lack access to external knowledge sources, limiting their usefulness in scientific applications.** In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. **By integrating 13 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge.** Our evaluation, including both LLM and expert human assessments, demonstrates **ChemCrow's effectiveness in automating a diverse set of chemical tasks.** Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and GPT-4 + ChemCrow performance. There is a significant risk of misuse of tools like ChemCrow and we discuss their potential harms. Employed responsibly, ChemCrow not only aids expert chemists and lowers barriers for non-experts, but also **fosters scientific advancement by bridging the gap between experimental and computational chemistry.** 

https://preview.redd.it/x0zp6m2npoua1.jpg?width=1415&format=pjpg&auto=webp&s=90f000706e85707f718b24f182f830943f0c0115

https://preview.redd.it/imolno2npoua1.jpg?width=1413&format=pjpg&auto=webp&s=60b125b6a60b1fc13f393764994cedab264303df

https://preview.redd.it/jfbqgo2npoua1.jpg?width=1020&format=pjpg&auto=webp&s=46033b8155e3f24e77bcf382ef4a15f3a0ab5538"
