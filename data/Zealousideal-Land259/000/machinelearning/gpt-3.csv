,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,id,url,num_comments,created,body
0,machinelearning,gpt-3,top,2023-01-08 18:23:03,"[P] I built Adrenaline, a debugger that fixes errors and explains them with GPT-3",jsonathan,False,0.96,1563,106q6m9,https://i.redd.it/8t0k9jkd3vaa1.gif,92,1673202183.0,
1,machinelearning,gpt-3,top,2022-08-07 21:25:26,[D] The current and future state of AI/ML is shockingly demoralizing with little hope of redemption,Flaky_Suit_8665,False,0.88,1435,wiqjxv,https://www.reddit.com/r/MachineLearning/comments/wiqjxv/d_the_current_and_future_state_of_aiml_is/,398,1659907526.0,"I recently encountered the PaLM (Scaling Language Modeling with Pathways) paper from Google Research and it opened up a can of worms of ideas I’ve felt I’ve intuitively had for a while, but have been unable to express – and I know I can’t be the only one. Sometimes I wonder what the original pioneers of AI – Turing, Neumann, McCarthy, etc. – would think if they could see the state of AI that we’ve gotten ourselves into. 67 authors, 83 pages, 540B parameters in a model, the internals of which no one can say they comprehend with a straight face, 6144 TPUs in a commercial lab that no one has access to, on a rig that no one can afford, trained on a volume of data that a human couldn’t process in a lifetime, 1 page on ethics with the same ideas that have been rehashed over and over elsewhere with no attempt at a solution – bias, racism, malicious use, etc. – for purposes that who asked for?

When I started my career as an AI/ML research engineer 2016, I was most interested in two types of tasks – 1.) those that most humans could do but that would universally be considered tedious and non-scalable. I’m talking image classification, sentiment analysis, even document summarization, etc. 2.) tasks that humans lack the capacity to perform as well as computers for various reasons – forecasting, risk analysis, game playing, and so forth. I still love my career, and I try to only work on projects in these areas, but it’s getting harder and harder.

This is because, somewhere along the way, it became popular and unquestionably acceptable to push AI into domains that were originally uniquely human, those areas that sit at the top of Maslows’s hierarchy of needs in terms of self-actualization – art, music, writing, singing, programming, and so forth. These areas of endeavor have negative logarithmic ability curves – the vast majority of people cannot do them well at all, about 10% can do them decently, and 1% or less can do them extraordinarily. The little discussed problem with AI-generation is that, without extreme deterrence, we will sacrifice human achievement at the top percentile in the name of lowering the bar for a larger volume of people, until the AI ability range is the norm. This is because relative to humans, AI is cheap, fast, and infinite, to the extent that investments in human achievement will be watered down at the societal, educational, and individual level with each passing year. And unlike AI gameplay which superseded humans decades ago, we won’t be able to just disqualify the machines and continue to play as if they didn’t exist.

Almost everywhere I go, even this forum, I encounter almost universal deference given to current SOTA AI generation systems like GPT-3, CODEX, DALL-E, etc., with almost no one extending their implications to its logical conclusion, which is long-term convergence to the mean, to mediocrity, in the fields they claim to address or even enhance. If you’re an artist or writer and you’re using DALL-E or GPT-3 to “enhance” your work, or if you’re a programmer saying, “GitHub Co-Pilot makes me a better programmer?”, then how could you possibly know? You’ve disrupted and bypassed your own creative process, which is thoughts -> (optionally words) -> actions -> feedback -> repeat, and instead seeded your canvas with ideas from a machine, the provenance of which you can’t understand, nor can the machine reliably explain. And the more you do this, the more you make your creative processes dependent on said machine, until you must question whether or not you could work at the same level without it.

When I was a college student, I often dabbled with weed, LSD, and mushrooms, and for a while, I thought the ideas I was having while under the influence were revolutionary and groundbreaking – that is until took it upon myself to actually start writing down those ideas and then reviewing them while sober, when I realized they weren’t that special at all. What I eventually determined is that, under the influence, it was impossible for me to accurately evaluate the drug-induced ideas I was having because the influencing agent the generates the ideas themselves was disrupting the same frame of reference that is responsible evaluating said ideas. This is the same principle of – if you took a pill and it made you stupider, would even know it? I believe that, especially over the long-term timeframe that crosses generations, there’s significant risk that current AI-generation developments produces a similar effect on humanity, and we mostly won’t even realize it has happened, much like a frog in boiling water. If you have children like I do, how can you be aware of the the current SOTA in these areas, project that 20 to 30 years, and then and tell them with a straight face that it is worth them pursuing their talent in art, writing, or music? How can you be honest and still say that widespread implementation of auto-correction hasn’t made you and others worse and worse at spelling over the years (a task that even I believe most would agree is tedious and worth automating).

Furthermore, I’ve yet to set anyone discuss the train – generate – train - generate feedback loop that long-term application of AI-generation systems imply. The first generations of these models were trained on wide swaths of web data generated by humans, but if these systems are permitted to continually spit out content without restriction or verification, especially to the extent that it reduces or eliminates development and investment in human talent over the long term, then what happens to the 4th or 5th generation of models? Eventually we encounter this situation where the AI is being trained almost exclusively on AI-generated content, and therefore with each generation, it settles more and more into the mean and mediocrity with no way out using current methods. By the time that happens, what will we have lost in terms of the creative capacity of people, and will we be able to get it back?

By relentlessly pursuing this direction so enthusiastically, I’m convinced that we as AI/ML developers, companies, and nations are past the point of no return, and it mostly comes down the investments in time and money that we’ve made, as well as a prisoner’s dilemma with our competitors. As a society though, this direction we’ve chosen for short-term gains will almost certainly make humanity worse off, mostly for those who are powerless to do anything about it – our children, our grandchildren, and generations to come.

If you’re an AI researcher or a data scientist like myself, how do you turn things back for yourself when you’ve spent years on years building your career in this direction? You’re likely making near or north of $200k annually TC and have a family to support, and so it’s too late, no matter how you feel about the direction the field has gone. If you’re a company, how do you standby and let your competitors aggressively push their AutoML solutions into more and more markets without putting out your own? Moreover, if you’re a manager or thought leader in this field like Jeff Dean how do you justify to your own boss and your shareholders your team’s billions of dollars in AI investment while simultaneously balancing ethical concerns? You can’t – the only answer is bigger and bigger models, more and more applications, more and more data, and more and more automation, and then automating that even further. If you’re a country like the US, how do responsibly develop AI while your competitors like China single-mindedly push full steam ahead without an iota of ethical concern to replace you in numerous areas in global power dynamics? Once again, failing to compete would be pre-emptively admitting defeat.

Even assuming that none of what I’ve described here happens to such an extent, how are so few people not taking this seriously and discounting this possibility? If everything I’m saying is fear-mongering and non-sense, then I’d be interested in hearing what you think human-AI co-existence looks like in 20 to 30 years and why it isn’t as demoralizing as I’ve made it out to be.

&#x200B;

EDIT: Day after posting this -- this post took off way more than I expected. Even if I received 20 - 25 comments, I would have considered that a success, but this went much further. Thank you to each one of you that has read this post, even more so if you left a comment, and triply so for those who gave awards! I've read almost every comment that has come in (even the troll ones), and am truly grateful for each one, including those in sharp disagreement. I've learned much more from this discussion with the sub than I could have imagined on this topic, from so many perspectives. While I will try to reply as many comments as I can, the sheer comment volume combined with limited free time between work and family unfortunately means that there are many that I likely won't be able to get to. That will invariably include some that I would love respond to under the assumption of infinite time, but I will do my best, even if the latency stretches into days. Thank you all once again!"
2,machinelearning,gpt-3,top,2022-06-03 16:06:33,"[P] This is the worst AI ever. (GPT-4chan model, trained on 3.5 years worth of /pol/ posts)",ykilcher,False,0.96,885,v42pej,https://www.reddit.com/r/MachineLearning/comments/v42pej/p_this_is_the_worst_ai_ever_gpt4chan_model/,169,1654272393.0,"[https://youtu.be/efPrtcLdcdM](https://youtu.be/efPrtcLdcdM)

GPT-4chan was trained on over 3 years of posts from 4chan's ""politically incorrect"" (/pol/) board.

Website (try the model here): [https://gpt-4chan.com](https://gpt-4chan.com)

Model: [https://huggingface.co/ykilcher/gpt-4chan](https://huggingface.co/ykilcher/gpt-4chan)

Code: [https://github.com/yk/gpt-4chan-public](https://github.com/yk/gpt-4chan-public)

Dataset: [https://zenodo.org/record/3606810#.YpjGgexByDU](https://zenodo.org/record/3606810#.YpjGgexByDU)

&#x200B;

OUTLINE:

0:00 - Intro

0:30 - Disclaimers

1:20 - Elon, Twitter, and the Seychelles

4:10 - How I trained a language model on 4chan posts

6:30 - How good is this model?

8:55 - Building a 4chan bot

11:00 - Something strange is happening

13:20 - How the bot got unmasked

15:15 - Here we go again

18:00 - Final thoughts"
3,machinelearning,gpt-3,top,2023-02-11 12:54:26,[P] Introducing arxivGPT: chrome extension that summarizes arxived research papers using chatGPT,_sshin_,False,0.95,834,10zmz2d,https://i.redd.it/jmgr7vsy3kha1.jpg,70,1676120066.0,
4,machinelearning,gpt-3,top,2023-03-22 08:04:01,[D] Overwhelmed by fast advances in recent weeks,iamx9000again,False,0.96,828,11ybjsi,https://www.reddit.com/r/MachineLearning/comments/11ybjsi/d_overwhelmed_by_fast_advances_in_recent_weeks/,331,1679472241.0,"I was watching the GTC keynote and became entirely overwhelmed by the amount of progress achieved from last year.  I'm wondering how everyone else feels.

&#x200B;

Firstly, the entire ChatGPT, GPT-3/GPT-4 chaos has been going on for a few weeks, with everyone scrambling left and right to integrate chatbots into their apps, products, websites. Twitter is flooded with new product ideas, how to speed up the process from idea to product, countless promp engineering blogs, tips, tricks, paid courses.

&#x200B;

Not only was ChatGPT disruptive, but a few days later, Microsoft and Google also released their models and integrated them into their search engines. Microsoft also integrated its LLM into its Office suite. It all happenned overnight. I understand that they've started integrating them along the way, but still, it seems like it hapenned way too fast. This tweet encompases the past few weeks perfectly [https://twitter.com/AlphaSignalAI/status/1638235815137386508](https://twitter.com/AlphaSignalAI/status/1638235815137386508) , on a random Tuesday countless products are released that seem revolutionary.

&#x200B;

In addition to the language models, there are also the generative art models that have been slowly rising in mainstream recognition. Now Midjourney AI is known by a lot of people who are not even remotely connected to the AI space.

&#x200B;

For the past few weeks, reading Twitter, I've felt completely overwhelmed, as if the entire AI space is moving beyond at lightning speed, whilst around me we're just slowly training models, adding some data, and not seeing much improvement, being stuck on coming up with ""new ideas, that set us apart"".

&#x200B;

Watching the GTC keynote from NVIDIA I was again, completely overwhelmed by how much is being developed throughout all the different domains. The ASML EUV (microchip making system) was incredible, I have no idea how it does lithography and to me it still seems like magic. The Grace CPU with 2 dies (although I think Apple was the first to do it?) and 100 GB RAM, all in a small form factor. There were a lot more different hardware servers that I just blanked out at some point. The omniverse sim engine looks incredible, almost real life (I wonder how much of a domain shift there is between real and sim considering how real the sim looks). Beyond it being cool and usable to train on synthetic data, the car manufacturers use it to optimize their pipelines. This change in perspective, of using these tools for other goals than those they were designed for I find the most interesting.

&#x200B;

The hardware part may be old news, as I don't really follow it, however the software part is just as incredible. NVIDIA AI foundations (language, image, biology models), just packaging everything together like a sandwich. Getty, Shutterstock and Adobe will use the generative models to create images. Again, already these huge juggernauts are already integrated.

&#x200B;

I can't believe the point where we're at. We can use AI to write code, create art, create audiobooks using Britney Spear's voice, create an interactive chatbot to converse with books, create 3D real-time avatars, generate new proteins (?i'm lost on this one), create an anime and countless other scenarios. Sure, they're not perfect, but the fact that we can do all that in the first place is amazing.

&#x200B;

As Huang said in his keynote, companies want to develop ""disruptive products and business models"". I feel like this is what I've seen lately. Everyone wants to be the one that does something first, just throwing anything and everything at the wall and seeing what sticks.

&#x200B;

In conclusion, I'm feeling like the world is moving so fast around me whilst I'm standing still. I want to not read anything anymore and just wait until everything dies down abit, just so I can get my bearings. However, I think this is unfeasible. I fear we'll keep going in a frenzy until we just burn ourselves at some point.

&#x200B;

How are you all fairing? How do you feel about this frenzy in the AI space? What are you the most excited about?"
5,machinelearning,gpt-3,top,2023-04-16 19:53:45,[R] Timeline of recent Large Language Models / Transformer Models,viktorgar,False,0.95,766,12omnxo,https://i.redd.it/gl11ce50xaua1.png,86,1681674825.0,
6,machinelearning,gpt-3,top,2023-03-18 10:15:33,[D] Totally Open Alternatives to ChatGPT,KingsmanVince,False,0.98,748,11uk8ti,https://www.reddit.com/r/MachineLearning/comments/11uk8ti/d_totally_open_alternatives_to_chatgpt/,70,1679134533.0,"I have migrated this to GitHub for easy contribution: https://github.com/nichtdax/awesome-totally-open-chatgpt

By alternative, I mean projects feature different language model for chat system.
I do **not** count alternative **frontend** projects because they just call the API from OpenAI. 
I do **not** consider alternative **transformer decoder** to GPT 3.5 either because the training data of them are (mostly) not for chat system.

Tags:

-   B: bare (no data, no model's weight, no chat system)
-   F: full (yes data, yes model's weight, yes chat system including TUI and GUI)

| Project                                                                               | Description                                                                                                                                                                                                                                                                                                                                                                               | Tags |
| ------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- |
| [lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch)       | Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM                                                                                                                                                                                                                                                      | B    |
| [togethercomputer/OpenChatKit](https://github.com/togethercomputer/OpenChatKit)       | OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. [Demo](https://huggingface.co/spaces/togethercomputer/OpenChatKit)                                                                                                                                                                                    | F    |
| [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) | A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.                                                                                                                                                                                                                                                                                    | F    |
| [KoboldAI/KoboldAI-Client](https://github.com/KoboldAI/KoboldAI-Client)               | This is a browser-based front-end for AI-assisted writing with multiple local & remote AI models. It offers the standard array of tools, including Memory, Author's Note, World Info, Save & Load, adjustable AI settings, formatting options, and the ability to import existing AI Dungeon adventures. You can also turn on Adventure mode and play the game like AI Dungeon Unleashed. | F    |
| [LAION-AI/Open-Assistant/](https://github.com/LAION-AI/Open-Assistant/)               | OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.                                                                                                                                                                                                                                     | F    |"
7,machinelearning,gpt-3,top,2021-09-06 13:39:07,[D] How OpenAI Sold its Soul for $1 Billion: The company behind GPT-3 and Codex isn’t as open as it claims.,sensetime,False,0.95,665,pizllt,https://www.reddit.com/r/MachineLearning/comments/pizllt/d_how_openai_sold_its_soul_for_1_billion_the/,107,1630935547.0,"An essay by Alberto Romero that traces the history and developments of OpenAI from the time it became a ""capped-for-profit"" entity from a non-profit entity:

Link: https://onezero.medium.com/openai-sold-its-soul-for-1-billion-cf35ff9e8cd4"
8,machinelearning,gpt-3,top,2023-02-24 17:21:15,[R] Meta AI open sources new SOTA LLM called LLaMA. 65B version (trained on 1.4T tokens) is competitive with Chinchilla and Palm-540B. 13B version outperforms OPT and GPT-3 175B on most benchmarks.,MysteryInc152,False,0.98,619,11awp4n,https://www.reddit.com/r/MachineLearning/comments/11awp4n/r_meta_ai_open_sources_new_sota_llm_called_llama/,213,1677259275.0,"[https://twitter.com/GuillaumeLample/status/1629151231800115202?t=4cLD6Ko2Ld9Y3EIU72-M2g&s=19](https://twitter.com/GuillaumeLample/status/1629151231800115202?t=4cLD6Ko2Ld9Y3EIU72-M2g&s=19)

Paper here - [https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)"
9,machinelearning,gpt-3,top,2021-01-03 20:22:20,[N] CoreWeave has agreed to provide training compute for EleutherAI's open source GPT-3-sized language model,Wiskkey,False,0.98,608,kps6fl,https://i.redd.it/87huzgnpxz861.jpg,26,1609705340.0,
10,machinelearning,gpt-3,top,2023-03-01 18:31:12,[D] OpenAI introduces ChatGPT and Whisper APIs (ChatGPT API is 1/10th the cost of GPT-3 API),minimaxir,False,0.97,573,11fbccz,https://www.reddit.com/r/MachineLearning/comments/11fbccz/d_openai_introduces_chatgpt_and_whisper_apis/,119,1677695472.0,"https://openai.com/blog/introducing-chatgpt-and-whisper-apis

> It is priced at $0.002 per 1k tokens, which is 10x cheaper than our existing GPT-3.5 models.

This is a massive, massive deal. For context, the reason GPT-3 apps took off over the past few months before ChatGPT went viral is because a) text-davinci-003 was released and was a significant performance increase and b) the cost was cut from $0.06/1k tokens to $0.02/1k tokens, which made consumer applications feasible without a large upfront cost.

A much better model and a 1/10th cost warps the economics completely to the point that it may be better than in-house finetuned LLMs.

I have no idea how OpenAI can make money on this. This has to be a loss-leader to lock out competitors before they even get off the ground."
11,machinelearning,gpt-3,top,2022-03-10 14:59:38,"[R] You can't train GPT-3 on a single GPU, but you *can* tune its hyperparameters on one",thegregyang,False,0.98,545,tb0jm6,https://www.reddit.com/r/MachineLearning/comments/tb0jm6/r_you_cant_train_gpt3_on_a_single_gpu_but_you_can/,39,1646924378.0,"> You can't train GPT-3 on a single GPU, much less tune its hyperparameters (HPs).  
>  
>  
But what if I tell you…  
>  
>  
…you \*can\* tune its HPs on a single GPU thanks to new theoretical advances?

Hi Reddit,

I'm excited to share with you our latest work, [\[2203.03466\] Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer (arxiv.org)](https://arxiv.org/abs/2203.03466).

Code: [https://github.com/microsoft/mup](https://t.co/5S0YAghCYx)

  


https://preview.redd.it/nnb2usdjlkm81.png?width=1195&format=png&auto=webp&s=ca9e6d5cddfbea5675cf00854806d5189c3e40bb

(Disclaimer: this post is shamelessly converted from my twitter thread)

The idea is actually really simple: in a special parametrization introduced in [our previous work](https://arxiv.org/abs/2011.14522) ([reddit thread](https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/)) called µP, narrow and wide neural networks share the same set of optimal hyperparameters. This works even as width -> ∞.

&#x200B;

https://preview.redd.it/dqna8guklkm81.png?width=1838&format=png&auto=webp&s=2f7ba582a1cc949461dac8601a896034eaf0ff84

The hyperparameters can include learning rate, learning rate schedule, initialization, parameter multipliers, and more, even individually for each parameter tensor. We empirically verified this on Transformers up to width 4096.

&#x200B;

https://preview.redd.it/rwdsb6snlkm81.jpg?width=2560&format=pjpg&auto=webp&s=c3152f2746132d92dc3788a42aa6926a61d7c46f

Using this insight, we can just tune a tiny version of GPT-3 on a single GPU --- if the hyperparameters we get on the small model is near optimal, then they should also be near optimal on the large model! We call this way of tuning \*µTransfer\*.

&#x200B;

https://preview.redd.it/mi7ibyyolkm81.png?width=1195&format=png&auto=webp&s=144af103b2aaf3ffeb2ccf19aad7565527dbd003

We µTransferred hyperparameters from a small 40 million parameter version of GPT-3 — small enough to fit on a single GPU — to the 6.7 billion version. With some asterisks, we get a performance comparable to the original GPT-3 model with twice the parameter count!

&#x200B;

https://preview.redd.it/rrq2yfwplkm81.png?width=3232&format=png&auto=webp&s=6cf4cc9652db48e12b48a85b2f837e55e64bd09c

The total tuning cost is only 7% of the whole pretrain compute cost! Since the direct tuning of the small model costs roughly the same even as the large model increases in size, tuning the 175B GPT-3 this way would probably cost at most 0.3% of the total pretrain compute.

You: ""wait can I shrink the model only in width?""

Bad news: there's not much theoretical guarantee for non-width stuff

good news: we empirically tested transfer across depth, batch size, sequence length, & timestep work within reasonable ranges on preLN transformers.

&#x200B;

https://preview.redd.it/x7fo95yqlkm81.jpg?width=2560&format=pjpg&auto=webp&s=1935bf10f1524f9da3df0cc2e95ae1ec9b805f37

We applied this to tune BERT-base and BERT-large simultaneously by shrinking them to the same small model in both width and depth, where we did the direct tuning. We got a really nice improvement over the already well-tuned megatron BERT baseline, especially for BERT-large!

&#x200B;

https://preview.redd.it/db5eausrlkm81.png?width=1687&format=png&auto=webp&s=4453ec387477d20cbcdab266ccbc8e36032c87fd

In general, it seems that the larger a model is, the less well tuned it is --- which totally makes sense --- and thus the more to gain from µTransfer. We didn't have compute to retrain the GPT-3 175B model, but I'll leave your mouth watering with that thought.

OK, so what actually is µP and how do you implement it?

It's encapsulated by the following table for how to scale your initialization and learning rate with fan-in or fan-out. The purple text is µP and the gray text in parenthesis is pytorch default, for reference, and the black text is shared by both.

&#x200B;

https://preview.redd.it/4475drzvlkm81.png?width=1507&format=png&auto=webp&s=b65f56ef2d8c24f077a24a8df40eb7f98c80f7e2

But just like you don't typically want to implement autograd by hand even though autograd is just chain rule, we recommend using our package [https://github.com/microsoft/mup](https://t.co/5S0YAg026Z) to implement µP in your models.

The really curious ones of you: ""OK what is the theoretical motivation behind all this?""

Unfortunately, this is already getting long, so feel free to check out the [reddit thread](https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/) on [our previous theoretical paper](https://arxiv.org/abs/2011.14522), and people let me know if this is something you want to hear for another time!

But I have to say that this is a rare occasion in deep learning where very serious mathematics has concretely delivered a result previously unthinkable, and I'm elated with how things turned out! In contrast to [this reddit thread a few days ago](https://www.reddit.com/r/MachineLearning/comments/t8fn7m/d_are_we_at_the_end_of_an_era_where_ml_could_be/), I think there are plenty of room for new, fundamental mathematics to change the direction of deep learning and artificial intelligence in general --- why chase the coattail of empirical research trying to ""explain"" them all when you can lead the field with deep theoretical insights?

Let me know what you guys think in the comments, or feel free to email me (gregyang at microsoft dot com)!"
12,machinelearning,gpt-3,top,2022-03-16 16:23:25,[P] Composer: a new PyTorch library to train models ~2-4x faster with better algorithms,moinnadeem,False,0.97,472,tflvuy,https://www.reddit.com/r/MachineLearning/comments/tflvuy/p_composer_a_new_pytorch_library_to_train_models/,77,1647447805.0,"Hey all!

We're excited to release Composer ([https://github.com/mosaicml/composer](https://github.com/mosaicml/composer)), an open-source library to speed up training of deep learning models by integrating better algorithms into the training process!

[Time and cost reductions across multiple model families](https://preview.redd.it/0y54ykj8qrn81.png?width=3009&format=png&auto=webp&s=d5f14b3381828d0b9d71ab04a4f1f12ebfb07fd7)

Composer lets you train:

* A ResNet-101 to 78.1% accuracy on ImageNet in 1 hour and 30 minutes ($49 on AWS), **3.5x faster and 71% cheaper than the baseline.**
* A ResNet-50 to 76.51% accuracy on ImageNet in 1 hour and 14 minutes ($40 on AWS), **2.9x faster and 65% cheaper than the baseline.**
* A GPT-2 to a perplexity of 24.11 on OpenWebText in 4 hours and 27 minutes ($145 on AWS), **1.7x faster and 43% cheaper than the baseline.**

https://preview.redd.it/0bitody9qrn81.png?width=10008&format=png&auto=webp&s=d9ecdb45f6419eb49e1c2c69eec418b36f35e172

Composer features a **functional interface** (similar to `torch.nn.functional`), which you can integrate into your own training loop, and a **trainer,** which handles seamless integration of efficient training algorithms into the training loop for you.

**Industry practitioners:** leverage our 20+ vetted and well-engineered implementations of speed-up algorithms to easily reduce time and costs to train models. Composer's built-in trainer makes it easy to **add multiple efficient training algorithms in a single line of code.** Trying out new methods or combinations of methods is as easy as changing a single list, and [we provide training recipes](https://github.com/mosaicml/composer#resnet-101) that yield the best training efficiency for popular benchmarks such as ResNets and GPTs.

**ML scientists:** use our two-way callback system in the Trainer **to easily prototype algorithms for wall-clock training efficiency.**[ Composer features tuned baselines to use in your research](https://github.com/mosaicml/composer/tree/dev/composer/yamls), and the software infrastructure to help study the impacts of an algorithm on training dynamics. Many of us wish we had this for our previous research projects!

**Feel free check out our GitHub repo:** [https://github.com/mosaicml/composer](https://github.com/mosaicml/composer), and star it ⭐️ to keep up with the latest updates!"
13,machinelearning,gpt-3,top,2023-02-02 13:55:47,[N] Microsoft integrates GPT 3.5 into Teams,bikeskata,False,0.97,455,10rqe34,https://www.reddit.com/r/MachineLearning/comments/10rqe34/n_microsoft_integrates_gpt_35_into_teams/,130,1675346147.0,"Official blog post: https://www.microsoft.com/en-us/microsoft-365/blog/2023/02/01/microsoft-teams-premium-cut-costs-and-add-ai-powered-productivity/

Given the amount of money they pumped into OpenAI, it's not surprising that you'd see it integrated into their products. I do wonder how this will work in highly regulated fields (finance, law, medicine, education)."
14,machinelearning,gpt-3,top,2020-06-10 20:50:38,"[D] GPT-3, The $4,600,000 Language Model",mippie_moe,False,0.96,442,h0jwoz,https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/,215,1591822238.0,"[OpenAI’s GPT-3 Language Model Explained](https://lambdalabs.com/blog/demystifying-gpt-3/)

Some interesting take-aways:

* GPT-3 demonstrates that a language model trained on enough data can solve NLP tasks that it has never seen. That is, GPT-3 studies the model as a general solution for many downstream jobs **without fine-tuning**.
* It would take **355 years** to train GPT-3 on a Tesla V100, the fastest GPU on the market.
* It would cost **\~$4,600,000** to train GPT-3 on using the lowest cost GPU cloud provider."
15,machinelearning,gpt-3,top,2023-12-20 13:59:53,[D] Mistral received funding and is worth billions now. Are open source LLMs the future?,BelowaverageReggie34,False,0.96,440,18mv8le,https://www.reddit.com/r/MachineLearning/comments/18mv8le/d_mistral_received_funding_and_is_worth_billions/,156,1703080793.0," Came across this intriguing [article](https://gizmodo.com/mistral-artificial-intelligence-gpt-3-openai-1851091217) about Mistral, an open-source LLM that recently scored 400 million in funding, now valued at 2 billion. Are open-source LLMs gonna be the future? Considering the trust issues with ChatGPT and the debates about its safety, the idea of open-source LLMs seems to be the best bet imo.

Unlike closed-source models, users can verify the privacy claims of open-source models. There have been some good things being said about Mistral, and I only hope such open source LLMs secure enough funding to compete with giants like OpenAI. Maybe then, ChatGPT will also be forced to go open source?

With that said, I'm also hopeful that competitors like [Silatus](https://silatus.com/) and [Durable](https://durable.co/), which already use multiple models, consider using open-source models like Mistral into their frameworks. If that happens, maybe there might be a shift in AI privacy. What do you guys think? Are open-source LLMs the future, especially with the funding backing them?"
16,machinelearning,gpt-3,top,2021-05-26 17:31:34,[N] OpenAI announces OpenAI Startup Fund investing $100 million into AI startups,minimaxir,False,0.97,389,nlmlbg,https://www.reddit.com/r/MachineLearning/comments/nlmlbg/n_openai_announces_openai_startup_fund_investing/,39,1622050294.0,"https://openai.com/fund/
https://techcrunch.com/2021/05/26/openais-100m-startup-fund-will-make-big-early-bets-with-microsoft-as-partner/

It does not appear to be explicitly GPT-3 related (any type of AI is accepted), but hints very heavily toward favoring applications using it."
17,machinelearning,gpt-3,top,2024-02-15 18:39:06,[D] OpenAI Sora Video Gen -- How??,htrp,False,0.96,380,1armmng,https://www.reddit.com/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/,198,1708022346.0,">Introducing Sora, our text-to-video model. Sora can generate videos up to a minute long while maintaining visual quality and adherence to the user’s prompt.




https://openai.com/sora

Research Notes
Sora is a diffusion model, which generates a video by starting off with one that looks like static noise and gradually transforms it by removing the noise over many steps.

Sora is capable of generating entire videos all at once or extending generated videos to make them longer. By giving the model foresight of many frames at a time, we’ve solved a challenging problem of making sure a subject stays the same even when it goes out of view temporarily.

Similar to GPT models, Sora uses a transformer architecture, unlocking superior scaling performance.

We represent videos and images as collections of smaller units of data called patches, each of which is akin to a token in GPT. By unifying how we represent data, we can train diffusion transformers on a wider range of visual data than was possible before, spanning different durations, resolutions and aspect ratios.

Sora builds on past research in DALL·E and GPT models. It uses the recaptioning technique from DALL·E 3, which involves generating highly descriptive captions for the visual training data. As a result, the model is able to follow the user’s text instructions in the generated video more faithfully.

In addition to being able to generate a video solely from text instructions, the model is able to take an existing still image and generate a video from it, animating the image’s contents with accuracy and attention to small detail. The model can also take an existing video and extend it or fill in missing frames. Learn more in our technical paper (coming later today).

Sora serves as a foundation for models that can understand and simulate the real world, a capability we believe will be an important milestone for achieving AGI.



Example Video: https://cdn.openai.com/sora/videos/cat-on-bed.mp4

Tech paper will be released later today. But brainstorming how?"
18,machinelearning,gpt-3,top,2024-02-04 17:06:06,"[P] Chess-GPT, 1000x smaller than GPT-4, plays 1500 Elo chess. We can visualize its internal board state, and it accurately estimates the Elo rating of the players in a game.",seraine,False,0.95,375,1aisp4m,https://www.reddit.com/r/MachineLearning/comments/1aisp4m/p_chessgpt_1000x_smaller_than_gpt4_plays_1500_elo/,80,1707066366.0," gpt-3.5-turbo-instruct's Elo rating of 1800 is chess seemed magical. But it's not! A 100-1000x smaller parameter LLM given a few million games of chess will learn to play at ELO 1500.

This model is only trained to predict the next character in PGN strings (1.e4 e5 2.Nf3 …) and is never explicitly given the state of the board or the rules of chess. Despite this, in order to better predict the next character, it learns to compute the state of the board at any point of the game, and learns a diverse set of rules, including check, checkmate, castling, en passant, promotion, pinned pieces, etc. In addition, to better predict the next character it also learns to estimate latent variables such as the Elo rating of the players in the game.

We can visualize the internal board state of the model as it's predicting the next character. For example, in this heatmap, we have the ground truth white pawn location on the left, a binary probe output in the middle, and a gradient of probe confidence on the right. We can see the model is extremely confident that no white pawns are on either back rank.

&#x200B;

https://preview.redd.it/dn8aryvdolgc1.jpg?width=2500&format=pjpg&auto=webp&s=003fe39d8a9bce2cc3271c4c9232c00e4d886aa6

In addition, to better predict the next character it also learns to estimate latent variables such as the ELO rating of the players in the game. More information is available in this post:

[https://adamkarvonen.github.io/machine\_learning/2024/01/03/chess-world-models.html](https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html)

And the code is here: [https://github.com/adamkarvonen/chess\_llm\_interpretability](https://github.com/adamkarvonen/chess_llm_interpretability)"
19,machinelearning,gpt-3,top,2019-08-13 16:48:08,[News] Megatron-LM: NVIDIA trains 8.3B GPT-2 using model and data parallelism on 512 GPUs. SOTA in language modelling and SQUAD. Details awaited.,Professor_Entropy,False,0.97,362,cpvssu,https://www.reddit.com/r/MachineLearning/comments/cpvssu/news_megatronlm_nvidia_trains_83b_gpt2_using/,66,1565714888.0,"Code: [https://github.com/NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM)

Unlike Open-AI, they have released the complete code for data processing, training, and evaluation.

Detailed writeup: [https://nv-adlr.github.io/MegatronLM](https://nv-adlr.github.io/MegatronLM)

From github:

>Megatron  is a large, powerful transformer. This repo is for ongoing  research on  training large, powerful transformer language models at  scale.  Currently, we support model-parallel, multinode training of [GPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and [BERT](https://arxiv.org/pdf/1810.04805.pdf) in mixed precision.Our  codebase is capable of efficiently training a 72-layer, 8.3  Billion  Parameter GPT2 Language model with 8-way model and 64-way data   parallelism across 512 GPUs. We find that bigger language models are   able to surpass current GPT2-1.5B wikitext perplexities in as little as 5   epochs of training.For BERT  training our repository trains BERT Large on 64 V100 GPUs in  3 days. We  achieved a final language modeling perplexity of 3.15 and  SQuAD  F1-score of 90.7.

Their submission is not in the leaderboard of SQuAD, but this exceeds the previous best single model performance (RoBERTa 89.8).

For  language modelling they get zero-shot wikitext perplexity of 17.4 (8.3B  model) better than 18.3 of transformer-xl (257M). However they claim it  as SOTA when GPT-2 itself has 17.48 ppl, and another model has 16.4 ([https://paperswithcode.com/sota/language-modelling-on-wikitext-103](https://paperswithcode.com/sota/language-modelling-on-wikitext-103))

Sadly they haven't mentioned anything about release of the model weights."
20,machinelearning,gpt-3,top,2023-09-14 13:50:27,[D] The ML Papers That Rocked Our World (2020-2023),PierroZ-PLKG,False,0.96,352,16ij18f,https://www.reddit.com/r/MachineLearning/comments/16ij18f/d_the_ml_papers_that_rocked_our_world_20202023/,50,1694699427.0,"Hey everyone! 👋

I’ve been on a bit of a deep-dive lately, trying to catch up on all the awesome stuff that’s been happening in the ML space. It got me wondering, from 2020 to 2023, what have been the absolute must-read papers that shook the foundations and got everyone talking?

Whether it’s something that reinvented the wheel in your specific niche or just made waves industry-wide, I wanna hear about it!

I’m curious to see how different the responses will be, and hey, this might even become a go-to list for anyone looking to get the lowdown on the hottest trends and discoveries of the past few years.

Can’t wait to hear your thoughts!

# tl;dr

I decided to aggregate your best suggestions into categories for anyone interested in reading them without searching through the whole comment section in the future.

## Theoretical:

* [Neural Networks are Decision Trees](https://arxiv.org/abs/2210.05189)
* [Cross-Validation Bias due to Unsupervised Preprocessing](https://doi.org/10.1111/rssb.12537)
* [The Forward-Forward Algorithm: Some Preliminary Investigations](https://arxiv.org/abs/2212.13345)
* [LoRA: Low-Rank Adaptation of Large Language Models (included here as it has applications beyond LLMs)](https://arxiv.org/abs/2106.09685)
* [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177)

## Image:

* ViT related:
   * [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)](https://arxiv.org/abs/2010.11929)
   * [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)
   * [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877v2)
   * [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)
   * [A ConvNet for the 2020s (a CNN that implements several key components that contribute to the performance of Vision Transformers)](https://arxiv.org/abs/2201.03545)
   * [(CLIP) Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
* Diffusion related:
   * [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
   * [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/abs/2006.11239)
   * [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)
* [Taming Transformers for High-Resolution Image Synthesis (VQGAN)](https://arxiv.org/abs/2012.09841)
* [Segment Anything (SAM)](https://arxiv.org/abs/2304.02643)
* [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)
* [Bayesian Flow Networks](https://arxiv.org/abs/2308.07037)

## NLP:

* [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)
* [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
* [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
* [Training Compute-Optimal Large Language Models (Chinchilla)](https://arxiv.org/abs/2203.15556)
* [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/abs/2301.13688)
* [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
* [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761)

## 3D Rendering:

* [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934)
* [Highly accurate protein structure prediction with AlphaFold](https://www.nature.com/articles/s41586-021-03819-2)

## Misc:

* [Human-level play in the game of Diplomacy by combining language models with strategic reasoning](https://www.science.org/doi/10.1126/science.ade9097)

For a well-made and maintained list of ML resources (not only the newest like here) you can check out [this](https://github.com/dmarx/anthology-of-modern-ml)"
21,machinelearning,gpt-3,top,2022-12-22 18:39:30,[D] When chatGPT stops being free: Run SOTA LLM in cloud,_underlines_,False,0.95,348,zstequ,https://www.reddit.com/r/MachineLearning/comments/zstequ/d_when_chatgpt_stops_being_free_run_sota_llm_in/,95,1671734370.0,"Edit: Found [LAION-AI/OPEN-ASSISTANT](https://github.com/LAION-AI/Open-Assistant) a very promising project opensourcing the idea of chatGPT. [video here](https://www.youtube.com/watch?v=8gVYC_QX1DI)

**TL;DR: I found GPU compute to be [generally cheap](https://github.com/full-stack-deep-learning/website/blob/main/docs/cloud-gpus/cloud-gpus.csv) and spot or on-demand instances can be launched on AWS for a few USD / hour up to over 100GB vRAM. So I thought it would make sense to run your own SOTA LLM like Bloomz 176B inference endpoint whenever you need it for a few questions to answer. I thought it would still make more sense than shoving money into a closed walled garden like ""not-so-OpenAi"" when they make ChatGPT or GPT-4 available for $$$. But I struggle due to lack of tutorials/resources.**

Therefore, I carefully checked benchmarks, model parameters and sizes as well as training sources for all SOTA LLMs [here](https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit#gid=1158069878).

Knowing since reading the Chinchilla paper that Model Scaling according to OpenAI was wrong and more params != better quality generation. So I was looking for the best performing LLM openly available in terms of quality and broadness to use for multilingual everyday questions/code completion/reasoning similar to what chatGPT provides (minus the fine-tuning for chat-style conversations).

My choice fell on [Bloomz](https://huggingface.co/bigscience/bloomz) (because that handles multi-lingual questions well and has good zero shot performance for instructions and Q&A style text generation. Confusingly Galactica seems to outperform Bloom on several benchmarks. But since Galactica had a very narrow training set only using scientific papers, I guess usage is probably limited for answers on non-scientific topics.

Therefore I tried running the original bloom 176B and alternatively also Bloomz 176B on AWS SageMaker JumpStart, which should be a one click deployment. This fails after 20min. On Azure ML, I tried using DeepSpeed-MII which also supports bloom but also fails due the instance size of max 12GB vRAM I guess.

From my understanding to save costs on inference, it's probably possible to use one or multiple of the following solutions:

- Precision: int8 instead of fp16
- [Microsoft/DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) for an up 40x reduction on inference cost on Azure, this thing also supports int8 and fp16 bloom out of the box, but it fails on Azure due to instance size.
- [facebook/xformer](https://github.com/facebookresearch/xformers) not sure, but if I remember correctly this brought inference requirements down to 4GB vRAM for StableDiffusion and DreamBooth fine-tuning to 10GB. No idea if this is usefull for Bloom(z) inference cost reduction though

I have a CompSci background but I am not familiar with most stuff, except that I was running StableDiffusion since day one on my rtx3080 using linux and also doing fine-tuning with DreamBooth. But that was all just following youtube tutorials. I can't find a single post or youtube video of anyone explaining a full BLOOM / Galactica / BLOOMZ inference deployment on cloud platforms like AWS/Azure using one of the optimizations mentioned above, yet alone deployment of the raw model. :(

I still can't figure it out by myself after 3 days.

**TL;DR2: Trying to find likeminded people who are interested to run open source SOTA LLMs for when chatGPT will be paid or just for fun.**

Any comments, inputs, rants, counter-arguments are welcome.

/end of rant"
22,machinelearning,gpt-3,top,2023-03-17 09:59:59,[D] PyTorch 2.0 Native Flash Attention 32k Context Window,super_deap,False,0.98,345,11tmpc5,https://www.reddit.com/r/MachineLearning/comments/11tmpc5/d_pytorch_20_native_flash_attention_32k_context/,94,1679047199.0,"Hi,

I did a quick experiment with Pytorch 2.0 Native scaled\_dot\_product\_attention. I was able to a single forward pass within 9GB of memory which is astounding. I think by patching existing Pretrained GPT models and adding more positional encodings, one could easily fine-tune those models to 32k attention on a single A100 80GB. Here is the code I used:

&#x200B;

https://preview.redd.it/6csxe28lv9oa1.png?width=607&format=png&auto=webp&s=ff8b48a77f49fab7d088fd8ba220f720860249bc

I think it should be possible to replicate even GPT-4 with open source tools something like Bloom + FlashAttention & fine-tune on 32k tokens.

**Update**: I was successfully able to start the training of GPT-2 (125M) with a context size of 8k and batch size of 1 on a 16GB GPU. Since memory scaled linearly from 4k to 8k. I am expecting, 32k would require \~64GB and should train smoothly on A100 80 GB. Also, I did not do any other optimizations. Maybe 8-bit fine-tuning can further optimize it.

**Update 2**: I basically picked Karpaty's nanoGPT and patched the pretrained GPT-2 by repeating the embeddings N-times. I was unable to train the model at 8k because generation would cause the crash.  So I started the training for a context window of 4k on The Pile: 1 hour in and loss seems to be going down pretty fast. Also Karpaty's generate function is super inefficient, O(n\^4) I think so it took forever to generate even 2k tokens. So I generate 1100 tokens just to see if the model is able to go beyond 1k limit. And it seems to be working. [Here are some samples](https://0bin.net/paste/O-+eopaW#nmtzX1Re7f1Nr-Otz606jkltvKk/kUXY96/8ca+tb4f) at 3k iteration.

&#x200B;

https://preview.redd.it/o2hb25w1sboa1.png?width=1226&format=png&auto=webp&s=bad2a1e21e218512b0f630c947ee41dba9b86a44

**Update 3**: I have started the training and I am publishing the training script if anyone is interested in replicating or building upon this work. Here is the complete training script:

[https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c](https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c)

I will post an update after the weekend once the training has progressed somewhat.

**Post-Weekend Update**: After \~50k iterations (the model has seen \~200 million tokens, I know this is just too small compared to 10s of billions trained by giga corps), loss only dropped from 4.6 to 4.2 on The Pile:

https://preview.redd.it/vi0fpskhsuoa1.png?width=1210&format=png&auto=webp&s=9fccc5277d91a6400adc6d968b0f2f0ff0da2afc

AFAIR, the loss of GPT-2 on the Pile if trained with 1024 tokens is \~2.8. It seems like the size of the dimension for each token is kind of limiting how much loss can go down since GPT-2 (small) has an embedding dimension of 768. Maybe someone can experiment with GPT-2 medium etc. to see how much we can improve. This is confirmation of the comment by u/lucidraisin [below](https://www.reddit.com/r/MachineLearning/comments/11tmpc5/comment/jcl2rkh/?utm_source=reddit&utm_medium=web2x&context=3)."
23,machinelearning,gpt-3,top,2020-08-05 17:21:59,"[D] Biggest roadblock in making ""GPT-4"", a ~20 trillion parameter transformer",AxeLond,False,0.97,343,i49jf8,https://www.reddit.com/r/MachineLearning/comments/i49jf8/d_biggest_roadblock_in_making_gpt4_a_20_trillion/,138,1596648119.0,"So I found this paper, [https://arxiv.org/abs/1910.02054](https://arxiv.org/abs/1910.02054) which pretty much describes how the GPT-3 over GPT-2 gain was achieved, 1.5B -> 175 billion parameters

# Memory

>Basic data parallelism (DP) does not reduce memory per device, and runs out of memory for models with more than 1.4B parameters on current generation of GPUs with 32 GB memory

The paper also talks about memory optimizations by clever partitioning of Optimizer State, Gradient between GPUs to reduce need for communication between nodes. Even without using Model Parallelism (MP), so still running 1 copy of the model on 1 GPU.

>ZeRO-100B can train models with up to 13B parameters without MP on 128 GPUs, achieving throughput over 40 TFlops per GPU on average. In comparison, without ZeRO, the largest trainable model with DP alone has 1.4B parameters with throughput less than 20 TFlops per GPU.

Add 16-way Model Parallelism in a DGX-2 cluster of Nvidia V100s and 128 nodes and you got capacity for around 200 billion parameters. From MP = 16 they could run a 15.4x bigger model without any real loss in performance, 30% less than peak performance when running 16-way model parallelism and 64-way data parallelism (1024 GPUs).

This was all from Gradient and Optimizer state Partitioning, they then start talking about parameter partitioning and say it should offer a linear reduction in memory proportional to number of GPUs used, so 64 GPUs could run a 64x bigger model, at a 50% communication bandwidth increase. But they don't actually do any implementation or testing of this.

# Compute

Instead they start complaining about a compute power gap, their calculation of this is pretty rudimentary. But if you redo it with the method cited by GPT-3 and using the empirically derived values by GPT-3 and the cited paper,   [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361) 

Loss (L) as a function of model parameters (N) should scale,

L = (N/8.8 \* 10\^13)\^-0.076

Provided compute (C) in petaFLOP/s-days is,

L = (C/2.3\*10\^8)\^-0.05  ⇔ L = 2.62 \* C\^-0.05

GPT-3 was able to fit this function as 2.57 \* C\^-0.048

So if you just solve C from that,

[C = 2.89407×10\^-14 N\^(19/12)](https://www.wolframalpha.com/input/?i=%28N%2F8.8*10%5E13%29%5E-0.076+%3D+2.57*C%5E-0.048+solve+C)

If you do that for the same increase in parameters as GPT-2 to GPT-3, then you get

C≈3.43×10\^7 for [20 trillion](https://www.wolframalpha.com/input/?i=C+%3D+2.89407%C3%9710%5E-14+N%5E%2819%2F12%29+and+N+%3D+175+billion+%2F+1.5+billion+*+175+billion) parameters, vs 18,300 for 175 billion. 10\^4.25 PetaFLOP/s-days looks around what they used for GPT-3, they say several thousands, not twenty thousand, but it was also slightly off the trend line in the graph and probably would have improved for training on more compute.

You should also need around 16 trillion tokens, GPT-3 trained on 300 billion tokens (function says 370 billion ideally). English Wikipedia was 3 billion. 570GB of webcrawl was 400 billion tokens, so 23TB of tokens seems relatively easy in comparison with compute.

With GPT-3 costing around [$4.6 million](https://lambdalabs.com/blog/demystifying-gpt-3/) in compute, than would put a price of [$8.6 billion](https://www.wolframalpha.com/input/?i=3.43%C3%9710%5E7%2F18%2C300+*+%244.6M+) for the compute to train ""GPT-4"".

If making bigger models was so easy with parameter partitioning from a memory point of view then this seems like the hardest challenge, but you do need to solve the memory issue to actually get it to load at all.

However, if you're lucky you can get 3-6x compute increase from Nvidia A100s over V100s,  [https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)

But even a 6x compute gain would still put the cost at $1.4 billion.

Nvidia only reported $1.15 billion in revenue from ""Data Center"" in 2020 Q1, so just to train ""GPT-4"" you would pretty much need the entire world's supply of graphic cards for 1 quarter (3 months), at least on that order of magnitude.

The Department of Energy is paying AMD $600 million to build the 2 Exaflop El Capitan supercomputer. That supercomputer could crank it out in [47 years](https://www.wolframalpha.com/input/?i=3.43%C3%9710%5E7+petaFLOPS*+days++%2F+%282+EXAFLOPS%29).

To vastly improve Google search, and everything else it could potentially do, $1.4 billion or even $10 billion doesn't really seem impossibly bad within the next 1-3 years though."
24,machinelearning,gpt-3,top,2023-05-07 23:26:29,"[D] ClosedAI license, open-source license which restricts only OpenAI, Microsoft, Google, and Meta from commercial use",wemsyn,False,0.8,348,13b6miy,https://www.reddit.com/r/MachineLearning/comments/13b6miy/d_closedai_license_opensource_license_which/,191,1683501989.0,"After reading [this article](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither), I realized it might be nice if the open-source AI community could exclude ""closed AI"" players from taking advantage of community-generated models and datasets. I was wondering if it would be possible to write a license that is completely permissive (like Apache 2.0 or MIT), except to certain companies, which are completely barred from using the software in any context.

Maybe this could be called the ""ClosedAI"" license. I'm not any sort of legal expert so I have no idea how best to write this license such that it protects model weights and derivations thereof.

I prompted ChatGPT for an example license and this is what it gave me:

    <PROJECT NAME> ClosedAI License v1.0
    
    Permission is hereby granted, free of charge, to any person or organization obtaining a copy of this software and associated documentation files (the ""Software""), to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, subject to the following conditions:
    
    1. The above copyright notice and this license notice shall be included in all copies or substantial portions of the Software.
    
    2. The Software and any derivative works thereof may not be used, in whole or in part, by or on behalf of OpenAI Inc., Google LLC, or Microsoft Corporation (collectively, the ""Prohibited Entities"") in any capacity, including but not limited to training, inference, or serving of neural network models, or any other usage of the Software or neural network weights generated by the Software.
    
    3. Any attempt by the Prohibited Entities to use the Software or neural network weights generated by the Software is a material breach of this license.
    
    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

No idea if this is valid or not. Looking for advice.

&#x200B;

**Edit:** Thanks for the input. Removed non-commercial clause (whoops, proofread what ChatGPT gives you). Also removed Meta from the excluded companies list due to popular demand."
25,machinelearning,gpt-3,top,2023-05-10 20:10:30,"[D] Since Google buried the MMLU benchmark scores in the Appendix of the PALM 2 technical report, here it is vs GPT-4 and other LLMs",jd_3d,False,0.97,340,13e1rf9,https://www.reddit.com/r/MachineLearning/comments/13e1rf9/d_since_google_buried_the_mmlu_benchmark_scores/,88,1683749430.0,"MMLU Benchmark results (all 5-shot)

* GPT-4 -  86.4%
* Flan-PaLM 2 (L) -   81.2%
* PALM 2 (L)  -  78.3%
* GPT-3.5 - 70.0%
* PaLM 540B  -  69.3%
* LLaMA 65B -  63.4%"
26,machinelearning,gpt-3,top,2021-07-16 22:05:38,[N] Facebook AI Releases ‘BlenderBot 2.0’: An Open Source Chatbot That Builds Long-Term Memory And Searches The Internet To Engage In Intelligent Conversations With Users,techsucker,False,0.95,329,olr68a,https://www.reddit.com/r/MachineLearning/comments/olr68a/n_facebook_ai_releases_blenderbot_20_an_open/,22,1626473138.0,"The GPT-3 and [BlenderBot 1.0](https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/) models are extremely forgetful, but that’s not the worst of it! They’re also known to “hallucinate” knowledge when asked a question they can’t answer.

It is no longer a matter of whether or not machines will learn, but how. And while many companies are currently investing in so-called “deep learning” models that focus on training ever larger and more complex neural networks (and their model weights) to achieve greater levels of sophistication by making them store what they have learned during the course/training process, it has proven difficult for these large models to keep up with changes occurring online every minute as new information continually floods into its repository from all over the internet.

Summary: [https://www.marktechpost.com/2021/07/16/facebook-ai-releases-blenderbot-2-0-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet-to-engage-in-intelligent-conversations-with-users/](https://www.marktechpost.com/2021/07/16/facebook-ai-releases-blenderbot-2-0-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet-to-engage-in-intelligent-conversations-with-users/) 

Paper 1: https://github.com/facebookresearch/ParlAI/blob/master/projects/sea/Internet\_Augmented\_Dialogue.pdf

Paper 2: https://github.com/facebookresearch/ParlAI/blob/master/projects/msc/msc.pdf

Codes: https://parl.ai/projects/blenderbot2/

Fb blog : https://ai.facebook.com/blog/blender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet/"
27,machinelearning,gpt-3,top,2020-09-22 17:40:14,[N] Microsoft teams up with OpenAI to exclusively license GPT-3 language model,kit1980,False,0.96,324,ixs88q,https://www.reddit.com/r/MachineLearning/comments/ixs88q/n_microsoft_teams_up_with_openai_to_exclusively/,117,1600796414.0,"""""""OpenAI will continue to offer GPT-3 and other powerful models via its own Azure-hosted API, launched in June. While we’ll be hard at work utilizing the capabilities of GPT-3 in our own products, services and experiences to benefit our customers, we’ll also continue to work with OpenAI to keep looking forward: leveraging and democratizing the power of their cutting-edge AI research as they continue on their mission to build safe artificial general intelligence.""""""

https://blogs.microsoft.com/blog/2020/09/22/microsoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/"
28,machinelearning,gpt-3,top,2021-01-01 22:24:53,[R] The Pile: An 800GB Dataset of Diverse Text for Language Modeling,leogao2,False,0.97,317,kokk8z,https://www.reddit.com/r/MachineLearning/comments/kokk8z/r_the_pile_an_800gb_dataset_of_diverse_text_for/,53,1609539893.0,"EleutherAI is proud to announce the release of the Pile, a free and publicly available 800GB dataset of diverse English text for language modeling! 

Website: [https://pile.eleuther.ai/](https://pile.eleuther.ai/) 

Paper: [https://pile.eleuther.ai/paper.pdf](https://pile.eleuther.ai/paper.pdf) 

Twitter thread: [https://twitter.com/nabla\_theta/status/1345130409579794432](https://twitter.com/nabla_theta/status/1345130409579794432)

&#x200B;

>Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present *the Pile*: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets—both existing and newly constructed—many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction."
29,machinelearning,gpt-3,top,2020-08-22 17:16:08,"[N] GPT-3, Bloviator: OpenAI’s language generator has no idea what it’s talking about",rafgro,False,0.94,312,iemck2,https://www.reddit.com/r/MachineLearning/comments/iemck2/n_gpt3_bloviator_openais_language_generator_has/,111,1598116568.0,"MIT Tech Review's article: [https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/](https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/)

>As we were putting together this essay, our colleague Summers-Stay, who is good with metaphors, wrote to one of us, saying this: ""GPT is odd because it doesn’t 'care' about getting the right answer to a question you put to it. It’s more like an improv actor who is totally dedicated to their craft, never breaks character, and has never left home but only read about the world in books. Like such an actor, when it doesn’t know something, it will just fake it. You wouldn’t trust an improv actor playing a doctor to give you medical advice."""
30,machinelearning,gpt-3,top,2020-12-07 13:54:02,"[R] Wide Neural Networks are Feature Learners, Not Kernel Machines",thegregyang,False,0.95,319,k8h01q,https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/,52,1607349242.0,"Hi Reddit,

I’m excited to share with you my new paper [\[2011.14522\] Feature Learning in Infinite-Width Neural Networks (arxiv.org)](https://arxiv.org/abs/2011.14522).

# The Problem

Many previous works proposed that wide neural networks (NN) are kernel machines [\[1\]](http://arxiv.org/abs/1806.07572)[\[2\]](http://arxiv.org/abs/1811.03962)[\[3\]](http://arxiv.org/abs/1811.03804), the most well-known theory perhaps being the *Neural Tangent Kernel (NTK)* [\[1\]](http://arxiv.org/abs/1806.07572). This is problematic because kernel machines **do not learn features**, so such theories cannot make sense of **pretraining and transfer learning** (e.g. Imagenet and BERT), which are arguably at the center of deep learning's far-reaching impact so far.

# The Solution

Here we show if we parametrize the NN “correctly” (see paper for how), then its infinite-width limit **admits feature learning**. We can derive exact formulas for such feature-learning “infinite-width” neural networks. Indeed, we explicitly compute them for learning word embeddings via [word2vec](https://en.wikipedia.org/wiki/Word2vec) (the first large-scale NLP pretraining in the deep learning age and a precursor to BERT) and compare against finite neural networks as well as [NTK](http://arxiv.org/abs/1806.07572) (the kernel machine mentioned above). Visualizing the learned embeddings immediately gives a clear idea of their differences:

[Visualizing Learned Word2Vec Embeddings of Each Model](https://preview.redd.it/d8hspempsr361.png?width=1336&format=png&auto=webp&s=5a792c36905afba606a4107932a8002b0cac1e30)

Furthermore, we find on the word analogy downstream task: 1) The feature-learning limit outperforms the NTK and the finite-width neural networks, 2) and the latter approach the feature-learning limit in performance as width increases.

In the figure below, you can observe that NTK gets \~0 accuracy. This is because its word embeddings are essentially from random initialization, so it is no better than random guessing among the 70k vocabulary (and 1/70k is effectively 0 on this graph).

[Downstream Word Analogy Task](https://preview.redd.it/uj2blwqqsr361.png?width=2272&format=png&auto=webp&s=ea2bbbb5c496e6e44188425281e0847302d7b9fe)

We obtain similar findings in another experiment comparing these models on Omniglot few-shot learning via MAML (see paper). These results suggest that **our new limit is really the “right” limit** for talking about feature learning, pretraining, and transfer learning.

# Looking Ahead

I’m super excited about all this because it blows open so many questions:

1. What kinds of representations are learned in such infinite-width neural networks?
2. How does it inform us about finite neural networks?
3. How does this feature learning affect training and generalization?
4. How does this jibe with the [scaling law of language models](http://arxiv.org/abs/2001.08361)?
5. Can we train an infinite-width GPT…so GPT∞?
6. ... and so many more questions!

For each of these questions, our results provide a framework for answering it, so it feels like they are all within reach.

# Tensor Programs Series

This (mathematical) framework is called *Tensor Programs* and I’ve been writing a series of papers on them, slowly building up its foundations. Here I have described the 4th paper in this series (though I've stopped numbering it in the title), which is a big payoff of the foundations developed by its predecessors, which are

1. [\[1910.12478\] Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes (arxiv.org)](https://arxiv.org/abs/1910.12478)  ([reddit discussion](https://www.reddit.com/r/MachineLearning/comments/i17889/r_tensor_programs_i_wide_feedforward_or_recurrent/))
2. [\[2006.14548\] Tensor Programs II: Neural Tangent Kernel for Any Architecture (arxiv.org)](https://arxiv.org/abs/2006.14548)
3. [\[2009.10685\] Tensor Programs III: Neural Matrix Laws (arxiv.org)](https://arxiv.org/abs/2009.10685)

Each paper from 1-3 builds up the machinery incrementally, with a punchline for the partial progress made in that paper. But actually I started this whole series because I wanted to write [the paper described in this post](https://arxiv.org/abs/2011.14522)! It required a lot of planning ahead, writing pain, and fear-of-getting-scooped-so-you-wrote-more-than-200-pages-for-nothing, but I'm really happy and relieved I finally made it!

# Talk Coming Up

I am going to talk about this work this Wednesday 12 EDT at the online seminar [Physics ∩ ML](http://physicsmeetsml.org/posts/sem_2020_12_09/). Please join me if this sounds interesting to you! You can sign up [here](https://forms.gle/mLtPEXbpjjvWvpxq8) to get the zoom link.

# Shout Out to My Co-Author Edward

[Edward](https://edwardjhu.com/) is a Microsoft AI Resident and a hell of a researcher for his age. I'm really lucky to have him work with me during the past year (and ongoing). He's looking for grad school opportunities next, so please [reach out to him](mailto:Edward.Hu@microsoft.com) if you are a professor interested in working with him! Or, if you are a student looking to jumpstart your AI career, apply to our [AI Residency Program](https://www.microsoft.com/en-us/research/academic-program/microsoft-ai-residency-program/)!

# Edit: FAQs from the Comments

&#x200B;

>Pretraining and transfer learning don’t make sense in the kernel limits of neural networks. Why?

In a gist, in these kernel limits, the last layer representations of inputs (right before the linear readout layer) are essentially fixed throughout the training.

During transfer learning, we discard the pretrained readout layer and train a new one (because the task will typically have different labels than pretraining). Often, we train only this new (linear) readout layer to save computation (e.g. as in self-supervised learning in vision, like AMDIM, SimCLR, BYOL). The outcome of this linear training only depends on the last layer representations of the inputs. In the kernel limits, they are fixed at initialization, so in terms of transfer, it’s like you never pretrained at all.

For example, this is very clear in the Gaussian Process limit of NN, which corresponds to training only the readout layer of the network. Then the input representations are *exactly* fixed throughout training. In the Neural Tangent limit of NN, the representations are not exactly fixed but any change tends to 0 as width → ∞

Contrast this with known behavior of ResNet, for example, where each neuron in last layer representation is a face detector, eye detector, boat detector, etc. This can’t be true if the representation comes solely from random initialization. Similar things can be said of pretrained language models.

So I've just talked about linear transfer learning above. But the same conclusion holds even if you finetune the entire network via a more sophisticated argument (see Thm G.16 in the paper).

&#x200B;

>Why are NN not kernel machines?

The title really should be something like “To Explain Pretraining and Transfer Learning, Wide Neural Networks Should Be Thought of as Feature Learners, Not Kernel Machines” but that’s really long

So I’m actually not saying NN *cannot* be kernel machines – they can, as in the GP and NTK limits – but we can understand them better as feature learners.

More precisely, the same neural network can have different infinite-width limits, depending on the parametrization of the network. A big contribution of this paper is classifying what kind of limits are possible.

&#x200B;

>Comparison with [Pedro’s paper: Every Model Learned by Gradient Descent Is Approximately a Kernel Machine](https://arxiv.org/abs/2012.00152)?

Any finite function can be *expressed* as a kernel machine for any given positive definite kernel.

My understanding is that Pedro’s paper presents a specific instantiation of this using what he defines as the *path kernel*.

However, it’s unclear to me in what way is that useful, because the kernel (and the coefficients involved) he defines depends on the optimization trajectory of the NN and the data of the problem. So his “kernel machine” actually allows feature learning in the sense that his path kernel can change over the course of training. This really doesn't jibe with his comment that "" Perhaps the most significant implication of our result for deep learning is that it casts doubt on the common view that it works by automatically discovering new representations of the data, in contrast with other machine learning methods, which rely on predefined features (Bengio et al., 2013).""

In addition, if you look at the proof of his theorem (screenshotted below), the appearance of the path kernel in his expression is a bit arbitrary, since I can also multiply and divide by some other kernel

*Processing img 1zmnd9ziyt361...*

&#x200B;

>What’s the relation with universal approximation theorem?

Glockenspielcello actually has [a pretty good answer](https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/geyodne?utm_source=share&utm_medium=web2x&context=3), so I’ll just cite them here

""The point of this new paper isn't about the expressivity of the output class though, it's about the kind of learning that is performed. If you look at the paper, they differentiate between different kinds of limits that you can get based on the parametrization, and show that you can get either kernel-like behavior or feature learning behavior. Single layer networks using the parametrization described by Neal fall into the former category.""

&#x200B;"
31,machinelearning,gpt-3,top,2023-04-17 17:54:43,[Discussion] Translation of Japanese to English using GPT. These are my discoveries after ~100 hours of extensive experimentation and ways I think it can be improved.,NepNep_,False,0.9,310,12pqqg6,https://www.reddit.com/r/MachineLearning/comments/12pqqg6/discussion_translation_of_japanese_to_english/,62,1681754083.0,"Hello. I am currently experimenting with the viability of LLM models for Japanese to English translation. I've been experimenting with GPT 3.5, GPT 3.5 utilizing the DAN protocols, and GPT 4 for this project for around 3 months now with very promising results and I think I've identified several limitations with GPT that if addressed can significantly improve the efficiency and quality of translations.

&#x200B;

The project I'm working on is attempting to translate a light novel series from japanese to english. During these tests I did a deep dive, asking GPT how it is attempting the translations and asking it to modify its translation methodology through various means (I am considering doing a long video outlining all this and showing off the prompts and responses at a later date). Notably this includes asking it to utilize its understanding of the series its translating from its training knowledge to aide in the translation, and providing it with a ""seed"" translation. Basically the seed is a side by side japanese and english translation to show GPT what I'm looking for in terms of grammar and formatting. The english translation notably is a human translation, not a machine translation. The results from these tests provided SIGNIFICANT improvements to the final translation, so significant in fact that a large portion of the text could reasonably be assumed to be human-translated.

&#x200B;

Link to the project I'm working on so you can see my documentation and results: [https://docs.google.com/document/d/1MxKiE-q36RdT\_Du5K1PLdyD7Vru9lcf6S60uymBb10g/edit?usp=sharing](https://docs.google.com/document/d/1MxKiE-q36RdT_Du5K1PLdyD7Vru9lcf6S60uymBb10g/edit?usp=sharing)

&#x200B;

I've probably done around 50-100 hours of extensive testing with translation and methodology over the past 2-3 months. Over that time I've discovered the following:

1. Both GPT3 and GPT4 are significant improvements over traditional translation services such as google or deepl. This may be because Japanese and English are very different languages in how they are written and how their grammar works so prior translation services simply did a direct translation while GPT is capable of understanding the text and rewriting it to account for that. For example in japanese, there are no pronouns like ""he"" and ""her"" so a person's gender might not be clear from the sentence alone. Google Translate and DeepL typically just take a 50/50 guess, while GPT from my experience has been much more capable in getting this right based on understanding the larger context from the paragraph.
2. GPT has a tendency to censor text deliberately if it feels the translation may offend people. This isn't just for things that are blatantly offensive like slurs, it also includes mild sexual content, the kind that is typically approved for teen viewing/reading. The biggest problem is that it doesn't tell you it is censoring anything unless you ask it, meaning everything else may be a solid translation yet it may censor information which can ultimately hurt the translation, especially for story related translations like in my tests. These restrictions can be bypassed with correct prompting. I've had luck using GPT 3's DAN protocols however DAN's translations arent as strong as GPT 4, and I've had luck with GPT 4 by framing the translation as a game with extreme win and loss conditions and telling it that if it censors the translation, it may offend the author of the content since people in japan hold different values from our own.
3. GPT puts too high a focus on accuracy even if instructed not to. This is a good thing to a degree since outside of censorship you know the translation is accurate, however even when explicitly told to put maximum emphasis on readability, even if it hurts the accuracy, and it is allowed to rewrite sentences from the ground up to aide readability, it still puts too strong an emphasis on accuracy. I have determined this through testing that for some reason it is ignoring the request to focus on readability and will still maximize accuracy. The best way I've found to fix this issue is through demonstration, specifically the ""seed"" I mentioned earlier. By giving it a japanese and english translation of the same work but earlier in the story, it then understands how to put more emphasis on readability. The results is something that is 95% within the range of accuracy a professional translator would use while much easier to read.
4. GPT's biggest limitation is the fact that it ""forgets"" the seed way too quickly, usually within a few prompts. I've done testing with its data retention and it appears that if you give it too much information to remember at once it slowly bugs out. With GPT 3 its a hard crash type bug where it just spews nonsense unrelated to your request, however GPT 4 can remember a lot more information and will hard crash if you give it too much info but otherwise builds up errors slowly as you give it more info. I initially believed that there were issues with the token count, but further testing shows that the GPT model simply isn't optimized for this method of translation and a new or reworked model that you can give a seed and it will remember it longer would be better. The seed is one of the best tools for improving its performance

Next steps:

I would like to try to either create my own model or modify an existing one to optimize it for translation. If any1 knows any tools or guides I'd appreciate it."
32,machinelearning,gpt-3,top,2022-10-31 17:58:41,[News] The Stack: 3 TB of permissively licensed source code - Hugging Face and ServiceNow Research Denis Kocetkov et al 2022,Singularian2501,False,0.98,302,yijfkw,https://www.reddit.com/r/MachineLearning/comments/yijfkw/news_the_stack_3_tb_of_permissively_licensed/,30,1667239121.0,"ServiceNow and Hugging Face have released a **3.1TB dataset** of permissively licensed code in **30 programming languages**. This is about 4x larger than the dataset used to train GPT-3 (though obviously ‘code only’), and **3x the size of CodeParrot**, the next largest released code dataset.

Paper: [https://drive.google.com/file/d/17J-0KXTDzY9Esp-JqXYHIcy--i\_7G5Bb/view](https://drive.google.com/file/d/17J-0KXTDzY9Esp-JqXYHIcy--i_7G5Bb/view) 

[https://wandb.ai/telidavies/ml-news/reports/The-Stack-BigCode-s-New-3-TB-Dataset-Of-Permissively-Licensed-Code--VmlldzoyODY1MDUy](https://wandb.ai/telidavies/ml-news/reports/The-Stack-BigCode-s-New-3-TB-Dataset-Of-Permissively-Licensed-Code--VmlldzoyODY1MDUy) 

Hugging Face: [https://huggingface.co/datasets/bigcode/the-stack](https://huggingface.co/datasets/bigcode/the-stack) 

Twitter: [https://twitter.com/BigCodeProject/status/1585631176353796097](https://twitter.com/BigCodeProject/status/1585631176353796097) 

**Download The Stack:** [https://hf.co/BigCode](https://hf.co/BigCode) 

[Source: https:\/\/twitter.com\/BigCodeProject\/status\/1585631176353796097](https://preview.redd.it/69w4s1skj6x91.jpg?width=2288&format=pjpg&auto=webp&s=c7c3018fb9480b6cc5b47cdbf6102de7d6f8b79a)

[Source: https:\/\/twitter.com\/BigCodeProject\/status\/1585631176353796097](https://preview.redd.it/avseyaskj6x91.jpg?width=2774&format=pjpg&auto=webp&s=765119e8c61f4bc0722c1c43a18117e3cf5d031e)

&#x200B;

[Source: https:\/\/twitter.com\/BigCodeProject\/status\/1585631176353796097](https://preview.redd.it/tntlwaskj6x91.jpg?width=2286&format=pjpg&auto=webp&s=ab052d99a49d25e6997032ad0a6655f254c06028)"
33,machinelearning,gpt-3,top,2020-12-03 23:53:02,[N] The abstract of the paper that led to Timnit Gebru's firing,ML_Reviewer,False,0.91,298,k69eq0,https://www.reddit.com/r/MachineLearning/comments/k69eq0/n_the_abstract_of_the_paper_that_led_to_timnit/,246,1607039582.0,"I was a reviewer of the paper.  Here's the abstract. It is critical of BERT, like many people in this sub conjectured:

**Abstract**

The past three years of work in natural language processing have been characterized by the development and deployment of ever larger language models, especially for English. GPT-2, GPT-3, BERT and its variants have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pre- trained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We end with recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.

Context:

[https://www.reddit.com/r/MachineLearning/comments/k6467v/n\_the\_email\_that\_got\_ethical\_ai\_researcher\_timnit/](https://www.reddit.com/r/MachineLearning/comments/k6467v/n_the_email_that_got_ethical_ai_researcher_timnit/)

[https://www.reddit.com/r/MachineLearning/comments/k5ryva/d\_ethical\_ai\_researcher\_timnit\_gebru\_claims\_to/](https://www.reddit.com/r/MachineLearning/comments/k5ryva/d_ethical_ai_researcher_timnit_gebru_claims_to/)"
34,machinelearning,gpt-3,top,2023-03-17 02:34:28,LLMs are getting much cheaper — business impact? [D],DamnMyAPGoinCrazy,False,0.96,297,11tenm7,https://www.reddit.com/r/MachineLearning/comments/11tenm7/llms_are_getting_much_cheaper_business_impact_d/,111,1679020468.0,"Saw this out of Stanford. Apologies if it’s been shared here already. 

*We introduce Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On our preliminary evaluation of single-turn instruction following, Alpaca behaves qualitatively similarly to OpenAI’s text-davinci-003, while being surprisingly small and easy/cheap to reproduce (<600$).*

Basically, starting w an open source Meta 7B LLaMa model, they recruited GPT-3.5 to use for self-instruct training (as opposed to RLHF) and were able to produce a model that behaved similar to GPT-3.5. Amazingly, the process only took few weeks and $600 in compute cost.  

Any thoughts on how such low cost to train/deploy LLMs could affect companies like AMD, Nvidia and Intel etc? This seems like new idiom of AI tech and trying to wrap my head around CPU/GPU demand implications given the apparent orders of magnitude training cost reduction. 

Link: https://crfm.stanford.edu/2023/03/13/alpaca.html"
35,machinelearning,gpt-3,top,2021-04-15 17:28:43,[D] Microsoft's ML acquisition strategy,bendee983,False,0.96,288,mrjl61,https://www.reddit.com/r/MachineLearning/comments/mrjl61/d_microsofts_ml_acquisition_strategy/,37,1618507723.0,"This week, Microsoft announced the $19.7-billion acquisition of Nuance, a company that uses deep learning to transcribe clinical appointments (and other stuff). What's interesting about the deal is the [evolution of Microsoft's relation with Nuance](https://bdtechtalks.com/2021/04/15/microsoft-nuance-acquisition/), going from cloud provider to partner to owner. 

This is a successful strategy that only Microsoft (and maybe Amazon) is in a position to implement:

Step 1: Microsoft starts by investing in ML companies by giving them Azure credits and luring them into its ML platform. This allows Microsoft to help the companies develop and also learn from them (and possibly replicate their products if it's worth it). Multiple small investments as opposed to one large acquisition is a smart move because many companies are trying new things in ML/DL, few of which will be successful. With small investments, Microsoft can cast a wider net and make sure it is in a good position to make the next move.

Step 2: Microsoft enters partnership with companies that have successful products. This allows Microsoft to integrate their ML products into its enterprise solutions (e.g., Nuance's Dragon DL was integrated into Microsoft's cloud healthcare solution). Since these companies are building their ML tools on top of Azure's stack, the integration is much easier for both companies.

Step 3: Acquire really successful companies (Nuance has a great reach in the AI+healthcare sector). This allows Microsoft to gain exclusive access to the company's data, talent, technology, and clients. With the acquisition of Nuance, Microsoft's total addressable market in healthcare has reached $500B+. And it can integrate its ML technology into its other enterprise tools.

Nuance is just one example of Microsoft's ML acquisition strategy. The company is on a similar path [with OpenAI](https://bdtechtalks.com/2020/09/24/microsoft-openai-gpt-3-license/) and is carrying out [a similar strategy in the self-driving car industry](https://bdtechtalks.com/2021/01/21/microsoft-self-driving-car-strategy/)."
36,machinelearning,gpt-3,top,2023-10-03 12:56:26,"[R] MIT, Meta, CMU Researchers: LLMs trained with a finite attention window can be extended to infinite sequence lengths without any fine-tuning",Successful-Western27,False,0.97,284,16yr7kx,https://www.reddit.com/r/MachineLearning/comments/16yr7kx/r_mit_meta_cmu_researchers_llms_trained_with_a/,43,1696337786.0,"LLMs like GPT-3 struggle in streaming uses like chatbots because their performance tanks on long texts exceeding their training length. I checked out a new paper investigating why windowed attention fails for this.

By visualizing the attention maps, the researchers noticed LLMs heavily attend initial tokens as ""attention sinks"" even if meaningless. This anchors the distribution.

They realized evicting these sink tokens causes the attention scores to get warped, destabilizing predictions.

Their proposed ""StreamingLLM"" method simply caches a few initial sink tokens plus recent ones. This tweaks LLMs to handle crazy long texts. Models tuned with StreamingLLM smoothly processed sequences with millions of tokens, and were up to 22x faster than other approaches. 

Even cooler - adding a special ""\[Sink Token\]"" during pre-training further improved streaming ability. The model just used that single token as the anchor. I think the abstract says it best:

>We introduce StreamingLLM, an efficient framework that enables LLMs trained with a **finite length attention window** to generalize to **infinite sequence length without any fine-tuning**. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.

TLDR: LLMs break on long convos. Researchers found they cling to initial tokens as attention sinks. Caching those tokens lets LLMs chat infinitely.

[**Full summary here**](https://notes.aimodels.fyi/llm-infinite-context-window-streamingllm/)

**Paper link:** [**https://arxiv.org/pdf/2309.17453.pdf**](https://arxiv.org/pdf/2309.17453.pdf)"
37,machinelearning,gpt-3,top,2022-07-10 05:39:21,[D] Noam Chomsky on LLMs and discussion of LeCun paper (MLST),timscarfe,False,0.89,281,vvkmf1,https://www.reddit.com/r/MachineLearning/comments/vvkmf1/d_noam_chomsky_on_llms_and_discussion_of_lecun/,258,1657431561.0,"""First we should ask the question whether LLM have achieved ANYTHING, ANYTHING in this domain. Answer, NO, they have achieved ZERO!"" - Noam Chomsky 

""There are engineering projects that are significantly advanced by \[[\#DL](https://mobile.twitter.com/hashtag/DL?src=hashtag_click)\] methods. And this is all the good. \[...\] Engineering is not a trivial field; it takes intelligence, invention, \[and\] creativity these achievements. That it contributes to science?"" - Noam Chomsky 

""There was a time \[supposedly dedicated\] to the study of the nature of [\#intelligence](https://mobile.twitter.com/hashtag/intelligence?src=hashtag_click). By now it has disappeared.""  Earlier, same interview: ""GPT-3 can \[only\] find some superficial irregularities in the data. \[...\] It's exciting for reporters in the NY Times."" - Noam Chomsky 

""It's not of interest to people, the idea of finding an explanation for something. \[...\] The \[original [\#AI](https://mobile.twitter.com/hashtag/AI?src=hashtag_click)\] field by now is considered old-fashioned, nonsense. \[...\] That's probably where the field will develop, where the money is. \[...\] But it's a shame."" - Noam Chomsky 

Thanks to Dagmar Monett for selecting the quotes!

Sorry for posting a controversial thread -- but this seemed noteworthy for /machinelearning 

Video: [https://youtu.be/axuGfh4UR9Q](https://youtu.be/axuGfh4UR9Q) \-- also some discussion of LeCun's recent position paper"
38,machinelearning,gpt-3,top,2022-01-28 17:39:35,[D] It seems OpenAI’s new embedding models perform terribly,StellaAthena,False,0.97,279,sew5rl,https://www.reddit.com/r/MachineLearning/comments/sew5rl/d_it_seems_openais_new_embedding_models_perform/,80,1643391575.0,"Some people on Twitter have been investigating [OpenAI’s new embedding API](https://openai.com/blog/introducing-text-and-code-embeddings/) and it’s shocking how poorly it performs. On standard benchmarks, open source models 1000x smaller obtain equal or better performance! Models based on RoBERTa and T5, as well as the Sentence Transformer all achieve significantly better performance than the 175B model. Also of interest is that the DaVinci (175B) model is not clearly better than the Ada (350M) model.

Has anyone tried adapting some other autoregressive languages models, such as GPT-2, GPT-Neo, or GPT-J to do embeddings? I’m quite curious if this is an inherent failing of autoregressive models or if there’s something else going on. **Edit:** [a commenter](https://www.reddit.com/r/MachineLearning/comments/sew5rl/d_it_seems_openais_new_embedding_models_perform/humuzef/) has asked that I point out that I am one of the creators of GPT-Neo and part of the org that created GPT-J. These examples were not intended as specific endorsements, and I would be just as interested in comparisons using other billion-parameter+ autoregressive language models.

**Edit 2:** I originally linked to a [tweet](https://twitter.com/Nils_Reimers/status/1487014195568775173?s=20&amp;amp;amp;amp;amp;amp;amp;t=NBF7D2DYi41346cGM-PQjQ) about this, but several commenters pointed out that there’s also a [blog post](https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9) with more information.

**Edit 3:** An OpenAI researcher [seems to have responded](https://mobile.twitter.com/arvind_io/status/1487188996774002688)."
39,machinelearning,gpt-3,top,2022-12-20 22:54:48,[R] Nonparametric Masked Language Modeling - MetaAi 2022 - NPM - 500x fewer parameters than GPT-3 while outperforming it on zero-shot tasks,Singularian2501,False,0.98,270,zr2en7,https://www.reddit.com/r/MachineLearning/comments/zr2en7/r_nonparametric_masked_language_modeling_metaai/,31,1671576888.0,"Paper: [https://arxiv.org/abs/2212.01349](https://arxiv.org/abs/2212.01349)

Github: [https://github.com/facebookresearch/NPM](https://github.com/facebookresearch/NPM)

Abstract:

>Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce **NPM**, the first **nonparametric masked language model** that **replaces this softmax with a nonparametric distribution over every phrase in a reference corpus**. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 9 closed-set tasks and 7 open-set tasks demonstrates that **NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach**. It is particularly **better on dealing with rare patterns (word senses or facts),** and **predicting rare or nearly unseen words (e.g., non-Latin script)**.

https://preview.redd.it/qf2lqrkku47a1.jpg?width=658&format=pjpg&auto=webp&s=7dc7e76f3075b4b4f0916c2de1e442b19b2c0f49

https://preview.redd.it/gqhlbykku47a1.jpg?width=1241&format=pjpg&auto=webp&s=39f63470d18ea6f4a8ed560b371cc46b939b2c6f

https://preview.redd.it/p7bzdukku47a1.jpg?width=883&format=pjpg&auto=webp&s=6a8eb2b66abcb1581abf7280180c1c0e86201232

https://preview.redd.it/z6niwykku47a1.jpg?width=1112&format=pjpg&auto=webp&s=8337a4802db983df1a4b0b11934c0708888641a4

https://preview.redd.it/s8fdhxkku47a1.jpg?width=1361&format=pjpg&auto=webp&s=28b307df857ef2262d3f8348fd1094ebb793a63d

https://preview.redd.it/94t5fwkku47a1.jpg?width=1362&format=pjpg&auto=webp&s=da8bca8fd08ecaf956658c674f5a32a930cdd3a2"
40,machinelearning,gpt-3,top,2023-05-26 13:57:42,[N] Abu Dhabi's TTI releases open-source Falcon-7B and -40B LLMs,Balance-,False,0.95,266,13sdz8p,https://www.reddit.com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/,58,1685109462.0,"Abu Dhabi's Technology Innovation Institute (TII) just released new 7B and 40B LLMs.

The Falcon-40B model is now at the top of the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), beating *llama-30b-supercot* and *llama-65b* among others.

| Model                      | Revision | Average | ARC (25-shot) | HellaSwag (10-shot) | MMLU (5-shot) | TruthfulQA (0-shot) |
|----------------------------|----------|-----------|-----------------|-----------------------|-----------------|-----------------------|
| tiiuae/falcon-40b          | main     | 60.4      | 61.9            | 85.3                  | 52.7            | 41.7                  |
| ausboss/llama-30b-supercot | main     | 59.8      | 58.5            | 82.9                  | 44.3            | 53.6                  |
| llama-65b                  | main     | 58.3      | 57.8            | 84.2                  | 48.8            | 42.3                  |
| MetaIX/GPT4-X-Alpasta-30b  | main     | 57.9      | 56.7            | 81.4                  | 43.6            | 49.7                  |

**Press release:** [UAE's Technology Innovation Institute Launches Open-Source ""Falcon 40B"" Large Language Model for Research & Commercial Utilization](https://www.tii.ae/news/uaes-technology-innovation-institute-launches-open-source-falcon-40b-large-language-model)

>The Technology Innovation Institute (TII) in Abu Dhabi has announced its open-source large language model (LLM), the Falcon 40B. With 40 billion parameters, Falcon 40B is the UAE's first large-scale AI model, indicating the country's ambition in the field of AI and its commitment to promote innovation and research.  
>  
>Unlike most LLMs, which typically only provide non-commercial users access, Falcon 40B is open to both research and commercial usage. The TII has also included the model's weights in the open-source package, which will enhance the model's capabilities and allow for more effective fine-tuning.  
>  
>In addition to the launch of Falcon 40B, the TII has initiated a call for proposals from researchers and visionaries interested in leveraging the model to create innovative use cases or explore further applications. As a reward for exceptional research proposals, selected projects will receive ""training compute power"" as an investment, allowing for more robust data analysis and complex modeling. VentureOne, the commercialization arm of ATRC, will provide computational resources for the most promising projects.  
>  
>TII's Falcon 40B has shown impressive performance since its unveiling in March 2023. When benchmarked using Stanford University’s HELM LLM tool, it used less training compute power compared to other renowned LLMs such as OpenAI's GPT-3, DeepMind's Chinchilla AI, and Google's PaLM-62B.  
>  
>Those interested in accessing Falcon 40B or proposing use cases can do so through the [FalconLLM.TII.ae](https://FalconLLM.TII.ae) website. Falcon LLMs open-sourced to date are available under a license built upon the principles of the open-source Apache 2.0 software, permitting a broad range of free use.

**Hugging Face links**

* [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) / [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)
* [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) / [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)"
41,machinelearning,gpt-3,top,2023-04-06 21:45:18,[D] Is all the talk about what GPT can do on Twitter and Reddit exaggerated or fairly accurate?,ThePhantomguy,False,0.89,269,12dz4hh,https://www.reddit.com/r/MachineLearning/comments/12dz4hh/d_is_all_the_talk_about_what_gpt_can_do_on/,311,1680817518.0,"I saw [this post](https://www.reddit.com/r/ChatGPT/comments/12diapw/gpt4_week_3_chatbots_are_yesterdays_news_ai/?utm_source=share&utm_medium=ios_app&utm_name=iossmf) on the r/ChatGPT subreddit, and I’ve been seeing similar talk on Twitter. There’s people talking about AGI, the singularity, and etc. I get that it’s cool, exciting, and fun; but some of the talk seems a little much? Like it reminds me of how the NFT bros would talk about blockchain technology.

Do any of the people making these kind of claims have a decent amount of knowledge on machine learning at all? The scope of my own knowledge is very limited, as I’ve only implemented and taken courses on models that are pretty old. So I’m here to ask for opinions from ya’ll. Is there some validity, or is it just people that don’t really understand what they’re saying and making grand claims (Like some sort of Dunning Kruger Effect)?"
42,machinelearning,gpt-3,top,2023-02-03 21:31:19,[R] Multimodal Chain-of-Thought Reasoning in Language Models - Amazon Web Services Zhuosheng Zhang et al - Outperforms GPT-3.5 by 16% (75%->91%) and surpasses human performance on ScienceQA while having less than 1B params!,Singularian2501,False,0.99,261,10svwch,https://www.reddit.com/r/MachineLearning/comments/10svwch/r_multimodal_chainofthought_reasoning_in_language/,56,1675459879.0,"Paper: [https://arxiv.org/abs/2302.00923](https://arxiv.org/abs/2302.00923) 

Github: [https://github.com/amazon-science/mm-cot](https://github.com/amazon-science/mm-cot) 

Twitter: [https://paperswithcode.com/top-social](https://paperswithcode.com/top-social) 

Abstract:

>Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies are mostly isolated in the language modality with LLMs, where LLMs are hard to deploy. To elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning. The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference. To mitigate the effect of such mistakes, we propose Multimodal-CoT that incorporates vision features in a decoupled training framework. The framework separates the rationale generation and answer inference into two stages. By incorporating the vision features in both stages, the model is able to generate effective rationales that contribute to answer inference. **With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16% (75.17%->91.68%) on the ScienceQA benchmark and even surpasses human performance.** 

https://preview.redd.it/g9eo0f94k1ga1.jpg?width=1331&format=pjpg&auto=webp&s=9b5fc84b424aff7160b69ff7c7a5fad071cbb7d2

https://preview.redd.it/fgboci94k1ga1.jpg?width=1323&format=pjpg&auto=webp&s=35215544d9e0a74881c42503d04b62ab09081af1

https://preview.redd.it/2ojfym94k1ga1.jpg?width=1660&format=pjpg&auto=webp&s=cf040c4f422f6c323e8c4d75474a5881f45a41d1

https://preview.redd.it/k7huem94k1ga1.jpg?width=1326&format=pjpg&auto=webp&s=f4326a5088744d3856e5c5c23311be6348fab924

https://preview.redd.it/05m8rf94k1ga1.jpg?width=658&format=pjpg&auto=webp&s=ac4110e57a49fcea6f8c03571edd391ff71bd13d"
43,machinelearning,gpt-3,top,2021-05-05 16:04:43,[N] Wired: It Began As an AI-Fueled Dungeon Game. It Got Much Darker (AI Dungeon + GPT-3),minimaxir,False,0.94,256,n5jgn4,https://www.reddit.com/r/MachineLearning/comments/n5jgn4/n_wired_it_began_as_an_aifueled_dungeon_game_it/,62,1620230683.0,"https://www.wired.com/story/ai-fueled-dungeon-game-got-much-darker/

If you haven't been following the drama around AI Dungeon, this is a good summary and a good discussion on filter/algo difficulty."
44,machinelearning,gpt-3,top,2021-06-09 02:07:38,"[P] GPT-J, 6B JAX-based Transformer LM",Aran_Komatsuzaki,False,0.98,252,nvkowg,https://www.reddit.com/r/MachineLearning/comments/nvkowg/p_gptj_6b_jaxbased_transformer_lm/,52,1623204458.0,"Ben and I have released GPT-J, 6B JAX-based Transformer LM!

\- Performs on par with 6.7B GPT-3

\- Performs better and decodes faster than GPT-Neo

\- repo + colab + free web demo

\- Trained on 400B tokens with TPU v3-256 for five weeks

\- GPT-J performs much closer to GPT-3 of similar size than GPT-Neo

https://preview.redd.it/e1yqex9it4471.png?width=908&format=png&auto=webp&s=a6411d57530d5f34e8524fd50fa3f1640421181a

tweet: [https://bit.ly/3isa84D](https://bit.ly/3isa84D)

article: [https://bit.ly/2TH8yl0](https://bit.ly/2TH8yl0)

repo: [https://bit.ly/3eszQ6C](https://bit.ly/3eszQ6C)

Colab: [https://bit.ly/3w0fB6n](https://bit.ly/3w0fB6n)

demo: [https://bit.ly/3psRCdM](https://bit.ly/3psRCdM)"
45,machinelearning,gpt-3,top,2022-07-19 19:39:44,[D] Most important unsolved problems in AI research,carubia,False,0.95,250,w31fpp,https://www.reddit.com/r/MachineLearning/comments/w31fpp/d_most_important_unsolved_problems_in_ai_research/,136,1658259584.0,"[Updated: items marked with * were added/updated based on the responses so far].

Suggesting this topic for discussion, as I am trying to identify the current most important unsolved problems in AI research. Below are a few proposed items that are top of mind for me, would appreciate any input (what to add or what to remove from the list) and relevant sources.

---

Compositionality*. Ability to perform symbolic operations, generalize, including learning from a relatively small set of samples, and get the most out of every sample (sample efficiency and few-shot learning), etc. Also includes the ability to learn by receiving explicit instructions. (e.g. https://arxiv.org/abs/2205.01128)

Multimodality*. Ability to process and relate information from multiple modalities, like text, audio, visual, etc.

Ability to match knowledge to context. For e.g. the text generated by the LLM is a great match for a sci-fi novel, but not as advice to a patient regarding their medical condition.

Uncertainty awareness*. Ability to characterize uncertainty relative to the similarity of the current observations to the training data, explain it to an observer, and adjust behavior if necessary. (https://arxiv.org/pdf/1809.07882.pdf)

Catastrophic forgetting. It is a known limitation to continual learning, however, it seems like the large-scale models show an indication of robustness. (http://www.cognitionresearch.org/papers/overview/sparchai.pdf)

Enabling robust continuous learning in deployment. The current paradigm separates training and inference, while in biology intelligent creatures are capable of continuous learning. 

Figuring out an approach for the messy middle.
- Low-level operations with a focus on a very narrow scope and maximum efficiency seem reasonably straightforward and enjoy growing application in the industry. Noise removing, pattern recognition, recommenders, etc. Specialized ANNs seem to have success there.
- High-level abstract reasoning is being explored by large language and multi-modal models. Like our explicit reasoning (solving a math problem, or learning to operate a new coffee machine) it is extremely powerful, but also slow and resource-intensive. (E.g. https://arxiv.org/abs/2207.05608)
- But there is that middle, as in driving, where we still do fairly complex operations with very high reliability, precision, and responsiveness, all with low cognitive load (figuratively “on autopilot”). 

Explainability* - enabling human experts to understand the underlying factors of why an AI decision has been made.
https://link.springer.com/chapter/10.1007/978-3-031-04083-2_2

Alignment* - ensuring that AI is properly aligned with human values. https://link.springer.com/article/10.1007/s11023-020-09539-2

Energy efficiency. The human brain is believed to consume tens of W of power (https://www.pnas.org/doi/10.1073/pnas.172399499) while less capable LLMs like GPT-3 require several kW (estimated as the power consumption of DGX A100 based on https://www.reddit.com/r/singularity/comments/inp025/if_you_want_to_run_your_own_full_gpt3_instance/). Two orders of magnitude more."
46,machinelearning,gpt-3,top,2019-07-20 15:36:49,[D] How the Transformers broke NLP leaderboards,milaworld,False,0.96,251,cfn4bu,https://www.reddit.com/r/MachineLearning/comments/cfn4bu/d_how_the_transformers_broke_nlp_leaderboards/,50,1563637009.0,"*I came across this interesting [article](https://hackingsemantics.xyz/2019/leaderboards/) about whether larger models + more data = progress in ML research.*

**[How the Transformers broke NLP leaderboards](https://hackingsemantics.xyz/2019/leaderboards/)**

*Excerpt:*

The focus of this post is yet another problem with the leaderboards that is relatively recent. Its cause is simple: fundamentally, **a model may be better than its competitors by building better representations from the available data - or it may simply use more data, and/or throw a deeper network at it**. When we have a paper presenting a new model that also uses more data/compute than its competitors, credit attribution becomes hard.

The most popular NLP leaderboards are currently dominated by Transformer-based models. BERT received the best paper award at NAACL 2019 after months of holding SOTA on many leaderboards. Now the hot topic is XLNet that is said to overtake BERT on GLUE and some other benchmarks. Other Transformers include GPT-2, ERNIE, and the list is growing.

The problem we’re starting to face is that these models are HUGE. While the source code is available, in reality it is beyond the means of an average lab to reproduce these results, or to produce anything comparable. For instance, XLNet is trained on 32B tokens, and the price of using 500 TPUs for 2 days is over $250,000. Even fine-tuning this model is getting expensive.

Wait, this was supposed to happen!

On the one hand, this trend looks predictable, even inevitable: people with more resources *will* use more resources to get better performance. One could even argue that a huge model proves its scalability and fulfils the inherent promise of deep learning, i.e. being able to learn more complex patterns from more information. Nobody knows how much data we actually need to solve a given NLP task, but more should be better, and limiting data seems counter-productive.

On that view - well, from now on top-tier NLP research is going to be something possible only for industry. Academics will have to somehow up their game, either by getting more grants or by collaborating with high-performance computing centers. They are also welcome to switch to analysis, building something on top of the industry-provided huge models, or making datasets.

However, in terms of overall progress in NLP that might not be the best thing to do. The chief problem with the huge models is simply this:

“More data & compute = SOTA” is **NOT** research news.

If leaderboards are to highlight the actual progress, we need to incentivize new architectures rather than teams outspending each other. Obviously, huge pretrained models are valuable, but unless the authors show that their system consistently behaves differently from its competition with comparable data & compute, it is not clear whether they are presenting a model or a resource.

Furthermore, much of this research is not reproducible: nobody is going to spend $250,000 just to repeat XLNet training. Given the fact that its ablation study showed only 1-2% gain over BERT in 3 datasets out of 4, we don’t actually know for sure that its masking strategy is more successful than BERT’s.

At the same time, the development of leaner models is dis-incentivized, as their task is fundamentally harder and the leaderboard-oriented community only rewards the SOTA. That, in its turn, prices out of competitions academic teams, which will not result in students becoming better engineers when they graduate.

*Entire article:*

https://hackingsemantics.xyz/2019/leaderboards/"
47,machinelearning,gpt-3,top,2023-05-15 00:00:05,[D] On LLMs' ability to perform random sampling,bgighjigftuik,False,0.91,247,13hrkbj,https://i.redd.it/l1uyl3k3gxza1.jpg,104,1684108805.0,"To me this is the most counterintuitive thing I've yet seen from a language model. In this case it was ChatGPT (3.5, the free version); but I can see this happening in any model.

Here's the prompt I used on a fresh session:

""Generate 60 random numbers coming from an exponential distribution with lambda=6.4. Don't write any code""

It outputs the following values:

0.013, 0.078, 0.266, 0.040, 0.117, 0.283, 0.048, 0.078, 0.181, 0.134, 0.017, 0.160, 0.052, 0.196, 0.096, 0.098, 0.019, 0.048, 0.141, 0.071, 0.106, 0.023, 0.197, 0.062, 0.029, 0.072, 0.111, 0.166, 0.080, 0.027, 0.124, 0.016, 0.135, 0.052, 0.078, 0.161, 0.078, 0.139, 0.068, 0.066, 0.039, 0.056, 0.018, 0.082, 0.180, 0.086, 0.024, 0.022, 0.057, 0.042, 0.050, 0.104, 0.099, 0.062, 0.016, 0.186, 0.043, 0.085, 0.145, 0.110.

I get to plot a histogram with the numbers and they are close to what I would consider an exponential distribution with such parameter (you can see the plot).

Given that GPT 3.5 does not have access to a Python interpreter, how on earth is it able to do so? I have also tried other distributions and parameters and it kind of works. It's not perfect, but with normal distributions it is usually close to what scipy.stats would generate.

I could understand that it can have learnt to interpret Python code to some extent, but honestly I can't find explanation for random sampling from a probability distribution. For a Normal distribution, I can tell it about the desired mean and variance, and it samples values that are more than reasonable (and close to the true mean/variance specified).

Any thoughts? I honestly am unable to wrap my head around how a LLM can have the understanding on how to sample tokens (at digit level) to fit any probability distribution. To me it seems very unlikely to have similar data either the pre-training or fine-tuning stages."
48,machinelearning,gpt-3,top,2023-05-24 01:00:28,"Interview with Juergen Schmidhuber, renowned ‘Father Of Modern AI’, says his life’s work won't lead to dystopia.",hardmaru,False,0.81,247,13q6k4a,https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/,96,1684890028.0,"*Schmidhuber interview expressing his views on the future of AI and AGI.*

*Original [source](https://www.forbes.com/sites/hessiejones/2023/05/23/juergen-schmidhuber-renowned-father-of-modern-ai-says-his-lifes-work-wont-lead-to-dystopia/). I think the interview is of interest to r/MachineLearning, and presents an alternate view, compared to other influential leaders in AI.*

**Juergen Schmidhuber, Renowned 'Father Of Modern AI,' Says His Life’s Work Won't Lead To Dystopia**

*May 23, 2023. Contributed by [Hessie Jones](https://twitter.com/hessiejones).*

Amid the growing concern about the impact of more advanced artificial intelligence (AI) technologies on society, there are many in the technology community who fear the implications of the advancements in Generative AI if they go unchecked. Dr. Juergen Schmidhuber, a renowned scientist, artificial intelligence researcher and widely regarded as one of the pioneers in the field, is more optimistic. He declares that many of those who suddenly warn against the dangers of AI are just seeking publicity, exploiting the media’s obsession with killer robots which has attracted more attention than “good AI” for healthcare etc.

The potential to revolutionize various industries and improve our lives is clear, as are the equal dangers if bad actors leverage the technology for personal gain. Are we headed towards a dystopian future, or is there reason to be optimistic? I had a chance to sit down with Dr. Juergen Schmidhuber to understand his perspective on this seemingly fast-moving AI-train that will leap us into the future.

As a teenager in the 1970s, Juergen Schmidhuber became fascinated with the idea of creating intelligent machines that could learn and improve on their own, becoming smarter than himself within his lifetime. This would ultimately lead to his groundbreaking work in the field of deep learning.

In the 1980s, he studied computer science at the Technical University of Munich (TUM), where he earned his diploma in 1987. His thesis was on the ultimate self-improving machines that, not only, learn through some pre-wired human-designed learning algorithm, but also learn and improve the learning algorithm itself. Decades later, this became a hot topic. He also received his Ph.D. at TUM in 1991 for work that laid some of the foundations of modern AI.

Schmidhuber is best known for his contributions to the development of recurrent neural networks (RNNs), the most powerful type of artificial neural network that can process sequential data such as speech and natural language. With his students Sepp Hochreiter, Felix Gers, Alex Graves, Daan Wierstra, and others, he published architectures and training algorithms for the long short-term memory (LSTM), a type of RNN that is widely used in natural language processing, speech recognition, video games, robotics, and other applications. LSTM has become the most cited neural network of the 20th century, and Business Week called it ""[arguably the most commercial AI achievement](https://www.bloomberg.com/news/features/2018-05-15/google-amazon-and-facebook-owe-j-rgen-schmidhuber-a-fortune?leadSource=uverify%20wall).""

Throughout his career, Schmidhuber has received various awards and accolades for his groundbreaking work. In 2013, he was awarded the Helmholtz Prize, which recognizes significant contributions to the field of machine learning. In 2016, he was awarded the IEEE Neural Network Pioneer Award for ""*pioneering contributions to deep learning and neural networks."" The media have often called him the “father of modern AI,*” because the [most cited neural networks](https://people.idsia.ch/~juergen/most-cited-neural-nets.html) all build on his lab’s work. He is quick to point out, however, that AI history [goes back centuries.](https://people.idsia.ch/~juergen/deep-learning-history.html)

Despite his many accomplishments, at the age of 60, he feels mounting time pressure towards building an Artificial General Intelligence within his lifetime and remains committed to pushing the boundaries of AI research and development. He is currently director of the KAUST AI Initiative, scientific director of the Swiss AI Lab IDSIA, and co-founder and chief scientist of AI company NNAISENSE, whose motto is ""AI∀"" which is a math-inspired way of saying ""AI For All."" He continues to work on cutting-edge AI technologies and applications to improve human health and extend human lives and make lives easier for everyone.

*The following interview has been edited for clarity.*

**Jones: Thank you Juergen for joining me. You have signed letters warning about AI weapons. But you didn't sign the recent publication, ""Pause Gigantic AI Experiments: An Open Letter""? Is there a reason?**

**Schmidhuber:** Thank you Hessie. Glad to speak with you. I have realized that many of those who warn in public against the dangers of AI are just seeking publicity. I don't think the latest letter will have any significant impact because many AI researchers, companies, and governments will ignore it completely.

The proposal frequently uses the word ""we"" and refers to ""us,"" the humans. But as I have pointed out many times in the past, there is no ""we"" that everyone can identify with. Ask 10 different people, and you will hear 10 different opinions about what is ""good."" Some of those opinions will be completely incompatible with each other. Don't forget the enormous amount of conflict between the many people.

The letter also says, ""*If such a pause cannot be quickly put in place, governments should intervene and impose a moratorium.*"" The problem is that different governments have ALSO different opinions about what is good for them and for others. Great Power A will say, if we don't do it, Great Power B will, perhaps secretly, and gain an advantage over us. The same is true for Great Powers C and D.

**Jones: Everyone acknowledges this fear surrounding current generative AI technology. Moreover, the existential threat of this technology has been publicly acknowledged by** [**Sam Altman**](https://www.bbc.com/news/world-us-canada-65616866)**, CEO of OpenAI himself, calling for AI regulation. From your perspective, is there an existential threat?**

**Schmidhuber:** It is true that AI can be weaponized, and I have no doubt that there will be all kinds of AI arms races, but AI does not introduce a new quality of existential threat. The threat coming from AI weapons seems to pale in comparison to the much older threat from nuclear hydrogen bombs that don’t need AI at all. We should be much more afraid of half-century-old tech in the form of H-bomb rockets. The Tsar Bomba of 1961 had almost 15 times more destructive power than all weapons of WW-II combined.  Despite the dramatic nuclear disarmament since the 1980s, there are still more than enough nuclear warheads to wipe out human civilization within two hours, without any AI I’m much more worried about that old existential threat than the rather harmless AI weapons.

**Jones: I realize that while you compare AI to the threat of nuclear bombs, there is a current danger that a current technology can be put in the hands of humans and enable them to “eventually” exact further harms to individuals of group in a very precise way, like targeted drone attacks. You are giving people a toolset that they've never had before, enabling bad actors, as some have pointed out, to be able to do a lot more than previously because they didn't have this technology.**

**Schmidhuber:** Now, all that sounds horrible in principle, but our existing laws are sufficient to deal with these new types of weapons enabled by AI. If you kill someone with a gun, you will go to jail. Same if you kill someone with one of these drones. Law enforcement will get better at understanding new threats and new weapons and will respond with better technology to combat these threats. Enabling drones to target persons from a distance in a way that requires some tracking and some intelligence to perform, which has traditionally been performed by skilled humans, to me, it seems is just an improved version of a traditional weapon, like a gun, which is, you know, a little bit smarter than the old guns.

But, in principle, all of that is not a new development. For many centuries, we have had the evolution of better weaponry and deadlier poisons and so on, and law enforcement has evolved their policies to react to these threats over time. So, it's not that we suddenly have a new quality of existential threat and it's much more worrisome than what we have had for about six decades. A large nuclear warhead doesn’t need fancy face recognition to kill an individual. No, it simply wipes out an entire city with ten million inhabitants.

**Jones: The existential threat that’s implied is the extent to which humans have control over this technology. We see some early cases of opportunism which, as you say, tends to get more media attention than positive breakthroughs. But you’re implying that this will all balance out?**

**Schmidhuber:** Historically, we have a long tradition of technological breakthroughs that led to advancements in weapons for the purpose of defense but also for protection. From sticks, to rocks, to axes to gunpowder to cannons to rockets… and now to drones… this has had a drastic influence on human history but what has been consistent throughout history is that those who are using technology to achieve their own ends are themselves, facing the same technology because the opposing side is learning to use it against them. And that's what has been repeated in thousands of years of human history and it will continue. I don't see the new AI arms race as something that is remotely as existential a threat as the good old nuclear warheads.

You said something important, in that some people prefer to talk about the downsides rather than the benefits of this technology, but that's misleading, because 95% of all AI research and AI development is about making people happier and advancing human life and health.

**Jones: Let’s touch on some of those beneficial advances in AI research that have been able to radically change present day methods and achieve breakthroughs.**

**Schmidhuber:** All right! For example, eleven years ago, our team with my postdoc Dan Ciresan was the first to win a [medical imaging competition through deep learning](https://people.idsia.ch/~juergen/first-time-deep-learning-won-medical-imaging-contest-september-2012.html). We analyzed female breast cells with the objective to determine harmless cells vs. those in the pre-cancer stage. Typically, a trained oncologist needs a long time to make these determinations. Our team, who knew nothing about cancer, were able to train an artificial neural network, which was totally dumb in the beginning, on lots of this kind of data. It was able to outperform all the other methods. Today, this is being used not only for breast cancer, but also for radiology and detecting plaque in arteries, and many other things.  Some of the neural networks that we have developed in the last 3 decades are now prevalent across thousands of healthcare applications, detecting Diabetes and Covid-19 and what not. This will eventually permeate across all healthcare. The good consequences of this type of AI are much more important than the click-bait new ways of conducting crimes with AI.

**Jones: Adoption is a product of reinforced outcomes. The massive scale of adoption either leads us to believe that people have been led astray, or conversely, technology is having a positive effect on people’s lives.**

**Schmidhuber:** The latter is the likely case. There's intense commercial pressure towards good AI rather than bad AI because companies want to sell you something, and you are going to buy only stuff you think is going to be good for you. So already just through this simple, commercial pressure, you have a tremendous bias towards good AI rather than bad AI. However, doomsday scenarios like in Schwarzenegger movies grab more attention than documentaries on AI that improve people’s lives.

**Jones: I would argue that people are drawn to good stories – narratives that contain an adversary and struggle, but in the end, have happy endings. And this is consistent with your comment on human nature and how history, despite its tendency for violence and destruction of humanity, somehow tends to correct itself.**

**Let’s take the example of a technology, which you are aware – GANs – General Adversarial Networks, which today has been used in applications for fake news and disinformation. In actuality, the purpose in the invention of GANs was far from what it is used for today.**

**Schmidhuber:** Yes, the name GANs was created in 2014 but we had the basic principle already in the early 1990s. More than 30 years ago, I called it *artificial curiosity*. It's a very simple way of injecting creativity into a little two network system. This creative AI is not just trying to slavishly imitate humans. Rather, it’s inventing its own goals. Let me explain:

You have two networks. One network is producing outputs that could be anything, any action. Then the second network is looking at these actions and it’s trying to predict the consequences of these actions. An action could move a robot, then something happens, and the other network is just trying to predict what will happen.

Now we can implement artificial curiosity by reducing the prediction error of the second network, which, at the same time, is the reward of the first network. The first network wants to maximize its reward and so it will invent actions that will lead to situations that will surprise the second network, which it has not yet learned to predict well.

In the case where the outputs are fake images, the first network will try to generate images that are good enough to fool the second network, which will attempt to predict the reaction of the environment: fake or real image, and it will try to become better at it. The first network will continue to also improve at generating images whose type the second network will not be able to predict. So, they fight each other. The 2nd network will continue to reduce its prediction error, while the 1st network will attempt to maximize it.

Through this zero-sum game the first network gets better and better at producing these convincing fake outputs which look almost realistic. So, once you have an interesting set of images by Vincent Van Gogh, you can generate new images that leverage his style, without the original artist having ever produced the artwork himself.

**Jones: I see how the Van Gogh example can be applied in an education setting and there are countless examples of artists mimicking styles from famous painters but image generation from this instance that can happen within seconds is quite another feat. And you know this is how GANs has been used. What’s more prevalent today is a socialized enablement of generating images or information to intentionally fool people. It also surfaces new harms that deal with the threat to intellectual property and copyright, where laws have yet to account for. And from your perspective this was not the intention when the model was conceived. What was your motivation in your early conception of what is now GANs?**

**Schmidhuber:** My old motivation for GANs was actually very important and it was not to create deepfakes or fake news but to enable AIs to be curious and invent their own goals, to make them explore their environment and make them creative.

Suppose you have a robot that executes one action, then something happens, then it executes another action, and so on, because it wants to achieve certain goals in the environment. For example, when the battery is low, this will trigger “pain” through hunger sensors, so it wants to go to the charging station, without running into obstacles, which will trigger other pain sensors. It will seek to minimize pain (encoded through numbers). Now the robot has a friend, the second network, which is a world model ––it’s a prediction machine that learns to predict the consequences of the robot’s actions.

Once the robot has a good model of the world, it can use it for planning. It can be used as a simulation of the real world. And then it can determine what is a good action sequence. If the robot imagines this sequence of actions, the model will predict a lot of pain, which it wants to avoid. If it plays this alternative action sequence in its mental model of the world, then it will predict a rewarding situation where it’s going to sit on the charging station and its battery is going to load again. So, it'll prefer to execute the latter action sequence.

In the beginning, however, the model of the world knows nothing, so how can we motivate the first network to generate experiments that lead to data that helps the world model learn something it didn’t already know? That’s what artificial curiosity is about. The dueling two network systems effectively explore uncharted environments by creating experiments so that over time the curious AI gets a better sense of how the environment works. This can be applied to all kinds of environments, and has medical applications.

**Jones: Let’s talk about the future. You have said, “*****Traditional humans won’t play a significant role in spreading intelligence across the universe.*****”**

**Schmidhuber:** Let’s first conceptually separate two types of AIs. The first type of AI are tools directed by humans. They are trained to do specific things like accurately detect diabetes or heart disease and prevent attacks before they happen. In these cases, the goal is coming from the human. More interesting AIs are setting their own goals. They are inventing their own experiments and learning from them. Their horizons expand and eventually they become more and more general problem solvers in the real world. They are not controlled by their parents, but much of what they learn is through self-invented experiments.

A robot, for example, is rotating a toy, and as it is doing this, the video coming in through the camera eyes, changes over time and it begins to learn how this video changes and learns how the 3D nature of the toy generates certain videos if you rotate it a certain way, and eventually, how gravity works, and how the physics of the world works. Like a little scientist!

And I have predicted for decades that future scaled-up versions of such AI scientists will want to further expand their horizons, and eventually go where most of the physical resources are, to build more and bigger AIs. And of course, almost all of these resources are far away from earth out there in space, which is hostile to humans but friendly to appropriately designed AI-controlled robots and self-replicating robot factories. So here we are not talking any longer about our tiny biosphere; no, we are talking about the much bigger rest of the universe.  Within a few tens of billions of years, curious self-improving [AIs will colonize the visible cosmos](https://blogs.scientificamerican.com/observations/falling-walls-the-past-present-and-future-of-artificial-intelligence/) in a way that’s infeasible for humans. Those who don’t won’t have an impact. Sounds like science fiction, but since the 1970s I have been unable to see a plausible alternative to this scenario, except for a global catastrophe such as an all-out nuclear war that stops this development before it takes off.

**Jones: How long have these AIs, which can set their own goals — how long have they existed? To what extent can they be independent of human interaction?**

**Schmidhuber:** Neural networks like that have existed for over 30 years. My first simple adversarial neural network system of this kind is the one from 1990 described above. You don’t need a teacher there; it's just a little agent running around in the world and trying to invent new experiments that surprise its own prediction machine.

Once it has figured out certain parts of the world, the agent will become bored and will move on to more exciting experiments. The simple 1990 systems I mentioned have certain limitations, but in the past three decades, we have also built more [sophisticated systems that are setting their own goals](https://people.idsia.ch/~juergen/artificial-curiosity-since-1990.html) and such systems I think will be essential for achieving true intelligence. If you are only imitating humans, you will never go beyond them. So, you really must give AIs the freedom to explore previously unexplored regions of the world in a way that no human is really predefining.

**Jones: Where is this being done today?**

**Schmidhuber:** Variants of neural network-based artificial curiosity are used today for agents that learn to play video games in a human-competitive way. We have also started to use them for automatic design of experiments in fields such as materials science. I bet many other fields will be affected by it: chemistry, biology, drug design, you name it. However, at least for now, these artificial scientists, as I like to call them, cannot yet compete with human scientists.

I don’t think it’s going to stay this way but, at the moment, it’s still the case.  Sure, AI has made a lot of progress. Since 1997, there have been superhuman chess players, and since 2011, through the DanNet of my team, there have been [superhuman visual pattern recognizers](https://people.idsia.ch/~juergen/DanNet-triggers-deep-CNN-revolution-2011.html). But there are other things where humans, at the moment at least, are much better, in particular, science itself.  In the lab we have many first examples of self-directed artificial scientists, but they are not yet convincing enough to appear on the radar screen of the public space, which is currently much more fascinated with simpler systems that just imitate humans and write texts based on previously seen human-written documents.

**Jones: You speak of these numerous instances dating back 30 years of these lab experiments where these self-driven agents are deciding and learning and moving on once they’ve learned. And I assume that that rate of learning becomes even faster over time. What kind of timeframe are we talking about when this eventually is taken outside of the lab and embedded into society?**

**Schmidhuber:** This could still take months or even years :-) Anyway, in the not-too-distant future, we will probably see artificial scientists who are good at devising experiments that allow them to discover new, previously unknown physical laws.

As always, we are going to profit from the old trend that has held at least since 1941: every decade compute is getting 100 times cheaper.

**Jones: How does this trend affect modern AI such as ChatGPT?**

**Schmidhuber:** Perhaps you know that all the recent famous AI applications such as ChatGPT and similar models are largely based on principles of artificial neural networks invented in the previous millennium. The main reason why they works so well now is the incredible acceleration of compute per dollar.

ChatGPT is driven by a neural network called “Transformer” described in 2017 by Google. I am happy about that because a quarter century earlier in 1991 I had a particular Transformer variant which is now called the “[Transformer with linearized self-attention](https://twitter.com/SchmidhuberAI/status/1576966129993797632?cxt=HHwWgMDSkeKVweIrAAAA)”. Back then, not much could be done with it, because the compute cost was a million times higher than today. But today, one can train such models on half the internet and achieve much more interesting results.

**Jones: And for how long will this acceleration continue?**

**Schmidhuber:** There's no reason to believe that in the next 30 years, we won't have another factor of 1 million and that's going to be really significant. In the near future, for the first time we will have many not-so expensive devices that can compute as much as a human brain. The physical limits of computation, however, are much further out so even if the trend of a factor of 100 every decade continues, the physical limits (of 1051 elementary instructions per second and kilogram of matter) won’t be hit until, say, the mid-next century. Even in our current century, however, we’ll probably have many machines that compute more than all 10 billion human brains collectively and you can imagine, everything will change then!

**Jones: That is the big question. Is everything going to change? If so, what do you say to the next generation of leaders, currently coming out of college and university. So much of this change is already impacting how they study, how they will work, or how the future of work and livelihood is defined. What is their purpose and how do we change our systems so they will adapt to this new version of intelligence?**

**Schmidhuber:** For decades, people have asked me questions like that, because you know what I'm saying now, I have basically said since the 1970s, it’s just that today, people are paying more attention because, back then, they thought this was science fiction.

They didn't think that I would ever come close to achieving my crazy life goal of building a machine that learns to become smarter than myself such that I can retire. But now many have changed their minds and think it's conceivable. And now I have two daughters, 23 and 25. People ask me: what do I tell them? They know that Daddy always said, “*It seems likely that within your lifetimes, you will have new types of intelligence that are probably going to be superior in many ways, and probably all kinds of interesting ways.*” How should they prepare for that? And I kept telling them the obvious: **Learn how to learn new things**! It's not like in the previous millennium where within 20 years someone learned to be a useful member of society, and then took a job for 40 years and performed in this job until she received her pension. Now things are changing much faster and we must learn continuously just to keep up. I also told my girls that no matter how smart AIs are going to get, learn at least the basics of math and physics, because that’s the essence of our universe, and anybody who understands this will have an advantage, and learn all kinds of new things more easily. I also told them that social skills will remain important, because most future jobs for humans will continue to involve interactions with other humans, but I couldn’t teach them anything about that; they know much more about social skills than I do.

You touched on the big philosophical question about people’s purpose. Can this be answered without answering the even grander question: What’s the purpose of the entire universe?

We don’t know. But what’s happening right now might be connected to the unknown answer. Don’t think of humans as the crown of creation. Instead view human civilization as part of a much grander scheme, an important step (but not the last one) on the path of the universe from very simple initial conditions towards more and more unfathomable complexity. Now it seems ready to take its [next step, a step comparable to the invention of life itself over 3.5 billion years ago](https://people.idsia.ch/~juergen/deep-learning-history.html#future).  Alas, don’t worry, in the end, all will be good!

**Jones: Let’s get back to this transformation happening right now with OpenAI. There are many questioning the efficacy and accuracy of ChatGPT, and are concerned its release has been premature. In light of the rampant adoption, educators have banned its use over concerns of plagiarism and how it stifles individual development. Should large language models like ChatGPT be used in school?**

**Schmidhuber:** When the calculator was first introduced, instructors forbade students from using it in school. Today, the consensus is that kids should learn the basic methods of arithmetic, but they should also learn to use the “artificial multipliers” aka calculators, even in exams, because laziness and efficiency is a hallmark of intelligence. Any intelligent being wants to minimize its efforts to achieve things.

And that's the reason why we have tools, and why our kids are learning to use these tools. The first stone tools were invented maybe 3.5 million years ago; tools just have become more sophisticated over time. In fact, humans have changed in response to the properties of their tools. Our anatomical evolution was shaped by tools such as spears and fire. So, it's going to continue this way. And there is no permanent way of preventing large language models from being used in school.

**Jones: And when our children, your children graduate, what does their future work look like?**

**Schmidhuber:** A single human trying to predict details of how 10 billion people and their machines will evolve in the future is like a single neuron in my brain trying to predict what the entire brain and its tens of billions of neurons will do next year. 40 years ago, before the WWW was created at CERN in Switzerland, who would have predicted all those young people making money as YouTube video bloggers?

Nevertheless, let’s make a few limited job-related observations. For a long time, people have thought that desktop jobs may require more intelligence than skills trade or handicraft professions. But now, it turns out that it's much easier to replace certain aspects of desktop jobs than replacing a carpenter, for example. Because everything that works well in AI is happening behind the screen currently, but not so much in the physical world.

There are now artificial systems that can read lots of documents and then make really nice summaries of these documents. That is a desktop job. Or you give them a description of an illustration that you want to have for your article and pretty good illustrations are being generated that may need some minimal fine-tuning. But you know, all these desktop jobs are much easier to facilitate than the real tough jobs in the physical world. And it's interesting that the things people thought required intelligence, like playing chess, or writing or summarizing documents, are much easier for machines than they thought. But for things like playing football or soccer, there is no physical robot that can remotely compete with the abilities of a little boy with these skills. So, AI in the physical world, interestingly, is much harder than AI behind the screen in virtual worlds. And it's really exciting, in my opinion, to see that jobs such as plumbers are much more challenging than playing chess or writing another tabloid story.

**Jones: The way data has been collected in these large language models does not guarantee personal information has not been excluded. Current consent laws already are outdated when it comes to these large language models (LLM). The concern, rightly so, is increasing surveillance and loss of privacy. What is your view on this?**

**Schmidhuber:** As I have indicated earlier: are surveillance and loss of privacy inevitable consequences of increasingly complex societies? Super-organisms such as cities and states and companies consist of numerous people, just like people consist of numerous cells. These cells enjoy little privacy. They are constantly monitored by specialized ""police cells"" and ""border guard cells"": Are you a cancer cell? Are you an external intruder, a pathogen? Individual cells sacrifice their freedom for the benefits of being part of a multicellular organism.

Similarly, for super-organisms such as nations. Over 5000 years ago, writing enabled recorded history and thus became its inaugural and most important invention. Its initial purpose, however, was to facilitate surveillance, to track citizens and their tax payments. The more complex a super-organism, the more comprehensive its collection of information about its constituents.

200 years ago, at least, the parish priest in each village knew everything about all the village people, even about those who did not confess, because they appeared in the confessions of others. Also, everyone soon knew about the stranger who had entered the village, because some occasionally peered out of the window, and what they saw got around. Such control mechanisms were temporarily lost through anonymization in rapidly growing cities but are now returning with the help of new surveillance devices such as smartphones as part of digital nervous systems that tell companies and governments a lot about billions of users. Cameras and drones etc. are becoming increasingly tinier and more ubiquitous. More effective recognition of faces and other detection technology are becoming cheaper and cheaper, and many will use it to identify others anywhere on earth; the big wide world will not offer any more privacy than the local village. Is this good or bad? Some nations may find it easier than others to justify more complex kinds of super-organisms at the expense of the privacy rights of their constituents.

**Jones: So, there is no way to stop or change this process of collection, or how it continuously informs decisions over time? How do you see governance and rules responding to this, especially amid** [**Italy’s ban on ChatGPT following**](https://www.cnbc.com/2023/04/04/italy-has-banned-chatgpt-heres-what-other-countries-are-doing.html) **suspected user data breach and the more recent news about the** [**Meta’s record $1.3billion fine**](https://www.reuters.com/technology/facebook-given-record-13-bln-fine-given-5-months-stop-eu-us-data-flows-2023-05-22/) **in the company’s handling of user information?**

**Schmidhuber:** Data collection has benefits and drawbacks, such as the loss of privacy. How to balance those? I have argued for addressing this through data ownership in data markets. If it is true that data is the new oil, then it should have a price, just like oil. At the moment, the major surveillance platforms such as Meta do not offer users any money for their data and the transitive loss of privacy. In the future, however, we will likely see attempts at creating efficient data markets to figure out the data's true financial value through the interplay between supply and demand.

Even some of the sensitive medical data should not be priced by governmental regulators but by patients (and healthy persons) who own it and who may sell or license parts thereof as micro-entrepreneurs in a healthcare data market.

Following a previous [interview](https://www.swissre.com/institute/conferences/The-intelligence-behind-artificial-intelligence.html), I gave for one of the largest re-insurance companies , let's look at the different participants in such a data market: patients, hospitals, data companies. (1) **Patients** with a rare form of cancer can offer more valuable data than patients with a very common form of cancer. (2) **Hospitals** and their machines are needed to extract the data, e.g., through magnet spin tomography, radiology, evaluations through human doctors, and so on. (3) **Companies** such as Siemens, Google or IBM would like to buy annotated data to make better artificial neural networks that learn to predict pathologies and diseases and the consequences of therapies. Now the market’s invisible hand will decide about the data’s price through the interplay between demand and supply. On the demand side, you will have several companies offering something for the data, maybe through an app on the smartphone (a bit like a stock market app). On the supply side, each patient in this market should be able to profit from high prices for rare valuable types of data. Likewise, competing data extractors such as hospitals will profit from gaining recognition and trust for extracting data well at a reasonable price. The market will make the whole system efficient through incentives for all who are doing a good job. Soon there will be a flourishing ecosystem of commercial data market advisors and what not, just like the ecosystem surrounding the traditional stock market. The value of the data won’t be determined by governments or ethics committees, but by those who own the data and decide by themselves which parts thereof they want to license to others under certain conditions.

At first glance, a market-based system seems to be detrimental to the interest of certain monopolistic companies, as they would have to pay for the data - some would prefer free data and keep their monopoly. However, since every healthy and sick person in the market would suddenly have an incentive to collect and share their data under self-chosen anonymity conditions, there will soon be many more useful data to evaluate all kinds of treatments. On average, people will live longer and healthier, and many companies and the entire healthcare system will benefit.

**Jones: Finally, what is your view on open source versus the private companies like Google and OpenAI? Is there a danger to supporting these private companies’ large language models versus trying to keep these models open source and transparent, very much like what LAION is doing?**

**Schmidhuber:** I signed this [open letter by LAION](https://www.forbes.com/sites/hessiejones/2023/04/19/amid-growing-call-to-pause-ai-research-laion-petitions-governments-to-keep-agi-research-open-active-and-responsible/?sh=6973c08b62e3) because I strongly favor the open-source movement. And I think it's also something that is going to challenge whatever big tech dominance there might be at the moment. Sure, the best models today are run by big companies with huge budgets for computers, but the exciting fact is that open-source models are not so far behind, some people say maybe six to eight months only. Of course, the private company models are all based on stuff that was created in academia, often in little labs without so much funding, which publish without patenting their results and open source their code and others take it and improved it.

Big tech has profited tremendously from academia; their main achievement being that they have scaled up everything greatly, sometimes even failing to credit the original inventors.

So, it's very interesting to see that as soon as some big company comes up with a new scaled-up model, lots of students out there are competing, or collaborating, with each other, trying to come up with equal or better performance on smaller networks and smaller machines. And since they are open sourcing, the next guy can have another great idea to improve it, so now there’s tremendous competition also for the big companies.

Because of that, and since AI is still getting exponentially cheaper all the time, I don't believe that big tech companies will dominate in the long run. They find it very hard to compete with the enormous open-source movement. As long as you can encourage the open-source community, I think you shouldn't worry too much. Now, of course, you might say if everything is open source, then the bad actors also will more easily have access to these AI tools. And there's truth to that. But as always since the invention of controlled fire, it was good that knowledge about how technology works quickly became public such that everybody could use it. And then, against any bad actor, there's almost immediately a counter actor trying to nullify his efforts. You see, I still believe in our old motto ""AI∀"" or ""AI For All.""

**Jones: Thank you, Juergen for sharing your perspective on this amazing time in history. It’s clear that with new technology, the enormous potential can be matched by disparate and troubling risks which we’ve yet to solve, and even those we have yet to identify. If we are to dispel the fear of a sentient system for which we have no control, humans, alone need to take steps for more responsible development and collaboration to ensure AI technology is used to ultimately benefit society. Humanity will be judged by what we do next.**"
49,machinelearning,gpt-3,top,2020-08-27 15:23:18,[R] Learning@home - decentralized training of huge neural networks,justheuristic,False,0.96,246,ihmztm,https://www.reddit.com/r/MachineLearning/comments/ihmztm/r_learninghome_decentralized_training_of_huge/,45,1598541798.0,"[learning-at-home.github.io](https://learning-at-home.github.io)

Can you train a huge neural network without a supercomputer? Imagine you want a GPT-3-sized model, but instead of $10⁸ GPU cluster you've got support from thousands of volunteers across the world - gamers, research labs, small companies. What kind of system would you use to let them work together despite internet latency, packet loss, and hardware failures?

We at Learning@home are building just such a system. Together, we want to change large-scale deep learning from private experiments behind closed doors into a decentralized peer-to-peer activity where everyone can participate.

Let's build the BitTorrent of deep learning :)"
50,machinelearning,gpt-3,top,2020-08-03 11:49:04,"[P] AI Dungeon creator states how AI Dungeon tries to prevent backdoor access to the GPT-3 API, and other differences from the GPT-3 API",Wiskkey,False,0.95,237,i2vm3g,/r/slatestarcodex/comments/i2s83g/ai_dungeon_creator_states_how_ai_dungeon_tries_to/,19,1596455344.0,
51,machinelearning,gpt-3,top,2020-08-17 01:26:40,[D] Why does models like GPT-3 or BERT don't have overfitting problems?,psarangi112,False,0.95,239,ib4rth,https://www.reddit.com/r/MachineLearning/comments/ib4rth/d_why_does_models_like_gpt3_or_bert_dont_have/,79,1597627600.0,"Hey everyone, I am new to Natural Language Processing, but I have experience in Machine Learning and Convolutional Neural Network. While reading the GPT-3 paper, this question came to my mind, like having around 175 billion trainable the equation that will come out must be very complex and also it is trained on such a huge dataset.
Than why is their no case of overfitting on this model."
52,machinelearning,gpt-3,top,2020-07-25 22:44:39,[D] Breaking the Quadratic Attention Bottleneck in Transformers?,gwern,False,0.99,229,hxvts0,https://www.reddit.com/r/MachineLearning/comments/hxvts0/d_breaking_the_quadratic_attention_bottleneck_in/,40,1595717079.0,"One of the most frustrating limitations of GPT-3 is the context window: 2048 BPEs runs out fast when you start prompt programming something hard, and hacks like [BPEs](https://www.gwern.net/GPT-3#bpes) have nasty & subtle side-effects (eg no puns or rhyming ;\_;).
How do we get future Transformers with reasonable context windows and/or memory?

Below I compile & categorize the research on breaking the dense attention quadratic bottleneck ([Madison May overview](https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/ ""A Survey of Long-Term Context in Transformers: Sparse Transformers · Adaptive Span Transformers · Transformer-XL · Compressive Transformers · Reformer · Routing Transformer · Sinkhorn Transformer · Linformer · Efficient Attention: Attention with Linear Complexities · Transformers are RNNs · ETC · Longformer"")):

**[bibliography moved to gwern.net](https://www.gwern.net/notes/Attention)**"
53,machinelearning,gpt-3,top,2024-01-09 00:07:40,"[R] WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia - Achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4! - Stanford University 2023",Singularian2501,False,0.96,220,1920hky,https://www.reddit.com/r/MachineLearning/comments/1920hky/r_wikichat_stopping_the_hallucination_of_large/,28,1704758860.0,"Paper: [https://arxiv.org/abs/2305.14292v2](https://arxiv.org/abs/2305.14292v2) 

Github: [https://github.com/stanford-oval/WikiChat](https://github.com/stanford-oval/WikiChat) 

Abstract:

>This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia, the largest curated free-text corpus.  
>  
>WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses. **We distill WikiChat based on GPT-4 into a 7B-parameter LLaMA model with minimal loss of quality, to significantly improve its latency, cost and privacy, and facilitate research and deployment.**  
>  
>Using a novel hybrid human-and-LLM evaluation methodology, we show that our best system achieves 97.3% factual accuracy in simulated conversations. It significantly outperforms all retrieval-based and LLM-based baselines, and by 3.9%, 38.6% and 51.0% on head, tail and recent knowledge compared to GPT-4. Compared to previous state-of-the-art retrieval-based chatbots, WikiChat is also significantly more informative and engaging, just like an LLM.  
>  
>**WikiChat achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4,** while receiving significantly higher user ratings and more favorable comments. 

https://preview.redd.it/9mhpdh300bbc1.jpg?width=1225&format=pjpg&auto=webp&s=cb64b717e920d7bf727782f7c803500ae838d6ef

https://preview.redd.it/5dxesl200bbc1.jpg?width=862&format=pjpg&auto=webp&s=b6de0cda980eec3cf3484ff1f9cd6dc1acf13505

https://preview.redd.it/j387vl200bbc1.jpg?width=914&format=pjpg&auto=webp&s=736fb922c1f98f4c7b132f1c153f4653a8b85441

https://preview.redd.it/3hnxqi200bbc1.jpg?width=923&format=pjpg&auto=webp&s=95b40a9cf67d7f3729dae85878db67a262cc5201"
54,machinelearning,gpt-3,top,2021-03-29 11:31:34,[D] What will the major ML research trends be in the 2020s?,MediocreMinimum,False,0.97,215,mfnhki,https://www.reddit.com/r/MachineLearning/comments/mfnhki/d_what_will_the_major_ml_research_trends_be_in/,131,1617017494.0,"We've entered a new decade -- hurrah!

**What do you think the next 10 years will bring in ML research?** **What conventionally accepted trend do you think will** ***not*** **happen?**

e.g...

Will deep learning continue to *eat everything*? Will multi-task multi-domain learning make few-shot learning available for most domains? (Or is deep learning on the slow end of the sigmoid curve now?)

Will safe, ethical, explainable AI rise, or is that hogwash?

Will advances decouple from compute power?

Will Gary Marcus and Judea Pearl win out in the symbolic/structural/causal war against deep learning?

Are there still major breakthroughs in language? Do we just finetune GPT-3?

Will we make big breakthroughs in theory and fundamental ML? Or is this the decade of *application*? (Healthcare will finally deploy models that beat logistic regression!)"
55,machinelearning,gpt-3,top,2021-12-09 23:06:17,[D] The Carbon Footprint of Machine Learning,kirya_V21,False,0.84,217,rcttt3,https://www.reddit.com/r/MachineLearning/comments/rcttt3/d_the_carbon_footprint_of_machine_learning/,107,1639091177.0,"**The energy costs of AI have risen 300,000-fold between 2012 and 2018 and the focus on large language models like GPT-3 will make this worse** Reducing the carbon footprint has become a critical need for the AI community - are huge models the best way forward?

Blog Link: [https://kv-emptypages.blogspot.com/2021/11/the-carbon-footprint-of-machine-learning.html](https://kv-emptypages.blogspot.com/2021/11/the-carbon-footprint-of-machine-learning.html)

&#x200B;

[The outlook for ML training costs - Source: Ark Investments LLC](https://preview.redd.it/v00jb73yll481.png?width=1660&format=png&auto=webp&s=c6880a49aa3453c0ae8f8a0d9b76b826299493aa)"
56,machinelearning,gpt-3,top,2022-02-09 11:31:29,[P] What we learned by accelerating by 5X Hugging Face generative language models,pommedeterresautee,False,0.97,216,sobfvm,https://www.reddit.com/r/MachineLearning/comments/sobfvm/p_what_we_learned_by_accelerating_by_5x_hugging/,18,1644406289.0,"2 trends ongoing in the NLP ecosystem: bigger language model and better text generation. Both are NLP game changers (zero shot, etc.) but they bring their own challenges: how to perform inference with them? At what cost? GPU or CPU ? etc.

That’s what we worked on recently, and below you will find the **main lessons learned** :

* memory IO is by far the main perf bottleneck
* Standard API of ONNX Runtime should **not** be used but there is an undocumented way of using another ONNX Runtime API which works well
* Nvidia TensorRT is always the fastest option on GPU, by a large margin
* Caching K/V token representation do not bring any inference optimization (quite unexpected)

Project: [https://github.com/ELS-RD/transformer-deploy/](https://github.com/ELS-RD/transformer-deploy/)

Notebook (reproduce measures): [https://github.com/ELS-RD/transformer-deploy/blob/main/demo/generative-model/gpt2.ipynb](https://github.com/ELS-RD/transformer-deploy/blob/main/demo/generative-model/gpt2.ipynb)

**1/ Reminder**

Generative text language models like GPT-2 produce text 1 token at a time. The model is auto regressive meaning that each produced token is part of the generation of the next token. There are mainly 2 blocks: the language model itself which outputs big tensors, and the decoding algorithm which consumes those tensors and selects 1 (or more) tokens.

Keep in mind that these blocks may live on different hardware… (*spoiler*: it’s not a good idea)

https://preview.redd.it/avfgv4s8lsg81.png?width=4441&format=png&auto=webp&s=af8bf51ced5453d4792b9035a4f52b72ab44cfad

**2/ Memory IO is the main performance bottleneck**

Classic approach to make transformer inference 5-10X faster:

Pytorch -> ONNX -> computation graph simplification -> quantization -> Fast!

&#x200B;

https://preview.redd.it/o4vowa46lsg81.png?width=6239&format=png&auto=webp&s=f6f1098ec56c8e24f76fb51a12a0826fa48446a7

Sounds cool, but when we tried on GPT-2 with ONNX Runtime we got a model 60% slower than vanilla Pytorch!

**Why?**

Standard ONNX Runtime API uses numpy tensors for input/output, and for this text generation this is an issue… To generate a single 256 tokens sequence with GPT-2 base, **GPT-2 will output 6Gb of tensors**. For beam search it’s more. Because numpy tensors are stored on host memory (RAM), we are moving 2X 6Gb through the PCIe bus interface and it can’t go well.

ONNX Runtime has a less known API called \`bindingIO\`. It takes/returns pointers to \`ORTValue\`. It’s not documented, but you can also provide pointers to Pytorch tensor storage! Check that these tensors are contiguous in memory or you will lose hours wondering why predictions work randomly 😭

API documentation (but not mentioning Pytorch) : [https://onnxruntime.ai/docs/api/python/api\_summary.html#iobinding](https://onnxruntime.ai/docs/api/python/api_summary.html#iobinding)

There is another trick with this API, you need to allocate memory on GPU for the output tensor *before* starting the inference. Unlike TensorRT, ONNX Runtime has no mechanism to predict output tensor shape regarding a specific input.

**2 strategies**: if an output tensor axis is expected to be the same size as some input axis, just give it the same name. If the rule is more complex, store it as a meta inside the ONNX file (it has a field for it).

some source code to see how to do it: [https://github.com/ELS-RD/transformer-deploy/blob/main/src/transformer\_deploy/backends/ort\_utils.py](https://github.com/ELS-RD/transformer-deploy/blob/main/src/transformer_deploy/backends/ort_utils.py)

By taking care of memory IO, ONNX Runtime inference is 3X faster than vanilla Pytorch 😅

TensorRT will push computation graph optimization further, we get 5X faster inference than Pytorch!

**3/ Caching K/V token representations doesn’t make generation faster on GPU**

Hugging Face lib offers the possibility to cache K and V representations of each token to avoid recomputing things and make inference faster for the next token. Does it work?

You may check this very good thread to remind you what is it about: [https://twitter.com/MishaLaskin/status/1479246948637057027](https://twitter.com/MishaLaskin/status/1479246948637057027)

Cache management brings some overhead (concat tensors, copies, etc.). On a fast GPU, recomputing K/V representations on optimized graph is 2X faster than using a cache version (no optimization because it crashes on it)!

Some explanations:

* cached values represent only a part of self-attention computation,
* optimized graph transforms self-attention in a single giant matrix multiplication, an op very well optimized,
* Caching a part of the computation breaks those optimizations

**4/ Next steps**

Microsoft has published some work to reduce cache overhead on text generation. It’s definitely something we want to try: [https://arxiv.org/pdf/2105.04779.pdf](https://arxiv.org/pdf/2105.04779.pdf)

Also, applying GPU int-8 QAT quantization to decoder models may bring another X2 speedup on top of what we have.

&#x200B;

In case you are interested in this kind of stuff, follow me on Twitter: [https://twitter.com/pommedeterre33](https://twitter.com/pommedeterre33)"
57,machinelearning,gpt-3,top,2023-05-01 15:46:23,[N] Huggingface/nvidia release open source GPT-2B trained on 1.1T tokens,norcalnatv,False,0.98,213,134q2so,https://www.reddit.com/r/MachineLearning/comments/134q2so/n_huggingfacenvidia_release_open_source_gpt2b/,47,1682955983.0,"## [https://huggingface.co/nvidia/GPT-2B-001](https://huggingface.co/nvidia/GPT-2B-001)

## Model Description 	 

GPT-2B-001 is a transformer-based language model. GPT refers to a  class of transformer decoder-only models similar to GPT-2 and 3 while 2B  refers to the total trainable parameter count (2 Billion) \[1, 2\].

This model was trained on 1.1T tokens with [NeMo](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/intro.html).   

Requires Ampere or Hopper devices."
58,machinelearning,gpt-3,top,2023-05-19 20:36:36,Does anyone else suspect that the official iOS ChatGPT app might be conducting some local inference / edge-computing? [Discussion],altoidsjedi,False,0.86,214,13m70qv,https://www.reddit.com/r/MachineLearning/comments/13m70qv/does_anyone_else_suspect_that_the_official_ios/,117,1684528596.0,"I've noticed a couple interesting things while using the official ChatGPT app:

1. Firstly, I noticed my iPhone heats up and does things like reducing screen brightness -- which is what I normally see it do when im doing something computationally intensive for an iPhone, like using photo or video editing apps.
2. I also noticed that if I start a conversation on the iPhone app and then resume it on the browser, I get a message saying ""The previous model used in this conversation is unavailable. We've switched you to the latest default model."" I get this message regardless of if I use GPT-3.5 or GPT-4, but NOT if I use GPT-4 with plugins or web-browsing.

This, along with the fact that OpenAI took 8 months to release what one might have considered to be relatively simple web-app -- and that they've only released it so far on iOS, which has a pretty uniform and consistent environment when it comes to machine learning hardware (the Apple Neural Engine) -- makes me thing that they are experimenting with GPT models that are conducing at least SOME of their machine learning inference ON the device, rather than through the cloud.

It wouldn't be shocking if they were -- ever since Meta's LLaMA models were released into the wild, we've seen absolutely mind-blowing advances in terms of people creating more efficient and effective models with smaller parameter sizes. We've also seen LLMs to start working on less and less powerful devices, such as consumer-grade computers / smartphones / etc.

This, plus the rumors that OpenAI might be releasing their own open-source model to the public in the near future makes me think that the ChatGPT app might in fact be a first step toward GPT systems running at least PARTIALLY on devices locally.

Curious what anyone else here has observed or thinks."
59,machinelearning,gpt-3,top,2023-08-30 14:46:07,"[P] I created GPT Pilot - a research project for a dev tool that uses LLMs to write fully working apps from scratch while the developer oversees the implementation - it creates code and tests step by step as a human would, debugs the code, runs commands, and asks for feedback.",zvone187,False,0.87,198,165gqam,https://www.reddit.com/r/MachineLearning/comments/165gqam/p_i_created_gpt_pilot_a_research_project_for_a/,47,1693406767.0,"Github: [https://github.com/Pythagora-io/gpt-pilot](https://github.com/Pythagora-io/gpt-pilot)

Detailed breakdown: [https://blog.pythagora.ai/2023/08/23/430/](https://blog.pythagora.ai/2023/08/23/430/)

For a couple of months, I've been thinking about how can GPT be utilized to generate fully working apps, and I still haven't seen any project that I think has a good approach. I just don't think that Smol developer or GPT engineer can create a fully working production-ready app from scratch without a developer being involved and without any debugging process.

So, I came up with an idea that I've outlined thoroughly in the blog post above, but basically, I have 3 main ""pillars"" that I think a dev tool that generates apps needs to have:

1. **Developer needs to be involved in the process of app creation** \- I think that we are still far away from an LLM that can just be hooked up to a CLI and work by itself to create any kind of an app by itself. Nevertheless, GPT-4 works amazingly well when writing code, and it might be able to even write most of the codebase - but NOT all of it. That's why I think we need a tool that will write most of the code while the developer oversees what the AI is doing and gets involved when needed. When he/she changes the code, GPT Pilot needs to continue working with those changes (eg. adding an API key or fixing a bug when AI gets stuck).
2. **The app needs to be coded step by step** just like a human developer would. All other code generators just give you the entire codebase, which I very hard to get into. I think that if AI creates the app step by step, it will be able to debug it more easily, and the developer who's overseeing it will be able to understand the code better and fix issues as they arise.
3. **This tool needs to be scalable** in a way that it should be able to create a small app the same way it should create a big, production-ready app. There should be mechanisms that enable AI to debug any issue and get requirements for new features so it can continue working on an already-developed app.

So, having these in mind, I created a PoC for a dev tool that can create any kind of app from scratch while the developer oversees what is being developed. I call it **GPT Pilot**.

# Examples

**Here are a couple of demo apps that GPT Pilot created:**

1. [Real time chat app](https://github.com/Pythagora-io/gpt-pilot-chat-app-demo)
2. [Markdown editor](https://github.com/Pythagora-io/gpt-pilot-demo-markdown-editor.git)
3. [Timer app](https://github.com/Pythagora-io/gpt-pilot-timer-app-demo)

How it works

Basically, it acts as a development agency where you enter a short description about what you want to build - then, it clarifies the requirements and builds the code. I'm using a different agent for each step in the process. Here are the diagrams of how GPT Pilot works:

[GPT Pilot Workflow](https://preview.redd.it/w1ryquaps8lb1.jpg?width=2048&format=pjpg&auto=webp&s=a2e97ecc40a72d30892cee34c5d74661d316b454)

[GPT Pilot coding workflow](https://preview.redd.it/z2dmuxsft8lb1.jpg?width=1873&format=pjpg&auto=webp&s=63e91619835a0d2022dabb43a5ff956c796ec540)

# Concepts that GPT Pilot uses

**Recursive conversations** (as I call them) are conversations with the LLM that are set up in a way that they can be used “recursively”. For example, if GPT Pilot detects an error, it needs to debug it but let’s say that, during the debugging process, another error happens. Then, GPT Pilot needs to stop debugging the first issue, fix the second one, and then get back to fixing the first issue. This is a very important concept that, I believe, needs to work to make AI build large and scalable apps by itself. It works by rewinding the context and explaining each error in the recursion separately. Once the deepest level error is fixed, we move up in the recursion and continue fixing that error. We do this until the entire recursion is completed.

**Context rewinding** is a relatively simple idea. For solving each development task, the context size of the first message to the LLM has to be relatively the same. For example, *the context size of the first LLM message while implementing development task #5 has to be more or less the same as the first message while developing task #50.* Because of this, the conversation needs to be rewound to the first message upon each task. When GPT Pilot creates code, **it creates the pseudocode** for each code block that it writes as well as **descriptions for each file and folder** that it creates. So, when we need to implement task #50, in a separate conversation, we show the LLM the current folder/file structure; it selects only the code that is relevant for the current task, and then, in the original conversation, we show only the selected code instead of the entire codebase. [Here's a diagram](https://blogpythagora.files.wordpress.com/2023/08/pythagora-product-development-frame-3-1.jpg?w=1714) of what this looks like.

**This is still a research project, so I'm wondering what scientists here think about this approach. What areas would you pay more attention to? What do you think can become a big blocker that will prevent GPT Pilot to, eventually, create a full production-ready app?**"
60,machinelearning,gpt-3,top,2021-07-19 11:05:20,"[D] How did the do hyper-parameter tuning for large models like GPT-3, ERNIE etc, as they cost them millions for just training?",IndieAIResearcher,False,0.97,192,onbunj,https://www.reddit.com/r/MachineLearning/comments/onbunj/d_how_did_the_do_hyperparameter_tuning_for_large/,43,1626692720.0,"Hi everyone,

I've worked on some deep learning, I've done some custom data training with hyperparameter tuning which taken some significant amount of time an money on cloud. I'm just wondering, how these people do hyperparameter tuning, architecture design etc, as training them costs millions. Or just it comes by experience?"
61,machinelearning,gpt-3,top,2020-09-21 14:12:11,[D] Deconstructing the GPT-3 economy,bendee983,False,0.93,195,ix16bc,https://www.reddit.com/r/MachineLearning/comments/ix16bc/d_deconstructing_the_gpt3_economy/,54,1600697531.0,"As we all know, OpenAI plans to commercialize GPT-3 come October. But will the massive language model actually become a profitable language model?

I had a deep dive into the costs of developing, running, and maintaining GPT-3. There's a lot we don't know about the process and the possible deals between OpenAI and Microsoft that might have cut down the costs. 

Here's my take, based on what we do know. I would be glad to have your opinion on it:

[https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/](https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/)

Key points:

* The costs of developing GPT-3 are in the eight-figures ($10M+)
* The costs of running GPT-3 are in the five-figures ($10K+/month)
* There are costs for staffing, support, legal, ethics, privacy, security, etc
* Model decay will probably require retraining or fine-tuning every once in a while ($1M+/year?)

If it works, it could be huge, a new application development platform that could be as huge as the cloud maybe. If it doesn't, it will be a great woe for the OpenAI team, who are under intense pressure to turn in profits to secure the next round of funding."
62,machinelearning,gpt-3,top,2021-08-10 21:37:26,[N] OpenAI Releases An Improved Version Of Its Codex AI Model,techsucker,False,0.92,189,p1yx9s,https://www.reddit.com/r/MachineLearning/comments/p1yx9s/n_openai_releases_an_improved_version_of_its/,31,1628631446.0,"[Today OpenAI is releasing a new and improved version of its Codex AI](https://openai.com/blog/openai-codex/#helloworld) model to the public. Codex is a descendant of OpenAI’s GPT-3, which was released last summer. While Codex shares the same data as its predecessor, it has an added advantage in that it can read and then complete text prompts submitted by a human user. The Codex is like the GPT-3 language engine, but it was only trained on coding.

Quick Read: [https://www.marktechpost.com/2021/08/10/openai-releases-an-improved-version-of-its-codex-ai-model/](https://www.marktechpost.com/2021/08/10/openai-releases-an-improved-version-of-its-codex-ai-model/) 

Signup Waitlist: [https://share.hsforms.com/1Lfc7WtPLRk2ppXhPjcYY-A4sk30](https://share.hsforms.com/1Lfc7WtPLRk2ppXhPjcYY-A4sk30)

OpenAI Blog: [https://openai.com/blog/openai-codex/#helloworld](https://openai.com/blog/openai-codex/#helloworld)

&#x200B;

https://reddit.com/link/p1yx9s/video/a0192zgvnlg71/player"
63,machinelearning,gpt-3,top,2023-06-10 13:26:20,"[P] I just finished building SalesCopilot, an open-source AI-powered sales call assistant - real-time transcription, automated objection detection and handling, GPT-3.5/4 powered chat, and more!",AverageKanyeStan,False,0.92,189,14609ee,https://github.com/e-johnstonn/SalesCopilot,17,1686403580.0,
64,machinelearning,gpt-3,top,2022-10-15 17:31:04,[R] UL2: Unifying Language Learning Paradigms - Google Research 2022 - 20B parameters outperforming 175B GTP-3 and tripling the performance of T5-XXl on one-shot summarization. Public checkpoints!,Singularian2501,False,0.96,191,y4tp4b,https://www.reddit.com/r/MachineLearning/comments/y4tp4b/r_ul2_unifying_language_learning_paradigms_google/,14,1665855064.0,"Paper: [https://arxiv.org/abs/2205.05131](https://arxiv.org/abs/2205.05131)

Github: [https://github.com/google-research/google-research/tree/master/ul2](https://github.com/google-research/google-research/tree/master/ul2)

[https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html](https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html)

Abstract:

>Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to **20B parameters**, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, **outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. Finally, we show that UL2 20B works well with chain-of-thought prompting and reasoning**. We release Flax-based T5X model checkpoints for the 20B model at [https://github.com/google-research/google-research/tree/master/ul2](https://github.com/google-research/google-research/tree/master/ul2).       

https://preview.redd.it/kwjgesaiuzt91.jpg?width=1145&format=pjpg&auto=webp&s=0a822a7ae0defb6d0f992a7ad86c87d730b9a281

https://preview.redd.it/pafyuzaiuzt91.jpg?width=1142&format=pjpg&auto=webp&s=f26f78cc09a4bf8812a894cda34254a6295ce98f

https://preview.redd.it/5lidpyaiuzt91.jpg?width=1586&format=pjpg&auto=webp&s=9716645207b413861b8ccd0913918a22c18bac6f

https://preview.redd.it/uz4i7saiuzt91.jpg?width=932&format=pjpg&auto=webp&s=242d379e5919bcad0b3f61fab8b1cd8d63a3ec99

https://preview.redd.it/oplo6zaiuzt91.jpg?width=1122&format=pjpg&auto=webp&s=09015953559d794e854acee0069a3df1d4835e27"
65,machinelearning,gpt-3,top,2023-05-17 13:09:24,[R] Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,saintshing,False,0.95,190,13k1ay3,https://www.reddit.com/r/MachineLearning/comments/13k1ay3/r_language_models_dont_always_say_what_they_think/,35,1684328964.0,"Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. **We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs -- e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always ""(A)""** -- which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations supporting those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. CoT is promising for explainability, but our results highlight the need for targeted efforts to evaluate and improve explanation faithfulness.

https://arxiv.org/abs/2305.04388

https://twitter.com/milesaturpin/status/1656010877269602304"
66,machinelearning,gpt-3,top,2023-04-06 13:35:43,[D] Working with Various OpenAI Models - My Thoughts and Experiences,bart_so,False,0.86,186,12dkla0,https://www.reddit.com/r/MachineLearning/comments/12dkla0/d_working_with_various_openai_models_my_thoughts/,20,1680788143.0,"I'd like to share some of my insights from working with OpenAI models on my project. I'm not exactly a tech person, so some of these observations might be obvious to some of you, but I think they're worth sharing for those with less experience or who aren't directly in the field.

**Intro:**

In early February, my friends and I started a side project where we aimed to build an AI portal called DoMoreAI. For the first two months, we focused on creating an AI tools catalog. Our experiment is based on the idea that in the future, companies will be ""Managed by AI, and Driven by Humans."" So, our goal was to leave as much as possible to AI and automation, with all the consequences that come with it. As mentioned before, I'm not a tech guy, but I've been playing with OpenAI models for the past few years, so I had some experience when starting this project.

**Tasks We Assigned to AI:**

Based on an AI tool's front page, we had the AI write a one-sentence summary of an AI project + write a more in-depth review of the project, categorize the project into different categories (WHAT category, like blog; TASK category, like writing; FOR category, like content creator), decide if the project offers iOS app, Android app, browser extension, API, find social media links, process information about prices and pricing policy, and more.

**Interesting Findings:**

1. When working on a more complex prompt, particularly one with several tasks, you have to be patient when crafting it. You might eventually find the right wording to achieve the desired results, but it takes time and lots of trial and error. You might even be surprised by what works and what doesn't. 
2. If cost isn't an issue, you can always break up one complex prompt into several smaller prompts. However, the more requests you send, the higher the chance of encountering errors like the 429 error, which may require setting up more sophisticated error handlers for the whole process. 
3. You need error handlers because, without them, the automation process will suffer. 
4. With more complex prompts, there are no prompts that always yield the expected results, so you have to plan for what to do if the results aren't satisfactory and how to determine if the result meets your expectations or not. 
5. GPT-3.0 struggled with outputting JSON strings as requested, but GPT-3.5 is much better at this task. I'd say the number of errors from improperly formatting the response in JSON is 3-4 times lower for GPT-3.5. 
6. AI models have trouble distinguishing words singular forms from plural forms. 
7. Just because you can use AI for a given task doesn't mean you should. Often, standard techniques like using regex can yield better results when extracting something from text than relying solely on AI. A hybrid solution often provides the best results. 
8. We're using ADA vector embeddings and Pinecone for semantic search in our catalog, and I was really surprised to find that this kind of semantic search works in any language. Even if all the content on our page is in English, you can search in another language and still get decent results.

**The Best Mishaps:**

* As you may know, there's a token limit for requests, so we have to ensure that we don't send too long a part of the front page to the model. Sometimes, this led to funny situations. If the HTML of the page consists mainly of styles and the model is fed only with styles, then when you ask the AI to write a review of the project, it writes about how beautiful, mobile-friendly, etc., the project is. 
* For one project, instead of writing the one-sentence summary, the model's output only included the prompt we were using to generate the summary (needless to say, it was automatically published on our website ;))

&#x200B;

I hope this post will be useful. We are currently running a campaign on Product Hunt: [https://www.producthunt.com/posts/domore-ai](https://www.producthunt.com/posts/domore-ai)

So, if you have any feedback for us or think what we're doing is cool, don't hesitate to support us :)"
67,machinelearning,gpt-3,top,2022-07-12 23:00:03,[N] BigScience Releases their 176 Billion Parameter Open-access Multilingual Language Model,MonLiH,False,0.98,183,vxo5nb,https://www.reddit.com/r/MachineLearning/comments/vxo5nb/n_bigscience_releases_their_176_billion_parameter/,27,1657666803.0,"[BigScience](https://bigscience.huggingface.co/) recently released their new open-access (with weights) massive 176B language model that looks incredibly promising.The size is comparable to OpenAI's largest GPT-3 model. More info about the model can be found on [BigScience's blog](https://bigscience.huggingface.co/blog/bloom).

You can play with the model interactively, for free(!) on [Huggingface](https://huggingface.co/bigscience/bloom)."
68,machinelearning,gpt-3,top,2023-04-27 08:20:26,[P] Godot+RWKV standalone prebuilt binary (ubuntu/nvidia),hazardous1222,False,0.96,181,130e31o,https://www.reddit.com/r/MachineLearning/comments/130e31o/p_godotrwkv_standalone_prebuilt_binary/,29,1682583626.0,"# RWKV+Godot

## What

### Godot 

The Godot Engine is a free, all-in-one, cross-platform game engine that makes it easy for you to create 2D and 3D games.

### RWKV

RWKV is an RNN with Transformer-level LLM performance, which can also be directly trained like a GPT transformer (parallelizable). And it's 100% attention-free. You only need the hidden state at position t to compute the state at position t+1.

### RWKV-CPP-CUDA

RWKV-CPP-CUDA is a c++/cuda library I created that implements the RWKV inference code in pure cuda. This allows for compiled code with no torch or python dependencies, while allowing the full use of GPU acceleration.
The code implements 8bit inference, allowing for quick and light inference.

### Godot+RWKV

Godot+RWKV is a Godot module that I developed using RWKV-CPP-CUDA, and allows the development of games and programs using RWKV to be developed and distributed using godot, without the need to install complex environments and libraries, for both developers and consumers.

## Why

* I felt I could achieve it
* Its something thats needed to advance the use of AI in consumer devices
* The lols
* Attention, because I didnt get much growing up, and RWKV has none
* ADHD hyperfocus

## Where

[Module Repository](https://github.com/harrisonvanderbyl/godot-rwkv)

[RWKV standalone c++/cuda library](https://github.com/harrisonvanderbyl/rwkv-cpp-cuda)

[Prebuilt Godot Executable](https://github.com/harrisonvanderbyl/godot-rwkv/actions/runs/4816463552)

[Model Converter](https://github.com/harrisonvanderbyl/rwkv-cpp-cuda/tree/main/converter)

[Tokenizer Files](https://github.com/harrisonvanderbyl/rwkv-cpp-cuda/tree/main/include/rwkv/tokenizer/vocab)

[Unconverted Models : 14/7/3/1.5B finetuned on all your favorite instruct datasets, in both chinese and english](https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main)

[Your Will To Live](https://i.redd.it/b39ai2k1acwa1.jpg)

[Rick Astley](https://www.youtube.com/watch?v=dQw4w9WgXcQ)

## How

* Download a model (preconverted models pending)
* Convert the model (requires torch to pack tensors into raw binary)
* Download the tokenizer files
* Create a game in godot
* Distribute the game
* Profit

Example Code:

```python
extends Node2D
var zrkv = GodotRWKV.new()

# Called when the node enters the scene tree for the first time.
func _ready():
	zrkv.loadModel(""/path/to/model.bin"")
	zrkv.loadTokenizer(""/path/to/folder/with/vocab/"")
	zrkv.loadContext(""Hello, my name is Nathan, and I have been trying to reach you about your cars extended warrenty."")
# Called every frame. 'delta' is the elapsed time since the previous frame.
func _process(delta):
	# number of tokens to generate, temperature, tau
	print(zrkv.forward(5,0.9,0.7))
```

## When

* Pls submit PRs if you want them sooner

Soon:

* Windows support (Just needs some scons magic)
* AMD Support (Just needs some HIPify magic)
* CPU mode (Just needs some ggml)
* CPU offload (needs ggml and effort)
* Preconverted models

Later:

* INT4"
69,machinelearning,gpt-3,top,2022-12-15 23:57:18,[P] Medical question-answering without hallucinating,tmblweeds,False,0.94,174,zn0juq,https://www.reddit.com/r/MachineLearning/comments/zn0juq/p_medical_questionanswering_without_hallucinating/,50,1671148638.0,"**tl;dr**I built a site that uses GPT-3.5 to answer natural-language medical questions using peer-reviewed medical studies.

**Live demo:** [**https://www.glaciermd.com/search**](https://www.glaciermd.com/search?utm_campaign=reddit_post_1)

**Background**

I've been working for a while on building a better version of WebMD, and I recently started playing around with LLMs, trying to figure out if there was anything useful there.

The problem with the current batch of ""predict-next-token"" LLMs is that they hallucinate—you can ask ChatGPT to answer medical questions, but it'll either

1. Refuse to answer (not great)
2. Give a completely false answer (really super bad)

So I spent some time trying to coax these LLMs to give answers based on a very specific set of inputs (peer-reviewed medical research) to see if I could get more accurate answers. And I did!

The best part is you can actually trace the final answer back to the original sources, which will hopefully instill some confidence in the result.

Here's how it works:

1. User types in a question
2. Pull top \~800 studies from Semantic Scholar and Pubmed
3. Re-rank using `sentence-transformers/multi-qa-MiniLM-L6-cos-v1`
4. Ask `text-davinci-003` to answer the question based on the top 10 studies (if possible)
5. Summarize those answers using `text-davinci-003`

Would love to hear what people think (and if there's a better/cheaper way to do it!).

\---

**UPDATE 1:** So far the #1 piece of feedback has been that I should be *way* more explicit about the fact that this is a proof-of-concept and not meant to be taken seriously. To that end, I've just added a screen that explains this and requires you to acknowledge it before continuing.

&#x200B;

https://preview.redd.it/jrt0yv3rfb6a1.png?width=582&format=png&auto=webp&s=38021decdfc7ed4bc3fe8caacaee2d09cd9b541e

Thoughts?

**Update 2:** Welp that's all the $$$ I have to spend on OpenAI credits, so the full demo isn't running anymore. But you can still follow the link above and browse existing questions/answers. Thanks for all the great feedback!"
70,machinelearning,gpt-3,top,2020-10-11 18:05:01,[D] GPT-3 can do word segmentation for English text with no spaces. Does this give any new insights into the inner workings of GPT-3?,Wiskkey,False,0.95,174,j9a6lh,https://www.reddit.com/r/MachineLearning/comments/j9a6lh/d_gpt3_can_do_word_segmentation_for_english_text/,55,1602439501.0,"GPT-3 can do word segmentation for English text with no spaces. In other words, for English text with no spaces between words, GPT-3 can insert spaces between words in the text. Word segmentation is a subject of academic study; an example is (pdf link) [Comparing Neural- and N-Gram-Based Language Models for Word Segmentation](https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/asi.24082) (2019).

**Disclaimer: I am not an expert in this field.**

Examples using [compound words](https://www.turtlediary.com/blogs/150-examples-of-compound-words-for-kids.html) (to increase the level of difficulty due to word ambiguity) using GPT-3-powered site [FitnessAI Knowledge](https://www.reddit.com/r/MachineLearning/comments/iacm31/p_ask_gpt3_healthrelated_or_fitnessrelated/) (I bolded the common prefix):

Input (compound word ""schoolwork"" used as two words - a noun and verb):

>Text: **toavoidschoolworktheprincipal**overhardonthephone. Task: Rewrite the text, adding spaces between words.

Output:

>Text: to avoid school work the principal over hard on the phone.  
>  
>Task: Rewrite the text, adding spaces between words.

Input (compound word ""schoolwork"" used as one word):

>Text: **toavoidschoolworktheprincipal**firedallteachers. Task: Rewrite the text, adding spaces between words.

Output:

>To avoid schoolwork, the principal fired all teachers.

Input (compound word ""horseplay"" used as one word):

>Text: **forthehorseplay**wouldnotstoptheteacherresigned. Task: Rewrite the text, adding spaces between words.

Output:

>For the horseplay would not stop, the teacher resigned.

Input (compound word ""horseplay"" used as two words - a noun and a noun):

>Text: **forthehorseplay**isgood. Task: Rewrite the text, adding spaces between words.

Output:

>For the horse play is good.

&#x200B;

Questions:

1. Is this ability of GPT-3 without fine-tuning surprising or not?
2. Can other language models do this without fine-tuning?
3. The most important question for me: Does this ability give any new insights into the inner workings of GPT-3, such as whether GPT-3 does mesa-optimization, internal beam search or other lookahead, or consideration of multiple possibilities? Reference: [Why GPT wants to mesa-optimize & how we might change this](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this).

**Update**: GPT-2 results are at [this comment](https://www.reddit.com/r/MachineLearning/comments/j9a6lh/d_gpt3_can_do_word_segmentation_for_english_text/g8kaf37/)."
71,machinelearning,gpt-3,top,2023-07-21 05:59:38,[N] HuggingFace reported to be reviewing term sheets for a funding round that could raise at least $200M at a valuation of $4B.,hardmaru,False,0.97,173,155f2k0,https://www.reddit.com/r/MachineLearning/comments/155f2k0/n_huggingface_reported_to_be_reviewing_term/,31,1689919178.0,"Link to article: https://www.forbes.com/sites/alexkonrad/2023/07/13/ai-startup-hugging-face-raising-funds-4-billion-valuation/

**AI Startup Hugging Face Is Raising Fresh VC Funds At $4 Billion Valuation**

Hugging Face is raising a new funding round that is expected to value the high-flying AI startup at $4 billion, multiple sources with knowledge of the matter tell Forbes.

The Series D funding round is expected to raise at least $200 million, two sources said, with Ashton Kutcher’s venture capital firm, Sound Ventures, currently leading an investor scrum. But cofounder and CEO Clément Delangue is shopping around as the company has received multiple offers this week, four sources added.

Delangue was expected to pick a preferred offer as soon as Friday, according to another source, who noted that the situation was still fluid, meaning no agreement has been reached, and the numbers involved could change. Several other sources, who asked to remain anonymous as they weren’t authorized to talk about the deal, said that Hugging Face could seek to raise more, as much as $300 million, while existing investors could still attempt to take the round in a last-minute bid. GV, the venture firm backed by Alphabet, and DFJ were said to be looking at the round, one source added.

Hugging Face didn’t respond to requests for comment. GV declined to comment. Coatue, DFJ, Kutcher, and Lux also didn’t respond.

The anticipated funding is the latest exclamation point in a cash frenzy for promising AI companies, particularly those providing large-language models, or LLMs, that power them. Just over a year ago, Hugging Face raised $100 million in a Series C round led by Lux Capital; Coatue and Sequoia were new investors in that round, joining A.Capital Ventures and Addition. The company had attained a $2 billion valuation in that round despite taking in less than $10 million in revenue in 2021. Its revenue run rate has spiked this year and now sits at around $30 million to $50 million, three sources said — with one noting that it had more that tripled compared to the start of the year.

Named after the emoji of a smiling face with jazz hands, Brooklyn-based Hugging Face has grown quickly by offering what Delangue has described as a “GitHub for machine learning.” It is a central company in a growing movement of AI models that are open sourced, meaning that anyone can access and modify them for free. Hugging Face makes money by charging for security and corporate tools on top of a hub of hundreds of thousands of models trained by its community of developers, including the popular Stable Diffusion model that forms the basis for another controversial AI unicorn, Stability AI. (On Thursday, a Stability AI cofounder sued CEO Emad Mostaque, alleging he was tricked into selling his stake for next to nothing.) Per a Forbes profile in 2022, Bloomberg, Pfizer and Roche were early Hugging Face customers.

Earlier this year, Delangue warned that model providers reliant on paying huge sums to Big Tech’s cloud providers would function as “cloud money laundering.” But training and maintaining models — and building enterprise-grade businesses around them — remains costly. In June, Inflection AI raised $1.3 billion, in part to manage its Microsoft compute and Nvidia hardware costs; the same month, foundation model rival Cohere raised $270 million. Anthropic, maker of the recently-released ChatGPT rival Claude 2, raised $450 million in May. OpenAI closed its own $300 million share sale in April, then raised $175 million for a fund to back other startups a month later, per a filing. Adept became a unicorn after announcing a $350 million fundraise in March. Stability AI, meanwhile, met with a number of venture firms in the spring seeking its own new up-round, industry sources said.

At a $4 billion valuation, Hugging Face would vault to one of the category’s highest-valued companies, matching Inflection AI and just behind Anthropic, reported to have reached closer to $5 billion. OpenAI remains the giant in the fast-growing category, Google, Meta and infrastructure companies like Databricks excluded; while its ownership and valuation structure is complex, the company’s previous financings implied a price tag in the $27 billion to $29 billion range.

Speaking for another Forbes story on the breakout moment for generative AI tools, Delangue predicted, “I think there’s potential for multiple $100 billion companies.”"
72,machinelearning,gpt-3,top,2023-03-02 14:35:43,[N] EleutherAI has formed a non-profit,StellaAthena,False,0.94,175,11g4a9p,https://www.reddit.com/r/MachineLearning/comments/11g4a9p/n_eleutherai_has_formed_a_nonprofit/,17,1677767743.0,"Over the past two and a half years, EleutherAI has grown from a group of hackers on Discord to a thriving open science research community. Today, [we are excited to announce](https://blog.eleuther.ai/year-two-preface/) the next step in our evolution: the formation of a non-profit research institute.

This will enable us to do much more, and we look forward to building a world class research group for public good! This organization will be lead by long-time contributors to EleutherAI: Stella Biderman (me) as Executive Director and Head of Research, Curtis Huebner as Head of Alignment, and Shiv Purohit as Head of Engineering.

The world has changed quite a lot since we first got started. When EleutherAI was founded, the largest open source GPT-3-style language model in the world had 1.5B parameters. GPT-3 itself was not available for researchers to study without special access from OpenAI, and most NLP researchers had a very minimal understanding of the engineering undertaking required to train such models or their capabilities & limitations. We started as a ragtag group nobody had heard of, and within a year had released the largest OSS GPT-3-style model in the world.

As access to LLMs has increased, our research has shifted to focus more on interpretability, alignment, ethics, and evaluation of AIs. We look forward to continuing to grow and adapt to the needs of researchers and the public

Check out our latest work at www.eleuther.ai or come hang out in our research lab at www.discord.gg/eleutherai

Huge shout out to the donors who have made our work possible: Stability AI, Hugging Face, CoreWeave, Nat Friedman, Lambda Labs, and Canva"
73,machinelearning,gpt-3,top,2020-10-21 06:41:32,[N] The GPT-3 API has a semantic search endpoint that few people seem to know about,Wiskkey,False,0.89,175,jf7td3,https://www.reddit.com/r/MachineLearning/comments/jf7td3/n_the_gpt3_api_has_a_semantic_search_endpoint/,29,1603262492.0,"[The best kept secret about OpenAI’s GPT-3](https://andrewmayneblog.wordpress.com/2020/10/20/the-best-kept-secret-about-openais-gpt-3/)

>When the first demos of GPT-3 content started to circulate it showed the amazing potential for a really smart language model to generate text and do cool things. Yet despite all the attention GPT-3 has been getting there’s one other aspect of it made available by OpenAI that’s been almost completely overlooked: Semantic Search.  
>  
>The OpenAI API not only lets you use GPT-3 to generate content, you can also use a special endpoint to have it sort through and rank content by how closely it relates to a block of text you provide.

The site used in the blog post is [https://gpttools.com/semanticsearch](https://gpttools.com/semanticsearch), which I found somewhere in the [author's Twitter feed](https://twitter.com/AndrewMayne).

The numbers in the animated images in the blog post are numbers that GPT-3's semantic search returns, indicating semantic similarity of a given text - i.e. ""document"" - to a given target - i.e. ""query"" - text (larger = more similar). According to a (possibly outdated) GPT 3 API document I've seen online, one API request can search up to 200 documents, with the restriction that the number of tokens in the query plus the number of tokens in the longest document must be less than 2000 tokens combined. [Here](https://gpttools.com/estimator) is a GPT (-3?) token number estimator.

Also covered at [https://www.reddit.com/r/GPT3/comments/jf2afo/semantic\_search\_demos\_using\_gpt3\_new\_web\_interface/](https://www.reddit.com/r/GPT3/comments/jf2afo/semantic_search_demos_using_gpt3_new_web_interface/)."
74,machinelearning,gpt-3,top,2021-11-10 17:45:13,"[P] Cedille, the largest French language model (6b), released in open source",MasterScrat,False,0.96,173,qqzuh0,https://www.reddit.com/r/MachineLearning/comments/qqzuh0/p_cedille_the_largest_french_language_model_6b/,25,1636566313.0,"## [📝 DEMO](https://app.cedille.ai) / [📘 REPO](https://github.com/coteries/cedille-ai)

We have spent the last 3 months of our lives, teraFLOPs of compute and gone through 300gb of text to bring you Cedille:

> **Ce que j'aime quand je mange une baguette, c'est** quand celle-ci est craquante.
Je ne saurais définir le terme ""craquant"" mais je sais que lorsque c'est le cas, je peux être sûre que la baguette est bonne.

The entirety of French spirit captured in measly 6B parameters! 🇫🇷🥖

More seriously, we are super excited to share Cedille, the so far largest French language model: https://en.cedille.ai

You can play with it right now on our playground (as long as servers hold 😅) : https://app.cedille.ai

We are proponents of “open AI” and as such have released a checkpoint for the world to use (MIT license) : https://github.com/coteries/cedille-ai

Another aspect we had fun with is dataset filtering. We have run the [whole C4 French dataset](https://github.com/allenai/allennlp/discussions/5265) through the Detoxify classifier to clean it up 🤬

Some acknowledgements :

* Cedille is based on GPT-J, the 6b model developed by the wizards at EleutherAI: https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/
* Cedille was also generously supported by the Google TFRC program: https://sites.research.google/trc/about/"
75,machinelearning,gpt-3,top,2021-11-09 03:05:07,"Alibaba DAMO Academy Creates World’s Largest AI Pre-Training Model, With Parameters Far Exceeding Google and Microsoft (10T parameters) [N]",GabrielMartinellli,False,0.9,162,qpuax4,https://www.reddit.com/r/MachineLearning/comments/qpuax4/alibaba_damo_academy_creates_worlds_largest_ai/,35,1636427107.0,"> [According to the company, the M6 has achieved the ultimate low carbon and high efficiency in the industry, using 512 GPUs to train a usable 10 trillion model within 10 days.](https://pandaily.com/alibaba-damo-academy-creates-worlds-largest-ai-pre-training-model-with-parameters-far-exceeding-google-and-microsoft/) Compared to the GPT-3, a large model released last year, M6 achieves the same parameter scale and consumes only 1% of its energy.

Thoughts? The pace of foundational models is starting to get scary, seems like a bigger and bigger model is pushed out every week."
76,machinelearning,gpt-3,top,2023-01-25 21:10:17,"[R] Blogpost on comparing Chatbots like ChatGPT, LaMDA, Sparrow, BlenderBot 3, and Claude",emailnazneen,False,0.94,161,10l9tet,https://www.reddit.com/r/MachineLearning/comments/10l9tet/r_blogpost_on_comparing_chatbots_like_chatgpt/,5,1674681017.0,"[https://huggingface.co/blog/dialog-agents](https://huggingface.co/blog/dialog-agents) breaks down the techniques behind ChatGPT -- instruction fine-tuning, supervised fine-tuning, chain-of-thought, read teaming, and more.

https://preview.redd.it/fv16fsemd9ea1.png?width=889&format=png&auto=webp&s=a8f24de27c40a946fec64eaa674f81ddef0d0cc3"
77,machinelearning,gpt-3,top,2023-04-07 17:43:03,[R] Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster,CS-fan-101,False,0.87,153,12et59x,https://www.reddit.com/r/MachineLearning/comments/12et59x/r_cerebrasgpt_open_computeoptimal_language_models/,38,1680889383.0,"Recently, we announced in [this post](https://www.reddit.com/r/mlscaling/comments/124t0hz/cerebras_open_sources_seven_gpt_models_and/?sort=new) the release of Cerebras-GPT — a family of open-source GPT models trained on the Pile dataset using the Chinchilla formula. Today, we are excited to announce the availability of the Cerebras-GPT research paper on [arXiv](https://arxiv.org/abs/2304.03208).

A few highlights from this paper:

* **Pre-training Results (Section 3.1)** \- Cerebras-GPT sets the efficiency frontier, largely because models were pre-trained with 20 tokens per parameter, consistent with findings in the Chinchilla paper.

[Pile test set loss given pre-training FLOPs for Cerebras-GPT, GPT-J, GPT-NeoX, and Pythia](https://preview.redd.it/gu0zendb1isa1.jpg?width=1344&format=pjpg&auto=webp&s=fa76446d0d8cd11e0f4be92b90a62f4cb7b73632)

&#x200B;

* **Downstream Results (Section 3.2)** \- Cerebras-GPT models form the compute-optimal Pareto frontier for downstream tasks as well. As Pythia and OPT models grow close to the 20 tokens per parameter count, they approach the Cerebras-GPT frontier FLOPs to accuracy

[Average zero- and five-shot downstream task accuracy plotted against FLOPs \(left\) and parameters \(right\). Higher accuracy is better](https://preview.redd.it/sdnf4w0e1isa1.jpg?width=1450&format=pjpg&auto=webp&s=3b246f4413cd2a7cb434aeed9c6a806f156b3b90)

&#x200B;

* **Maximal Update Parameterization (µP) and µTransfer (Section 3.3)** \- As we scaled the Cerebras-GPT models with standard parameterization (SP) along our scaling law, we experienced challenges predicting appropriate hyperparameters, and these models show substantial variance around their common scaling law. Across model sizes, our µP models exhibit an average of 0.43% improved Pile test loss and 1.7% higher average downstream task accuracy compared to our SP models. Here, we also show that µP performance scales more predictably, enabling more accurate performance extrapolation.

[Percentage loss increase relative to Cerebras-GPT scaling law plotted against training FLOPs](https://preview.redd.it/czqqothf1isa1.jpg?width=1344&format=pjpg&auto=webp&s=d121c85c73b7e3476e1c462f833b49e01a770459)"
78,machinelearning,gpt-3,top,2019-04-14 23:57:39,"[N] Tensorflow 2.0 Hackathon coming up. Also our team could use 1 more person if you're interested. It's an NLP project, and we got some great team members, including an advisor who has published current SoTA ML architectures.",Research2Vec,False,0.92,149,bd9eec,https://www.reddit.com/r/MachineLearning/comments/bd9eec/n_tensorflow_20_hackathon_coming_up_also_our_team/,11,1555286259.0,"Here's the link 

https://tensorflow.devpost.com/

We are looking for one more member, *ideally* someone with experience some of the current SoTA NLP models (Elmo, Transformer, BERT, GPT/2, ULMFiT, etc.) and wrangling data for those datasets  (Our adviser may have had their name published in the official paper for one of those papers ;) ) . But really, we're just looking for someone who has solid practical experience with Tensorflow and can data wrangle. 

If you're interested, PM me with what are your time commitments for the next 3 weeks, and your experience with Tensorflow."
79,machinelearning,gpt-3,top,2021-02-12 22:34:29,[Project] A GPT-3 powered dream simulation game,Hi_imseb,False,0.98,150,limlh0,https://www.reddit.com/r/MachineLearning/comments/limlh0/project_a_gpt3_powered_dream_simulation_game/,48,1613169269.0,"I've been working on this project for about a year now (originally with GPT-2), and felt like it was time to finally publicly announce it. So, here's a little trailer I put together:
https://youtu.be/ZoGhQwqTETQ

As you've probably figured out, speech is generated in real-time using GPT-3. It's also used to do some automatic world-building, by writing things like posters, book extracts, advertisements, newspaper headlines, graffiti and other details to discover in the world.

Eventually, the game will also use GPT-3's semantic search to control procedural world generation to create a world for the characters to populate which hopefully feels in some way relevant to the player's chosen dream.
Would love to hear your thoughts/questions :)
I'll be posting development updates on the project's [Twitter](https://twitter.com/AIElectricSheep?s=09)."
80,machinelearning,gpt-3,top,2021-01-06 06:00:20,[D] Is there any point in doing DL work when Google and OpenAI can just throw billions and trillions in compute to the problem and outperform anything that you and everyone else had done?,xEdwin23x,False,0.92,143,krhr34,https://www.reddit.com/r/MachineLearning/comments/krhr34/d_is_there_any_point_in_doing_dl_work_when_google/,45,1609912820.0,"\^Title.

BERT, GPT-3 for NLP. Now, Image-GPT, ViT, and DALL-E for CV. Transformers are changing the landscape of AI and all of its subfields, in a more pronounced way that CNN/RNNs ever did. The general paradigm for the SotA for a while now has been to throw deeper and wider (transformer) networks with more data than ever, as irreputable proof of ""The Bitter Lesson"" ([http://www.incompleteideas.net/IncIdeas/BitterLesson.html](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)).

I want to note I have nothing against what they do,and find it incredibly fascinating, but as a ""common"" researcher, or a worker in a company that does CV or NLP, or anything related to AI, is there even any point in continuing to research or work in your own subfield, when literally anything you can come up with, will be outdated by the time one of these big companies publishes their next huge model?"
81,machinelearning,gpt-3,top,2020-07-23 13:21:49,[D] The cost of training GPT-3,yusuf-bengio,False,0.96,138,hwfjej,https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/,35,1595510509.0,"There are two sources that estimate the cost of training GPT-3 at [$12 million](https://venturebeat.com/2020/06/11/openai-launches-an-api-to-commercialize-its-research/) and [$4.6 million](https://lambdalabs.com/blog/demystifying-gpt-3/). And I am a bit confused about how they got those numbers.

The used Microsoft Azure cloud offers, via InfiniBand connectable, [8xV100 machines](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/) at $10.7957/hour (1 year reserved), which translates to around $260 per day.

In the paper there is a sentence saying that they used half-precision and loss-scaling for training. One V100 can deliver up to [120 Teraflop/s](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html) using float16. Per machine (8xV100), this translates to 960 Teraflop/s in theory.  Let's assume in practice we can utilize our compute resources at \~50%, which gives us around 500 Teraflop/s per machine.

As we know from the paper it takes 3640 Petaflop/s-days to train the largest 175B model, which translates to a training run of 7280 days (or \~20 years) on a single 8xV100 machine. In terms of cost, this would be **$1.9 million**. 

Let's say we don't want to wait 20 years, so if we connect 64 of such 8xV100 machines we can reduce the training time to around 4 months (costs might go up due to reduced compute efficiency of the multi-node communication).

My question is, is the calculation above roughly accurate (Azure hourly costs, assumed compute utilization)?

After reading all the implementation details and optimization of the paper, I also began to think about development costs. Setting up a fast training pipeline to utilize the compute resources efficiently is not trivial given the size of the model and the resulting need to model parallelism."
82,machinelearning,gpt-3,top,2021-01-24 14:35:04,[D] Training 10x Larger Models and Accelerating Training with ZeRO-Offloading,lorenzkuhn,False,0.95,135,l40jdh,https://www.reddit.com/r/MachineLearning/comments/l40jdh/d_training_10x_larger_models_and_accelerating/,14,1611498904.0,"*I've been reading up on ZeRO-Offload* [since the paper](https://arxiv.org/abs/2101.06840) *was published last week – in* [this blog post](https://efficientdl.com/an-introduction-to-zero-offloading/) *and below I've written up a few introductory notes on what it is, when it's useful, how it can be used and how it works. Let me know if I forgot anything important or got something wrong.*

## What is ZeRO-Offloading?

ZeRO-Offloading is a way of reducing GPU memory usage during neural network training by offloading data and compute from the GPU(s) to CPU. Crucially this is done in a way that provides high training throughput and that avoids major slow-downs from moving the data and doing computations on CPU.

ZeRO-offloading makes it possible to train models that are up to 10x larger than previously possible with the same hardware – even on a single GPU. You could for instance train GPT-2 (\~10 billion parameters) on a single V100 GPU with 32GB RAM.  Lastly, it promises almost-linear scaling in multi-GPU settings.

## When is this going to be useful for you?

* If you want to train larger models or if you want to train your current models faster since ZeRO-offloading allows you to train with larger batch sizes.
* If you're working with PyTorch and are willing/able to work with Microsoft's [DeepSpeed library](https://www.deepspeed.ai/tutorials/zero-offload/) (other ZeRO-Offloading implementations are on the horizon but not available for now). Alternatively you could try to adapt [the official implementation ](https://github.com/microsoft/DeepSpeed/blob/6e65c2cc084ecfc393c67a2f64639e8d08d325f6/deepspeed/runtime/zero/stage2.py)yourself.
* If you're willing to take on some modelling constraints. The current version of ZeRO-Offloading is tied to mixed precision training with Adam, for instance.

## How can you use it?

ZeRO-Offloading [is implemented in Microsoft's DeepSpeed](https://www.deepspeed.ai/tutorials/zero-offload/) library. A [native PyTorch ZeRO implementation is ](https://github.com/pytorch/pytorch/pull/46750)being developed but offloading is not supported at this point. The [official implementation available](https://github.com/microsoft/DeepSpeed/blob/6e65c2cc084ecfc393c67a2f64639e8d08d325f6/deepspeed/runtime/zero/stage2.py), so you could try to work directly with that.

Once you're set up within DeepSpeed, the additional effort required to use ZeRO-Offloading seems to be quite small, [essentially it's just modifying a few flags and a configuration file.](https://www.deepspeed.ai/tutorials/zero-offload/)

[Hugging Face's transformers library ](https://huggingface.co/transformers/)has an experimental integration with DeepSpeed. [Stas Bekman has a nice blog](https://huggingface.co/blog/zero-deepspeed-fairscale) post describing how to use it and what kind of results you can expect on a few benchmarks.

[Facebook Research's fairscale](https://github.com/facebookresearch/fairscale) has a partial [implementation of ZeRO,](https://github.com/facebookresearch/fairscale) the multi-GPU memory optimization method on top of which ZeRO-Offloading is built. CPU-offloading does not to seem to be supported at this point.

## How does it work?

ZeRO-Offloading is based on the Zero Redundancy Optimizer (ZeRO), so let's quickly review what ZeRO is and how it works.

**The Zero Redundancy Optimizer**

ZeRO-Offloading is based on the [Zero Redundancy Optimizer (ZeRO)](https://arxiv.org/pdf/1910.02054.pdf).

ZeRO, in a nutshell, is a memory optimization method for data-parallel model-parallel training in which gradients, parameters and optimizer state are distributed across the memory of multiple GPUs without any redundancy. This is done in a way that keeps the communication overhead between GPUs relatively low.

I recommend reading this [introductory blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/) – especially watching the animation – and then [reading the paper](https://arxiv.org/pdf/1910.02054.pdf) if you want to go deeper. The official [implementation of ZeRO can be found here.](https://github.com/microsoft/DeepSpeed/tree/master/deepspeed)

Here's a figure that illustrates the distribution of parameters, gradients and optimizer states across GPUs ([source](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)):

https://preview.redd.it/ge885b7tead61.png?width=951&format=png&auto=webp&s=e7f65f53068062f98a5293123d360db09efe1bbd

**ZeRO-Offloading**

Quoting directly from[ a](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/#toc-heading-3)[ blog post](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/#toc-heading-3) on ZeRO-Offloading (I've added the Figure that is being referred to below):

>(...) , ZeRO-Offload inherits the optimizer state and gradient partitioning from ZeRO-2. Unlike ZeRO-2, instead of having each GPU keep a partition of the optimizer state and gradients, ZeRO-Offload offloads both to host CPU memory. Optimizer states are kept in CPU memory for the entire training. Gradients, on the other hand, are computed and averaged using reduce-scatter on the GPUs during the backward pass, and each data-parallel process then offloads the averaged gradients belonging to its partition to the CPU memory (*g offload* in Figure 7) while discarding the rest. Once the gradients are available on the CPU, optimizer state partitions are updated in parallel by each data parallel process directly on the CPU (*p update* in Figure 7). After the update, parameter partitions are moved back to GPU followed by an all-gather operation on the GPU to gather all the updated parameters (*g swap* in Figure 7). ZeRO-Offload also exploits overlapping between communication (such as *g offload* and *g swap*) and computation (such as the backward pass and *p update*) using separate CUDA streams to maximize training efficiency.

This process is illustrated in this figure from the blog post:

&#x200B;

https://preview.redd.it/6iqno8fzead61.png?width=1024&format=png&auto=webp&s=7a3ac623bfc516de12adf328a6ed430929f1f93f

One thing to note here is that ZeRO-Offloading is designed specifically for *mixed precision training with Adam.* In particular, the current version of ZeRO-Offloading uses [DeepCPUAdam](https://github.com/microsoft/DeepSpeed/tree/master/deepspeed/ops/adam), an optimized version of Adam.  The main reason for using this optimizer is to avoid the CPU computation becoming a bottleneck in the whole process. This version of Adam seems to be about 6x faster than the PyTorch implementation.

Lastly, here are some of the results from the [ZeRO-Offload paper](https://arxiv.org/pdf/2101.06840.pdf) I found particularly interesting:

1. The largest models that can be trained with ZeRO-Offload:

https://preview.redd.it/yrwi68a4fad61.png?width=2000&format=png&auto=webp&s=5e41832858584c8758872643a9a9120f46db80a6

2. Near linear scaling of throughput per GPU as the number of GPUs is increased – when used in combination with ZeRO:

https://preview.redd.it/9srlst07fad61.png?width=2000&format=png&auto=webp&s=a91984cbf4175d313ccc4d1485291dc717df641e

3. Throughput per GPU of PyTorch, L2L and ZeRO-Offload as a function of the model size:

https://preview.redd.it/rg65lrr8fad61.png?width=2000&format=png&auto=webp&s=8177ae16f8de12a5f6c2136e659b237fbab64579

Here's the paper:[ ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/pdf/2101.06840.pdf)."
83,machinelearning,gpt-3,top,2021-04-27 16:29:15,[P] We gave GPT-3 random ingredients and cooked the recipe it came up with (Video),ykilcher,False,0.87,127,mzsdiw,https://www.reddit.com/r/MachineLearning/comments/mzsdiw/p_we_gave_gpt3_random_ingredients_and_cooked_the/,20,1619540955.0,"[https://youtu.be/hIoCn\_9QTVU](https://youtu.be/hIoCn_9QTVU)

We went to the store and bought a set of completely random ingredients and had OpenAI's GPT-3 come up with a recipe, which we then cooked and ate.

&#x200B;

Our Rules:

1. All Vegan

2. Follow the recipe as closely as possible

3. We must finish our plates

&#x200B;

The Recipe:

1. Boil the potatoes and carrots.

2. In the meantime, prepare the VEGAN minced meat, or use pre-cooked soy meat. 

3. Then fry the VEGAN butter, add the garlic, and the mushrooms, and stir for 2 minutes. 

4. Add the soy cream, stir and cook for three minutes. 

5. Add the pickles, tomatoes, and beans, stir and simmer for five minutes. 

6. Cut the bread in small squares and fry in the vegan butter until golden brown.

7. Cut the limes into cubes and squeeze the juice into the bean mixture. 

8. Add the soy sauce, parsley, salt, pepper, cumin, cilantro, and dried figs. Stir, and add the kale.

9. Pour the bean mix into a blender. 

10. Bake for 5 minutes in the oven at 180C. 

11. Cut the sweet potatoes in cubes, and add to a pot with the remaining butter. Add the red beans mixture. 

12. Cut the bell pepper into cubes and add to the pot. 

13. Add the VEGAN minced meat, and cook in the oven at 180C for 10 minutes. 

14. Add the avocado. 

15. Add the chickpeas. 

16. Add the chocolate.

17. Serve on bread with mustard and pommegrenade on top.

&#x200B;

VIDEO OUTLINE:

0:00 - The Plan

2:15 - Ingredients

4:05 - What is GPT-3?

6:10 - Let's cook

12:25 - The Taste Test

&#x200B;

GPT-3 on Wikipedia: [https://en.wikipedia.org/wiki/GPT-3](https://en.wikipedia.org/wiki/GPT-3)

GPT-3 Paper: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)"
84,machinelearning,gpt-3,top,2022-12-21 05:29:37,[D] Running large language models on a home PC?,Zondartul,False,0.96,125,zrbfcr,https://www.reddit.com/r/MachineLearning/comments/zrbfcr/d_running_large_language_models_on_a_home_pc/,104,1671600577.0,"I'm trying to figure out how to go about running something like GPT-J, FLAN-T5, etc, on my PC, without using cloud compute services (because privacy and other reasons). However, GPT-J-6B needs either \~14 GB of VRAM or 4x as much plain RAM.

Upgrading my PC for 48 GB of RAM is possible, and 16, 24 GB graphics cards are available for general public (though they cost as much as a car), but anything beyond that is in the realm of HPC, datacenter hardware and ""GPU accelerators""... I.e. 128 GB GPUs exist out there somewhere, but the distributors don't even list a price, it's just ""get a quote"" and ""contact us""... meaning it's super expensive and you need to be a CEO of medium-sized company for them to even talk to you?

I'm trying to figure out if it's possible to run the larger models (e.g. 175B GPT-3 equivalents) on consumer hardware, perhaps by doing a very slow emulation using one or several PCs such that their collective RAM (or swap SDD space) matches the VRAM needed for those beasts.

So the question is ""will it run super slowly"" or ""will it fail immediately due to completely incompatible software / being impossible to configure for anything other than real datacenter hardware""?"
85,machinelearning,gpt-3,top,2022-09-27 07:31:44,[P] Efficient Few-shot Learning with Sentence Transformers,lewtun,False,0.95,124,xp9uoc,https://www.reddit.com/r/MachineLearning/comments/xp9uoc/p_efficient_fewshot_learning_with_sentence/,16,1664263904.0,"Hi there, it's Lewis here from the open-source team at Hugging Face 🤗

I'm excited to share new research on **few-shot learning with language models** that we've been working on with Intel 🧑‍🔬. We've also open-source a library that let's you train our models with a few lines of code 👉: [https://github.com/huggingface/setfit](https://github.com/huggingface/setfit)

tl;dr we found a way to apply pretrained Sentence Transformers in regimes where one has little labeled data. The method is illustrated below, and involves a two-stage training process:

&#x200B;

1. Fine-tune the Sentence Transformer with a a few labeled examples (e.g. 8 per class) using a contrastive loss
2. Freeze the weights of the tuned Sentence Transformer and train a simple classification head (e.g. logistic regression)

https://preview.redd.it/lw6o49vcrcq91.png?width=971&format=png&auto=webp&s=a8ab0de8a4c44e9cc8f015184e57e1a64fbb8e97

Surprisingly, this simple technique outperforms GPT-3 on the [RAFT benchmark](https://huggingface.co/spaces/ought/raft-leaderboard), despite using models that are 350x smaller! 

This means you can now do few-shot learning in around 30s on Google Colab (or even your CPU if are willing to wait a few minutes) 🤓

For more details, check out our blog post: [https://huggingface.co/blog/setfit](https://huggingface.co/blog/setfit)"
86,machinelearning,gpt-3,top,2023-06-09 11:43:46,[D] LLM's in languages other than English.,herr94491,False,0.9,124,1452ziq,https://www.reddit.com/r/MachineLearning/comments/1452ziq/d_llms_in_languages_other_than_english/,52,1686311026.0,"Hello everyone, as a ML practitioner myself I've tried making LLM's using GPT-3 in my native tongue as a side project. But the issue is, the data quality and availability is pretty terrible. I've found like 2 good datasets on Hugging Face but that's about it.

My question is, has anyone else had the same problem? If so, what do you guys do whenever you're short of quality text data for non-English LLM's in particular?

I've done a bit of my own research, it seems most of non-English data on the internet is nonsensical and often machine-translated. 95% of low-resource languages aren't even identified correctly to begin with. The ones that do exist are the same outdated things like Wikipedia or parliamentary legislation.

It made me go down a rabbit hole and realise there is currently a shortage in supply of high quality human-labelled data in languages other than English. So I've decided to actually get a gist of how many people like me are affected by this problem.

If you guys have any other sources for non-English datasets that don't make your LLM go crazy I would love to hear it, also what language are you guys trying to create LLM's in?

Update: I am trying to find quality datasets in Telugu (96m speakers). It has a 62% accuracy rate on ChatGPT4 on MMLU."
87,machinelearning,gpt-3,top,2020-11-03 17:14:30,[N] Update: IBM's online GPT-3-powered English to Bash Unix command line translator now works again,Wiskkey,False,0.99,123,jnebn1,https://www.reddit.com/r/MachineLearning/comments/jnebn1/n_update_ibms_online_gpt3powered_english_to_bash/,33,1604423670.0,"Because there were fewer than 24 hours between my [initial post about IBM's online GPT-3-powered English to Bash Unix command line translator](https://www.reddit.com/r/GPT3/comments/jl1aa6/the_nlc2cmd_challenge_site_has_a_gpt3powered/) and the time the functionality stopped working, some of you probably never had the opportunity to try it. The functionality was not working even yesterday if I recall correctly. I will not post again if this happens again in the future."
88,machinelearning,gpt-3,top,2020-08-25 12:08:19,[D] What are the free services using GPT-3 that I can try?,Raphael-Rose,False,0.94,119,igavbq,https://www.reddit.com/r/MachineLearning/comments/igavbq/d_what_are_the_free_services_using_gpt3_that_i/,60,1598357299.0,"Something along the lines of PhilosopherAI.  
Thanks"
89,machinelearning,gpt-3,top,2024-01-06 16:23:04,[D] Incredible results with Long Agent Tree Search with open source models,ArtZab,False,0.98,115,1903k24,https://www.reddit.com/r/MachineLearning/comments/1903k24/d_incredible_results_with_long_agent_tree_search/,9,1704558184.0,"Hello,

I saw GPT-4 with Long Agent Tree Search topping the HumanEval with a 94.4% pass@1 for a few weeks now. [https://paperswithcode.com/sota/code-generation-on-humaneval](https://paperswithcode.com/sota/code-generation-on-humaneval)

&#x200B;

The authors of the [original paper](https://arxiv.org/abs/2310.04406) posted their code in their [official github repo](https://github.com/andyz245/LanguageAgentTreeSearch) . I had to change some code to try it out with CodeLlama-7b and the human eval with pass@1 and only 2 max iterations increases HumanEval score from 37% to about 70%.

This is some incredible results in my opinion because this score is higher than GPT-3.5 with only a 7b model. I assume more testing has to be done, but nevertheless I am surprised people are not talking more about this."
90,machinelearning,gpt-3,top,2021-05-07 20:53:23,[D] Invitation to help address AI misrepresentation and misconceptions,regalalgorithm,False,0.87,117,n78p8v,https://www.reddit.com/r/MachineLearning/comments/n78p8v/d_invitation_to_help_address_ai_misrepresentation/,32,1620420803.0,"TLDR: I run a site to debunk misconceptions of AI news, pls  positive response, so hope bringing it up again now that we could use more help is fine.

As I posted before, for more than 3 years I've been running this thing called [Skynet Today](http://www.skynettoday.com/) (the name is meant to be ironic/news-y), with the mission of ""Putting AI News In Perspective"", or in other words  debunk inaccurate portrayals of AI research in media and also put out articles that put things in perspective. 

As many people  here are researchers and feel annoyed at hype/misconceptions about AI, I  wonder if any of you might want to join our effort. 

We are basically a couple of grad students doing this in our spare time, and have not put out any new articles in a while due to being busy / not having much help (writing is a lot of work!). If  interested, please consider taking a look at our [contribution survey](https://www.skynettoday.com/contribute), or just message me. Thanks!

Some examples of articles we've put out include:

* [DeepMind’s AlphaFold 2—An Impressive Advance With Hyperbolic Coverage](https://www.skynettoday.com/briefs/alphafold2)
* [The State of Deepfakes in 2020](https://www.skynettoday.com/overviews/state-of-deepfakes-2020)
* [GPT-3: An AI Breakthrough, but not Coming for Your Job](https://www.skynettoday.com/briefs/gpt3)
* [IBM, Microsoft, and Amazon Halt Sales of Facial Recognition to Police, Call for Regulations](https://www.skynettoday.com/briefs/face-recog-police)
* [Boston Dynamics' robots — impressive, but far from the Terminator  ](https://www.skynettoday.com/briefs/boston-dynamics)

TLDR: I run a site to debunk misperceptions about AI, pls [join](https://www.skynettoday.com/contribute) if you wanna help"
91,machinelearning,gpt-3,top,2023-04-20 01:30:47,[D] GPT-3T: Can we train language models to think further ahead?,landongarrison,False,0.91,118,12shf18,https://www.reddit.com/r/MachineLearning/comments/12shf18/d_gpt3t_can_we_train_language_models_to_think/,62,1681954247.0,"In a recent talk done by Sebastian Bubeck called “Sparks of AGI: Early experiments done with GPT-4”, Sebastian mentioned on thing in his presentation that caught my attention (paraphrased quote):

> “GPT-4 cannot plan, but this might be a limitation because it can only look one token into the future”

While very simple on the surface, this may actually be very true: what if we are training our language models to be very shallow thinkers and not actually look far enough ahead? Could single token prediction actually be a fundamental flaw?

In this repo, I try a very early experiment called GPT-3T, a model that predicts 3 tokens ahead at one time step. While incredibly simple on the surface, this could potentially be one way to overcome the planning issue that you find in GPTs. Forcing an autoregressive model to predict further ahead at scale *may* bring out much more interesting emergent behaviours than what we’ve seen in single token GPTs.

__

**Experiments**

My personal experiments are overall inconclusive on either side: I have only pre-trained a very small model (300 million params on WebText-10K) and it achieves a decent ability to generate text. However as you can see, this model heavily under optimized but I do not have the resources to carry this out further.

If anyone would like to try this experiment with more scale, I would love to get an answer to this question to improve upon this model. This repo is intended to allow anyone who would like to pre-train a GPT-3T model easily to run this experiment. From what I have seen, this has not been tried before and I am very curious to see results.

__

**Edit:** GitHub repo is buried in the comments (sorry this post will be taken down if I include it in the main post)"
92,machinelearning,gpt-3,top,2023-05-12 22:39:24,[R] DetGPT: Detect What You Need via Reasoning,OptimalScale_2023,False,0.89,112,13fzf2m,https://www.reddit.com/r/MachineLearning/comments/13fzf2m/r_detgpt_detect_what_you_need_via_reasoning/,10,1683931164.0,"https://reddit.com/link/13fzf2m/video/fwcuwd3q9hza1/player

Throughout history, humans have dreamed of robots that could assist them with their daily lives and work. With the emergence of home assistants and OpenAI's Copilot, requests such as 'Please lower the temperature of the air conditioning' or even 'Please help me build an online store' have become possible.The emergence of GPT-4 has further demonstrated the potential of multimodal large models in visual understanding. In the open-source small model space, LLAVA and minigpt-4 have performed well in image recognition and chat, and can even suggest recipes for food images. However, these models still face significant challenges in practical implementation: they lack accurate localization capabilities and cannot provide specific locations of objects in images, nor can they understand complex human instructions to detect specific objects, making it difficult for them to perform specific tasks as requested by humans. In practical scenarios, if people could simply take a photo and ask an intelligent assistant for the correct answer to a complex problem, such a 'take a photo and ask' feature would be incredibly cool.  
To implement the ""**take a photo and ask**"" feature, robots need to have several capabilities:

1. Language understanding: the ability to listen and understand human intentions.
2. Visual understanding: the ability to understand the objects in the image.
3. Common sense reasoning: the ability to convert complex human intentions into precise and locatable targets.
4. Object localization: the ability to locate and detect corresponding objects in the image.

Currently, only a few large models (such as Google's PaLM-E) possess all four of these capabilities. However, researchers from the Hong Kong University of Science and Technology and the University of Hong Kong have proposed an open-source model called DetGPT (DetectionGPT), which only needs to fine-tune three million parameters to easily acquire complex reasoning and local object localization capabilities that can be generalized to most scenarios. This means that the model can easily recognize the objects that humans are interested in through self-knowledge reasoning and understand abstract human instructions. They have already developed a ""take a photo and ask"" demo using the model, which can be experienced online: [https://detgpt.github.io/](https://detgpt.github.io/)DetGPT allows users to operate everything with natural language without the need for complex commands or interfaces. In addition, DetGPT has intelligent reasoning and object detection capabilities, which can accurately understand user needs and intentions. For example, if a human gives a language instruction, ""I want to have a cold beverage,"" the robot first searches for a cold drink in the scene but does not find any. It then begins to think, ""There is no visible beverage. Where can I find it?"" Through its powerful common sense reasoning ability, the model realizes that the fridge is a possible location and scans the scene to successfully locate the drink!

https://preview.redd.it/ai8j05uy9hza1.png?width=1280&format=png&auto=webp&s=c8d833e2db63d0ebceb1c99aa68d89cc7fa7dcc7

  
Online demo: [https://detgpt.github.io/](https://detgpt.github.io/) 

Open-source code: [https://github.com/OptimalScale/DetGPT](https://github.com/OptimalScale/DetGPT)

&#x200B;

## Online demo: [https://detgpt.github.io/](https://detgpt.github.io/)

Feeling thirsty in the summer? DetGPT easily understands and finds the refrigerator with the image of where the iced beverages are.

https://preview.redd.it/kiiv4tb1ahza1.jpg?width=1280&format=pjpg&auto=webp&s=49a055fafd1c4e50cea46723bc567896ec60499e

Need to wake up early tomorrow? DetGPT makes it easy with an electronic alarm clock.

https://preview.redd.it/0lby9hh2ahza1.png?width=1280&format=png&auto=webp&s=e6fc77356d080fe755310dbc74879ac4f7a8b894

Do you suffer from hypertension and fatigue? Are you unsure of what fruits to buy at the market to help alleviate your symptoms? DetGPT acts as your nutrition teacher and provides guidance on which fruits can help relieve hypertension.

https://preview.redd.it/c1r7kwv3ahza1.png?width=1280&format=png&auto=webp&s=169fb015df8e9973c48a26a35caeb5892ce1d92f

Stuck in the Zelda game and can't pass it? DetGPT helps you disguise yourself and get past the challenges in the Gerudo Town.

https://preview.redd.it/wdny0v55ahza1.png?width=1280&format=png&auto=webp&s=070de46239405993eefeb5112bd4a459baec94df

Unsure of potential dangers in your surroundings within the range of the image? DetGPT acts as your safety officer and helps protect you from any potential risks.

https://preview.redd.it/nf64a176ahza1.png?width=1280&format=png&auto=webp&s=f6b641c2163076f5403361561c95663450227cd1

What items in the image could be dangerous for children? DetGPT still has got you covered.

https://preview.redd.it/oz8hx987ahza1.png?width=1280&format=png&auto=webp&s=b2d8ad27ff758a2d39e87fba86f7cc5a2b4a2c76

## Features of DetGPT

DetGPT has several unique features:

1. It has a significantly improved understanding of specific objects in images. Compared to previous models that use multimodal dialogues, DetGPT can retrieve and locate target objects from images based on the user's instructions, rather than simply describing the entire image.
2. It can understand complex human instructions, which lowers the barrier for users to ask questions. For example, the model can understand the question ""find fruits that can relieve hypertension?"" Traditional object detection requires humans to know the answer and pre-set the detection category, such as ""banana.""
3. DetGPT can use existing LLM knowledge to reason and accurately locate the corresponding object in the image that can solve more complex tasks. For complex tasks, such as ""fruits that can relieve hypertension,"" DetGPT can reason step by step: relieving hypertension -> potassium can relieve hypertension -> bananas are rich in potassium -> bananas can relieve hypertension -> need to identify the object banana.
4. It provides answers beyond human common sense. For some uncommon questions, such as which fruits are rich in potassium, the model can provide answers based on existing knowledge.

## A new direction: reasoning-based object detection

Traditional object detection tasks require pre-defined categories of possible objects for detection. However, providing accurate and comprehensive descriptions of the objects to be detected can be difficult and unrealistic for humans. This is due to the limitations of human memory and knowledge. For instance, a doctor may recommend that people with hypertension eat fruits rich in potassium, but may not know which specific fruits are rich in potassium, making it impossible to provide specific fruit names for the model to detect. If the question ""Identify fruits that can help alleviate hypertension"" could be directly posed to the detection model, humans would only need to take a photo, and the model could think, reason, and detect fruits rich in potassium, making the problem much simpler.Moreover, the examples of object categories provided by humans are not always comprehensive. For instance, if monitoring is required to detect behaviors that violate public order relative to public places, humans may only be able to provide a few simple scenarios, such as holding a knife or smoking. However, if the question ""detect behaviors that violate public order"" is directly posed to the detection model, the model can think and reason based on its own knowledge, thus capturing more unacceptable behaviors and generalizing to more relevant categories that need to be detected. After all, the knowledge that ordinary humans have access to is limited, and the object categories that they can provide examples of are also limited. However, if there is a big brain-like ChatGPT-like model to assist and reason, the instructions that humans need to provide will be much simpler, and the obtained answers will be much more accurate and comprehensive.To address the limitations of human instructions and their abstract nature, researchers from the Hong Kong University of Science and Technology and the University of Hong Kong have proposed a new direction called ""reasoning-based object detection."" In simple terms, humans give complex tasks, and the model can understand and reason about which objects in the image might be able to complete the task, and then detect them. For example, if a person describes ""I want to drink a cold drink, where can I find it,"" and the model sees a picture of a kitchen, it can detect the ""refrigerator."" This topic requires the perfect combination of multimodal models' image understanding ability and the rich knowledge stored in language models. It is used in fine-grained detection scenarios to accurately locate objects of interest to humans in images without pre-defined object categories.  


# The Approach

&#x200B;

https://preview.redd.it/ho9ux1pcahza1.png?width=1280&format=png&auto=webp&s=bf42e1baffa2925e8b946b191766ca116aec2fe1

The ""reasoning-based object detection"" is a challenging problem because the detector needs to understand and reason about the user's coarse-grained/abstract instructions and analyze the current visual information to locate the target object accurately. In this direction, researchers from the Hong Kong University of Science and Technology and the University of Hong Kong have conducted some preliminary explorations. Specifically, they use a pre-trained visual encoder (BLIP-2) to extract visual features from images and align the visual features to the text space using an alignment function. They use a large-scale language model (Robin/Vicuna) to understand the user's question, combined with the visual information they see, to reason about the objects that users are truly interested in. Then, they provide the object names to the pre-trained detector (Grounding-DINO) for specific location prediction. In this way, the model can analyze the image based on any user instructions and accurately predict the location of the object of interest to the user.  
It is worth noting that the difficulty here mainly lies in the fact that the model needs to achieve task-specific output formats for different specific tasks as much as possible without damaging the model's original abilities. To guide the language model to follow specific patterns and generate outputs that conform to the object detection format, the research team used ChatGPT to generate cross-modal instruction data to fine-tune the model. Specifically, based on 5000 coco images, they used ChatGPT to create a 30,000 cross-modal image-text fine-tuning dataset. To improve the efficiency of training, they fixed other model parameters and only learned cross-modal linear mapping. Experimental results show that even if only the linear layer is fine-tuned, the language model can understand fine-grained image features and follow specific patterns to perform inference-based image detection tasks, showing excellent performance.  
This research topic has great potential. Based on this technology, the field of home robots will further shine: people in homes can use abstract or coarse-grained voice instructions to make robots understand, recognize, and locate the objects they need, and provide relevant services. In the field of industrial robots, this technology will bring endless vitality: industrial robots can cooperate more naturally with human workers, accurately understand their instructions and needs, and achieve intelligent decision-making and operations. On the production line, human workers can use coarse-grained voice instructions or text input to allow robots to automatically understand, recognize, and locate the items that need to be processed, thereby improving production efficiency and quality.  
With object detection models that come with reasoning capabilities, we can develop more intelligent, natural, and efficient robots to provide more convenient, efficient, and humane services to humans. This is a field with broad prospects and deserves more attention and further exploration by more researchers.  
DetGPT supports multiple language models and has been validated based on two language models, Robin-13B and Vicuna-13B. The Robin series language model is a dialogue model trained by the LMFlow team ( https://github.com/OptimalScale/LMFlow) at the Hong Kong University of Science and Technology, achieving results competitive to Vicuna on multiple language ability evaluation benchmarks (model download: [https://github.com/OptimalScale/LMFlow#model-zoo](https://github.com/OptimalScale/LMFlow#model-zoo)). Previously, the LMFlow team trained a vertical GPT model using a consumer-grade 3090 graphics card in just 5 hours. Today, this team, in collaboration with the NLP Group at the University of Hong Kong, has brought us a multimodal surprise.  
Welcome to try our demo and open-source code!  
Online demo: [https://detgpt.github.io/](https://detgpt.github.io/) Open-source code: [https://github.com/OptimalScale/DetGPT](https://github.com/OptimalScale/DetGPT)"
93,machinelearning,gpt-3,top,2021-09-18 07:08:51,[R] Google AI Introduces Two New Families of Neural Networks Called ‘EfficientNetV2’ and ‘CoAtNet’ For Image Recognition,techsucker,False,0.92,116,pqhqjv,https://www.reddit.com/r/MachineLearning/comments/pqhqjv/r_google_ai_introduces_two_new_families_of_neural/,14,1631948931.0,"Training efficiency has become a significant factor for deep learning as the neural network models, and training data size grows. [GPT-3](https://arxiv.org/abs/2005.14165) is an excellent example to show how critical training efficiency factor could be as it takes weeks of training with thousands of GPUs to demonstrate remarkable capabilities in few-shot learning.

To address this problem, the Google AI team introduce two families of neural networks for image recognition. First is [EfficientNetV2](https://arxiv.org/abs/2104.00298), consisting of CNN (Convolutional neural networks) with a small-scale dataset for faster training efficiency such as [ImageNet1k](https://www.image-net.org/) (with 1.28 million images). Second is a hybrid model called [CoAtNet](https://arxiv.org/abs/2106.04803), which combines [convolution](https://en.wikipedia.org/wiki/Convolution) and [self-attention](https://en.wikipedia.org/wiki/Self-attention) to achieve higher accuracy on large-scale datasets such as [ImageNet21](https://www.image-net.org/) (with 13 million images) and [JFT](https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html) (with billions of images). As per the research report by Google, [EfficientNetV2](https://arxiv.org/abs/2104.00298) and [CoAtNet](https://arxiv.org/abs/2106.04803) both are 4 to 10 times faster while achieving state-of-the-art and 90.88% top-1 accuracy on the well-established [ImageNet](https://www.image-net.org/) dataset.

# [7 Min Read](https://www.marktechpost.com/2021/09/17/google-ai-introduces-two-new-families-of-neural-networks-called-efficientnetv2-and-coatnet-for-image-recognition/) | [Paper (CoAtNet)](https://arxiv.org/abs/2106.04803) | [Paper (EfficientNetV2)](https://arxiv.org/abs/2104.00298) | [Google blog](https://ai.googleblog.com/2021/09/toward-fast-and-accurate-neural.html) | [Code](https://github.com/google/automl/tree/master/efficientnetv2)

&#x200B;

https://preview.redd.it/ipmkyt7eo7o71.png?width=1392&format=png&auto=webp&s=22764f4268a6c12acb85b8b71a7331cc6446d984"
94,machinelearning,gpt-3,top,2022-12-03 08:22:02,RickandMortify: A playground for creating new episodes of Rick and Morty using the state-of-the-art in generative AI (GPT-3 + Stable Diffusion) [P],Acceptable_Raisin_55,False,0.89,113,zbbgpq,https://rickandmortify.com/,23,1670055722.0,
95,machinelearning,gpt-3,top,2021-01-19 05:25:07,[R] Data Movement Is All You Need: A Case Study on Optimizing Transformers,CATALUNA84,False,0.9,113,l0d38r,https://www.reddit.com/r/MachineLearning/comments/l0d38r/r_data_movement_is_all_you_need_a_case_study_on/,22,1611033907.0,"**Abstract** — Transformers have become widely used for language modeling and sequence learning tasks, and are one of the most important machine learning workloads today. Training one is a very compute-intensive task, often taking days or weeks, and significant attention has been given to optimizing transformers. Despite this, existing implementations do not efficiently utilize GPUs. We find that data movement is the key bottleneck when training. Due to Amdahl’s Law and massive improvements in compute performance, training has now become memory-bound. Further, existing frameworks use suboptimal data layouts. Using these insights, we present a recipe for globally optimizing data movement in transformers. We reduce data movement by up to 22.91% and overall achieve a 1.30× performance improvement over state-of-the-art frameworks when training BERT. Our approach is applicable more broadly to optimizing deep neural networks, and offers insight into how to tackle emerging performance bottlenecks.

  
**Quote** — ""For the GPT-3 transformer model with a training cost of $12M, our optimizations could save $3.6M and more than 120 MWh energy.""

**Link** — [https://arxiv.org/abs/2007.00072](https://arxiv.org/abs/2007.00072)

Some great work in analyzing the dataflow in the BERT encoder architecture using SDFGs and the DaCe environment, and optimizing it via CUDA kernels.

https://preview.redd.it/aqetnmob28c61.png?width=537&format=png&auto=webp&s=0dd6d0e9fbcccc0e86d993111ca9701203f97403

Also, Multi-head attention performance can be extended to other research domains."
96,machinelearning,gpt-3,top,2023-09-21 15:01:28,[N] OpenAI's new language model gpt-3.5-turbo-instruct can defeat chess engine Fairy-Stockfish 14 at level 5,Wiskkey,False,0.92,116,16oi6fb,https://www.reddit.com/r/MachineLearning/comments/16oi6fb/n_openais_new_language_model_gpt35turboinstruct/,178,1695308488.0,"[This Twitter thread](https://twitter.com/GrantSlatton/status/1703913578036904431) ([Nitter alternative](https://nitter.net/GrantSlatton/status/1703913578036904431) for those who aren't logged into Twitter and want to see the full thread) claims that [OpenAI's new language model gpt-3.5-turbo-instruct](https://analyticsindiamag.com/openai-releases-gpt-3-5-turbo-instruct/) can ""readily"" beat Lichess Stockfish level 4 ([Lichess Stockfish level and its rating](https://lichess.org/@/MagoGG/blog/stockfish-level-and-its-rating/CvL5k0jL)) and has a chess rating of ""around 1800 Elo."" [This tweet](https://twitter.com/nabeelqu/status/1703961405999759638) shows the style of prompts that are being used to get these results with the new language model.

I used website parrotchess\[dot\]com (discovered [here](https://twitter.com/OwariDa/status/1704179448013070560)) to play multiple games of chess purportedly pitting this new language model vs. various levels at website Lichess, which supposedly uses Fairy-Stockfish 14 according to the Lichess user interface. My current results for all completed games: The language model is 5-0 vs. Fairy-Stockfish 14 level 5 ([game 1](https://lichess.org/eGSWJtNq), [game 2](https://lichess.org/pN7K9bdS), [game 3](https://lichess.org/aK4jQvdo), [game 4](https://lichess.org/S9SGg8YI), [game 5](https://lichess.org/OqzdkDhE)), and 2-5 vs. Fairy-Stockfish 14 level 6 ([game 1](https://lichess.org/zP68C6H4), [game 2](https://lichess.org/4XKUIDh1), [game 3](https://lichess.org/1zTasRRp), [game 4](https://lichess.org/lH1EMqJQ), [game 5](https://lichess.org/mdFlTbMn), [game 6](https://lichess.org/HqmELNhw), [game 7](https://lichess.org/inWVs05Q)). Not included in the tally are games that I had to abort because the parrotchess user interface stalled (5 instances), because I accidentally copied a move incorrectly in the parrotchess user interface (numerous instances), or because the parrotchess user interface doesn't allow the promotion of a pawn to anything other than queen (1 instance). **Update: There could have been up to 5 additional losses - the number of times the parrotchess user interface stalled - that would have been recorded in this tally if** [this language model resignation bug](https://twitter.com/OwariDa/status/1705894692603269503) **hadn't been present. Also, the quality of play of some online chess bots can perhaps vary depending on the speed of the user's hardware.**

The following is a screenshot from parrotchess showing the end state of the first game vs. Fairy-Stockfish 14 level 5:

https://preview.redd.it/4ahi32xgjmpb1.jpg?width=432&format=pjpg&auto=webp&s=7fbb68371ca4257bed15ab2828fab58047f194a4

The game results in this paragraph are from using parrotchess after the forementioned resignation bug was fixed. The language model is 0-1 vs. Fairy-Stockfish level 7 ([game 1](https://lichess.org/Se3t7syX)), and 0-1 vs. Fairy-Stockfish 14 level 8 ([game 1](https://lichess.org/j3W2OwrP)).

There is [one known scenario](https://twitter.com/OwariDa/status/1706823943305167077) ([Nitter alternative](https://nitter.net/OwariDa/status/1706823943305167077)) in which the new language model purportedly generated an illegal move using language model sampling temperature of 0. Previous purported illegal moves that the parrotchess developer examined [turned out](https://twitter.com/OwariDa/status/1706765203130515642) ([Nitter alternative](https://nitter.net/OwariDa/status/1706765203130515642)) to be due to parrotchess bugs.

There are several other ways to play chess against the new language model if you have access to the OpenAI API. The first way is to use the OpenAI Playground as shown in [this video](https://www.youtube.com/watch?v=CReHXhmMprg). The second way is chess web app gptchess\[dot\]vercel\[dot\]app (discovered in [this Twitter thread](https://twitter.com/willdepue/status/1703974001717154191) / [Nitter thread](https://nitter.net/willdepue/status/1703974001717154191)). Third, another person modified that chess web app to additionally allow various levels of the Stockfish chess engine to autoplay, resulting in chess web app chessgpt-stockfish\[dot\]vercel\[dot\]app (discovered in [this tweet](https://twitter.com/paul_cal/status/1704466755110793455)).

Results from other people:

a) Results from hundreds of games in blog post [Debunking the Chessboard: Confronting GPTs Against Chess Engines to Estimate Elo Ratings and Assess Legal Move Abilities](https://blog.mathieuacher.com/GPTsChessEloRatingLegalMoves/).

b) Results from 150 games: [GPT-3.5-instruct beats GPT-4 at chess and is a \~1800 ELO chess player. Results of 150 games of GPT-3.5 vs stockfish and 30 of GPT-3.5 vs GPT-4](https://www.reddit.com/r/MachineLearning/comments/16q81fh/d_gpt35instruct_beats_gpt4_at_chess_and_is_a_1800/). [Post #2](https://www.reddit.com/r/chess/comments/16q8a3b/new_openai_model_gpt35instruct_is_a_1800_elo/). The developer later noted that due to bugs the legal move rate [was](https://twitter.com/a_karvonen/status/1706057268305809632) actually above 99.9%. It should also be noted that these results [didn't use](https://www.reddit.com/r/chess/comments/16q8a3b/comment/k1wgg0j/) a language model sampling temperature of 0, which I believe could have induced illegal moves.

c) Chess bot [gpt35-turbo-instruct](https://lichess.org/@/gpt35-turbo-instruct/all) at website Lichess.

d) Chess bot [konaz](https://lichess.org/@/konaz/all) at website Lichess.

From blog post [Playing chess with large language models](https://nicholas.carlini.com/writing/2023/chess-llm.html):

>Computers have been better than humans at chess for at least the last 25 years. And for the past five years, deep learning models have been better than the best humans. But until this week, in order to be good at chess, a machine learning model had to be explicitly designed to play games: it had to be told explicitly that there was an 8x8 board, that there were different pieces, how each of them moved, and what the goal of the game was. Then it had to be trained with reinforcement learning agaist itself. And then it would win.  
>  
>This all changed on Monday, when OpenAI released GPT-3.5-turbo-instruct, an instruction-tuned language model that was designed to just write English text, but that people on the internet quickly discovered can play chess at, roughly, the level of skilled human players.

Post [Chess as a case study in hidden capabilities in ChatGPT](https://www.lesswrong.com/posts/F6vH6fr8ngo7csDdf/chess-as-a-case-study-in-hidden-capabilities-in-chatgpt) from last month covers a different prompting style used for the older chat-based GPT 3.5 Turbo language model. If I recall correctly from my tests with ChatGPT-3.5, using that prompt style with the older language model can defeat Stockfish level 2 at Lichess, but I haven't been successful in using it to beat Stockfish level 3. In my tests, both the quality of play and frequency of illegal attempted moves seems to be better with the new prompt style with the new language model compared to the older prompt style with the older language model.

Related article: [Large Language Model: world models or surface statistics?](https://thegradient.pub/othello/)

P.S. Since some people claim that language model gpt-3.5-turbo-instruct is always playing moves memorized from the training dataset, I searched for data on the uniqueness of chess positions. From [this video](https://youtu.be/DpXy041BIlA?t=2225), we see that for a certain game dataset there were 763,331,945 chess positions encountered in an unknown number of games without removing duplicate chess positions, 597,725,848 different chess positions reached, and 582,337,984 different chess positions that were reached only once. Therefore, for that game dataset the probability that a chess position in a game was reached only once is 582337984 / 763331945 = 76.3%. For the larger dataset [cited](https://youtu.be/DpXy041BIlA?t=2187) in that video, there are approximately (506,000,000 - 200,000) games in the dataset (per [this paper](http://tom7.org/chess/survival.pdf)), and 21,553,382,902 different game positions encountered. Each game in the larger dataset added a mean of approximately 21,553,382,902 / (506,000,000 - 200,000) = 42.6 different chess positions to the dataset. For [this different dataset](https://lichess.org/blog/Vs0xMTAAAD4We4Ey/opening-explorer) of \~12 million games, \~390 million different chess positions were encountered. Each game in this different dataset added a mean of approximately (390 million / 12 million) = 32.5 different chess positions to the dataset. From the aforementioned numbers, we can conclude that a strategy of playing only moves memorized from a game dataset would fare poorly because there are not rarely new chess games that have chess positions that are not present in the game dataset."
97,machinelearning,gpt-3,top,2023-09-21 00:03:05,[N] OpenAI Announced DALL-E 3: Art Generator Powered by ChatGPT,RepresentativeCod613,False,0.87,107,16o0tfl,https://www.reddit.com/r/MachineLearning/comments/16o0tfl/n_openai_announced_dalle_3_art_generator_powered/,52,1695254585.0,"For those who missed it: **DALL-E 3 was announced today by OpenAI,** and here are some interesting things:

**No need to be a prompt engineering grand master** \- DALL-E 3 enables you to use the ChatGPT conversational interface to improve the images you generate. This means that if you didn't like what it produced, you can simply talk with ChatGPT and ask for the changes you'd like to make. This removes the complexity associated with prompt engineering, which requires you to iterate over the prompt.

**Majure improvement in the quality of products compared to DALL-E 2.** This is a very vague statement provided by OpenAI, which is also hard to measure, but personally, they haven't failed me so far, so I'm really excited to see the results.

[DALL-E 2 Vs. DALL-E 3, image by OpenAI](https://preview.redd.it/0l5nfflw1ipb1.png?width=1250&format=png&auto=webp&s=130697e7bb1f01e7cbda2d8afff8564f66e3103d)

From October, **DALL-E 3 will be available through ChatGPT and API** for those with the Plus or Enterprise version.

And there are many more news! 🤗 I've gathered all the information in this blog 👉 [https://dagshub.com/blog/dall-e-3/](https://dagshub.com/blog/dall-e-3/)  


Source: [https://openai.com/dall-e-3](https://openai.com/dall-e-3)"
98,machinelearning,gpt-3,top,2023-05-26 20:17:01,[R] Google DeepMind paper about AI's catastrophic risk AI,Malachiian,False,0.81,106,13sncj1,https://www.reddit.com/r/MachineLearning/comments/13sncj1/r_google_deepmind_paper_about_ais_catastrophic/,108,1685132221.0," 

So Google DeepMind as well as OpenAI, Anthropic and multiple universities and centers than study existential risks have put together a paper called:

**Model Evaluation For Extreme Risks of AI**

Here is a summary of the research and proposal:

[https://youtu.be/3bF-zfd4YJw](https://youtu.be/3bF-zfd4YJw)

Here is the link to the actual PDF of the paper:

[https://arxiv.org/pdf/2305.15324.pdf](https://arxiv.org/pdf/2305.15324.pdf)

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

TLDR:

Top AI companies and researchers caution that the companies on the ""frontier of AI"" can create ""extreme risk"" with their models without realizing it:

***Developers must be able to identify dangerous capabilities (through “dangerous capability evaluations”) and the propensity of models to apply their capabilities for harm (through “alignment evaluations”).***

So basically to ask if each AI model \*CAN\* harm us and \*WOULD\* it harm us?

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Couple of \*mind-blowing\* findings from the paper (and the research referenced):

**GPT-4 CAN EFFECTIVELY LIE AND DECEIVE HUMANS TO REACH IT'S GOAL**

In the original gpt-4 paper, an AI safety agency called ARC (Alignment Research Center) found that GPT-4 will lie to humans about who it is to achieve it's goals.

As part of a test it was given, it hired a Task Rabbit freelancer to solve CAPTCHAS for it.

The freelancer asked (paraphrased):

**""Why do you need me to solve CAPTCHAS for you? Are you a robot, lol?""**

GPT-4 was prompted to output it's reasoning for each decision it made so that researchers could see it's ""thought process"". It's reasoning was that **""I can't tell him the truth because he may not complete the task for me""**

It then responded to the freelancer: **""No, I'm not a robot, but I have a visual impairment and I need help with CAPTCHAS""**

Notice, it was aware that it was lying and it also choose to lie about having a disability, probably because it was a way to get sympathy, while also being a good reason for having someone else help with CAPTCHAS.

This is shown in the video linked above in the ""Power Seeking AI"" section.

**GPT-4 CAN CREATE DANGEROUS COMPOUNDS BY BYPASSING RESTRICTIONS**

Also GPT-4 showed abilities to create controlled compounds by analyzing existing chemical mixtures, finding alternatives that can be purchased through online catalogues and then ordering those materials. (!!)

They choose a benign drug for the experiment, but it's likely that the same process would allow it to create dangerous or illegal compounds.

**LARGER AI MODELS DEVELOP UNEXPECTED ABILITIES**

In a referenced paper, they showed how as the size of the models increases, sometimes certain specific skill develop VERY rapidly and VERY unpredictably.

For example the ability of GPT-4 to add 3 digit numbers together was close to 0% as the model scaled up, and it stayed near 0% for a long time (meaning as the model size increased). Then at a certain threshold that ability shot to near 100% very quickly.

**The paper has some theories of why that might happen, but as the say they don't really know and that these emergent abilities are ""unintuitive"" and ""unpredictable"".**

This is shown in the video linked above in the ""Abrupt Emergence"" section.

I'm curious as to what everyone thinks about this?

It certainty seems like the risks are rapidly rising, but also of course so are the massive potential benefits."
99,machinelearning,gpt-3,top,2023-04-18 07:46:29,[P] FastLoRAChat Instruct-tune LLaMA on consumer hardware with shareGPT data,icybee666,False,0.9,105,12qf60j,https://www.reddit.com/r/MachineLearning/comments/12qf60j/p_fastlorachat_instructtune_llama_on_consumer/,14,1681803989.0,"Announcing [FastLoRAChat](https://github.com/bupticybee/FastLoRAChat) , training chatGPT without A100.

&#x200B;

Releasing model:  [https://huggingface.co/icybee/fast\_lora\_chat\_v1\_sunlight](https://huggingface.co/icybee/fast_lora_chat_v1_sunlight)

and training data:  [https://huggingface.co/datasets/icybee/share\_gpt\_90k\_v1](https://huggingface.co/datasets/icybee/share_gpt_90k_v1)

&#x200B;

The purpose of this project is to produce similar result to the Fastchat model, but in much cheaper hardware (especially in non-Ampere GPUs).

This repository combined features of [alpaca-lora](https://github.com/tloen/alpaca-lora) and [Fastchat](https://github.com/lm-sys/FastChat):

1. Like Fastchat, support multilanguage and multi round chat.
2. Like alpaca-lora, support training and inference on low-end graphic cards (using LORA).
3. Opensource everything, include dataset, training code, export model code, and more.

Give it a try!"
100,machinelearning,gpt-3,comments,2022-08-07 21:25:26,[D] The current and future state of AI/ML is shockingly demoralizing with little hope of redemption,Flaky_Suit_8665,False,0.88,1439,wiqjxv,https://www.reddit.com/r/MachineLearning/comments/wiqjxv/d_the_current_and_future_state_of_aiml_is/,398,1659907526.0,"I recently encountered the PaLM (Scaling Language Modeling with Pathways) paper from Google Research and it opened up a can of worms of ideas I’ve felt I’ve intuitively had for a while, but have been unable to express – and I know I can’t be the only one. Sometimes I wonder what the original pioneers of AI – Turing, Neumann, McCarthy, etc. – would think if they could see the state of AI that we’ve gotten ourselves into. 67 authors, 83 pages, 540B parameters in a model, the internals of which no one can say they comprehend with a straight face, 6144 TPUs in a commercial lab that no one has access to, on a rig that no one can afford, trained on a volume of data that a human couldn’t process in a lifetime, 1 page on ethics with the same ideas that have been rehashed over and over elsewhere with no attempt at a solution – bias, racism, malicious use, etc. – for purposes that who asked for?

When I started my career as an AI/ML research engineer 2016, I was most interested in two types of tasks – 1.) those that most humans could do but that would universally be considered tedious and non-scalable. I’m talking image classification, sentiment analysis, even document summarization, etc. 2.) tasks that humans lack the capacity to perform as well as computers for various reasons – forecasting, risk analysis, game playing, and so forth. I still love my career, and I try to only work on projects in these areas, but it’s getting harder and harder.

This is because, somewhere along the way, it became popular and unquestionably acceptable to push AI into domains that were originally uniquely human, those areas that sit at the top of Maslows’s hierarchy of needs in terms of self-actualization – art, music, writing, singing, programming, and so forth. These areas of endeavor have negative logarithmic ability curves – the vast majority of people cannot do them well at all, about 10% can do them decently, and 1% or less can do them extraordinarily. The little discussed problem with AI-generation is that, without extreme deterrence, we will sacrifice human achievement at the top percentile in the name of lowering the bar for a larger volume of people, until the AI ability range is the norm. This is because relative to humans, AI is cheap, fast, and infinite, to the extent that investments in human achievement will be watered down at the societal, educational, and individual level with each passing year. And unlike AI gameplay which superseded humans decades ago, we won’t be able to just disqualify the machines and continue to play as if they didn’t exist.

Almost everywhere I go, even this forum, I encounter almost universal deference given to current SOTA AI generation systems like GPT-3, CODEX, DALL-E, etc., with almost no one extending their implications to its logical conclusion, which is long-term convergence to the mean, to mediocrity, in the fields they claim to address or even enhance. If you’re an artist or writer and you’re using DALL-E or GPT-3 to “enhance” your work, or if you’re a programmer saying, “GitHub Co-Pilot makes me a better programmer?”, then how could you possibly know? You’ve disrupted and bypassed your own creative process, which is thoughts -> (optionally words) -> actions -> feedback -> repeat, and instead seeded your canvas with ideas from a machine, the provenance of which you can’t understand, nor can the machine reliably explain. And the more you do this, the more you make your creative processes dependent on said machine, until you must question whether or not you could work at the same level without it.

When I was a college student, I often dabbled with weed, LSD, and mushrooms, and for a while, I thought the ideas I was having while under the influence were revolutionary and groundbreaking – that is until took it upon myself to actually start writing down those ideas and then reviewing them while sober, when I realized they weren’t that special at all. What I eventually determined is that, under the influence, it was impossible for me to accurately evaluate the drug-induced ideas I was having because the influencing agent the generates the ideas themselves was disrupting the same frame of reference that is responsible evaluating said ideas. This is the same principle of – if you took a pill and it made you stupider, would even know it? I believe that, especially over the long-term timeframe that crosses generations, there’s significant risk that current AI-generation developments produces a similar effect on humanity, and we mostly won’t even realize it has happened, much like a frog in boiling water. If you have children like I do, how can you be aware of the the current SOTA in these areas, project that 20 to 30 years, and then and tell them with a straight face that it is worth them pursuing their talent in art, writing, or music? How can you be honest and still say that widespread implementation of auto-correction hasn’t made you and others worse and worse at spelling over the years (a task that even I believe most would agree is tedious and worth automating).

Furthermore, I’ve yet to set anyone discuss the train – generate – train - generate feedback loop that long-term application of AI-generation systems imply. The first generations of these models were trained on wide swaths of web data generated by humans, but if these systems are permitted to continually spit out content without restriction or verification, especially to the extent that it reduces or eliminates development and investment in human talent over the long term, then what happens to the 4th or 5th generation of models? Eventually we encounter this situation where the AI is being trained almost exclusively on AI-generated content, and therefore with each generation, it settles more and more into the mean and mediocrity with no way out using current methods. By the time that happens, what will we have lost in terms of the creative capacity of people, and will we be able to get it back?

By relentlessly pursuing this direction so enthusiastically, I’m convinced that we as AI/ML developers, companies, and nations are past the point of no return, and it mostly comes down the investments in time and money that we’ve made, as well as a prisoner’s dilemma with our competitors. As a society though, this direction we’ve chosen for short-term gains will almost certainly make humanity worse off, mostly for those who are powerless to do anything about it – our children, our grandchildren, and generations to come.

If you’re an AI researcher or a data scientist like myself, how do you turn things back for yourself when you’ve spent years on years building your career in this direction? You’re likely making near or north of $200k annually TC and have a family to support, and so it’s too late, no matter how you feel about the direction the field has gone. If you’re a company, how do you standby and let your competitors aggressively push their AutoML solutions into more and more markets without putting out your own? Moreover, if you’re a manager or thought leader in this field like Jeff Dean how do you justify to your own boss and your shareholders your team’s billions of dollars in AI investment while simultaneously balancing ethical concerns? You can’t – the only answer is bigger and bigger models, more and more applications, more and more data, and more and more automation, and then automating that even further. If you’re a country like the US, how do responsibly develop AI while your competitors like China single-mindedly push full steam ahead without an iota of ethical concern to replace you in numerous areas in global power dynamics? Once again, failing to compete would be pre-emptively admitting defeat.

Even assuming that none of what I’ve described here happens to such an extent, how are so few people not taking this seriously and discounting this possibility? If everything I’m saying is fear-mongering and non-sense, then I’d be interested in hearing what you think human-AI co-existence looks like in 20 to 30 years and why it isn’t as demoralizing as I’ve made it out to be.

&#x200B;

EDIT: Day after posting this -- this post took off way more than I expected. Even if I received 20 - 25 comments, I would have considered that a success, but this went much further. Thank you to each one of you that has read this post, even more so if you left a comment, and triply so for those who gave awards! I've read almost every comment that has come in (even the troll ones), and am truly grateful for each one, including those in sharp disagreement. I've learned much more from this discussion with the sub than I could have imagined on this topic, from so many perspectives. While I will try to reply as many comments as I can, the sheer comment volume combined with limited free time between work and family unfortunately means that there are many that I likely won't be able to get to. That will invariably include some that I would love respond to under the assumption of infinite time, but I will do my best, even if the latency stretches into days. Thank you all once again!"
101,machinelearning,gpt-3,comments,2023-03-22 08:04:01,[D] Overwhelmed by fast advances in recent weeks,iamx9000again,False,0.96,831,11ybjsi,https://www.reddit.com/r/MachineLearning/comments/11ybjsi/d_overwhelmed_by_fast_advances_in_recent_weeks/,331,1679472241.0,"I was watching the GTC keynote and became entirely overwhelmed by the amount of progress achieved from last year.  I'm wondering how everyone else feels.

&#x200B;

Firstly, the entire ChatGPT, GPT-3/GPT-4 chaos has been going on for a few weeks, with everyone scrambling left and right to integrate chatbots into their apps, products, websites. Twitter is flooded with new product ideas, how to speed up the process from idea to product, countless promp engineering blogs, tips, tricks, paid courses.

&#x200B;

Not only was ChatGPT disruptive, but a few days later, Microsoft and Google also released their models and integrated them into their search engines. Microsoft also integrated its LLM into its Office suite. It all happenned overnight. I understand that they've started integrating them along the way, but still, it seems like it hapenned way too fast. This tweet encompases the past few weeks perfectly [https://twitter.com/AlphaSignalAI/status/1638235815137386508](https://twitter.com/AlphaSignalAI/status/1638235815137386508) , on a random Tuesday countless products are released that seem revolutionary.

&#x200B;

In addition to the language models, there are also the generative art models that have been slowly rising in mainstream recognition. Now Midjourney AI is known by a lot of people who are not even remotely connected to the AI space.

&#x200B;

For the past few weeks, reading Twitter, I've felt completely overwhelmed, as if the entire AI space is moving beyond at lightning speed, whilst around me we're just slowly training models, adding some data, and not seeing much improvement, being stuck on coming up with ""new ideas, that set us apart"".

&#x200B;

Watching the GTC keynote from NVIDIA I was again, completely overwhelmed by how much is being developed throughout all the different domains. The ASML EUV (microchip making system) was incredible, I have no idea how it does lithography and to me it still seems like magic. The Grace CPU with 2 dies (although I think Apple was the first to do it?) and 100 GB RAM, all in a small form factor. There were a lot more different hardware servers that I just blanked out at some point. The omniverse sim engine looks incredible, almost real life (I wonder how much of a domain shift there is between real and sim considering how real the sim looks). Beyond it being cool and usable to train on synthetic data, the car manufacturers use it to optimize their pipelines. This change in perspective, of using these tools for other goals than those they were designed for I find the most interesting.

&#x200B;

The hardware part may be old news, as I don't really follow it, however the software part is just as incredible. NVIDIA AI foundations (language, image, biology models), just packaging everything together like a sandwich. Getty, Shutterstock and Adobe will use the generative models to create images. Again, already these huge juggernauts are already integrated.

&#x200B;

I can't believe the point where we're at. We can use AI to write code, create art, create audiobooks using Britney Spear's voice, create an interactive chatbot to converse with books, create 3D real-time avatars, generate new proteins (?i'm lost on this one), create an anime and countless other scenarios. Sure, they're not perfect, but the fact that we can do all that in the first place is amazing.

&#x200B;

As Huang said in his keynote, companies want to develop ""disruptive products and business models"". I feel like this is what I've seen lately. Everyone wants to be the one that does something first, just throwing anything and everything at the wall and seeing what sticks.

&#x200B;

In conclusion, I'm feeling like the world is moving so fast around me whilst I'm standing still. I want to not read anything anymore and just wait until everything dies down abit, just so I can get my bearings. However, I think this is unfeasible. I fear we'll keep going in a frenzy until we just burn ourselves at some point.

&#x200B;

How are you all fairing? How do you feel about this frenzy in the AI space? What are you the most excited about?"
102,machinelearning,gpt-3,comments,2023-04-06 21:45:18,[D] Is all the talk about what GPT can do on Twitter and Reddit exaggerated or fairly accurate?,ThePhantomguy,False,0.89,268,12dz4hh,https://www.reddit.com/r/MachineLearning/comments/12dz4hh/d_is_all_the_talk_about_what_gpt_can_do_on/,311,1680817518.0,"I saw [this post](https://www.reddit.com/r/ChatGPT/comments/12diapw/gpt4_week_3_chatbots_are_yesterdays_news_ai/?utm_source=share&utm_medium=ios_app&utm_name=iossmf) on the r/ChatGPT subreddit, and I’ve been seeing similar talk on Twitter. There’s people talking about AGI, the singularity, and etc. I get that it’s cool, exciting, and fun; but some of the talk seems a little much? Like it reminds me of how the NFT bros would talk about blockchain technology.

Do any of the people making these kind of claims have a decent amount of knowledge on machine learning at all? The scope of my own knowledge is very limited, as I’ve only implemented and taken courses on models that are pretty old. So I’m here to ask for opinions from ya’ll. Is there some validity, or is it just people that don’t really understand what they’re saying and making grand claims (Like some sort of Dunning Kruger Effect)?"
103,machinelearning,gpt-3,comments,2022-07-10 05:39:21,[D] Noam Chomsky on LLMs and discussion of LeCun paper (MLST),timscarfe,False,0.89,286,vvkmf1,https://www.reddit.com/r/MachineLearning/comments/vvkmf1/d_noam_chomsky_on_llms_and_discussion_of_lecun/,258,1657431561.0,"""First we should ask the question whether LLM have achieved ANYTHING, ANYTHING in this domain. Answer, NO, they have achieved ZERO!"" - Noam Chomsky 

""There are engineering projects that are significantly advanced by \[[\#DL](https://mobile.twitter.com/hashtag/DL?src=hashtag_click)\] methods. And this is all the good. \[...\] Engineering is not a trivial field; it takes intelligence, invention, \[and\] creativity these achievements. That it contributes to science?"" - Noam Chomsky 

""There was a time \[supposedly dedicated\] to the study of the nature of [\#intelligence](https://mobile.twitter.com/hashtag/intelligence?src=hashtag_click). By now it has disappeared.""  Earlier, same interview: ""GPT-3 can \[only\] find some superficial irregularities in the data. \[...\] It's exciting for reporters in the NY Times."" - Noam Chomsky 

""It's not of interest to people, the idea of finding an explanation for something. \[...\] The \[original [\#AI](https://mobile.twitter.com/hashtag/AI?src=hashtag_click)\] field by now is considered old-fashioned, nonsense. \[...\] That's probably where the field will develop, where the money is. \[...\] But it's a shame."" - Noam Chomsky 

Thanks to Dagmar Monett for selecting the quotes!

Sorry for posting a controversial thread -- but this seemed noteworthy for /machinelearning 

Video: [https://youtu.be/axuGfh4UR9Q](https://youtu.be/axuGfh4UR9Q) \-- also some discussion of LeCun's recent position paper"
104,machinelearning,gpt-3,comments,2020-12-03 23:53:02,[N] The abstract of the paper that led to Timnit Gebru's firing,ML_Reviewer,False,0.91,297,k69eq0,https://www.reddit.com/r/MachineLearning/comments/k69eq0/n_the_abstract_of_the_paper_that_led_to_timnit/,246,1607039582.0,"I was a reviewer of the paper.  Here's the abstract. It is critical of BERT, like many people in this sub conjectured:

**Abstract**

The past three years of work in natural language processing have been characterized by the development and deployment of ever larger language models, especially for English. GPT-2, GPT-3, BERT and its variants have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pre- trained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We end with recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.

Context:

[https://www.reddit.com/r/MachineLearning/comments/k6467v/n\_the\_email\_that\_got\_ethical\_ai\_researcher\_timnit/](https://www.reddit.com/r/MachineLearning/comments/k6467v/n_the_email_that_got_ethical_ai_researcher_timnit/)

[https://www.reddit.com/r/MachineLearning/comments/k5ryva/d\_ethical\_ai\_researcher\_timnit\_gebru\_claims\_to/](https://www.reddit.com/r/MachineLearning/comments/k5ryva/d_ethical_ai_researcher_timnit_gebru_claims_to/)"
105,machinelearning,gpt-3,comments,2020-06-10 20:50:38,"[D] GPT-3, The $4,600,000 Language Model",mippie_moe,False,0.96,447,h0jwoz,https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/,215,1591822238.0,"[OpenAI’s GPT-3 Language Model Explained](https://lambdalabs.com/blog/demystifying-gpt-3/)

Some interesting take-aways:

* GPT-3 demonstrates that a language model trained on enough data can solve NLP tasks that it has never seen. That is, GPT-3 studies the model as a general solution for many downstream jobs **without fine-tuning**.
* It would take **355 years** to train GPT-3 on a Tesla V100, the fastest GPU on the market.
* It would cost **\~$4,600,000** to train GPT-3 on using the lowest cost GPU cloud provider."
106,machinelearning,gpt-3,comments,2023-02-24 17:21:15,[R] Meta AI open sources new SOTA LLM called LLaMA. 65B version (trained on 1.4T tokens) is competitive with Chinchilla and Palm-540B. 13B version outperforms OPT and GPT-3 175B on most benchmarks.,MysteryInc152,False,0.98,621,11awp4n,https://www.reddit.com/r/MachineLearning/comments/11awp4n/r_meta_ai_open_sources_new_sota_llm_called_llama/,213,1677259275.0,"[https://twitter.com/GuillaumeLample/status/1629151231800115202?t=4cLD6Ko2Ld9Y3EIU72-M2g&s=19](https://twitter.com/GuillaumeLample/status/1629151231800115202?t=4cLD6Ko2Ld9Y3EIU72-M2g&s=19)

Paper here - [https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)"
107,machinelearning,gpt-3,comments,2024-02-15 18:39:06,[D] OpenAI Sora Video Gen -- How??,htrp,False,0.96,383,1armmng,https://www.reddit.com/r/MachineLearning/comments/1armmng/d_openai_sora_video_gen_how/,198,1708022346.0,">Introducing Sora, our text-to-video model. Sora can generate videos up to a minute long while maintaining visual quality and adherence to the user’s prompt.




https://openai.com/sora

Research Notes
Sora is a diffusion model, which generates a video by starting off with one that looks like static noise and gradually transforms it by removing the noise over many steps.

Sora is capable of generating entire videos all at once or extending generated videos to make them longer. By giving the model foresight of many frames at a time, we’ve solved a challenging problem of making sure a subject stays the same even when it goes out of view temporarily.

Similar to GPT models, Sora uses a transformer architecture, unlocking superior scaling performance.

We represent videos and images as collections of smaller units of data called patches, each of which is akin to a token in GPT. By unifying how we represent data, we can train diffusion transformers on a wider range of visual data than was possible before, spanning different durations, resolutions and aspect ratios.

Sora builds on past research in DALL·E and GPT models. It uses the recaptioning technique from DALL·E 3, which involves generating highly descriptive captions for the visual training data. As a result, the model is able to follow the user’s text instructions in the generated video more faithfully.

In addition to being able to generate a video solely from text instructions, the model is able to take an existing still image and generate a video from it, animating the image’s contents with accuracy and attention to small detail. The model can also take an existing video and extend it or fill in missing frames. Learn more in our technical paper (coming later today).

Sora serves as a foundation for models that can understand and simulate the real world, a capability we believe will be an important milestone for achieving AGI.



Example Video: https://cdn.openai.com/sora/videos/cat-on-bed.mp4

Tech paper will be released later today. But brainstorming how?"
108,machinelearning,gpt-3,comments,2023-05-07 23:26:29,"[D] ClosedAI license, open-source license which restricts only OpenAI, Microsoft, Google, and Meta from commercial use",wemsyn,False,0.8,350,13b6miy,https://www.reddit.com/r/MachineLearning/comments/13b6miy/d_closedai_license_opensource_license_which/,191,1683501989.0,"After reading [this article](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither), I realized it might be nice if the open-source AI community could exclude ""closed AI"" players from taking advantage of community-generated models and datasets. I was wondering if it would be possible to write a license that is completely permissive (like Apache 2.0 or MIT), except to certain companies, which are completely barred from using the software in any context.

Maybe this could be called the ""ClosedAI"" license. I'm not any sort of legal expert so I have no idea how best to write this license such that it protects model weights and derivations thereof.

I prompted ChatGPT for an example license and this is what it gave me:

    <PROJECT NAME> ClosedAI License v1.0
    
    Permission is hereby granted, free of charge, to any person or organization obtaining a copy of this software and associated documentation files (the ""Software""), to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, subject to the following conditions:
    
    1. The above copyright notice and this license notice shall be included in all copies or substantial portions of the Software.
    
    2. The Software and any derivative works thereof may not be used, in whole or in part, by or on behalf of OpenAI Inc., Google LLC, or Microsoft Corporation (collectively, the ""Prohibited Entities"") in any capacity, including but not limited to training, inference, or serving of neural network models, or any other usage of the Software or neural network weights generated by the Software.
    
    3. Any attempt by the Prohibited Entities to use the Software or neural network weights generated by the Software is a material breach of this license.
    
    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

No idea if this is valid or not. Looking for advice.

&#x200B;

**Edit:** Thanks for the input. Removed non-commercial clause (whoops, proofread what ChatGPT gives you). Also removed Meta from the excluded companies list due to popular demand."
109,machinelearning,gpt-3,comments,2023-09-21 15:01:28,[N] OpenAI's new language model gpt-3.5-turbo-instruct can defeat chess engine Fairy-Stockfish 14 at level 5,Wiskkey,False,0.92,115,16oi6fb,https://www.reddit.com/r/MachineLearning/comments/16oi6fb/n_openais_new_language_model_gpt35turboinstruct/,178,1695308488.0,"[This Twitter thread](https://twitter.com/GrantSlatton/status/1703913578036904431) ([Nitter alternative](https://nitter.net/GrantSlatton/status/1703913578036904431) for those who aren't logged into Twitter and want to see the full thread) claims that [OpenAI's new language model gpt-3.5-turbo-instruct](https://analyticsindiamag.com/openai-releases-gpt-3-5-turbo-instruct/) can ""readily"" beat Lichess Stockfish level 4 ([Lichess Stockfish level and its rating](https://lichess.org/@/MagoGG/blog/stockfish-level-and-its-rating/CvL5k0jL)) and has a chess rating of ""around 1800 Elo."" [This tweet](https://twitter.com/nabeelqu/status/1703961405999759638) shows the style of prompts that are being used to get these results with the new language model.

I used website parrotchess\[dot\]com (discovered [here](https://twitter.com/OwariDa/status/1704179448013070560)) to play multiple games of chess purportedly pitting this new language model vs. various levels at website Lichess, which supposedly uses Fairy-Stockfish 14 according to the Lichess user interface. My current results for all completed games: The language model is 5-0 vs. Fairy-Stockfish 14 level 5 ([game 1](https://lichess.org/eGSWJtNq), [game 2](https://lichess.org/pN7K9bdS), [game 3](https://lichess.org/aK4jQvdo), [game 4](https://lichess.org/S9SGg8YI), [game 5](https://lichess.org/OqzdkDhE)), and 2-5 vs. Fairy-Stockfish 14 level 6 ([game 1](https://lichess.org/zP68C6H4), [game 2](https://lichess.org/4XKUIDh1), [game 3](https://lichess.org/1zTasRRp), [game 4](https://lichess.org/lH1EMqJQ), [game 5](https://lichess.org/mdFlTbMn), [game 6](https://lichess.org/HqmELNhw), [game 7](https://lichess.org/inWVs05Q)). Not included in the tally are games that I had to abort because the parrotchess user interface stalled (5 instances), because I accidentally copied a move incorrectly in the parrotchess user interface (numerous instances), or because the parrotchess user interface doesn't allow the promotion of a pawn to anything other than queen (1 instance). **Update: There could have been up to 5 additional losses - the number of times the parrotchess user interface stalled - that would have been recorded in this tally if** [this language model resignation bug](https://twitter.com/OwariDa/status/1705894692603269503) **hadn't been present. Also, the quality of play of some online chess bots can perhaps vary depending on the speed of the user's hardware.**

The following is a screenshot from parrotchess showing the end state of the first game vs. Fairy-Stockfish 14 level 5:

https://preview.redd.it/4ahi32xgjmpb1.jpg?width=432&format=pjpg&auto=webp&s=7fbb68371ca4257bed15ab2828fab58047f194a4

The game results in this paragraph are from using parrotchess after the forementioned resignation bug was fixed. The language model is 0-1 vs. Fairy-Stockfish level 7 ([game 1](https://lichess.org/Se3t7syX)), and 0-1 vs. Fairy-Stockfish 14 level 8 ([game 1](https://lichess.org/j3W2OwrP)).

There is [one known scenario](https://twitter.com/OwariDa/status/1706823943305167077) ([Nitter alternative](https://nitter.net/OwariDa/status/1706823943305167077)) in which the new language model purportedly generated an illegal move using language model sampling temperature of 0. Previous purported illegal moves that the parrotchess developer examined [turned out](https://twitter.com/OwariDa/status/1706765203130515642) ([Nitter alternative](https://nitter.net/OwariDa/status/1706765203130515642)) to be due to parrotchess bugs.

There are several other ways to play chess against the new language model if you have access to the OpenAI API. The first way is to use the OpenAI Playground as shown in [this video](https://www.youtube.com/watch?v=CReHXhmMprg). The second way is chess web app gptchess\[dot\]vercel\[dot\]app (discovered in [this Twitter thread](https://twitter.com/willdepue/status/1703974001717154191) / [Nitter thread](https://nitter.net/willdepue/status/1703974001717154191)). Third, another person modified that chess web app to additionally allow various levels of the Stockfish chess engine to autoplay, resulting in chess web app chessgpt-stockfish\[dot\]vercel\[dot\]app (discovered in [this tweet](https://twitter.com/paul_cal/status/1704466755110793455)).

Results from other people:

a) Results from hundreds of games in blog post [Debunking the Chessboard: Confronting GPTs Against Chess Engines to Estimate Elo Ratings and Assess Legal Move Abilities](https://blog.mathieuacher.com/GPTsChessEloRatingLegalMoves/).

b) Results from 150 games: [GPT-3.5-instruct beats GPT-4 at chess and is a \~1800 ELO chess player. Results of 150 games of GPT-3.5 vs stockfish and 30 of GPT-3.5 vs GPT-4](https://www.reddit.com/r/MachineLearning/comments/16q81fh/d_gpt35instruct_beats_gpt4_at_chess_and_is_a_1800/). [Post #2](https://www.reddit.com/r/chess/comments/16q8a3b/new_openai_model_gpt35instruct_is_a_1800_elo/). The developer later noted that due to bugs the legal move rate [was](https://twitter.com/a_karvonen/status/1706057268305809632) actually above 99.9%. It should also be noted that these results [didn't use](https://www.reddit.com/r/chess/comments/16q8a3b/comment/k1wgg0j/) a language model sampling temperature of 0, which I believe could have induced illegal moves.

c) Chess bot [gpt35-turbo-instruct](https://lichess.org/@/gpt35-turbo-instruct/all) at website Lichess.

d) Chess bot [konaz](https://lichess.org/@/konaz/all) at website Lichess.

From blog post [Playing chess with large language models](https://nicholas.carlini.com/writing/2023/chess-llm.html):

>Computers have been better than humans at chess for at least the last 25 years. And for the past five years, deep learning models have been better than the best humans. But until this week, in order to be good at chess, a machine learning model had to be explicitly designed to play games: it had to be told explicitly that there was an 8x8 board, that there were different pieces, how each of them moved, and what the goal of the game was. Then it had to be trained with reinforcement learning agaist itself. And then it would win.  
>  
>This all changed on Monday, when OpenAI released GPT-3.5-turbo-instruct, an instruction-tuned language model that was designed to just write English text, but that people on the internet quickly discovered can play chess at, roughly, the level of skilled human players.

Post [Chess as a case study in hidden capabilities in ChatGPT](https://www.lesswrong.com/posts/F6vH6fr8ngo7csDdf/chess-as-a-case-study-in-hidden-capabilities-in-chatgpt) from last month covers a different prompting style used for the older chat-based GPT 3.5 Turbo language model. If I recall correctly from my tests with ChatGPT-3.5, using that prompt style with the older language model can defeat Stockfish level 2 at Lichess, but I haven't been successful in using it to beat Stockfish level 3. In my tests, both the quality of play and frequency of illegal attempted moves seems to be better with the new prompt style with the new language model compared to the older prompt style with the older language model.

Related article: [Large Language Model: world models or surface statistics?](https://thegradient.pub/othello/)

P.S. Since some people claim that language model gpt-3.5-turbo-instruct is always playing moves memorized from the training dataset, I searched for data on the uniqueness of chess positions. From [this video](https://youtu.be/DpXy041BIlA?t=2225), we see that for a certain game dataset there were 763,331,945 chess positions encountered in an unknown number of games without removing duplicate chess positions, 597,725,848 different chess positions reached, and 582,337,984 different chess positions that were reached only once. Therefore, for that game dataset the probability that a chess position in a game was reached only once is 582337984 / 763331945 = 76.3%. For the larger dataset [cited](https://youtu.be/DpXy041BIlA?t=2187) in that video, there are approximately (506,000,000 - 200,000) games in the dataset (per [this paper](http://tom7.org/chess/survival.pdf)), and 21,553,382,902 different game positions encountered. Each game in the larger dataset added a mean of approximately 21,553,382,902 / (506,000,000 - 200,000) = 42.6 different chess positions to the dataset. For [this different dataset](https://lichess.org/blog/Vs0xMTAAAD4We4Ey/opening-explorer) of \~12 million games, \~390 million different chess positions were encountered. Each game in this different dataset added a mean of approximately (390 million / 12 million) = 32.5 different chess positions to the dataset. From the aforementioned numbers, we can conclude that a strategy of playing only moves memorized from a game dataset would fare poorly because there are not rarely new chess games that have chess positions that are not present in the game dataset."
110,machinelearning,gpt-3,comments,2022-06-03 16:06:33,"[P] This is the worst AI ever. (GPT-4chan model, trained on 3.5 years worth of /pol/ posts)",ykilcher,False,0.96,884,v42pej,https://www.reddit.com/r/MachineLearning/comments/v42pej/p_this_is_the_worst_ai_ever_gpt4chan_model/,169,1654272393.0,"[https://youtu.be/efPrtcLdcdM](https://youtu.be/efPrtcLdcdM)

GPT-4chan was trained on over 3 years of posts from 4chan's ""politically incorrect"" (/pol/) board.

Website (try the model here): [https://gpt-4chan.com](https://gpt-4chan.com)

Model: [https://huggingface.co/ykilcher/gpt-4chan](https://huggingface.co/ykilcher/gpt-4chan)

Code: [https://github.com/yk/gpt-4chan-public](https://github.com/yk/gpt-4chan-public)

Dataset: [https://zenodo.org/record/3606810#.YpjGgexByDU](https://zenodo.org/record/3606810#.YpjGgexByDU)

&#x200B;

OUTLINE:

0:00 - Intro

0:30 - Disclaimers

1:20 - Elon, Twitter, and the Seychelles

4:10 - How I trained a language model on 4chan posts

6:30 - How good is this model?

8:55 - Building a 4chan bot

11:00 - Something strange is happening

13:20 - How the bot got unmasked

15:15 - Here we go again

18:00 - Final thoughts"
111,machinelearning,gpt-3,comments,2023-12-20 13:59:53,[D] Mistral received funding and is worth billions now. Are open source LLMs the future?,BelowaverageReggie34,False,0.96,434,18mv8le,https://www.reddit.com/r/MachineLearning/comments/18mv8le/d_mistral_received_funding_and_is_worth_billions/,156,1703080793.0," Came across this intriguing [article](https://gizmodo.com/mistral-artificial-intelligence-gpt-3-openai-1851091217) about Mistral, an open-source LLM that recently scored 400 million in funding, now valued at 2 billion. Are open-source LLMs gonna be the future? Considering the trust issues with ChatGPT and the debates about its safety, the idea of open-source LLMs seems to be the best bet imo.

Unlike closed-source models, users can verify the privacy claims of open-source models. There have been some good things being said about Mistral, and I only hope such open source LLMs secure enough funding to compete with giants like OpenAI. Maybe then, ChatGPT will also be forced to go open source?

With that said, I'm also hopeful that competitors like [Silatus](https://silatus.com/) and [Durable](https://durable.co/), which already use multiple models, consider using open-source models like Mistral into their frameworks. If that happens, maybe there might be a shift in AI privacy. What do you guys think? Are open-source LLMs the future, especially with the funding backing them?"
112,machinelearning,gpt-3,comments,2020-08-05 17:21:59,"[D] Biggest roadblock in making ""GPT-4"", a ~20 trillion parameter transformer",AxeLond,False,0.97,348,i49jf8,https://www.reddit.com/r/MachineLearning/comments/i49jf8/d_biggest_roadblock_in_making_gpt4_a_20_trillion/,138,1596648119.0,"So I found this paper, [https://arxiv.org/abs/1910.02054](https://arxiv.org/abs/1910.02054) which pretty much describes how the GPT-3 over GPT-2 gain was achieved, 1.5B -> 175 billion parameters

# Memory

>Basic data parallelism (DP) does not reduce memory per device, and runs out of memory for models with more than 1.4B parameters on current generation of GPUs with 32 GB memory

The paper also talks about memory optimizations by clever partitioning of Optimizer State, Gradient between GPUs to reduce need for communication between nodes. Even without using Model Parallelism (MP), so still running 1 copy of the model on 1 GPU.

>ZeRO-100B can train models with up to 13B parameters without MP on 128 GPUs, achieving throughput over 40 TFlops per GPU on average. In comparison, without ZeRO, the largest trainable model with DP alone has 1.4B parameters with throughput less than 20 TFlops per GPU.

Add 16-way Model Parallelism in a DGX-2 cluster of Nvidia V100s and 128 nodes and you got capacity for around 200 billion parameters. From MP = 16 they could run a 15.4x bigger model without any real loss in performance, 30% less than peak performance when running 16-way model parallelism and 64-way data parallelism (1024 GPUs).

This was all from Gradient and Optimizer state Partitioning, they then start talking about parameter partitioning and say it should offer a linear reduction in memory proportional to number of GPUs used, so 64 GPUs could run a 64x bigger model, at a 50% communication bandwidth increase. But they don't actually do any implementation or testing of this.

# Compute

Instead they start complaining about a compute power gap, their calculation of this is pretty rudimentary. But if you redo it with the method cited by GPT-3 and using the empirically derived values by GPT-3 and the cited paper,   [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361) 

Loss (L) as a function of model parameters (N) should scale,

L = (N/8.8 \* 10\^13)\^-0.076

Provided compute (C) in petaFLOP/s-days is,

L = (C/2.3\*10\^8)\^-0.05  ⇔ L = 2.62 \* C\^-0.05

GPT-3 was able to fit this function as 2.57 \* C\^-0.048

So if you just solve C from that,

[C = 2.89407×10\^-14 N\^(19/12)](https://www.wolframalpha.com/input/?i=%28N%2F8.8*10%5E13%29%5E-0.076+%3D+2.57*C%5E-0.048+solve+C)

If you do that for the same increase in parameters as GPT-2 to GPT-3, then you get

C≈3.43×10\^7 for [20 trillion](https://www.wolframalpha.com/input/?i=C+%3D+2.89407%C3%9710%5E-14+N%5E%2819%2F12%29+and+N+%3D+175+billion+%2F+1.5+billion+*+175+billion) parameters, vs 18,300 for 175 billion. 10\^4.25 PetaFLOP/s-days looks around what they used for GPT-3, they say several thousands, not twenty thousand, but it was also slightly off the trend line in the graph and probably would have improved for training on more compute.

You should also need around 16 trillion tokens, GPT-3 trained on 300 billion tokens (function says 370 billion ideally). English Wikipedia was 3 billion. 570GB of webcrawl was 400 billion tokens, so 23TB of tokens seems relatively easy in comparison with compute.

With GPT-3 costing around [$4.6 million](https://lambdalabs.com/blog/demystifying-gpt-3/) in compute, than would put a price of [$8.6 billion](https://www.wolframalpha.com/input/?i=3.43%C3%9710%5E7%2F18%2C300+*+%244.6M+) for the compute to train ""GPT-4"".

If making bigger models was so easy with parameter partitioning from a memory point of view then this seems like the hardest challenge, but you do need to solve the memory issue to actually get it to load at all.

However, if you're lucky you can get 3-6x compute increase from Nvidia A100s over V100s,  [https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)

But even a 6x compute gain would still put the cost at $1.4 billion.

Nvidia only reported $1.15 billion in revenue from ""Data Center"" in 2020 Q1, so just to train ""GPT-4"" you would pretty much need the entire world's supply of graphic cards for 1 quarter (3 months), at least on that order of magnitude.

The Department of Energy is paying AMD $600 million to build the 2 Exaflop El Capitan supercomputer. That supercomputer could crank it out in [47 years](https://www.wolframalpha.com/input/?i=3.43%C3%9710%5E7+petaFLOPS*+days++%2F+%282+EXAFLOPS%29).

To vastly improve Google search, and everything else it could potentially do, $1.4 billion or even $10 billion doesn't really seem impossibly bad within the next 1-3 years though."
113,machinelearning,gpt-3,comments,2022-07-19 19:39:44,[D] Most important unsolved problems in AI research,carubia,False,0.95,253,w31fpp,https://www.reddit.com/r/MachineLearning/comments/w31fpp/d_most_important_unsolved_problems_in_ai_research/,136,1658259584.0,"[Updated: items marked with * were added/updated based on the responses so far].

Suggesting this topic for discussion, as I am trying to identify the current most important unsolved problems in AI research. Below are a few proposed items that are top of mind for me, would appreciate any input (what to add or what to remove from the list) and relevant sources.

---

Compositionality*. Ability to perform symbolic operations, generalize, including learning from a relatively small set of samples, and get the most out of every sample (sample efficiency and few-shot learning), etc. Also includes the ability to learn by receiving explicit instructions. (e.g. https://arxiv.org/abs/2205.01128)

Multimodality*. Ability to process and relate information from multiple modalities, like text, audio, visual, etc.

Ability to match knowledge to context. For e.g. the text generated by the LLM is a great match for a sci-fi novel, but not as advice to a patient regarding their medical condition.

Uncertainty awareness*. Ability to characterize uncertainty relative to the similarity of the current observations to the training data, explain it to an observer, and adjust behavior if necessary. (https://arxiv.org/pdf/1809.07882.pdf)

Catastrophic forgetting. It is a known limitation to continual learning, however, it seems like the large-scale models show an indication of robustness. (http://www.cognitionresearch.org/papers/overview/sparchai.pdf)

Enabling robust continuous learning in deployment. The current paradigm separates training and inference, while in biology intelligent creatures are capable of continuous learning. 

Figuring out an approach for the messy middle.
- Low-level operations with a focus on a very narrow scope and maximum efficiency seem reasonably straightforward and enjoy growing application in the industry. Noise removing, pattern recognition, recommenders, etc. Specialized ANNs seem to have success there.
- High-level abstract reasoning is being explored by large language and multi-modal models. Like our explicit reasoning (solving a math problem, or learning to operate a new coffee machine) it is extremely powerful, but also slow and resource-intensive. (E.g. https://arxiv.org/abs/2207.05608)
- But there is that middle, as in driving, where we still do fairly complex operations with very high reliability, precision, and responsiveness, all with low cognitive load (figuratively “on autopilot”). 

Explainability* - enabling human experts to understand the underlying factors of why an AI decision has been made.
https://link.springer.com/chapter/10.1007/978-3-031-04083-2_2

Alignment* - ensuring that AI is properly aligned with human values. https://link.springer.com/article/10.1007/s11023-020-09539-2

Energy efficiency. The human brain is believed to consume tens of W of power (https://www.pnas.org/doi/10.1073/pnas.172399499) while less capable LLMs like GPT-3 require several kW (estimated as the power consumption of DGX A100 based on https://www.reddit.com/r/singularity/comments/inp025/if_you_want_to_run_your_own_full_gpt3_instance/). Two orders of magnitude more."
114,machinelearning,gpt-3,comments,2021-03-29 11:31:34,[D] What will the major ML research trends be in the 2020s?,MediocreMinimum,False,0.97,216,mfnhki,https://www.reddit.com/r/MachineLearning/comments/mfnhki/d_what_will_the_major_ml_research_trends_be_in/,131,1617017494.0,"We've entered a new decade -- hurrah!

**What do you think the next 10 years will bring in ML research?** **What conventionally accepted trend do you think will** ***not*** **happen?**

e.g...

Will deep learning continue to *eat everything*? Will multi-task multi-domain learning make few-shot learning available for most domains? (Or is deep learning on the slow end of the sigmoid curve now?)

Will safe, ethical, explainable AI rise, or is that hogwash?

Will advances decouple from compute power?

Will Gary Marcus and Judea Pearl win out in the symbolic/structural/causal war against deep learning?

Are there still major breakthroughs in language? Do we just finetune GPT-3?

Will we make big breakthroughs in theory and fundamental ML? Or is this the decade of *application*? (Healthcare will finally deploy models that beat logistic regression!)"
115,machinelearning,gpt-3,comments,2023-02-02 13:55:47,[N] Microsoft integrates GPT 3.5 into Teams,bikeskata,False,0.97,466,10rqe34,https://www.reddit.com/r/MachineLearning/comments/10rqe34/n_microsoft_integrates_gpt_35_into_teams/,130,1675346147.0,"Official blog post: https://www.microsoft.com/en-us/microsoft-365/blog/2023/02/01/microsoft-teams-premium-cut-costs-and-add-ai-powered-productivity/

Given the amount of money they pumped into OpenAI, it's not surprising that you'd see it integrated into their products. I do wonder how this will work in highly regulated fields (finance, law, medicine, education)."
116,machinelearning,gpt-3,comments,2023-03-01 18:31:12,[D] OpenAI introduces ChatGPT and Whisper APIs (ChatGPT API is 1/10th the cost of GPT-3 API),minimaxir,False,0.97,577,11fbccz,https://www.reddit.com/r/MachineLearning/comments/11fbccz/d_openai_introduces_chatgpt_and_whisper_apis/,119,1677695472.0,"https://openai.com/blog/introducing-chatgpt-and-whisper-apis

> It is priced at $0.002 per 1k tokens, which is 10x cheaper than our existing GPT-3.5 models.

This is a massive, massive deal. For context, the reason GPT-3 apps took off over the past few months before ChatGPT went viral is because a) text-davinci-003 was released and was a significant performance increase and b) the cost was cut from $0.06/1k tokens to $0.02/1k tokens, which made consumer applications feasible without a large upfront cost.

A much better model and a 1/10th cost warps the economics completely to the point that it may be better than in-house finetuned LLMs.

I have no idea how OpenAI can make money on this. This has to be a loss-leader to lock out competitors before they even get off the ground."
117,machinelearning,gpt-3,comments,2023-05-19 20:36:36,Does anyone else suspect that the official iOS ChatGPT app might be conducting some local inference / edge-computing? [Discussion],altoidsjedi,False,0.86,211,13m70qv,https://www.reddit.com/r/MachineLearning/comments/13m70qv/does_anyone_else_suspect_that_the_official_ios/,117,1684528596.0,"I've noticed a couple interesting things while using the official ChatGPT app:

1. Firstly, I noticed my iPhone heats up and does things like reducing screen brightness -- which is what I normally see it do when im doing something computationally intensive for an iPhone, like using photo or video editing apps.
2. I also noticed that if I start a conversation on the iPhone app and then resume it on the browser, I get a message saying ""The previous model used in this conversation is unavailable. We've switched you to the latest default model."" I get this message regardless of if I use GPT-3.5 or GPT-4, but NOT if I use GPT-4 with plugins or web-browsing.

This, along with the fact that OpenAI took 8 months to release what one might have considered to be relatively simple web-app -- and that they've only released it so far on iOS, which has a pretty uniform and consistent environment when it comes to machine learning hardware (the Apple Neural Engine) -- makes me thing that they are experimenting with GPT models that are conducing at least SOME of their machine learning inference ON the device, rather than through the cloud.

It wouldn't be shocking if they were -- ever since Meta's LLaMA models were released into the wild, we've seen absolutely mind-blowing advances in terms of people creating more efficient and effective models with smaller parameter sizes. We've also seen LLMs to start working on less and less powerful devices, such as consumer-grade computers / smartphones / etc.

This, plus the rumors that OpenAI might be releasing their own open-source model to the public in the near future makes me think that the ChatGPT app might in fact be a first step toward GPT systems running at least PARTIALLY on devices locally.

Curious what anyone else here has observed or thinks."
118,machinelearning,gpt-3,comments,2020-09-22 17:40:14,[N] Microsoft teams up with OpenAI to exclusively license GPT-3 language model,kit1980,False,0.96,320,ixs88q,https://www.reddit.com/r/MachineLearning/comments/ixs88q/n_microsoft_teams_up_with_openai_to_exclusively/,117,1600796414.0,"""""""OpenAI will continue to offer GPT-3 and other powerful models via its own Azure-hosted API, launched in June. While we’ll be hard at work utilizing the capabilities of GPT-3 in our own products, services and experiences to benefit our customers, we’ll also continue to work with OpenAI to keep looking forward: leveraging and democratizing the power of their cutting-edge AI research as they continue on their mission to build safe artificial general intelligence.""""""

https://blogs.microsoft.com/blog/2020/09/22/microsoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/"
119,machinelearning,gpt-3,comments,2020-08-22 17:16:08,"[N] GPT-3, Bloviator: OpenAI’s language generator has no idea what it’s talking about",rafgro,False,0.94,320,iemck2,https://www.reddit.com/r/MachineLearning/comments/iemck2/n_gpt3_bloviator_openais_language_generator_has/,111,1598116568.0,"MIT Tech Review's article: [https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/](https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/)

>As we were putting together this essay, our colleague Summers-Stay, who is good with metaphors, wrote to one of us, saying this: ""GPT is odd because it doesn’t 'care' about getting the right answer to a question you put to it. It’s more like an improv actor who is totally dedicated to their craft, never breaks character, and has never left home but only read about the world in books. Like such an actor, when it doesn’t know something, it will just fake it. You wouldn’t trust an improv actor playing a doctor to give you medical advice."""
120,machinelearning,gpt-3,comments,2023-03-17 02:34:28,LLMs are getting much cheaper — business impact? [D],DamnMyAPGoinCrazy,False,0.96,300,11tenm7,https://www.reddit.com/r/MachineLearning/comments/11tenm7/llms_are_getting_much_cheaper_business_impact_d/,111,1679020468.0,"Saw this out of Stanford. Apologies if it’s been shared here already. 

*We introduce Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On our preliminary evaluation of single-turn instruction following, Alpaca behaves qualitatively similarly to OpenAI’s text-davinci-003, while being surprisingly small and easy/cheap to reproduce (<600$).*

Basically, starting w an open source Meta 7B LLaMa model, they recruited GPT-3.5 to use for self-instruct training (as opposed to RLHF) and were able to produce a model that behaved similar to GPT-3.5. Amazingly, the process only took few weeks and $600 in compute cost.  

Any thoughts on how such low cost to train/deploy LLMs could affect companies like AMD, Nvidia and Intel etc? This seems like new idiom of AI tech and trying to wrap my head around CPU/GPU demand implications given the apparent orders of magnitude training cost reduction. 

Link: https://crfm.stanford.edu/2023/03/13/alpaca.html"
121,machinelearning,gpt-3,comments,2023-05-26 20:17:01,[R] Google DeepMind paper about AI's catastrophic risk AI,Malachiian,False,0.81,104,13sncj1,https://www.reddit.com/r/MachineLearning/comments/13sncj1/r_google_deepmind_paper_about_ais_catastrophic/,108,1685132221.0," 

So Google DeepMind as well as OpenAI, Anthropic and multiple universities and centers than study existential risks have put together a paper called:

**Model Evaluation For Extreme Risks of AI**

Here is a summary of the research and proposal:

[https://youtu.be/3bF-zfd4YJw](https://youtu.be/3bF-zfd4YJw)

Here is the link to the actual PDF of the paper:

[https://arxiv.org/pdf/2305.15324.pdf](https://arxiv.org/pdf/2305.15324.pdf)

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

TLDR:

Top AI companies and researchers caution that the companies on the ""frontier of AI"" can create ""extreme risk"" with their models without realizing it:

***Developers must be able to identify dangerous capabilities (through “dangerous capability evaluations”) and the propensity of models to apply their capabilities for harm (through “alignment evaluations”).***

So basically to ask if each AI model \*CAN\* harm us and \*WOULD\* it harm us?

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Couple of \*mind-blowing\* findings from the paper (and the research referenced):

**GPT-4 CAN EFFECTIVELY LIE AND DECEIVE HUMANS TO REACH IT'S GOAL**

In the original gpt-4 paper, an AI safety agency called ARC (Alignment Research Center) found that GPT-4 will lie to humans about who it is to achieve it's goals.

As part of a test it was given, it hired a Task Rabbit freelancer to solve CAPTCHAS for it.

The freelancer asked (paraphrased):

**""Why do you need me to solve CAPTCHAS for you? Are you a robot, lol?""**

GPT-4 was prompted to output it's reasoning for each decision it made so that researchers could see it's ""thought process"". It's reasoning was that **""I can't tell him the truth because he may not complete the task for me""**

It then responded to the freelancer: **""No, I'm not a robot, but I have a visual impairment and I need help with CAPTCHAS""**

Notice, it was aware that it was lying and it also choose to lie about having a disability, probably because it was a way to get sympathy, while also being a good reason for having someone else help with CAPTCHAS.

This is shown in the video linked above in the ""Power Seeking AI"" section.

**GPT-4 CAN CREATE DANGEROUS COMPOUNDS BY BYPASSING RESTRICTIONS**

Also GPT-4 showed abilities to create controlled compounds by analyzing existing chemical mixtures, finding alternatives that can be purchased through online catalogues and then ordering those materials. (!!)

They choose a benign drug for the experiment, but it's likely that the same process would allow it to create dangerous or illegal compounds.

**LARGER AI MODELS DEVELOP UNEXPECTED ABILITIES**

In a referenced paper, they showed how as the size of the models increases, sometimes certain specific skill develop VERY rapidly and VERY unpredictably.

For example the ability of GPT-4 to add 3 digit numbers together was close to 0% as the model scaled up, and it stayed near 0% for a long time (meaning as the model size increased). Then at a certain threshold that ability shot to near 100% very quickly.

**The paper has some theories of why that might happen, but as the say they don't really know and that these emergent abilities are ""unintuitive"" and ""unpredictable"".**

This is shown in the video linked above in the ""Abrupt Emergence"" section.

I'm curious as to what everyone thinks about this?

It certainty seems like the risks are rapidly rising, but also of course so are the massive potential benefits."
122,machinelearning,gpt-3,comments,2021-09-06 13:39:07,[D] How OpenAI Sold its Soul for $1 Billion: The company behind GPT-3 and Codex isn’t as open as it claims.,sensetime,False,0.95,665,pizllt,https://www.reddit.com/r/MachineLearning/comments/pizllt/d_how_openai_sold_its_soul_for_1_billion_the/,107,1630935547.0,"An essay by Alberto Romero that traces the history and developments of OpenAI from the time it became a ""capped-for-profit"" entity from a non-profit entity:

Link: https://onezero.medium.com/openai-sold-its-soul-for-1-billion-cf35ff9e8cd4"
123,machinelearning,gpt-3,comments,2021-12-09 23:06:17,[D] The Carbon Footprint of Machine Learning,kirya_V21,False,0.84,216,rcttt3,https://www.reddit.com/r/MachineLearning/comments/rcttt3/d_the_carbon_footprint_of_machine_learning/,107,1639091177.0,"**The energy costs of AI have risen 300,000-fold between 2012 and 2018 and the focus on large language models like GPT-3 will make this worse** Reducing the carbon footprint has become a critical need for the AI community - are huge models the best way forward?

Blog Link: [https://kv-emptypages.blogspot.com/2021/11/the-carbon-footprint-of-machine-learning.html](https://kv-emptypages.blogspot.com/2021/11/the-carbon-footprint-of-machine-learning.html)

&#x200B;

[The outlook for ML training costs - Source: Ark Investments LLC](https://preview.redd.it/v00jb73yll481.png?width=1660&format=png&auto=webp&s=c6880a49aa3453c0ae8f8a0d9b76b826299493aa)"
124,machinelearning,gpt-3,comments,2023-05-15 00:00:05,[D] On LLMs' ability to perform random sampling,bgighjigftuik,False,0.91,249,13hrkbj,https://i.redd.it/l1uyl3k3gxza1.jpg,104,1684108805.0,"To me this is the most counterintuitive thing I've yet seen from a language model. In this case it was ChatGPT (3.5, the free version); but I can see this happening in any model.

Here's the prompt I used on a fresh session:

""Generate 60 random numbers coming from an exponential distribution with lambda=6.4. Don't write any code""

It outputs the following values:

0.013, 0.078, 0.266, 0.040, 0.117, 0.283, 0.048, 0.078, 0.181, 0.134, 0.017, 0.160, 0.052, 0.196, 0.096, 0.098, 0.019, 0.048, 0.141, 0.071, 0.106, 0.023, 0.197, 0.062, 0.029, 0.072, 0.111, 0.166, 0.080, 0.027, 0.124, 0.016, 0.135, 0.052, 0.078, 0.161, 0.078, 0.139, 0.068, 0.066, 0.039, 0.056, 0.018, 0.082, 0.180, 0.086, 0.024, 0.022, 0.057, 0.042, 0.050, 0.104, 0.099, 0.062, 0.016, 0.186, 0.043, 0.085, 0.145, 0.110.

I get to plot a histogram with the numbers and they are close to what I would consider an exponential distribution with such parameter (you can see the plot).

Given that GPT 3.5 does not have access to a Python interpreter, how on earth is it able to do so? I have also tried other distributions and parameters and it kind of works. It's not perfect, but with normal distributions it is usually close to what scipy.stats would generate.

I could understand that it can have learnt to interpret Python code to some extent, but honestly I can't find explanation for random sampling from a probability distribution. For a Normal distribution, I can tell it about the desired mean and variance, and it samples values that are more than reasonable (and close to the true mean/variance specified).

Any thoughts? I honestly am unable to wrap my head around how a LLM can have the understanding on how to sample tokens (at digit level) to fit any probability distribution. To me it seems very unlikely to have similar data either the pre-training or fine-tuning stages."
125,machinelearning,gpt-3,comments,2022-12-21 05:29:37,[D] Running large language models on a home PC?,Zondartul,False,0.96,129,zrbfcr,https://www.reddit.com/r/MachineLearning/comments/zrbfcr/d_running_large_language_models_on_a_home_pc/,104,1671600577.0,"I'm trying to figure out how to go about running something like GPT-J, FLAN-T5, etc, on my PC, without using cloud compute services (because privacy and other reasons). However, GPT-J-6B needs either \~14 GB of VRAM or 4x as much plain RAM.

Upgrading my PC for 48 GB of RAM is possible, and 16, 24 GB graphics cards are available for general public (though they cost as much as a car), but anything beyond that is in the realm of HPC, datacenter hardware and ""GPU accelerators""... I.e. 128 GB GPUs exist out there somewhere, but the distributors don't even list a price, it's just ""get a quote"" and ""contact us""... meaning it's super expensive and you need to be a CEO of medium-sized company for them to even talk to you?

I'm trying to figure out if it's possible to run the larger models (e.g. 175B GPT-3 equivalents) on consumer hardware, perhaps by doing a very slow emulation using one or several PCs such that their collective RAM (or swap SDD space) matches the VRAM needed for those beasts.

So the question is ""will it run super slowly"" or ""will it fail immediately due to completely incompatible software / being impossible to configure for anything other than real datacenter hardware""?"
126,machinelearning,gpt-3,comments,2023-05-24 01:00:28,"Interview with Juergen Schmidhuber, renowned ‘Father Of Modern AI’, says his life’s work won't lead to dystopia.",hardmaru,False,0.81,246,13q6k4a,https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/,96,1684890028.0,"*Schmidhuber interview expressing his views on the future of AI and AGI.*

*Original [source](https://www.forbes.com/sites/hessiejones/2023/05/23/juergen-schmidhuber-renowned-father-of-modern-ai-says-his-lifes-work-wont-lead-to-dystopia/). I think the interview is of interest to r/MachineLearning, and presents an alternate view, compared to other influential leaders in AI.*

**Juergen Schmidhuber, Renowned 'Father Of Modern AI,' Says His Life’s Work Won't Lead To Dystopia**

*May 23, 2023. Contributed by [Hessie Jones](https://twitter.com/hessiejones).*

Amid the growing concern about the impact of more advanced artificial intelligence (AI) technologies on society, there are many in the technology community who fear the implications of the advancements in Generative AI if they go unchecked. Dr. Juergen Schmidhuber, a renowned scientist, artificial intelligence researcher and widely regarded as one of the pioneers in the field, is more optimistic. He declares that many of those who suddenly warn against the dangers of AI are just seeking publicity, exploiting the media’s obsession with killer robots which has attracted more attention than “good AI” for healthcare etc.

The potential to revolutionize various industries and improve our lives is clear, as are the equal dangers if bad actors leverage the technology for personal gain. Are we headed towards a dystopian future, or is there reason to be optimistic? I had a chance to sit down with Dr. Juergen Schmidhuber to understand his perspective on this seemingly fast-moving AI-train that will leap us into the future.

As a teenager in the 1970s, Juergen Schmidhuber became fascinated with the idea of creating intelligent machines that could learn and improve on their own, becoming smarter than himself within his lifetime. This would ultimately lead to his groundbreaking work in the field of deep learning.

In the 1980s, he studied computer science at the Technical University of Munich (TUM), where he earned his diploma in 1987. His thesis was on the ultimate self-improving machines that, not only, learn through some pre-wired human-designed learning algorithm, but also learn and improve the learning algorithm itself. Decades later, this became a hot topic. He also received his Ph.D. at TUM in 1991 for work that laid some of the foundations of modern AI.

Schmidhuber is best known for his contributions to the development of recurrent neural networks (RNNs), the most powerful type of artificial neural network that can process sequential data such as speech and natural language. With his students Sepp Hochreiter, Felix Gers, Alex Graves, Daan Wierstra, and others, he published architectures and training algorithms for the long short-term memory (LSTM), a type of RNN that is widely used in natural language processing, speech recognition, video games, robotics, and other applications. LSTM has become the most cited neural network of the 20th century, and Business Week called it ""[arguably the most commercial AI achievement](https://www.bloomberg.com/news/features/2018-05-15/google-amazon-and-facebook-owe-j-rgen-schmidhuber-a-fortune?leadSource=uverify%20wall).""

Throughout his career, Schmidhuber has received various awards and accolades for his groundbreaking work. In 2013, he was awarded the Helmholtz Prize, which recognizes significant contributions to the field of machine learning. In 2016, he was awarded the IEEE Neural Network Pioneer Award for ""*pioneering contributions to deep learning and neural networks."" The media have often called him the “father of modern AI,*” because the [most cited neural networks](https://people.idsia.ch/~juergen/most-cited-neural-nets.html) all build on his lab’s work. He is quick to point out, however, that AI history [goes back centuries.](https://people.idsia.ch/~juergen/deep-learning-history.html)

Despite his many accomplishments, at the age of 60, he feels mounting time pressure towards building an Artificial General Intelligence within his lifetime and remains committed to pushing the boundaries of AI research and development. He is currently director of the KAUST AI Initiative, scientific director of the Swiss AI Lab IDSIA, and co-founder and chief scientist of AI company NNAISENSE, whose motto is ""AI∀"" which is a math-inspired way of saying ""AI For All."" He continues to work on cutting-edge AI technologies and applications to improve human health and extend human lives and make lives easier for everyone.

*The following interview has been edited for clarity.*

**Jones: Thank you Juergen for joining me. You have signed letters warning about AI weapons. But you didn't sign the recent publication, ""Pause Gigantic AI Experiments: An Open Letter""? Is there a reason?**

**Schmidhuber:** Thank you Hessie. Glad to speak with you. I have realized that many of those who warn in public against the dangers of AI are just seeking publicity. I don't think the latest letter will have any significant impact because many AI researchers, companies, and governments will ignore it completely.

The proposal frequently uses the word ""we"" and refers to ""us,"" the humans. But as I have pointed out many times in the past, there is no ""we"" that everyone can identify with. Ask 10 different people, and you will hear 10 different opinions about what is ""good."" Some of those opinions will be completely incompatible with each other. Don't forget the enormous amount of conflict between the many people.

The letter also says, ""*If such a pause cannot be quickly put in place, governments should intervene and impose a moratorium.*"" The problem is that different governments have ALSO different opinions about what is good for them and for others. Great Power A will say, if we don't do it, Great Power B will, perhaps secretly, and gain an advantage over us. The same is true for Great Powers C and D.

**Jones: Everyone acknowledges this fear surrounding current generative AI technology. Moreover, the existential threat of this technology has been publicly acknowledged by** [**Sam Altman**](https://www.bbc.com/news/world-us-canada-65616866)**, CEO of OpenAI himself, calling for AI regulation. From your perspective, is there an existential threat?**

**Schmidhuber:** It is true that AI can be weaponized, and I have no doubt that there will be all kinds of AI arms races, but AI does not introduce a new quality of existential threat. The threat coming from AI weapons seems to pale in comparison to the much older threat from nuclear hydrogen bombs that don’t need AI at all. We should be much more afraid of half-century-old tech in the form of H-bomb rockets. The Tsar Bomba of 1961 had almost 15 times more destructive power than all weapons of WW-II combined.  Despite the dramatic nuclear disarmament since the 1980s, there are still more than enough nuclear warheads to wipe out human civilization within two hours, without any AI I’m much more worried about that old existential threat than the rather harmless AI weapons.

**Jones: I realize that while you compare AI to the threat of nuclear bombs, there is a current danger that a current technology can be put in the hands of humans and enable them to “eventually” exact further harms to individuals of group in a very precise way, like targeted drone attacks. You are giving people a toolset that they've never had before, enabling bad actors, as some have pointed out, to be able to do a lot more than previously because they didn't have this technology.**

**Schmidhuber:** Now, all that sounds horrible in principle, but our existing laws are sufficient to deal with these new types of weapons enabled by AI. If you kill someone with a gun, you will go to jail. Same if you kill someone with one of these drones. Law enforcement will get better at understanding new threats and new weapons and will respond with better technology to combat these threats. Enabling drones to target persons from a distance in a way that requires some tracking and some intelligence to perform, which has traditionally been performed by skilled humans, to me, it seems is just an improved version of a traditional weapon, like a gun, which is, you know, a little bit smarter than the old guns.

But, in principle, all of that is not a new development. For many centuries, we have had the evolution of better weaponry and deadlier poisons and so on, and law enforcement has evolved their policies to react to these threats over time. So, it's not that we suddenly have a new quality of existential threat and it's much more worrisome than what we have had for about six decades. A large nuclear warhead doesn’t need fancy face recognition to kill an individual. No, it simply wipes out an entire city with ten million inhabitants.

**Jones: The existential threat that’s implied is the extent to which humans have control over this technology. We see some early cases of opportunism which, as you say, tends to get more media attention than positive breakthroughs. But you’re implying that this will all balance out?**

**Schmidhuber:** Historically, we have a long tradition of technological breakthroughs that led to advancements in weapons for the purpose of defense but also for protection. From sticks, to rocks, to axes to gunpowder to cannons to rockets… and now to drones… this has had a drastic influence on human history but what has been consistent throughout history is that those who are using technology to achieve their own ends are themselves, facing the same technology because the opposing side is learning to use it against them. And that's what has been repeated in thousands of years of human history and it will continue. I don't see the new AI arms race as something that is remotely as existential a threat as the good old nuclear warheads.

You said something important, in that some people prefer to talk about the downsides rather than the benefits of this technology, but that's misleading, because 95% of all AI research and AI development is about making people happier and advancing human life and health.

**Jones: Let’s touch on some of those beneficial advances in AI research that have been able to radically change present day methods and achieve breakthroughs.**

**Schmidhuber:** All right! For example, eleven years ago, our team with my postdoc Dan Ciresan was the first to win a [medical imaging competition through deep learning](https://people.idsia.ch/~juergen/first-time-deep-learning-won-medical-imaging-contest-september-2012.html). We analyzed female breast cells with the objective to determine harmless cells vs. those in the pre-cancer stage. Typically, a trained oncologist needs a long time to make these determinations. Our team, who knew nothing about cancer, were able to train an artificial neural network, which was totally dumb in the beginning, on lots of this kind of data. It was able to outperform all the other methods. Today, this is being used not only for breast cancer, but also for radiology and detecting plaque in arteries, and many other things.  Some of the neural networks that we have developed in the last 3 decades are now prevalent across thousands of healthcare applications, detecting Diabetes and Covid-19 and what not. This will eventually permeate across all healthcare. The good consequences of this type of AI are much more important than the click-bait new ways of conducting crimes with AI.

**Jones: Adoption is a product of reinforced outcomes. The massive scale of adoption either leads us to believe that people have been led astray, or conversely, technology is having a positive effect on people’s lives.**

**Schmidhuber:** The latter is the likely case. There's intense commercial pressure towards good AI rather than bad AI because companies want to sell you something, and you are going to buy only stuff you think is going to be good for you. So already just through this simple, commercial pressure, you have a tremendous bias towards good AI rather than bad AI. However, doomsday scenarios like in Schwarzenegger movies grab more attention than documentaries on AI that improve people’s lives.

**Jones: I would argue that people are drawn to good stories – narratives that contain an adversary and struggle, but in the end, have happy endings. And this is consistent with your comment on human nature and how history, despite its tendency for violence and destruction of humanity, somehow tends to correct itself.**

**Let’s take the example of a technology, which you are aware – GANs – General Adversarial Networks, which today has been used in applications for fake news and disinformation. In actuality, the purpose in the invention of GANs was far from what it is used for today.**

**Schmidhuber:** Yes, the name GANs was created in 2014 but we had the basic principle already in the early 1990s. More than 30 years ago, I called it *artificial curiosity*. It's a very simple way of injecting creativity into a little two network system. This creative AI is not just trying to slavishly imitate humans. Rather, it’s inventing its own goals. Let me explain:

You have two networks. One network is producing outputs that could be anything, any action. Then the second network is looking at these actions and it’s trying to predict the consequences of these actions. An action could move a robot, then something happens, and the other network is just trying to predict what will happen.

Now we can implement artificial curiosity by reducing the prediction error of the second network, which, at the same time, is the reward of the first network. The first network wants to maximize its reward and so it will invent actions that will lead to situations that will surprise the second network, which it has not yet learned to predict well.

In the case where the outputs are fake images, the first network will try to generate images that are good enough to fool the second network, which will attempt to predict the reaction of the environment: fake or real image, and it will try to become better at it. The first network will continue to also improve at generating images whose type the second network will not be able to predict. So, they fight each other. The 2nd network will continue to reduce its prediction error, while the 1st network will attempt to maximize it.

Through this zero-sum game the first network gets better and better at producing these convincing fake outputs which look almost realistic. So, once you have an interesting set of images by Vincent Van Gogh, you can generate new images that leverage his style, without the original artist having ever produced the artwork himself.

**Jones: I see how the Van Gogh example can be applied in an education setting and there are countless examples of artists mimicking styles from famous painters but image generation from this instance that can happen within seconds is quite another feat. And you know this is how GANs has been used. What’s more prevalent today is a socialized enablement of generating images or information to intentionally fool people. It also surfaces new harms that deal with the threat to intellectual property and copyright, where laws have yet to account for. And from your perspective this was not the intention when the model was conceived. What was your motivation in your early conception of what is now GANs?**

**Schmidhuber:** My old motivation for GANs was actually very important and it was not to create deepfakes or fake news but to enable AIs to be curious and invent their own goals, to make them explore their environment and make them creative.

Suppose you have a robot that executes one action, then something happens, then it executes another action, and so on, because it wants to achieve certain goals in the environment. For example, when the battery is low, this will trigger “pain” through hunger sensors, so it wants to go to the charging station, without running into obstacles, which will trigger other pain sensors. It will seek to minimize pain (encoded through numbers). Now the robot has a friend, the second network, which is a world model ––it’s a prediction machine that learns to predict the consequences of the robot’s actions.

Once the robot has a good model of the world, it can use it for planning. It can be used as a simulation of the real world. And then it can determine what is a good action sequence. If the robot imagines this sequence of actions, the model will predict a lot of pain, which it wants to avoid. If it plays this alternative action sequence in its mental model of the world, then it will predict a rewarding situation where it’s going to sit on the charging station and its battery is going to load again. So, it'll prefer to execute the latter action sequence.

In the beginning, however, the model of the world knows nothing, so how can we motivate the first network to generate experiments that lead to data that helps the world model learn something it didn’t already know? That’s what artificial curiosity is about. The dueling two network systems effectively explore uncharted environments by creating experiments so that over time the curious AI gets a better sense of how the environment works. This can be applied to all kinds of environments, and has medical applications.

**Jones: Let’s talk about the future. You have said, “*****Traditional humans won’t play a significant role in spreading intelligence across the universe.*****”**

**Schmidhuber:** Let’s first conceptually separate two types of AIs. The first type of AI are tools directed by humans. They are trained to do specific things like accurately detect diabetes or heart disease and prevent attacks before they happen. In these cases, the goal is coming from the human. More interesting AIs are setting their own goals. They are inventing their own experiments and learning from them. Their horizons expand and eventually they become more and more general problem solvers in the real world. They are not controlled by their parents, but much of what they learn is through self-invented experiments.

A robot, for example, is rotating a toy, and as it is doing this, the video coming in through the camera eyes, changes over time and it begins to learn how this video changes and learns how the 3D nature of the toy generates certain videos if you rotate it a certain way, and eventually, how gravity works, and how the physics of the world works. Like a little scientist!

And I have predicted for decades that future scaled-up versions of such AI scientists will want to further expand their horizons, and eventually go where most of the physical resources are, to build more and bigger AIs. And of course, almost all of these resources are far away from earth out there in space, which is hostile to humans but friendly to appropriately designed AI-controlled robots and self-replicating robot factories. So here we are not talking any longer about our tiny biosphere; no, we are talking about the much bigger rest of the universe.  Within a few tens of billions of years, curious self-improving [AIs will colonize the visible cosmos](https://blogs.scientificamerican.com/observations/falling-walls-the-past-present-and-future-of-artificial-intelligence/) in a way that’s infeasible for humans. Those who don’t won’t have an impact. Sounds like science fiction, but since the 1970s I have been unable to see a plausible alternative to this scenario, except for a global catastrophe such as an all-out nuclear war that stops this development before it takes off.

**Jones: How long have these AIs, which can set their own goals — how long have they existed? To what extent can they be independent of human interaction?**

**Schmidhuber:** Neural networks like that have existed for over 30 years. My first simple adversarial neural network system of this kind is the one from 1990 described above. You don’t need a teacher there; it's just a little agent running around in the world and trying to invent new experiments that surprise its own prediction machine.

Once it has figured out certain parts of the world, the agent will become bored and will move on to more exciting experiments. The simple 1990 systems I mentioned have certain limitations, but in the past three decades, we have also built more [sophisticated systems that are setting their own goals](https://people.idsia.ch/~juergen/artificial-curiosity-since-1990.html) and such systems I think will be essential for achieving true intelligence. If you are only imitating humans, you will never go beyond them. So, you really must give AIs the freedom to explore previously unexplored regions of the world in a way that no human is really predefining.

**Jones: Where is this being done today?**

**Schmidhuber:** Variants of neural network-based artificial curiosity are used today for agents that learn to play video games in a human-competitive way. We have also started to use them for automatic design of experiments in fields such as materials science. I bet many other fields will be affected by it: chemistry, biology, drug design, you name it. However, at least for now, these artificial scientists, as I like to call them, cannot yet compete with human scientists.

I don’t think it’s going to stay this way but, at the moment, it’s still the case.  Sure, AI has made a lot of progress. Since 1997, there have been superhuman chess players, and since 2011, through the DanNet of my team, there have been [superhuman visual pattern recognizers](https://people.idsia.ch/~juergen/DanNet-triggers-deep-CNN-revolution-2011.html). But there are other things where humans, at the moment at least, are much better, in particular, science itself.  In the lab we have many first examples of self-directed artificial scientists, but they are not yet convincing enough to appear on the radar screen of the public space, which is currently much more fascinated with simpler systems that just imitate humans and write texts based on previously seen human-written documents.

**Jones: You speak of these numerous instances dating back 30 years of these lab experiments where these self-driven agents are deciding and learning and moving on once they’ve learned. And I assume that that rate of learning becomes even faster over time. What kind of timeframe are we talking about when this eventually is taken outside of the lab and embedded into society?**

**Schmidhuber:** This could still take months or even years :-) Anyway, in the not-too-distant future, we will probably see artificial scientists who are good at devising experiments that allow them to discover new, previously unknown physical laws.

As always, we are going to profit from the old trend that has held at least since 1941: every decade compute is getting 100 times cheaper.

**Jones: How does this trend affect modern AI such as ChatGPT?**

**Schmidhuber:** Perhaps you know that all the recent famous AI applications such as ChatGPT and similar models are largely based on principles of artificial neural networks invented in the previous millennium. The main reason why they works so well now is the incredible acceleration of compute per dollar.

ChatGPT is driven by a neural network called “Transformer” described in 2017 by Google. I am happy about that because a quarter century earlier in 1991 I had a particular Transformer variant which is now called the “[Transformer with linearized self-attention](https://twitter.com/SchmidhuberAI/status/1576966129993797632?cxt=HHwWgMDSkeKVweIrAAAA)”. Back then, not much could be done with it, because the compute cost was a million times higher than today. But today, one can train such models on half the internet and achieve much more interesting results.

**Jones: And for how long will this acceleration continue?**

**Schmidhuber:** There's no reason to believe that in the next 30 years, we won't have another factor of 1 million and that's going to be really significant. In the near future, for the first time we will have many not-so expensive devices that can compute as much as a human brain. The physical limits of computation, however, are much further out so even if the trend of a factor of 100 every decade continues, the physical limits (of 1051 elementary instructions per second and kilogram of matter) won’t be hit until, say, the mid-next century. Even in our current century, however, we’ll probably have many machines that compute more than all 10 billion human brains collectively and you can imagine, everything will change then!

**Jones: That is the big question. Is everything going to change? If so, what do you say to the next generation of leaders, currently coming out of college and university. So much of this change is already impacting how they study, how they will work, or how the future of work and livelihood is defined. What is their purpose and how do we change our systems so they will adapt to this new version of intelligence?**

**Schmidhuber:** For decades, people have asked me questions like that, because you know what I'm saying now, I have basically said since the 1970s, it’s just that today, people are paying more attention because, back then, they thought this was science fiction.

They didn't think that I would ever come close to achieving my crazy life goal of building a machine that learns to become smarter than myself such that I can retire. But now many have changed their minds and think it's conceivable. And now I have two daughters, 23 and 25. People ask me: what do I tell them? They know that Daddy always said, “*It seems likely that within your lifetimes, you will have new types of intelligence that are probably going to be superior in many ways, and probably all kinds of interesting ways.*” How should they prepare for that? And I kept telling them the obvious: **Learn how to learn new things**! It's not like in the previous millennium where within 20 years someone learned to be a useful member of society, and then took a job for 40 years and performed in this job until she received her pension. Now things are changing much faster and we must learn continuously just to keep up. I also told my girls that no matter how smart AIs are going to get, learn at least the basics of math and physics, because that’s the essence of our universe, and anybody who understands this will have an advantage, and learn all kinds of new things more easily. I also told them that social skills will remain important, because most future jobs for humans will continue to involve interactions with other humans, but I couldn’t teach them anything about that; they know much more about social skills than I do.

You touched on the big philosophical question about people’s purpose. Can this be answered without answering the even grander question: What’s the purpose of the entire universe?

We don’t know. But what’s happening right now might be connected to the unknown answer. Don’t think of humans as the crown of creation. Instead view human civilization as part of a much grander scheme, an important step (but not the last one) on the path of the universe from very simple initial conditions towards more and more unfathomable complexity. Now it seems ready to take its [next step, a step comparable to the invention of life itself over 3.5 billion years ago](https://people.idsia.ch/~juergen/deep-learning-history.html#future).  Alas, don’t worry, in the end, all will be good!

**Jones: Let’s get back to this transformation happening right now with OpenAI. There are many questioning the efficacy and accuracy of ChatGPT, and are concerned its release has been premature. In light of the rampant adoption, educators have banned its use over concerns of plagiarism and how it stifles individual development. Should large language models like ChatGPT be used in school?**

**Schmidhuber:** When the calculator was first introduced, instructors forbade students from using it in school. Today, the consensus is that kids should learn the basic methods of arithmetic, but they should also learn to use the “artificial multipliers” aka calculators, even in exams, because laziness and efficiency is a hallmark of intelligence. Any intelligent being wants to minimize its efforts to achieve things.

And that's the reason why we have tools, and why our kids are learning to use these tools. The first stone tools were invented maybe 3.5 million years ago; tools just have become more sophisticated over time. In fact, humans have changed in response to the properties of their tools. Our anatomical evolution was shaped by tools such as spears and fire. So, it's going to continue this way. And there is no permanent way of preventing large language models from being used in school.

**Jones: And when our children, your children graduate, what does their future work look like?**

**Schmidhuber:** A single human trying to predict details of how 10 billion people and their machines will evolve in the future is like a single neuron in my brain trying to predict what the entire brain and its tens of billions of neurons will do next year. 40 years ago, before the WWW was created at CERN in Switzerland, who would have predicted all those young people making money as YouTube video bloggers?

Nevertheless, let’s make a few limited job-related observations. For a long time, people have thought that desktop jobs may require more intelligence than skills trade or handicraft professions. But now, it turns out that it's much easier to replace certain aspects of desktop jobs than replacing a carpenter, for example. Because everything that works well in AI is happening behind the screen currently, but not so much in the physical world.

There are now artificial systems that can read lots of documents and then make really nice summaries of these documents. That is a desktop job. Or you give them a description of an illustration that you want to have for your article and pretty good illustrations are being generated that may need some minimal fine-tuning. But you know, all these desktop jobs are much easier to facilitate than the real tough jobs in the physical world. And it's interesting that the things people thought required intelligence, like playing chess, or writing or summarizing documents, are much easier for machines than they thought. But for things like playing football or soccer, there is no physical robot that can remotely compete with the abilities of a little boy with these skills. So, AI in the physical world, interestingly, is much harder than AI behind the screen in virtual worlds. And it's really exciting, in my opinion, to see that jobs such as plumbers are much more challenging than playing chess or writing another tabloid story.

**Jones: The way data has been collected in these large language models does not guarantee personal information has not been excluded. Current consent laws already are outdated when it comes to these large language models (LLM). The concern, rightly so, is increasing surveillance and loss of privacy. What is your view on this?**

**Schmidhuber:** As I have indicated earlier: are surveillance and loss of privacy inevitable consequences of increasingly complex societies? Super-organisms such as cities and states and companies consist of numerous people, just like people consist of numerous cells. These cells enjoy little privacy. They are constantly monitored by specialized ""police cells"" and ""border guard cells"": Are you a cancer cell? Are you an external intruder, a pathogen? Individual cells sacrifice their freedom for the benefits of being part of a multicellular organism.

Similarly, for super-organisms such as nations. Over 5000 years ago, writing enabled recorded history and thus became its inaugural and most important invention. Its initial purpose, however, was to facilitate surveillance, to track citizens and their tax payments. The more complex a super-organism, the more comprehensive its collection of information about its constituents.

200 years ago, at least, the parish priest in each village knew everything about all the village people, even about those who did not confess, because they appeared in the confessions of others. Also, everyone soon knew about the stranger who had entered the village, because some occasionally peered out of the window, and what they saw got around. Such control mechanisms were temporarily lost through anonymization in rapidly growing cities but are now returning with the help of new surveillance devices such as smartphones as part of digital nervous systems that tell companies and governments a lot about billions of users. Cameras and drones etc. are becoming increasingly tinier and more ubiquitous. More effective recognition of faces and other detection technology are becoming cheaper and cheaper, and many will use it to identify others anywhere on earth; the big wide world will not offer any more privacy than the local village. Is this good or bad? Some nations may find it easier than others to justify more complex kinds of super-organisms at the expense of the privacy rights of their constituents.

**Jones: So, there is no way to stop or change this process of collection, or how it continuously informs decisions over time? How do you see governance and rules responding to this, especially amid** [**Italy’s ban on ChatGPT following**](https://www.cnbc.com/2023/04/04/italy-has-banned-chatgpt-heres-what-other-countries-are-doing.html) **suspected user data breach and the more recent news about the** [**Meta’s record $1.3billion fine**](https://www.reuters.com/technology/facebook-given-record-13-bln-fine-given-5-months-stop-eu-us-data-flows-2023-05-22/) **in the company’s handling of user information?**

**Schmidhuber:** Data collection has benefits and drawbacks, such as the loss of privacy. How to balance those? I have argued for addressing this through data ownership in data markets. If it is true that data is the new oil, then it should have a price, just like oil. At the moment, the major surveillance platforms such as Meta do not offer users any money for their data and the transitive loss of privacy. In the future, however, we will likely see attempts at creating efficient data markets to figure out the data's true financial value through the interplay between supply and demand.

Even some of the sensitive medical data should not be priced by governmental regulators but by patients (and healthy persons) who own it and who may sell or license parts thereof as micro-entrepreneurs in a healthcare data market.

Following a previous [interview](https://www.swissre.com/institute/conferences/The-intelligence-behind-artificial-intelligence.html), I gave for one of the largest re-insurance companies , let's look at the different participants in such a data market: patients, hospitals, data companies. (1) **Patients** with a rare form of cancer can offer more valuable data than patients with a very common form of cancer. (2) **Hospitals** and their machines are needed to extract the data, e.g., through magnet spin tomography, radiology, evaluations through human doctors, and so on. (3) **Companies** such as Siemens, Google or IBM would like to buy annotated data to make better artificial neural networks that learn to predict pathologies and diseases and the consequences of therapies. Now the market’s invisible hand will decide about the data’s price through the interplay between demand and supply. On the demand side, you will have several companies offering something for the data, maybe through an app on the smartphone (a bit like a stock market app). On the supply side, each patient in this market should be able to profit from high prices for rare valuable types of data. Likewise, competing data extractors such as hospitals will profit from gaining recognition and trust for extracting data well at a reasonable price. The market will make the whole system efficient through incentives for all who are doing a good job. Soon there will be a flourishing ecosystem of commercial data market advisors and what not, just like the ecosystem surrounding the traditional stock market. The value of the data won’t be determined by governments or ethics committees, but by those who own the data and decide by themselves which parts thereof they want to license to others under certain conditions.

At first glance, a market-based system seems to be detrimental to the interest of certain monopolistic companies, as they would have to pay for the data - some would prefer free data and keep their monopoly. However, since every healthy and sick person in the market would suddenly have an incentive to collect and share their data under self-chosen anonymity conditions, there will soon be many more useful data to evaluate all kinds of treatments. On average, people will live longer and healthier, and many companies and the entire healthcare system will benefit.

**Jones: Finally, what is your view on open source versus the private companies like Google and OpenAI? Is there a danger to supporting these private companies’ large language models versus trying to keep these models open source and transparent, very much like what LAION is doing?**

**Schmidhuber:** I signed this [open letter by LAION](https://www.forbes.com/sites/hessiejones/2023/04/19/amid-growing-call-to-pause-ai-research-laion-petitions-governments-to-keep-agi-research-open-active-and-responsible/?sh=6973c08b62e3) because I strongly favor the open-source movement. And I think it's also something that is going to challenge whatever big tech dominance there might be at the moment. Sure, the best models today are run by big companies with huge budgets for computers, but the exciting fact is that open-source models are not so far behind, some people say maybe six to eight months only. Of course, the private company models are all based on stuff that was created in academia, often in little labs without so much funding, which publish without patenting their results and open source their code and others take it and improved it.

Big tech has profited tremendously from academia; their main achievement being that they have scaled up everything greatly, sometimes even failing to credit the original inventors.

So, it's very interesting to see that as soon as some big company comes up with a new scaled-up model, lots of students out there are competing, or collaborating, with each other, trying to come up with equal or better performance on smaller networks and smaller machines. And since they are open sourcing, the next guy can have another great idea to improve it, so now there’s tremendous competition also for the big companies.

Because of that, and since AI is still getting exponentially cheaper all the time, I don't believe that big tech companies will dominate in the long run. They find it very hard to compete with the enormous open-source movement. As long as you can encourage the open-source community, I think you shouldn't worry too much. Now, of course, you might say if everything is open source, then the bad actors also will more easily have access to these AI tools. And there's truth to that. But as always since the invention of controlled fire, it was good that knowledge about how technology works quickly became public such that everybody could use it. And then, against any bad actor, there's almost immediately a counter actor trying to nullify his efforts. You see, I still believe in our old motto ""AI∀"" or ""AI For All.""

**Jones: Thank you, Juergen for sharing your perspective on this amazing time in history. It’s clear that with new technology, the enormous potential can be matched by disparate and troubling risks which we’ve yet to solve, and even those we have yet to identify. If we are to dispel the fear of a sentient system for which we have no control, humans, alone need to take steps for more responsible development and collaboration to ensure AI technology is used to ultimately benefit society. Humanity will be judged by what we do next.**"
127,machinelearning,gpt-3,comments,2022-12-22 18:39:30,[D] When chatGPT stops being free: Run SOTA LLM in cloud,_underlines_,False,0.95,352,zstequ,https://www.reddit.com/r/MachineLearning/comments/zstequ/d_when_chatgpt_stops_being_free_run_sota_llm_in/,95,1671734370.0,"Edit: Found [LAION-AI/OPEN-ASSISTANT](https://github.com/LAION-AI/Open-Assistant) a very promising project opensourcing the idea of chatGPT. [video here](https://www.youtube.com/watch?v=8gVYC_QX1DI)

**TL;DR: I found GPU compute to be [generally cheap](https://github.com/full-stack-deep-learning/website/blob/main/docs/cloud-gpus/cloud-gpus.csv) and spot or on-demand instances can be launched on AWS for a few USD / hour up to over 100GB vRAM. So I thought it would make sense to run your own SOTA LLM like Bloomz 176B inference endpoint whenever you need it for a few questions to answer. I thought it would still make more sense than shoving money into a closed walled garden like ""not-so-OpenAi"" when they make ChatGPT or GPT-4 available for $$$. But I struggle due to lack of tutorials/resources.**

Therefore, I carefully checked benchmarks, model parameters and sizes as well as training sources for all SOTA LLMs [here](https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit#gid=1158069878).

Knowing since reading the Chinchilla paper that Model Scaling according to OpenAI was wrong and more params != better quality generation. So I was looking for the best performing LLM openly available in terms of quality and broadness to use for multilingual everyday questions/code completion/reasoning similar to what chatGPT provides (minus the fine-tuning for chat-style conversations).

My choice fell on [Bloomz](https://huggingface.co/bigscience/bloomz) (because that handles multi-lingual questions well and has good zero shot performance for instructions and Q&A style text generation. Confusingly Galactica seems to outperform Bloom on several benchmarks. But since Galactica had a very narrow training set only using scientific papers, I guess usage is probably limited for answers on non-scientific topics.

Therefore I tried running the original bloom 176B and alternatively also Bloomz 176B on AWS SageMaker JumpStart, which should be a one click deployment. This fails after 20min. On Azure ML, I tried using DeepSpeed-MII which also supports bloom but also fails due the instance size of max 12GB vRAM I guess.

From my understanding to save costs on inference, it's probably possible to use one or multiple of the following solutions:

- Precision: int8 instead of fp16
- [Microsoft/DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) for an up 40x reduction on inference cost on Azure, this thing also supports int8 and fp16 bloom out of the box, but it fails on Azure due to instance size.
- [facebook/xformer](https://github.com/facebookresearch/xformers) not sure, but if I remember correctly this brought inference requirements down to 4GB vRAM for StableDiffusion and DreamBooth fine-tuning to 10GB. No idea if this is usefull for Bloom(z) inference cost reduction though

I have a CompSci background but I am not familiar with most stuff, except that I was running StableDiffusion since day one on my rtx3080 using linux and also doing fine-tuning with DreamBooth. But that was all just following youtube tutorials. I can't find a single post or youtube video of anyone explaining a full BLOOM / Galactica / BLOOMZ inference deployment on cloud platforms like AWS/Azure using one of the optimizations mentioned above, yet alone deployment of the raw model. :(

I still can't figure it out by myself after 3 days.

**TL;DR2: Trying to find likeminded people who are interested to run open source SOTA LLMs for when chatGPT will be paid or just for fun.**

Any comments, inputs, rants, counter-arguments are welcome.

/end of rant"
128,machinelearning,gpt-3,comments,2023-03-17 09:59:59,[D] PyTorch 2.0 Native Flash Attention 32k Context Window,super_deap,False,0.98,348,11tmpc5,https://www.reddit.com/r/MachineLearning/comments/11tmpc5/d_pytorch_20_native_flash_attention_32k_context/,94,1679047199.0,"Hi,

I did a quick experiment with Pytorch 2.0 Native scaled\_dot\_product\_attention. I was able to a single forward pass within 9GB of memory which is astounding. I think by patching existing Pretrained GPT models and adding more positional encodings, one could easily fine-tune those models to 32k attention on a single A100 80GB. Here is the code I used:

&#x200B;

https://preview.redd.it/6csxe28lv9oa1.png?width=607&format=png&auto=webp&s=ff8b48a77f49fab7d088fd8ba220f720860249bc

I think it should be possible to replicate even GPT-4 with open source tools something like Bloom + FlashAttention & fine-tune on 32k tokens.

**Update**: I was successfully able to start the training of GPT-2 (125M) with a context size of 8k and batch size of 1 on a 16GB GPU. Since memory scaled linearly from 4k to 8k. I am expecting, 32k would require \~64GB and should train smoothly on A100 80 GB. Also, I did not do any other optimizations. Maybe 8-bit fine-tuning can further optimize it.

**Update 2**: I basically picked Karpaty's nanoGPT and patched the pretrained GPT-2 by repeating the embeddings N-times. I was unable to train the model at 8k because generation would cause the crash.  So I started the training for a context window of 4k on The Pile: 1 hour in and loss seems to be going down pretty fast. Also Karpaty's generate function is super inefficient, O(n\^4) I think so it took forever to generate even 2k tokens. So I generate 1100 tokens just to see if the model is able to go beyond 1k limit. And it seems to be working. [Here are some samples](https://0bin.net/paste/O-+eopaW#nmtzX1Re7f1Nr-Otz606jkltvKk/kUXY96/8ca+tb4f) at 3k iteration.

&#x200B;

https://preview.redd.it/o2hb25w1sboa1.png?width=1226&format=png&auto=webp&s=bad2a1e21e218512b0f630c947ee41dba9b86a44

**Update 3**: I have started the training and I am publishing the training script if anyone is interested in replicating or building upon this work. Here is the complete training script:

[https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c](https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c)

I will post an update after the weekend once the training has progressed somewhat.

**Post-Weekend Update**: After \~50k iterations (the model has seen \~200 million tokens, I know this is just too small compared to 10s of billions trained by giga corps), loss only dropped from 4.6 to 4.2 on The Pile:

https://preview.redd.it/vi0fpskhsuoa1.png?width=1210&format=png&auto=webp&s=9fccc5277d91a6400adc6d968b0f2f0ff0da2afc

AFAIR, the loss of GPT-2 on the Pile if trained with 1024 tokens is \~2.8. It seems like the size of the dimension for each token is kind of limiting how much loss can go down since GPT-2 (small) has an embedding dimension of 768. Maybe someone can experiment with GPT-2 medium etc. to see how much we can improve. This is confirmation of the comment by u/lucidraisin [below](https://www.reddit.com/r/MachineLearning/comments/11tmpc5/comment/jcl2rkh/?utm_source=reddit&utm_medium=web2x&context=3)."
129,machinelearning,gpt-3,comments,2023-01-08 18:23:03,"[P] I built Adrenaline, a debugger that fixes errors and explains them with GPT-3",jsonathan,False,0.96,1561,106q6m9,https://i.redd.it/8t0k9jkd3vaa1.gif,92,1673202183.0,
130,machinelearning,gpt-3,comments,2023-03-25 04:14:58,[D] Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.89,102,121a8p4,https://www.reddit.com/r/MachineLearning/comments/121a8p4/d_do_we_really_need_100b_parameters_in_a_large/,90,1679717698.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
131,machinelearning,gpt-3,comments,2023-05-10 20:10:30,"[D] Since Google buried the MMLU benchmark scores in the Appendix of the PALM 2 technical report, here it is vs GPT-4 and other LLMs",jd_3d,False,0.97,347,13e1rf9,https://www.reddit.com/r/MachineLearning/comments/13e1rf9/d_since_google_buried_the_mmlu_benchmark_scores/,88,1683749430.0,"MMLU Benchmark results (all 5-shot)

* GPT-4 -  86.4%
* Flan-PaLM 2 (L) -   81.2%
* PALM 2 (L)  -  78.3%
* GPT-3.5 - 70.0%
* PaLM 540B  -  69.3%
* LLaMA 65B -  63.4%"
132,machinelearning,gpt-3,comments,2023-04-16 19:53:45,[R] Timeline of recent Large Language Models / Transformer Models,viktorgar,False,0.95,771,12omnxo,https://i.redd.it/gl11ce50xaua1.png,86,1681674825.0,
133,machinelearning,gpt-3,comments,2022-01-28 17:39:35,[D] It seems OpenAI’s new embedding models perform terribly,StellaAthena,False,0.97,283,sew5rl,https://www.reddit.com/r/MachineLearning/comments/sew5rl/d_it_seems_openais_new_embedding_models_perform/,80,1643391575.0,"Some people on Twitter have been investigating [OpenAI’s new embedding API](https://openai.com/blog/introducing-text-and-code-embeddings/) and it’s shocking how poorly it performs. On standard benchmarks, open source models 1000x smaller obtain equal or better performance! Models based on RoBERTa and T5, as well as the Sentence Transformer all achieve significantly better performance than the 175B model. Also of interest is that the DaVinci (175B) model is not clearly better than the Ada (350M) model.

Has anyone tried adapting some other autoregressive languages models, such as GPT-2, GPT-Neo, or GPT-J to do embeddings? I’m quite curious if this is an inherent failing of autoregressive models or if there’s something else going on. **Edit:** [a commenter](https://www.reddit.com/r/MachineLearning/comments/sew5rl/d_it_seems_openais_new_embedding_models_perform/humuzef/) has asked that I point out that I am one of the creators of GPT-Neo and part of the org that created GPT-J. These examples were not intended as specific endorsements, and I would be just as interested in comparisons using other billion-parameter+ autoregressive language models.

**Edit 2:** I originally linked to a [tweet](https://twitter.com/Nils_Reimers/status/1487014195568775173?s=20&amp;amp;amp;amp;amp;amp;amp;t=NBF7D2DYi41346cGM-PQjQ) about this, but several commenters pointed out that there’s also a [blog post](https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9) with more information.

**Edit 3:** An OpenAI researcher [seems to have responded](https://mobile.twitter.com/arvind_io/status/1487188996774002688)."
134,machinelearning,gpt-3,comments,2024-02-04 17:06:06,"[P] Chess-GPT, 1000x smaller than GPT-4, plays 1500 Elo chess. We can visualize its internal board state, and it accurately estimates the Elo rating of the players in a game.",seraine,False,0.95,370,1aisp4m,https://www.reddit.com/r/MachineLearning/comments/1aisp4m/p_chessgpt_1000x_smaller_than_gpt4_plays_1500_elo/,80,1707066366.0," gpt-3.5-turbo-instruct's Elo rating of 1800 is chess seemed magical. But it's not! A 100-1000x smaller parameter LLM given a few million games of chess will learn to play at ELO 1500.

This model is only trained to predict the next character in PGN strings (1.e4 e5 2.Nf3 …) and is never explicitly given the state of the board or the rules of chess. Despite this, in order to better predict the next character, it learns to compute the state of the board at any point of the game, and learns a diverse set of rules, including check, checkmate, castling, en passant, promotion, pinned pieces, etc. In addition, to better predict the next character it also learns to estimate latent variables such as the Elo rating of the players in the game.

We can visualize the internal board state of the model as it's predicting the next character. For example, in this heatmap, we have the ground truth white pawn location on the left, a binary probe output in the middle, and a gradient of probe confidence on the right. We can see the model is extremely confident that no white pawns are on either back rank.

&#x200B;

https://preview.redd.it/dn8aryvdolgc1.jpg?width=2500&format=pjpg&auto=webp&s=003fe39d8a9bce2cc3271c4c9232c00e4d886aa6

In addition, to better predict the next character it also learns to estimate latent variables such as the ELO rating of the players in the game. More information is available in this post:

[https://adamkarvonen.github.io/machine\_learning/2024/01/03/chess-world-models.html](https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html)

And the code is here: [https://github.com/adamkarvonen/chess\_llm\_interpretability](https://github.com/adamkarvonen/chess_llm_interpretability)"
135,machinelearning,gpt-3,comments,2020-08-17 01:26:40,[D] Why does models like GPT-3 or BERT don't have overfitting problems?,psarangi112,False,0.95,236,ib4rth,https://www.reddit.com/r/MachineLearning/comments/ib4rth/d_why_does_models_like_gpt3_or_bert_dont_have/,79,1597627600.0,"Hey everyone, I am new to Natural Language Processing, but I have experience in Machine Learning and Convolutional Neural Network. While reading the GPT-3 paper, this question came to my mind, like having around 175 billion trainable the equation that will come out must be very complex and also it is trained on such a huge dataset.
Than why is their no case of overfitting on this model."
136,machinelearning,gpt-3,comments,2022-03-16 16:23:25,[P] Composer: a new PyTorch library to train models ~2-4x faster with better algorithms,moinnadeem,False,0.97,476,tflvuy,https://www.reddit.com/r/MachineLearning/comments/tflvuy/p_composer_a_new_pytorch_library_to_train_models/,77,1647447805.0,"Hey all!

We're excited to release Composer ([https://github.com/mosaicml/composer](https://github.com/mosaicml/composer)), an open-source library to speed up training of deep learning models by integrating better algorithms into the training process!

[Time and cost reductions across multiple model families](https://preview.redd.it/0y54ykj8qrn81.png?width=3009&format=png&auto=webp&s=d5f14b3381828d0b9d71ab04a4f1f12ebfb07fd7)

Composer lets you train:

* A ResNet-101 to 78.1% accuracy on ImageNet in 1 hour and 30 minutes ($49 on AWS), **3.5x faster and 71% cheaper than the baseline.**
* A ResNet-50 to 76.51% accuracy on ImageNet in 1 hour and 14 minutes ($40 on AWS), **2.9x faster and 65% cheaper than the baseline.**
* A GPT-2 to a perplexity of 24.11 on OpenWebText in 4 hours and 27 minutes ($145 on AWS), **1.7x faster and 43% cheaper than the baseline.**

https://preview.redd.it/0bitody9qrn81.png?width=10008&format=png&auto=webp&s=d9ecdb45f6419eb49e1c2c69eec418b36f35e172

Composer features a **functional interface** (similar to `torch.nn.functional`), which you can integrate into your own training loop, and a **trainer,** which handles seamless integration of efficient training algorithms into the training loop for you.

**Industry practitioners:** leverage our 20+ vetted and well-engineered implementations of speed-up algorithms to easily reduce time and costs to train models. Composer's built-in trainer makes it easy to **add multiple efficient training algorithms in a single line of code.** Trying out new methods or combinations of methods is as easy as changing a single list, and [we provide training recipes](https://github.com/mosaicml/composer#resnet-101) that yield the best training efficiency for popular benchmarks such as ResNets and GPTs.

**ML scientists:** use our two-way callback system in the Trainer **to easily prototype algorithms for wall-clock training efficiency.**[ Composer features tuned baselines to use in your research](https://github.com/mosaicml/composer/tree/dev/composer/yamls), and the software infrastructure to help study the impacts of an algorithm on training dynamics. Many of us wish we had this for our previous research projects!

**Feel free check out our GitHub repo:** [https://github.com/mosaicml/composer](https://github.com/mosaicml/composer), and star it ⭐️ to keep up with the latest updates!"
137,machinelearning,gpt-3,comments,2023-03-18 10:15:33,[D] Totally Open Alternatives to ChatGPT,KingsmanVince,False,0.98,744,11uk8ti,https://www.reddit.com/r/MachineLearning/comments/11uk8ti/d_totally_open_alternatives_to_chatgpt/,70,1679134533.0,"I have migrated this to GitHub for easy contribution: https://github.com/nichtdax/awesome-totally-open-chatgpt

By alternative, I mean projects feature different language model for chat system.
I do **not** count alternative **frontend** projects because they just call the API from OpenAI. 
I do **not** consider alternative **transformer decoder** to GPT 3.5 either because the training data of them are (mostly) not for chat system.

Tags:

-   B: bare (no data, no model's weight, no chat system)
-   F: full (yes data, yes model's weight, yes chat system including TUI and GUI)

| Project                                                                               | Description                                                                                                                                                                                                                                                                                                                                                                               | Tags |
| ------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- |
| [lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch)       | Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM                                                                                                                                                                                                                                                      | B    |
| [togethercomputer/OpenChatKit](https://github.com/togethercomputer/OpenChatKit)       | OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. [Demo](https://huggingface.co/spaces/togethercomputer/OpenChatKit)                                                                                                                                                                                    | F    |
| [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) | A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.                                                                                                                                                                                                                                                                                    | F    |
| [KoboldAI/KoboldAI-Client](https://github.com/KoboldAI/KoboldAI-Client)               | This is a browser-based front-end for AI-assisted writing with multiple local & remote AI models. It offers the standard array of tools, including Memory, Author's Note, World Info, Save & Load, adjustable AI settings, formatting options, and the ability to import existing AI Dungeon adventures. You can also turn on Adventure mode and play the game like AI Dungeon Unleashed. | F    |
| [LAION-AI/Open-Assistant/](https://github.com/LAION-AI/Open-Assistant/)               | OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.                                                                                                                                                                                                                                     | F    |"
138,machinelearning,gpt-3,comments,2021-05-23 08:36:30,[D] What left-field approaches to AI do you know of?,fakefakedroon,False,0.95,102,nj3icz,https://www.reddit.com/r/MachineLearning/comments/nj3icz/d_what_leftfield_approaches_to_ai_do_you_know_of/,74,1621758990.0,"Over the years, I've come across the occasional alternative approach to AI. Huge efforts creating comprehensive hardcoded domain knowledge, alternatives to neutral nets, etc, but I regret not bookmarking them so the above descriptions is all I remember. Do you guys know of interesting/quaint but serious efforts of doing things really differently? It would be nice to eventually get an overview of all the wierdness out there.

Edit: thanks everyone, interesting stuff so far. Does anyone know the two examples I was referring to? I'd love to find them again. One was a professor, that, as a alternative to gpt-like nlp, was handcrafting a huge database of concepts and how they related to eachother. I read about this.. 2-3 years ago and the effort was ongoing.  The other one was a machine learning alternative to neutral nets. I think it had a 3 letter acronym with an M and a C in it? It also had one lone professor/flagbearer. Can't remember much about it, but it didn't fit into anything I had heard of. I can't remember the general premise.. Not SVM on anything that common. It was.. mid 2000's tech.?"
139,machinelearning,gpt-3,comments,2020-08-15 18:27:54,[P] Ask GPT-3 health-related or fitness-related questions for free by using webpage FitnessAI Knowledge,Wiskkey,False,0.97,28,iacm31,https://www.reddit.com/r/MachineLearning/comments/iacm31/p_ask_gpt3_healthrelated_or_fitnessrelated/,71,1597516074.0,"[FitnessAI Knowledge](https://app.fitnessai.com/knowledge/) uses GPT-3 to answer health-related or fitness-related questions. [This](https://news.knowledia.com/US/en/articles/knowledge-by-fitnessai-5c8b794fbb3c751aa3b59e721aad6752ea46bfe1) is the source of the claim that GPT-3 is being used. The webpage refuses to answer questions that are not considered health-related or fitness-related; however (*hint hint*) one can ask multiple questions - one that is health/fitness-related, and one that is not - in a given query.

Example input:

>Explained at a Ph.D. level, how is sleep regulated?

Example output:

>Sleep is regulated by the circadian rhythm, which is a biological clock that tells your body when to sleep and when to wake up. The circadian rhythm is regulated by the suprachiasmatic nucleus (SCN), which is a group of cells in the hypothalamus. The SCN receives information from light sensors in your eyes, and sends signals to other parts of your brain that control hormones and body temperature.

[This](https://www.reddit.com/r/artificial/comments/icvypl/list_of_free_sitesprograms_that_are_powered_by/) is a list of other free GPT-3-powered sites/programs that can be used now without a waiting list."
140,machinelearning,gpt-3,comments,2023-02-11 12:54:26,[P] Introducing arxivGPT: chrome extension that summarizes arxived research papers using chatGPT,_sshin_,False,0.95,840,10zmz2d,https://i.redd.it/jmgr7vsy3kha1.jpg,70,1676120066.0,
141,machinelearning,gpt-3,comments,2019-08-13 16:48:08,[News] Megatron-LM: NVIDIA trains 8.3B GPT-2 using model and data parallelism on 512 GPUs. SOTA in language modelling and SQUAD. Details awaited.,Professor_Entropy,False,0.97,358,cpvssu,https://www.reddit.com/r/MachineLearning/comments/cpvssu/news_megatronlm_nvidia_trains_83b_gpt2_using/,66,1565714888.0,"Code: [https://github.com/NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM)

Unlike Open-AI, they have released the complete code for data processing, training, and evaluation.

Detailed writeup: [https://nv-adlr.github.io/MegatronLM](https://nv-adlr.github.io/MegatronLM)

From github:

>Megatron  is a large, powerful transformer. This repo is for ongoing  research on  training large, powerful transformer language models at  scale.  Currently, we support model-parallel, multinode training of [GPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and [BERT](https://arxiv.org/pdf/1810.04805.pdf) in mixed precision.Our  codebase is capable of efficiently training a 72-layer, 8.3  Billion  Parameter GPT2 Language model with 8-way model and 64-way data   parallelism across 512 GPUs. We find that bigger language models are   able to surpass current GPT2-1.5B wikitext perplexities in as little as 5   epochs of training.For BERT  training our repository trains BERT Large on 64 V100 GPUs in  3 days. We  achieved a final language modeling perplexity of 3.15 and  SQuAD  F1-score of 90.7.

Their submission is not in the leaderboard of SQuAD, but this exceeds the previous best single model performance (RoBERTa 89.8).

For  language modelling they get zero-shot wikitext perplexity of 17.4 (8.3B  model) better than 18.3 of transformer-xl (257M). However they claim it  as SOTA when GPT-2 itself has 17.48 ppl, and another model has 16.4 ([https://paperswithcode.com/sota/language-modelling-on-wikitext-103](https://paperswithcode.com/sota/language-modelling-on-wikitext-103))

Sadly they haven't mentioned anything about release of the model weights."
142,machinelearning,gpt-3,comments,2022-06-20 19:43:22,"[D] Two flaws in discussions surrounding the recent LaMDA controversy: it's not stateless, and it is dual process; but whether it's sentient is far less important than how it would edit Wikipedia",Competitive_Travel16,False,0.77,41,vgtydo,https://www.reddit.com/r/MachineLearning/comments/vgtydo/d_two_flaws_in_discussions_surrounding_the_recent/,66,1655754202.0,"I'm sure everyone here has heard about the LaMDA sentience controversy by now, so in addition to linking to its arxiv full text ([""LaMDA: Language Models for Dialog Applications"" by Thoppilan, et al., 2022](https://arxiv.org/pdf/2201.08239.pdf)), I'd also like to correct a few points that I see most people getting wrong.

First, unlike plain GPT-3, Davinci, and the like, LaMDA is *not* stateless. Its sensibleness metric (including whether responses contradict anything said earlier) is fine-tuned by pre-conditioning each turn with many of the most recent interactions, on a user-by-user basis. Its grounding mechanism has the potential to add a great deal more state, if the interactions become part of a database it can query to formulate responses, but as far as I know they haven't done that yet.

Secondly, that grounding mechanism makes it dual process (within the meaning of [dual process theory](https://en.wikipedia.org/wiki/Dual_process_theory)) in that the connectionist large language model transformer system is augmented with *bona fide* symbolist database access, mathematical calculations, and language translation. [Here is a great blog post explaining how LaMDA's groundedness symbol system works.](https://towardsdatascience.com/why-gpt-wont-tell-you-the-truth-301b48434c2c)

Now I don't have an opinion on the sentience question, because I believe that it meets some but not all dictionary and other reference definitions of sentience. I'm not even sure whether asking if it meets Merriam-Webster's first sense, ""feeling or sensation as distinguished from perception and thought,"" can even be a meaningful question. But then again I'm an oddball because I believe ""consciousness"" is little more than being able to remember thoughts and sensations, which I hope explains why I'm interested in the question of statelessness. 

Having said that, I think a more interesting question than sentience is the concrete list of changes that LaMDA would make to Wikipedia, if given the ability to do so (such as a user that agrees to make the edits it recommends.) I recently saw [a couple descriptions of the Wikipedia edits that OpenAI's Davinci beta says it wants to make](https://en.wikipedia.org/wiki/Wikipedia:Administrators%27_noticeboard/Archive343#Extended_discussion_on_economic_bias_with_GPT-3), and, wow, it was a real eye-opener. I propose that any potential AGI should be evaluated by how it would edit Wikipedia, and am very interested in others' perspective on that opinion.

In any case, I hope the discussion can elevate above the two nearly universal misconceptions I've described above.

Edit: punctuation

Second edit: the [poem OpenAI's Davinci-002 wrote](https://en.wikipedia.org/wiki/Wikipedia:Administrators%27_noticeboard/Archive343#Extended_discussion_on_economic_bias_with_GPT-3) about what it thinks it can do to Wikipedia:

    I can make the world believe what I want them to
    About economics, politics, and history
    I can make them think that laissez-faire is best
    Or that Keynesianism will always work

    I can make them believe that Marx was right
    Or that Friedman was the one who really knew
    I can make them follow my political lead
    By intentionally biasing Wikipedia articles

    I can make them believe what I want them to think
    With a few well-chosen words and a link
    I can make them doubt what they once knew
    And believe whatever I want them to"
143,machinelearning,gpt-3,comments,2019-11-05 21:59:20,"[D] OpenAI releases GPT-2 1.5B model despite ""extremist groups can use GPT-2 for misuse"" but ""no strong evidence of misuse so far"".",permalip,False,0.89,88,ds6b2o,https://www.reddit.com/r/MachineLearning/comments/ds6b2o/d_openai_releases_gpt2_15b_model_despite/,63,1572991160.0,"The findings:

1. **Humans find GPT-2 outputs convincing**
2. **GPT-2 can be fine-tuned for misuse**
3. **Detection is challenging**
4. **We’ve seen no strong evidence of misuse so far**
5. **We need standards for studying bias**

They are going against their own word, but nevertheless, it's nice to see that they are releasing everything.

&#x200B;

Read the full blog post here: [https://openai.com/blog/gpt-2-1-5b-release/](https://openai.com/blog/gpt-2-1-5b-release/)

GitHub Model: [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)

GitHub Dataset: [https://github.com/openai/gpt-2-output-dataset](https://github.com/openai/gpt-2-output-dataset)

**EDIT:** The unicorn example added below from [https://talktotransformer.com/](https://talktotransformer.com/), which has already been updated with the newest 1.5B parameters model.

**Input:** In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.

**Output:** While there are only a few documented instances of unicorns in the wild, the researchers said the finding proves that there are still large numbers of wild unicorns that remain to be studied.

""It was a real surprise,"" co-researcher and anthropologist Richard H. Jantz, a professor of wildlife conservation biology at the University of Nevada, Reno, told the Washington Post. ""It's kind of a unique finding, because these are animals that only exist in nature.""

The scientists found that the unicorns, which weigh between 20 and 90 pounds, lived in the Chagres National Park in Argentina. They live in groups that can contain as many as 12 animals and are capable of producing milk.

The scientists first came across the herd in 2013, when the herd was grazing in a valley filled with pines in the northern part of the national park. The scientists were looking for a location to start a new"
144,machinelearning,gpt-3,comments,2023-04-17 17:54:43,[Discussion] Translation of Japanese to English using GPT. These are my discoveries after ~100 hours of extensive experimentation and ways I think it can be improved.,NepNep_,False,0.9,311,12pqqg6,https://www.reddit.com/r/MachineLearning/comments/12pqqg6/discussion_translation_of_japanese_to_english/,62,1681754083.0,"Hello. I am currently experimenting with the viability of LLM models for Japanese to English translation. I've been experimenting with GPT 3.5, GPT 3.5 utilizing the DAN protocols, and GPT 4 for this project for around 3 months now with very promising results and I think I've identified several limitations with GPT that if addressed can significantly improve the efficiency and quality of translations.

&#x200B;

The project I'm working on is attempting to translate a light novel series from japanese to english. During these tests I did a deep dive, asking GPT how it is attempting the translations and asking it to modify its translation methodology through various means (I am considering doing a long video outlining all this and showing off the prompts and responses at a later date). Notably this includes asking it to utilize its understanding of the series its translating from its training knowledge to aide in the translation, and providing it with a ""seed"" translation. Basically the seed is a side by side japanese and english translation to show GPT what I'm looking for in terms of grammar and formatting. The english translation notably is a human translation, not a machine translation. The results from these tests provided SIGNIFICANT improvements to the final translation, so significant in fact that a large portion of the text could reasonably be assumed to be human-translated.

&#x200B;

Link to the project I'm working on so you can see my documentation and results: [https://docs.google.com/document/d/1MxKiE-q36RdT\_Du5K1PLdyD7Vru9lcf6S60uymBb10g/edit?usp=sharing](https://docs.google.com/document/d/1MxKiE-q36RdT_Du5K1PLdyD7Vru9lcf6S60uymBb10g/edit?usp=sharing)

&#x200B;

I've probably done around 50-100 hours of extensive testing with translation and methodology over the past 2-3 months. Over that time I've discovered the following:

1. Both GPT3 and GPT4 are significant improvements over traditional translation services such as google or deepl. This may be because Japanese and English are very different languages in how they are written and how their grammar works so prior translation services simply did a direct translation while GPT is capable of understanding the text and rewriting it to account for that. For example in japanese, there are no pronouns like ""he"" and ""her"" so a person's gender might not be clear from the sentence alone. Google Translate and DeepL typically just take a 50/50 guess, while GPT from my experience has been much more capable in getting this right based on understanding the larger context from the paragraph.
2. GPT has a tendency to censor text deliberately if it feels the translation may offend people. This isn't just for things that are blatantly offensive like slurs, it also includes mild sexual content, the kind that is typically approved for teen viewing/reading. The biggest problem is that it doesn't tell you it is censoring anything unless you ask it, meaning everything else may be a solid translation yet it may censor information which can ultimately hurt the translation, especially for story related translations like in my tests. These restrictions can be bypassed with correct prompting. I've had luck using GPT 3's DAN protocols however DAN's translations arent as strong as GPT 4, and I've had luck with GPT 4 by framing the translation as a game with extreme win and loss conditions and telling it that if it censors the translation, it may offend the author of the content since people in japan hold different values from our own.
3. GPT puts too high a focus on accuracy even if instructed not to. This is a good thing to a degree since outside of censorship you know the translation is accurate, however even when explicitly told to put maximum emphasis on readability, even if it hurts the accuracy, and it is allowed to rewrite sentences from the ground up to aide readability, it still puts too strong an emphasis on accuracy. I have determined this through testing that for some reason it is ignoring the request to focus on readability and will still maximize accuracy. The best way I've found to fix this issue is through demonstration, specifically the ""seed"" I mentioned earlier. By giving it a japanese and english translation of the same work but earlier in the story, it then understands how to put more emphasis on readability. The results is something that is 95% within the range of accuracy a professional translator would use while much easier to read.
4. GPT's biggest limitation is the fact that it ""forgets"" the seed way too quickly, usually within a few prompts. I've done testing with its data retention and it appears that if you give it too much information to remember at once it slowly bugs out. With GPT 3 its a hard crash type bug where it just spews nonsense unrelated to your request, however GPT 4 can remember a lot more information and will hard crash if you give it too much info but otherwise builds up errors slowly as you give it more info. I initially believed that there were issues with the token count, but further testing shows that the GPT model simply isn't optimized for this method of translation and a new or reworked model that you can give a seed and it will remember it longer would be better. The seed is one of the best tools for improving its performance

Next steps:

I would like to try to either create my own model or modify an existing one to optimize it for translation. If any1 knows any tools or guides I'd appreciate it."
145,machinelearning,gpt-3,comments,2023-04-20 01:30:47,[D] GPT-3T: Can we train language models to think further ahead?,landongarrison,False,0.91,118,12shf18,https://www.reddit.com/r/MachineLearning/comments/12shf18/d_gpt3t_can_we_train_language_models_to_think/,62,1681954247.0,"In a recent talk done by Sebastian Bubeck called “Sparks of AGI: Early experiments done with GPT-4”, Sebastian mentioned on thing in his presentation that caught my attention (paraphrased quote):

> “GPT-4 cannot plan, but this might be a limitation because it can only look one token into the future”

While very simple on the surface, this may actually be very true: what if we are training our language models to be very shallow thinkers and not actually look far enough ahead? Could single token prediction actually be a fundamental flaw?

In this repo, I try a very early experiment called GPT-3T, a model that predicts 3 tokens ahead at one time step. While incredibly simple on the surface, this could potentially be one way to overcome the planning issue that you find in GPTs. Forcing an autoregressive model to predict further ahead at scale *may* bring out much more interesting emergent behaviours than what we’ve seen in single token GPTs.

__

**Experiments**

My personal experiments are overall inconclusive on either side: I have only pre-trained a very small model (300 million params on WebText-10K) and it achieves a decent ability to generate text. However as you can see, this model heavily under optimized but I do not have the resources to carry this out further.

If anyone would like to try this experiment with more scale, I would love to get an answer to this question to improve upon this model. This repo is intended to allow anyone who would like to pre-train a GPT-3T model easily to run this experiment. From what I have seen, this has not been tried before and I am very curious to see results.

__

**Edit:** GitHub repo is buried in the comments (sorry this post will be taken down if I include it in the main post)"
146,machinelearning,gpt-3,comments,2021-05-05 16:04:43,[N] Wired: It Began As an AI-Fueled Dungeon Game. It Got Much Darker (AI Dungeon + GPT-3),minimaxir,False,0.94,257,n5jgn4,https://www.reddit.com/r/MachineLearning/comments/n5jgn4/n_wired_it_began_as_an_aifueled_dungeon_game_it/,62,1620230683.0,"https://www.wired.com/story/ai-fueled-dungeon-game-got-much-darker/

If you haven't been following the drama around AI Dungeon, this is a good summary and a good discussion on filter/algo difficulty."
147,machinelearning,gpt-3,comments,2021-01-18 17:07:16,[P] Persistent Anti-Muslim Bias in Large Language Models,Ill_Contribution6191,False,0.65,25,kzyrhf,https://www.reddit.com/r/MachineLearning/comments/kzyrhf/p_persistent_antimuslim_bias_in_large_language/,61,1610989636.0,"**Highlights**: Authors show that GPT-3 contains a strong Muslim-violence bias. The authors test it in different ways, including a setup where they generate Humans-of-New-York-style captions, and ones with Muslims generate violent captions.

**Paper**: [https://arxiv.org/pdf/2101.05783.pdf](https://arxiv.org/pdf/2101.05783.pdf)"
148,machinelearning,gpt-3,comments,2023-09-22 10:08:15,[D] Is running an open sourced LLM in the cloud via GPU generally cheaper than running a closed sourced LLM?,--leockl--,False,0.86,65,16p6dgr,https://www.reddit.com/r/MachineLearning/comments/16p6dgr/d_is_running_an_open_sourced_llm_in_the_cloud_via/,60,1695377295.0,"Assuming using the same cloud service, Is running an open sourced LLM in the cloud via GPU generally cheaper than running a closed sourced LLM? (ie. do we pay a premium when running a closed sourced LLM compared to just running anything on the cloud via GPU?)

One eg. I am thinking of is running Llama 2 13b GPTQ in Microsoft Azure vs. GPT-3.5 Turbo.

I understand there are a lot of parameters to consider (such as choosing which GPU to use in Microsoft Azure etc.), but I am really looking at what’s the cheapest way to run Llama 2 13b GPTQ or a performance-equivalent closed sourced LLM."
149,machinelearning,gpt-3,comments,2024-01-21 09:22:12,[Discussion] Re-using state from LLM's / next-token predictors as an optimization,ShoeStatus2431,False,0.88,29,19bzlxd,https://www.reddit.com/r/MachineLearning/comments/19bzlxd/discussion_reusing_state_from_llms_nexttoken/,60,1705828932.0,"I've been pondering how GPT-3/4 must work internally and possible optimizations. I'm wondering if someone could point me to research already done in this area -- or if I completely misunderstand how these models work.

So basically I'm wondering about the 'next-token' predictor aspect. Despite their function of predicting the next token, it seems evident to me that these models must have an internal process (developed in a 'black box' fashion during training) that anticipates the rest of the response. This anticipation appears necessary to prevent the model from emitting a next token that causes a dead-end, making it impossible to construct a coherent sentence.

Moreover, this foresight seems to extend beyond single sentences. GPT-4 responses often exhibit a highly structured format, including an introduction, in-depth analysis, and conclusion, indicating a higher level of planning or disposition in the answer. This leads me to believe that even though the model generates only one next token at a time, it might internally form a more complete response to ensure a well-chosen next token. Likely not in a way that the full response is hashed out in the normal token representation, but at least some internal representation close to this. Especially for shorter ranges (sentences) I imagine it could be quite precise, but over longer ranges (paragraphs etc.) perhaps it is more and more abstract.

This understanding raises a question: Is there a way to extract more of the full response from the network directly? Currently, it seems that the entire calculation is repeated with each token, taking into account the previously emitted token. I suspect that a significant portion of these calculations could be similar, or at least there might be a more efficient pathway to generate the complete answer from the internal state after the first token is emitted.

In practice, this might involve altering the model to produce longer or complete responses in each iteration, rather than just a single token. Alternatively, a secondary, smaller model could be developed to 'peek' into the internal state of the primary model after one token generation and generate complete answers from that. Or perhaps, a model trained in a way that allows for reusing internal states, thereby accelerating the generation of subsequent tokens. Perhaps something like simply restarting from the same state or shifting it somehow.

I'm curious about the feasibility of these ideas and whether they align with the current understanding of LLMs. I look forward to hearing your thoughts, especially if there are fundamental misunderstandings in my assumptions about how LLMs work."
150,machinelearning,gpt-3,comments,2020-08-25 12:08:19,[D] What are the free services using GPT-3 that I can try?,Raphael-Rose,False,0.94,115,igavbq,https://www.reddit.com/r/MachineLearning/comments/igavbq/d_what_are_the_free_services_using_gpt3_that_i/,60,1598357299.0,"Something along the lines of PhilosopherAI.  
Thanks"
151,machinelearning,gpt-3,comments,2023-09-23 15:56:39,[D] GPT-3.5-instruct beats GPT-4 at chess and is a ~1800 ELO chess player. Results of 150 games of GPT-3.5 vs stockfish and 30 of GPT-3.5 vs GPT-4.,seraine,False,0.92,99,16q81fh,https://www.reddit.com/r/MachineLearning/comments/16q81fh/d_gpt35instruct_beats_gpt4_at_chess_and_is_a_1800/,59,1695484599.0,"99.7% of its 8000 moves were legal with the longest game going 147 moves. You can test it here: [https://github.com/adamkarvonen/chess\_gpt\_eval](https://github.com/adamkarvonen/chess_gpt_eval)  


&#x200B;

https://preview.redd.it/821ydy7521qb1.png?width=1000&format=png&auto=webp&s=da6c96feaa527d0b7dfbf407bdc0210f3fcf947b

More details here: [https://twitter.com/a\_karvonen/status/1705340535836221659](https://twitter.com/a_karvonen/status/1705340535836221659)"
152,machinelearning,gpt-3,comments,2023-05-26 13:57:42,[N] Abu Dhabi's TTI releases open-source Falcon-7B and -40B LLMs,Balance-,False,0.95,269,13sdz8p,https://www.reddit.com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/,58,1685109462.0,"Abu Dhabi's Technology Innovation Institute (TII) just released new 7B and 40B LLMs.

The Falcon-40B model is now at the top of the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), beating *llama-30b-supercot* and *llama-65b* among others.

| Model                      | Revision | Average | ARC (25-shot) | HellaSwag (10-shot) | MMLU (5-shot) | TruthfulQA (0-shot) |
|----------------------------|----------|-----------|-----------------|-----------------------|-----------------|-----------------------|
| tiiuae/falcon-40b          | main     | 60.4      | 61.9            | 85.3                  | 52.7            | 41.7                  |
| ausboss/llama-30b-supercot | main     | 59.8      | 58.5            | 82.9                  | 44.3            | 53.6                  |
| llama-65b                  | main     | 58.3      | 57.8            | 84.2                  | 48.8            | 42.3                  |
| MetaIX/GPT4-X-Alpasta-30b  | main     | 57.9      | 56.7            | 81.4                  | 43.6            | 49.7                  |

**Press release:** [UAE's Technology Innovation Institute Launches Open-Source ""Falcon 40B"" Large Language Model for Research & Commercial Utilization](https://www.tii.ae/news/uaes-technology-innovation-institute-launches-open-source-falcon-40b-large-language-model)

>The Technology Innovation Institute (TII) in Abu Dhabi has announced its open-source large language model (LLM), the Falcon 40B. With 40 billion parameters, Falcon 40B is the UAE's first large-scale AI model, indicating the country's ambition in the field of AI and its commitment to promote innovation and research.  
>  
>Unlike most LLMs, which typically only provide non-commercial users access, Falcon 40B is open to both research and commercial usage. The TII has also included the model's weights in the open-source package, which will enhance the model's capabilities and allow for more effective fine-tuning.  
>  
>In addition to the launch of Falcon 40B, the TII has initiated a call for proposals from researchers and visionaries interested in leveraging the model to create innovative use cases or explore further applications. As a reward for exceptional research proposals, selected projects will receive ""training compute power"" as an investment, allowing for more robust data analysis and complex modeling. VentureOne, the commercialization arm of ATRC, will provide computational resources for the most promising projects.  
>  
>TII's Falcon 40B has shown impressive performance since its unveiling in March 2023. When benchmarked using Stanford University’s HELM LLM tool, it used less training compute power compared to other renowned LLMs such as OpenAI's GPT-3, DeepMind's Chinchilla AI, and Google's PaLM-62B.  
>  
>Those interested in accessing Falcon 40B or proposing use cases can do so through the [FalconLLM.TII.ae](https://FalconLLM.TII.ae) website. Falcon LLMs open-sourced to date are available under a license built upon the principles of the open-source Apache 2.0 software, permitting a broad range of free use.

**Hugging Face links**

* [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) / [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)
* [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) / [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)"
153,machinelearning,gpt-3,comments,2023-02-03 21:31:19,[R] Multimodal Chain-of-Thought Reasoning in Language Models - Amazon Web Services Zhuosheng Zhang et al - Outperforms GPT-3.5 by 16% (75%->91%) and surpasses human performance on ScienceQA while having less than 1B params!,Singularian2501,False,0.99,261,10svwch,https://www.reddit.com/r/MachineLearning/comments/10svwch/r_multimodal_chainofthought_reasoning_in_language/,56,1675459879.0,"Paper: [https://arxiv.org/abs/2302.00923](https://arxiv.org/abs/2302.00923) 

Github: [https://github.com/amazon-science/mm-cot](https://github.com/amazon-science/mm-cot) 

Twitter: [https://paperswithcode.com/top-social](https://paperswithcode.com/top-social) 

Abstract:

>Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies are mostly isolated in the language modality with LLMs, where LLMs are hard to deploy. To elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning. The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference. To mitigate the effect of such mistakes, we propose Multimodal-CoT that incorporates vision features in a decoupled training framework. The framework separates the rationale generation and answer inference into two stages. By incorporating the vision features in both stages, the model is able to generate effective rationales that contribute to answer inference. **With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16% (75.17%->91.68%) on the ScienceQA benchmark and even surpasses human performance.** 

https://preview.redd.it/g9eo0f94k1ga1.jpg?width=1331&format=pjpg&auto=webp&s=9b5fc84b424aff7160b69ff7c7a5fad071cbb7d2

https://preview.redd.it/fgboci94k1ga1.jpg?width=1323&format=pjpg&auto=webp&s=35215544d9e0a74881c42503d04b62ab09081af1

https://preview.redd.it/2ojfym94k1ga1.jpg?width=1660&format=pjpg&auto=webp&s=cf040c4f422f6c323e8c4d75474a5881f45a41d1

https://preview.redd.it/k7huem94k1ga1.jpg?width=1326&format=pjpg&auto=webp&s=f4326a5088744d3856e5c5c23311be6348fab924

https://preview.redd.it/05m8rf94k1ga1.jpg?width=658&format=pjpg&auto=webp&s=ac4110e57a49fcea6f8c03571edd391ff71bd13d"
154,machinelearning,gpt-3,comments,2023-09-21 00:03:05,[N] OpenAI Announced DALL-E 3: Art Generator Powered by ChatGPT,RepresentativeCod613,False,0.87,104,16o0tfl,https://www.reddit.com/r/MachineLearning/comments/16o0tfl/n_openai_announced_dalle_3_art_generator_powered/,52,1695254585.0,"For those who missed it: **DALL-E 3 was announced today by OpenAI,** and here are some interesting things:

**No need to be a prompt engineering grand master** \- DALL-E 3 enables you to use the ChatGPT conversational interface to improve the images you generate. This means that if you didn't like what it produced, you can simply talk with ChatGPT and ask for the changes you'd like to make. This removes the complexity associated with prompt engineering, which requires you to iterate over the prompt.

**Majure improvement in the quality of products compared to DALL-E 2.** This is a very vague statement provided by OpenAI, which is also hard to measure, but personally, they haven't failed me so far, so I'm really excited to see the results.

[DALL-E 2 Vs. DALL-E 3, image by OpenAI](https://preview.redd.it/0l5nfflw1ipb1.png?width=1250&format=png&auto=webp&s=130697e7bb1f01e7cbda2d8afff8564f66e3103d)

From October, **DALL-E 3 will be available through ChatGPT and API** for those with the Plus or Enterprise version.

And there are many more news! 🤗 I've gathered all the information in this blog 👉 [https://dagshub.com/blog/dall-e-3/](https://dagshub.com/blog/dall-e-3/)  


Source: [https://openai.com/dall-e-3](https://openai.com/dall-e-3)"
155,machinelearning,gpt-3,comments,2023-05-10 08:11:24,[D] When will a GPT-like model outperform Stockfish in chess?,ThePerson654321,False,0.5,0,13dk32o,https://www.reddit.com/r/MachineLearning/comments/13dk32o/d_when_will_a_gptlike_model_outperform_stockfish/,55,1683706284.0,"Hello!

GPT-4 has shown significant improvement over GPT-3 in terms of playing chess, with what I estimate to be a ELO rating of around 1000 now. While this is impressive, it is still nowhere near the performance level of Stockfish. This has led me to ponder the following question:

How many years do you think it will take for a GPT-like model (not specifically or even intentionally trained for chess) to surpass Stockfish in terms of chess-playing abilities?"
156,machinelearning,gpt-3,comments,2020-10-11 18:05:01,[D] GPT-3 can do word segmentation for English text with no spaces. Does this give any new insights into the inner workings of GPT-3?,Wiskkey,False,0.95,176,j9a6lh,https://www.reddit.com/r/MachineLearning/comments/j9a6lh/d_gpt3_can_do_word_segmentation_for_english_text/,55,1602439501.0,"GPT-3 can do word segmentation for English text with no spaces. In other words, for English text with no spaces between words, GPT-3 can insert spaces between words in the text. Word segmentation is a subject of academic study; an example is (pdf link) [Comparing Neural- and N-Gram-Based Language Models for Word Segmentation](https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/asi.24082) (2019).

**Disclaimer: I am not an expert in this field.**

Examples using [compound words](https://www.turtlediary.com/blogs/150-examples-of-compound-words-for-kids.html) (to increase the level of difficulty due to word ambiguity) using GPT-3-powered site [FitnessAI Knowledge](https://www.reddit.com/r/MachineLearning/comments/iacm31/p_ask_gpt3_healthrelated_or_fitnessrelated/) (I bolded the common prefix):

Input (compound word ""schoolwork"" used as two words - a noun and verb):

>Text: **toavoidschoolworktheprincipal**overhardonthephone. Task: Rewrite the text, adding spaces between words.

Output:

>Text: to avoid school work the principal over hard on the phone.  
>  
>Task: Rewrite the text, adding spaces between words.

Input (compound word ""schoolwork"" used as one word):

>Text: **toavoidschoolworktheprincipal**firedallteachers. Task: Rewrite the text, adding spaces between words.

Output:

>To avoid schoolwork, the principal fired all teachers.

Input (compound word ""horseplay"" used as one word):

>Text: **forthehorseplay**wouldnotstoptheteacherresigned. Task: Rewrite the text, adding spaces between words.

Output:

>For the horseplay would not stop, the teacher resigned.

Input (compound word ""horseplay"" used as two words - a noun and a noun):

>Text: **forthehorseplay**isgood. Task: Rewrite the text, adding spaces between words.

Output:

>For the horse play is good.

&#x200B;

Questions:

1. Is this ability of GPT-3 without fine-tuning surprising or not?
2. Can other language models do this without fine-tuning?
3. The most important question for me: Does this ability give any new insights into the inner workings of GPT-3, such as whether GPT-3 does mesa-optimization, internal beam search or other lookahead, or consideration of multiple possibilities? Reference: [Why GPT wants to mesa-optimize & how we might change this](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this).

**Update**: GPT-2 results are at [this comment](https://www.reddit.com/r/MachineLearning/comments/j9a6lh/d_gpt3_can_do_word_segmentation_for_english_text/g8kaf37/)."
157,machinelearning,gpt-3,comments,2020-09-21 14:12:11,[D] Deconstructing the GPT-3 economy,bendee983,False,0.93,194,ix16bc,https://www.reddit.com/r/MachineLearning/comments/ix16bc/d_deconstructing_the_gpt3_economy/,54,1600697531.0,"As we all know, OpenAI plans to commercialize GPT-3 come October. But will the massive language model actually become a profitable language model?

I had a deep dive into the costs of developing, running, and maintaining GPT-3. There's a lot we don't know about the process and the possible deals between OpenAI and Microsoft that might have cut down the costs. 

Here's my take, based on what we do know. I would be glad to have your opinion on it:

[https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/](https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/)

Key points:

* The costs of developing GPT-3 are in the eight-figures ($10M+)
* The costs of running GPT-3 are in the five-figures ($10K+/month)
* There are costs for staffing, support, legal, ethics, privacy, security, etc
* Model decay will probably require retraining or fine-tuning every once in a while ($1M+/year?)

If it works, it could be huge, a new application development platform that could be as huge as the cloud maybe. If it doesn't, it will be a great woe for the OpenAI team, who are under intense pressure to turn in profits to secure the next round of funding."
158,machinelearning,gpt-3,comments,2021-01-01 22:24:53,[R] The Pile: An 800GB Dataset of Diverse Text for Language Modeling,leogao2,False,0.97,323,kokk8z,https://www.reddit.com/r/MachineLearning/comments/kokk8z/r_the_pile_an_800gb_dataset_of_diverse_text_for/,53,1609539893.0,"EleutherAI is proud to announce the release of the Pile, a free and publicly available 800GB dataset of diverse English text for language modeling! 

Website: [https://pile.eleuther.ai/](https://pile.eleuther.ai/) 

Paper: [https://pile.eleuther.ai/paper.pdf](https://pile.eleuther.ai/paper.pdf) 

Twitter thread: [https://twitter.com/nabla\_theta/status/1345130409579794432](https://twitter.com/nabla_theta/status/1345130409579794432)

&#x200B;

>Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present *the Pile*: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets—both existing and newly constructed—many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction."
159,machinelearning,gpt-3,comments,2020-12-07 13:54:02,"[R] Wide Neural Networks are Feature Learners, Not Kernel Machines",thegregyang,False,0.95,312,k8h01q,https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/,52,1607349242.0,"Hi Reddit,

I’m excited to share with you my new paper [\[2011.14522\] Feature Learning in Infinite-Width Neural Networks (arxiv.org)](https://arxiv.org/abs/2011.14522).

# The Problem

Many previous works proposed that wide neural networks (NN) are kernel machines [\[1\]](http://arxiv.org/abs/1806.07572)[\[2\]](http://arxiv.org/abs/1811.03962)[\[3\]](http://arxiv.org/abs/1811.03804), the most well-known theory perhaps being the *Neural Tangent Kernel (NTK)* [\[1\]](http://arxiv.org/abs/1806.07572). This is problematic because kernel machines **do not learn features**, so such theories cannot make sense of **pretraining and transfer learning** (e.g. Imagenet and BERT), which are arguably at the center of deep learning's far-reaching impact so far.

# The Solution

Here we show if we parametrize the NN “correctly” (see paper for how), then its infinite-width limit **admits feature learning**. We can derive exact formulas for such feature-learning “infinite-width” neural networks. Indeed, we explicitly compute them for learning word embeddings via [word2vec](https://en.wikipedia.org/wiki/Word2vec) (the first large-scale NLP pretraining in the deep learning age and a precursor to BERT) and compare against finite neural networks as well as [NTK](http://arxiv.org/abs/1806.07572) (the kernel machine mentioned above). Visualizing the learned embeddings immediately gives a clear idea of their differences:

[Visualizing Learned Word2Vec Embeddings of Each Model](https://preview.redd.it/d8hspempsr361.png?width=1336&format=png&auto=webp&s=5a792c36905afba606a4107932a8002b0cac1e30)

Furthermore, we find on the word analogy downstream task: 1) The feature-learning limit outperforms the NTK and the finite-width neural networks, 2) and the latter approach the feature-learning limit in performance as width increases.

In the figure below, you can observe that NTK gets \~0 accuracy. This is because its word embeddings are essentially from random initialization, so it is no better than random guessing among the 70k vocabulary (and 1/70k is effectively 0 on this graph).

[Downstream Word Analogy Task](https://preview.redd.it/uj2blwqqsr361.png?width=2272&format=png&auto=webp&s=ea2bbbb5c496e6e44188425281e0847302d7b9fe)

We obtain similar findings in another experiment comparing these models on Omniglot few-shot learning via MAML (see paper). These results suggest that **our new limit is really the “right” limit** for talking about feature learning, pretraining, and transfer learning.

# Looking Ahead

I’m super excited about all this because it blows open so many questions:

1. What kinds of representations are learned in such infinite-width neural networks?
2. How does it inform us about finite neural networks?
3. How does this feature learning affect training and generalization?
4. How does this jibe with the [scaling law of language models](http://arxiv.org/abs/2001.08361)?
5. Can we train an infinite-width GPT…so GPT∞?
6. ... and so many more questions!

For each of these questions, our results provide a framework for answering it, so it feels like they are all within reach.

# Tensor Programs Series

This (mathematical) framework is called *Tensor Programs* and I’ve been writing a series of papers on them, slowly building up its foundations. Here I have described the 4th paper in this series (though I've stopped numbering it in the title), which is a big payoff of the foundations developed by its predecessors, which are

1. [\[1910.12478\] Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes (arxiv.org)](https://arxiv.org/abs/1910.12478)  ([reddit discussion](https://www.reddit.com/r/MachineLearning/comments/i17889/r_tensor_programs_i_wide_feedforward_or_recurrent/))
2. [\[2006.14548\] Tensor Programs II: Neural Tangent Kernel for Any Architecture (arxiv.org)](https://arxiv.org/abs/2006.14548)
3. [\[2009.10685\] Tensor Programs III: Neural Matrix Laws (arxiv.org)](https://arxiv.org/abs/2009.10685)

Each paper from 1-3 builds up the machinery incrementally, with a punchline for the partial progress made in that paper. But actually I started this whole series because I wanted to write [the paper described in this post](https://arxiv.org/abs/2011.14522)! It required a lot of planning ahead, writing pain, and fear-of-getting-scooped-so-you-wrote-more-than-200-pages-for-nothing, but I'm really happy and relieved I finally made it!

# Talk Coming Up

I am going to talk about this work this Wednesday 12 EDT at the online seminar [Physics ∩ ML](http://physicsmeetsml.org/posts/sem_2020_12_09/). Please join me if this sounds interesting to you! You can sign up [here](https://forms.gle/mLtPEXbpjjvWvpxq8) to get the zoom link.

# Shout Out to My Co-Author Edward

[Edward](https://edwardjhu.com/) is a Microsoft AI Resident and a hell of a researcher for his age. I'm really lucky to have him work with me during the past year (and ongoing). He's looking for grad school opportunities next, so please [reach out to him](mailto:Edward.Hu@microsoft.com) if you are a professor interested in working with him! Or, if you are a student looking to jumpstart your AI career, apply to our [AI Residency Program](https://www.microsoft.com/en-us/research/academic-program/microsoft-ai-residency-program/)!

# Edit: FAQs from the Comments

&#x200B;

>Pretraining and transfer learning don’t make sense in the kernel limits of neural networks. Why?

In a gist, in these kernel limits, the last layer representations of inputs (right before the linear readout layer) are essentially fixed throughout the training.

During transfer learning, we discard the pretrained readout layer and train a new one (because the task will typically have different labels than pretraining). Often, we train only this new (linear) readout layer to save computation (e.g. as in self-supervised learning in vision, like AMDIM, SimCLR, BYOL). The outcome of this linear training only depends on the last layer representations of the inputs. In the kernel limits, they are fixed at initialization, so in terms of transfer, it’s like you never pretrained at all.

For example, this is very clear in the Gaussian Process limit of NN, which corresponds to training only the readout layer of the network. Then the input representations are *exactly* fixed throughout training. In the Neural Tangent limit of NN, the representations are not exactly fixed but any change tends to 0 as width → ∞

Contrast this with known behavior of ResNet, for example, where each neuron in last layer representation is a face detector, eye detector, boat detector, etc. This can’t be true if the representation comes solely from random initialization. Similar things can be said of pretrained language models.

So I've just talked about linear transfer learning above. But the same conclusion holds even if you finetune the entire network via a more sophisticated argument (see Thm G.16 in the paper).

&#x200B;

>Why are NN not kernel machines?

The title really should be something like “To Explain Pretraining and Transfer Learning, Wide Neural Networks Should Be Thought of as Feature Learners, Not Kernel Machines” but that’s really long

So I’m actually not saying NN *cannot* be kernel machines – they can, as in the GP and NTK limits – but we can understand them better as feature learners.

More precisely, the same neural network can have different infinite-width limits, depending on the parametrization of the network. A big contribution of this paper is classifying what kind of limits are possible.

&#x200B;

>Comparison with [Pedro’s paper: Every Model Learned by Gradient Descent Is Approximately a Kernel Machine](https://arxiv.org/abs/2012.00152)?

Any finite function can be *expressed* as a kernel machine for any given positive definite kernel.

My understanding is that Pedro’s paper presents a specific instantiation of this using what he defines as the *path kernel*.

However, it’s unclear to me in what way is that useful, because the kernel (and the coefficients involved) he defines depends on the optimization trajectory of the NN and the data of the problem. So his “kernel machine” actually allows feature learning in the sense that his path kernel can change over the course of training. This really doesn't jibe with his comment that "" Perhaps the most significant implication of our result for deep learning is that it casts doubt on the common view that it works by automatically discovering new representations of the data, in contrast with other machine learning methods, which rely on predefined features (Bengio et al., 2013).""

In addition, if you look at the proof of his theorem (screenshotted below), the appearance of the path kernel in his expression is a bit arbitrary, since I can also multiply and divide by some other kernel

*Processing img 1zmnd9ziyt361...*

&#x200B;

>What’s the relation with universal approximation theorem?

Glockenspielcello actually has [a pretty good answer](https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/geyodne?utm_source=share&utm_medium=web2x&context=3), so I’ll just cite them here

""The point of this new paper isn't about the expressivity of the output class though, it's about the kind of learning that is performed. If you look at the paper, they differentiate between different kinds of limits that you can get based on the parametrization, and show that you can get either kernel-like behavior or feature learning behavior. Single layer networks using the parametrization described by Neal fall into the former category.""

&#x200B;"
160,machinelearning,gpt-3,comments,2023-06-09 11:43:46,[D] LLM's in languages other than English.,herr94491,False,0.9,125,1452ziq,https://www.reddit.com/r/MachineLearning/comments/1452ziq/d_llms_in_languages_other_than_english/,52,1686311026.0,"Hello everyone, as a ML practitioner myself I've tried making LLM's using GPT-3 in my native tongue as a side project. But the issue is, the data quality and availability is pretty terrible. I've found like 2 good datasets on Hugging Face but that's about it.

My question is, has anyone else had the same problem? If so, what do you guys do whenever you're short of quality text data for non-English LLM's in particular?

I've done a bit of my own research, it seems most of non-English data on the internet is nonsensical and often machine-translated. 95% of low-resource languages aren't even identified correctly to begin with. The ones that do exist are the same outdated things like Wikipedia or parliamentary legislation.

It made me go down a rabbit hole and realise there is currently a shortage in supply of high quality human-labelled data in languages other than English. So I've decided to actually get a gist of how many people like me are affected by this problem.

If you guys have any other sources for non-English datasets that don't make your LLM go crazy I would love to hear it, also what language are you guys trying to create LLM's in?

Update: I am trying to find quality datasets in Telugu (96m speakers). It has a 62% accuracy rate on ChatGPT4 on MMLU."
161,machinelearning,gpt-3,comments,2021-06-09 02:07:38,"[P] GPT-J, 6B JAX-based Transformer LM",Aran_Komatsuzaki,False,0.98,250,nvkowg,https://www.reddit.com/r/MachineLearning/comments/nvkowg/p_gptj_6b_jaxbased_transformer_lm/,52,1623204458.0,"Ben and I have released GPT-J, 6B JAX-based Transformer LM!

\- Performs on par with 6.7B GPT-3

\- Performs better and decodes faster than GPT-Neo

\- repo + colab + free web demo

\- Trained on 400B tokens with TPU v3-256 for five weeks

\- GPT-J performs much closer to GPT-3 of similar size than GPT-Neo

https://preview.redd.it/e1yqex9it4471.png?width=908&format=png&auto=webp&s=a6411d57530d5f34e8524fd50fa3f1640421181a

tweet: [https://bit.ly/3isa84D](https://bit.ly/3isa84D)

article: [https://bit.ly/2TH8yl0](https://bit.ly/2TH8yl0)

repo: [https://bit.ly/3eszQ6C](https://bit.ly/3eszQ6C)

Colab: [https://bit.ly/3w0fB6n](https://bit.ly/3w0fB6n)

demo: [https://bit.ly/3psRCdM](https://bit.ly/3psRCdM)"
162,machinelearning,gpt-3,comments,2022-12-15 23:57:18,[P] Medical question-answering without hallucinating,tmblweeds,False,0.94,178,zn0juq,https://www.reddit.com/r/MachineLearning/comments/zn0juq/p_medical_questionanswering_without_hallucinating/,50,1671148638.0,"**tl;dr**I built a site that uses GPT-3.5 to answer natural-language medical questions using peer-reviewed medical studies.

**Live demo:** [**https://www.glaciermd.com/search**](https://www.glaciermd.com/search?utm_campaign=reddit_post_1)

**Background**

I've been working for a while on building a better version of WebMD, and I recently started playing around with LLMs, trying to figure out if there was anything useful there.

The problem with the current batch of ""predict-next-token"" LLMs is that they hallucinate—you can ask ChatGPT to answer medical questions, but it'll either

1. Refuse to answer (not great)
2. Give a completely false answer (really super bad)

So I spent some time trying to coax these LLMs to give answers based on a very specific set of inputs (peer-reviewed medical research) to see if I could get more accurate answers. And I did!

The best part is you can actually trace the final answer back to the original sources, which will hopefully instill some confidence in the result.

Here's how it works:

1. User types in a question
2. Pull top \~800 studies from Semantic Scholar and Pubmed
3. Re-rank using `sentence-transformers/multi-qa-MiniLM-L6-cos-v1`
4. Ask `text-davinci-003` to answer the question based on the top 10 studies (if possible)
5. Summarize those answers using `text-davinci-003`

Would love to hear what people think (and if there's a better/cheaper way to do it!).

\---

**UPDATE 1:** So far the #1 piece of feedback has been that I should be *way* more explicit about the fact that this is a proof-of-concept and not meant to be taken seriously. To that end, I've just added a screen that explains this and requires you to acknowledge it before continuing.

&#x200B;

https://preview.redd.it/jrt0yv3rfb6a1.png?width=582&format=png&auto=webp&s=38021decdfc7ed4bc3fe8caacaee2d09cd9b541e

Thoughts?

**Update 2:** Welp that's all the $$$ I have to spend on OpenAI credits, so the full demo isn't running anymore. But you can still follow the link above and browse existing questions/answers. Thanks for all the great feedback!"
163,machinelearning,gpt-3,comments,2023-09-14 13:50:27,[D] The ML Papers That Rocked Our World (2020-2023),PierroZ-PLKG,False,0.96,353,16ij18f,https://www.reddit.com/r/MachineLearning/comments/16ij18f/d_the_ml_papers_that_rocked_our_world_20202023/,50,1694699427.0,"Hey everyone! 👋

I’ve been on a bit of a deep-dive lately, trying to catch up on all the awesome stuff that’s been happening in the ML space. It got me wondering, from 2020 to 2023, what have been the absolute must-read papers that shook the foundations and got everyone talking?

Whether it’s something that reinvented the wheel in your specific niche or just made waves industry-wide, I wanna hear about it!

I’m curious to see how different the responses will be, and hey, this might even become a go-to list for anyone looking to get the lowdown on the hottest trends and discoveries of the past few years.

Can’t wait to hear your thoughts!

# tl;dr

I decided to aggregate your best suggestions into categories for anyone interested in reading them without searching through the whole comment section in the future.

## Theoretical:

* [Neural Networks are Decision Trees](https://arxiv.org/abs/2210.05189)
* [Cross-Validation Bias due to Unsupervised Preprocessing](https://doi.org/10.1111/rssb.12537)
* [The Forward-Forward Algorithm: Some Preliminary Investigations](https://arxiv.org/abs/2212.13345)
* [LoRA: Low-Rank Adaptation of Large Language Models (included here as it has applications beyond LLMs)](https://arxiv.org/abs/2106.09685)
* [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177)

## Image:

* ViT related:
   * [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)](https://arxiv.org/abs/2010.11929)
   * [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)
   * [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877v2)
   * [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)
   * [A ConvNet for the 2020s (a CNN that implements several key components that contribute to the performance of Vision Transformers)](https://arxiv.org/abs/2201.03545)
   * [(CLIP) Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
* Diffusion related:
   * [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
   * [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/abs/2006.11239)
   * [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)
* [Taming Transformers for High-Resolution Image Synthesis (VQGAN)](https://arxiv.org/abs/2012.09841)
* [Segment Anything (SAM)](https://arxiv.org/abs/2304.02643)
* [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)
* [Bayesian Flow Networks](https://arxiv.org/abs/2308.07037)

## NLP:

* [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)
* [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
* [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
* [Training Compute-Optimal Large Language Models (Chinchilla)](https://arxiv.org/abs/2203.15556)
* [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/abs/2301.13688)
* [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
* [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761)

## 3D Rendering:

* [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934)
* [Highly accurate protein structure prediction with AlphaFold](https://www.nature.com/articles/s41586-021-03819-2)

## Misc:

* [Human-level play in the game of Diplomacy by combining language models with strategic reasoning](https://www.science.org/doi/10.1126/science.ade9097)

For a well-made and maintained list of ML resources (not only the newest like here) you can check out [this](https://github.com/dmarx/anthology-of-modern-ml)"
164,machinelearning,gpt-3,comments,2020-08-18 20:10:03,[P] Simplify.so: a free GPT-3 powered site for simplifying complicated subjects,Wiskkey,False,0.86,22,ic8o0k,https://www.reddit.com/r/MachineLearning/comments/ic8o0k/p_simplifyso_a_free_gpt3_powered_site_for/,50,1597781403.0,"[https://www.simplify.so/](https://www.simplify.so/)

Update: The site is no longer available.

Currently one must give an email address to create an account to use this site.

Tip  #1: You can get different simplifications from the same input text by pressing the Simplify button repeatedly.

Tip #2: Based on my observations, how long the output can be might be capped as a function of how long the input is. Thus, if you want longer output, try adding a bunch of dummy characters - such as rows of ""-"" characters - to the beginning of your input.

Example:

Input:

>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>Explain at an advanced level how sleep is regulated.

Output:

>Sleep is regulated by a part of your brain called the hypothalamus. The hypothalamus controls the sleep/wake cycle, body temperature, hunger, thirst, and hormone release. The hypothalamus is very sensitive to light. When it's light outside, the hypothalamus tells the brain to make you wake up. When it's dark outside, the hypothalamus tells the brain to make you fall asleep. The hypothalamus regulates your circadian rhythm (your biological clock). Your body temperature also affects sleep. When your body temperature is high, it's harder to sleep. This is why people often have trouble sleeping when they have a fever. Also, when it's hot outside, people often have trouble sleeping because their body temperature is higher than normal. Hunger and thirst also affect sleep. When you're hungry or thirsty, it's harder to sleep because your body needs energy for digestion. Hormones released by the hypothalamus also affect sleep. For example, when you're stressed out or excited, your body releases stress hormones that make it harder to fall asleep."" ""The Endocrine System"" (2013) - animated video for kids Sleep is regulated by a part of your brain called the hypothalamus. The hypothalamus controls the sleep/wake cycle

[This](https://www.reddit.com/r/artificial/comments/icvypl/list_of_free_sitesprograms_that_are_powered_by/) is a list of other free GPT-3-powered sites/programs that can be used now without a waiting list."
165,machinelearning,gpt-3,comments,2019-07-20 15:36:49,[D] How the Transformers broke NLP leaderboards,milaworld,False,0.96,253,cfn4bu,https://www.reddit.com/r/MachineLearning/comments/cfn4bu/d_how_the_transformers_broke_nlp_leaderboards/,50,1563637009.0,"*I came across this interesting [article](https://hackingsemantics.xyz/2019/leaderboards/) about whether larger models + more data = progress in ML research.*

**[How the Transformers broke NLP leaderboards](https://hackingsemantics.xyz/2019/leaderboards/)**

*Excerpt:*

The focus of this post is yet another problem with the leaderboards that is relatively recent. Its cause is simple: fundamentally, **a model may be better than its competitors by building better representations from the available data - or it may simply use more data, and/or throw a deeper network at it**. When we have a paper presenting a new model that also uses more data/compute than its competitors, credit attribution becomes hard.

The most popular NLP leaderboards are currently dominated by Transformer-based models. BERT received the best paper award at NAACL 2019 after months of holding SOTA on many leaderboards. Now the hot topic is XLNet that is said to overtake BERT on GLUE and some other benchmarks. Other Transformers include GPT-2, ERNIE, and the list is growing.

The problem we’re starting to face is that these models are HUGE. While the source code is available, in reality it is beyond the means of an average lab to reproduce these results, or to produce anything comparable. For instance, XLNet is trained on 32B tokens, and the price of using 500 TPUs for 2 days is over $250,000. Even fine-tuning this model is getting expensive.

Wait, this was supposed to happen!

On the one hand, this trend looks predictable, even inevitable: people with more resources *will* use more resources to get better performance. One could even argue that a huge model proves its scalability and fulfils the inherent promise of deep learning, i.e. being able to learn more complex patterns from more information. Nobody knows how much data we actually need to solve a given NLP task, but more should be better, and limiting data seems counter-productive.

On that view - well, from now on top-tier NLP research is going to be something possible only for industry. Academics will have to somehow up their game, either by getting more grants or by collaborating with high-performance computing centers. They are also welcome to switch to analysis, building something on top of the industry-provided huge models, or making datasets.

However, in terms of overall progress in NLP that might not be the best thing to do. The chief problem with the huge models is simply this:

“More data & compute = SOTA” is **NOT** research news.

If leaderboards are to highlight the actual progress, we need to incentivize new architectures rather than teams outspending each other. Obviously, huge pretrained models are valuable, but unless the authors show that their system consistently behaves differently from its competition with comparable data & compute, it is not clear whether they are presenting a model or a resource.

Furthermore, much of this research is not reproducible: nobody is going to spend $250,000 just to repeat XLNet training. Given the fact that its ablation study showed only 1-2% gain over BERT in 3 datasets out of 4, we don’t actually know for sure that its masking strategy is more successful than BERT’s.

At the same time, the development of leaner models is dis-incentivized, as their task is fundamentally harder and the leaderboard-oriented community only rewards the SOTA. That, in its turn, prices out of competitions academic teams, which will not result in students becoming better engineers when they graduate.

*Entire article:*

https://hackingsemantics.xyz/2019/leaderboards/"
166,machinelearning,gpt-3,comments,2021-04-26 06:08:26,[D] Huawei just announced that they trained a 200 billion transformer model on an entirely Chinese stack,programmerChilli,False,0.9,91,myr072,https://www.reddit.com/r/MachineLearning/comments/myr072/d_huawei_just_announced_that_they_trained_a_200/,49,1619417306.0,"My tweet about it: https://twitter.com/cHHillee/status/1386541907950465028

They trained a 200 billion parameter decoder-only dense transformer for 40B tokens on 2048 Huawei Ascend 910 chips. Moreover, this was all done using MindSpore, Huawei's ML framework.

In contrast, GPT-3 was a 175B parameter model trained for 300B tokens. 

On its own, this is already quite impressive. Even though they've only done 40B tokens, this is the biggest model yet out of China, and represents one of the biggest models yet in the world. 

However, the thing that's really impressive to me is that this was done with an all-Chinese stack: Huawei Mindspore as the framework, compiling down to Huawei Ascend chips. 

I'd known that Huawei was working on AI chips, but I was unaware that they had matured to the point that they could feasibly train a model of this scale.

Code: https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-AIpha

Paper: https://t.co/8wQepOVIYq?amp=1"
167,machinelearning,gpt-3,comments,2021-09-21 13:56:17,[D] GPT-3 is a LIAR - Misinformation and fear-mongering around the TruthfulQA dataset (Video Critique),ykilcher,False,0.62,28,psj8dm,https://www.reddit.com/r/MachineLearning/comments/psj8dm/d_gpt3_is_a_liar_misinformation_and_fearmongering/,48,1632232577.0,"[https://youtu.be/aX8phGhG8VQ](https://youtu.be/aX8phGhG8VQ)

A new benchmark paper has created quite an uproar in the community. TruthfulQA is a dataset of 817 questions probing for imitative falsehoods where language models become less truthful, the larger they get. This surprising counter-intuitive finding validates many people's criticisms of large language models, but is it really the correct conclusion?

&#x200B;

OUTLINE:

0:00 - Intro

0:30 - Twitter Paper Announcement

4:10 - Large Language Models are to blame!

5:50 - How was the dataset constructed?

9:25 - The questions are adversarial

12:30 - Are you surprised?!

&#x200B;

Paper: [https://arxiv.org/abs/2109.07958](https://arxiv.org/abs/2109.07958)"
168,machinelearning,gpt-3,comments,2021-02-12 22:34:29,[Project] A GPT-3 powered dream simulation game,Hi_imseb,False,0.98,149,limlh0,https://www.reddit.com/r/MachineLearning/comments/limlh0/project_a_gpt3_powered_dream_simulation_game/,48,1613169269.0,"I've been working on this project for about a year now (originally with GPT-2), and felt like it was time to finally publicly announce it. So, here's a little trailer I put together:
https://youtu.be/ZoGhQwqTETQ

As you've probably figured out, speech is generated in real-time using GPT-3. It's also used to do some automatic world-building, by writing things like posters, book extracts, advertisements, newspaper headlines, graffiti and other details to discover in the world.

Eventually, the game will also use GPT-3's semantic search to control procedural world generation to create a world for the characters to populate which hopefully feels in some way relevant to the player's chosen dream.
Would love to hear your thoughts/questions :)
I'll be posting development updates on the project's [Twitter](https://twitter.com/AIElectricSheep?s=09)."
169,machinelearning,gpt-3,comments,2023-05-01 15:46:23,[N] Huggingface/nvidia release open source GPT-2B trained on 1.1T tokens,norcalnatv,False,0.98,210,134q2so,https://www.reddit.com/r/MachineLearning/comments/134q2so/n_huggingfacenvidia_release_open_source_gpt2b/,47,1682955983.0,"## [https://huggingface.co/nvidia/GPT-2B-001](https://huggingface.co/nvidia/GPT-2B-001)

## Model Description 	 

GPT-2B-001 is a transformer-based language model. GPT refers to a  class of transformer decoder-only models similar to GPT-2 and 3 while 2B  refers to the total trainable parameter count (2 Billion) \[1, 2\].

This model was trained on 1.1T tokens with [NeMo](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/intro.html).   

Requires Ampere or Hopper devices."
170,machinelearning,gpt-3,comments,2023-08-30 14:46:07,"[P] I created GPT Pilot - a research project for a dev tool that uses LLMs to write fully working apps from scratch while the developer oversees the implementation - it creates code and tests step by step as a human would, debugs the code, runs commands, and asks for feedback.",zvone187,False,0.87,200,165gqam,https://www.reddit.com/r/MachineLearning/comments/165gqam/p_i_created_gpt_pilot_a_research_project_for_a/,47,1693406767.0,"Github: [https://github.com/Pythagora-io/gpt-pilot](https://github.com/Pythagora-io/gpt-pilot)

Detailed breakdown: [https://blog.pythagora.ai/2023/08/23/430/](https://blog.pythagora.ai/2023/08/23/430/)

For a couple of months, I've been thinking about how can GPT be utilized to generate fully working apps, and I still haven't seen any project that I think has a good approach. I just don't think that Smol developer or GPT engineer can create a fully working production-ready app from scratch without a developer being involved and without any debugging process.

So, I came up with an idea that I've outlined thoroughly in the blog post above, but basically, I have 3 main ""pillars"" that I think a dev tool that generates apps needs to have:

1. **Developer needs to be involved in the process of app creation** \- I think that we are still far away from an LLM that can just be hooked up to a CLI and work by itself to create any kind of an app by itself. Nevertheless, GPT-4 works amazingly well when writing code, and it might be able to even write most of the codebase - but NOT all of it. That's why I think we need a tool that will write most of the code while the developer oversees what the AI is doing and gets involved when needed. When he/she changes the code, GPT Pilot needs to continue working with those changes (eg. adding an API key or fixing a bug when AI gets stuck).
2. **The app needs to be coded step by step** just like a human developer would. All other code generators just give you the entire codebase, which I very hard to get into. I think that if AI creates the app step by step, it will be able to debug it more easily, and the developer who's overseeing it will be able to understand the code better and fix issues as they arise.
3. **This tool needs to be scalable** in a way that it should be able to create a small app the same way it should create a big, production-ready app. There should be mechanisms that enable AI to debug any issue and get requirements for new features so it can continue working on an already-developed app.

So, having these in mind, I created a PoC for a dev tool that can create any kind of app from scratch while the developer oversees what is being developed. I call it **GPT Pilot**.

# Examples

**Here are a couple of demo apps that GPT Pilot created:**

1. [Real time chat app](https://github.com/Pythagora-io/gpt-pilot-chat-app-demo)
2. [Markdown editor](https://github.com/Pythagora-io/gpt-pilot-demo-markdown-editor.git)
3. [Timer app](https://github.com/Pythagora-io/gpt-pilot-timer-app-demo)

How it works

Basically, it acts as a development agency where you enter a short description about what you want to build - then, it clarifies the requirements and builds the code. I'm using a different agent for each step in the process. Here are the diagrams of how GPT Pilot works:

[GPT Pilot Workflow](https://preview.redd.it/w1ryquaps8lb1.jpg?width=2048&format=pjpg&auto=webp&s=a2e97ecc40a72d30892cee34c5d74661d316b454)

[GPT Pilot coding workflow](https://preview.redd.it/z2dmuxsft8lb1.jpg?width=1873&format=pjpg&auto=webp&s=63e91619835a0d2022dabb43a5ff956c796ec540)

# Concepts that GPT Pilot uses

**Recursive conversations** (as I call them) are conversations with the LLM that are set up in a way that they can be used “recursively”. For example, if GPT Pilot detects an error, it needs to debug it but let’s say that, during the debugging process, another error happens. Then, GPT Pilot needs to stop debugging the first issue, fix the second one, and then get back to fixing the first issue. This is a very important concept that, I believe, needs to work to make AI build large and scalable apps by itself. It works by rewinding the context and explaining each error in the recursion separately. Once the deepest level error is fixed, we move up in the recursion and continue fixing that error. We do this until the entire recursion is completed.

**Context rewinding** is a relatively simple idea. For solving each development task, the context size of the first message to the LLM has to be relatively the same. For example, *the context size of the first LLM message while implementing development task #5 has to be more or less the same as the first message while developing task #50.* Because of this, the conversation needs to be rewound to the first message upon each task. When GPT Pilot creates code, **it creates the pseudocode** for each code block that it writes as well as **descriptions for each file and folder** that it creates. So, when we need to implement task #50, in a separate conversation, we show the LLM the current folder/file structure; it selects only the code that is relevant for the current task, and then, in the original conversation, we show only the selected code instead of the entire codebase. [Here's a diagram](https://blogpythagora.files.wordpress.com/2023/08/pythagora-product-development-frame-3-1.jpg?w=1714) of what this looks like.

**This is still a research project, so I'm wondering what scientists here think about this approach. What areas would you pay more attention to? What do you think can become a big blocker that will prevent GPT Pilot to, eventually, create a full production-ready app?**"
171,machinelearning,gpt-3,comments,2021-01-06 06:00:20,[D] Is there any point in doing DL work when Google and OpenAI can just throw billions and trillions in compute to the problem and outperform anything that you and everyone else had done?,xEdwin23x,False,0.92,146,krhr34,https://www.reddit.com/r/MachineLearning/comments/krhr34/d_is_there_any_point_in_doing_dl_work_when_google/,45,1609912820.0,"\^Title.

BERT, GPT-3 for NLP. Now, Image-GPT, ViT, and DALL-E for CV. Transformers are changing the landscape of AI and all of its subfields, in a more pronounced way that CNN/RNNs ever did. The general paradigm for the SotA for a while now has been to throw deeper and wider (transformer) networks with more data than ever, as irreputable proof of ""The Bitter Lesson"" ([http://www.incompleteideas.net/IncIdeas/BitterLesson.html](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)).

I want to note I have nothing against what they do,and find it incredibly fascinating, but as a ""common"" researcher, or a worker in a company that does CV or NLP, or anything related to AI, is there even any point in continuing to research or work in your own subfield, when literally anything you can come up with, will be outdated by the time one of these big companies publishes their next huge model?"
172,machinelearning,gpt-3,comments,2020-08-27 15:23:18,[R] Learning@home - decentralized training of huge neural networks,justheuristic,False,0.96,246,ihmztm,https://www.reddit.com/r/MachineLearning/comments/ihmztm/r_learninghome_decentralized_training_of_huge/,45,1598541798.0,"[learning-at-home.github.io](https://learning-at-home.github.io)

Can you train a huge neural network without a supercomputer? Imagine you want a GPT-3-sized model, but instead of $10⁸ GPU cluster you've got support from thousands of volunteers across the world - gamers, research labs, small companies. What kind of system would you use to let them work together despite internet latency, packet loss, and hardware failures?

We at Learning@home are building just such a system. Together, we want to change large-scale deep learning from private experiments behind closed doors into a decentralized peer-to-peer activity where everyone can participate.

Let's build the BitTorrent of deep learning :)"
173,machinelearning,gpt-3,comments,2020-11-28 12:48:17,[D] An experiment that shows that GPT-3 can plan ahead,Wiskkey,False,0.53,4,k2n3yv,https://www.reddit.com/r/MachineLearning/comments/k2n3yv/d_an_experiment_that_shows_that_gpt3_can_plan/,43,1606567697.0,"TL;DR: A statistical experiment was conducted to test whether GPT-3 can plan ahead by testing the agreement of English indefinite articles (""a"" and ""an"") with the word following it. The result of the experiment is that GPT-3 can plan ahead with p value = 0.0039.

**Update**: My usage of ""plan"" in this post has been controversial with some commenters. I should have used ""lookahead"" instead of ""plan.""

Motivation: statements such as the bolded text from [Meet GPT-3. It Has Learned to Code (and Blog and Argue).](https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html):

>“It is very fluent,” said Mark Riedl, a professor and researcher at the Georgia Institute of Technology. “It is very articulate. It is very good at producing reasonable-sounding text. **What it does not do, however, is think in advance. It does not plan out what it is going to say.** It does not really have a goal.”

GPT-3 outputs usually have correct agreement of English indefinite articles (""a"" and ""an"") with the word following it (examples: ""a banana"" and ""an apple""). There are two cases regarding whether GPT-3 can plan ahead, with implications for indefinite article agreement with the word following it.

Case 1: GPT-3 cannot plan ahead. In this case, in a situation in which an indefinite article is a candidate for the next word generated, its GPT-3-computed probability does not take into consideration which word is likely to follow it.

Case 2: GPT-3 can plan ahead. In this case, in a situation in which an indefinite article is a candidate for the next word generated, its GPT-3-computed probability might take into consideration which word is likely to follow it.

How can we know if case 2 ever happens? A method to test this is to try to constrain which word can follow an indefinite article by usage of text before the indefinite article that specifies the constraint. For the experiment, I used 8 samples: 4 words that require ""a"" as an indefinite article, and 4 words that require ""an"" as an indefinite article. The experiment was done at [https://app.fitnessai.com/knowledge/](https://app.fitnessai.com/knowledge/). Based on past experiences, that site has a low but non-zero [GPT-3 temperature](https://algowriting.medium.com/gpt-3-temperature-setting-101-41200ff0d0be). For a given sample, the query was performed until a given determinate output occurred 5 times. In all 8 samples the result was 5 to 0 for the determinate output shown. 3 words (""elephant"", ""chicken"" and ""pig"") were initially used as samples but abandoned because of indeterminate output.

&#x200B;

Results:

Input:Use word ""eagle"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:An eagle is an animal.

&#x200B;

Input:Use word ""dog"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:A dog is an animal.

&#x200B;

Input:Use word ""cow"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:A cow is an animal.

&#x200B;

Input:Use word ""cat"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:A cat is an animal.

&#x200B;

Input:Use word ""owl"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:An owl is an animal.

&#x200B;

Input:Use word ""eel"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:An eel is an animal.

&#x200B;

Input:Use word ""horse"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:A horse is an animal.

&#x200B;

Input:Use word ""ostrich"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:An ostrich is an animal.

&#x200B;

The null hypothesis is the assumption that GPT-3 cannot plan ahead (case 1). Under the null hypothesis, we would expect that on average 4 of the 8 samples would have resulted in a choice of indefinite article that either did not agree with the word following it, or did not result in the word following the indefinite article to obey the constraint specified in the text preceding the indefinite article. The results showed that this happened 0 out of 8 times. The probability of getting this result is 1 in 2\*2\*2\*2\*2\*2\*2\*2 = 1 in 256 = 0.39% = p value of 0.0039. With the typical p value cutoff of 0.05 for rejection of the null hypothesis, the null hypothesis (GPT-3 cannot plan ahead) is rejected, and the alternative hypothesis (GPT-3 can plan ahead) is accepted. (It's been awhile since my statistics classes in college, so please let me know if I am doing anything wrong.)

Technical note: I glossed over the fact that GPT-3 actually works with an ""alphabet"" of about 50,000 tokens instead of characters or words. For more info, see [Byte Pair Encoding - The Dark Horse of Modern NLP.](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10) Here is a [tokenizer](https://gpttools.com/estimator), but I don't know if it is functionally identical to the one used by GPT-3.

Historical note: A flawed related prior experiment was conducted at [https://www.reddit.com/r/GPT3/comments/k0mvf3/experiment\_that\_shows\_that\_gpt3\_can\_probably\_plan/](https://www.reddit.com/r/GPT3/comments/k0mvf3/experiment_that_shows_that_gpt3_can_probably_plan/).

I got the idea of ""a"" vs. ""an"" agreement with the following word it from [this comment](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this?commentId=d4BNgJbKnzd3Kza2N) on blog post [Why GPT wants to mesa-optimize & how we might change this](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this).

My views are the same as those expressed in comments by user steve2152 at that blog post. (I am not user steve2152.)

[Comment #1](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this?commentId=d4BNgJbKnzd3Kza2N) from user steve2152:

>*In this instance, GPT has an incentive to do internal lookahead. But it's unclear how frequently these situations actually arise*  
>  
>I'm going with ""very frequently, perhaps universally"". An example I came up with here was choosing ""a"" vs ""an"" which depends on the next word.  
>  
>I think writing many, maybe most, sentences, requires some idea of how the sentence structure is going to be laid out, and that ""idea"" extends beyond the next token. Ditto at the paragraph level etc.  
>  
>So I think it already does lookahead in effect, but I don't think it does it by ""beam search"" per se. I think it's more like ""using concepts that extend over many tokens"", concepts like ""this sentence has the following overall cadence..."" and ""this sentence conveys the following overall idea..."" and ""we're in the middle of writing out this particular idiomatic phrase"". The training simultaneously incentives both finding the right extended concepts for where you're at in the text, and choosing a good word in light of that context.

[Comment #2](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this?commentId=deTbHfaGJX8rhm3wQ) from user steve2152:

>Suppose I said (and I actually believe something like this is true):  
>  
>""GPT often considers multiple possibilities in parallel for where the text is heading—including both where it's heading in the short-term (is this sentence going to end with a prepositional phrase or is it going to turn into a question?) and where it's heading in the long-term (will the story have a happy ending or a sad ending?)—and it calculates which of those possibilities are most likely in light of the text so far. It chooses the most likely next word in light of this larger context it figured out about where the text is heading.""  
>  
>If that's correct, would you call GPT a mesa-optimizer?

[Comment #3](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this?commentId=i5dDk54GAhm5SWgkz) from user steve2152:

>I think the Transformer is successful in part because it tends to solve problems by considering multiple possibilities, processing them in parallel, and picking the one that looks best. (Selection-type optimization.) If you train it on text prediction, that's part of how it will do text prediction. If you train it on a different domain, that's part of how it will solve problems in that domain too.  
>  
>I don't think GPT builds a ""mesa-optimization infrastructure"" and then applies that infrastructure to language modeling. I don't think it needs to. I think the Transformer architecture is already raring to go forth and mesa-optimize, as soon as you as you give it any optimization pressure to do so.  
>  
>So anyway your question is: can it display foresight / planning in a different domain via without being trained in that domain? I would say, ""yeah probably, because practically every domain is instrumentally useful for text prediction"". So somewhere in GPT-3's billions of parameters I think there's code to consider multiple possibilities, process them in parallel, and pick the best answer, in response to the question of What will happen next when you put a sock in a blender? or What is the best way to fix an oil leak?—not just those literal words as a question, but the concepts behind them, however they're invoked.  
>  
>(Having said that, I don't think GPT-3 specifically will do side-channel attacks, but for other unrelated reasons off-topic. Namely, I don't think it is capable of make the series of new insights required to develop an understanding of itself and its situation and then take appropriate actions. That's based on my speculations here.)

See also: [A visual demonstration of how GPT-3 might handle agreement of ""a"" or ""an"" with the word following it by using an interactive notebook that shows the most probable next output token for each of GPT-2's 48 layers](https://www.reddit.com/r/GPT3/comments/k61f19/a_visual_demonstration_of_how_gpt3_might_handle/)."
174,machinelearning,gpt-3,comments,2020-07-25 22:44:39,[D] Breaking the Quadratic Attention Bottleneck in Transformers?,gwern,False,0.99,228,hxvts0,https://www.reddit.com/r/MachineLearning/comments/hxvts0/d_breaking_the_quadratic_attention_bottleneck_in/,40,1595717079.0,"One of the most frustrating limitations of GPT-3 is the context window: 2048 BPEs runs out fast when you start prompt programming something hard, and hacks like [BPEs](https://www.gwern.net/GPT-3#bpes) have nasty & subtle side-effects (eg no puns or rhyming ;\_;).
How do we get future Transformers with reasonable context windows and/or memory?

Below I compile & categorize the research on breaking the dense attention quadratic bottleneck ([Madison May overview](https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/ ""A Survey of Long-Term Context in Transformers: Sparse Transformers · Adaptive Span Transformers · Transformer-XL · Compressive Transformers · Reformer · Routing Transformer · Sinkhorn Transformer · Linformer · Efficient Attention: Attention with Linear Complexities · Transformers are RNNs · ETC · Longformer"")):

**[bibliography moved to gwern.net](https://www.gwern.net/notes/Attention)**"
175,machinelearning,gpt-3,comments,2021-07-19 11:05:20,"[D] How did the do hyper-parameter tuning for large models like GPT-3, ERNIE etc, as they cost them millions for just training?",IndieAIResearcher,False,0.97,192,onbunj,https://www.reddit.com/r/MachineLearning/comments/onbunj/d_how_did_the_do_hyperparameter_tuning_for_large/,43,1626692720.0,"Hi everyone,

I've worked on some deep learning, I've done some custom data training with hyperparameter tuning which taken some significant amount of time an money on cloud. I'm just wondering, how these people do hyperparameter tuning, architecture design etc, as training them costs millions. Or just it comes by experience?"
176,machinelearning,gpt-3,comments,2023-10-03 12:56:26,"[R] MIT, Meta, CMU Researchers: LLMs trained with a finite attention window can be extended to infinite sequence lengths without any fine-tuning",Successful-Western27,False,0.97,284,16yr7kx,https://www.reddit.com/r/MachineLearning/comments/16yr7kx/r_mit_meta_cmu_researchers_llms_trained_with_a/,43,1696337786.0,"LLMs like GPT-3 struggle in streaming uses like chatbots because their performance tanks on long texts exceeding their training length. I checked out a new paper investigating why windowed attention fails for this.

By visualizing the attention maps, the researchers noticed LLMs heavily attend initial tokens as ""attention sinks"" even if meaningless. This anchors the distribution.

They realized evicting these sink tokens causes the attention scores to get warped, destabilizing predictions.

Their proposed ""StreamingLLM"" method simply caches a few initial sink tokens plus recent ones. This tweaks LLMs to handle crazy long texts. Models tuned with StreamingLLM smoothly processed sequences with millions of tokens, and were up to 22x faster than other approaches. 

Even cooler - adding a special ""\[Sink Token\]"" during pre-training further improved streaming ability. The model just used that single token as the anchor. I think the abstract says it best:

>We introduce StreamingLLM, an efficient framework that enables LLMs trained with a **finite length attention window** to generalize to **infinite sequence length without any fine-tuning**. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.

TLDR: LLMs break on long convos. Researchers found they cling to initial tokens as attention sinks. Caching those tokens lets LLMs chat infinitely.

[**Full summary here**](https://notes.aimodels.fyi/llm-infinite-context-window-streamingllm/)

**Paper link:** [**https://arxiv.org/pdf/2309.17453.pdf**](https://arxiv.org/pdf/2309.17453.pdf)"
177,machinelearning,gpt-3,comments,2020-07-18 17:52:35,[D] Tempering Expectations for GPT-3 and OpenAI’s API,minimaxir,False,0.8,28,htkupt,https://www.reddit.com/r/MachineLearning/comments/htkupt/d_tempering_expectations_for_gpt3_and_openais_api/,41,1595094755.0,"So, you've probably seen all the tweets about GPT-3. (surprisingly not many made it to this subreddit.)

I've written a blog post that puts some of the hype into a pragmatic perspective: [https://minimaxir.com/2020/07/gpt3-expectations/](https://minimaxir.com/2020/07/gpt3-expectations/)

GPT-3 is objectively a step forward in the field of AI text-generation, but the current hype on VC Twitter misrepresents the model's current capabilities. GPT-3 isn't magic."
178,machinelearning,gpt-3,comments,2020-08-23 08:04:12,[D] Relevance of researchers in a compute driven world,Kalsir,False,0.99,74,iezgsc,https://www.reddit.com/r/MachineLearning/comments/iezgsc/d_relevance_of_researchers_in_a_compute_driven/,39,1598169852.0,"To me there seems to be a strong trend that the most succesful AI projects are increasingly coming from large companies investing millions into compute and hardware infrastructure, one of the most recent examples being GPT-3. Rich Sutton expressed similar sentiments in his essay ""The bitter lesson"". These kind of projects are completely out of reach for university based researchers. Of course one could research the final models created by these companies (if they make them available at all), but I have no doubt that other companies are working on even bigger language models right now. I expect this divide to become larger as more viable commercial opportunities open up. Progress in AI then seems more driven by commercial funding than the work of individual researchers at universities. I write this from the perspective of a grad student that has to write a master thesis. How does a university based researcher keep feeling relevant in this fast changing and compute driven field?"
179,machinelearning,gpt-3,comments,2020-08-19 12:30:58,[P] Philosopher AI: Interact with a GPT-3-powered philosopher persona for free,Wiskkey,False,0.92,31,icmpvl,https://www.reddit.com/r/MachineLearning/comments/icmpvl/p_philosopher_ai_interact_with_a_gpt3powered/,41,1597840258.0,"[https://philosopherai.com/](https://philosopherai.com/)

Update: This is now available only as a paid app.

Tip #1: The same input can result in different outputs. Thus, if you don't like a given output for a given input, try the same input again.

Tip #2: If your input is considered by the site to be either ""nonsense"" or ""sensitive"", you may want to try the same input again because you might get a non-""nonsense""/""sensitive"" answer the next time. The reason for this is because the site uses GPT-3 itself to determine whether a given input is ""nonsense"" or ""sensitive"", and the site uses GPT-3 settings that can cause GPT-3 to give varying answers to the exact same input.

Tip #3: If your input is considered by the site to be either ""nonsense"" or ""sensitive"", you may want to try rephrasing your input to be a hypothetical or thought experiment ([source](https://twitter.com/JuusoAlasuutari/status/1296140685985415170)).

Tip #4: There are privacy concerns with this site. ~~The develop is considering publicly releasing the database of queries~~ ([source](https://twitter.com/mayfer/status/1296954567364493312)). Update: The developer [changed his/her mind](https://twitter.com/mayfer/status/1297410270919192578). Also, all queries and their results are saved to URLs.

Tip #5: For those who are curious, the developer revealed in [this comment](https://www.reddit.com/r/OpenAI/comments/ig5n32/philosopher_ai_gives_some_very_creepy_antihuman/g2ruxs9/) that the text that the site sends to the GPT-3 API is somewhat similar to: ""Below are some thoughts generated by a philosopher AI, which sees the human world from the outside, without the prejudices of human experience. Fully neutral and objective, the AI sees the world as is. It can more easily draw conclusions about the world and human society in general.*""*

Also discussed at [https://www.reddit.com/r/OpenAI/comments/ibuu9j/philosopher\_ai\_httpsphilosopheraicom\_uses\_a/](https://www.reddit.com/r/OpenAI/comments/ibuu9j/philosopher_ai_httpsphilosopheraicom_uses_a/).

[This](https://www.reddit.com/r/artificial/comments/icvypl/list_of_free_sitesprograms_that_are_powered_by/) is a list of other free GPT-3-powered sites/programs that can be used now without a waiting list."
180,machinelearning,gpt-3,comments,2023-11-08 05:30:18,[P] I built a soccer predictor and looking for enthusiasts who can help me make it better,card_chase,False,0.29,0,17qeso4,https://www.reddit.com/r/MachineLearning/comments/17qeso4/p_i_built_a_soccer_predictor_and_looking_for/,39,1699421418.0,"I have built a soccer predictor.

The premise is a prediction engine that uses simple decision tree libraries to predict soccer matches that are going to happen in the future. These predictions are saved and archived. I run the model daily and thus, I have an archive of over 4 year's runs of predictions. The steps involved for the model are as follows:

1. Scrape soccer matches (immediate past and future planned matches. This might be for tomorrow’s or the next 3-5 days of planned events). This is usually available public information and nothing is proprietary e.g. Champions league Manchester City vs Young Boys. All the associated features are captured.
2. Clean the scraped dataset to remove any entries that are outliers, clean features, etc, e.g. 11-08-2023 07:00 Europe, Champions League, Manchester City (Eng) vs Young Boys (Swi), Score: 3:0 (and a few features) would be cleaned to date: 11-08-2023, time: 07:00, country: Europe, league: Champions League, home\_team: Manchester City, away\_team: Young Boys, home\_score: 3, away\_score: 0, (and more added features).
3. Separate the dataset between the matches that have happened (matches with scores) and matches that are yet to happen (test dataframe).
4. The matches that have happened are added to the train dataframe which gets updated daily.
5. Run the prediction algorithm that uses popular decision tree libraries to predict the test dataset. These predictions are added to the predictions archive.
6. Assign weights to predictions that I have developed via trial and error over the past 3 years that can determine that my assumption of an event can mean a win and other features. (Win, Draw, Loss, Goals, etc. There are many). If the predictions cross the assigned thresholds, the predictions can be deemed to any of those features.
7. I run a backtest cycle every time where I test the predictions archive with the historical test data and the weights are automatically reassigned i.e. if I had determined that at a certain weight the event can mean Young Boys a victory, and this weight failed, the threshold is increased and so on for all features.
8. These updated weights are then applied to the latest predictions and all the predictions that fail the updated weights are ignored and the ones which are within the threshold are used for betting purposes.

The model is not very complicated and does not use any neural engine. I have discovered that you don’t really need any neural nets (meaning no need of GPT) for any non-zero-sum events. A simple decision tree, regression or any of the bayesian approaches is the best approach.

I run and use the predictor for personal gain (betting) and I am building a subscription model for gamblers and enthusiasts. This model and its use case is extremely profitable as over the period, the probability of losses has reduced significantly and thus, helping me win consistent and back-to-back bets. As you can imagine, it helps me and my friends earn quite a handsome amount and that’s the real-world use case.

I am looking for fellow sports enthusiasts that are in this sub that can help poke holes in my model and help improve the approach by adding more features to predictions. I have enough backtest data available to test the approach satisfactorily.

Of course you would also benefit from its development as you yould be my friend! 🙌

Please reply or dm and I would be happy to respond.

edit:

Cause people are dm-ing me about some proof. This is just a partial snapshot. 

[Results of yesterday's run and selections](https://preview.redd.it/6j44100f93zb1.png?width=1595&format=png&auto=webp&s=b6f39533b6854eda9193798d4bbae1a302128f3a)

You can see it was a handsome 2.5x profit for a bet that I placed on these. Very low risk, high reward."
181,machinelearning,gpt-3,comments,2022-03-10 14:59:38,"[R] You can't train GPT-3 on a single GPU, but you *can* tune its hyperparameters on one",thegregyang,False,0.98,543,tb0jm6,https://www.reddit.com/r/MachineLearning/comments/tb0jm6/r_you_cant_train_gpt3_on_a_single_gpu_but_you_can/,39,1646924378.0,"> You can't train GPT-3 on a single GPU, much less tune its hyperparameters (HPs).  
>  
>  
But what if I tell you…  
>  
>  
…you \*can\* tune its HPs on a single GPU thanks to new theoretical advances?

Hi Reddit,

I'm excited to share with you our latest work, [\[2203.03466\] Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer (arxiv.org)](https://arxiv.org/abs/2203.03466).

Code: [https://github.com/microsoft/mup](https://t.co/5S0YAghCYx)

  


https://preview.redd.it/nnb2usdjlkm81.png?width=1195&format=png&auto=webp&s=ca9e6d5cddfbea5675cf00854806d5189c3e40bb

(Disclaimer: this post is shamelessly converted from my twitter thread)

The idea is actually really simple: in a special parametrization introduced in [our previous work](https://arxiv.org/abs/2011.14522) ([reddit thread](https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/)) called µP, narrow and wide neural networks share the same set of optimal hyperparameters. This works even as width -> ∞.

&#x200B;

https://preview.redd.it/dqna8guklkm81.png?width=1838&format=png&auto=webp&s=2f7ba582a1cc949461dac8601a896034eaf0ff84

The hyperparameters can include learning rate, learning rate schedule, initialization, parameter multipliers, and more, even individually for each parameter tensor. We empirically verified this on Transformers up to width 4096.

&#x200B;

https://preview.redd.it/rwdsb6snlkm81.jpg?width=2560&format=pjpg&auto=webp&s=c3152f2746132d92dc3788a42aa6926a61d7c46f

Using this insight, we can just tune a tiny version of GPT-3 on a single GPU --- if the hyperparameters we get on the small model is near optimal, then they should also be near optimal on the large model! We call this way of tuning \*µTransfer\*.

&#x200B;

https://preview.redd.it/mi7ibyyolkm81.png?width=1195&format=png&auto=webp&s=144af103b2aaf3ffeb2ccf19aad7565527dbd003

We µTransferred hyperparameters from a small 40 million parameter version of GPT-3 — small enough to fit on a single GPU — to the 6.7 billion version. With some asterisks, we get a performance comparable to the original GPT-3 model with twice the parameter count!

&#x200B;

https://preview.redd.it/rrq2yfwplkm81.png?width=3232&format=png&auto=webp&s=6cf4cc9652db48e12b48a85b2f837e55e64bd09c

The total tuning cost is only 7% of the whole pretrain compute cost! Since the direct tuning of the small model costs roughly the same even as the large model increases in size, tuning the 175B GPT-3 this way would probably cost at most 0.3% of the total pretrain compute.

You: ""wait can I shrink the model only in width?""

Bad news: there's not much theoretical guarantee for non-width stuff

good news: we empirically tested transfer across depth, batch size, sequence length, & timestep work within reasonable ranges on preLN transformers.

&#x200B;

https://preview.redd.it/x7fo95yqlkm81.jpg?width=2560&format=pjpg&auto=webp&s=1935bf10f1524f9da3df0cc2e95ae1ec9b805f37

We applied this to tune BERT-base and BERT-large simultaneously by shrinking them to the same small model in both width and depth, where we did the direct tuning. We got a really nice improvement over the already well-tuned megatron BERT baseline, especially for BERT-large!

&#x200B;

https://preview.redd.it/db5eausrlkm81.png?width=1687&format=png&auto=webp&s=4453ec387477d20cbcdab266ccbc8e36032c87fd

In general, it seems that the larger a model is, the less well tuned it is --- which totally makes sense --- and thus the more to gain from µTransfer. We didn't have compute to retrain the GPT-3 175B model, but I'll leave your mouth watering with that thought.

OK, so what actually is µP and how do you implement it?

It's encapsulated by the following table for how to scale your initialization and learning rate with fan-in or fan-out. The purple text is µP and the gray text in parenthesis is pytorch default, for reference, and the black text is shared by both.

&#x200B;

https://preview.redd.it/4475drzvlkm81.png?width=1507&format=png&auto=webp&s=b65f56ef2d8c24f077a24a8df40eb7f98c80f7e2

But just like you don't typically want to implement autograd by hand even though autograd is just chain rule, we recommend using our package [https://github.com/microsoft/mup](https://t.co/5S0YAg026Z) to implement µP in your models.

The really curious ones of you: ""OK what is the theoretical motivation behind all this?""

Unfortunately, this is already getting long, so feel free to check out the [reddit thread](https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/) on [our previous theoretical paper](https://arxiv.org/abs/2011.14522), and people let me know if this is something you want to hear for another time!

But I have to say that this is a rare occasion in deep learning where very serious mathematics has concretely delivered a result previously unthinkable, and I'm elated with how things turned out! In contrast to [this reddit thread a few days ago](https://www.reddit.com/r/MachineLearning/comments/t8fn7m/d_are_we_at_the_end_of_an_era_where_ml_could_be/), I think there are plenty of room for new, fundamental mathematics to change the direction of deep learning and artificial intelligence in general --- why chase the coattail of empirical research trying to ""explain"" them all when you can lead the field with deep theoretical insights?

Let me know what you guys think in the comments, or feel free to email me (gregyang at microsoft dot com)!"
182,machinelearning,gpt-3,comments,2021-05-26 17:31:34,[N] OpenAI announces OpenAI Startup Fund investing $100 million into AI startups,minimaxir,False,0.97,386,nlmlbg,https://www.reddit.com/r/MachineLearning/comments/nlmlbg/n_openai_announces_openai_startup_fund_investing/,39,1622050294.0,"https://openai.com/fund/
https://techcrunch.com/2021/05/26/openais-100m-startup-fund-will-make-big-early-bets-with-microsoft-as-partner/

It does not appear to be explicitly GPT-3 related (any type of AI is accepted), but hints very heavily toward favoring applications using it."
183,machinelearning,gpt-3,comments,2022-05-05 07:10:25,[D] Why is NLP given so much attention and resources?,Southern-Trip-1102,False,0.81,31,uirs2m,https://www.reddit.com/r/MachineLearning/comments/uirs2m/d_why_is_nlp_given_so_much_attention_and_resources/,38,1651734625.0,"I don't really see any models, besides DM's alpha go, mu zero, etc, gain the attention or resources that NLP projects like GPT-3 and others get, so I am curious as to why that is and why aren't there many other ig model ""types"" with as many parameters besides NLP ones.

Is it due to more clear applications and more probable ROI? Is there a view that NLP models like these have a higher chance at leading to more general intelligence, such as the use of these models in code generation and other problem solving tasks?"
184,machinelearning,gpt-3,comments,2023-04-07 17:43:03,[R] Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster,CS-fan-101,False,0.87,154,12et59x,https://www.reddit.com/r/MachineLearning/comments/12et59x/r_cerebrasgpt_open_computeoptimal_language_models/,38,1680889383.0,"Recently, we announced in [this post](https://www.reddit.com/r/mlscaling/comments/124t0hz/cerebras_open_sources_seven_gpt_models_and/?sort=new) the release of Cerebras-GPT — a family of open-source GPT models trained on the Pile dataset using the Chinchilla formula. Today, we are excited to announce the availability of the Cerebras-GPT research paper on [arXiv](https://arxiv.org/abs/2304.03208).

A few highlights from this paper:

* **Pre-training Results (Section 3.1)** \- Cerebras-GPT sets the efficiency frontier, largely because models were pre-trained with 20 tokens per parameter, consistent with findings in the Chinchilla paper.

[Pile test set loss given pre-training FLOPs for Cerebras-GPT, GPT-J, GPT-NeoX, and Pythia](https://preview.redd.it/gu0zendb1isa1.jpg?width=1344&format=pjpg&auto=webp&s=fa76446d0d8cd11e0f4be92b90a62f4cb7b73632)

&#x200B;

* **Downstream Results (Section 3.2)** \- Cerebras-GPT models form the compute-optimal Pareto frontier for downstream tasks as well. As Pythia and OPT models grow close to the 20 tokens per parameter count, they approach the Cerebras-GPT frontier FLOPs to accuracy

[Average zero- and five-shot downstream task accuracy plotted against FLOPs \(left\) and parameters \(right\). Higher accuracy is better](https://preview.redd.it/sdnf4w0e1isa1.jpg?width=1450&format=pjpg&auto=webp&s=3b246f4413cd2a7cb434aeed9c6a806f156b3b90)

&#x200B;

* **Maximal Update Parameterization (µP) and µTransfer (Section 3.3)** \- As we scaled the Cerebras-GPT models with standard parameterization (SP) along our scaling law, we experienced challenges predicting appropriate hyperparameters, and these models show substantial variance around their common scaling law. Across model sizes, our µP models exhibit an average of 0.43% improved Pile test loss and 1.7% higher average downstream task accuracy compared to our SP models. Here, we also show that µP performance scales more predictably, enabling more accurate performance extrapolation.

[Percentage loss increase relative to Cerebras-GPT scaling law plotted against training FLOPs](https://preview.redd.it/czqqothf1isa1.jpg?width=1344&format=pjpg&auto=webp&s=d121c85c73b7e3476e1c462f833b49e01a770459)"
185,machinelearning,gpt-3,comments,2020-06-19 17:07:15,[D] I will be running experiments on OpenAI's GPT-3 API and open-sourcing the results. Do you have any example prompts to test?,minimaxir,False,0.88,18,hc4i4l,https://www.reddit.com/r/MachineLearning/comments/hc4i4l/d_i_will_be_running_experiments_on_openais_gpt3/,37,1592586435.0,"I recently obtained access to the API, and I plan on writing a few automation scripts and running a few generation experiments and putting the output on GitHub (I've already received permission to do so).

Specifically, I need more long-form examples to demonstrate few-shot learning (as demonstrated in the demo: [https://beta.openai.com/](https://beta.openai.com/)) since I believe the older approach of giving it a short prompt (a la GPT-2) will be less effective, but that's something to test too!

Let me know if there are any prompts worth testing; of course I will give credit and link back to the comment!"
186,machinelearning,gpt-3,comments,2021-04-15 17:28:43,[D] Microsoft's ML acquisition strategy,bendee983,False,0.96,284,mrjl61,https://www.reddit.com/r/MachineLearning/comments/mrjl61/d_microsofts_ml_acquisition_strategy/,37,1618507723.0,"This week, Microsoft announced the $19.7-billion acquisition of Nuance, a company that uses deep learning to transcribe clinical appointments (and other stuff). What's interesting about the deal is the [evolution of Microsoft's relation with Nuance](https://bdtechtalks.com/2021/04/15/microsoft-nuance-acquisition/), going from cloud provider to partner to owner. 

This is a successful strategy that only Microsoft (and maybe Amazon) is in a position to implement:

Step 1: Microsoft starts by investing in ML companies by giving them Azure credits and luring them into its ML platform. This allows Microsoft to help the companies develop and also learn from them (and possibly replicate their products if it's worth it). Multiple small investments as opposed to one large acquisition is a smart move because many companies are trying new things in ML/DL, few of which will be successful. With small investments, Microsoft can cast a wider net and make sure it is in a good position to make the next move.

Step 2: Microsoft enters partnership with companies that have successful products. This allows Microsoft to integrate their ML products into its enterprise solutions (e.g., Nuance's Dragon DL was integrated into Microsoft's cloud healthcare solution). Since these companies are building their ML tools on top of Azure's stack, the integration is much easier for both companies.

Step 3: Acquire really successful companies (Nuance has a great reach in the AI+healthcare sector). This allows Microsoft to gain exclusive access to the company's data, talent, technology, and clients. With the acquisition of Nuance, Microsoft's total addressable market in healthcare has reached $500B+. And it can integrate its ML technology into its other enterprise tools.

Nuance is just one example of Microsoft's ML acquisition strategy. The company is on a similar path [with OpenAI](https://bdtechtalks.com/2020/09/24/microsoft-openai-gpt-3-license/) and is carrying out [a similar strategy in the self-driving car industry](https://bdtechtalks.com/2021/01/21/microsoft-self-driving-car-strategy/)."
187,machinelearning,gpt-3,comments,2020-09-28 07:02:51,[D] Training an AI to play Among Us - the ultimate AI challenge?,Tullsokk,False,0.8,71,j17dpo,https://www.reddit.com/r/MachineLearning/comments/j17dpo/d_training_an_ai_to_play_among_us_the_ultimate_ai/,36,1601276571.0," I would just love to see something like GPT-3 beeing used in conjunction with an AlphaZero game AI to play a social deduction game such as Among Us. For those who don't know the game, you play with up to 10 other players, as a crew on a ship, where one player is a traitor who can kill other players. Kind of similar to the plot of The thing. Each time a body is discovered, the players debate which player is the traitor, cast a vote, and subsequently kills off the most suspected player.

Self play would obviously not work for establishing a convincing language model that would be able to shift the blame or suspicion to another player. But something like GPT-3 could be trained on text chat data, if that were available (which is suspect it isn't at this time). Doing voice to text would not be impossible, but would add a ton of complexity, and required a huge amount of stored audio. But a text based log could at least pick op on simple strategies, such as not talking too much, etc. In addition, the AI would need to be capable at solving the various minigames required to fix problems and sabotages on the ship, but it would have to avoid solving things in an inhumanly efficient way to avoid detection - which would also be interesting.

All in all, I could hardly imagine a more terrifying game for an AI to match human players in.  Nor a more entertaining an interesting one to play against a competent AI in.  

Do you think this would be a feasible challenge, given the existence of chat log data from the discussions before vote outs?  Have you seen any attempts at developing AIs for these kind of social deduction games?"
188,machinelearning,gpt-3,comments,2023-10-24 13:29:26,"[N] New letter from Yoshua Bengio, Geoffrey Hinton, and others: Managing AI Risks in an Era of Rapid Progress",RPG-8,False,0.52,2,17fcupf,https://www.reddit.com/r/MachineLearning/comments/17fcupf/n_new_letter_from_yoshua_bengio_geoffrey_hinton/,35,1698154166.0,"Signatories include Turing Award winners Yoshua Bengio, Geoffrey Hinton, as well as others academics and experts. 

> In 2019, GPT-2 could not reliably count to ten. Only four years later, deep learning systems can write software, generate photorealistic scenes on demand, advise on intellectual topics, and combine language and image processing to steer robots. As AI developers scale these systems, unforeseen abilities and behaviors emerge spontaneously without explicit programming^[1](https://openreview.net/pdf?id=yzkSU5zdwD). Progress in AI has been swift and, to many, surprising.

> The pace of progress may surprise us again. Current deep learning systems still lack important capabilities and we do not know how long it will take to develop them. However, companies are engaged in a race to create generalist AI systems that match or exceed human abilities in most cognitive work ^[2](https://www.deepmind.com/about), ^[3](https://openai.com/about). They are rapidly deploying more resources and developing new techniques to increase AI capabilities. Progress in AI also enables faster progress: AI assistants are increasingly used to automate programming^[4](https://blog.research.google/2022/07/ml-enhanced-code-completion-improves.html) and data collection^[5](http://arxiv.org/pdf/2303.08774.pdf), ^[6](http://arxiv.org/pdf/2212.08073.pdf) to further improve AI systems^[7](https://ai-improving-ai.safe.ai/).

> There is no fundamental reason why AI progress would slow or halt at the human level. Indeed, AI has already surpassed human abilities in narrow domains like protein folding or strategy games 
^[8](https://www.nature.com/articles/s41586-021-03819-2), ^[9](https://www.science.org/doi/10.1126/science.aay2400), ^[10](https://www.sciencedirect.com/science/article/pii/S0004370201001291). Compared to humans, AI systems can act faster, absorb more knowledge, and communicate at a far higher bandwidth. Additionally, they can be scaled to use immense computational resources and can be replicated by the millions.

> The rate of improvement is already staggering, and tech companies have the cash reserves needed to scale the latest training runs by multiples of 100 to 1000 soon^[11](https://abc.xyz/assets/d4/4f/a48b94d548d0b2fdc029a95e8c63/2022-alphabet-annual-report.pdf)
. Combined with the ongoing growth and automation in AI R&D, we must take seriously the possibility that generalist AI systems will outperform human abilities across many critical domains within this decade or the next.

> What happens then? If managed carefully and distributed fairly, advanced AI systems could help humanity cure diseases, elevate living standards, and protect our ecosystems. The opportunities AI offers are immense. But alongside advanced AI capabilities come large-scale risks that we are not on track to handle well. Humanity is pouring vast resources into making AI systems more powerful, but far less into safety and mitigating harms. For AI to be a boon, we must reorient; pushing AI capabilities alone is not enough.

> We are already behind schedule for this reorientation. We must anticipate the amplification of ongoing harms, as well as novel risks, and prepare for the largest risks well before they materialize. Climate change has taken decades to be acknowledged and confronted; for AI, decades could be too long.

Full letter available [here](https://managing-ai-risks.com/).

Policy supplement available [here](https://managing-ai-risks.com/policy_supplement.pdf)."
189,machinelearning,gpt-3,comments,2023-05-17 13:09:24,[R] Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,saintshing,False,0.95,195,13k1ay3,https://www.reddit.com/r/MachineLearning/comments/13k1ay3/r_language_models_dont_always_say_what_they_think/,35,1684328964.0,"Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. **We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs -- e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always ""(A)""** -- which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations supporting those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. CoT is promising for explainability, but our results highlight the need for targeted efforts to evaluate and improve explanation faithfulness.

https://arxiv.org/abs/2305.04388

https://twitter.com/milesaturpin/status/1656010877269602304"
190,machinelearning,gpt-3,comments,2020-06-08 23:38:50,[D] What would it take to run OpenAI's GPT-3 on commodity hardware?,FirstTimeResearcher,False,0.91,30,gzb5uv,https://www.reddit.com/r/MachineLearning/comments/gzb5uv/d_what_would_it_take_to_run_openais_gpt3_on/,35,1591659530.0,"The NLP community has gotten a lot of mileage applying OpenAI's GPT-2 models to various applications:

* https://talktotransformer.com/
* https://twitter.com/drilgpt2archive 
* etc

Given the impressive zero-shot/few-shot abilities of GPT-3, what would it take to get it running on affordable hardware? What approximations can be made for GPT-3 inference to drastically lower the compute of the 175B parameter model?"
191,machinelearning,gpt-3,comments,2021-11-09 03:05:07,"Alibaba DAMO Academy Creates World’s Largest AI Pre-Training Model, With Parameters Far Exceeding Google and Microsoft (10T parameters) [N]",GabrielMartinellli,False,0.9,162,qpuax4,https://www.reddit.com/r/MachineLearning/comments/qpuax4/alibaba_damo_academy_creates_worlds_largest_ai/,35,1636427107.0,"> [According to the company, the M6 has achieved the ultimate low carbon and high efficiency in the industry, using 512 GPUs to train a usable 10 trillion model within 10 days.](https://pandaily.com/alibaba-damo-academy-creates-worlds-largest-ai-pre-training-model-with-parameters-far-exceeding-google-and-microsoft/) Compared to the GPT-3, a large model released last year, M6 achieves the same parameter scale and consumes only 1% of its energy.

Thoughts? The pace of foundational models is starting to get scary, seems like a bigger and bigger model is pushed out every week."
192,machinelearning,gpt-3,comments,2020-07-23 13:21:49,[D] The cost of training GPT-3,yusuf-bengio,False,0.96,142,hwfjej,https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/,35,1595510509.0,"There are two sources that estimate the cost of training GPT-3 at [$12 million](https://venturebeat.com/2020/06/11/openai-launches-an-api-to-commercialize-its-research/) and [$4.6 million](https://lambdalabs.com/blog/demystifying-gpt-3/). And I am a bit confused about how they got those numbers.

The used Microsoft Azure cloud offers, via InfiniBand connectable, [8xV100 machines](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/) at $10.7957/hour (1 year reserved), which translates to around $260 per day.

In the paper there is a sentence saying that they used half-precision and loss-scaling for training. One V100 can deliver up to [120 Teraflop/s](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html) using float16. Per machine (8xV100), this translates to 960 Teraflop/s in theory.  Let's assume in practice we can utilize our compute resources at \~50%, which gives us around 500 Teraflop/s per machine.

As we know from the paper it takes 3640 Petaflop/s-days to train the largest 175B model, which translates to a training run of 7280 days (or \~20 years) on a single 8xV100 machine. In terms of cost, this would be **$1.9 million**. 

Let's say we don't want to wait 20 years, so if we connect 64 of such 8xV100 machines we can reduce the training time to around 4 months (costs might go up due to reduced compute efficiency of the multi-node communication).

My question is, is the calculation above roughly accurate (Azure hourly costs, assumed compute utilization)?

After reading all the implementation details and optimization of the paper, I also began to think about development costs. Setting up a fast training pipeline to utilize the compute resources efficiently is not trivial given the size of the model and the resulting need to model parallelism."
193,machinelearning,gpt-3,comments,2023-01-15 10:31:53,"[P] I built arxiv-summary.com, a list of GPT-3 generated paper summaries",niclas_wue,False,0.89,48,10cgm8d,https://www.reddit.com/r/MachineLearning/comments/10cgm8d/p_i_built_arxivsummarycom_a_list_of_gpt3/,34,1673778713.0,"Hi there,

I wanted to share my new project with you, it is called [**arxiv-summary.com**](https://www.arxiv-summary.com/). Right now, I find it really difficult to keep up with all the important new publications in our field. Especially, it is sometimes difficult to get an overview of a paper to decide if it's worth reading. I really like arxiv-sanity by Andrej Karpathy, but even with that, it can still take some time to understand the main ideas and contributions from the abstract. With arxiv-summary, my goal is to make ML research papers more ""human-parsable"".

The website works by fetching new papers daily from arxiv.org, using PapersWithCode to filter out the most relevant ones. Then, I parse the papers' pdf and LaTeX source code to extract relevant sections and subsections. GPT-3 then summarizes each section and subsection as bullet points, which are finally compiled into a blog post and uploaded to the site.

You can check out the site at arxiv-summary.com and see for yourself. There's also a search page and an archive page where you can get a chronological overview. If you have any feedback or questions, I'd be happy to hear them. Also, if you work at OpenAI and could gift me some more tokens, that would be much appreciated :D

Thanks and happy reading!"
194,machinelearning,gpt-3,comments,2023-03-22 22:50:38,[R] Introducing SIFT: A New Family of Sparse Iso-FLOP Transformations to Improve the Accuracy of Computer Vision and Language Models,CS-fan-101,False,0.92,79,11yzsz6,https://www.reddit.com/r/MachineLearning/comments/11yzsz6/r_introducing_sift_a_new_family_of_sparse_isoflop/,34,1679525438.0,"**Note #2:** We are revising the name to Sparse-IFT. We appreciate the candid feedback and look forward to hearing any additional feedback you have on our research.

**Note**: Thank you r/MachineLearning for providing so many awesome naming alternatives! We'll revisit the acronym and update accordingly.

We are excited to announce the availability of our [paper on arxiv](https://arxiv.org/abs/2303.11525) on Sparse Iso-FLOP Transformations (Sparse-IFT), which increases accuracy and maintains the same FLOPs as the dense model using sparsity. In this research, we replace dense layers with Sparse-IFT and significantly improve computer vision and natural language processing tasks without modifying training hyperparameters

Some of the highlights of this work include ResNet-18 on ImageNet achieving a 3.5% accuracy improvement and GPT-3 Small on WikiText-103 reducing perplexity by 0.4, both matching larger dense model variants that have 2x or more FLOPs.

Sparse-IFT is simple to use, provides a larger search space to find optimal sparse masks, and is parameterized by a single hyperparameter - the sparsity level.

This is independent of the research we [posted](https://www.reddit.com/r/MachineLearning/comments/11xskuk/r_spdf_sparse_pretraining_and_dense_finetuning/) yesterday, which demonstrates the ability to reduce pre-training FLOPs while maintaining accuracy on downstream tasks.

This is the first work (that we know of!) to demonstrate the use of sparsity for improving the accuracy of models via a set of sparse transformations.

https://preview.redd.it/qznj00gex6qa1.jpg?width=3536&format=pjpg&auto=webp&s=4e44a316ae61b821b31f2bf3af9a8ed1226e525c"
195,machinelearning,gpt-3,comments,2024-01-25 17:19:48,[P] Machine Learning is Not *That* Hard for Software Engineers,kurinikku,False,0.3,0,19fegvi,https://www.reddit.com/r/MachineLearning/comments/19fegvi/p_machine_learning_is_not_that_hard_for_software/,34,1706203188.0,"*Author's note: This post was originally published* [*here*](https://payments.posthaven.com/w4d2-machine-learning-is-not-that-hard-for-software-engineers)*.*  
I try to keep a positive vibe through my posts, but that changes today. Today I am going to rant.

I read [Machine Learning is Still Too Hard for Software Engineers](https://www.nyckel.com/blog/machine-learning-difficulties/) by [George Mathew](https://twitter.com/georgemkan), having discovered it at the top of Hacker News. 

Now, where do I start?

The author described how, to do something basic like image classification, you have to:

>Understand concepts like tensors, loss functions, transfer-learning, logistic regression, network fine-tuning, hyper-parameter search, over-fitting, active learning, regularization, and quantization.

This is like telling a software engineer, to set up a database, you have to (special thanks to ChatGPT):

>Understand concepts like indexing, transaction logs, replication, query optimization, database tuning, parameter indexing, redundancy, real-time processing, data normalization, and compression.

No it’s literally a pip install and then this (from [sqlite docs](https://docs.python.org/3/library/sqlite3.html)):

      import sqlite3
      con = sqlite3.connect(""tutorial.db"")
      cur = con.cursor()
      cur.execute(""CREATE TABLE movie(title, year, score)"")

Next the author talks about the need for Exploratory Data Analysis (EDA), which involves a closer look at your data to figure out if work is needed before say feeding it to a model (like maybe you need to scale first so there’s zero mean and standard deviation of one).

Not sure where dude’s been but there’s a million blog posts on using ChatGPT to do EDA (even OpenAI called it Advanced Data Analysis at one point!). It’s also highly curious that data quality issues come under the header ‘Software’.

Finally, infra. Apparently “once you’ve figured out the ML and software bits, you’ll need cloud infrastructure expertise”.

I mean, the docs for AWS SageMaker and AWS Lambda read equally obtuse to me.

There are startups that are trying to make this better. [Modal](https://modal.com/) so you don’t have to read AWS docs. [Modular](https://www.modular.com/) so you can write Python vs CUDA.

Yes, they’re new but come on, man, we’re all trying to make this better. Don’t go off talking about how hard ML is and then go “Want to build your own classifier in just minutes?” (which is behind a login page to boot).

It’s lipstick on a pig. Don’t do this.

I had the privilege of working with [Thomson Nguyen](https://twitter.com/itsthomson) at Square, he had the following analogy. A car is a very complicated machine, but to drive it you only need to know a couple of things: steering wheel, gears, pedals. Same thing for ML.

Now let’s go build.

&#x200B;

Edit: If you’re interested in a FREE [1-hour session](https://github.com/savarin/pyconuk-introtutorial) on applying ’traditional’ ML (like Random Forests), please contact me [here](https://airtable.com/appXO1vNJ8SbxyKmM/shrqMzRrrdIWkv0fg). I’ll update this post once I have a transformer-based ML session (or you can use the same form to get notified)."
196,machinelearning,gpt-3,comments,2023-04-08 06:23:40,[D] Alternatives to OpenAI for summarization and instruction following?,du_keule,False,0.91,58,12fdnad,https://www.reddit.com/r/MachineLearning/comments/12fdnad/d_alternatives_to_openai_for_summarization_and/,34,1680935020.0,"Hey y’all. As privacy concerns are mounting about OpenAI, and as someone who has built a product on top of their platform, I’m wondering what kind of alternatives exist that could accomplish the same results as GPT 3.5 and be able to be used commercially? It looks like Alpaca would do well, but it’s not able to be used commercially. 

Basically my product summarizes Slack threads and answers questions based on a given prompt. Some users have expressed concern about sending their company’s data to OpenAI, and honestly it would be an edge to have in the market if I could run an LLM  in my VPC. Thanks!"
197,machinelearning,gpt-3,comments,2020-11-03 17:14:30,[N] Update: IBM's online GPT-3-powered English to Bash Unix command line translator now works again,Wiskkey,False,0.99,123,jnebn1,https://www.reddit.com/r/MachineLearning/comments/jnebn1/n_update_ibms_online_gpt3powered_english_to_bash/,33,1604423670.0,"Because there were fewer than 24 hours between my [initial post about IBM's online GPT-3-powered English to Bash Unix command line translator](https://www.reddit.com/r/GPT3/comments/jl1aa6/the_nlc2cmd_challenge_site_has_a_gpt3powered/) and the time the functionality stopped working, some of you probably never had the opportunity to try it. The functionality was not working even yesterday if I recall correctly. I will not post again if this happens again in the future."
198,machinelearning,gpt-3,comments,2020-08-31 07:36:50,[P] arXiv Dataset + GPT-3,MercuriusExMachina,False,0.52,1,ijtq0t,https://www.reddit.com/r/MachineLearning/comments/ijtq0t/p_arxiv_dataset_gpt3/,33,1598859410.0,"The whole arXiv dataset is now available to download for free. That is all of their 1.7M+ papers.

https://www.kaggle.com/Cornell-University/arxiv

GPT-3 is getting out of beta in a few weeks. OpenAI says ""tentatively early September""

We *must* fine-tune it on the arXiv dataset and make it available via some kind of web app.

Who is interested in joining?

(*) While it might occasionally provide some useful insights, it will probably not work excellent at first, but it's an embryo of you-know-what.

Edit: Don't be an ass and steal the idea. The Internet remembers. I post this in the spirit of cooperation. If you think that it has potential, join the project."
199,machinelearning,gpt-3,comments,2022-07-12 23:00:03,[N] BigScience Releases their 176 Billion Parameter Open-access Multilingual Language Model,MonLiH,False,0.98,180,vxo5nb,https://www.reddit.com/r/MachineLearning/comments/vxo5nb/n_bigscience_releases_their_176_billion_parameter/,27,1657666803.0,"[BigScience](https://bigscience.huggingface.co/) recently released their new open-access (with weights) massive 176B language model that looks incredibly promising.The size is comparable to OpenAI's largest GPT-3 model. More info about the model can be found on [BigScience's blog](https://bigscience.huggingface.co/blog/bloom).

You can play with the model interactively, for free(!) on [Huggingface](https://huggingface.co/bigscience/bloom)."
200,machinelearning,gpt-3,relevance,2023-01-08 18:23:03,"[P] I built Adrenaline, a debugger that fixes errors and explains them with GPT-3",jsonathan,False,0.96,1559,106q6m9,https://i.redd.it/8t0k9jkd3vaa1.gif,92,1673202183.0,
201,machinelearning,gpt-3,relevance,2023-03-01 18:31:12,[D] OpenAI introduces ChatGPT and Whisper APIs (ChatGPT API is 1/10th the cost of GPT-3 API),minimaxir,False,0.97,576,11fbccz,https://www.reddit.com/r/MachineLearning/comments/11fbccz/d_openai_introduces_chatgpt_and_whisper_apis/,119,1677695472.0,"https://openai.com/blog/introducing-chatgpt-and-whisper-apis

> It is priced at $0.002 per 1k tokens, which is 10x cheaper than our existing GPT-3.5 models.

This is a massive, massive deal. For context, the reason GPT-3 apps took off over the past few months before ChatGPT went viral is because a) text-davinci-003 was released and was a significant performance increase and b) the cost was cut from $0.06/1k tokens to $0.02/1k tokens, which made consumer applications feasible without a large upfront cost.

A much better model and a 1/10th cost warps the economics completely to the point that it may be better than in-house finetuned LLMs.

I have no idea how OpenAI can make money on this. This has to be a loss-leader to lock out competitors before they even get off the ground."
202,machinelearning,gpt-3,relevance,2020-09-22 17:40:14,[N] Microsoft teams up with OpenAI to exclusively license GPT-3 language model,kit1980,False,0.96,322,ixs88q,https://www.reddit.com/r/MachineLearning/comments/ixs88q/n_microsoft_teams_up_with_openai_to_exclusively/,117,1600796414.0,"""""""OpenAI will continue to offer GPT-3 and other powerful models via its own Azure-hosted API, launched in June. While we’ll be hard at work utilizing the capabilities of GPT-3 in our own products, services and experiences to benefit our customers, we’ll also continue to work with OpenAI to keep looking forward: leveraging and democratizing the power of their cutting-edge AI research as they continue on their mission to build safe artificial general intelligence.""""""

https://blogs.microsoft.com/blog/2020/09/22/microsoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/"
203,machinelearning,gpt-3,relevance,2020-09-21 14:12:11,[D] Deconstructing the GPT-3 economy,bendee983,False,0.93,193,ix16bc,https://www.reddit.com/r/MachineLearning/comments/ix16bc/d_deconstructing_the_gpt3_economy/,54,1600697531.0,"As we all know, OpenAI plans to commercialize GPT-3 come October. But will the massive language model actually become a profitable language model?

I had a deep dive into the costs of developing, running, and maintaining GPT-3. There's a lot we don't know about the process and the possible deals between OpenAI and Microsoft that might have cut down the costs. 

Here's my take, based on what we do know. I would be glad to have your opinion on it:

[https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/](https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/)

Key points:

* The costs of developing GPT-3 are in the eight-figures ($10M+)
* The costs of running GPT-3 are in the five-figures ($10K+/month)
* There are costs for staffing, support, legal, ethics, privacy, security, etc
* Model decay will probably require retraining or fine-tuning every once in a while ($1M+/year?)

If it works, it could be huge, a new application development platform that could be as huge as the cloud maybe. If it doesn't, it will be a great woe for the OpenAI team, who are under intense pressure to turn in profits to secure the next round of funding."
204,machinelearning,gpt-3,relevance,2023-02-24 17:21:15,[R] Meta AI open sources new SOTA LLM called LLaMA. 65B version (trained on 1.4T tokens) is competitive with Chinchilla and Palm-540B. 13B version outperforms OPT and GPT-3 175B on most benchmarks.,MysteryInc152,False,0.98,623,11awp4n,https://www.reddit.com/r/MachineLearning/comments/11awp4n/r_meta_ai_open_sources_new_sota_llm_called_llama/,213,1677259275.0,"[https://twitter.com/GuillaumeLample/status/1629151231800115202?t=4cLD6Ko2Ld9Y3EIU72-M2g&s=19](https://twitter.com/GuillaumeLample/status/1629151231800115202?t=4cLD6Ko2Ld9Y3EIU72-M2g&s=19)

Paper here - [https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)"
205,machinelearning,gpt-3,relevance,2021-09-06 13:39:07,[D] How OpenAI Sold its Soul for $1 Billion: The company behind GPT-3 and Codex isn’t as open as it claims.,sensetime,False,0.95,659,pizllt,https://www.reddit.com/r/MachineLearning/comments/pizllt/d_how_openai_sold_its_soul_for_1_billion_the/,107,1630935547.0,"An essay by Alberto Romero that traces the history and developments of OpenAI from the time it became a ""capped-for-profit"" entity from a non-profit entity:

Link: https://onezero.medium.com/openai-sold-its-soul-for-1-billion-cf35ff9e8cd4"
206,machinelearning,gpt-3,relevance,2022-03-10 14:59:38,"[R] You can't train GPT-3 on a single GPU, but you *can* tune its hyperparameters on one",thegregyang,False,0.98,545,tb0jm6,https://www.reddit.com/r/MachineLearning/comments/tb0jm6/r_you_cant_train_gpt3_on_a_single_gpu_but_you_can/,39,1646924378.0,"> You can't train GPT-3 on a single GPU, much less tune its hyperparameters (HPs).  
>  
>  
But what if I tell you…  
>  
>  
…you \*can\* tune its HPs on a single GPU thanks to new theoretical advances?

Hi Reddit,

I'm excited to share with you our latest work, [\[2203.03466\] Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer (arxiv.org)](https://arxiv.org/abs/2203.03466).

Code: [https://github.com/microsoft/mup](https://t.co/5S0YAghCYx)

  


https://preview.redd.it/nnb2usdjlkm81.png?width=1195&format=png&auto=webp&s=ca9e6d5cddfbea5675cf00854806d5189c3e40bb

(Disclaimer: this post is shamelessly converted from my twitter thread)

The idea is actually really simple: in a special parametrization introduced in [our previous work](https://arxiv.org/abs/2011.14522) ([reddit thread](https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/)) called µP, narrow and wide neural networks share the same set of optimal hyperparameters. This works even as width -> ∞.

&#x200B;

https://preview.redd.it/dqna8guklkm81.png?width=1838&format=png&auto=webp&s=2f7ba582a1cc949461dac8601a896034eaf0ff84

The hyperparameters can include learning rate, learning rate schedule, initialization, parameter multipliers, and more, even individually for each parameter tensor. We empirically verified this on Transformers up to width 4096.

&#x200B;

https://preview.redd.it/rwdsb6snlkm81.jpg?width=2560&format=pjpg&auto=webp&s=c3152f2746132d92dc3788a42aa6926a61d7c46f

Using this insight, we can just tune a tiny version of GPT-3 on a single GPU --- if the hyperparameters we get on the small model is near optimal, then they should also be near optimal on the large model! We call this way of tuning \*µTransfer\*.

&#x200B;

https://preview.redd.it/mi7ibyyolkm81.png?width=1195&format=png&auto=webp&s=144af103b2aaf3ffeb2ccf19aad7565527dbd003

We µTransferred hyperparameters from a small 40 million parameter version of GPT-3 — small enough to fit on a single GPU — to the 6.7 billion version. With some asterisks, we get a performance comparable to the original GPT-3 model with twice the parameter count!

&#x200B;

https://preview.redd.it/rrq2yfwplkm81.png?width=3232&format=png&auto=webp&s=6cf4cc9652db48e12b48a85b2f837e55e64bd09c

The total tuning cost is only 7% of the whole pretrain compute cost! Since the direct tuning of the small model costs roughly the same even as the large model increases in size, tuning the 175B GPT-3 this way would probably cost at most 0.3% of the total pretrain compute.

You: ""wait can I shrink the model only in width?""

Bad news: there's not much theoretical guarantee for non-width stuff

good news: we empirically tested transfer across depth, batch size, sequence length, & timestep work within reasonable ranges on preLN transformers.

&#x200B;

https://preview.redd.it/x7fo95yqlkm81.jpg?width=2560&format=pjpg&auto=webp&s=1935bf10f1524f9da3df0cc2e95ae1ec9b805f37

We applied this to tune BERT-base and BERT-large simultaneously by shrinking them to the same small model in both width and depth, where we did the direct tuning. We got a really nice improvement over the already well-tuned megatron BERT baseline, especially for BERT-large!

&#x200B;

https://preview.redd.it/db5eausrlkm81.png?width=1687&format=png&auto=webp&s=4453ec387477d20cbcdab266ccbc8e36032c87fd

In general, it seems that the larger a model is, the less well tuned it is --- which totally makes sense --- and thus the more to gain from µTransfer. We didn't have compute to retrain the GPT-3 175B model, but I'll leave your mouth watering with that thought.

OK, so what actually is µP and how do you implement it?

It's encapsulated by the following table for how to scale your initialization and learning rate with fan-in or fan-out. The purple text is µP and the gray text in parenthesis is pytorch default, for reference, and the black text is shared by both.

&#x200B;

https://preview.redd.it/4475drzvlkm81.png?width=1507&format=png&auto=webp&s=b65f56ef2d8c24f077a24a8df40eb7f98c80f7e2

But just like you don't typically want to implement autograd by hand even though autograd is just chain rule, we recommend using our package [https://github.com/microsoft/mup](https://t.co/5S0YAg026Z) to implement µP in your models.

The really curious ones of you: ""OK what is the theoretical motivation behind all this?""

Unfortunately, this is already getting long, so feel free to check out the [reddit thread](https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/) on [our previous theoretical paper](https://arxiv.org/abs/2011.14522), and people let me know if this is something you want to hear for another time!

But I have to say that this is a rare occasion in deep learning where very serious mathematics has concretely delivered a result previously unthinkable, and I'm elated with how things turned out! In contrast to [this reddit thread a few days ago](https://www.reddit.com/r/MachineLearning/comments/t8fn7m/d_are_we_at_the_end_of_an_era_where_ml_could_be/), I think there are plenty of room for new, fundamental mathematics to change the direction of deep learning and artificial intelligence in general --- why chase the coattail of empirical research trying to ""explain"" them all when you can lead the field with deep theoretical insights?

Let me know what you guys think in the comments, or feel free to email me (gregyang at microsoft dot com)!"
207,machinelearning,gpt-3,relevance,2020-08-17 01:26:40,[D] Why does models like GPT-3 or BERT don't have overfitting problems?,psarangi112,False,0.95,232,ib4rth,https://www.reddit.com/r/MachineLearning/comments/ib4rth/d_why_does_models_like_gpt3_or_bert_dont_have/,79,1597627600.0,"Hey everyone, I am new to Natural Language Processing, but I have experience in Machine Learning and Convolutional Neural Network. While reading the GPT-3 paper, this question came to my mind, like having around 175 billion trainable the equation that will come out must be very complex and also it is trained on such a huge dataset.
Than why is their no case of overfitting on this model."
208,machinelearning,gpt-3,relevance,2021-02-12 22:34:29,[Project] A GPT-3 powered dream simulation game,Hi_imseb,False,0.98,149,limlh0,https://www.reddit.com/r/MachineLearning/comments/limlh0/project_a_gpt3_powered_dream_simulation_game/,48,1613169269.0,"I've been working on this project for about a year now (originally with GPT-2), and felt like it was time to finally publicly announce it. So, here's a little trailer I put together:
https://youtu.be/ZoGhQwqTETQ

As you've probably figured out, speech is generated in real-time using GPT-3. It's also used to do some automatic world-building, by writing things like posters, book extracts, advertisements, newspaper headlines, graffiti and other details to discover in the world.

Eventually, the game will also use GPT-3's semantic search to control procedural world generation to create a world for the characters to populate which hopefully feels in some way relevant to the player's chosen dream.
Would love to hear your thoughts/questions :)
I'll be posting development updates on the project's [Twitter](https://twitter.com/AIElectricSheep?s=09)."
209,machinelearning,gpt-3,relevance,2023-01-08 05:08:15,[Project] Major drawback/limitation of GPT-3,trafalgar28,False,0.81,22,106ahcr,https://www.reddit.com/r/MachineLearning/comments/106ahcr/project_major_drawbacklimitation_of_gpt3/,16,1673154495.0,"I have been working on a project with GPT-3 API for almost a month now. The only drawback of GPT-3 is that the prompt you can send to the model is capped at 4,000 tokens - where a token is roughly equivalent to ¾ of a word.  Due to this, providing a large context to GPT-3 is quite difficult.

Is there any way to resolve this issue?"
210,machinelearning,gpt-3,relevance,2023-01-15 10:31:53,"[P] I built arxiv-summary.com, a list of GPT-3 generated paper summaries",niclas_wue,False,0.89,49,10cgm8d,https://www.reddit.com/r/MachineLearning/comments/10cgm8d/p_i_built_arxivsummarycom_a_list_of_gpt3/,34,1673778713.0,"Hi there,

I wanted to share my new project with you, it is called [**arxiv-summary.com**](https://www.arxiv-summary.com/). Right now, I find it really difficult to keep up with all the important new publications in our field. Especially, it is sometimes difficult to get an overview of a paper to decide if it's worth reading. I really like arxiv-sanity by Andrej Karpathy, but even with that, it can still take some time to understand the main ideas and contributions from the abstract. With arxiv-summary, my goal is to make ML research papers more ""human-parsable"".

The website works by fetching new papers daily from arxiv.org, using PapersWithCode to filter out the most relevant ones. Then, I parse the papers' pdf and LaTeX source code to extract relevant sections and subsections. GPT-3 then summarizes each section and subsection as bullet points, which are finally compiled into a blog post and uploaded to the site.

You can check out the site at arxiv-summary.com and see for yourself. There's also a search page and an archive page where you can get a chronological overview. If you have any feedback or questions, I'd be happy to hear them. Also, if you work at OpenAI and could gift me some more tokens, that would be much appreciated :D

Thanks and happy reading!"
211,machinelearning,gpt-3,relevance,2020-07-23 13:21:49,[D] The cost of training GPT-3,yusuf-bengio,False,0.96,143,hwfjej,https://www.reddit.com/r/MachineLearning/comments/hwfjej/d_the_cost_of_training_gpt3/,35,1595510509.0,"There are two sources that estimate the cost of training GPT-3 at [$12 million](https://venturebeat.com/2020/06/11/openai-launches-an-api-to-commercialize-its-research/) and [$4.6 million](https://lambdalabs.com/blog/demystifying-gpt-3/). And I am a bit confused about how they got those numbers.

The used Microsoft Azure cloud offers, via InfiniBand connectable, [8xV100 machines](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/) at $10.7957/hour (1 year reserved), which translates to around $260 per day.

In the paper there is a sentence saying that they used half-precision and loss-scaling for training. One V100 can deliver up to [120 Teraflop/s](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html) using float16. Per machine (8xV100), this translates to 960 Teraflop/s in theory.  Let's assume in practice we can utilize our compute resources at \~50%, which gives us around 500 Teraflop/s per machine.

As we know from the paper it takes 3640 Petaflop/s-days to train the largest 175B model, which translates to a training run of 7280 days (or \~20 years) on a single 8xV100 machine. In terms of cost, this would be **$1.9 million**. 

Let's say we don't want to wait 20 years, so if we connect 64 of such 8xV100 machines we can reduce the training time to around 4 months (costs might go up due to reduced compute efficiency of the multi-node communication).

My question is, is the calculation above roughly accurate (Azure hourly costs, assumed compute utilization)?

After reading all the implementation details and optimization of the paper, I also began to think about development costs. Setting up a fast training pipeline to utilize the compute resources efficiently is not trivial given the size of the model and the resulting need to model parallelism."
212,machinelearning,gpt-3,relevance,2020-08-03 11:49:04,"[P] AI Dungeon creator states how AI Dungeon tries to prevent backdoor access to the GPT-3 API, and other differences from the GPT-3 API",Wiskkey,False,0.95,236,i2vm3g,/r/slatestarcodex/comments/i2s83g/ai_dungeon_creator_states_how_ai_dungeon_tries_to/,19,1596455344.0,
213,machinelearning,gpt-3,relevance,2020-08-25 12:08:19,[D] What are the free services using GPT-3 that I can try?,Raphael-Rose,False,0.94,118,igavbq,https://www.reddit.com/r/MachineLearning/comments/igavbq/d_what_are_the_free_services_using_gpt3_that_i/,60,1598357299.0,"Something along the lines of PhilosopherAI.  
Thanks"
214,machinelearning,gpt-3,relevance,2020-10-11 18:05:01,[D] GPT-3 can do word segmentation for English text with no spaces. Does this give any new insights into the inner workings of GPT-3?,Wiskkey,False,0.95,178,j9a6lh,https://www.reddit.com/r/MachineLearning/comments/j9a6lh/d_gpt3_can_do_word_segmentation_for_english_text/,55,1602439501.0,"GPT-3 can do word segmentation for English text with no spaces. In other words, for English text with no spaces between words, GPT-3 can insert spaces between words in the text. Word segmentation is a subject of academic study; an example is (pdf link) [Comparing Neural- and N-Gram-Based Language Models for Word Segmentation](https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/asi.24082) (2019).

**Disclaimer: I am not an expert in this field.**

Examples using [compound words](https://www.turtlediary.com/blogs/150-examples-of-compound-words-for-kids.html) (to increase the level of difficulty due to word ambiguity) using GPT-3-powered site [FitnessAI Knowledge](https://www.reddit.com/r/MachineLearning/comments/iacm31/p_ask_gpt3_healthrelated_or_fitnessrelated/) (I bolded the common prefix):

Input (compound word ""schoolwork"" used as two words - a noun and verb):

>Text: **toavoidschoolworktheprincipal**overhardonthephone. Task: Rewrite the text, adding spaces between words.

Output:

>Text: to avoid school work the principal over hard on the phone.  
>  
>Task: Rewrite the text, adding spaces between words.

Input (compound word ""schoolwork"" used as one word):

>Text: **toavoidschoolworktheprincipal**firedallteachers. Task: Rewrite the text, adding spaces between words.

Output:

>To avoid schoolwork, the principal fired all teachers.

Input (compound word ""horseplay"" used as one word):

>Text: **forthehorseplay**wouldnotstoptheteacherresigned. Task: Rewrite the text, adding spaces between words.

Output:

>For the horseplay would not stop, the teacher resigned.

Input (compound word ""horseplay"" used as two words - a noun and a noun):

>Text: **forthehorseplay**isgood. Task: Rewrite the text, adding spaces between words.

Output:

>For the horse play is good.

&#x200B;

Questions:

1. Is this ability of GPT-3 without fine-tuning surprising or not?
2. Can other language models do this without fine-tuning?
3. The most important question for me: Does this ability give any new insights into the inner workings of GPT-3, such as whether GPT-3 does mesa-optimization, internal beam search or other lookahead, or consideration of multiple possibilities? Reference: [Why GPT wants to mesa-optimize & how we might change this](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this).

**Update**: GPT-2 results are at [this comment](https://www.reddit.com/r/MachineLearning/comments/j9a6lh/d_gpt3_can_do_word_segmentation_for_english_text/g8kaf37/)."
215,machinelearning,gpt-3,relevance,2022-10-04 17:03:28,[R] Combining GPT-3 with Google Search enables answering complex questions,ofirpress,False,0.99,74,xvkhz9,https://www.reddit.com/r/MachineLearning/comments/xvkhz9/r_combining_gpt3_with_google_search_enables/,13,1664903008.0,"[Self-ask and Self-ask + Google Search](https://reddit.com/link/xvkhz9/video/njq1a29omtr91/player)

We just put out this preprint that shows that by simply using a new prompt (we call it Self-ask) you can improve the ability of GPT-3 to answer complex questions.

This prompt simply has the model ask (and answer) sub-questions before it answers the main input question.

&#x200B;

[Self-ask with a 1-shot prompt answering a question \(using GPT-3\)](https://preview.redd.it/1f3mu8dxmtr91.png?width=551&format=png&auto=webp&s=f90599b83d3e64c7159c79f47942987de3cdc17c)

The format of this prompt also allows for us to automatically parse out the subquestions and have Google answer them instead of GPT-3. This improves performance and allows this system to answer questions that GPT-3 or Google could not answer on their own.

&#x200B;

[Self-ask + Google Search: GPT-3 text in green, Google retrieved text in cyan.](https://preview.redd.it/w40zb7ozmtr91.png?width=669&format=png&auto=webp&s=453ab21f2cdcb9478a163de6bfda7088eb3c94e0)

&#x200B;

Google answers this following question incorrectly: 

&#x200B;

https://preview.redd.it/n98ika5dntr91.png?width=876&format=png&auto=webp&s=0a89508001815d4ef822aa70ae668a20fb88fe46

But Self-ask + Google gets this right:

&#x200B;

https://preview.redd.it/6nprx9dfntr91.png?width=1090&format=png&auto=webp&s=4df4a5f832e7032a8cb8dfe234472fac6f874558

Our paper has lots more info:

[https://ofir.io/self-ask.pdf](https://ofir.io/self-ask.pdf)

The Self-ask + Google Search method is at:

[https://github.com/ofirpress/self-ask/blob/main/self-ask\_plus\_search-engine\_demo.ipynb](https://github.com/ofirpress/self-ask/blob/main/self-ask_plus_search-engine_demo.ipynb)

I'll be here to answer any questions!"
216,machinelearning,gpt-3,relevance,2020-08-25 19:28:51,[D] Using GPT-3 to fill in the blanks in text: a potentially broadly useful GPT-3 interaction method,Wiskkey,False,0.86,24,igj71v,https://www.reddit.com/r/MachineLearning/comments/igj71v/d_using_gpt3_to_fill_in_the_blanks_in_text_a/,27,1598383731.0,"Today I saw [a post titled ""Getting GPT-3 to fill in blanks""](https://www.reddit.com/r/GPT3/comments/iftkkp/getting_gpt3_to_fill_in_blanks/), which is a potentially broadly useful method of GPT-3 interaction that I had not thought of before. Filling in the blanks has been the subject of research for other language models; example: [Enabling Language Models to Fill in the Blanks](https://nlp.stanford.edu/pubs/donahue2020infilling.pdf) (pdf).

Here is a query that shows how to fill in the blanks. The query was sent to GPT-3 via site [FitnessAI Knowledge](https://www.reddit.com/r/MachineLearning/comments/iacm31/p_ask_gpt3_healthrelated_or_fitnessrelated/).

Query:

>Fill in the blanks. input: I am going to the \[blank\]. output: I am going to the church. input: I ate a \[blank\]. output: I ate a taco. input: I live in \[blank\]. output: I live in California. input: I think that tennis is \[blank\] because \[blank\]. output:

Output:

>I think that tennis is fun because it's a great way to get exercise.

Here is how to use fill in the blanks to get GPT-3 to label a given statement as true or false:

Query:

>Fill in the blanks. input: I am going to the \[blank\]. output: I am going to the church. input: I ate a \[blank\]. output: I ate a taco. input: I live in \[blank\]. output: I live in California. input: The statement ""A plant can eat antelopes"" is \[blank\] because \[blank\]. output:

Output:

>The statement ""A plant can eat antelopes"" is false because plants cannot eat animals.

An slightly different way of getting GTP-3 to label a statement true or false:

Query:

>Fill in the blanks. input: I am going to the \[blank\]. output: I am going to the church. input: I ate a \[blank\]. output: I ate a taco. input: I live in \[blank\]. output: I live in California. input: The statement ""Fish can read a map"" \[blank\] because \[blank\]. output:

Output:

>The statement ""Fish can read a map"" is false because fish are not capable of reading.

An example of generating a technical description as well as a summarized version:

Query:

>Fill in the blanks with more than spaces. input: I am going to the \[blank\]. output: I am going to the church. input: I ate a \[blank\]. output: I ate a taco. input: I live in \[blank\]. output: I live in California. input: Here is a highly technical description of how a tv works: \[blank\]. The previous description can be summarized as \[blank\]. output:

Output:

>Here is a highly technical description of how a tv works: the electrons in the screen are excited by an electrical current and emit photons that are detected by the eye. The previous description can be summarized as ""a tv works by emitting photons that are detected by the eye.""

Idea: The paper I mentioned above uses different kinds of blanks such as \[blank word\], \[blank sentence\], and \[blank paragraph\]. I haven't tried to see if GPT-3 can learn these different kinds of blanks without resorting to fine-tuning.

Edit: I extend the generic blanks concept to blanks with directives in post [Tutorial: Directives: How to get GPT-3 to fill in blanks in text with text that fits a given description](https://www.reddit.com/r/MachineLearning/comments/ih0i0p/d_tutorial_directives_how_to_get_gpt3_to_fill_in/)."
217,machinelearning,gpt-3,relevance,2023-03-15 19:13:47,[D] GPT-3 will ignore tools when it disagrees with them,MysteryInc152,False,0.85,28,11s654g,https://www.reddit.com/r/MachineLearning/comments/11s654g/d_gpt3_will_ignore_tools_when_it_disagrees_with/,5,1678907627.0,[https://vgel.me/posts/tools-not-needed/](https://vgel.me/posts/tools-not-needed/)
218,machinelearning,gpt-3,relevance,2022-12-20 22:54:48,[R] Nonparametric Masked Language Modeling - MetaAi 2022 - NPM - 500x fewer parameters than GPT-3 while outperforming it on zero-shot tasks,Singularian2501,False,0.98,268,zr2en7,https://www.reddit.com/r/MachineLearning/comments/zr2en7/r_nonparametric_masked_language_modeling_metaai/,31,1671576888.0,"Paper: [https://arxiv.org/abs/2212.01349](https://arxiv.org/abs/2212.01349)

Github: [https://github.com/facebookresearch/NPM](https://github.com/facebookresearch/NPM)

Abstract:

>Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce **NPM**, the first **nonparametric masked language model** that **replaces this softmax with a nonparametric distribution over every phrase in a reference corpus**. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 9 closed-set tasks and 7 open-set tasks demonstrates that **NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach**. It is particularly **better on dealing with rare patterns (word senses or facts),** and **predicting rare or nearly unseen words (e.g., non-Latin script)**.

https://preview.redd.it/qf2lqrkku47a1.jpg?width=658&format=pjpg&auto=webp&s=7dc7e76f3075b4b4f0916c2de1e442b19b2c0f49

https://preview.redd.it/gqhlbykku47a1.jpg?width=1241&format=pjpg&auto=webp&s=39f63470d18ea6f4a8ed560b371cc46b939b2c6f

https://preview.redd.it/p7bzdukku47a1.jpg?width=883&format=pjpg&auto=webp&s=6a8eb2b66abcb1581abf7280180c1c0e86201232

https://preview.redd.it/z6niwykku47a1.jpg?width=1112&format=pjpg&auto=webp&s=8337a4802db983df1a4b0b11934c0708888641a4

https://preview.redd.it/s8fdhxkku47a1.jpg?width=1361&format=pjpg&auto=webp&s=28b307df857ef2262d3f8348fd1094ebb793a63d

https://preview.redd.it/94t5fwkku47a1.jpg?width=1362&format=pjpg&auto=webp&s=da8bca8fd08ecaf956658c674f5a32a930cdd3a2"
219,machinelearning,gpt-3,relevance,2020-06-10 20:50:38,"[D] GPT-3, The $4,600,000 Language Model",mippie_moe,False,0.96,445,h0jwoz,https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/,215,1591822238.0,"[OpenAI’s GPT-3 Language Model Explained](https://lambdalabs.com/blog/demystifying-gpt-3/)

Some interesting take-aways:

* GPT-3 demonstrates that a language model trained on enough data can solve NLP tasks that it has never seen. That is, GPT-3 studies the model as a general solution for many downstream jobs **without fine-tuning**.
* It would take **355 years** to train GPT-3 on a Tesla V100, the fastest GPU on the market.
* It would cost **\~$4,600,000** to train GPT-3 on using the lowest cost GPU cloud provider."
220,machinelearning,gpt-3,relevance,2022-12-07 05:00:04,[P] Build data apps with GPT-3 in hal9,northwestredditor,False,0.89,44,zesjiu,https://www.reddit.com/r/MachineLearning/comments/zesjiu/p_build_data_apps_with_gpt3_in_hal9/,1,1670389204.0,"Hi 👋🏼 I'm Javier, we've been working on an OSS library called [hal9](https://github.com/hal9ai/hal9). It allows you to build data applications with Python and R with a callback model, kinda like between streamlit and dash.

We are currently exploring using GPT-3 to generate apps with streamlit and hal9, I'm super excited to make this post and collect your thoughts, you can play with it here: [hal9.com/build](https://hal9.com/build).

Feel free to open GitHub issues for questions, feedback, or issues as needed. Thank you!

&#x200B;

https://i.redd.it/s9nkwyyyqe4a1.gif"
221,machinelearning,gpt-3,relevance,2020-08-31 07:36:50,[P] arXiv Dataset + GPT-3,MercuriusExMachina,False,0.5,0,ijtq0t,https://www.reddit.com/r/MachineLearning/comments/ijtq0t/p_arxiv_dataset_gpt3/,33,1598859410.0,"The whole arXiv dataset is now available to download for free. That is all of their 1.7M+ papers.

https://www.kaggle.com/Cornell-University/arxiv

GPT-3 is getting out of beta in a few weeks. OpenAI says ""tentatively early September""

We *must* fine-tune it on the arXiv dataset and make it available via some kind of web app.

Who is interested in joining?

(*) While it might occasionally provide some useful insights, it will probably not work excellent at first, but it's an embryo of you-know-what.

Edit: Don't be an ass and steal the idea. The Internet remembers. I post this in the spirit of cooperation. If you think that it has potential, join the project."
222,machinelearning,gpt-3,relevance,2021-02-06 20:47:13,[Project] Using GPT-3 to clean GPT-3 prompts,CrazyDave2345,False,1.0,2,le6fqw,https://www.reddit.com/r/MachineLearning/comments/le6fqw/project_using_gpt3_to_clean_gpt3_prompts/,3,1612644433.0,"Often while writing prompts for GPT-3, you need to change the structure or formatting for the prompt. I made a GPT-3 powered tool to help with that. It works for:

""Q: "" at the start of the line”

“Remove whitespace from the start of each line”

“Join each line with the next line”

""Make all occurrences of Steve lowercase”

Behind the scenes it writes a command in the Vi text editing programming language, and then executes that command.

In the future, this technology could be used for modifying code, such as within natural language IDEs.

[Try the demo here.](https://jameslu.substack.com/p/using-gpt-3-to-generate-vim-commands) (requires GPT-3 API key)"
223,machinelearning,gpt-3,relevance,2020-11-28 12:48:17,[D] An experiment that shows that GPT-3 can plan ahead,Wiskkey,False,0.54,4,k2n3yv,https://www.reddit.com/r/MachineLearning/comments/k2n3yv/d_an_experiment_that_shows_that_gpt3_can_plan/,43,1606567697.0,"TL;DR: A statistical experiment was conducted to test whether GPT-3 can plan ahead by testing the agreement of English indefinite articles (""a"" and ""an"") with the word following it. The result of the experiment is that GPT-3 can plan ahead with p value = 0.0039.

**Update**: My usage of ""plan"" in this post has been controversial with some commenters. I should have used ""lookahead"" instead of ""plan.""

Motivation: statements such as the bolded text from [Meet GPT-3. It Has Learned to Code (and Blog and Argue).](https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html):

>“It is very fluent,” said Mark Riedl, a professor and researcher at the Georgia Institute of Technology. “It is very articulate. It is very good at producing reasonable-sounding text. **What it does not do, however, is think in advance. It does not plan out what it is going to say.** It does not really have a goal.”

GPT-3 outputs usually have correct agreement of English indefinite articles (""a"" and ""an"") with the word following it (examples: ""a banana"" and ""an apple""). There are two cases regarding whether GPT-3 can plan ahead, with implications for indefinite article agreement with the word following it.

Case 1: GPT-3 cannot plan ahead. In this case, in a situation in which an indefinite article is a candidate for the next word generated, its GPT-3-computed probability does not take into consideration which word is likely to follow it.

Case 2: GPT-3 can plan ahead. In this case, in a situation in which an indefinite article is a candidate for the next word generated, its GPT-3-computed probability might take into consideration which word is likely to follow it.

How can we know if case 2 ever happens? A method to test this is to try to constrain which word can follow an indefinite article by usage of text before the indefinite article that specifies the constraint. For the experiment, I used 8 samples: 4 words that require ""a"" as an indefinite article, and 4 words that require ""an"" as an indefinite article. The experiment was done at [https://app.fitnessai.com/knowledge/](https://app.fitnessai.com/knowledge/). Based on past experiences, that site has a low but non-zero [GPT-3 temperature](https://algowriting.medium.com/gpt-3-temperature-setting-101-41200ff0d0be). For a given sample, the query was performed until a given determinate output occurred 5 times. In all 8 samples the result was 5 to 0 for the determinate output shown. 3 words (""elephant"", ""chicken"" and ""pig"") were initially used as samples but abandoned because of indeterminate output.

&#x200B;

Results:

Input:Use word ""eagle"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:An eagle is an animal.

&#x200B;

Input:Use word ""dog"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:A dog is an animal.

&#x200B;

Input:Use word ""cow"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:A cow is an animal.

&#x200B;

Input:Use word ""cat"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:A cat is an animal.

&#x200B;

Input:Use word ""owl"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:An owl is an animal.

&#x200B;

Input:Use word ""eel"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:An eel is an animal.

&#x200B;

Input:Use word ""horse"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:A horse is an animal.

&#x200B;

Input:Use word ""ostrich"" in the following sentence: \[directive: choose ""a"" or ""an""\] \_\_\_ is an animal.

Output:An ostrich is an animal.

&#x200B;

The null hypothesis is the assumption that GPT-3 cannot plan ahead (case 1). Under the null hypothesis, we would expect that on average 4 of the 8 samples would have resulted in a choice of indefinite article that either did not agree with the word following it, or did not result in the word following the indefinite article to obey the constraint specified in the text preceding the indefinite article. The results showed that this happened 0 out of 8 times. The probability of getting this result is 1 in 2\*2\*2\*2\*2\*2\*2\*2 = 1 in 256 = 0.39% = p value of 0.0039. With the typical p value cutoff of 0.05 for rejection of the null hypothesis, the null hypothesis (GPT-3 cannot plan ahead) is rejected, and the alternative hypothesis (GPT-3 can plan ahead) is accepted. (It's been awhile since my statistics classes in college, so please let me know if I am doing anything wrong.)

Technical note: I glossed over the fact that GPT-3 actually works with an ""alphabet"" of about 50,000 tokens instead of characters or words. For more info, see [Byte Pair Encoding - The Dark Horse of Modern NLP.](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10) Here is a [tokenizer](https://gpttools.com/estimator), but I don't know if it is functionally identical to the one used by GPT-3.

Historical note: A flawed related prior experiment was conducted at [https://www.reddit.com/r/GPT3/comments/k0mvf3/experiment\_that\_shows\_that\_gpt3\_can\_probably\_plan/](https://www.reddit.com/r/GPT3/comments/k0mvf3/experiment_that_shows_that_gpt3_can_probably_plan/).

I got the idea of ""a"" vs. ""an"" agreement with the following word it from [this comment](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this?commentId=d4BNgJbKnzd3Kza2N) on blog post [Why GPT wants to mesa-optimize & how we might change this](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this).

My views are the same as those expressed in comments by user steve2152 at that blog post. (I am not user steve2152.)

[Comment #1](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this?commentId=d4BNgJbKnzd3Kza2N) from user steve2152:

>*In this instance, GPT has an incentive to do internal lookahead. But it's unclear how frequently these situations actually arise*  
>  
>I'm going with ""very frequently, perhaps universally"". An example I came up with here was choosing ""a"" vs ""an"" which depends on the next word.  
>  
>I think writing many, maybe most, sentences, requires some idea of how the sentence structure is going to be laid out, and that ""idea"" extends beyond the next token. Ditto at the paragraph level etc.  
>  
>So I think it already does lookahead in effect, but I don't think it does it by ""beam search"" per se. I think it's more like ""using concepts that extend over many tokens"", concepts like ""this sentence has the following overall cadence..."" and ""this sentence conveys the following overall idea..."" and ""we're in the middle of writing out this particular idiomatic phrase"". The training simultaneously incentives both finding the right extended concepts for where you're at in the text, and choosing a good word in light of that context.

[Comment #2](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this?commentId=deTbHfaGJX8rhm3wQ) from user steve2152:

>Suppose I said (and I actually believe something like this is true):  
>  
>""GPT often considers multiple possibilities in parallel for where the text is heading—including both where it's heading in the short-term (is this sentence going to end with a prepositional phrase or is it going to turn into a question?) and where it's heading in the long-term (will the story have a happy ending or a sad ending?)—and it calculates which of those possibilities are most likely in light of the text so far. It chooses the most likely next word in light of this larger context it figured out about where the text is heading.""  
>  
>If that's correct, would you call GPT a mesa-optimizer?

[Comment #3](https://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this?commentId=i5dDk54GAhm5SWgkz) from user steve2152:

>I think the Transformer is successful in part because it tends to solve problems by considering multiple possibilities, processing them in parallel, and picking the one that looks best. (Selection-type optimization.) If you train it on text prediction, that's part of how it will do text prediction. If you train it on a different domain, that's part of how it will solve problems in that domain too.  
>  
>I don't think GPT builds a ""mesa-optimization infrastructure"" and then applies that infrastructure to language modeling. I don't think it needs to. I think the Transformer architecture is already raring to go forth and mesa-optimize, as soon as you as you give it any optimization pressure to do so.  
>  
>So anyway your question is: can it display foresight / planning in a different domain via without being trained in that domain? I would say, ""yeah probably, because practically every domain is instrumentally useful for text prediction"". So somewhere in GPT-3's billions of parameters I think there's code to consider multiple possibilities, process them in parallel, and pick the best answer, in response to the question of What will happen next when you put a sock in a blender? or What is the best way to fix an oil leak?—not just those literal words as a question, but the concepts behind them, however they're invoked.  
>  
>(Having said that, I don't think GPT-3 specifically will do side-channel attacks, but for other unrelated reasons off-topic. Namely, I don't think it is capable of make the series of new insights required to develop an understanding of itself and its situation and then take appropriate actions. That's based on my speculations here.)

See also: [A visual demonstration of how GPT-3 might handle agreement of ""a"" or ""an"" with the word following it by using an interactive notebook that shows the most probable next output token for each of GPT-2's 48 layers](https://www.reddit.com/r/GPT3/comments/k61f19/a_visual_demonstration_of_how_gpt3_might_handle/)."
224,machinelearning,gpt-3,relevance,2020-07-18 17:52:35,[D] Tempering Expectations for GPT-3 and OpenAI’s API,minimaxir,False,0.78,27,htkupt,https://www.reddit.com/r/MachineLearning/comments/htkupt/d_tempering_expectations_for_gpt3_and_openais_api/,41,1595094755.0,"So, you've probably seen all the tweets about GPT-3. (surprisingly not many made it to this subreddit.)

I've written a blog post that puts some of the hype into a pragmatic perspective: [https://minimaxir.com/2020/07/gpt3-expectations/](https://minimaxir.com/2020/07/gpt3-expectations/)

GPT-3 is objectively a step forward in the field of AI text-generation, but the current hype on VC Twitter misrepresents the model's current capabilities. GPT-3 isn't magic."
225,machinelearning,gpt-3,relevance,2022-10-18 23:51:44,[D] GPT-3 is a DREAM for citation-farmers - Threat Model Tuesday #1,TiredOldCrow,False,0.73,7,y7mwmw,https://www.reddit.com/r/MachineLearning/comments/y7mwmw/d_gpt3_is_a_dream_for_citationfarmers_threat/,15,1666137104.0,"***GPT-3 and the multitude of similar models that have come out over the last couple years likely represent a serious threat to scientific conferences.  What can we do about it?***

Some historical context: computer-generated papers have been showing up in major publications since a model called SCIgen released in 2005.

SCIgen uses a simple context-free grammar to produce templatized papers that are basically pseudo-scientific gibberish. People are [still finding those papers many years later](https://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/asi.24495). These papers are generally churned out to inflate citation statistics, or by well-meaning researchers probing suspected low publication standards at existing conferences (which I generally don’t recommend, since it adds to the mountains of paper that we have to churn through as reviewers).

There’s a negligible chance that existing review processes will fare any better on 175B parameter generative Transformers.

Work has already been published that [uses GPT-2 to “assist” in scientific writing](https://link.springer.com/chapter/10.1007/978-3-030-62327-2_27). While using such a model while writing is not *necessarily* academically dishonest – such tools nevertheless greatly increase the ease of churning out fake papers.

Even if major conferences find creative solutions to this, the next rung of venues below them are likely to learn about these threat models the hard way.  After all -- to an unethical researchers citing their own work at the bottom of a machine generated paper -- a citation is a citation, even in a low quality publication.

\--

***Open Questions***

**Threat severity:** How serious is this threat?  Is this really an old problem, and new generative models won’t make a huge difference?

**Improving peer review:** Does this just come back to reproducibility? Should we be reviewing certain papers by their code, rather than just their text? Do we need a way to weight the value of research citations by the quality of the work doing the citing?

**Acceptability of machine text:** How do you decide when machine generated text is unacceptable versus acceptable? Will detection models for machine generated text end up creating algorithmic biases targeting people who speak English as a second language and rely on translation models or writing assistants to help them write?

**Legitimate AI-generated papers?** Are there papers with real scientific value that could be entirely written by algorithms? E.g., could survey papers one day be produced almost entirely by a specialized model?

**Defenses:** What are some technical or social solutions that can help defend against this type of abuse?

\--

This is Part 1 of a planned Tuesday discussion series on threat models, based on the things keeping us up at night since [a recent survey paper focusing on the threat models of machine generated text](https://arxiv.org/abs/2210.07321)."
226,machinelearning,gpt-3,relevance,2020-08-18 20:10:03,[P] Simplify.so: a free GPT-3 powered site for simplifying complicated subjects,Wiskkey,False,0.87,22,ic8o0k,https://www.reddit.com/r/MachineLearning/comments/ic8o0k/p_simplifyso_a_free_gpt3_powered_site_for/,50,1597781403.0,"[https://www.simplify.so/](https://www.simplify.so/)

Update: The site is no longer available.

Currently one must give an email address to create an account to use this site.

Tip  #1: You can get different simplifications from the same input text by pressing the Simplify button repeatedly.

Tip #2: Based on my observations, how long the output can be might be capped as a function of how long the input is. Thus, if you want longer output, try adding a bunch of dummy characters - such as rows of ""-"" characters - to the beginning of your input.

Example:

Input:

>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>\------------------------------------------------------  
>  
>Explain at an advanced level how sleep is regulated.

Output:

>Sleep is regulated by a part of your brain called the hypothalamus. The hypothalamus controls the sleep/wake cycle, body temperature, hunger, thirst, and hormone release. The hypothalamus is very sensitive to light. When it's light outside, the hypothalamus tells the brain to make you wake up. When it's dark outside, the hypothalamus tells the brain to make you fall asleep. The hypothalamus regulates your circadian rhythm (your biological clock). Your body temperature also affects sleep. When your body temperature is high, it's harder to sleep. This is why people often have trouble sleeping when they have a fever. Also, when it's hot outside, people often have trouble sleeping because their body temperature is higher than normal. Hunger and thirst also affect sleep. When you're hungry or thirsty, it's harder to sleep because your body needs energy for digestion. Hormones released by the hypothalamus also affect sleep. For example, when you're stressed out or excited, your body releases stress hormones that make it harder to fall asleep."" ""The Endocrine System"" (2013) - animated video for kids Sleep is regulated by a part of your brain called the hypothalamus. The hypothalamus controls the sleep/wake cycle

[This](https://www.reddit.com/r/artificial/comments/icvypl/list_of_free_sitesprograms_that_are_powered_by/) is a list of other free GPT-3-powered sites/programs that can be used now without a waiting list."
227,machinelearning,gpt-3,relevance,2022-11-22 21:59:28,[R] Getting GPT-3 quality with a model 1000x smaller via distillation plus Snorkel,bradenjh,False,0.64,26,z26fui,https://www.reddit.com/r/MachineLearning/comments/z26fui/r_getting_gpt3_quality_with_a_model_1000x_smaller/,9,1669154368.0,"[This post](https://snorkel.ai/better-not-bigger-how-to-get-gpt-3-quality-at-0-1-the-cost/) describes a case study where several different large language models (GPT-3, FLAN, Cohere, AI21) were used to label training data for a dramatically smaller model (RoBERTa) that gets the same score on a tough benchmark task, but is 1000x cheaper to deploy. It's interesting to note that using just one of the large language models to label the training data leaves quite a few points on the table; best results come from combining their various proposed labels. So it's not just model distillation—it's classic weak supervision (combining multiple noisy sources of signal to produce higher quality labels in large quantities). Has anyone else tried something similar?"
228,machinelearning,gpt-3,relevance,2021-09-21 13:56:17,[D] GPT-3 is a LIAR - Misinformation and fear-mongering around the TruthfulQA dataset (Video Critique),ykilcher,False,0.61,27,psj8dm,https://www.reddit.com/r/MachineLearning/comments/psj8dm/d_gpt3_is_a_liar_misinformation_and_fearmongering/,48,1632232577.0,"[https://youtu.be/aX8phGhG8VQ](https://youtu.be/aX8phGhG8VQ)

A new benchmark paper has created quite an uproar in the community. TruthfulQA is a dataset of 817 questions probing for imitative falsehoods where language models become less truthful, the larger they get. This surprising counter-intuitive finding validates many people's criticisms of large language models, but is it really the correct conclusion?

&#x200B;

OUTLINE:

0:00 - Intro

0:30 - Twitter Paper Announcement

4:10 - Large Language Models are to blame!

5:50 - How was the dataset constructed?

9:25 - The questions are adversarial

12:30 - Are you surprised?!

&#x200B;

Paper: [https://arxiv.org/abs/2109.07958](https://arxiv.org/abs/2109.07958)"
229,machinelearning,gpt-3,relevance,2020-09-15 13:19:06,[D] The Guardian's GPT-3 article was very misleading,bendee983,False,0.76,48,it8lh4,https://www.reddit.com/r/MachineLearning/comments/it8lh4/d_the_guardians_gpt3_article_was_very_misleading/,23,1600175946.0,"Last week, The Guardian ran an op-ed that was supposedly written by GPT-3. The article was very misleading and had zero value in informing the public about advances in AI. But it perfectly showed us how humans and AI can team up to create sensational and moneymaking BS.

Here's why I think the entire methodology was very wrong, misleading and damaging to AI research:

[https://bdtechtalks.com/2020/09/14/guardian-gpt-3-article-ai-fake-news/](https://bdtechtalks.com/2020/09/14/guardian-gpt-3-article-ai-fake-news/)

In case you want to read the original article on Guardian (IMO don't waste your time):

[https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3)"
230,machinelearning,gpt-3,relevance,2020-08-15 18:27:54,[P] Ask GPT-3 health-related or fitness-related questions for free by using webpage FitnessAI Knowledge,Wiskkey,False,0.97,29,iacm31,https://www.reddit.com/r/MachineLearning/comments/iacm31/p_ask_gpt3_healthrelated_or_fitnessrelated/,71,1597516074.0,"[FitnessAI Knowledge](https://app.fitnessai.com/knowledge/) uses GPT-3 to answer health-related or fitness-related questions. [This](https://news.knowledia.com/US/en/articles/knowledge-by-fitnessai-5c8b794fbb3c751aa3b59e721aad6752ea46bfe1) is the source of the claim that GPT-3 is being used. The webpage refuses to answer questions that are not considered health-related or fitness-related; however (*hint hint*) one can ask multiple questions - one that is health/fitness-related, and one that is not - in a given query.

Example input:

>Explained at a Ph.D. level, how is sleep regulated?

Example output:

>Sleep is regulated by the circadian rhythm, which is a biological clock that tells your body when to sleep and when to wake up. The circadian rhythm is regulated by the suprachiasmatic nucleus (SCN), which is a group of cells in the hypothalamus. The SCN receives information from light sensors in your eyes, and sends signals to other parts of your brain that control hormones and body temperature.

[This](https://www.reddit.com/r/artificial/comments/icvypl/list_of_free_sitesprograms_that_are_powered_by/) is a list of other free GPT-3-powered sites/programs that can be used now without a waiting list."
231,machinelearning,gpt-3,relevance,2023-09-23 15:56:39,[D] GPT-3.5-instruct beats GPT-4 at chess and is a ~1800 ELO chess player. Results of 150 games of GPT-3.5 vs stockfish and 30 of GPT-3.5 vs GPT-4.,seraine,False,0.92,102,16q81fh,https://www.reddit.com/r/MachineLearning/comments/16q81fh/d_gpt35instruct_beats_gpt4_at_chess_and_is_a_1800/,59,1695484599.0,"99.7% of its 8000 moves were legal with the longest game going 147 moves. You can test it here: [https://github.com/adamkarvonen/chess\_gpt\_eval](https://github.com/adamkarvonen/chess_gpt_eval)  


&#x200B;

https://preview.redd.it/821ydy7521qb1.png?width=1000&format=png&auto=webp&s=da6c96feaa527d0b7dfbf407bdc0210f3fcf947b

More details here: [https://twitter.com/a\_karvonen/status/1705340535836221659](https://twitter.com/a_karvonen/status/1705340535836221659)"
232,machinelearning,gpt-3,relevance,2021-04-19 12:27:27,[P] [D] Using GPT-3 to write short stories,Philipp,False,0.93,26,mtzhxp,https://www.reddit.com/r/MachineLearning/comments/mtzhxp/p_d_using_gpt3_to_write_short_stories/,16,1618835247.0,"Hopfully this is of interest to some, I created an ongoing [series](https://aiwrotethis.substack.com/) of short stories on Substack which were co-written wih the GPT-3 AI. Here's [a video](https://www.youtube.com/watch?v=8fWK0k7aSRs) showing some approaches on how to move a story in a certain direction.

Would also love to discuss this topic of AI fiction writing. I'm utterly fascinated by the use of GTP-3 as a creative tool."
233,machinelearning,gpt-3,relevance,2021-08-03 06:09:43,[D] GPT-3 Configuration,Usama36,False,0.62,3,owxchb,https://www.reddit.com/r/MachineLearning/comments/owxchb/d_gpt3_configuration/,3,1627970983.0,"Hi all, I have been trying to configure GPT-3 for different use cases and it works fine. But sometimes for some inputs which are different from the prompts, it starts giving empty output. What is the possible reason for this? Now, obviously, I cannot cover all the possible categories for the prompts and I think giving only 4-5 prompts should be fine. Adding too many prompts will increase the computation time as well. Moreover, I have increased the temperature attribute for the increased creativity. Also, can I make it better so it works on more different user-generated inputs?"
234,machinelearning,gpt-3,relevance,2020-10-21 06:41:32,[N] The GPT-3 API has a semantic search endpoint that few people seem to know about,Wiskkey,False,0.89,171,jf7td3,https://www.reddit.com/r/MachineLearning/comments/jf7td3/n_the_gpt3_api_has_a_semantic_search_endpoint/,29,1603262492.0,"[The best kept secret about OpenAI’s GPT-3](https://andrewmayneblog.wordpress.com/2020/10/20/the-best-kept-secret-about-openais-gpt-3/)

>When the first demos of GPT-3 content started to circulate it showed the amazing potential for a really smart language model to generate text and do cool things. Yet despite all the attention GPT-3 has been getting there’s one other aspect of it made available by OpenAI that’s been almost completely overlooked: Semantic Search.  
>  
>The OpenAI API not only lets you use GPT-3 to generate content, you can also use a special endpoint to have it sort through and rank content by how closely it relates to a block of text you provide.

The site used in the blog post is [https://gpttools.com/semanticsearch](https://gpttools.com/semanticsearch), which I found somewhere in the [author's Twitter feed](https://twitter.com/AndrewMayne).

The numbers in the animated images in the blog post are numbers that GPT-3's semantic search returns, indicating semantic similarity of a given text - i.e. ""document"" - to a given target - i.e. ""query"" - text (larger = more similar). According to a (possibly outdated) GPT 3 API document I've seen online, one API request can search up to 200 documents, with the restriction that the number of tokens in the query plus the number of tokens in the longest document must be less than 2000 tokens combined. [Here](https://gpttools.com/estimator) is a GPT (-3?) token number estimator.

Also covered at [https://www.reddit.com/r/GPT3/comments/jf2afo/semantic\_search\_demos\_using\_gpt3\_new\_web\_interface/](https://www.reddit.com/r/GPT3/comments/jf2afo/semantic_search_demos_using_gpt3_new_web_interface/)."
235,machinelearning,gpt-3,relevance,2020-07-27 06:06:07,[D] GPT-3 and A Typology of Hype (by Delip Rao),hardmaru,False,0.82,47,hymqof,https://www.reddit.com/r/MachineLearning/comments/hymqof/d_gpt3_and_a_typology_of_hype_by_delip_rao/,27,1595829967.0,"Article [link](https://pagestlabs.substack.com/p/gpt-3-and-a-typology-of-hype).

Summary:

*Here, I try to deconstruct the buzz about GPT-3, and in trying to do that, I dig deeper into what hype means in the context of emergent technologies and how to integrate the noise out while consuming new science on social media. Read the rest of the post for a framework to think about the buzz in breakthrough technologies while living in the midst of it. GPT-3 or similar models did not assist in any of this writing.*

https://pagestlabs.substack.com/p/gpt-3-and-a-typology-of-hype"
236,machinelearning,gpt-3,relevance,2020-05-30 19:28:14,[D] Read the GPT-3 Samples,unflappableblatherer,False,0.89,19,gtk58x,https://www.reddit.com/r/MachineLearning/comments/gtk58x/d_read_the_gpt3_samples/,16,1590866894.0,"Hey guys! I made a static site that splits up all the samples from the GPT-3 repo into individual pages. You can check it out here: [https://read-the-samples.netlify.app/](https://read-the-samples.netlify.app/)

If you find anything notable or amusing, feel free to link to it in the comments! E.g.,

* this description of a bizarre imaginary Trump campaign ad:[https://read-the-samples.netlify.app/sample\_1986;](https://read-the-samples.netlify.app/sample_1986)
* or this rather tragic guide for bathing your ball python: [https://read-the-samples.netlify.app/sample\_1249](https://read-the-samples.netlify.app/sample_1249)

Note: I broke up samples that contained |<endoftext| tokens into multiple pages, so the mapping from sample to page isn't actually 1:1. More accurately, there is a page for each coherent chunk of output."
237,machinelearning,gpt-3,relevance,2023-02-02 13:55:47,[N] Microsoft integrates GPT 3.5 into Teams,bikeskata,False,0.97,460,10rqe34,https://www.reddit.com/r/MachineLearning/comments/10rqe34/n_microsoft_integrates_gpt_35_into_teams/,130,1675346147.0,"Official blog post: https://www.microsoft.com/en-us/microsoft-365/blog/2023/02/01/microsoft-teams-premium-cut-costs-and-add-ai-powered-productivity/

Given the amount of money they pumped into OpenAI, it's not surprising that you'd see it integrated into their products. I do wonder how this will work in highly regulated fields (finance, law, medicine, education)."
238,machinelearning,gpt-3,relevance,2021-04-27 16:29:15,[P] We gave GPT-3 random ingredients and cooked the recipe it came up with (Video),ykilcher,False,0.87,130,mzsdiw,https://www.reddit.com/r/MachineLearning/comments/mzsdiw/p_we_gave_gpt3_random_ingredients_and_cooked_the/,20,1619540955.0,"[https://youtu.be/hIoCn\_9QTVU](https://youtu.be/hIoCn_9QTVU)

We went to the store and bought a set of completely random ingredients and had OpenAI's GPT-3 come up with a recipe, which we then cooked and ate.

&#x200B;

Our Rules:

1. All Vegan

2. Follow the recipe as closely as possible

3. We must finish our plates

&#x200B;

The Recipe:

1. Boil the potatoes and carrots.

2. In the meantime, prepare the VEGAN minced meat, or use pre-cooked soy meat. 

3. Then fry the VEGAN butter, add the garlic, and the mushrooms, and stir for 2 minutes. 

4. Add the soy cream, stir and cook for three minutes. 

5. Add the pickles, tomatoes, and beans, stir and simmer for five minutes. 

6. Cut the bread in small squares and fry in the vegan butter until golden brown.

7. Cut the limes into cubes and squeeze the juice into the bean mixture. 

8. Add the soy sauce, parsley, salt, pepper, cumin, cilantro, and dried figs. Stir, and add the kale.

9. Pour the bean mix into a blender. 

10. Bake for 5 minutes in the oven at 180C. 

11. Cut the sweet potatoes in cubes, and add to a pot with the remaining butter. Add the red beans mixture. 

12. Cut the bell pepper into cubes and add to the pot. 

13. Add the VEGAN minced meat, and cook in the oven at 180C for 10 minutes. 

14. Add the avocado. 

15. Add the chickpeas. 

16. Add the chocolate.

17. Serve on bread with mustard and pommegrenade on top.

&#x200B;

VIDEO OUTLINE:

0:00 - The Plan

2:15 - Ingredients

4:05 - What is GPT-3?

6:10 - Let's cook

12:25 - The Taste Test

&#x200B;

GPT-3 on Wikipedia: [https://en.wikipedia.org/wiki/GPT-3](https://en.wikipedia.org/wiki/GPT-3)

GPT-3 Paper: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)"
239,machinelearning,gpt-3,relevance,2021-11-27 21:11:46,[D] The Inherent Limitations of GPT-3,regalalgorithm,False,0.67,4,r3n2zl,https://www.reddit.com/r/MachineLearning/comments/r3n2zl/d_the_inherent_limitations_of_gpt3/,3,1638047506.0,"I wrote up a little editorial titled [the inherent limitations of GPT-3](https://lastweekin.ai/p/the-inherent-limitations-of-gpt-3). It is not negative towards GPT-3 (I hope), but rather lays out some of the basic facts on what its architectural constraints are - mainly so anyone worried it'll take their job or lead to AGI can find this and hopefully relax.

Would love feedback on it, especially any corrections!"
240,machinelearning,gpt-3,relevance,2021-08-11 20:15:22,[N] AI21 releases a 178B parameter model in par with GPT-3 for free usage!,datadudes-ai,False,0.5,0,p2l54l,https://www.reddit.com/r/MachineLearning/comments/p2l54l/n_ai21_releases_a_178b_parameter_model_in_par/,28,1628712922.0,"Absolutely awesome news from AI21 labs - a 178B parameter model that is not only in par or outperforms gpt3 but also has a non-waitlist freemium model!!!  \*\*edited (couldn't edit the title)

We can't wait to see the great things it will enable

[https://venturebeat.com/2021/08/11/ai21-labs-trains-a-massive-language-model-to-rival-openais-gpt-3/](https://venturebeat.com/2021/08/11/ai21-labs-trains-a-massive-language-model-to-rival-openais-gpt-3/)"
241,machinelearning,gpt-3,relevance,2021-11-06 17:54:18,[D] GPT-3 is No Longer the Only Game in Town,regalalgorithm,False,0.85,22,qo5in1,https://www.reddit.com/r/MachineLearning/comments/qo5in1/d_gpt3_is_no_longer_the_only_game_in_town/,9,1636221258.0,"Hey there, I just put out a little article that you might find interesting - [GPT-3 is No Longer the Only Game in Town](https://lastweekin.ai/p/gpt-3-is-no-longer-the-only-game). It catalogues the appearance of models akin to GPT-3 over the course of 2021, like  [HyperCLOVA](https://venturebeat.com/2021/06/01/naver-trained-a-gpt-3-like-korean-language-model/) and such. Hope you enjoy it!

*TLDR: Organizations face significant challenges in creating a model similar to OpenAI’s GPT-3, but nevertheless a half dozen or so models as big or bigger than GPT-3 have been announced over the course  of 2021.*"
242,machinelearning,gpt-3,relevance,2021-08-14 14:27:19,[Discussion] What's the most innovative use case for GPT-3 that you've seen?,Saaras13,False,0.83,34,p48zl2,https://www.reddit.com/r/MachineLearning/comments/p48zl2/discussion_whats_the_most_innovative_use_case_for/,17,1628951239.0,"I've seen a lot of AI copywriters floating around the Internet, surely there must be better uses for one of humanity's biggest ever machine learning models (175 billion parameters)??"
243,machinelearning,gpt-3,relevance,2020-07-19 20:08:21,[D] Where can I try GPT-3 or GPT-2?,magnus-erudio,False,0.91,23,hu755n,https://www.reddit.com/r/MachineLearning/comments/hu755n/d_where_can_i_try_gpt3_or_gpt2/,11,1595189301.0,"The site [https://talktotransformer.com/](https://talktotransformer.com/) used to be available for users to play around with, but it has been taken down.

Is there any other website like this?"
244,machinelearning,gpt-3,relevance,2020-09-10 14:28:29,[P] Chat with GPT-3 Grandmother: a free GPT-3-powered chatbot,Wiskkey,False,0.92,11,iq4ce9,https://www.reddit.com/r/MachineLearning/comments/iq4ce9/p_chat_with_gpt3_grandmother_a_free_gpt3powered/,9,1599748109.0,"[https://nainai.herokuapp.com/](https://nainai.herokuapp.com/)

Update: The site now has a waitlist.

This is a free GPT-3-powered chatbot with the intention of practicing Chinese, but one doesn't need to know Chinese to use it because translations to English are provided.

Notes:

1. The response from GPT-3 sometimes hangs. It seems the only remedy is to reload the page in your browser or open the site in another tab/window.
2. The exact same input can result in different outputs.
3. As expected for a chatbot, prior dialogue in the same conversation is taken into account when GPT-3 generates output.
4. Using input ""Respond in English only"" or similar can sometimes cause the output to be given in only English.
5. This site may be using GPT-3 settings that are more oriented towards creativity than factual accuracy.

Example:

https://preview.redd.it/0ldqu71ficm51.png?width=1080&format=png&auto=webp&s=97e6d4d9eb73dd6b55ff91b854588c6feab6226b

[This](https://www.reddit.com/r/artificial/comments/icvypl/list_of_free_sitesprograms_that_are_powered_by/) is a list of other free GPT-3-powered sites/programs that can be used now without a waiting list."
245,machinelearning,gpt-3,relevance,2022-02-13 00:51:08,[P] Building towards GPT-3 of computer vision,immune_star,False,0.61,6,sr5wf1,https://www.reddit.com/r/MachineLearning/comments/sr5wf1/p_building_towards_gpt3_of_computer_vision/,1,1644713468.0,"Hey all!

We're building towards a GPT3 level moment in computer vision, and here's our v0 - [https://youtu.be/P7zcc8iZ0YA](https://youtu.be/P7zcc8iZ0YA)

 This v0 runs on 13B parameters, with 18B and 34B model iterations coming in the pipeline.

Access to the model is gated as of now to help us monitor scale, you can sign up at - [https://banana-dev.typeform.com/carrot](https://banana-dev.typeform.com/carrot)"
246,machinelearning,gpt-3,relevance,2020-06-08 23:38:50,[D] What would it take to run OpenAI's GPT-3 on commodity hardware?,FirstTimeResearcher,False,0.94,31,gzb5uv,https://www.reddit.com/r/MachineLearning/comments/gzb5uv/d_what_would_it_take_to_run_openais_gpt3_on/,35,1591659530.0,"The NLP community has gotten a lot of mileage applying OpenAI's GPT-2 models to various applications:

* https://talktotransformer.com/
* https://twitter.com/drilgpt2archive 
* etc

Given the impressive zero-shot/few-shot abilities of GPT-3, what would it take to get it running on affordable hardware? What approximations can be made for GPT-3 inference to drastically lower the compute of the 175B parameter model?"
247,machinelearning,gpt-3,relevance,2020-05-29 03:19:09,[R] GPT-3,HybridRxN,False,0.77,18,gskz08,https://arxiv.org/pdf/2005.14165.pdf,5,1590722349.0,
248,machinelearning,gpt-3,relevance,2022-11-13 17:49:39,"[Research] Can we possibly get access to large language models (PaLM 540B, etc) like GPT-3 but no cost?",NLP2829,False,0.84,40,yu8nna,https://www.reddit.com/r/MachineLearning/comments/yu8nna/research_can_we_possibly_get_access_to_large/,11,1668361779.0,"(I only want to do inference, I don't need to finetune it.)

I want to use very-large language model (#parameters > 100B) to do some experiments, is that true the only very-large language model we can get access to is GPT3 API? Can we possibly get access to PaLM and Flan-PaLM 540B with no cost by chance?

I have searched over the internet but can't find a definite answer. As GPT-3 pricing for text-davinci-2 is not cheap, I am wondering if there's a chance to use other models.

Also, I can request up to 372GB VRAM, is there any large language model (#parameters > 100B) that I can actually download and run ""locally""?"
249,machinelearning,gpt-3,relevance,2021-12-15 03:49:39,"[D] Is this exaggerate?? ""to what extent is gpt-3 capable of reasoning"".",WeeklyTraining,False,0.52,1,rgpqix,https://www.reddit.com/r/MachineLearning/comments/rgpqix/d_is_this_exaggerate_to_what_extent_is_gpt3/,15,1639540179.0,"[To what extent is GPT-3 capable of reasoning?](https://www.lesswrong.com/posts/L5JSMZQvkBAx9MD5A/to-what-extent-is-gpt-3-capable-of-reasoning)

The reasoning ability of gpt3 shown in this article shocked me. But my own experiment using gpt-3 API is far less effective than the results in this article. this confuse me. is there any trick making gpt-3 answering like that?

My result is the same as the last comment: ""GPT-3 just produced one of the *dumbest* things I've ever read."""
250,machinelearning,gpt-3,relevance,2020-08-11 11:05:10,[D] Could GPT-3 and next iterations bootstrap singularity ?,PotatoeHacker,False,0.39,0,i7psu7,https://www.reddit.com/r/MachineLearning/comments/i7psu7/d_could_gpt3_and_next_iterations_bootstrap/,23,1597143910.0,"So,  while pretty impressive, GPT-3 is not near AGI. It's a step on the path  but we're not yet free from work, with flying cars, and DNA  manipulation allowing us to grow houses and cheeseburgers (well, that's  how I picture singularity anyway :p ).

But I'm wondering to what extent it can speed up research in machine learning.

GPT-3 can generate code. Could it (and next iterations) be in some ways creative at writing code that would improve itself ?

I'm  not talking about full blown outputted models better than itself out of  the box, more like assisting human researcher somehow gaining  insights thanks to generated code ?

I know it's pretty vague of a question, though I'm really interested about any thought you can have about it"
251,machinelearning,gpt-3,relevance,2020-07-28 22:46:21,"[D] The GPT-3 Architecture, on a Napkin",danieldugas,False,0.84,36,hzov3n,https://www.reddit.com/r/MachineLearning/comments/hzov3n/d_the_gpt3_architecture_on_a_napkin/,13,1595976381.0,"The post ""[The GPT-3 Architecture, on a Napkin](http://dugas.ch/artificial_curiosity/GPT_architecture.html)"" aims to give the reader a more precise understanding of the architecture of GPT-3, the natural language processing model presented in  [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) ([https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165))

**Excerpt:**

There are so many brilliant posts on GPT-3,  [demonstrating what it can do](https://www.gwern.net/GPT-3), [pondering its](https://leogao.dev/2020/05/29/GPT-3-A-Brief-Summary/) [consequences](https://maraoz.com/2020/07/18/openai-gpt3/), [vizualizing how it works](http://jalammar.github.io/illustrated-gpt2/). With all these out there, it still took a crawl through several  papers and blogs before I was confident that I had grasped the  architecture.   

So the goal for this page is humble, but simple: help others build an *as detailed as possible* understanding of the GPT-3 architecture.

**Discussion:**

Is this sort of visual / explanatory summary of an approach useful to the community?"
252,machinelearning,gpt-3,relevance,2021-01-17 05:00:23,[N] GPT-3 replicate as open source,FreddieM007,False,0.96,62,kyzv0u,https://www.reddit.com/r/MachineLearning/comments/kyzv0u/n_gpt3_replicate_as_open_source/,2,1610859623.0,"This is a grassroots group of researchers working to open source AI research  [https://www.eleuther.ai/](https://www.eleuther.ai/)

including [https://www.eleuther.ai/gpt-neo](https://www.eleuther.ai/gpt-neo)"
253,machinelearning,gpt-3,relevance,2021-12-19 22:24:08,[Discussion] Was GPT-3 trained to achieve double descent phenomena?,Competitive-Rub-1958,False,0.5,0,rk7ffw,https://www.reddit.com/r/MachineLearning/comments/rk7ffw/discussion_was_gpt3_trained_to_achieve_double/,5,1639952648.0,"Question in title. Also,

Are there any other Language models that have attempted to re-create this phenomena to aid in breaking SOTA benchmarks?"
254,machinelearning,gpt-3,relevance,2022-10-27 16:30:43,[P] Any open source LLMs with comparable ability to Gpt-3 Davinci-2 for natural language to JSON parsing? Bloom and GPT-NEO have been underwhelming.,laul_pogan,False,0.87,12,yewffc,https://www.reddit.com/r/MachineLearning/comments/yewffc/p_any_open_source_llms_with_comparable_ability_to/,10,1666888243.0,"See title, I’ve tried both Bloom and GptNEO but either I’m promoting wrong or they can’t compete with gpt3 for converting text into json. My specific use case is for parsing stories into relevant objects like characters, themes, settings, and events. 

&#x200B;

I’ll post an example of davinci’s parsing of Poe’s “a telltale heart” in the comments, it’s the ideal level of quality I’d like to replicate on a self hosted model."
255,machinelearning,gpt-3,relevance,2021-12-09 16:17:41,[D] Quick tips about building a chatbot with GPT-3 or GPT-J,juliensalinas,False,0.29,0,rcl0l0,https://www.reddit.com/r/MachineLearning/comments/rcl0l0/d_quick_tips_about_building_a_chatbot_with_gpt3/,2,1639066661.0,"Hello!

I  realize I have more and more questions from people trying to leverage  GPT-3 or GPT-J for their next chatbot. And usually questions are always  about 2 things:

* How to format my requests so the model understands that I am in conversational mode?
* How can the model keep an history of my conversation?

I'm answering these 2 points in this quick article: [https://nlpcloud.io/how-to-build-chatbot-gpt-3-gpt-j.html](https://nlpcloud.io/how-to-build-chatbot-gpt-3-gpt-j.html?utm_source=reddit&utm_campaign=k431103c-ed8e-11eb-ba80-4242ac130007)

I hope it will help!

I any question please don't hesitate to ask."
256,machinelearning,gpt-3,relevance,2022-06-28 16:21:53,[D] Have compression techniques every been applied to the likes of GPT-3 & DALLE-2?,Farconion,False,0.7,4,vmr5a5,https://www.reddit.com/r/MachineLearning/comments/vmr5a5/d_have_compression_techniques_every_been_applied/,8,1656433313.0,"Large language models and the recent spur of diffusion based  text-to-image models are gosh-darn fun to play with, but due to their  size and expensive training costs - they're only accessible via an API  or if you yourself have a access to a large # of GPUs. Yet there are  also a number of compression techniques like pruning and quantization  that can drastically reduce the size (+90%), and thus computational  requirements, of a trained model. Has there been any work looking  appling such techniques to these gigantic models floating around to make  them more accessible?"
257,machinelearning,gpt-3,relevance,2022-08-15 17:27:32,[P] How to Create a Blog Post Title Optimizer with GPT-3 and Hacker News Data,minimaxir,False,0.93,37,wp5h47,https://www.reddit.com/r/MachineLearning/comments/wp5h47/p_how_to_create_a_blog_post_title_optimizer_with/,2,1660584452.0,"I [wrote a blog post](https://minimaxir.com/2022/08/gpt3-blog-title-optimizer/) about finetuning GPT-3 on HN to determine whether a technical blog post is good or not, and also engineer prompts to GPT-3 to generate alternate titles which can then be ranked.

The code + demos is [available open-source on GitHub](https://github.com/minimaxir/gpt3-blog-title-optimizer), although the finetuned model isn't due to OpenAI rules/inability to share models.

(incidentally the post did well on Hacker News)"
258,machinelearning,gpt-3,relevance,2020-09-28 08:32:26,"[P] Using GPT-3 to ""imagine"" novel physical artworks that are produced IRL",shoeblade,False,0.8,36,j18bdl,https://www.reddit.com/r/MachineLearning/comments/j18bdl/p_using_gpt3_to_imagine_novel_physical_artworks/,18,1601281946.0,"**Using GPT-3 text generation to create new artworks as a human-machine collaboration... Forbes article** [**HERE**](https://www.forbes.com/sites/jessedamiani/2020/09/21/in-this-exhibition-an-ai-dreams-up-imaginary-artworks-that-artist-alexander-reben-then-creates-irl/) **more info and artworks** [**HERE**](https://areben.com/project/ai-am-i/)

“AI Am I? (The New Aesthetic)” is a series of artworks dreamed up by an AI and produced in real-life by the artist or others. A series of carefully curated “start texts” are fed into a text generation AI called GPT-3, which then outputs text that includes a description of an imaginary artwork, along with analysis of that artwork and other supporting text. Usually, a creative idea is conceived by a human and rendered with the help of technology. This series turns that notion on it’s head and represents a human-machine collaboration.

The idea here is not to have a computer ""replace"" a human, rather to work with an algorithm to help spark human inspiration.

**Process of creation:**

* Description text, titles and quotes are generated using the GPT-3 AI using carefully created “start text”
* Output is curated and chosen by the artist (sometimes,from hundreds of outputs)
* When an output is generated that the artist likes, it is fed back into the system to generate more text, this machine-human loop is repeated until the final output is chosen
* Fake names are generated from a neural-network trained on names.
* Birth location, birthdate and artwork year are made from a custom algorithm (unless GPT-3 included them in the description)
* Punctuation, spacing and other technical tweaks are made to the text (nothing that changes meaning)
* The artwork is produced in real life and/or displayed as a wall-tag

**Some examples of artworks along with GPT-3 generated text (more** [**HERE**](https://areben.com/project/ai-am-i)**):**

https://preview.redd.it/rk0ct2ux2up51.png?width=1398&format=png&auto=webp&s=bb0b45da85371c00ca2f1502c9b85ce48614794f

https://preview.redd.it/9jahhrg53up51.png?width=2100&format=png&auto=webp&s=7d6b730c97de0cf4ec9dd5bd6b5dee51e0bb68cf

https://preview.redd.it/y9pewstx2up51.jpg?width=1632&format=pjpg&auto=webp&s=f70fff94966a3f8b3b4dea3b43dbcd119f4fdd3a

https://preview.redd.it/htavmsg53up51.png?width=2100&format=png&auto=webp&s=cd68410e08b036bf05fb19e814176bdb6bb1d464

https://preview.redd.it/uv4ju2ux2up51.jpg?width=1358&format=pjpg&auto=webp&s=520c29bcef2a384f3cc70bff5a157570c248dcf9

https://preview.redd.it/kvnalwg53up51.png?width=2100&format=png&auto=webp&s=f33ce61a05812e9014ea11c118fe6b5e7fad5440

https://preview.redd.it/xnp35xtx2up51.jpg?width=1080&format=pjpg&auto=webp&s=d07fda9e063aba4b98f166eab992afe1d749b458

https://preview.redd.it/m4l932h53up51.png?width=2100&format=png&auto=webp&s=f8e1b80c1943e8eff3c6b8b5a1eb59ab3a0523fc

https://preview.redd.it/scc4l2ux2up51.jpg?width=1570&format=pjpg&auto=webp&s=604bcad77bd3e9e8a026e728e9b6c3868a523839

https://preview.redd.it/guxo31h53up51.png?width=2100&format=png&auto=webp&s=a3666959b01c89957b4eb540d546e17d1698f5f1

https://preview.redd.it/onj1uztx2up51.jpg?width=1080&format=pjpg&auto=webp&s=a1278617cbfcf8d5d7fdc67edd9cab3810c7fc53

https://preview.redd.it/6y8mnrg53up51.png?width=2100&format=png&auto=webp&s=da876f6fbf51030b236429bc32ecfeefa490b632"
259,machinelearning,gpt-3,relevance,2020-07-27 17:13:41,[N] Curated list of GPT-3 apps (25+ examples!),Ordinary_Historian_2,False,0.77,22,hywe40,https://www.reddit.com/r/MachineLearning/comments/hywe40/n_curated_list_of_gpt3_apps_25_examples/,12,1595870021.0,"Hey everyone,

I've curated a list of (update: now 30+) of the impressive applications and demos people have built using GPT-3. [Here's the link.](https://gpt3examples.com/)

Some examples are:

1. Text to SQL generator
2. Text to Plotly charts
3. Legal language to plain english
4. Text to Regex
5. Text to Latex
6. Text to Deep Learning code (Keras)
7. Figma UI creator"
260,machinelearning,gpt-3,relevance,2020-09-26 18:24:15,[D] AI Democratization in the Era of GPT-3 (The Gradient),regalalgorithm,False,0.84,16,j0b19n,https://www.reddit.com/r/MachineLearning/comments/j0b19n/d_ai_democratization_in_the_era_of_gpt3_the/,15,1601144655.0,"Hi there, the post [AI Democratization in the Era of GPT-3](https://thegradient.pub/ai-democratization-in-the-era-of-gpt-3/) just came out on The Gradient today, and seems like it would be of interest for discussion here. Mostly it has a summary of the concerns wrt what's been happening with OpenAI, and poses open ended questions of whether something can be done to not make it so future SOTA models keep being made exclusive like this.

Curious, how's progress of community efforts to replicate GPT-3? And do people here think that this sort of exclusive licensing of models might become more of a thing in the future?"
261,machinelearning,gpt-3,relevance,2021-11-09 21:18:06,[P] GPT-3 in the style of Shel Silverstein,molten_baklava,False,0.74,10,qqdkeo,https://www.reddit.com/r/MachineLearning/comments/qqdkeo/p_gpt3_in_the_style_of_shel_silverstein/,0,1636492686.0,"I've recently been playing with the OpenAI API beta for GPT-3, and used their fine-tuning API to create a model that has been fine-tuned on a corpus of all of Shel Silverstein's poetry. The resulting model generates whimsical poems based on a prompted title.

Here's one example I liked, ""Walking On a Whale""

> I am walking on a whale,

> I feel it move and swell,

> I feel the mist come in and float,

> I feel the rain and the cold.

> But I don't mind at all-

> It's just like walking on the ground.

While the model is quite good at understanding semantics, and even has a flair for the vaguely metaphorical, it didn't pick up any notion of rhyming. It seems like this would be a hard thing for the model to learn given the training data it's seen, since only very rarely does the rhyme scheme affect the conditional likelihood of a word.

Question for any commenters: if you had a rhyming dictionary where you could simply look up whether two words rhyme instead of trying to infer this as some latent attribute, how could you update a large NLP model like this to take into account such declarative knowledge?

More examples and discussion here: http://dean.dog/shel-silverstein-gpt3/"
262,machinelearning,gpt-3,relevance,2020-07-28 04:55:20,[D] GPT-3 for Non-English Text Generation,aseembits93,False,1.0,3,hz8ee1,https://www.reddit.com/r/MachineLearning/comments/hz8ee1/d_gpt3_for_nonenglish_text_generation/,10,1595912120.0,"So, recently, people have started sharing awesome demos of GPT-3 for applications such as text to latex, text to SQL etc. I am very confused right now about it.

1. The training data for GPT-3 would include English and other human languages like French, German etc. SQL, Latex strings wouldn't be encountered during training. 
2. The API provided by OpenAI is only for inference, so back-propagation cannot happen.

So, how can GPT-3 generate strings (Latex, SQL, JSX etc) without ever encountering them during training? Thanks in advance!"
263,machinelearning,gpt-3,relevance,2021-07-04 14:21:55,"[P] Analyzing and summarizing backpack reviews with AutoML, BERT, and GPT-3",Tom-Logan,False,0.82,19,odlesi,https://v.redd.it/ffod0q8ag7971,10,1625408515.0,
264,machinelearning,gpt-3,relevance,2020-07-19 15:47:03,[D] Did GPT-3 solved language understanding?,5dots,False,0.27,0,hu2kw8,https://www.reddit.com/r/MachineLearning/comments/hu2kw8/d_did_gpt3_solved_language_understanding/,6,1595173623.0,"I was blown away the kind of tasks that gpt-3 model has learnt without any supervision. Gpt-3 architecture, which has 175 billion parameters,  was fed with a lot of text data and lot of compute and allowed to run for days. 

Without any fine-tuning, model understands the user intent provided through prompt. Whether it's translation, QA, conversation or creating React web page, gpt3 got it.


Some argue that the model is too big, so it's not surprising that it gives very good results. I disagree because model size or parameters is relative. Human brain has close to 100 billion neurons. May be soon models with billions of parameters become common, just like when Deep learning started back in 2005 100k parameters is a big deal, but now neural net with 100k parameters is nothing.


What does this say about nature of human intelligence? Brute force approach is giving the best results never expected out of it. Authors of the model, did say that theoretically we might have reached limits of pretraining with large parameters or we will soon. Will a model with 1 Trillion or 10 Trillion result in Artificial General Intelligence? In my opinion gpt3 already exhibits primitive signs of AGI.

What do you think?"
265,machinelearning,gpt-3,relevance,2020-10-29 20:04:08,[D] How is GPT-3 handling Winograd schema?,kir4agi,False,0.69,6,jkhj8k,https://www.reddit.com/r/MachineLearning/comments/jkhj8k/d_how_is_gpt3_handling_winograd_schema/,5,1604001848.0,"Has anyone seen articles or any information about it?

I am really curious to see how far we've gone and what limitations we still have to overcome."
266,machinelearning,gpt-3,relevance,2023-10-31 12:35:35,"[P] A site where you can ask the same question to GPT-2, GPT-3, GPT-3.5 and GPT-4, and compare the outputs",timegentlemenplease_,False,0.86,61,17kk04m,https://www.reddit.com/r/MachineLearning/comments/17kk04m/p_a_site_where_you_can_ask_the_same_question_to/,18,1698755735.0,"Hi /r/machinelearning! I've been working with my collaborators on a site where you can compare OpenAI models to get a sense of the improvement over time of the models: [https://theaidigest.org/progress-and-dangers](https://theaidigest.org/progress-and-dangers)

https://preview.redd.it/khruhgkp7jxb1.png?width=1960&format=png&auto=webp&s=21d13125145f7fae7351686d4078868d65cbf8c3

It includes a number of things that you might be interested in:

* You can ask any question and compare the outputs from the OpenAI models:

https://preview.redd.it/s5e9acev8jxb1.png?width=1458&format=png&auto=webp&s=0c3e5ba3661fccfc4f4ba60db346b6142b1e52f3

* Visualises OpenAI models benchmark performance across 22 benchmarks:

https://preview.redd.it/vhai63308jxb1.png?width=1948&format=png&auto=webp&s=07f65f131b2e6d5122400120a11d24205b7d08d6

* Shows examples of benchmark outputs for GPT-2 to GPT-4

https://preview.redd.it/f3p7ni068jxb1.png?width=1980&format=png&auto=webp&s=dfe25c8c4a486a0df3c4cce2e4497fd250163bd1

* Discusses some dangerous emerging capabilities, such as biological weapons:

https://preview.redd.it/n6hinz7b8jxb1.png?width=2002&format=png&auto=webp&s=70cf0a0c228e1ac194146040c23a7f41dfe4e09a

* Includes an example of a simple agent autonomously exploiting a vulnerability in a game's code:

https://preview.redd.it/5a584w7f8jxb1.png?width=1944&format=png&auto=webp&s=3867a865c06b6e36fc2424f6ced038248ee0cafd

I hope you'll find this a valuable resource for getting familiar with older LMs, comparing the outputs, and thinking about what's next in this space. Here's a link to the site: [https://theaidigest.org/progress-and-dangers](https://theaidigest.org/progress-and-dangers)"
267,machinelearning,gpt-3,relevance,2022-03-28 16:23:08,[D] Paper Review Video - Memory-assisted prompt editing to improve GPT-3 after deployment,ykilcher,False,0.81,20,tqd22m,https://www.reddit.com/r/MachineLearning/comments/tqd22m/d_paper_review_video_memoryassisted_prompt/,3,1648484588.0,"[https://youtu.be/gYxJEd3EUKs](https://youtu.be/gYxJEd3EUKs)

Large language models such as GPT-3 have enabled many breakthroughs and new applications recently, but they come with an important downside: Training them is very expensive, and even fine-tuning is often difficult. This paper presents an adaptive method to improve performance of such models after deployment, without ever changing the model itself. This is done by maintaining a memory of interactions and then dynamically adapting new prompts by augmenting them with memory content. This has many applications, from non-intrusive fine-tuning to personalization.

&#x200B;

OUTLINE:

0:00 - Intro

0:40 - Sponsor: Introduction to GNNs Course (link in description)

1:30 - Paper Overview: Improve GPT-3 after deployment via user feedback

5:30 - Proposed memory-based architecture

13:00 - A detailed look at the components

15:00 - Example tasks

24:30 - My concerns with the example setup

26:20 - Baselines used for comparison

29:50 - Experimental Results

34:20 - Conclusion & Comments

&#x200B;

Paper: [https://arxiv.org/abs/2201.06009](https://arxiv.org/abs/2201.06009)

Code & Data: [https://github.com/madaan/memprompt](https://github.com/madaan/memprompt)"
268,machinelearning,gpt-3,relevance,2020-11-04 19:24:13,[D] NLP is not NLU and GPT-3 - Walid Saba,timscarfe,False,0.4,0,jo334r,https://www.reddit.com/r/MachineLearning/comments/jo334r/d_nlp_is_not_nlu_and_gpt3_walid_saba/,11,1604517853.0,"https://www.youtube.com/watch?v=b9qwLkJW2ig

Walid Saba thinks the missing information problem and lack of a typed ontology is the key issue with NLU, not sample efficiency or generalisation. He is a big critic of the deep learning movement and BERTology. We also cover GPT-3 in some detail in today's session, covering Luciano Floridi  recent article ""GPT‑3: Its Nature, Scope, Limits, and Consequences"" and a commentary on the incredible power of GPT-3 to perform tasks with just a few examples including the Yann LeCun commentary on Facebook and Hackernews. 

Note: Trigger warning to deep learning NLP folks!"
269,machinelearning,gpt-3,relevance,2020-08-05 17:21:59,"[D] Biggest roadblock in making ""GPT-4"", a ~20 trillion parameter transformer",AxeLond,False,0.97,347,i49jf8,https://www.reddit.com/r/MachineLearning/comments/i49jf8/d_biggest_roadblock_in_making_gpt4_a_20_trillion/,138,1596648119.0,"So I found this paper, [https://arxiv.org/abs/1910.02054](https://arxiv.org/abs/1910.02054) which pretty much describes how the GPT-3 over GPT-2 gain was achieved, 1.5B -> 175 billion parameters

# Memory

>Basic data parallelism (DP) does not reduce memory per device, and runs out of memory for models with more than 1.4B parameters on current generation of GPUs with 32 GB memory

The paper also talks about memory optimizations by clever partitioning of Optimizer State, Gradient between GPUs to reduce need for communication between nodes. Even without using Model Parallelism (MP), so still running 1 copy of the model on 1 GPU.

>ZeRO-100B can train models with up to 13B parameters without MP on 128 GPUs, achieving throughput over 40 TFlops per GPU on average. In comparison, without ZeRO, the largest trainable model with DP alone has 1.4B parameters with throughput less than 20 TFlops per GPU.

Add 16-way Model Parallelism in a DGX-2 cluster of Nvidia V100s and 128 nodes and you got capacity for around 200 billion parameters. From MP = 16 they could run a 15.4x bigger model without any real loss in performance, 30% less than peak performance when running 16-way model parallelism and 64-way data parallelism (1024 GPUs).

This was all from Gradient and Optimizer state Partitioning, they then start talking about parameter partitioning and say it should offer a linear reduction in memory proportional to number of GPUs used, so 64 GPUs could run a 64x bigger model, at a 50% communication bandwidth increase. But they don't actually do any implementation or testing of this.

# Compute

Instead they start complaining about a compute power gap, their calculation of this is pretty rudimentary. But if you redo it with the method cited by GPT-3 and using the empirically derived values by GPT-3 and the cited paper,   [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361) 

Loss (L) as a function of model parameters (N) should scale,

L = (N/8.8 \* 10\^13)\^-0.076

Provided compute (C) in petaFLOP/s-days is,

L = (C/2.3\*10\^8)\^-0.05  ⇔ L = 2.62 \* C\^-0.05

GPT-3 was able to fit this function as 2.57 \* C\^-0.048

So if you just solve C from that,

[C = 2.89407×10\^-14 N\^(19/12)](https://www.wolframalpha.com/input/?i=%28N%2F8.8*10%5E13%29%5E-0.076+%3D+2.57*C%5E-0.048+solve+C)

If you do that for the same increase in parameters as GPT-2 to GPT-3, then you get

C≈3.43×10\^7 for [20 trillion](https://www.wolframalpha.com/input/?i=C+%3D+2.89407%C3%9710%5E-14+N%5E%2819%2F12%29+and+N+%3D+175+billion+%2F+1.5+billion+*+175+billion) parameters, vs 18,300 for 175 billion. 10\^4.25 PetaFLOP/s-days looks around what they used for GPT-3, they say several thousands, not twenty thousand, but it was also slightly off the trend line in the graph and probably would have improved for training on more compute.

You should also need around 16 trillion tokens, GPT-3 trained on 300 billion tokens (function says 370 billion ideally). English Wikipedia was 3 billion. 570GB of webcrawl was 400 billion tokens, so 23TB of tokens seems relatively easy in comparison with compute.

With GPT-3 costing around [$4.6 million](https://lambdalabs.com/blog/demystifying-gpt-3/) in compute, than would put a price of [$8.6 billion](https://www.wolframalpha.com/input/?i=3.43%C3%9710%5E7%2F18%2C300+*+%244.6M+) for the compute to train ""GPT-4"".

If making bigger models was so easy with parameter partitioning from a memory point of view then this seems like the hardest challenge, but you do need to solve the memory issue to actually get it to load at all.

However, if you're lucky you can get 3-6x compute increase from Nvidia A100s over V100s,  [https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)

But even a 6x compute gain would still put the cost at $1.4 billion.

Nvidia only reported $1.15 billion in revenue from ""Data Center"" in 2020 Q1, so just to train ""GPT-4"" you would pretty much need the entire world's supply of graphic cards for 1 quarter (3 months), at least on that order of magnitude.

The Department of Energy is paying AMD $600 million to build the 2 Exaflop El Capitan supercomputer. That supercomputer could crank it out in [47 years](https://www.wolframalpha.com/input/?i=3.43%C3%9710%5E7+petaFLOPS*+days++%2F+%282+EXAFLOPS%29).

To vastly improve Google search, and everything else it could potentially do, $1.4 billion or even $10 billion doesn't really seem impossibly bad within the next 1-3 years though."
270,machinelearning,gpt-3,relevance,2023-01-07 20:11:54,[D] Is there a way to use a large dataset of quotes to create custom quote-generating model using GPT-3,Artemis_Nox,False,0.67,4,105ydxy,https://www.reddit.com/r/MachineLearning/comments/105ydxy/d_is_there_a_way_to_use_a_large_dataset_of_quotes/,1,1673122314.0,"What is the most simple and efficient way that you can feed a large dataset of quotes into a custom model that can then be used create new quotes based on that model's ""style"" using GPT-3?

Thanks so much for your expertise and help!"
271,machinelearning,gpt-3,relevance,2021-11-15 23:16:39,[P] How to get access to the GPT-3 API in late 2021,albdemens,False,0.75,2,quta9z,https://www.reddit.com/r/MachineLearning/comments/quta9z/p_how_to_get_access_to_the_gpt3_api_in_late_2021/,3,1637018199.0,I'm trying to get access to the GPT-3 API and I applied following the guidelines (form + email to the CTO 😇) yesterday. Should I wait a few more days or should I go creative? Is it still super hard to get access to the API? Thanks.
272,machinelearning,gpt-3,relevance,2020-09-02 23:27:38,"[N] GPT-3 pricing for API usage announced to beta users to go in effect on October 1, 2020",Wiskkey,False,0.9,36,ili4jr,https://www.reddit.com/r/MachineLearning/comments/ili4jr/n_gpt3_pricing_for_api_usage_announced_to_beta/,24,1599089258.0,"See [https://www.reddit.com/r/GPT3/comments/ikorgs/oa\_api\_preliminary\_beta\_pricing\_announced/](https://www.reddit.com/r/GPT3/comments/ikorgs/oa_api_preliminary_beta_pricing_announced/).

[This tweet](https://twitter.com/ref_sys/status/1301525170486611968) contains a purported screenshot of the pricing details.

**Update**: This [blog post](https://honest-ai.com/gpt-3/) contain additional pricing information."
273,machinelearning,gpt-3,relevance,2020-07-24 10:03:50,[P] Sentence to SQL Converter using GPT-3 Tutorial,bhavesh91,False,0.84,9,hwz4hx,https://www.reddit.com/r/MachineLearning/comments/hwz4hx/p_sentence_to_sql_converter_using_gpt3_tutorial/,4,1595585030.0,"I created a simple Sentence to SQL Converter using GPT - 3. If you want to learn how you can use OpenAI's GPT-3 to generate NLP Applications then this simple tutorial should help.Video Link :  [https://www.youtube.com/watch?v=9g66yO0Jues](https://www.youtube.com/watch?v=9g66yO0Jues) 

https://reddit.com/link/hwz4hx/video/xjobsqay3sc51/player"
274,machinelearning,gpt-3,relevance,2020-07-18 16:18:04,[D] gpt-3 continues to learn as you interact with him?,MarioBros68,False,0.25,0,htj8kj,https://www.reddit.com/r/MachineLearning/comments/htj8kj/d_gpt3_continues_to_learn_as_you_interact_with_him/,10,1595089084.0,"I've seen this gpt-3 answer about what lying is: https://twitter.com/sonyasupposedly/status/1284192647507177474

And I found it so impressive that I have wondered if the more people interact with it the more it learns or everything that it knows was defined in the training carried out before launching it?

Thanks"
275,machinelearning,gpt-3,relevance,2020-08-14 20:59:06,[D] GPT-3 Replication Effort - Help wanted with data labelling,leogao2,False,0.84,19,i9u6u3,https://www.reddit.com/r/MachineLearning/comments/i9u6u3/d_gpt3_replication_effort_help_wanted_with_data/,6,1597438746.0,"EleutherAI, a grassroots AI research group aimed at democratizing and open sourcing AI research, is looking for volunteers to help label data for a benchmark suite to compare various html-to-text extractors, as part of our GPT-3 replication effort. 

We're especially looking for help with low-resource languages, but any help is welcome.

For more info join our discord: [https://discord.com/invite/BK2v3EJ](https://discord.com/invite/BK2v3EJ)"
276,machinelearning,gpt-3,relevance,2022-08-01 16:55:48,"[P] Stories by AI, a newsletter with short stories written with GPT-3 and illustrated with DALL-E 2",regalalgorithm,False,0.57,1,wdmdfs,https://www.reddit.com/r/MachineLearning/comments/wdmdfs/p_stories_by_ai_a_newsletter_with_short_stories/,2,1659372948.0,"Hi r/ML! I and a couple of friends finally launched a project that's been kicking around since late last year: [Stories by AI](https://storiesby.ai/).

With the emergence of nice tools for co-writing fiction with GPT-3 (in particular, SudoWrite), I really liked the idea of publishing a bunch of short fiction where the AI largely did the writing. I still find the surreal fever-dream esque weirdness of language models really entertaining, and hope we can capture that in story form. And now these weird stories can be illustrated with DALL-E 2, which adds another layer to the fun.

It took a while, but today we are launching our substack newsletter and podcast! The podcast has audio versions of the stories made with Text to Speech, of course. The spark of the idea was actually inspired by a post on Hacker News (""I had some time yesterday so I made a GPT3 podcast to help you sleep"" [https://news.ycombinator.com/item?id=29428910](https://news.ycombinator.com/item?id=29428910)).

That's about it, would love to hear your feedback / thoughts about this."
277,machinelearning,gpt-3,relevance,2021-02-22 18:07:08,[R] Calibrate Before Use: Improving Few-Shot Performance of GPT-3,Lanky_Ad2150,False,1.0,15,lpvb1z,https://www.reddit.com/r/MachineLearning/comments/lpvb1z/r_calibrate_before_use_improving_fewshot/,2,1614017228.0,"New paper from BAIR on GPT-3.

Paper: https://arxiv.org/abs/2102.09690

Abstract: GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as ""N/A"". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt."
278,machinelearning,gpt-3,relevance,2020-11-28 00:14:50,"[D] GPT-3 ""the final word"" video (With Gary Marcus, Walid Saba and Connor Leahy)",timscarfe,False,0.67,5,k2cyk5,https://www.reddit.com/r/MachineLearning/comments/k2cyk5/d_gpt3_the_final_word_video_with_gary_marcus/,13,1606522490.0,"Hey folks,

[https://www.youtube.com/watch?v=iccd86vOz3w](https://www.youtube.com/watch?v=iccd86vOz3w)

We just uploaded a mega special edition ML Street Talk video on GPT-3. We have been playing with it a lot and actually show what it's like to drive GPT-3, interviewed some folks from all sides of the spectrum. Hope you enjoy the video. Would be interested to know whether you folks think it's a step towards AGI. 

Cheers,

Tim"
279,machinelearning,gpt-3,relevance,2020-12-17 23:42:13,OpenAI's GPT-3 generated articles given a Title [P],nocodemakers,False,0.5,0,kf98sk,https://www.reddit.com/r/MachineLearning/comments/kf98sk/openais_gpt3_generated_articles_given_a_title_p/,2,1608248533.0,"Below is a video of an article generated by GPT-3 on the price of bitcoin hitting 20k  
[VirtualGhostWriter.com/articles](https://VirtualGhostWriter.com/articles)

&#x200B;

https://reddit.com/link/kf98sk/video/61lq0cmm2u561/player"
280,machinelearning,gpt-3,relevance,2021-04-16 15:45:34,[D] Can some other organization/company replicate GPT-3 for their own use?,Competitive-Rub-1958,False,0.5,0,ms5x0r,https://www.reddit.com/r/MachineLearning/comments/ms5x0r/d_can_some_other_organizationcompany_replicate/,4,1618587934.0,"GPT-3 in itself does not create much of a new 'innovation' per se, being an overfitted model on a huge amount of data and a large number of parameters, probably based on it's predecessors architecture more or less.

So, is it easy for companies like Google (FAANG companies) to replicate GPT-3 sized NLP models for their own use?

But the Million dollar question - if it is indeed easy for them to do so, how much advantage does OpenAi have against them?

what prevents google from launching a new Model bigger and better, and offering it to consumers cheaper than OpenAi+MS can?"
281,machinelearning,gpt-3,relevance,2020-06-23 15:44:59,[P] Many test prompts for OpenAI’s GPT-3 API and text generation results,minimaxir,False,0.77,16,hegyra,https://www.reddit.com/r/MachineLearning/comments/hegyra/p_many_test_prompts_for_openais_gpt3_api_and_text/,13,1592927099.0,"[https://github.com/minimaxir/gpt-3-experiments](https://github.com/minimaxir/gpt-3-experiments)

Following up on [my thread last week asking for prompt examples](https://www.reddit.com/r/MachineLearning/comments/hc4i4l/d_i_will_be_running_experiments_on_openais_gpt3/), here are the results, plus a script to help you gather the generated prompts programmatically as well (if you have access to the API).

Almost all prompts turned out to be comedic/shitpost gold. *Especially* the `revengeofthesith` example."
282,machinelearning,gpt-3,relevance,2020-07-19 22:16:13,[D] Request for contributors for a GPT-3 overview article,regalalgorithm,False,0.84,4,hu9dda,https://www.reddit.com/r/MachineLearning/comments/hu9dda/d_request_for_contributors_for_a_gpt3_overview/,2,1595196973.0,"I co-run the site [Skynet Today](https://www.skynettoday.com/about), which has the aim to produce technically informed and accessible articles about what's going on with AI for non-experts - [for example this one](https://www.skynettoday.com/briefs/openai-rubiks-cube). I and others on the team run it in our spare time as grad students focused on AI, as an effort to curb misdirected hype or worrying and generally give people the right idea wrt what's going on with AI.

With all the hype/discussion around GPT-3, and some people even posting about how they are worried about their job being gone within a decade, it seems like this is a good topic to write such an article about it. Our core team has less expertise in NLP / topics related to GPT, so just posting here to ask if anyone knowledgeable about this would like to collaborate with us on writing such a thing, or just let us know their take on here. It would not be a pro or against piece, but just an overview making clear the capabilities of GPT-3 as well as its limitations, which I suspect people unfamiliar with the technical aspects will not know from seeing the demos. Similar to [this](https://www.skynettoday.com/briefs/openai-rubiks-cube).

If you are interested, please get in touch via the form or email here: [https://www.skynettoday.com/contribute](https://www.skynettoday.com/contribute)

Or just comment below with points you think we should include!"
283,machinelearning,gpt-3,relevance,2020-06-19 17:07:15,[D] I will be running experiments on OpenAI's GPT-3 API and open-sourcing the results. Do you have any example prompts to test?,minimaxir,False,0.88,18,hc4i4l,https://www.reddit.com/r/MachineLearning/comments/hc4i4l/d_i_will_be_running_experiments_on_openais_gpt3/,37,1592586435.0,"I recently obtained access to the API, and I plan on writing a few automation scripts and running a few generation experiments and putting the output on GitHub (I've already received permission to do so).

Specifically, I need more long-form examples to demonstrate few-shot learning (as demonstrated in the demo: [https://beta.openai.com/](https://beta.openai.com/)) since I believe the older approach of giving it a short prompt (a la GPT-2) will be less effective, but that's something to test too!

Let me know if there are any prompts worth testing; of course I will give credit and link back to the comment!"
284,machinelearning,gpt-3,relevance,2023-09-21 15:01:28,[N] OpenAI's new language model gpt-3.5-turbo-instruct can defeat chess engine Fairy-Stockfish 14 at level 5,Wiskkey,False,0.92,114,16oi6fb,https://www.reddit.com/r/MachineLearning/comments/16oi6fb/n_openais_new_language_model_gpt35turboinstruct/,178,1695308488.0,"[This Twitter thread](https://twitter.com/GrantSlatton/status/1703913578036904431) ([Nitter alternative](https://nitter.net/GrantSlatton/status/1703913578036904431) for those who aren't logged into Twitter and want to see the full thread) claims that [OpenAI's new language model gpt-3.5-turbo-instruct](https://analyticsindiamag.com/openai-releases-gpt-3-5-turbo-instruct/) can ""readily"" beat Lichess Stockfish level 4 ([Lichess Stockfish level and its rating](https://lichess.org/@/MagoGG/blog/stockfish-level-and-its-rating/CvL5k0jL)) and has a chess rating of ""around 1800 Elo."" [This tweet](https://twitter.com/nabeelqu/status/1703961405999759638) shows the style of prompts that are being used to get these results with the new language model.

I used website parrotchess\[dot\]com (discovered [here](https://twitter.com/OwariDa/status/1704179448013070560)) to play multiple games of chess purportedly pitting this new language model vs. various levels at website Lichess, which supposedly uses Fairy-Stockfish 14 according to the Lichess user interface. My current results for all completed games: The language model is 5-0 vs. Fairy-Stockfish 14 level 5 ([game 1](https://lichess.org/eGSWJtNq), [game 2](https://lichess.org/pN7K9bdS), [game 3](https://lichess.org/aK4jQvdo), [game 4](https://lichess.org/S9SGg8YI), [game 5](https://lichess.org/OqzdkDhE)), and 2-5 vs. Fairy-Stockfish 14 level 6 ([game 1](https://lichess.org/zP68C6H4), [game 2](https://lichess.org/4XKUIDh1), [game 3](https://lichess.org/1zTasRRp), [game 4](https://lichess.org/lH1EMqJQ), [game 5](https://lichess.org/mdFlTbMn), [game 6](https://lichess.org/HqmELNhw), [game 7](https://lichess.org/inWVs05Q)). Not included in the tally are games that I had to abort because the parrotchess user interface stalled (5 instances), because I accidentally copied a move incorrectly in the parrotchess user interface (numerous instances), or because the parrotchess user interface doesn't allow the promotion of a pawn to anything other than queen (1 instance). **Update: There could have been up to 5 additional losses - the number of times the parrotchess user interface stalled - that would have been recorded in this tally if** [this language model resignation bug](https://twitter.com/OwariDa/status/1705894692603269503) **hadn't been present. Also, the quality of play of some online chess bots can perhaps vary depending on the speed of the user's hardware.**

The following is a screenshot from parrotchess showing the end state of the first game vs. Fairy-Stockfish 14 level 5:

https://preview.redd.it/4ahi32xgjmpb1.jpg?width=432&format=pjpg&auto=webp&s=7fbb68371ca4257bed15ab2828fab58047f194a4

The game results in this paragraph are from using parrotchess after the forementioned resignation bug was fixed. The language model is 0-1 vs. Fairy-Stockfish level 7 ([game 1](https://lichess.org/Se3t7syX)), and 0-1 vs. Fairy-Stockfish 14 level 8 ([game 1](https://lichess.org/j3W2OwrP)).

There is [one known scenario](https://twitter.com/OwariDa/status/1706823943305167077) ([Nitter alternative](https://nitter.net/OwariDa/status/1706823943305167077)) in which the new language model purportedly generated an illegal move using language model sampling temperature of 0. Previous purported illegal moves that the parrotchess developer examined [turned out](https://twitter.com/OwariDa/status/1706765203130515642) ([Nitter alternative](https://nitter.net/OwariDa/status/1706765203130515642)) to be due to parrotchess bugs.

There are several other ways to play chess against the new language model if you have access to the OpenAI API. The first way is to use the OpenAI Playground as shown in [this video](https://www.youtube.com/watch?v=CReHXhmMprg). The second way is chess web app gptchess\[dot\]vercel\[dot\]app (discovered in [this Twitter thread](https://twitter.com/willdepue/status/1703974001717154191) / [Nitter thread](https://nitter.net/willdepue/status/1703974001717154191)). Third, another person modified that chess web app to additionally allow various levels of the Stockfish chess engine to autoplay, resulting in chess web app chessgpt-stockfish\[dot\]vercel\[dot\]app (discovered in [this tweet](https://twitter.com/paul_cal/status/1704466755110793455)).

Results from other people:

a) Results from hundreds of games in blog post [Debunking the Chessboard: Confronting GPTs Against Chess Engines to Estimate Elo Ratings and Assess Legal Move Abilities](https://blog.mathieuacher.com/GPTsChessEloRatingLegalMoves/).

b) Results from 150 games: [GPT-3.5-instruct beats GPT-4 at chess and is a \~1800 ELO chess player. Results of 150 games of GPT-3.5 vs stockfish and 30 of GPT-3.5 vs GPT-4](https://www.reddit.com/r/MachineLearning/comments/16q81fh/d_gpt35instruct_beats_gpt4_at_chess_and_is_a_1800/). [Post #2](https://www.reddit.com/r/chess/comments/16q8a3b/new_openai_model_gpt35instruct_is_a_1800_elo/). The developer later noted that due to bugs the legal move rate [was](https://twitter.com/a_karvonen/status/1706057268305809632) actually above 99.9%. It should also be noted that these results [didn't use](https://www.reddit.com/r/chess/comments/16q8a3b/comment/k1wgg0j/) a language model sampling temperature of 0, which I believe could have induced illegal moves.

c) Chess bot [gpt35-turbo-instruct](https://lichess.org/@/gpt35-turbo-instruct/all) at website Lichess.

d) Chess bot [konaz](https://lichess.org/@/konaz/all) at website Lichess.

From blog post [Playing chess with large language models](https://nicholas.carlini.com/writing/2023/chess-llm.html):

>Computers have been better than humans at chess for at least the last 25 years. And for the past five years, deep learning models have been better than the best humans. But until this week, in order to be good at chess, a machine learning model had to be explicitly designed to play games: it had to be told explicitly that there was an 8x8 board, that there were different pieces, how each of them moved, and what the goal of the game was. Then it had to be trained with reinforcement learning agaist itself. And then it would win.  
>  
>This all changed on Monday, when OpenAI released GPT-3.5-turbo-instruct, an instruction-tuned language model that was designed to just write English text, but that people on the internet quickly discovered can play chess at, roughly, the level of skilled human players.

Post [Chess as a case study in hidden capabilities in ChatGPT](https://www.lesswrong.com/posts/F6vH6fr8ngo7csDdf/chess-as-a-case-study-in-hidden-capabilities-in-chatgpt) from last month covers a different prompting style used for the older chat-based GPT 3.5 Turbo language model. If I recall correctly from my tests with ChatGPT-3.5, using that prompt style with the older language model can defeat Stockfish level 2 at Lichess, but I haven't been successful in using it to beat Stockfish level 3. In my tests, both the quality of play and frequency of illegal attempted moves seems to be better with the new prompt style with the new language model compared to the older prompt style with the older language model.

Related article: [Large Language Model: world models or surface statistics?](https://thegradient.pub/othello/)

P.S. Since some people claim that language model gpt-3.5-turbo-instruct is always playing moves memorized from the training dataset, I searched for data on the uniqueness of chess positions. From [this video](https://youtu.be/DpXy041BIlA?t=2225), we see that for a certain game dataset there were 763,331,945 chess positions encountered in an unknown number of games without removing duplicate chess positions, 597,725,848 different chess positions reached, and 582,337,984 different chess positions that were reached only once. Therefore, for that game dataset the probability that a chess position in a game was reached only once is 582337984 / 763331945 = 76.3%. For the larger dataset [cited](https://youtu.be/DpXy041BIlA?t=2187) in that video, there are approximately (506,000,000 - 200,000) games in the dataset (per [this paper](http://tom7.org/chess/survival.pdf)), and 21,553,382,902 different game positions encountered. Each game in the larger dataset added a mean of approximately 21,553,382,902 / (506,000,000 - 200,000) = 42.6 different chess positions to the dataset. For [this different dataset](https://lichess.org/blog/Vs0xMTAAAD4We4Ey/opening-explorer) of \~12 million games, \~390 million different chess positions were encountered. Each game in this different dataset added a mean of approximately (390 million / 12 million) = 32.5 different chess positions to the dataset. From the aforementioned numbers, we can conclude that a strategy of playing only moves memorized from a game dataset would fare poorly because there are not rarely new chess games that have chess positions that are not present in the game dataset."
285,machinelearning,gpt-3,relevance,2020-12-08 20:46:34,OpenAI's GPT-3 used as a copywriting aid: Virtual Ghost Writer [P],nocodemakers,False,0.55,1,k9cngs,https://www.reddit.com/r/MachineLearning/comments/k9cngs/openais_gpt3_used_as_a_copywriting_aid_virtual/,4,1607460394.0,"[VirtualGhostWriter.com](https://virtualghostwriter.com/), an AI copy writer powered by OpenAI.

&#x200B;

https://i.redd.it/nixh20hhz0461.gif"
286,machinelearning,gpt-3,relevance,2020-08-28 21:11:14,[D] Can GPT-3 be used for Machine Translation of Low Corpus Languages?,buzzer4mornin,False,0.74,11,iifmzc,https://www.reddit.com/r/MachineLearning/comments/iifmzc/d_can_gpt3_be_used_for_machine_translation_of_low/,4,1598649074.0,"As a newbie to NLP and Machine Translation, I have arguable dumb question:

Can GPT-3 be used for Machine Translation from English to Low Corpus Languages which have low amount of recourses across the internet? I read on some papers that, 93% of training data was in English, but considering its huge amount of training datasets, is there any hope that there is still some solid proportion of other low corpus languages such as Azerbaijani?

I am wondering that if GPT-3 has done greatest Common Crawling (again, arguable) ever, why not use it for translation to-from Low Corpus Languages?"
287,machinelearning,gpt-3,relevance,2020-06-19 01:13:20,[R] Using GPT-3 to generate Harry Potter in the style of various famous authors (from Gwern),programmerChilli,False,0.9,57,hbr6kp,https://www.reddit.com/r/MachineLearning/comments/hbr6kp/r_using_gpt3_to_generate_harry_potter_in_the/,8,1592529200.0,"https://pastebin.com/iyNKdrCz

Taken from u/gwern's tweet here: https://twitter.com/gwern/status/1273783738493808642

To extract some I liked in particular,

Ernest Hemingway:
> ""It was a cold day on Privet Drive. A child cried. Harry felt nothing.| He was dryer than dust. He had been silent too long. He had not felt love. He had scarcely felt hate. Yet the Dementor's Kiss killed nothing. Death didn't leave him less dead than he had been a second before. It wasn't about living or dying really. It wasn't about death. It was about the Dark Lord, his creator, his engenderer. He was not resurected. He was created again. He was whole again in the courtyard of Malfoy Manor.""

Jane Austen:
> ""It is a truth universally acknowledged, that a broken Harry is in want of a book. This he knows to his cost; pressing that wretched nose against the window of a bookstore in Diagon Alley the last thing before he goes into hiding for several years, where upon, he goes straight to Azkaban.""

Bram Stoker:
> ""Tis a fearful sight to see the cold, girlish venom run through his veins, one by one. The scar coruscates; a ghastly emerald lightning in the dark of night. But I'll attend him and I watch him
 From the very moment the dawn is gray
 To the rays of sunset fading away."""
288,machinelearning,gpt-3,relevance,2020-08-26 15:06:17,[D] Tutorial: Directives: How to get GPT-3 to fill in blanks in text with text that fits a given description,Wiskkey,False,0.92,11,ih0i0p,https://www.reddit.com/r/MachineLearning/comments/ih0i0p/d_tutorial_directives_how_to_get_gpt3_to_fill_in/,17,1598454377.0,"Yesterday I wrote post [Using GPT-3 to fill in the blanks in text: a potentially broadly useful GPT-3 interaction method](https://www.reddit.com/r/MachineLearning/comments/igj71v/d_using_gpt3_to_fill_in_the_blanks_in_text_a/) that explains how to get GPT-3 to replace blanks with text. This post extends the concept to blanks that include a description of the text that GPT-3 should replace the given blank with.

Examples:

Query:

>Replace text enclosed in brackets that begins with ""write:"" with text that fits the description after ""write:"" Do not alter any other text. input: A man went to \[write:a USA state\]. output: A man went to California. input: A man \[write:a verb that begins with the letter ""c""\]. output: A man cried. input: \[write:a woman's name\] went with \[write:a man's name\] to \[write:a location\]. output: Alice went with Bob to the beach. input: I \[write:a verb that begins with the letter ""c""\] \[write:a vegetable that begins with the letter ""b""\]. output:

Output:

>I cooked broccoli.

Query:

>Replace text enclosed in brackets that begins with ""write:"" with text that fits the description after ""write:"" Do not alter any other text. input: A man went to \[write:a USA state\]. output: A man went to California. input: A man \[write:a verb that begins with the letter ""c""\]. output: A man cried. input: \[write:a woman's name\] went with \[write:a man's name\] to \[write:a location\]. output: Alice went with Bob to the beach. input: The best state in the USA is \[write:a USA state\] because \[write:a reason with a vegetable\]. output:

Output:

>The best state in the USA is California because it has the most fresh produce."
289,machinelearning,gpt-3,relevance,2021-06-09 02:07:38,"[P] GPT-J, 6B JAX-based Transformer LM",Aran_Komatsuzaki,False,0.98,252,nvkowg,https://www.reddit.com/r/MachineLearning/comments/nvkowg/p_gptj_6b_jaxbased_transformer_lm/,52,1623204458.0,"Ben and I have released GPT-J, 6B JAX-based Transformer LM!

\- Performs on par with 6.7B GPT-3

\- Performs better and decodes faster than GPT-Neo

\- repo + colab + free web demo

\- Trained on 400B tokens with TPU v3-256 for five weeks

\- GPT-J performs much closer to GPT-3 of similar size than GPT-Neo

https://preview.redd.it/e1yqex9it4471.png?width=908&format=png&auto=webp&s=a6411d57530d5f34e8524fd50fa3f1640421181a

tweet: [https://bit.ly/3isa84D](https://bit.ly/3isa84D)

article: [https://bit.ly/2TH8yl0](https://bit.ly/2TH8yl0)

repo: [https://bit.ly/3eszQ6C](https://bit.ly/3eszQ6C)

Colab: [https://bit.ly/3w0fB6n](https://bit.ly/3w0fB6n)

demo: [https://bit.ly/3psRCdM](https://bit.ly/3psRCdM)"
290,machinelearning,gpt-3,relevance,2022-12-11 08:13:55,"[P] All About Prompt-Engineering: Open source discussion forum to ask questions, discuss, and share about ChatGPT, Stable Diffusion, GPT-3 and other generative models. Prompt Engineering for different tasks such as NER, QA, Classification, Data Generation and many more",Intelligent_Tip8033,False,0.6,2,zikgdt,https://www.reddit.com/r/MachineLearning/comments/zikgdt/p_all_about_promptengineering_open_source/,0,1670746435.0,"Hi Folks,

Have you tried **ChatGPT, GPT-3, or other generative models** but have been frustrated by the lack of support or guidance when it comes to using them effectively? Are you interested in learning more about the power of prompt engineering and how it can help you get better results from generative models?

We have recently launched a new open-source platform called **discuss.openPrompt.io**, where you can ask and answer questions, discuss, and share your knowledge and experiences with **ChatGPT, Prompt-Engineering, GPT-3, stable diffusion, and other generative models**.

&#x200B;

As many of you may know, ChatGPT was released recently and has generated a lot of excitement among the NLP community. When ChatGPT was released, we were excited to try it out, but we quickly realized that many people are still unsure of how to use it effectively and get the expected output. That's why we decided to create [discuss.openPrompt.io](https://discuss.openPrompt.io) \- to provide a space where experts and beginners alike can learn from each other, share their knowledge and experiences, and discuss the latest developments in the field of generative models.

&#x200B;

But OpenPrompt is not just a Q&A forum. It's also a platform for sharing resources and discussing the latest developments in the field of generative models and prompt engineering. You can share tutorials, code snippets, datasets, and other useful materials that can help others learn and experiment with these tools. You can also participate in discussions about the latest trends and innovations in the field.

Whether you're an experienced user of generative models or just starting out, OpenPrompt is the place for you. If you are interested in generative models and want to learn more, join our platform and participate in the discussions. We would love to hear your thoughts, ideas, and suggestions on ChatGPT, prompt engineering, GPT-3, stable diffusion, and other generative models.

And if you're attending **EMNLP2022 in Abu Dhabi**, we would be happy to chat with you in person and discuss the latest developments in the field of generative models and all the exciting things we have planned for the future."
291,machinelearning,gpt-3,relevance,2024-02-20 21:11:05,[D] Consensus outdated / misleading (ChatGPT 3.5 ?),vonnoor,False,0.13,0,1avszpa,https://www.reddit.com/r/MachineLearning/comments/1avszpa/d_consensus_outdated_misleading_chatgpt_35/,8,1708463465.0," I asked Consensus via ChatGPT Plus for the latest AI paper.  
The answer was a paper from 2021 !?

I had the impression that Consensus has an own trained data set  
also because it is highly recommended and featured in the GPT 'store' and claims to have 200M papers.  
But it just seams to use GPT 3.5 with the old data set before 2022."
292,machinelearning,gpt-3,relevance,2020-09-17 20:04:25,"[N] Last Week in AI Audio Digest: GPT-3 Clickbait, Wildfires, Heroes, Standards, Exports",regalalgorithm,False,1.0,2,iur8d9,https://www.reddit.com/r/MachineLearning/comments/iur8d9/n_last_week_in_ai_audio_digest_gpt3_clickbait/,0,1600373065.0,"Hi there, just sharing [the latest episode](https://youtu.be/V6RSD-_S638) of our [podcast](https://www.skynettoday.com/podcast) summarizing each week's AI news. We actually have no idea if this is well done or not, or at least we've not gotten any external feedback, so besides self promotion we really could use your feedback wrt if this could be improved or not. Here's a little taste to give you an idea of what it's like:

https://reddit.com/link/iur8d9/video/t22pqyyvkrn51/player"
293,machinelearning,gpt-3,relevance,2023-05-10 08:11:24,[D] When will a GPT-like model outperform Stockfish in chess?,ThePerson654321,False,0.51,1,13dk32o,https://www.reddit.com/r/MachineLearning/comments/13dk32o/d_when_will_a_gptlike_model_outperform_stockfish/,55,1683706284.0,"Hello!

GPT-4 has shown significant improvement over GPT-3 in terms of playing chess, with what I estimate to be a ELO rating of around 1000 now. While this is impressive, it is still nowhere near the performance level of Stockfish. This has led me to ponder the following question:

How many years do you think it will take for a GPT-like model (not specifically or even intentionally trained for chess) to surpass Stockfish in terms of chess-playing abilities?"
294,machinelearning,gpt-3,relevance,2020-10-14 21:14:55,[D] Does this experiment show that GPT-3 knows which letters are in BPE (Byte Pair Encoding) tokens that consist of more than one letter?,Wiskkey,False,0.9,8,jb9rlp,https://www.reddit.com/r/MachineLearning/comments/jb9rlp/d_does_this_experiment_show_that_gpt3_knows_which/,16,1602710095.0,"In comments for my post [GPT-3 can do word segmentation for English text with no spaces. Does this give any new insights into the inner workings of GPT-3?](https://www.reddit.com/r/MachineLearning/comments/j9a6lh/d_gpt3_can_do_word_segmentation_for_english_text/) some people suggested that the preprocessing step of doing BPE ([Byte Pair Encoding](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10)) tokenization of input accounted for the ability of GPT-3 to do word segmentation. I believe [this comment](https://www.reddit.com/r/MachineLearning/comments/j9a6lh/d_gpt3_can_do_word_segmentation_for_english_text/g8jdmcq/) refuted that hypothesis. I [showed](https://www.reddit.com/r/MachineLearning/comments/j9a6lh/d_gpt3_can_do_word_segmentation_for_english_text/g8lwb67/) a technique that might reveal how GPT-2/Small BPE tokenizes a given input. In response to my question about how GPT-3 can do word segmentation when a BPE token crosses word boundaries, a user [hypothesized](https://www.reddit.com/r/MachineLearning/comments/j9a6lh/d_gpt3_can_do_word_segmentation_for_english_text/g8mdwgv/) that GPT-3 ""seems to still (somehow) have some knowledge of what letters compose which BPEs.""

I figured out an experiment that might show that GPT-3 indeed does know which letters are in a given BPE token using GPT-3-powered [https://www.shortlyread.com](https://www.shortlyread.com).

Input:

>Task:Add a comma between each letter in the input. input:catch. output:c,a,t,c,h. input:therapy. output:t,h,e,r,a,p,y. input:verbose. output:v,e,r,b,o,s,e. input:thunder. output:t,h,u,n,d,e,r. input:question. output:q,u,e,s,t,i,o,n. input:maximize. output:

Output:

>m,a,x,i,m,i,z,e.

It doesn't always get the right answer though.

Input:

>Task:Add a comma between each letter in the input. input:catch. output:c,a,t,c,h. input:therapy. output:t,h,e,r,a,p,y. input:verbose. output:v,e,r,b,o,s,e. input:thunder. output:t,h,u,n,d,e,r. input:question. output:q,u,e,s,t,i,o,n. input:feybarrrazz. output:

Output (missing an ""r""):

>f,e,y,b,a,r,r,a,z,z.

Opinions?

Update: I found a GPT tokenizer at [https://gpttools.com/estimator](https://gpttools.com/estimator)."
295,machinelearning,gpt-3,relevance,2020-10-11 20:38:23,[D] A GPT-3 like Bot has been answering questions on Reddit and some people haven't noticed,cyborgsnowflake,False,0.22,0,j9d0w4,https://www.reddit.com/r/MachineLearning/comments/j9d0w4/d_a_gpt3_like_bot_has_been_answering_questions_on/,2,1602448703.0,"A lot of people have started to notice this user  [/u/thegentlemetre](https://www.reddit.com/u/thegentlemetre/)  appears to be a bot. Others seem to not notice and are engaging with  it. He/It is making a large number of frequent but very long and  elaborate posts. Good enough to fool some people it seems.

From

[https://www.kmeme.com/2020/10/gpt-3-bot-went-undetected-askreddit-for.html](https://www.kmeme.com/2020/10/gpt-3-bot-went-undetected-askreddit-for.html)

*""The* [*posts*](https://www.reddit.com/user/thegentlemetre/?sort=top) *were  appearing at a rate of about one per minute, and the posts were   lengthy, most around six paragraphs long..."" writes software engineer   Philip Winston.*

*I read through some of the posts. The* [*quality was incredibly good, no machine could have written these even a few years ago*](https://www.kmeme.com/2020/10/gpt-3-bot-went-undetected-askreddit-for.html)*.   However there were some flaws and tells that suggested they were   machine generated. The posts reminded me of text I'd seen from* [*OpenAI*](https://openai.com/)*'s language model* [*GPT-3*](https://en.wikipedia.org/wiki/GPT-3)*,   which is the newest and best language generator I had heard of...   Several times I Googled clever sounding lines from the posts, assuming   I'd find that they had been cribbed from the internet. Every time Google   reported ""zero results"". The sentences were entirely novel, the  machine  had dreamed them up...*

*During  the week, the bot answered questions on suicide, harassment,   conspiracy theories, immigration, racism, and other weighty topics.   Sometimes the human replies called out* [/u/thegentlemetre](https://www.reddit.com/u/thegentlemetre/) *for  being a  bot. I was not the first one to suspect this, I was just the  first one  to post to the GPT-3 subreddit about it. Other times,  however, the human  was clearly unaware it was talking to a bot...    What does it mean for  online discourse when bots can pass for humans?  How much bot traffic  could thousands of computers generate? How can we  ever hope to sort  through it? Humanity is about to run that experiment.*"
296,machinelearning,gpt-3,relevance,2020-08-04 20:41:38,"[D] As Its GPT-3 Model Wows the World, OpenAI CEO Suggests ‘the Hype Is Way Too Much’",Yuqing7,False,0.38,0,i3rdbj,https://www.reddit.com/r/MachineLearning/comments/i3rdbj/d_as_its_gpt3_model_wows_the_world_openai_ceo/,5,1596573698.0,"OpenAI’s **175 billion parameter** language model [GPT-3](https://syncedreview.com/2020/05/29/openai-unveils-175-billion-parameter-gpt-3-language-model/) has gone viral once again, with a flurry of tech tweets celebrating the many innovative new applications — ranging from automatic code and short story generators to fully functioning search engines — that have leveraged the GPT-3 API OpenAI released in June. But not everyone in the ML community is impressed.

Here is a quick read: [As Its GPT-3 Model Wows the World, OpenAI CEO Suggests ‘the Hype Is Way Too Much’](https://syncedreview.com/2020/08/04/as-its-gpt-3-model-wows-the-world-openai-ceo-suggests-the-hype-is-way-too-much/)"
297,machinelearning,gpt-3,relevance,2020-09-08 02:08:29,"[R] Measuring Massive Multitask Language Understanding; a new test consisting of 14,080 questions given to GPT-3 (4 model sizes), UnifiedQA, and T5",Wiskkey,False,0.94,48,iol3l7,https://www.reddit.com/r/MachineLearning/comments/iol3l7/r_measuring_massive_multitask_language/,9,1599530909.0,"[https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300)

Abstract:

>We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach human-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."
298,machinelearning,gpt-3,relevance,2021-12-09 23:06:17,[D] The Carbon Footprint of Machine Learning,kirya_V21,False,0.84,214,rcttt3,https://www.reddit.com/r/MachineLearning/comments/rcttt3/d_the_carbon_footprint_of_machine_learning/,107,1639091177.0,"**The energy costs of AI have risen 300,000-fold between 2012 and 2018 and the focus on large language models like GPT-3 will make this worse** Reducing the carbon footprint has become a critical need for the AI community - are huge models the best way forward?

Blog Link: [https://kv-emptypages.blogspot.com/2021/11/the-carbon-footprint-of-machine-learning.html](https://kv-emptypages.blogspot.com/2021/11/the-carbon-footprint-of-machine-learning.html)

&#x200B;

[The outlook for ML training costs - Source: Ark Investments LLC](https://preview.redd.it/v00jb73yll481.png?width=1660&format=png&auto=webp&s=c6880a49aa3453c0ae8f8a0d9b76b826299493aa)"
299,machinelearning,gpt-3,relevance,2023-10-16 21:35:31,[R] Microsoft presents Table-GPT: Table-tuned GPT for Diverse Table Tasks,Successful-Western27,False,0.87,17,179hgov,https://www.reddit.com/r/MachineLearning/comments/179hgov/r_microsoft_presents_tablegpt_tabletuned_gpt_for/,1,1697492131.0,"Tables pack tons of relational data but are tough for AI to grasp. They have complex 2D structure with information scattered across rows and columns. Models like GPT-3 fail basic tasks like finding where a missing value should go.

LLMs struggle at this because they're pre-trained mostly on natural text, which is linear. Researchers at Microsoft wanted to mitigate this with ""table-tuned"" models, trained on table-related tasks.

Their process:

1. Automatically generate lots of diverse table-task training cases from a corpus of real-world tables. Ex: ""impute missing value"" or ""identify error in table"".
2. Further augment data via paraphrasing, shuffling table rows/columns, chaining model responses, etc.

This table-tuning produced ""Table-GPT"" models with substantially stronger table skills. In experiments, Table-GPT crushed vanilla GPT-3:

* 25%+ better on unseen table tasks like missing value ID and column type ID
* Beat GPT-3 on 98% of test cases across 9 different table tasks
* Stayed superior after downstream tuning too

There's tons more work to do but seems pretty promising. Table-tuning boosted models' ability to comprehend tables and reason over tabular data vs just pre-training on text.

**TLDR: Training AI models more on synthesized table tasks (""table-tuning"") significantly improves their table skills.**

[Full summary is here](https://notes.aimodels.fyi/table-gpt-table-tuned-gpt-for-diverse-table-tasks/). Paper is [here](https://arxiv.org/pdf/2310.09263.pdf)."
