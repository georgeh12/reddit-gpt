,id,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,url,num_comments,created,body
0,13ilm03,datascience,GPT,top,2023-05-15 21:45:36,I investigated the Underground Economy of Glassdoor Reviews,ibsurvivors,False,0.99,1165,https://www.reddit.com/r/datascience/comments/13ilm03/i_investigated_the_underground_economy_of/,63,1684187136.0,"Online company reviews are high stakes.

Top reviews on sites like Glassdoor and Google can get thousands of impressions each month and are major drivers of brand perception.

Employers know this. And when I come across multiple 5 star reviews left with no cons, or a Pulitzer worthy essay from a former intern, I become suspicious.

These reviews start to resemble 30 under 30 lists: so artificially constructed that you begin to question their credibility in the first place.

The scrutiny around company reviews is well documented; some companies file lawsuits worth over a million dollars to reveal anonymous reviewers that complain about their jobs.

Whilst it's the flashy lawsuits that make the headlines, there also exists an underground economy of company reviews operating quietly every single day.

In this underground economy, some companies pay over $150 to freelancers to try and get a negative review removed. If they want ‚Äúbetter‚Äù results, they go to the plethora of Online Reputation Management services (ORMs) in the United States that can charge retainers worth thousands of dollars.

The supply of positive reviews exists too. My research led me to find companies, including a prominent Y-Combinator backed startup, that solicit fake positive reviews from online freelancers to improve their rating.

Many of these mercenary fake reviewers, often based in South East Asia, make a full time living doing this, netting over $2,000 per month.

Some of these run such sophisticated operations that they‚Äôve even created their own pricing tiers (e.g $35 per original review, $20 to post an already created review from an email address), a la SaaS offering.

Others operate on a contingency fee agreement model, where they only get paid if they‚Äôre able to take a negative review down.

The underground economy of company reviews is well and truly alive. And today we‚Äôre going to find out how it operates.

***Note***: For more content like this, [*subscribe*](https://www.careerfair.io/subscribe) *to my newsletter. In a couple of weeks, I'll be releasing my guide to writing a killer resume.*

**Adding reviews**

The barriers to entry for adding fake reviews are much lower than for getting reviews removed, so that‚Äôs where we‚Äôll start.

To write an employer review, all you really need is the ability to create an email address. For most sites, you don‚Äôt need any proof of employment (say like a company specific email address).

I went on a gig marketplace site and posted a pretty vague post related to wanting to find out more on how to improve a company‚Äôs online presence.

Within minutes of posting a gig, my inbox was flooded with proposals:

https://preview.redd.it/esx3904qa20b1.png?width=3064&format=png&auto=webp&s=2ff3a2f8528fee99aabb830f27ea71a7569ebb2e

After a bit of chatting, I narrowed the scope of their services and summarized their rates into the table below:

|Channel|Cost|Timeline|Model|
|:-|:-|:-|:-|
|Freelancer #1|$10 per review|Monthly|Unlimited|
|Freelancer #2|$35 per original review, $20 per already created review|Monthly|Unlimited|
|Freelancer #3|$25 per review|Monthly|Unlimited|
|Freelancer #4|$25 per review|Monthly|10 reviews|
|Freelancer #5|$20 per review|Monthly|Unlimited|
|Online Reputation Management Agency|$300 subscription|Monthly|8 reviews|

Let‚Äôs dive a bit deeper into the services that Freelancer #5 offered.

Freelancer #5 explained to me he had been writing reviews for one particular company for the past 4 months now. Each month he wrote them 10 reviews.

&#x200B;

https://preview.redd.it/n1ddox6cb20b1.png?width=2684&format=png&auto=webp&s=5c271d0eec4328cb78d7d2cb85dfffa3f9eb72f8

In another message, he tells me he‚Äôs offering the same services to 5 other companies. Doing some quick math:

5 companies x 10 reviews per company x $25 per review = $1,250 per month

Considering the average person in Pakistan earns $150 per month, that‚Äôs not bad change at all.

One of the companies that he‚Äôs offering his services to includes a Y-Combinator backed startup. I won‚Äôt name the company, but here‚Äôs what its average Glassdoor review rating distribution looks like:

https://preview.redd.it/2np5b6fdb20b1.png?width=2420&format=png&auto=webp&s=f8cafaa85453b0933a18eb5c30f931b3bb893c46

5 star reviews account for over 77% of the company‚Äôs total reviews. Obviously, no one is buying fake reviews that make them look bad.

But here‚Äôs the thing: freelancers are getting quite smart when it comes to writing reviews that don‚Äôt look too fishy. They tend to do this by spacing the reviews out (so that they don‚Äôt come in ‚Äúspikes‚Äù ‚Äì more on this later) and they also make sure that they‚Äôre not always leaving the ‚Äúcons‚Äù section blank.

Don‚Äôt get me wrong, if you come across this company‚Äôs reviews, it‚Äôd be pretty easy to tell they‚Äôre quite strange. In fact, I can‚Äôt even post some screenshots here because it‚Äôd give the company away immediately.

But it would be challenging to conclude that the above company is buying reviews just by analyzing review volume and distribution without actually reading some of the reviews.

The same company is also buying reviews on Google Reviews.

Sidenote: I got curious about how he‚Äôs been writing 50 reviews from 50 different emails per month. Would he actually create 50 different email addresses? And what about the IP address ‚Äì doesn‚Äôt Glassdoor flag multiple reviews from the same IP?

One of the freelancers answered my question:

&#x200B;

https://preview.redd.it/g4id2yqeb20b1.png?width=2572&format=png&auto=webp&s=c2a77fdea8834a6d90f02b8b3eb67b3a874f3df2

Moving on ‚Äì another company that seems to buy fake reviews seems to be having some more trouble. Approximately a month after a freelancer linked me to fake reviews he had written for this company, all five reviews that he had linked me to had been removed:

&#x200B;

https://preview.redd.it/99fdvcgfb20b1.png?width=3116&format=png&auto=webp&s=b7e244529fc62b5c824d925feb61fd2cc16cbfd5

Based on this [Glassdoor webinar](https://youtu.be/3iy0JWOS1gs) from 2018, ‚Äúif it is found that a user has created multiple email accounts to submit reviews, then ALL submissions from that user are deleted‚Äù ‚Äì so likely Glassdoor‚Äôs content moderation team flagged one of the initial reviews and the same freelancer who was writing reviews for that company had all the fake reviews deleted.

So far, it looks like the key to an effective fake review creation strategy lies in:

* Spacing the fake reviews out
* Writing each review from a different IP address (i.e benefit of being part of a team)
* Using language that isn‚Äôt an obvious giveaway

On that third point: the reality is that many of these freelancers‚Äô first language is not English.

As an experiment, I turned to everybody‚Äôs favorite new toy, ChatGPT, and asked it to write me a positive Glassdoor review:

https://preview.redd.it/8w7cal9gb20b1.png?width=3164&format=png&auto=webp&s=860c39b11c5813e8b7fabdbb038d73c565cc98cf

And I‚Äôd say that the above answer was better than 95% of the fake reviews I came across.

**Removing reviews**

The process for removing an employer review usually works like this:

1. You identify one or multiple reviews that you want removed
2. You verify whether the review violates the site's Guidelines, or whether there‚Äôs something else about the review(s) that could get it removed.
3. You file an appeal to get it removed.

As an example, Glassdoor‚Äôs Review guidelines can be found [here](https://help.glassdoor.com/s/article/Community-Guidelines?language=en_US#:~:text=See%20More-,Review%C2%A0Guidelines,-Millions%20of%20job). Mainly, they forbid mentioning anyone by name who‚Äôs not an executive and revealing proprietary or confidential information, amongst a host of other things.

Sounds simple enough right? Well, according to one of the freelancers I messaged:

&#x200B;

https://preview.redd.it/x6s8hsyac20b1.png?width=2036&format=png&auto=webp&s=f86c386f864198dc43faeb41faea378090c20107

After some research, I summarized the different vendors and prices in the table below:

&#x200B;

|Channel|Cost|Timeline|Model|Self reported success rate|
|:-|:-|:-|:-|:-|
|Freelancer #1|$100 per review|3 days|Contingency Agreement Model|100%|
|Freelancer #2|$30 per review|7 days|Contingency Agreement Model|100%|
|Reputation management service #2|$450 per review|21 business days|Contingency Agreement Model|Unknown|
|Reputation management service #3|$1000 per review|Undefined|Contingency Agreement Model|100%|
|Reputation management service #4 Plan 1|$550 per review|5-6 weeks|Contingency Agreement Model|50-75%|
|Reputation management service #4 Plan 2|$300 Subscription + $100 per each review removed|Monthly service|Subscription plan|50-75%|
|Freelancer #3|$20|Undefined|Pay regardless|Undefined|
|Freelancer #4|$500|Undefined|Contingency Agreement Model|Undefined|

As you can see, unlike the fake review generation market, the prices vary quite a bit for getting reviews removed.

At one end, you have freelancers on gig marketplaces that will attempt to remove a review for less than $100. And then on the other end, you have ORMs (Online Reputation Management Agencies) that have multiple employees and more comprehensive packages in place. The one constant seems to be that most companies operate on a contingency agreement model (i.e pay only if review gets removed).

**Analyzing reviews**

ReviewMeta is a site that analyzes Amazon reviews and tells you how many are legitimate. The creator of the site, Tommy Noonan, mentions in an [interview with NPR](https://www.npr.org/sections/money/2018/06/27/623990036/episode-850-the-fake-review-hunter) that the main giveaway that a product is soliciting fake reviews is:

* A large, suspicious flood of positive reviews at the exact same time. For example, a 3 day stretch of time constituting 30% of total reviews.
* Phrases and words that are constantly repeated, especially in the section with no cons
* Brand monogamists (only review products from one company)

Whilst the last two bullets are hard to track, the first can be used to analyze different companies‚Äô reviews and to check if there might be some funky business going on.

After a couple of days, I have the ability to track review volume and review ratings over time for any company that I specify:

https://preview.redd.it/ehcbw2oje20b1.png?width=1653&format=png&auto=webp&s=b448ff35eb9878fbb1686de2fa8cf031e4ed3e05

Let the games begin.

## Voluntary Response Bias

One of the biggest challenges that review platforms face is the Voluntary Response bias.

Research shows many of today‚Äôs most popular online review platforms (e.g Amazon) have a distribution of opinion that is highly polarized, with many extreme positive and/or negative reviews, and few moderate opinions.

Think about it: have you ever felt moderately satisfied at your job and thought to yourself, now would be a great time to leave a Glassdoor review? Probably not.

On the other hand, if you‚Äôve had a terrible experience or even just had one thing really flip you off, you might be quite likely to leave an angry review.

Consider when a company goes through layoffs. You‚Äôre going to have a flood of angry reviews coming your way and are likely going to experience a ‚Äúspike‚Äù in reviews.

**Note:** Just like the Wall Street Journal‚Äôs methodology described [here](https://archive.is/20201016094732/https://www.wsj.com/articles/companies-manipulate-glassdoor-by-inflating-rankings-and-pressuring-employees-11548171977#selection-3965.0-3968.0), I considered there to be a spike if the total number of reviews in the month was greater than three standard deviations above the mean of the surrounding months.

Let‚Äôs take the company below. Here‚Äôs a graph of of their review volume since Jan 2020, including when they announced one of their first round of layoffs in June 2022:

https://preview.redd.it/n6kd9ejle20b1.png?width=3216&format=png&auto=webp&s=9eea2f3836617feca37eb88b1d3f67c8fa1b6fe2

In June 2022, approximately 19% of this company's 52 reviews were 1 star reviews (compared to an overall average of around 10%). This is what we could call a statistically significant spike in reviews. It also illustrates how the employees most likely to leave reviews are the ones that obviously had a bad experience (i.e getting laid off).

Here‚Äôs another company that had a similar spike in negative reviews due to layoffs in November 2022:

https://preview.redd.it/4vcnr1ine20b1.png?width=2408&format=png&auto=webp&s=f3877fb315ccc5d9a9294306a9f86616cb0fabd2

This company had an approximate 20% 1 star review rate (compared to an overall average of 12%) in November 2022, as well as an Avg Rating of 2.96 that month (compared to an overall average rating of 3.73).Unless HR is proactive, their reviews page risks succumbing to an echochamber of negative reviews that can really tilt one way.

**Note:** Glassdoor does state (based on [this video](https://www.youtube.com/watch?v=3iy0JWOS1gs) from 2017) that about 75% of the reviews on their platform are neutral. Their ‚Äúgive to get policy‚Äù has helped in keeping the platform from becoming too polarized.

I can understand why HR teams, like the ones that Nader talked to me about earlier, take a proactive stance towards managing their reviews. If they don‚Äôt try to control their reputation themselves, then their reputation risks getting controlled by the employees that had the worst possible experience.

## Goodhart‚Äôs Law

Goodhart‚Äôs law states the following:

*""When a measure becomes a target, it ceases to be a good measure""*

Every October, Glassdoor publishes their Best Places To Work ranking.

In a [report](https://www.wsj.com/articles/companies-manipulate-glassdoor-by-inflating-rankings-and-pressuring-employees-11548171977) that the WSJ did a couple of years ago, they found large spikes in the number of reviews that some companies (e.g SpaceX, Bain & Co, etc) got in September. The logic here is that some companies try to artificially inflate their Glassdoor reviews right before the October deadline.

I decided to revisit some of this analysis with Glassdoor‚Äôs 2023 Best Places To Work Ranking.

One of the companies I examined is rated as one of the best places to work in 2023. Let‚Äôs refer to this company as FunPlaceToWork.

Here is how their review volume looks like for all of 2022:

https://preview.redd.it/4e656zkqe20b1.png?width=2516&format=png&auto=webp&s=07141a66c56be7a6818efb9b1a4d912ee0021c91

FunPlaceToWork got around 50 reviews in September 2022. Of those 50 reviews, 96% were 5 star reviews.

FunPlaceToWork averaged 12 reviews per month up till then in 2022. Also, in the prior six months, the average percent of 5 star reviews received every month was \~75%.

Both the spike in volume of reviews and the spike in percentage of five star reviews are statistically significant.

I find it strange that Glassdoor‚Äôs proprietary algorithm and/or Human Content Moderation team did not find a spike of this nature unusual. If we look at Glassdoor‚Äôs eligibility criteria for the award, it‚Äôs as follows:

https://preview.redd.it/hag04y7se20b1.png?width=2868&format=png&auto=webp&s=ec2b920e126a8ea42b40d35aaa55d5341e69d022

The goal, according to Glassdoor, is to collect ‚Äúauthentic and unbiased reviews‚Äù.

Whilst there‚Äôs nothing against the rules for asking your employees to leave you reviews, I find the statistically significant spike of reviews at odds with the goal of collecting ""unbiased and authentic"" reviews (which Glassdoor states is the purpose of the awards).

Glassdoor states that an employer is allowed to ask its employees to leave reviews, but that they are not allowed to ‚Äúcoerce‚Äù them. Examples of what you can‚Äôt do:

* Offer incentives like Gift Cards in exchange for positive reviews.
* Withholding their reference letter unless they leave you a positive review.
* Anything that leads you to require proof for the employee to show you that they wrote a review.

It is possible to play by the rules (i.e not break any of the above rules) and to still in my opinion not collect authentic and unbiased reviews.

They say that you shouldn‚Äôt hate the player but the game ‚Äì I think **FunPlaceToWork** played by the rules, won fair and square, and that this is simply a perfect example of Goodhart‚Äôs Law.

I reached out to Glassdoor ([awards@glassdoor.com](mailto:awards@glassdoor.com)) about the above and this is the reply I got:

https://preview.redd.it/x0dqq39ue20b1.png?width=4800&format=png&auto=webp&s=c0102c963be9486370b340f2f473cbc6650fc48a

**Conclusion**

When I was 22, on an [F1 visa with 3 months to find work](https://www.careerfair.io/job-hunt-story), I didn‚Äôt give a damn about bad reviews. I needed a job and I‚Äôd sign any piece of paper you put in front of me.

Compare that to someone at the peak of their career, someone with optionality and a multitude of job offers; an ‚ÄúA-Player‚Äù, as the experts call it, would absolutely have the luxury of choice and discard a job offer based on bad company reviews.

For most people, the impact of online company reviews lies somewhere in the middle. In marketing, there‚Äôs a concept of a ‚Äúmarketing touchpoint‚Äù - an interaction with the brand over the course of the whole buying journey.

Company reviews are one of the many touchpoints a job seeker experiences over their interview process. And with the technology industry booming the past couple of years, companies couldn‚Äôt afford to slack on any touchpoints, including this one.

After all, when others start to game the system, you‚Äôre at a disadvantage if you don‚Äôt. The rewards can be quite high. Certainly higher than just trying to be as transparent as possible.

HR leaders are often more incentivized to inflate their metrics than to get honest feedback. Fake review writers have bills to pay. ORMs know that companies are desperate. And the platforms, well, aren‚Äôt always paying attention.

The result is a potluck of interests that leads to an underground economy.

One that ends up hurting the job seeker.

\*\*\*

Whew. That took a while (about 3 months in fact). Thanks for reading. For more content like this, [subscribe](https://www.careerfair.io/subscribe) to my newsletter. It's my best content delivered to your inbox once every 2 weeks."
1,zpraee,datascience,GPT,top,2022-12-19 13:15:11,The real reason ChatGPT was created,xdonvanx,False,0.94,742,https://i.redd.it/g5z2t4zeuu6a1.png,73,1671455711.0,
2,10k528k,datascience,GPT,top,2023-01-24 13:07:12,ChatGPT got 50% more marks on data science assignment than me. What‚Äôs next?,rifat_monzur,False,0.92,504,https://www.reddit.com/r/datascience/comments/10k528k/chatgpt_got_50_more_marks_on_data_science/,208,1674565632.0,"For context, in my data science master course, one of my classmate submit his assignment report using chatgpt and got almost 80%. Though, my report wasn‚Äôt the best, still bit sad, isn‚Äôt it?"
3,zgoxwa,datascience,GPT,top,2022-12-09 06:18:39,Gaussian Processes for pirates. Courtesy of ChatGPT,Sid__darthVader,False,0.98,496,https://i.redd.it/4cs2kirjwu4a1.jpg,34,1670566719.0,
4,10eye8i,datascience,GPT,top,2023-01-18 04:50:28,"I asked ChatGPT to explain ROC AUC, the level of collaboration is beyond my expectation",trantrikien239,False,0.9,477,https://master-data.science/assets/images/eli5ml-meth1.jpg,78,1674017428.0,
5,123tx9p,datascience,GPT,top,2023-03-27 17:25:43,Has ChatGPT killed doomers?,GreatStats4ItsCost,False,0.89,441,https://www.reddit.com/r/datascience/comments/123tx9p/has_chatgpt_killed_doomers/,90,1679937943.0,"Sorry for another ChatGPT post but I think it really is the end of asking whether certain job sectors will exist on r/DataScience due to ChatGPT making them redundant.

Whilst reading all of the 100's of doomer posts 'Will Data Science survive because ChatGPT' - it dawned on me that Chat GPT can replace all of the users creating these posts. They've all been made redundant. A simple prompt to an AI like 'Write a profoundly dumb Reddit post asking if Chat GPT has made Data Science redundant' - will return exactly that. With a simple workflow/pipeline the response from the API can be posted directly to r/DataScience. 

This really is the future and I'm worried."
6,zwppsu,datascience,GPT,top,2022-12-27 20:48:02,ChatGPT Extension for Jupyter Notebooks: Personal Code Assistant,Tieskeman,False,0.98,421,https://www.reddit.com/r/datascience/comments/zwppsu/chatgpt_extension_for_jupyter_notebooks_personal/,32,1672174082.0,"Hi!

I want to share a [browser extension](https://github.com/TiesdeKok/chat-gpt-jupyter-extension) that I have been working on. This extension is designed to help programmers get assistance with their code directly from within their Jupyter Notebooks, through ChatGPT.

The extension can help with code formatting (e.g., auto-comments), it can explain code snippets or errors, or you can use it to generate code based on your instructions. It's like having a personal code assistant right at your fingertips!

I find it boosts my coding productivity, and I hope you find it useful too. Give it a try, and let me know what you think!

You can find an early version here: 
https://github.com/TiesdeKok/chat-gpt-jupyter-extension"
7,10a7kq4,datascience,GPT,top,2023-01-12 19:01:24,I wrote up a guide showing how to do Data Science with ChatGPT.,Own-Anteater4164,False,0.83,275,https://www.reddit.com/r/datascience/comments/10a7kq4/i_wrote_up_a_guide_showing_how_to_do_data_science/,93,1673550084.0,"Just recently, I wrote up a guide on how to use [ChatGPT to build a website with Replit](https://buildspace.so/notes/chatgpt-replit-website?utm_source=r).

Got some pretty good responses, so I decided to write + document more of the applications I'm discovering.

**I'm actually really excited about this one, since I was in a graduate program for statistics.**

[Here's the guide](https://buildspace.so/notes/chatgpt-data-science?utm_source=r) for doing data sci with ChatGPT

The tl;dr is that I show you some of the crazy data sci stuff ChatGPT can do:

\- Read and analyze raw CSV data. I just had to copy and paste.

\- It could tell what kind of data you're feeding it judging by the header columns!

\- It will give you the python/r code on how to run specific analysis.

\- It even knew how to use scikit-learn to run regression models ü§Ø (I mean, this makes sense since it's an AI tool lol).

Honestly, this is just crazy to me.

**Before I dropped out of graduate school for statistics, I often consulted non-technical researchers in the social sciences. It was always a pain for them to run datasets by themselves just to get some answers to their questions.**

Although ChatGPT isn't perfect (and does make mistakes), it's crazy where the tool is going.

I think this is really good news for a lot of people who are interested in doing research, but might feel too intimidated by needing to do stats. Obvi...some bad stuff could come from it. We'll see!

https://preview.redd.it/ggd96gyhnnba1.png?width=619&format=png&auto=webp&s=5aa2f39199bb0ce56518e2972e0ec8a36ccbb69d"
8,zejzzs,datascience,GPT,top,2022-12-06 22:21:47,Chat_GPT,WeirdDiscipline1862,False,0.92,273,https://www.reddit.com/r/datascience/comments/zejzzs/chat_gpt/,136,1670365307.0,"This weekend millions of people rushed to check the Chat_GPT. This fueled many discussions regarding the job security of the future. People like Paul Krugman started talking about the future of job and massive job loss as the result of the AI which will be disruptive of course. And this time unless previously that the job loss was happening in the low skilled job categories, it will happen to the skilled workers. Any thoughts about what to do and how to persuade a new job specially after knowing that data analysis related jobs will be very vulnerable to AI technologies. 

‚ÄúIt's true that as AI and machine learning technologies continue to advance, they are likely to have an impact on many different fields, including data science. However, it's important to remember that while AI may automate some tasks and make certain job roles obsolete, it is also likely to create new job opportunities in areas such as AI research, development, and implementation.

In terms of what job you should pursue in the future, it's difficult to say for certain. The best thing to do is to stay up-to-date on the latest developments in AI and machine learning, and consider pursuing education and training in these areas. This will give you the skills and knowledge you need to adapt to the changing job market and take advantage of the new opportunities that are likely to arise.

It's also important to remember that there will always be a need for human expertise and creativity in many fields, including data science. So, even as AI continues to advance, there will likely still be plenty of opportunities for skilled data scientists who are able to think critically, solve complex problems, and apply their expertise to new challenges.‚Äù


This is the Chat_GPT‚Äôs answer to what to do as data scientist question. üòÄ"
9,12vl384,datascience,GPT,top,2023-04-22 22:04:04,Found this on an analyst position job ad on LinkedIn. Do you think the shade is reasonable?,BiggusCinnamusRollus,False,0.98,254,https://i.redd.it/cxcftudodiva1.png,64,1682201044.0,
10,125fd6p,datascience,GPT,top,2023-03-29 06:33:08,[D] Very good article about the current limitations of GPT-n models,fripperML,False,0.98,233,https://www.reddit.com/r/datascience/comments/125fd6p/d_very_good_article_about_the_current_limitations/,95,1680071588.0,"I count myself among the people that are amazed of what those models can do and how they can impact our society.

However, it's very important to understand that they are not magical solutions for every problem and that they cannot reason at all.

[ChatGPT as a query engine on a giant corpus of text ‚Äì r y x, r (ryxcommar.com)](https://ryxcommar.com/2023/03/28/chatgpt-as-a-query-engine-on-a-giant-corpus-of-text/)

What is more impressing is that, given this mental model of ChatGPT as a giant query engine, how can it perform activities that involve creativity outside of his training data? Like, for example, writing a poem in the style of Shakespeare about the proof that there are infinite prime numbers? Surely there are no examples of that in the training data! My answer would be that for some tasks interpolation works well (you can somehow get something meaningful by interpolating known stylistic elements and known semantical elements into something ""new""). But when the task is more symbolic or discrete, instead of interpolative, like true reasoning, and there are no examples to retrieve an answer from, the system has a much harder time.

That is, I am alligned with F. Chollet views on this:

[Fran√ßois Chollet en Twitter: ""You can retrieve not just what was seen at training time, but arbitrary combinations of it. It's an interpolative database and program store, with a natural language interface. https://t.co/2mv2gnI3oM"" / Twitter](https://twitter.com/fchollet/status/1637122108357738496)

[Fran√ßois Chollet en Twitter: ""This paper has the right idea: use symbolic logic for discrete reasoning and lean on deep learning models for perception and common-sense intuition. https://t.co/9lP8eDZKkO I expect to see a lot more progress along these lines in the coming months / years."" / Twitter](https://twitter.com/fchollet/status/1636838039703126016)"
11,1162ssq,datascience,GPT,top,2023-02-19 05:43:37,Buzz around new Deep Learning Models and Incorrect Usage of them.,brokened00,False,0.94,187,https://www.reddit.com/r/datascience/comments/1162ssq/buzz_around_new_deep_learning_models_and/,100,1676785417.0," In my job as a data scientist, I use deep learning models regularly to classify a lot of textual data (mostly transformer models like BERT finetuned for the needs of the company). Sentiment analysis and topic classification are the two most common natural language processing tasks that I perform, or rather, that is performed downstream in a pipeline that I am building for a company. 

The other day someone high up (with no technical knowledge) was telling me, during a meeting, that we should be harnessing the power of ChatGPT to perform sentiment analysis and do other various data analysis tasks, noting that it should be a particularly powerful tool to analyze large volumes of data coming in (both in sentiment analysis and in querying and summarizing data tables). I mentioned that the tools we are currently using are more specialized for our analysis needs than this chat bot. They pushed back, insisting that ChatGPT is the way to go for data analysis and that I'm not doing my due diligence. I feel that AI becoming a topic of mainstream interest is emboldening people to speak confidently on it when they have no education or experience in the field. 

After just a few minutes playing around with ChatGPT, I was able to get it to give me a wrong answer to a VERY EASY question (see below for the transcript). It spoke so confidently in it's answer, even going as far as to provide a formula, which it basically abandoned in practice. Then, when I pointed out it's mistake, it corrected the answer to another wrong one. 

The point of this long post was to point out that AI tool have their uses, but they should not be given the benefit of the doubt in every scenario, simply due to hype. If a model is to be used for a specific task, it should be rigorously tested and benchmarked before replacing more thoroughly proven methods.

ChatGPT is a really promising chat bot and it can definitely seem knowledgeable about a wide range of topics, since it was trained on basically the entire internet, but I wouldn't trust it to do something that  a simple pandas query could accomplish. Nor would I use it to perform sentiment analysis when there are a million other transformer models that were specifically trained to predict sentiment labels and were rigorously evaluated on industry standard benchmarks (like GLUE).

https://preview.redd.it/sz3ejc1313ja1.png?width=1700&format=png&auto=webp&s=76c48939903f8d34810000d1a71cb25f86237130"
12,11l5jqm,datascience,GPT,top,2023-03-07 17:31:31,"My AI tool to writes SQL queries for me now, so I don't have to. Thoughts?",slingshoota,False,0.89,125,https://www.reddit.com/r/datascience/comments/11l5jqm/my_ai_tool_to_writes_sql_queries_for_me_now_so_i/,60,1678210291.0,"I often write SQL queries for my Data Science job, but it can be really tedious and time-consuming. First I have to think about how to even approach the query, and then I have to google stuff to fix issues and refresh my memory.

I started using ChatGPT for help, but it was annoying to have to explain the tables/views every time.

To fix this, I built a tool that remembers your whole schema. It gives you a query to extract all the necessary info in one go and then you just copy-paste it once (it's saved with encryption). Then, all you have to do is write what you need in plain English, Ex. ""Users who have been online over 5 days this week"", and it writes the SQL query for you.

I showed it to my colleagues and they went crazy and are obsessed with it, as are my ex-colleagues from my last company.

What do you think? Would love to get your feedback. It's 100% free, you couldn't pay me even if you wanted to: www.blazesql.com"
13,10a1mik,datascience,GPT,top,2023-01-12 14:59:04,New Research From Google Shines Light On The Future Of Language Models ‚≠ï,LesleyFair,False,0.84,124,https://www.reddit.com/r/datascience/comments/10a1mik/new_research_from_google_shines_light_on_the/,18,1673535544.0,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs will allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* Stop hallucinating and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example are words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/w7xffqjimmba1.png?width=800&format=png&auto=webp&s=e2d9cb63f750efcbfa45c4bb7a985d4dcb5b0319)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.  
At **TheDecoding** ‚≠ï, I send out a thoughtful newsletter about ML research and the data economy once a week.  
No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)"
14,10fw1a3,datascience,GPT,top,2023-01-19 07:54:24,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.94,122,https://www.reddit.com/r/datascience/comments/10fw1a3/gpt4_will_be_500x_smaller_than_people_think_here/,14,1674114864.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/t6m0epzlgyca1.png?width=575&format=png&auto=webp&s=a5972941053f833e76fd3a5009fc68a61f9e5406)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ‚≠ï, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
15,122nwrk,datascience,GPT,top,2023-03-26 14:31:39,I asked ChatGPT for the best random state value,liquidInkRocks,False,0.89,97,https://www.reddit.com/r/datascience/comments/122nwrk/i_asked_chatgpt_for_the_best_random_state_value/,36,1679841099.0,"This thing is like a student forced to write 1000 words on the topic. 

**me:**  What is a good random state value to use in general 

**Bot:**  

In machine learning, a random state is used to initialize the random number generator. The random number generator is used to shuffle the data, split the data into training and test sets, initialize the parameters of the model, and make other random decisions during training and evaluation.

Choosing a good random state value is important because it can affect the results of your model. If you use a different random state, you may get different results.

In general, there is no one ""best"" random state value to use. The value you choose depends on the specific problem you're working on and the specific data you're working with.

However, it's a good practice to set a random state to ensure reproducibility of your results. You can choose any integer value for the random state, but it's common to use numbers like 0, 42, or 2022.

It's also common to use cross-validation techniques like k-fold cross-validation, which helps to reduce the variability of your results due to different random state values."
16,zcgpbp,datascience,GPT,top,2022-12-04 18:05:07,What do you guys think of OpenAI‚Äôs ChatGPT?,Loud_Ad_6272,False,0.95,56,https://www.reddit.com/r/datascience/comments/zcgpbp/what_do_you_guys_think_of_openais_chatgpt/,77,1670177107.0,"As the title goes, what do you guys think of this and what effect do you think it would have on the discipline and field going forward?"
17,11lt96v,datascience,GPT,top,2023-03-08 10:51:40,[Q] Are you wary of using ChatGPT because of sensitive data concerns?,limedove,False,0.87,39,https://www.reddit.com/r/datascience/comments/11lt96v/q_are_you_wary_of_using_chatgpt_because_of/,37,1678272700.0,
18,11szca1,datascience,GPT,top,2023-03-16 16:45:29,"What is your digital workspace, tools, setup, etc. for ETL, research, production?",BreathAether,False,0.92,36,https://www.reddit.com/r/datascience/comments/11szca1/what_is_your_digital_workspace_tools_setup_etc/,16,1678985129.0,"I'm new to this and so I've been wanting to know what other people have been using to make their work feel as smooth as butter. Since I've been learning lots and not just the industry standard stuff, I wanted to share what little I found to be valuable which others may want to try. **The main goal of this post is to share, critique, and provide suggestions so that we can all find the setup we like most. I  am also looking for new, up and coming tech, and definitely not afraid to try new things!**

IDE: **VSCode with the Jupyter Notebook Extension**. What I like about it is that I can view data structures like series/dataframes in a table format by clicking the variable in the Jupyter: Variables pane at the bottom. I started with plain vanilla jupyter notebooks from Anaconda so this was pretty nice. I have seen demos that **Jupyter Lab** has something like this, so if anyone has used both VSCode's notebooks and used Lab, your input would be appreciated. I hear good things about **PyCharm and Spyder**. Some people also use **Google Collab, DataSpell, and DeepNote** but I don't know enough about it. I did play around with DeepNote, and it was very cool but I didn't feel compelled to switch (and you have to pay for it!). 

Tools:

* A code helper: A few months back I was googling everything and I would've listed **Stackoverflow**. I might actually use that occasionally, but these days I use **ChatGPT** and **Bing AI**. For more current info or news-based I'll use Bing AI since it uses live search results, and for information that is knowledge based I might use ChatGPT. ChatGPT saves conversations so it's great for exploring topics in depth and referencing that conversation later. For those who have used both, maybe you know what I'm talking about and can provide a better explanation as to which is better for what purpose.
* Software: **Excel** is an obvious one. For instance, if I have a huge dataset and I just want to delete out columns that I don't need with Ctrl+click to select, it's easier and quicker than copy + pasting or typing out each of the string column names I want to ""df.drop()"". Excel is great for quick and simple stuff. Some software I have been learning about are I guess what I would consider as no- or low-code data analytics platforms, such as **Alteryx, KNIME, and Orange**. These software let you practically run an entire ETL pipeline. I believe Alteryx and KNIME are the gold-standard in this category, and Orange is a ""lite"" version of the two and is available in Anaconda. I think these are pretty cool, and I personally haven't found a huge use case for them since I've been chugging away in my notebooks with Python, but I can see the value. Would love for someone to chime in on these tools and how they compare to manually doing stuff in code, especially for large datasets.
* Version Control: This is where I'm primarily lacking, but I know that **Github** is the go-to. I don't use this but I know that a ton of people do. I don't even know where to start to be honest. I usually just create a new .ipynb file for each analysis or phase of an ETL pipeline haha. I'm also not too aware of what other innovative tools for version control exist.
* Python Libraries: Besides the obvious stuff like Pandas/NumPy, MatplotLib/Seaborn, and your popular ML libraries, I've recently found out about this library called **Polars**. It's basically a Rust version of Pandas, and it's super powerful. Some operations that I've run, that would've taken hours with Pandas, took me minutes. But I've been hearing that **Pandas 2.0** which will be released some time this month, has been looking at using PyArrow dtypes (if I recall correctly) and the speed is comparable to Polars. I mean these two are FAST. Another contender is **DuckDB** but I think the new Pandas and Polars are still faster. I mostly use Pandas but if there is some heavy lifting, I'll swap the dataframe to a polars one with a quick function, run it with polars, then back to pandas.

Anyway, that's just some things I can immediately think of. Looking forward to your suggestions! Bonus points for anything new and innovative. Cheers.

https://preview.redd.it/qj2cywt1r4oa1.png?width=1920&format=png&auto=webp&s=c4e02f5fec0b3768df336c7a3f63cc382b3954a8"
19,117736x,datascience,GPT,top,2023-02-20 13:47:54,Is NLP a sub-field with a lot of growth?,sonicking12,False,0.78,32,https://www.reddit.com/r/datascience/comments/117736x/is_nlp_a_subfield_with_a_lot_of_growth/,35,1676900874.0,I am thinking about how Amazon reportedly axed a lot of Alexa teams.  This seems to point to negative.  But ChatGPT is taking up a lot of interest. This seems to point to positive.   What are your thoughts?
20,130hqft,datascience,GPT,top,2023-04-27 10:53:05,Low hanging fruit projects for business with non-mature data science/analytics?,BobzzYourUncle,False,0.89,33,https://www.reddit.com/r/datascience/comments/130hqft/low_hanging_fruit_projects_for_business_with/,23,1682592785.0,"Hey data legends,

I've just started to learn a bit of Python and it's got me going down the rabbit hole of possible business applications for data analysis/science in this small/medium business (B2B with typically only a couple of transactions per customer each year).  What's currently done is very basic stuff in excel and no machine learning etc. (I have no background in data science other than basic knowledge but I feel there is a lot of potential)

I've managed to automate a PDF report that has some basic stuff using Plotly and Pandas and am wondering where I should focus my efforts next.

What are the general low hanging fruits that I should try and start out with for a business that has very little maturity on this front?

Chat GPT has come back with some suggestions like: Customer segmentation, Churn analysis, sales forecasting, website optimisation, recommendation engines, predictive CLV.  

Any help or insights would be appreciated pointing me in the right direction.  Thanks"
21,12m27p4,datascience,GPT,top,2023-04-14 16:08:04,Public Bloom Instance?,fokke2508,False,0.84,31,https://www.reddit.com/r/datascience/comments/12m27p4/public_bloom_instance/,10,1681488484.0,"I was looking into self-hosting Bloom as an alternative to GPT. Besides concerns about the context window being too small and the overall quality, I do really like it from a privacy and availability perspective.   


But a production machine running it would cost about 280K per year. I am contemplating setting this up as a shared resource and making it publicly available as an alternative to GPT. Would anyone be interested in that?"
22,126ndlu,datascience,GPT,top,2023-03-30 13:41:51,Seeing a lot of job openings for high-level AI and Data Analytics positions...,fingin,False,0.77,23,https://www.reddit.com/r/datascience/comments/126ndlu/seeing_a_lot_of_job_openings_for_highlevel_ai_and/,20,1680183711.0,"I have noticed an uptick in jobs for things like prompt engineer, AI ethics lead, AI manager. When you look at these requirements it looks like relatively low entry: a familiarity with general AI and AI regulations (not that there is a ton of expertise to be had in this latter category). They don't require much or any technical skill. 

I'll admit, I find myself frustrated as I work in a highly technical role and feel like these opportunities are really 'low hanging fruit', due to the vagueness of the requirements. I'm sure many of us wear not just technical hats but also those of product management, coaching and training, etc. 

What do you think? Is it just a fad stemming from ChatGPT and Image gen promotion? Are you going to make a job switch and apply for these roles?"
23,zmye7g,datascience,GPT,top,2022-12-15 22:34:22,Have you used ChatGPT to write code for you?,is_this_the_place,False,0.79,26,https://www.reddit.com/r/datascience/comments/zmye7g/have_you_used_chatgpt_to_write_code_for_you/,33,1671143662.0,Is so what did you ask and how did it go?
24,107khox,datascience,GPT,top,2023-01-09 17:37:48,FYI: GPT-3 & Beyond (Stanford AI Webinar),itedelweiss,False,0.86,22,https://www.reddit.com/r/datascience/comments/107khox/fyi_gpt3_beyond_stanford_ai_webinar/,1,1673285868.0,"The field of Natural Language Processing (NLP) has seen tremendous progress. In particular, algorithms for Natural Language Understanding (NLU) are evolving and improving at a stunning pace. Systems that seemed like science fiction as recently as ten years ago are now commonplace. One such system is GPT-3 (Generative Pretrained Transformer 3), a state-of-the-art model capable of generating human-like text for a wide range of tasks, such as conversation modeling, summarization, and question answering.  

In this live webinar Stanford Professor Christopher Potts will discuss the significance and implications of recent Natural Language Understanding developments including GPT-3. He will outline the fundamental building blocks of these new systems and describe how we can reliably assess and understand them.

Topics will include:

- Technical capabilities, limitations, and applications of new Natural Language Understanding systems

- Analysis of the performance of GPT-3 on various language tasks

- Potential future developments in Natural Language Processing

DATE: January 18, 2023
 	

TIME: 11:00 AM - 12:00 PM (PST)


[Registration link](https://event.on24.com/wcc/r/4076653/D32B5B8B45C4099498D2555AB941504C?mkt_tok=MTk0LU9DUS00ODcAAAGJNpCzioiOp8O-K8z9UB050H0f-EbLnEk9bZVBNI2vmlidmSvXCca2T1T2-BFseDFQyKNr0wv1iH8RGEXMDOj_E1d47fWkanGqcP2AjCkwjYFtaGU)"
25,10mi1x8,datascience,GPT,top,2023-01-27 10:52:18,‚≠ï What People Are Missing About Microsoft‚Äôs $10B Investment In OpenAI,LesleyFair,False,0.76,18,https://www.reddit.com/r/datascience/comments/10mi1x8/what_people_are_missing_about_microsofts_10b/,3,1674816738.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/35vtxrwnekea1.png?width=720&format=png&auto=webp&s=a61dd557e1d00c96448c429c9f9bb78516205a6f)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI‚Äôs models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let‚Äôs first look at Sam Altman‚Äôs backstory.

*Let‚Äôs go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI ‚Äî one of the most exciting and impactful organizations in all of tech.

However, OpenAI ‚Äî the rocket ship of AI innovation ‚Äî is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn‚Äôt take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can‚Äôt they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT‚Äôs outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That‚Äôs gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI‚Äôs profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI‚Äôs API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don‚Äôt have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman‚Äôs foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft‚Äôs sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI ‚Äî whatever that looks like ‚Äî the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ‚≠ï.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)‚Äã

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)‚Äã

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)‚Äã

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)‚Äã

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)‚Äã

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
26,zcmlp0,datascience,GPT,top,2022-12-04 21:39:24,Unofficial Python SDK for OpenAI's ChatGPT,brunneis,False,0.9,15,https://github.com/labteral/chatgpt-python,1,1670189964.0,
27,1360alu,datascience,GPT,top,2023-05-02 21:06:32,How busy are you? Under/over utilization,MorningDarkMountain,False,0.77,13,https://www.reddit.com/r/datascience/comments/1360alu/how_busy_are_you_underover_utilization/,13,1683061592.0,"As simple as the title: I hear more and more often about Data Scientists feeling themselves to be under-utilized. Lack of projects, no real impact on the business, fear of being replaced by ChatGPT, stakeholders that do not really understand Data Science at all.

Are you currently under-utilized as well?"
28,138ckjn,datascience,GPT,top,2023-05-05 06:07:33,The language in which GPT 3.5 communicates changes how it thinks.....,NoCartographer4725,False,0.64,12,https://www.reddit.com/r/datascience/comments/138ckjn/the_language_in_which_gpt_35_communicates_changes/,19,1683266853.0,"Seems like a new paper shows that GPT is more patient when talking in Mandarin and German vs when talking in English and Russian.

[https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=4437617](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4437617)

&#x200B;"
29,10iue4e,datascience,GPT,top,2023-01-22 21:12:11,"I wrote a tiny library this morning for openai's (suddenly really good, really cheap) embeddings to search stuff - Semantic search, smarter replies w/ GPT-3, easier.",morganpartee,False,0.85,14,https://www.sensibledefaults.io/blog/chatgpt/easy-python-embeddings,14,1674421931.0,"I've had a few buddies in the community ask about something like this in the last week, and then a reader did today so I finally took the time to write it up. It's basically a generic interface to try embeddings, and it's less than a hundred lines of code. Built this so you can just authenticate and test without having to fuss with numpy, pinecone, etc.

The only thing you do is put dictionaries with a name and text key, then search your embeddings instance. We return anything you stick in there with it too for your programming pleasure - think dates, page numbers, file names, whatever.

You can just... Build chat bots that are pretty smart like this for next to nothing by reading in your docs or whatever. It's cheap and incredibly effective. I know next to nothing about how to preprocess data for this thing, I've tried different sizes of chunks, lines, sentences and haven't found a one size fits all solution (other than more gpt to summarize) but - just try it, it's cheap as hell and there's so much you can do. Like the rest of the current state of gpt - it just kinda works. Start with one to five sentences (or lines for code) and experiment!

For you proper data scientists (nerds!) - you can just get the numpy array out and do regular embeddings stuff with it. If you do, I'd love to hear about it! Clustering, classification, etc are suddenly super easy, and the raw data is stored in json so you can upload it to something better when you're ready.

I know it's small, but little abstractions make life better. Anything we can do to lower the barrier to experimentation is worth doing. Hope it helps someone!"
30,1289p4d,datascience,GPT,top,2023-04-01 03:24:17,Do you think NLP will increase with LLM models?,Muted_Standard175,False,0.86,14,https://www.reddit.com/r/datascience/comments/1289p4d/do_you_think_nlp_will_increase_with_llm_models/,7,1680319457.0,"I am thinking in studying this, some say NLP will decrease as GPT can beat most of NLP tasks in a low cost. What do you say?"
31,13a4wtf,datascience,GPT,top,2023-05-06 22:52:26,"ChatGPT Code Interpreter customer segmentation workflow demo: Data description, feature selection, preprocessing, clustering, fine-tuning, interpretation and market strategy (5:12 video)",datasciencepro,False,0.72,10,https://twitter.com/aakashg0/status/1654703707869822976/video/1,3,1683413546.0,
32,118ybon,datascience,GPT,top,2023-02-22 13:15:21,Overview of state-of-art algorithms in their respective usecase?,Different_Day_3821,False,0.81,10,https://www.reddit.com/r/datascience/comments/118ybon/overview_of_stateofart_algorithms_in_their/,12,1677071721.0,"Fellow scientists... On the second year of data science i've become increasingly confused as to when, what and why to use certain algorithms... Specially in a field as rapidly developing as ours.

therefore i ask you if any of you got an overview of what the current best algorithms is according to their uses? Is there a systemisc ""cheat-sheet"" or overview of the current best libraries to import for X types of data and analysis?

For example learning NLP. First you learn Bag of Word models, then IDF-TF models. Then you learn thats outdated and people use Word2Vec models that retain semantic meaning. Then you learn thats outdated and that people use Bert models. But now even that in competition with transformer-based models as ChatGPT is based on etc.

Same thing with time series. First you like ARIMA, then you learn RNN, then LSTM, then Autoencoders, then to use autoencoders as unsupervised feature extraction into a supervised model etc.

And the more complicated you modern you get, the more specific becomes that imported libraries. Anomaly detection? Go PyOD/Prophet/PySAD etc.

How do you guys navigate in this? Is there some conversative principles that would keep your methods within some kind of boundary of usefullnes? I miss the old days of only knowing basic sklearn and basic naive bayes and clustering algorithms lol. On the otherhand if state of the art just means importing and fitting the right algorithm then i guess it ain't so bad.

tl;dr: Is there a cheat-sheet of X-kind of analsis then Y-kind of algorithm performs best? And how do you guys manage all the libraries/algorithms?"
33,12oiyni,datascience,GPT,top,2023-04-16 17:57:00,Challenge: Use data science to predict ChatGPT failures,Neurosymbolic,False,0.69,9,https://www.reddit.com/r/datascience/comments/12oiyni/challenge_use_data_science_to_predict_chatgpt/,2,1681667820.0,"Last month, at the AAAI-MAKE conference, we introduced a new challenge problem: predict the failures of ChatGPT when solving math problems.

We have compiled a dataset (based on DRAW-1K) that consists of 1,000 math problem and ChatGPT's response.  We introduced some baseline models at AAAI-MAKE that showed you can predict ChatGPT's failures, we are asking the community to improve on the results.  No need to write a ChatGPT app or anything like that - you can use pure data science techniques.

The challenge results will be due in early 2024 and presented at AAAI-MAKE next year.  You can pre-register here: [https://www.aaai-make.info/next/](https://www.aaai-make.info/next/)

To learn more about the challenge, visit this website: [https://neurosymbolic.asu.edu/chatgpt-mwp-challenge/](https://neurosymbolic.asu.edu/chatgpt-mwp-challenge/)"
34,12jb54e,datascience,GPT,top,2023-04-12 05:21:27,Is OpenAI‚Äôs Study On The Labor Market Impacts Of AI Flawed?,LesleyFair,False,0.8,9,https://www.reddit.com/r/datascience/comments/12jb54e/is_openais_study_on_the_labor_market_impacts_of/,0,1681276887.0,"[Example img\_name](https://preview.redd.it/wzz3wtwu1eta1.png?width=1451&format=png&auto=webp&s=9a10cc08b28effc9cbda57b43d625bfcc5c03be2)

We all have heard an uncountable amount of predictions about how AI will¬†***terk err jerbs!***

However, here we have a proper study on the topic from OpenAI and the University of Pennsylvania. They investigate how Generative Pre-trained Transformers (GPTs) could automate tasks across different occupations \[1\].

Although I‚Äôm going to discuss how the study comes with a set of ‚Äúimperfections‚Äù, the findings still make me really excited. The findings suggest that machine learning is going to deliver some serious productivity gains.

People in the data science world fought tooth and nail for years to squeeze some value out of incomplete data sets from scattered sources while hand-holding people on their way toward a data-driven organization. At the same time, the media was flooded with predictions of omniscient AI right around the corner.

*Let‚Äôs dive in and take an*¬†exciting glimpse into the future of labor markets\*!\*

# What They Did

The study looks at all US occupations. It breaks them down into tasks and assesses the possible level of for each task. They use that to estimate how much automation is possible for a given occupation.

The researchers used the¬†[O\*NET database,](https://www.onetcenter.org/database.html)¬†which is an occupation database specifically for the U.S. market. It lists 1,016 occupations along with its standardized descriptions of tasks.

The researchers annotated each task once manually and once using GPT-4. Thereby, each task was labeled as either somewhat (<50%) or significantly (>50%) automatable through LLMs. In their judgment, they considered both the direct ‚Äúexposure‚Äù of a task to GPT as well as to a secondary GPT-powered system, e.g. LLMs integrated with image generation systems.

To reiterate, a higher ‚Äúexposure‚Äù means that an occupation is more likely to get automated.

Lastly, they enriched the occupation data with wages and demographic information. This was used to determine whether e. g. high or low-paying jobs are at higher risk to be automated.

So far so good. This all sounds pretty decent. Sure, there is a lot of qualitative judgment going into their data acquisition process. However, we gotta cut them some slag. These kinds of studies always struggle to get any hard data and so far they did a good job.

However, there are a few obvious things to criticize. But before we get to that let‚Äôs look at their results.

# Key Findings

The study finds that 80% of the US workforce, across all industries, could have at least some tasks affected. Even more significantly, 19% of occupations are expected to have at least half of their tasks significantly automated!

Furthermore, they find that higher levels of automation exposure are associated with:

* Programming and writing skills
* Higher wages (contrary to previous research!)
* Higher levels of education (Bachelor‚Äôs and up)

Lower levels of exposure are associated with:

* Science and critical thinking skills
* Manual work and tasks that might potentially be done using physical robots

This is somewhat unsurprising. We of course know that LLMs will likely not increase productivity in the plumbing business. However, their findings underline again how different this wave is. In the past, simple and repetitive tasks fell prey to automation.

*This time it‚Äôs the suits!*

If we took this study at face value, many of us could start thinking about life as full-time pensioners.

But not so fast! This, like all the other studies on the topic, has a number of flaws.

# Necessary Criticism

First, let‚Äôs address the elephant in the room!

OpenAI co-authored the study. They have a vested interest in the hype around AI, both for commercial and regulatory reasons. Even if the external researchers performed their work with the utmost thoroughness and integrity, which I am sure they did, the involvement of OpenAI could have introduced an unconscious bias.

*But there‚Äôs more!*

The occupation database contains over 1000 occupations broken down into tasks. Neither GPT-4 nor the human labelers can possibly have a complete understanding of all the tasks across all occupations. Hence, their judgment about how much a certain task can be automated has to be rather hand-wavy in many cases.

Flaws in the data also arise from the GPT-based labeling itself.

The internet is flooded with countless sensationalist articles about how AI will replace jobs. It is hard to gauge whether this actually causes GPT models to be more optimistic when it comes to their own impact on society. However, it is possible and should not be neglected.

The authors do also not really distinguish between labor-augmenting and labor-displacing effects and it is hard to know what ‚Äúaffected by‚Äù or ‚Äúexposed to LLMs‚Äù actually means. Will people be replaced or will they just be able to do more?

Last but not least, lists of tasks most likely do not capture all requirements in a given occupation. For instance ""making someone feel cared for"" can be an essential part of a job but might be neglected in such a list.

# Take-Away And Implications

GPT models have the world in a frenzy - rightfully so.

Nobody knows whether 19% of knowledge work gets heavily automated or if it is only 10%.

As the dust settles, we will begin to see how the ecosystem develops and how productivity in different industries can be increased. Time will tell whether foundational LLMs, specialized smaller models, or vertical tools built on top of APIs will be having the biggest impact.

In any case, these technologies have the potential to create unimaginable value for the world. At the same time, change rarely happens without pain. I strongly believe in human ingenuity and our ability to adapt to change. All in all, the study - flaws aside - represents an honest attempt at gauging the future.

Efforts like this and their scrutiny are our best shot at navigating the future. Well, or we all get chased out of the city by pitchforks.

Jokes aside!

What an exciting time for science and humanity!

As always, I really enjoyed making this for you and I sincerely hope you found value in it!

If you are not subscribed to the newsletter yet,¬†[click here to sign up](https://thedecoding.net/)! I send out a thoughtful 5-minute email every week to keep you in the loop about machine learning research and the data economy.

*Thank you for reading and I see you next week ‚≠ï!*

**References:**

\[1\]¬†[https://arxiv.org/abs/2303.10130](https://arxiv.org/abs/2303.10130)"
35,123yr9p,datascience,GPT,top,2023-03-27 20:09:45,ChatGPT and made up citations,its_the_llama,False,0.69,6,https://www.reddit.com/r/datascience/comments/123yr9p/chatgpt_and_made_up_citations/,10,1679947785.0,"Hopefully this hasn't been posted here before. I was using ChatGPT to try and find a few reviews to read on parameter search strategies in system biology. Helpfully, it returned a list of seminal papers: 

 

1. ""Parameter estimation and inference for differential equations"" by P.J. Haario et al. (2006). This review article discusses various methods for parameter estimation in differential equations, including Bayesian methods, gradient-based optimization, and particle filtering.
2. ""Parameter estimation in differential equations: a Bayesian perspective"" by S. S√§rkk√§ (2013). This review article focuses on Bayesian methods for parameter estimation in differential equations, including Markov chain Monte Carlo (MCMC) methods and sequential Monte Carlo (SMC) methods.
3. ""Optimization methods for parameter estimation in nonlinear differential equation models"" by C.L. Lawson et al. (2013). This review article discusses various optimization methods for parameter estimation in nonlinear differential equation models, including gradient-based methods, genetic algorithms, and particle swarm optimization.
4. ""Parameter estimation and sensitivity analysis in computational systems biology"" by D. Ge and J.J. Liu (2019). This review article provides an overview of various methods for parameter estimation and sensitivity analysis in computational systems biology, including optimization-based methods, Bayesian methods, and global sensitivity analysis.

Great, right? Except, **none of these papers actually exist**. The authors sound similar to people in the field (""P.J. Haario is probably ""inspired"" by Heikki Haario who's well known in the field, and Simo S√§rkk√§ is an actual author who's published on this), but the work does not exist. 

In hindsight, this makes sense considering how chatGPT works. It's still pretty interesting though, and I wonder how many people have turned in college assays with completely fabricated references."
36,13c8ewn,datascience,GPT,top,2023-05-08 23:21:30,I asked ChatGPT4 to do some stats modelling - it was okay‚Ä¶ish,AFL_gains,False,0.61,4,https://www.reddit.com/r/datascience/comments/13c8ewn/i_asked_chatgpt4_to_do_some_stats_modelling_it/,11,1683588090.0,"Hi guys ! There‚Äôs been some debate, especially on here, about the ‚Äúfuture of data science‚Äù and ‚Äúwhose job is going to be taken‚Äù etc etc. Imo I don‚Äôt know the answer, but I think LLMs have definitely changed the landscape.

One of the really interesting things ChatGPT has unlocked is that people can now code without really knowing how to. I think if you already are familiar with coding, using ChatGPT to improve productivity is awesome. But if you‚Äôre just starting out and use it generate code you can‚Äôt explain, then I think you can get into lots of trouble. 

And I think this is especially true when there‚Äôs a mathematical modelling choice aspect to your code. My thought was that just because something works / compiles, doesn‚Äôt mean it‚Äôs a very good model and doesn‚Äôt mean that the explicit choices / assumptions make sense. This, of course, isn‚Äôt chatGPTs fault, it‚Äôs the users fault for not checking! 

Anyway, to investigate this point, I recently tested ChatGPT to write a Stan code (bayesian inference ) to predict premier league matches. My feeling was that the task simple enough for it to do an okay job, but not so generic it there‚Äôs a million examples on the internet.

I put the results on YouTube (link below), but in summary I found the following: 

1. ChatGPT made a decent model, but with some really weird choices. Eg It decided to use a normal distribution to model goal differences , where I think a Skellam would have been better. It also decided not to model the variance of this distribution , instead deciding that it was 1. Super weird!

2. It wasn‚Äôt able to rationalise about things like over parameterisation. The model it build had way too many parameters, unnecessarily. The idea of parsimony wasn‚Äôt really there. Maybe with better prompts it would have, but out of the box it made the model overly complex


3. Prompt engineering really makes a difference. I think with better prompts, the model Could have been better. There was even a point where I spotted an error and prompted chatGPT to fix it and it did! But again, this all relied on me being able to read Stan code and know what was good and bad. 


For me, I learnt that at least for tasks where lots of modelling choices need to be made, humans still beat GPT. But perhaps in the future, those that win will be the data scientists/ engineers that know what they are doing but are able to prompt GPT optimally to maximise their productivity boost.



The videos are here : 
Part 1: https://m.youtube.com/watch?v=4LTUYTxKuIk&t=66s&pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D
Part 2: https://m.youtube.com/watch?v=XjQpV6c9K5g&t=1s&pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D"
37,134o5fe,datascience,GPT,top,2023-05-01 14:33:50,"[colabdog.com] I build an aggregator for AI News that combines OpenAI, Google AI, MIT, BAIR.",colabDog,False,0.75,4,https://i.redd.it/rlizo23od8xa1.png,1,1682951630.0,
38,zy1brw,datascience,GPT,top,2022-12-29 09:38:58,Adding Chat GPT to RStudio with the GPT Studio add-in package,DrLyndonWalker,False,0.73,5,https://www.reddit.com/r/datascience/comments/zy1brw/adding_chat_gpt_to_rstudio_with_the_gpt_studio/,0,1672306738.0,"You can now add the incredible functionality of Chat GPT (including writing text and code) to RStudio using the GPT Studio package. In this video I cover the steps for setting up GPT Studio and then try some basic operations including spell correction and code writing. As I demonstrated in my recent RTutor video, Chat GPT is a game-changer. It can write code and text with a scary degree of precision. 

[https://youtu.be/QQfDTLExoNU](https://youtu.be/QQfDTLExoNU)"
39,10mu9ru,datascience,GPT,top,2023-01-27 19:53:34,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,1.0,5,https://www.reddit.com/r/datascience/comments/10mu9ru/a_python_module_to_generate_optimized_prompts/,0,1674849214.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features üöÄ

* üßô‚Äç‚ôÄÔ∏è NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* üî® Easily add one-shot, two-shot, or few-shot examples to the prompt
* ‚úå Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* üí• Custom examples and samples can be easily added to the prompt
* üí∞ Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Examples](https://preview.redd.it/x232msli2nea1.png?width=1236&format=png&auto=webp&s=6071efdd8cb12801230af6572991ba8aaff1a9ec)

&#x200B;

https://preview.redd.it/5o5uqk3k2nea1.png?width=1398&format=png&auto=webp&s=96c25820a698a83dfeb0e8f7f37682d9d27c06cb"
40,zrvydv,datascience,GPT,top,2022-12-21 18:04:50,Advice for a recent college graduate who majored in Computer Science and Statistics looking to start a career in data? How does job security look?,OGlogicgate,False,0.62,3,https://www.reddit.com/r/datascience/comments/zrvydv/advice_for_a_recent_college_graduate_who_majored/,3,1671645890.0,"Hello, I graduated from university (not elite but notable) this semester with a major in Computer Science and Statistics. My internship experience is limited to front end development but as I was accruing credits towards my statistics major over the past year and a half and taking a course on machine learning, I've decided that I want to go into the field of data science. Unfortunately I don't have any internship experience in the field but after alot of consideration, a career in data science seems to be what I have the most interest in.

Through my coursework and projects I've had alot of exposure to python and data and ml frameworks, namely numpy and pyTorch. I also have experience in SQL and backend query languages.  

My question is, what should I focus on doing to make myself a more appealing candidate to get into the field of data science? Are there any certificate programs like the TensorFlow developer certificate that would help in me getting a job in this field? I should note that I had my fair share of personal issues in college and that my gpa is a 2.7 which really concerns me about my chances.

I apologize if this question is too open-ended or lacks basic research on my part, I've been struggling on what career I wanna go into and just recently decided I want to orient myself towards data science. Any advice would be greatly appreciated.

Also, as a side if you can touch a little bit on what job security looks like in the field I would greatly appreciate it. I've been paying attention to the openAI language model and like I'm sure many others, was frightened by what it could do. I understand there's no way it could replace a data scientist in it's current state however who knows what it can do in future iterations? How likely is it that large language models like chatGPT will either replace or displace a large percentage of data scientists in the field?"
41,10qecvo,datascience,GPT,top,2023-01-31 23:43:50,Yann LeCun Hating on ChatGPT,MGeeeeeezy,False,0.56,4,https://www.reddit.com/r/datascience/comments/10qecvo/yann_lecun_hating_on_chatgpt/,35,1675208630.0,"Has anyone else noticed how much Yan LeCun has been hating on ChatGPT lately? I think it‚Äôs one thing to call out issues, but I have a feeling he‚Äôs trying to discredit it for business politics more than anything (Meta v. Microsoft)"
42,10k528k,datascience,GPT,comments,2023-01-24 13:07:12,ChatGPT got 50% more marks on data science assignment than me. What‚Äôs next?,rifat_monzur,False,0.92,502,https://www.reddit.com/r/datascience/comments/10k528k/chatgpt_got_50_more_marks_on_data_science/,208,1674565632.0,"For context, in my data science master course, one of my classmate submit his assignment report using chatgpt and got almost 80%. Though, my report wasn‚Äôt the best, still bit sad, isn‚Äôt it?"
43,11aq3j0,datascience,GPT,comments,2023-02-24 12:20:01,I genuinely think this field will die,dataentryadmin,False,0.22,0,https://www.reddit.com/r/datascience/comments/11aq3j0/i_genuinely_think_this_field_will_die/,146,1677241201.0,"The first release of Chat-GPT is already hugely transformative. We are kidding ourselves that within a few years, AI won‚Äôt be able to go from prompt to hundreds of line of working code in seconds.

The only limitation of AI currently is navigation of the physical world, which won‚Äôt be more than 10 years away.

Seriously, what was once a niche field for 1% academics is about to be taken over by a much smarter, faster AI. In hindsight, quite obviously AI would be able to navigate code and data better than us. 

The ONLY thing we have left is understanding and translating business needs of humans. We navigate complex social structures in a physical world and translating human needs into technical requirements is all we have left.

Someone prove me wrong please."
44,zejzzs,datascience,GPT,comments,2022-12-06 22:21:47,Chat_GPT,WeirdDiscipline1862,False,0.92,268,https://www.reddit.com/r/datascience/comments/zejzzs/chat_gpt/,136,1670365307.0,"This weekend millions of people rushed to check the Chat_GPT. This fueled many discussions regarding the job security of the future. People like Paul Krugman started talking about the future of job and massive job loss as the result of the AI which will be disruptive of course. And this time unless previously that the job loss was happening in the low skilled job categories, it will happen to the skilled workers. Any thoughts about what to do and how to persuade a new job specially after knowing that data analysis related jobs will be very vulnerable to AI technologies. 

‚ÄúIt's true that as AI and machine learning technologies continue to advance, they are likely to have an impact on many different fields, including data science. However, it's important to remember that while AI may automate some tasks and make certain job roles obsolete, it is also likely to create new job opportunities in areas such as AI research, development, and implementation.

In terms of what job you should pursue in the future, it's difficult to say for certain. The best thing to do is to stay up-to-date on the latest developments in AI and machine learning, and consider pursuing education and training in these areas. This will give you the skills and knowledge you need to adapt to the changing job market and take advantage of the new opportunities that are likely to arise.

It's also important to remember that there will always be a need for human expertise and creativity in many fields, including data science. So, even as AI continues to advance, there will likely still be plenty of opportunities for skilled data scientists who are able to think critically, solve complex problems, and apply their expertise to new challenges.‚Äù


This is the Chat_GPT‚Äôs answer to what to do as data scientist question. üòÄ"
45,1162ssq,datascience,GPT,comments,2023-02-19 05:43:37,Buzz around new Deep Learning Models and Incorrect Usage of them.,brokened00,False,0.94,193,https://www.reddit.com/r/datascience/comments/1162ssq/buzz_around_new_deep_learning_models_and/,100,1676785417.0," In my job as a data scientist, I use deep learning models regularly to classify a lot of textual data (mostly transformer models like BERT finetuned for the needs of the company). Sentiment analysis and topic classification are the two most common natural language processing tasks that I perform, or rather, that is performed downstream in a pipeline that I am building for a company. 

The other day someone high up (with no technical knowledge) was telling me, during a meeting, that we should be harnessing the power of ChatGPT to perform sentiment analysis and do other various data analysis tasks, noting that it should be a particularly powerful tool to analyze large volumes of data coming in (both in sentiment analysis and in querying and summarizing data tables). I mentioned that the tools we are currently using are more specialized for our analysis needs than this chat bot. They pushed back, insisting that ChatGPT is the way to go for data analysis and that I'm not doing my due diligence. I feel that AI becoming a topic of mainstream interest is emboldening people to speak confidently on it when they have no education or experience in the field. 

After just a few minutes playing around with ChatGPT, I was able to get it to give me a wrong answer to a VERY EASY question (see below for the transcript). It spoke so confidently in it's answer, even going as far as to provide a formula, which it basically abandoned in practice. Then, when I pointed out it's mistake, it corrected the answer to another wrong one. 

The point of this long post was to point out that AI tool have their uses, but they should not be given the benefit of the doubt in every scenario, simply due to hype. If a model is to be used for a specific task, it should be rigorously tested and benchmarked before replacing more thoroughly proven methods.

ChatGPT is a really promising chat bot and it can definitely seem knowledgeable about a wide range of topics, since it was trained on basically the entire internet, but I wouldn't trust it to do something that  a simple pandas query could accomplish. Nor would I use it to perform sentiment analysis when there are a million other transformer models that were specifically trained to predict sentiment labels and were rigorously evaluated on industry standard benchmarks (like GLUE).

https://preview.redd.it/sz3ejc1313ja1.png?width=1700&format=png&auto=webp&s=76c48939903f8d34810000d1a71cb25f86237130"
46,125fd6p,datascience,GPT,comments,2023-03-29 06:33:08,[D] Very good article about the current limitations of GPT-n models,fripperML,False,0.98,236,https://www.reddit.com/r/datascience/comments/125fd6p/d_very_good_article_about_the_current_limitations/,95,1680071588.0,"I count myself among the people that are amazed of what those models can do and how they can impact our society.

However, it's very important to understand that they are not magical solutions for every problem and that they cannot reason at all.

[ChatGPT as a query engine on a giant corpus of text ‚Äì r y x, r (ryxcommar.com)](https://ryxcommar.com/2023/03/28/chatgpt-as-a-query-engine-on-a-giant-corpus-of-text/)

What is more impressing is that, given this mental model of ChatGPT as a giant query engine, how can it perform activities that involve creativity outside of his training data? Like, for example, writing a poem in the style of Shakespeare about the proof that there are infinite prime numbers? Surely there are no examples of that in the training data! My answer would be that for some tasks interpolation works well (you can somehow get something meaningful by interpolating known stylistic elements and known semantical elements into something ""new""). But when the task is more symbolic or discrete, instead of interpolative, like true reasoning, and there are no examples to retrieve an answer from, the system has a much harder time.

That is, I am alligned with F. Chollet views on this:

[Fran√ßois Chollet en Twitter: ""You can retrieve not just what was seen at training time, but arbitrary combinations of it. It's an interpolative database and program store, with a natural language interface. https://t.co/2mv2gnI3oM"" / Twitter](https://twitter.com/fchollet/status/1637122108357738496)

[Fran√ßois Chollet en Twitter: ""This paper has the right idea: use symbolic logic for discrete reasoning and lean on deep learning models for perception and common-sense intuition. https://t.co/9lP8eDZKkO I expect to see a lot more progress along these lines in the coming months / years."" / Twitter](https://twitter.com/fchollet/status/1636838039703126016)"
47,10a7kq4,datascience,GPT,comments,2023-01-12 19:01:24,I wrote up a guide showing how to do Data Science with ChatGPT.,Own-Anteater4164,False,0.83,274,https://www.reddit.com/r/datascience/comments/10a7kq4/i_wrote_up_a_guide_showing_how_to_do_data_science/,93,1673550084.0,"Just recently, I wrote up a guide on how to use [ChatGPT to build a website with Replit](https://buildspace.so/notes/chatgpt-replit-website?utm_source=r).

Got some pretty good responses, so I decided to write + document more of the applications I'm discovering.

**I'm actually really excited about this one, since I was in a graduate program for statistics.**

[Here's the guide](https://buildspace.so/notes/chatgpt-data-science?utm_source=r) for doing data sci with ChatGPT

The tl;dr is that I show you some of the crazy data sci stuff ChatGPT can do:

\- Read and analyze raw CSV data. I just had to copy and paste.

\- It could tell what kind of data you're feeding it judging by the header columns!

\- It will give you the python/r code on how to run specific analysis.

\- It even knew how to use scikit-learn to run regression models ü§Ø (I mean, this makes sense since it's an AI tool lol).

Honestly, this is just crazy to me.

**Before I dropped out of graduate school for statistics, I often consulted non-technical researchers in the social sciences. It was always a pain for them to run datasets by themselves just to get some answers to their questions.**

Although ChatGPT isn't perfect (and does make mistakes), it's crazy where the tool is going.

I think this is really good news for a lot of people who are interested in doing research, but might feel too intimidated by needing to do stats. Obvi...some bad stuff could come from it. We'll see!

https://preview.redd.it/ggd96gyhnnba1.png?width=619&format=png&auto=webp&s=5aa2f39199bb0ce56518e2972e0ec8a36ccbb69d"
48,123tx9p,datascience,GPT,comments,2023-03-27 17:25:43,Has ChatGPT killed doomers?,GreatStats4ItsCost,False,0.89,442,https://www.reddit.com/r/datascience/comments/123tx9p/has_chatgpt_killed_doomers/,90,1679937943.0,"Sorry for another ChatGPT post but I think it really is the end of asking whether certain job sectors will exist on r/DataScience due to ChatGPT making them redundant.

Whilst reading all of the 100's of doomer posts 'Will Data Science survive because ChatGPT' - it dawned on me that Chat GPT can replace all of the users creating these posts. They've all been made redundant. A simple prompt to an AI like 'Write a profoundly dumb Reddit post asking if Chat GPT has made Data Science redundant' - will return exactly that. With a simple workflow/pipeline the response from the API can be posted directly to r/DataScience. 

This really is the future and I'm worried."
49,10eye8i,datascience,GPT,comments,2023-01-18 04:50:28,"I asked ChatGPT to explain ROC AUC, the level of collaboration is beyond my expectation",trantrikien239,False,0.9,475,https://master-data.science/assets/images/eli5ml-meth1.jpg,78,1674017428.0,
50,zcgpbp,datascience,GPT,comments,2022-12-04 18:05:07,What do you guys think of OpenAI‚Äôs ChatGPT?,Loud_Ad_6272,False,0.94,56,https://www.reddit.com/r/datascience/comments/zcgpbp/what_do_you_guys_think_of_openais_chatgpt/,77,1670177107.0,"As the title goes, what do you guys think of this and what effect do you think it would have on the discipline and field going forward?"
51,zpraee,datascience,GPT,comments,2022-12-19 13:15:11,The real reason ChatGPT was created,xdonvanx,False,0.94,739,https://i.redd.it/g5z2t4zeuu6a1.png,73,1671455711.0,
52,114qfkp,datascience,GPT,comments,2023-02-17 16:14:47,I am worried about the future of working as a Data Scientist in industry,Slumi,False,0.51,2,https://www.reddit.com/r/datascience/comments/114qfkp/i_am_worried_about_the_future_of_working_as_a/,71,1676650487.0,"I once watched a video where someone argued that Michael Jordan had ruined basketball. As a person whose only notions of basketball come from Space Jam, this surprised me. Wasn't the guy the best out there? How could someone who's best in his field ruin it? After hearing the argument out though, I began to understand what the video was really getting at: to them, Michael Jordan had ruined the field BECAUSE he was so good. A sport that was once full of diverse personalities and strategies then turned into multiple teams trying to produce their own Michael Jordan copycat with copycat moves and copycat strategies.

I'm not well versed enough in basketball to know how right or wrong that guy was. But this concept that something could ruin an entire field simply by being too good stuck with me. And all these years later, I consistently think about it while doing my job as a Data Scientist. And the more GPT-X and ChatGPT advance, the more this concept haunts me.

I've been working in this field for 5 years. During the first 4 years, the job was exciting. Every new problem required extensive research on my part: looking for papers, datasets, implementations, implementing my own stuff, experimenting, comparing the results... That was the biggest part of my job. Yeah I still had to do some software engineering stuff here and there, but I felt like I had a well defined and specialized role at my company. Even in the NLP field, the variety of technologies I had to work with was a lot of fun: sometimes I used RNNs, sometimes embedding based similarity functions, sometimes more classic approaches, and then towards the end it was transformer after transformer, but at least I still had to finetune them myself.

Over the past few months and years, this has started to change, however. GPT-1 and 2 were promising, but were more of a proof of concept than anything realistically usable. But with GPT3's and ChatGPT's latest performance, it feels like NLP is becoming more and more standardized. I wouldn't call it a solved field, far from it. But, sadly, unless you work for a big tech company,  I think the days of exploratory research-type work in the NLP field are over.

Even in academia, the latest NLP papers I've seen come out of prestigious universities went from actual engineering/mathematical advances to ""Look, if you ""engineer"" the prompt like this, it works better sometimes!""

And now, with the exploding popularity of ChatGPT, any random lambda would come to the conclusion that if you need to use AI, ChatGPT is the answer.

Now, some of you may already be thinking ""but hold on, with my finetuned transformer I get a 1% increase in accuracy over out-of-the-box GPT-3!"". The thing is: even if you can, it doesn't really matter. I really doubt the sales team of your company is gonna pay much attention to your pleas to give you 6 months to work on a product rather than 6 minutes. Because yes, that's the increase in productivity we're dealing with: What took me months a few years ago only takes minutes now. The range of problems I can apply NLP too has also increased, as out-of-the-box GPT-3 works well enough not to require a dataset for more ""general"" tasks. And the results are either comparable or even better.

In the span of a year, NLP went from my favorite ML field to one I never see myself working in again if given the choice. And while a lot of non NLP fields are, for now, untouched by the exploding popularity of the GPT family, I'm afraid that what happened to the NLP field will be replicated in other fields, and that in a few years, only super specialized issues won't have a solution somewhere in the form of a ridiculously big model accessible via an API key provided by a big tech company. And even if GPT3 was open source, the sheer size of it makes it close to impossible to realistically train for anything less than the biggest of tech companies out there. Even finetuning it would be a challenge.

I'm not saying the job of data scientist will disappear, in fact, I'd say it will be asked more than ever since boomers who don't understand what the title actually implies will want some in their company. But we won't be doing nearly as much actual data sciency stuff as we did in the past. Instead we'll have to become even bigger software engineer/data analyst/DevOps/project manager hybrids than we already were. The only exception is for people who either work in big tech companies or on very specialized problems that require an in-house model.

It's not that much of a problem for people who like the roles I listed above. But for people like me, whose enjoyment came out of the exploratory side of the job, I believe the future to be bleak."
53,12vl384,datascience,GPT,comments,2023-04-22 22:04:04,Found this on an analyst position job ad on LinkedIn. Do you think the shade is reasonable?,BiggusCinnamusRollus,False,0.98,253,https://i.redd.it/cxcftudodiva1.png,64,1682201044.0,
54,13ilm03,datascience,GPT,comments,2023-05-15 21:45:36,I investigated the Underground Economy of Glassdoor Reviews,ibsurvivors,False,0.99,1163,https://www.reddit.com/r/datascience/comments/13ilm03/i_investigated_the_underground_economy_of/,63,1684187136.0,"Online company reviews are high stakes.

Top reviews on sites like Glassdoor and Google can get thousands of impressions each month and are major drivers of brand perception.

Employers know this. And when I come across multiple 5 star reviews left with no cons, or a Pulitzer worthy essay from a former intern, I become suspicious.

These reviews start to resemble 30 under 30 lists: so artificially constructed that you begin to question their credibility in the first place.

The scrutiny around company reviews is well documented; some companies file lawsuits worth over a million dollars to reveal anonymous reviewers that complain about their jobs.

Whilst it's the flashy lawsuits that make the headlines, there also exists an underground economy of company reviews operating quietly every single day.

In this underground economy, some companies pay over $150 to freelancers to try and get a negative review removed. If they want ‚Äúbetter‚Äù results, they go to the plethora of Online Reputation Management services (ORMs) in the United States that can charge retainers worth thousands of dollars.

The supply of positive reviews exists too. My research led me to find companies, including a prominent Y-Combinator backed startup, that solicit fake positive reviews from online freelancers to improve their rating.

Many of these mercenary fake reviewers, often based in South East Asia, make a full time living doing this, netting over $2,000 per month.

Some of these run such sophisticated operations that they‚Äôve even created their own pricing tiers (e.g $35 per original review, $20 to post an already created review from an email address), a la SaaS offering.

Others operate on a contingency fee agreement model, where they only get paid if they‚Äôre able to take a negative review down.

The underground economy of company reviews is well and truly alive. And today we‚Äôre going to find out how it operates.

***Note***: For more content like this, [*subscribe*](https://www.careerfair.io/subscribe) *to my newsletter. In a couple of weeks, I'll be releasing my guide to writing a killer resume.*

**Adding reviews**

The barriers to entry for adding fake reviews are much lower than for getting reviews removed, so that‚Äôs where we‚Äôll start.

To write an employer review, all you really need is the ability to create an email address. For most sites, you don‚Äôt need any proof of employment (say like a company specific email address).

I went on a gig marketplace site and posted a pretty vague post related to wanting to find out more on how to improve a company‚Äôs online presence.

Within minutes of posting a gig, my inbox was flooded with proposals:

https://preview.redd.it/esx3904qa20b1.png?width=3064&format=png&auto=webp&s=2ff3a2f8528fee99aabb830f27ea71a7569ebb2e

After a bit of chatting, I narrowed the scope of their services and summarized their rates into the table below:

|Channel|Cost|Timeline|Model|
|:-|:-|:-|:-|
|Freelancer #1|$10 per review|Monthly|Unlimited|
|Freelancer #2|$35 per original review, $20 per already created review|Monthly|Unlimited|
|Freelancer #3|$25 per review|Monthly|Unlimited|
|Freelancer #4|$25 per review|Monthly|10 reviews|
|Freelancer #5|$20 per review|Monthly|Unlimited|
|Online Reputation Management Agency|$300 subscription|Monthly|8 reviews|

Let‚Äôs dive a bit deeper into the services that Freelancer #5 offered.

Freelancer #5 explained to me he had been writing reviews for one particular company for the past 4 months now. Each month he wrote them 10 reviews.

&#x200B;

https://preview.redd.it/n1ddox6cb20b1.png?width=2684&format=png&auto=webp&s=5c271d0eec4328cb78d7d2cb85dfffa3f9eb72f8

In another message, he tells me he‚Äôs offering the same services to 5 other companies. Doing some quick math:

5 companies x 10 reviews per company x $25 per review = $1,250 per month

Considering the average person in Pakistan earns $150 per month, that‚Äôs not bad change at all.

One of the companies that he‚Äôs offering his services to includes a Y-Combinator backed startup. I won‚Äôt name the company, but here‚Äôs what its average Glassdoor review rating distribution looks like:

https://preview.redd.it/2np5b6fdb20b1.png?width=2420&format=png&auto=webp&s=f8cafaa85453b0933a18eb5c30f931b3bb893c46

5 star reviews account for over 77% of the company‚Äôs total reviews. Obviously, no one is buying fake reviews that make them look bad.

But here‚Äôs the thing: freelancers are getting quite smart when it comes to writing reviews that don‚Äôt look too fishy. They tend to do this by spacing the reviews out (so that they don‚Äôt come in ‚Äúspikes‚Äù ‚Äì more on this later) and they also make sure that they‚Äôre not always leaving the ‚Äúcons‚Äù section blank.

Don‚Äôt get me wrong, if you come across this company‚Äôs reviews, it‚Äôd be pretty easy to tell they‚Äôre quite strange. In fact, I can‚Äôt even post some screenshots here because it‚Äôd give the company away immediately.

But it would be challenging to conclude that the above company is buying reviews just by analyzing review volume and distribution without actually reading some of the reviews.

The same company is also buying reviews on Google Reviews.

Sidenote: I got curious about how he‚Äôs been writing 50 reviews from 50 different emails per month. Would he actually create 50 different email addresses? And what about the IP address ‚Äì doesn‚Äôt Glassdoor flag multiple reviews from the same IP?

One of the freelancers answered my question:

&#x200B;

https://preview.redd.it/g4id2yqeb20b1.png?width=2572&format=png&auto=webp&s=c2a77fdea8834a6d90f02b8b3eb67b3a874f3df2

Moving on ‚Äì another company that seems to buy fake reviews seems to be having some more trouble. Approximately a month after a freelancer linked me to fake reviews he had written for this company, all five reviews that he had linked me to had been removed:

&#x200B;

https://preview.redd.it/99fdvcgfb20b1.png?width=3116&format=png&auto=webp&s=b7e244529fc62b5c824d925feb61fd2cc16cbfd5

Based on this [Glassdoor webinar](https://youtu.be/3iy0JWOS1gs) from 2018, ‚Äúif it is found that a user has created multiple email accounts to submit reviews, then ALL submissions from that user are deleted‚Äù ‚Äì so likely Glassdoor‚Äôs content moderation team flagged one of the initial reviews and the same freelancer who was writing reviews for that company had all the fake reviews deleted.

So far, it looks like the key to an effective fake review creation strategy lies in:

* Spacing the fake reviews out
* Writing each review from a different IP address (i.e benefit of being part of a team)
* Using language that isn‚Äôt an obvious giveaway

On that third point: the reality is that many of these freelancers‚Äô first language is not English.

As an experiment, I turned to everybody‚Äôs favorite new toy, ChatGPT, and asked it to write me a positive Glassdoor review:

https://preview.redd.it/8w7cal9gb20b1.png?width=3164&format=png&auto=webp&s=860c39b11c5813e8b7fabdbb038d73c565cc98cf

And I‚Äôd say that the above answer was better than 95% of the fake reviews I came across.

**Removing reviews**

The process for removing an employer review usually works like this:

1. You identify one or multiple reviews that you want removed
2. You verify whether the review violates the site's Guidelines, or whether there‚Äôs something else about the review(s) that could get it removed.
3. You file an appeal to get it removed.

As an example, Glassdoor‚Äôs Review guidelines can be found [here](https://help.glassdoor.com/s/article/Community-Guidelines?language=en_US#:~:text=See%20More-,Review%C2%A0Guidelines,-Millions%20of%20job). Mainly, they forbid mentioning anyone by name who‚Äôs not an executive and revealing proprietary or confidential information, amongst a host of other things.

Sounds simple enough right? Well, according to one of the freelancers I messaged:

&#x200B;

https://preview.redd.it/x6s8hsyac20b1.png?width=2036&format=png&auto=webp&s=f86c386f864198dc43faeb41faea378090c20107

After some research, I summarized the different vendors and prices in the table below:

&#x200B;

|Channel|Cost|Timeline|Model|Self reported success rate|
|:-|:-|:-|:-|:-|
|Freelancer #1|$100 per review|3 days|Contingency Agreement Model|100%|
|Freelancer #2|$30 per review|7 days|Contingency Agreement Model|100%|
|Reputation management service #2|$450 per review|21 business days|Contingency Agreement Model|Unknown|
|Reputation management service #3|$1000 per review|Undefined|Contingency Agreement Model|100%|
|Reputation management service #4 Plan 1|$550 per review|5-6 weeks|Contingency Agreement Model|50-75%|
|Reputation management service #4 Plan 2|$300 Subscription + $100 per each review removed|Monthly service|Subscription plan|50-75%|
|Freelancer #3|$20|Undefined|Pay regardless|Undefined|
|Freelancer #4|$500|Undefined|Contingency Agreement Model|Undefined|

As you can see, unlike the fake review generation market, the prices vary quite a bit for getting reviews removed.

At one end, you have freelancers on gig marketplaces that will attempt to remove a review for less than $100. And then on the other end, you have ORMs (Online Reputation Management Agencies) that have multiple employees and more comprehensive packages in place. The one constant seems to be that most companies operate on a contingency agreement model (i.e pay only if review gets removed).

**Analyzing reviews**

ReviewMeta is a site that analyzes Amazon reviews and tells you how many are legitimate. The creator of the site, Tommy Noonan, mentions in an [interview with NPR](https://www.npr.org/sections/money/2018/06/27/623990036/episode-850-the-fake-review-hunter) that the main giveaway that a product is soliciting fake reviews is:

* A large, suspicious flood of positive reviews at the exact same time. For example, a 3 day stretch of time constituting 30% of total reviews.
* Phrases and words that are constantly repeated, especially in the section with no cons
* Brand monogamists (only review products from one company)

Whilst the last two bullets are hard to track, the first can be used to analyze different companies‚Äô reviews and to check if there might be some funky business going on.

After a couple of days, I have the ability to track review volume and review ratings over time for any company that I specify:

https://preview.redd.it/ehcbw2oje20b1.png?width=1653&format=png&auto=webp&s=b448ff35eb9878fbb1686de2fa8cf031e4ed3e05

Let the games begin.

## Voluntary Response Bias

One of the biggest challenges that review platforms face is the Voluntary Response bias.

Research shows many of today‚Äôs most popular online review platforms (e.g Amazon) have a distribution of opinion that is highly polarized, with many extreme positive and/or negative reviews, and few moderate opinions.

Think about it: have you ever felt moderately satisfied at your job and thought to yourself, now would be a great time to leave a Glassdoor review? Probably not.

On the other hand, if you‚Äôve had a terrible experience or even just had one thing really flip you off, you might be quite likely to leave an angry review.

Consider when a company goes through layoffs. You‚Äôre going to have a flood of angry reviews coming your way and are likely going to experience a ‚Äúspike‚Äù in reviews.

**Note:** Just like the Wall Street Journal‚Äôs methodology described [here](https://archive.is/20201016094732/https://www.wsj.com/articles/companies-manipulate-glassdoor-by-inflating-rankings-and-pressuring-employees-11548171977#selection-3965.0-3968.0), I considered there to be a spike if the total number of reviews in the month was greater than three standard deviations above the mean of the surrounding months.

Let‚Äôs take the company below. Here‚Äôs a graph of of their review volume since Jan 2020, including when they announced one of their first round of layoffs in June 2022:

https://preview.redd.it/n6kd9ejle20b1.png?width=3216&format=png&auto=webp&s=9eea2f3836617feca37eb88b1d3f67c8fa1b6fe2

In June 2022, approximately 19% of this company's 52 reviews were 1 star reviews (compared to an overall average of around 10%). This is what we could call a statistically significant spike in reviews. It also illustrates how the employees most likely to leave reviews are the ones that obviously had a bad experience (i.e getting laid off).

Here‚Äôs another company that had a similar spike in negative reviews due to layoffs in November 2022:

https://preview.redd.it/4vcnr1ine20b1.png?width=2408&format=png&auto=webp&s=f3877fb315ccc5d9a9294306a9f86616cb0fabd2

This company had an approximate 20% 1 star review rate (compared to an overall average of 12%) in November 2022, as well as an Avg Rating of 2.96 that month (compared to an overall average rating of 3.73).Unless HR is proactive, their reviews page risks succumbing to an echochamber of negative reviews that can really tilt one way.

**Note:** Glassdoor does state (based on [this video](https://www.youtube.com/watch?v=3iy0JWOS1gs) from 2017) that about 75% of the reviews on their platform are neutral. Their ‚Äúgive to get policy‚Äù has helped in keeping the platform from becoming too polarized.

I can understand why HR teams, like the ones that Nader talked to me about earlier, take a proactive stance towards managing their reviews. If they don‚Äôt try to control their reputation themselves, then their reputation risks getting controlled by the employees that had the worst possible experience.

## Goodhart‚Äôs Law

Goodhart‚Äôs law states the following:

*""When a measure becomes a target, it ceases to be a good measure""*

Every October, Glassdoor publishes their Best Places To Work ranking.

In a [report](https://www.wsj.com/articles/companies-manipulate-glassdoor-by-inflating-rankings-and-pressuring-employees-11548171977) that the WSJ did a couple of years ago, they found large spikes in the number of reviews that some companies (e.g SpaceX, Bain & Co, etc) got in September. The logic here is that some companies try to artificially inflate their Glassdoor reviews right before the October deadline.

I decided to revisit some of this analysis with Glassdoor‚Äôs 2023 Best Places To Work Ranking.

One of the companies I examined is rated as one of the best places to work in 2023. Let‚Äôs refer to this company as FunPlaceToWork.

Here is how their review volume looks like for all of 2022:

https://preview.redd.it/4e656zkqe20b1.png?width=2516&format=png&auto=webp&s=07141a66c56be7a6818efb9b1a4d912ee0021c91

FunPlaceToWork got around 50 reviews in September 2022. Of those 50 reviews, 96% were 5 star reviews.

FunPlaceToWork averaged 12 reviews per month up till then in 2022. Also, in the prior six months, the average percent of 5 star reviews received every month was \~75%.

Both the spike in volume of reviews and the spike in percentage of five star reviews are statistically significant.

I find it strange that Glassdoor‚Äôs proprietary algorithm and/or Human Content Moderation team did not find a spike of this nature unusual. If we look at Glassdoor‚Äôs eligibility criteria for the award, it‚Äôs as follows:

https://preview.redd.it/hag04y7se20b1.png?width=2868&format=png&auto=webp&s=ec2b920e126a8ea42b40d35aaa55d5341e69d022

The goal, according to Glassdoor, is to collect ‚Äúauthentic and unbiased reviews‚Äù.

Whilst there‚Äôs nothing against the rules for asking your employees to leave you reviews, I find the statistically significant spike of reviews at odds with the goal of collecting ""unbiased and authentic"" reviews (which Glassdoor states is the purpose of the awards).

Glassdoor states that an employer is allowed to ask its employees to leave reviews, but that they are not allowed to ‚Äúcoerce‚Äù them. Examples of what you can‚Äôt do:

* Offer incentives like Gift Cards in exchange for positive reviews.
* Withholding their reference letter unless they leave you a positive review.
* Anything that leads you to require proof for the employee to show you that they wrote a review.

It is possible to play by the rules (i.e not break any of the above rules) and to still in my opinion not collect authentic and unbiased reviews.

They say that you shouldn‚Äôt hate the player but the game ‚Äì I think **FunPlaceToWork** played by the rules, won fair and square, and that this is simply a perfect example of Goodhart‚Äôs Law.

I reached out to Glassdoor ([awards@glassdoor.com](mailto:awards@glassdoor.com)) about the above and this is the reply I got:

https://preview.redd.it/x0dqq39ue20b1.png?width=4800&format=png&auto=webp&s=c0102c963be9486370b340f2f473cbc6650fc48a

**Conclusion**

When I was 22, on an [F1 visa with 3 months to find work](https://www.careerfair.io/job-hunt-story), I didn‚Äôt give a damn about bad reviews. I needed a job and I‚Äôd sign any piece of paper you put in front of me.

Compare that to someone at the peak of their career, someone with optionality and a multitude of job offers; an ‚ÄúA-Player‚Äù, as the experts call it, would absolutely have the luxury of choice and discard a job offer based on bad company reviews.

For most people, the impact of online company reviews lies somewhere in the middle. In marketing, there‚Äôs a concept of a ‚Äúmarketing touchpoint‚Äù - an interaction with the brand over the course of the whole buying journey.

Company reviews are one of the many touchpoints a job seeker experiences over their interview process. And with the technology industry booming the past couple of years, companies couldn‚Äôt afford to slack on any touchpoints, including this one.

After all, when others start to game the system, you‚Äôre at a disadvantage if you don‚Äôt. The rewards can be quite high. Certainly higher than just trying to be as transparent as possible.

HR leaders are often more incentivized to inflate their metrics than to get honest feedback. Fake review writers have bills to pay. ORMs know that companies are desperate. And the platforms, well, aren‚Äôt always paying attention.

The result is a potluck of interests that leads to an underground economy.

One that ends up hurting the job seeker.

\*\*\*

Whew. That took a while (about 3 months in fact). Thanks for reading. For more content like this, [subscribe](https://www.careerfair.io/subscribe) to my newsletter. It's my best content delivered to your inbox once every 2 weeks."
55,11l5jqm,datascience,GPT,comments,2023-03-07 17:31:31,"My AI tool to writes SQL queries for me now, so I don't have to. Thoughts?",slingshoota,False,0.89,122,https://www.reddit.com/r/datascience/comments/11l5jqm/my_ai_tool_to_writes_sql_queries_for_me_now_so_i/,60,1678210291.0,"I often write SQL queries for my Data Science job, but it can be really tedious and time-consuming. First I have to think about how to even approach the query, and then I have to google stuff to fix issues and refresh my memory.

I started using ChatGPT for help, but it was annoying to have to explain the tables/views every time.

To fix this, I built a tool that remembers your whole schema. It gives you a query to extract all the necessary info in one go and then you just copy-paste it once (it's saved with encryption). Then, all you have to do is write what you need in plain English, Ex. ""Users who have been online over 5 days this week"", and it writes the SQL query for you.

I showed it to my colleagues and they went crazy and are obsessed with it, as are my ex-colleagues from my last company.

What do you think? Would love to get your feedback. It's 100% free, you couldn't pay me even if you wanted to: www.blazesql.com"
56,10qecvo,datascience,GPT,comments,2023-01-31 23:43:50,Yann LeCun Hating on ChatGPT,MGeeeeeezy,False,0.56,4,https://www.reddit.com/r/datascience/comments/10qecvo/yann_lecun_hating_on_chatgpt/,35,1675208630.0,"Has anyone else noticed how much Yan LeCun has been hating on ChatGPT lately? I think it‚Äôs one thing to call out issues, but I have a feeling he‚Äôs trying to discredit it for business politics more than anything (Meta v. Microsoft)"
57,12sogo9,datascience,GPT,comments,2023-04-20 06:19:10,Will GPT and its friends lead to a Data Science winter (or have they already)?,AntiqueFigure6,False,0.06,0,https://www.reddit.com/r/datascience/comments/12sogo9/will_gpt_and_its_friends_lead_to_a_data_science/,38,1681971550.0,"Data science was the great hype a few years ago, but a lot of that hype has abated, particularly as businesses found that implementing models in a way that lead to increased profits was significantly more difficult that just making them.  
Now we have the GPT family, and the hype is more than 10x the hype around DS ever was. Whether or not the hype is justified or not, will that mean or has it already meant, that businesses searching for the next way to improve profitability will bypass DS and head straight to GPT, possibly closing DS programs in the process?

Hopefully clearer restatement- will businesses reduce investment in DS in order to invest in implementing GPT style AI solutions?"
58,11lt96v,datascience,GPT,comments,2023-03-08 10:51:40,[Q] Are you wary of using ChatGPT because of sensitive data concerns?,limedove,False,0.84,36,https://www.reddit.com/r/datascience/comments/11lt96v/q_are_you_wary_of_using_chatgpt_because_of/,37,1678272700.0,
59,122nwrk,datascience,GPT,comments,2023-03-26 14:31:39,I asked ChatGPT for the best random state value,liquidInkRocks,False,0.89,97,https://www.reddit.com/r/datascience/comments/122nwrk/i_asked_chatgpt_for_the_best_random_state_value/,36,1679841099.0,"This thing is like a student forced to write 1000 words on the topic. 

**me:**  What is a good random state value to use in general 

**Bot:**  

In machine learning, a random state is used to initialize the random number generator. The random number generator is used to shuffle the data, split the data into training and test sets, initialize the parameters of the model, and make other random decisions during training and evaluation.

Choosing a good random state value is important because it can affect the results of your model. If you use a different random state, you may get different results.

In general, there is no one ""best"" random state value to use. The value you choose depends on the specific problem you're working on and the specific data you're working with.

However, it's a good practice to set a random state to ensure reproducibility of your results. You can choose any integer value for the random state, but it's common to use numbers like 0, 42, or 2022.

It's also common to use cross-validation techniques like k-fold cross-validation, which helps to reduce the variability of your results due to different random state values."
60,117736x,datascience,GPT,comments,2023-02-20 13:47:54,Is NLP a sub-field with a lot of growth?,sonicking12,False,0.77,32,https://www.reddit.com/r/datascience/comments/117736x/is_nlp_a_subfield_with_a_lot_of_growth/,35,1676900874.0,I am thinking about how Amazon reportedly axed a lot of Alexa teams.  This seems to point to negative.  But ChatGPT is taking up a lot of interest. This seems to point to positive.   What are your thoughts?
61,zgoxwa,datascience,GPT,comments,2022-12-09 06:18:39,Gaussian Processes for pirates. Courtesy of ChatGPT,Sid__darthVader,False,0.98,501,https://i.redd.it/4cs2kirjwu4a1.jpg,34,1670566719.0,
62,zmye7g,datascience,GPT,comments,2022-12-15 22:34:22,Have you used ChatGPT to write code for you?,is_this_the_place,False,0.77,23,https://www.reddit.com/r/datascience/comments/zmye7g/have_you_used_chatgpt_to_write_code_for_you/,33,1671143662.0,Is so what did you ask and how did it go?
63,zwppsu,datascience,GPT,comments,2022-12-27 20:48:02,ChatGPT Extension for Jupyter Notebooks: Personal Code Assistant,Tieskeman,False,0.98,427,https://www.reddit.com/r/datascience/comments/zwppsu/chatgpt_extension_for_jupyter_notebooks_personal/,32,1672174082.0,"Hi!

I want to share a [browser extension](https://github.com/TiesdeKok/chat-gpt-jupyter-extension) that I have been working on. This extension is designed to help programmers get assistance with their code directly from within their Jupyter Notebooks, through ChatGPT.

The extension can help with code formatting (e.g., auto-comments), it can explain code snippets or errors, or you can use it to generate code based on your instructions. It's like having a personal code assistant right at your fingertips!

I find it boosts my coding productivity, and I hope you find it useful too. Give it a try, and let me know what you think!

You can find an early version here: 
https://github.com/TiesdeKok/chat-gpt-jupyter-extension"
64,108q84k,datascience,GPT,comments,2023-01-11 00:30:20,Future of of DS? Over saturated field?,alx1056,False,0.38,0,https://www.reddit.com/r/datascience/comments/108q84k/future_of_of_ds_over_saturated_field/,30,1673397020.0,"Hello all, I wanted to ask your opinion if you think DS is over saturated? I know DS is an umbrella term since it can be applied to various industries and academia but what direction do we see the field moving? It seems with all of these new ‚Äúbreakthroughs‚Äù like ChatGPT that we will have no need for as many humans to code and work through standard data related problems if computers can just learn to solve it themselves. Maybe a pessimistic view but Ive just started learning DS and really enjoy it. Just seeing how others feel in general if their will be a big need 10 years from now."
65,137fijm,datascience,GPT,comments,2023-05-04 09:33:40,Excuse me if this has been asked before but isn‚Äôt ChatGPT going to make this entire field redundant?,data_tryingtist,False,0.22,0,https://www.reddit.com/r/datascience/comments/137fijm/excuse_me_if_this_has_been_asked_before_but_isnt/,30,1683192820.0,"I‚Äôm halfway through a certification in Data Science, and after trying ChatGPT out for a week or two I am suddenly unable to convince myself that it‚Äôs worth becoming certified in this field when AI is going to take all of the jobs. 

Anyone have any thoughts on this? Should I even bother completing the cert?"
66,12avx58,datascience,GPT,comments,2023-04-03 20:01:07,Has ChatGPT/AI changed anything about your career as a data scientist?,NavidsonsCloset,False,0.54,2,https://www.reddit.com/r/datascience/comments/12avx58/has_chatgptai_changed_anything_about_your_career/,30,1680552067.0,
67,zvsc8h,datascience,GPT,comments,2022-12-26 17:43:47,Chat GPT taking over,BlackLotus8888,False,0.4,0,https://www.reddit.com/r/datascience/comments/zvsc8h/chat_gpt_taking_over/,29,1672076627.0,"Have y'all tried using chat GPT?  It's ridiculous!

1) I asked it to write a definition that takes in a df, imputes missing values, and returns a df and boom!

2) do t like the model it used, just ask it to use a different model

3) need cross validation?  No problem.  

4) need to get data using xx API?  You got it!  

It can do anything.  You just need to talk to it to get it right."
68,10m2qda,datascience,GPT,comments,2023-01-26 21:15:30,Will Data science be automated and replaced by AI,Mysterious-Idea-9087,False,0.25,0,https://www.reddit.com/r/datascience/comments/10m2qda/will_data_science_be_automated_and_replaced_by_ai/,28,1674767730.0,"I'm beginning to second-guess my decision to get an MSc in data science. Perhaps you're wondering why? I'm beginning to think that artificial intelligence will eventually replace the necessity for data science in this scenario because the majority of huge data can now be automated. In addition, I'd like to cite that ""Data Scientists' skill set will be rendered irrelevant in 12 to 18 months as technology progresses"" ( Pedro Uria-Recio ,2018) I was amazed and worried at the same time when I first started using ChatGpt, a lately popular platform. I wondered if artificial intelligence will eventually replace the majority of currently held occupations, which would eventually lead up to more unemployment. Hence, here i am, looking for your opinion, for whether should i continue my MSc in data science or A.I? 

&#x200B;

This ,is just my understanding and opinion. Please feel free to comment your viewpoints. Regards!"
69,114zcik,datascience,GPT,comments,2023-02-17 22:19:56,"What is something ChatGPT (or any LLM) could do, that it can‚Äôt currently, that would actually worry you about the future of data science?",cjrook,False,0.43,0,https://www.reddit.com/r/datascience/comments/114zcik/what_is_something_chatgpt_or_any_llm_could_do/,26,1676672396.0,"Lately on this sub there have been many ‚Äúsky is falling‚Äù posts related to ChatGPT. Most of the posts have drastically overestimated ChatGPT‚Äôs current use cases in the industry. What is a capability that if ChatGPT could do it, you would actually worry about the future of the data science field? More specifically worried about mass job loss within the field, if you foresee that."
70,138ckjn,datascience,GPT,comments,2023-05-05 06:07:33,The language in which GPT 3.5 communicates changes how it thinks.....,NoCartographer4725,False,0.64,12,https://www.reddit.com/r/datascience/comments/138ckjn/the_language_in_which_gpt_35_communicates_changes/,19,1683266853.0,"Seems like a new paper shows that GPT is more patient when talking in Mandarin and German vs when talking in English and Russian.

[https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=4437617](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4437617)

&#x200B;"
71,11skvpf,datascience,GPT,comments,2023-03-16 05:10:41,"When hiring, how would you react if a candidate data scientist used ChatGPT heavily throughout a technical interview/coding session, but did a great job communicating what they were doing and why?",MyNotWittyHandle,False,0.5,0,https://www.reddit.com/r/datascience/comments/11skvpf/when_hiring_how_would_you_react_if_a_candidate/,21,1678943441.0,"It is the stack-overflow of 2023 and beyond, and will only get better.

Would you penalize a candidate for using a resource like ChatGPT?  Specifically if it made them more efficient, and were able to solve more problems in the same amount of time as someone who used more traditional resources (stack-overflow, google, etc.)

EDIT: to clarify, I want to emphasize my point above where in this case, the candidate needs to be able to describe what they are doing, why it works, pros and cons vs other approaches, etc.  I‚Äôm also assuming if they have gotten to the point of an in-person coding technical interview, they have already passed prior interview steps where they have demonstrated foundational knowledge of the field.

Additionally, if you are in the role of hiring and you haven‚Äôt deeply probed the capacity of ChatGPT to write effective code given an appropriate prompt, I‚Äôd say that is step 1."
72,137xo27,datascience,GPT,comments,2023-05-04 19:42:30,"""The new ChatGPT Code Interpreter is like a Data Scientist on steroids""",datasciencepro,False,0.42,0,https://twitter.com/moritzkremb/status/1654107314528612355,23,1683229350.0,
73,130hqft,datascience,GPT,comments,2023-04-27 10:53:05,Low hanging fruit projects for business with non-mature data science/analytics?,BobzzYourUncle,False,0.89,33,https://www.reddit.com/r/datascience/comments/130hqft/low_hanging_fruit_projects_for_business_with/,23,1682592785.0,"Hey data legends,

I've just started to learn a bit of Python and it's got me going down the rabbit hole of possible business applications for data analysis/science in this small/medium business (B2B with typically only a couple of transactions per customer each year).  What's currently done is very basic stuff in excel and no machine learning etc. (I have no background in data science other than basic knowledge but I feel there is a lot of potential)

I've managed to automate a PDF report that has some basic stuff using Plotly and Pandas and am wondering where I should focus my efforts next.

What are the general low hanging fruits that I should try and start out with for a business that has very little maturity on this front?

Chat GPT has come back with some suggestions like: Customer segmentation, Churn analysis, sales forecasting, website optimisation, recommendation engines, predictive CLV.  

Any help or insights would be appreciated pointing me in the right direction.  Thanks"
74,126ndlu,datascience,GPT,comments,2023-03-30 13:41:51,Seeing a lot of job openings for high-level AI and Data Analytics positions...,fingin,False,0.78,25,https://www.reddit.com/r/datascience/comments/126ndlu/seeing_a_lot_of_job_openings_for_highlevel_ai_and/,20,1680183711.0,"I have noticed an uptick in jobs for things like prompt engineer, AI ethics lead, AI manager. When you look at these requirements it looks like relatively low entry: a familiarity with general AI and AI regulations (not that there is a ton of expertise to be had in this latter category). They don't require much or any technical skill. 

I'll admit, I find myself frustrated as I work in a highly technical role and feel like these opportunities are really 'low hanging fruit', due to the vagueness of the requirements. I'm sure many of us wear not just technical hats but also those of product management, coaching and training, etc. 

What do you think? Is it just a fad stemming from ChatGPT and Image gen promotion? Are you going to make a job switch and apply for these roles?"
75,13futqv,datascience,GPT,comments,2023-05-12 19:37:31,Will ChatGPT kills data science jobs?,Born-Comment3359,False,0.2,0,https://www.reddit.com/r/datascience/comments/13futqv/will_chatgpt_kills_data_science_jobs/,20,1683920251.0,
76,124p2uv,datascience,GPT,comments,2023-03-28 13:38:56,"How will companies go about integrating GPT into their ecosystem (databases, documents, websites, etc.)? Is this possible already or not yet?",KidzKlub,False,0.62,3,https://www.reddit.com/r/datascience/comments/124p2uv/how_will_companies_go_about_integrating_gpt_into/,20,1680010736.0,"I would love for GPT to have the context of our ecosystem and be able to ask it questions about our data, have it write code that works with the rest of our infrastructure, analyze documents that we have stored, and more. Would this involve training a bespoke model on a company's data? Will OpenAI offer enterprise solutions where they help set you up with a model that meets your needs? I'm curious for my own purposes, but also I think this would be a major way that companies will start using this technology in the near future."
77,10a1mik,datascience,GPT,comments,2023-01-12 14:59:04,New Research From Google Shines Light On The Future Of Language Models ‚≠ï,LesleyFair,False,0.84,119,https://www.reddit.com/r/datascience/comments/10a1mik/new_research_from_google_shines_light_on_the/,18,1673535544.0,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs will allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* Stop hallucinating and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example are words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/w7xffqjimmba1.png?width=800&format=png&auto=webp&s=e2d9cb63f750efcbfa45c4bb7a985d4dcb5b0319)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.  
At **TheDecoding** ‚≠ï, I send out a thoughtful newsletter about ML research and the data economy once a week.  
No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)"
78,123nr31,datascience,GPT,comments,2023-03-27 14:00:59,Is object recognition now a trivial task because of OpenAI?,throwitfaarawayy,False,0.18,0,https://www.reddit.com/r/datascience/comments/123nr31/is_object_recognition_now_a_trivial_task_because/,16,1679925659.0,"I'm working on a project where we are tasked with classifying different types of vehicles. I am thinking that now because of OpenAI models especially GPT-4 with vision, this is now a redundant effort. In a few weeks to months this will be available to everyone and for really cheap. Then why am I building this?"
79,11szca1,datascience,GPT,comments,2023-03-16 16:45:29,"What is your digital workspace, tools, setup, etc. for ETL, research, production?",BreathAether,False,0.91,33,https://www.reddit.com/r/datascience/comments/11szca1/what_is_your_digital_workspace_tools_setup_etc/,16,1678985129.0,"I'm new to this and so I've been wanting to know what other people have been using to make their work feel as smooth as butter. Since I've been learning lots and not just the industry standard stuff, I wanted to share what little I found to be valuable which others may want to try. **The main goal of this post is to share, critique, and provide suggestions so that we can all find the setup we like most. I  am also looking for new, up and coming tech, and definitely not afraid to try new things!**

IDE: **VSCode with the Jupyter Notebook Extension**. What I like about it is that I can view data structures like series/dataframes in a table format by clicking the variable in the Jupyter: Variables pane at the bottom. I started with plain vanilla jupyter notebooks from Anaconda so this was pretty nice. I have seen demos that **Jupyter Lab** has something like this, so if anyone has used both VSCode's notebooks and used Lab, your input would be appreciated. I hear good things about **PyCharm and Spyder**. Some people also use **Google Collab, DataSpell, and DeepNote** but I don't know enough about it. I did play around with DeepNote, and it was very cool but I didn't feel compelled to switch (and you have to pay for it!). 

Tools:

* A code helper: A few months back I was googling everything and I would've listed **Stackoverflow**. I might actually use that occasionally, but these days I use **ChatGPT** and **Bing AI**. For more current info or news-based I'll use Bing AI since it uses live search results, and for information that is knowledge based I might use ChatGPT. ChatGPT saves conversations so it's great for exploring topics in depth and referencing that conversation later. For those who have used both, maybe you know what I'm talking about and can provide a better explanation as to which is better for what purpose.
* Software: **Excel** is an obvious one. For instance, if I have a huge dataset and I just want to delete out columns that I don't need with Ctrl+click to select, it's easier and quicker than copy + pasting or typing out each of the string column names I want to ""df.drop()"". Excel is great for quick and simple stuff. Some software I have been learning about are I guess what I would consider as no- or low-code data analytics platforms, such as **Alteryx, KNIME, and Orange**. These software let you practically run an entire ETL pipeline. I believe Alteryx and KNIME are the gold-standard in this category, and Orange is a ""lite"" version of the two and is available in Anaconda. I think these are pretty cool, and I personally haven't found a huge use case for them since I've been chugging away in my notebooks with Python, but I can see the value. Would love for someone to chime in on these tools and how they compare to manually doing stuff in code, especially for large datasets.
* Version Control: This is where I'm primarily lacking, but I know that **Github** is the go-to. I don't use this but I know that a ton of people do. I don't even know where to start to be honest. I usually just create a new .ipynb file for each analysis or phase of an ETL pipeline haha. I'm also not too aware of what other innovative tools for version control exist.
* Python Libraries: Besides the obvious stuff like Pandas/NumPy, MatplotLib/Seaborn, and your popular ML libraries, I've recently found out about this library called **Polars**. It's basically a Rust version of Pandas, and it's super powerful. Some operations that I've run, that would've taken hours with Pandas, took me minutes. But I've been hearing that **Pandas 2.0** which will be released some time this month, has been looking at using PyArrow dtypes (if I recall correctly) and the speed is comparable to Polars. I mean these two are FAST. Another contender is **DuckDB** but I think the new Pandas and Polars are still faster. I mostly use Pandas but if there is some heavy lifting, I'll swap the dataframe to a polars one with a quick function, run it with polars, then back to pandas.

Anyway, that's just some things I can immediately think of. Looking forward to your suggestions! Bonus points for anything new and innovative. Cheers.

https://preview.redd.it/qj2cywt1r4oa1.png?width=1920&format=png&auto=webp&s=c4e02f5fec0b3768df336c7a3f63cc382b3954a8"
80,10fw1a3,datascience,GPT,comments,2023-01-19 07:54:24,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.94,119,https://www.reddit.com/r/datascience/comments/10fw1a3/gpt4_will_be_500x_smaller_than_people_think_here/,14,1674114864.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/t6m0epzlgyca1.png?width=575&format=png&auto=webp&s=a5972941053f833e76fd3a5009fc68a61f9e5406)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ‚≠ï, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
81,10iue4e,datascience,GPT,comments,2023-01-22 21:12:11,"I wrote a tiny library this morning for openai's (suddenly really good, really cheap) embeddings to search stuff - Semantic search, smarter replies w/ GPT-3, easier.",morganpartee,False,0.79,13,https://www.sensibledefaults.io/blog/chatgpt/easy-python-embeddings,14,1674421931.0,"I've had a few buddies in the community ask about something like this in the last week, and then a reader did today so I finally took the time to write it up. It's basically a generic interface to try embeddings, and it's less than a hundred lines of code. Built this so you can just authenticate and test without having to fuss with numpy, pinecone, etc.

The only thing you do is put dictionaries with a name and text key, then search your embeddings instance. We return anything you stick in there with it too for your programming pleasure - think dates, page numbers, file names, whatever.

You can just... Build chat bots that are pretty smart like this for next to nothing by reading in your docs or whatever. It's cheap and incredibly effective. I know next to nothing about how to preprocess data for this thing, I've tried different sizes of chunks, lines, sentences and haven't found a one size fits all solution (other than more gpt to summarize) but - just try it, it's cheap as hell and there's so much you can do. Like the rest of the current state of gpt - it just kinda works. Start with one to five sentences (or lines for code) and experiment!

For you proper data scientists (nerds!) - you can just get the numpy array out and do regular embeddings stuff with it. If you do, I'd love to hear about it! Clustering, classification, etc are suddenly super easy, and the raw data is stored in json so you can upload it to something better when you're ready.

I know it's small, but little abstractions make life better. Anything we can do to lower the barrier to experimentation is worth doing. Hope it helps someone!"
82,1360alu,datascience,GPT,comments,2023-05-02 21:06:32,How busy are you? Under/over utilization,MorningDarkMountain,False,0.83,16,https://www.reddit.com/r/datascience/comments/1360alu/how_busy_are_you_underover_utilization/,13,1683061592.0,"As simple as the title: I hear more and more often about Data Scientists feeling themselves to be under-utilized. Lack of projects, no real impact on the business, fear of being replaced by ChatGPT, stakeholders that do not really understand Data Science at all.

Are you currently under-utilized as well?"
83,135uvm8,datascience,GPT,comments,2023-05-02 17:42:08,Why won‚Äôt AutoGPT take our jobs?,dataentryadmin,False,0.16,0,https://www.reddit.com/r/datascience/comments/135uvm8/why_wont_autogpt_take_our_jobs/,12,1683049328.0,"https://autogpt.net/auto-gpt-vs-chatgpt-how-do-they-differ-and-everything-you-need-to-know/

We are within years of AI handling the entirety of database management, pipelines, transformations, dash-boarding and analysis.

AutoGPT can write and recursively correct/improve its code. 

You convinced me in my last post that data would be the last industry to go, but to me it‚Äôs now looking to be the least safe. 

Data is relatively simple to a machine, and there is less nuance or requirement for a human-touch that software engineering might have (when building software for human application)."
84,118ybon,datascience,GPT,comments,2023-02-22 13:15:21,Overview of state-of-art algorithms in their respective usecase?,Different_Day_3821,False,0.81,10,https://www.reddit.com/r/datascience/comments/118ybon/overview_of_stateofart_algorithms_in_their/,12,1677071721.0,"Fellow scientists... On the second year of data science i've become increasingly confused as to when, what and why to use certain algorithms... Specially in a field as rapidly developing as ours.

therefore i ask you if any of you got an overview of what the current best algorithms is according to their uses? Is there a systemisc ""cheat-sheet"" or overview of the current best libraries to import for X types of data and analysis?

For example learning NLP. First you learn Bag of Word models, then IDF-TF models. Then you learn thats outdated and people use Word2Vec models that retain semantic meaning. Then you learn thats outdated and that people use Bert models. But now even that in competition with transformer-based models as ChatGPT is based on etc.

Same thing with time series. First you like ARIMA, then you learn RNN, then LSTM, then Autoencoders, then to use autoencoders as unsupervised feature extraction into a supervised model etc.

And the more complicated you modern you get, the more specific becomes that imported libraries. Anomaly detection? Go PyOD/Prophet/PySAD etc.

How do you guys navigate in this? Is there some conversative principles that would keep your methods within some kind of boundary of usefullnes? I miss the old days of only knowing basic sklearn and basic naive bayes and clustering algorithms lol. On the otherhand if state of the art just means importing and fitting the right algorithm then i guess it ain't so bad.

tl;dr: Is there a cheat-sheet of X-kind of analsis then Y-kind of algorithm performs best? And how do you guys manage all the libraries/algorithms?"
85,13fvzkv,datascience,GPT,comments,2023-05-12 20:23:46,ChatGPT code interpreter is crazy! What next?,__ped,False,0.4,0,https://www.reddit.com/r/datascience/comments/13fvzkv/chatgpt_code_interpreter_is_crazy_what_next/,12,1683923026.0,"Hey folks. I was watching a couple of videos about code interpreter plug-in for chatGPT which will he available next week for plus users.

This plug-in let's you upload text/image/gif/... files to it and then will take action on them based on what you request. As an example you can upload a CSV file and ask for some visualizations, or even clustering using K-Means. There could be more to it, but I don't have my hands on yet.

I know how generative AI has been on a fast pace forward in the past couple of months, but this one actually hit me harder. To a sense that I am questioning my knowledge and usefulness for the near future. I am genuinely clueless of how and what to train myself on for the next couple of years, in order to stay useful and creative.

If you have also thought about this and have some possible ideas or thoughts about this topic, please go on and write it down. Maybe it helps me stop freaking out."
86,13gsrkv,datascience,GPT,comments,2023-05-13 21:06:58,Relying too much on ChatGPT to learn DS programming?,PhisheadS1,False,0.44,0,https://www.reddit.com/r/datascience/comments/13gsrkv/relying_too_much_on_chatgpt_to_learn_ds/,11,1684012018.0,"Hello, I've been learning DS the past few months and I really enjoy it but I find the programming part (Python) quite difficult as I'm coming from a non-DS career (though I had learned SQL pretty well). 

So while doing projects I basically us ChatGPT to tell what code to write for what I want. For example I say ""For each value in the Neighborhood column, get the median home price and rank them so I can assign a numeric value instead of categorical...and while you're add it include the code to do this to the test df""...and voil√† (of course many times I will need to correct it/tinker with the code/or re-explain my request. 

So am I making a big mistake? I mean I know I'll have to learn eventually but I kinda feel like it's like teaching a child to use a calculator without teaching him to do arithmetic. Or is this just going to be the future of learning to program?"
87,121tjz7,datascience,GPT,comments,2023-03-25 17:53:22,"GPT-4 can solve most SQL interview questions. In 5 years, do you think Acing a SQL Interview will still be important?",NickSinghTechCareers,False,0.36,0,/r/SQL/comments/121q7nt/gpt4_can_solve_most_sql_interview_questions_in_5/,11,1679766802.0,
88,10dfbc7,datascience,GPT,comments,2023-01-16 14:02:26,Are you using ChatGPT for work?,Conscious-Rush-9646,False,0.64,3,https://www.reddit.com/r/datascience/comments/10dfbc7/are_you_using_chatgpt_for_work/,11,1673877746.0,I started using it just for some silly things like describe what this code does. I'm curios to see if anyone is actually using it on a daily basis and it's making a difference.
89,13c8ewn,datascience,GPT,comments,2023-05-08 23:21:30,I asked ChatGPT4 to do some stats modelling - it was okay‚Ä¶ish,AFL_gains,False,0.67,6,https://www.reddit.com/r/datascience/comments/13c8ewn/i_asked_chatgpt4_to_do_some_stats_modelling_it/,11,1683588090.0,"Hi guys ! There‚Äôs been some debate, especially on here, about the ‚Äúfuture of data science‚Äù and ‚Äúwhose job is going to be taken‚Äù etc etc. Imo I don‚Äôt know the answer, but I think LLMs have definitely changed the landscape.

One of the really interesting things ChatGPT has unlocked is that people can now code without really knowing how to. I think if you already are familiar with coding, using ChatGPT to improve productivity is awesome. But if you‚Äôre just starting out and use it generate code you can‚Äôt explain, then I think you can get into lots of trouble. 

And I think this is especially true when there‚Äôs a mathematical modelling choice aspect to your code. My thought was that just because something works / compiles, doesn‚Äôt mean it‚Äôs a very good model and doesn‚Äôt mean that the explicit choices / assumptions make sense. This, of course, isn‚Äôt chatGPTs fault, it‚Äôs the users fault for not checking! 

Anyway, to investigate this point, I recently tested ChatGPT to write a Stan code (bayesian inference ) to predict premier league matches. My feeling was that the task simple enough for it to do an okay job, but not so generic it there‚Äôs a million examples on the internet.

I put the results on YouTube (link below), but in summary I found the following: 

1. ChatGPT made a decent model, but with some really weird choices. Eg It decided to use a normal distribution to model goal differences , where I think a Skellam would have been better. It also decided not to model the variance of this distribution , instead deciding that it was 1. Super weird!

2. It wasn‚Äôt able to rationalise about things like over parameterisation. The model it build had way too many parameters, unnecessarily. The idea of parsimony wasn‚Äôt really there. Maybe with better prompts it would have, but out of the box it made the model overly complex


3. Prompt engineering really makes a difference. I think with better prompts, the model Could have been better. There was even a point where I spotted an error and prompted chatGPT to fix it and it did! But again, this all relied on me being able to read Stan code and know what was good and bad. 


For me, I learnt that at least for tasks where lots of modelling choices need to be made, humans still beat GPT. But perhaps in the future, those that win will be the data scientists/ engineers that know what they are doing but are able to prompt GPT optimally to maximise their productivity boost.



The videos are here : 
Part 1: https://m.youtube.com/watch?v=4LTUYTxKuIk&t=66s&pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D
Part 2: https://m.youtube.com/watch?v=XjQpV6c9K5g&t=1s&pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D"
90,12m27p4,datascience,GPT,comments,2023-04-14 16:08:04,Public Bloom Instance?,fokke2508,False,0.86,32,https://www.reddit.com/r/datascience/comments/12m27p4/public_bloom_instance/,10,1681488484.0,"I was looking into self-hosting Bloom as an alternative to GPT. Besides concerns about the context window being too small and the overall quality, I do really like it from a privacy and availability perspective.   


But a production machine running it would cost about 280K per year. I am contemplating setting this up as a shared resource and making it publicly available as an alternative to GPT. Would anyone be interested in that?"
91,11innm2,datascience,GPT,comments,2023-03-05 04:37:52,Data Science Project Help: Future Oil Purchase based on S&P500 of different countries,_CynicalCyanide,False,0.67,1,https://www.reddit.com/r/datascience/comments/11innm2/data_science_project_help_future_oil_purchase/,10,1677991072.0,"So I need to do a Data Science project where I look at the price of Oil over the last fifty years and look at the data of the Index (S&P500 and equivalent ) and predict prices of oil based on that. 

How will this data influence purchasing decisions of big companies? 

Does that make sense? I need someone to explain how to go about this, if possible. I would appreciate a roadmap very much. 

I may be completely off here but I was thinking of taking the price datapoints from the last fifty years and then using a prediction algorithm- regression. 
Correlating the Oil prices and the S&P500 value or whatever. 

I have four weeks. Is it doable? 
Can you link me to resources that might help? I jus need some clarity on how to do this. How can I ask ChatGPT to help?"
92,zejzzs,datascience,GPT,relevance,2022-12-06 22:21:47,Chat_GPT,WeirdDiscipline1862,False,0.92,269,https://www.reddit.com/r/datascience/comments/zejzzs/chat_gpt/,136,1670365307.0,"This weekend millions of people rushed to check the Chat_GPT. This fueled many discussions regarding the job security of the future. People like Paul Krugman started talking about the future of job and massive job loss as the result of the AI which will be disruptive of course. And this time unless previously that the job loss was happening in the low skilled job categories, it will happen to the skilled workers. Any thoughts about what to do and how to persuade a new job specially after knowing that data analysis related jobs will be very vulnerable to AI technologies. 

‚ÄúIt's true that as AI and machine learning technologies continue to advance, they are likely to have an impact on many different fields, including data science. However, it's important to remember that while AI may automate some tasks and make certain job roles obsolete, it is also likely to create new job opportunities in areas such as AI research, development, and implementation.

In terms of what job you should pursue in the future, it's difficult to say for certain. The best thing to do is to stay up-to-date on the latest developments in AI and machine learning, and consider pursuing education and training in these areas. This will give you the skills and knowledge you need to adapt to the changing job market and take advantage of the new opportunities that are likely to arise.

It's also important to remember that there will always be a need for human expertise and creativity in many fields, including data science. So, even as AI continues to advance, there will likely still be plenty of opportunities for skilled data scientists who are able to think critically, solve complex problems, and apply their expertise to new challenges.‚Äù


This is the Chat_GPT‚Äôs answer to what to do as data scientist question. üòÄ"
93,123tx9p,datascience,GPT,relevance,2023-03-27 17:25:43,Has ChatGPT killed doomers?,GreatStats4ItsCost,False,0.89,440,https://www.reddit.com/r/datascience/comments/123tx9p/has_chatgpt_killed_doomers/,90,1679937943.0,"Sorry for another ChatGPT post but I think it really is the end of asking whether certain job sectors will exist on r/DataScience due to ChatGPT making them redundant.

Whilst reading all of the 100's of doomer posts 'Will Data Science survive because ChatGPT' - it dawned on me that Chat GPT can replace all of the users creating these posts. They've all been made redundant. A simple prompt to an AI like 'Write a profoundly dumb Reddit post asking if Chat GPT has made Data Science redundant' - will return exactly that. With a simple workflow/pipeline the response from the API can be posted directly to r/DataScience. 

This really is the future and I'm worried."
94,zpraee,datascience,GPT,relevance,2022-12-19 13:15:11,The real reason ChatGPT was created,xdonvanx,False,0.94,741,https://i.redd.it/g5z2t4zeuu6a1.png,73,1671455711.0,
95,10k528k,datascience,GPT,relevance,2023-01-24 13:07:12,ChatGPT got 50% more marks on data science assignment than me. What‚Äôs next?,rifat_monzur,False,0.92,506,https://www.reddit.com/r/datascience/comments/10k528k/chatgpt_got_50_more_marks_on_data_science/,208,1674565632.0,"For context, in my data science master course, one of my classmate submit his assignment report using chatgpt and got almost 80%. Though, my report wasn‚Äôt the best, still bit sad, isn‚Äôt it?"
96,125fd6p,datascience,GPT,relevance,2023-03-29 06:33:08,[D] Very good article about the current limitations of GPT-n models,fripperML,False,0.98,232,https://www.reddit.com/r/datascience/comments/125fd6p/d_very_good_article_about_the_current_limitations/,95,1680071588.0,"I count myself among the people that are amazed of what those models can do and how they can impact our society.

However, it's very important to understand that they are not magical solutions for every problem and that they cannot reason at all.

[ChatGPT as a query engine on a giant corpus of text ‚Äì r y x, r (ryxcommar.com)](https://ryxcommar.com/2023/03/28/chatgpt-as-a-query-engine-on-a-giant-corpus-of-text/)

What is more impressing is that, given this mental model of ChatGPT as a giant query engine, how can it perform activities that involve creativity outside of his training data? Like, for example, writing a poem in the style of Shakespeare about the proof that there are infinite prime numbers? Surely there are no examples of that in the training data! My answer would be that for some tasks interpolation works well (you can somehow get something meaningful by interpolating known stylistic elements and known semantical elements into something ""new""). But when the task is more symbolic or discrete, instead of interpolative, like true reasoning, and there are no examples to retrieve an answer from, the system has a much harder time.

That is, I am alligned with F. Chollet views on this:

[Fran√ßois Chollet en Twitter: ""You can retrieve not just what was seen at training time, but arbitrary combinations of it. It's an interpolative database and program store, with a natural language interface. https://t.co/2mv2gnI3oM"" / Twitter](https://twitter.com/fchollet/status/1637122108357738496)

[Fran√ßois Chollet en Twitter: ""This paper has the right idea: use symbolic logic for discrete reasoning and lean on deep learning models for perception and common-sense intuition. https://t.co/9lP8eDZKkO I expect to see a lot more progress along these lines in the coming months / years."" / Twitter](https://twitter.com/fchollet/status/1636838039703126016)"
97,zgoxwa,datascience,GPT,relevance,2022-12-09 06:18:39,Gaussian Processes for pirates. Courtesy of ChatGPT,Sid__darthVader,False,0.98,498,https://i.redd.it/4cs2kirjwu4a1.jpg,34,1670566719.0,
98,zwppsu,datascience,GPT,relevance,2022-12-27 20:48:02,ChatGPT Extension for Jupyter Notebooks: Personal Code Assistant,Tieskeman,False,0.98,425,https://www.reddit.com/r/datascience/comments/zwppsu/chatgpt_extension_for_jupyter_notebooks_personal/,32,1672174082.0,"Hi!

I want to share a [browser extension](https://github.com/TiesdeKok/chat-gpt-jupyter-extension) that I have been working on. This extension is designed to help programmers get assistance with their code directly from within their Jupyter Notebooks, through ChatGPT.

The extension can help with code formatting (e.g., auto-comments), it can explain code snippets or errors, or you can use it to generate code based on your instructions. It's like having a personal code assistant right at your fingertips!

I find it boosts my coding productivity, and I hope you find it useful too. Give it a try, and let me know what you think!

You can find an early version here: 
https://github.com/TiesdeKok/chat-gpt-jupyter-extension"
99,10eye8i,datascience,GPT,relevance,2023-01-18 04:50:28,"I asked ChatGPT to explain ROC AUC, the level of collaboration is beyond my expectation",trantrikien239,False,0.9,478,https://master-data.science/assets/images/eli5ml-meth1.jpg,78,1674017428.0,
100,10a7kq4,datascience,GPT,relevance,2023-01-12 19:01:24,I wrote up a guide showing how to do Data Science with ChatGPT.,Own-Anteater4164,False,0.83,276,https://www.reddit.com/r/datascience/comments/10a7kq4/i_wrote_up_a_guide_showing_how_to_do_data_science/,93,1673550084.0,"Just recently, I wrote up a guide on how to use [ChatGPT to build a website with Replit](https://buildspace.so/notes/chatgpt-replit-website?utm_source=r).

Got some pretty good responses, so I decided to write + document more of the applications I'm discovering.

**I'm actually really excited about this one, since I was in a graduate program for statistics.**

[Here's the guide](https://buildspace.so/notes/chatgpt-data-science?utm_source=r) for doing data sci with ChatGPT

The tl;dr is that I show you some of the crazy data sci stuff ChatGPT can do:

\- Read and analyze raw CSV data. I just had to copy and paste.

\- It could tell what kind of data you're feeding it judging by the header columns!

\- It will give you the python/r code on how to run specific analysis.

\- It even knew how to use scikit-learn to run regression models ü§Ø (I mean, this makes sense since it's an AI tool lol).

Honestly, this is just crazy to me.

**Before I dropped out of graduate school for statistics, I often consulted non-technical researchers in the social sciences. It was always a pain for them to run datasets by themselves just to get some answers to their questions.**

Although ChatGPT isn't perfect (and does make mistakes), it's crazy where the tool is going.

I think this is really good news for a lot of people who are interested in doing research, but might feel too intimidated by needing to do stats. Obvi...some bad stuff could come from it. We'll see!

https://preview.redd.it/ggd96gyhnnba1.png?width=619&format=png&auto=webp&s=5aa2f39199bb0ce56518e2972e0ec8a36ccbb69d"
101,zcgpbp,datascience,GPT,relevance,2022-12-04 18:05:07,What do you guys think of OpenAI‚Äôs ChatGPT?,Loud_Ad_6272,False,0.94,54,https://www.reddit.com/r/datascience/comments/zcgpbp/what_do_you_guys_think_of_openais_chatgpt/,77,1670177107.0,"As the title goes, what do you guys think of this and what effect do you think it would have on the discipline and field going forward?"
102,zvsc8h,datascience,GPT,relevance,2022-12-26 17:43:47,Chat GPT taking over,BlackLotus8888,False,0.4,0,https://www.reddit.com/r/datascience/comments/zvsc8h/chat_gpt_taking_over/,29,1672076627.0,"Have y'all tried using chat GPT?  It's ridiculous!

1) I asked it to write a definition that takes in a df, imputes missing values, and returns a df and boom!

2) do t like the model it used, just ask it to use a different model

3) need cross validation?  No problem.  

4) need to get data using xx API?  You got it!  

It can do anything.  You just need to talk to it to get it right."
103,122nwrk,datascience,GPT,relevance,2023-03-26 14:31:39,I asked ChatGPT for the best random state value,liquidInkRocks,False,0.89,96,https://www.reddit.com/r/datascience/comments/122nwrk/i_asked_chatgpt_for_the_best_random_state_value/,36,1679841099.0,"This thing is like a student forced to write 1000 words on the topic. 

**me:**  What is a good random state value to use in general 

**Bot:**  

In machine learning, a random state is used to initialize the random number generator. The random number generator is used to shuffle the data, split the data into training and test sets, initialize the parameters of the model, and make other random decisions during training and evaluation.

Choosing a good random state value is important because it can affect the results of your model. If you use a different random state, you may get different results.

In general, there is no one ""best"" random state value to use. The value you choose depends on the specific problem you're working on and the specific data you're working with.

However, it's a good practice to set a random state to ensure reproducibility of your results. You can choose any integer value for the random state, but it's common to use numbers like 0, 42, or 2022.

It's also common to use cross-validation techniques like k-fold cross-validation, which helps to reduce the variability of your results due to different random state values."
104,10qecvo,datascience,GPT,relevance,2023-01-31 23:43:50,Yann LeCun Hating on ChatGPT,MGeeeeeezy,False,0.57,5,https://www.reddit.com/r/datascience/comments/10qecvo/yann_lecun_hating_on_chatgpt/,35,1675208630.0,"Has anyone else noticed how much Yan LeCun has been hating on ChatGPT lately? I think it‚Äôs one thing to call out issues, but I have a feeling he‚Äôs trying to discredit it for business politics more than anything (Meta v. Microsoft)"
105,11yh9t1,datascience,GPT,relevance,2023-03-22 12:16:40,New version of SmartyGPT with ChatGPT and GPT4!,usc-ur,False,0.29,0,https://www.reddit.com/r/datascience/comments/11yh9t1/new_version_of_smartygpt_with_chatgpt_and_gpt4/,0,1679487400.0,[https://github.com/citiususc/Smarty-GPT](https://github.com/citiususc/Smarty-GPT)
106,13futqv,datascience,GPT,relevance,2023-05-12 19:37:31,Will ChatGPT kills data science jobs?,Born-Comment3359,False,0.18,0,https://www.reddit.com/r/datascience/comments/13futqv/will_chatgpt_kills_data_science_jobs/,20,1683920251.0,
107,11sp0yn,datascience,GPT,relevance,2023-03-16 09:21:40,Smarty-GPT: library of prompts/contexts (connected with Awesome Prompts Chat GPT),usc-ur,False,0.57,1,https://www.reddit.com/r/datascience/comments/11sp0yn/smartygpt_library_of_promptscontexts_connected/,1,1678958500.0,"This is a simple wrapper that introduces any imaginable complex context to each question submitted to Open AI API. The main goal is to enhance the accuracy obtained in its answers in a **TRANSPARENT** way to end users.

[https://github.com/citiususc/Smarty-GPT](https://github.com/citiususc/Smarty-GPT)"
108,13fu3mh,datascience,GPT,relevance,2023-05-12 19:08:42,ChatGPT In Financial Economics?,nkafr,False,0.5,0,https://www.reddit.com/r/datascience/comments/13fu3mh/chatgpt_in_financial_economics/,6,1683918522.0,"I found 2 new research papers that explore Language Models in Financial Economics.

* The first one \[Hansen et al\] shows that LLMs like ChatGPT can decode **Fedspeak** better than humans financial experts and BERT-based models.
* The second one \[Lopez-Lira et al\] uses ChatGPT to formulate trading strategies based on sentiment analysis.

I am not an economics expert, but I think both of them have some limitations. For example, the second one does not consider trading costs. What do you think?

I discuss them [here](https://medium.datadriveninvestor.com/using-chatgpt-to-decode-stock-price-movements-an-academic-survey-b1b6cf2bbf0b?sk=af3bc35f39032c0e26a908d88987916f)"
109,zy1brw,datascience,GPT,relevance,2022-12-29 09:38:58,Adding Chat GPT to RStudio with the GPT Studio add-in package,DrLyndonWalker,False,0.73,5,https://www.reddit.com/r/datascience/comments/zy1brw/adding_chat_gpt_to_rstudio_with_the_gpt_studio/,0,1672306738.0,"You can now add the incredible functionality of Chat GPT (including writing text and code) to RStudio using the GPT Studio package. In this video I cover the steps for setting up GPT Studio and then try some basic operations including spell correction and code writing. As I demonstrated in my recent RTutor video, Chat GPT is a game-changer. It can write code and text with a scary degree of precision. 

[https://youtu.be/QQfDTLExoNU](https://youtu.be/QQfDTLExoNU)"
110,11lt96v,datascience,GPT,relevance,2023-03-08 10:51:40,[Q] Are you wary of using ChatGPT because of sensitive data concerns?,limedove,False,0.86,38,https://www.reddit.com/r/datascience/comments/11lt96v/q_are_you_wary_of_using_chatgpt_because_of/,37,1678272700.0,
111,zmye7g,datascience,GPT,relevance,2022-12-15 22:34:22,Have you used ChatGPT to write code for you?,is_this_the_place,False,0.79,25,https://www.reddit.com/r/datascience/comments/zmye7g/have_you_used_chatgpt_to_write_code_for_you/,33,1671143662.0,Is so what did you ask and how did it go?
112,126id1o,datascience,GPT,relevance,2023-03-30 10:10:02,Save all your conversations via scraping with python and chatGPT Frontend API (no apikey needed) [ GitHub - rodolflying/GPT_scraper ] https://github.com/rodolflying/GPT_scraper,Rodolflying,False,0.67,1,https://www.reddit.com/gallery/126gr0p,0,1680171002.0,
113,123yr9p,datascience,GPT,relevance,2023-03-27 20:09:45,ChatGPT and made up citations,its_the_llama,False,0.65,5,https://www.reddit.com/r/datascience/comments/123yr9p/chatgpt_and_made_up_citations/,10,1679947785.0,"Hopefully this hasn't been posted here before. I was using ChatGPT to try and find a few reviews to read on parameter search strategies in system biology. Helpfully, it returned a list of seminal papers: 

 

1. ""Parameter estimation and inference for differential equations"" by P.J. Haario et al. (2006). This review article discusses various methods for parameter estimation in differential equations, including Bayesian methods, gradient-based optimization, and particle filtering.
2. ""Parameter estimation in differential equations: a Bayesian perspective"" by S. S√§rkk√§ (2013). This review article focuses on Bayesian methods for parameter estimation in differential equations, including Markov chain Monte Carlo (MCMC) methods and sequential Monte Carlo (SMC) methods.
3. ""Optimization methods for parameter estimation in nonlinear differential equation models"" by C.L. Lawson et al. (2013). This review article discusses various optimization methods for parameter estimation in nonlinear differential equation models, including gradient-based methods, genetic algorithms, and particle swarm optimization.
4. ""Parameter estimation and sensitivity analysis in computational systems biology"" by D. Ge and J.J. Liu (2019). This review article provides an overview of various methods for parameter estimation and sensitivity analysis in computational systems biology, including optimization-based methods, Bayesian methods, and global sensitivity analysis.

Great, right? Except, **none of these papers actually exist**. The authors sound similar to people in the field (""P.J. Haario is probably ""inspired"" by Heikki Haario who's well known in the field, and Simo S√§rkk√§ is an actual author who's published on this), but the work does not exist. 

In hindsight, this makes sense considering how chatGPT works. It's still pretty interesting though, and I wonder how many people have turned in college assays with completely fabricated references."
114,z9ba36,datascience,GPT,relevance,2022-12-01 02:21:12,OpenAI debuts ChatGPT: a conversational AI on GPT-3.5,Opitmus_Prime,False,0.6,1,https://ithinkbot.com/openai-debuts-chatgpt-50dd611278a4,0,1669861272.0,
115,13fvzkv,datascience,GPT,relevance,2023-05-12 20:23:46,ChatGPT code interpreter is crazy! What next?,__ped,False,0.37,0,https://www.reddit.com/r/datascience/comments/13fvzkv/chatgpt_code_interpreter_is_crazy_what_next/,12,1683923026.0,"Hey folks. I was watching a couple of videos about code interpreter plug-in for chatGPT which will he available next week for plus users.

This plug-in let's you upload text/image/gif/... files to it and then will take action on them based on what you request. As an example you can upload a CSV file and ask for some visualizations, or even clustering using K-Means. There could be more to it, but I don't have my hands on yet.

I know how generative AI has been on a fast pace forward in the past couple of months, but this one actually hit me harder. To a sense that I am questioning my knowledge and usefulness for the near future. I am genuinely clueless of how and what to train myself on for the next couple of years, in order to stay useful and creative.

If you have also thought about this and have some possible ideas or thoughts about this topic, please go on and write it down. Maybe it helps me stop freaking out."
116,135uvm8,datascience,GPT,relevance,2023-05-02 17:42:08,Why won‚Äôt AutoGPT take our jobs?,dataentryadmin,False,0.16,0,https://www.reddit.com/r/datascience/comments/135uvm8/why_wont_autogpt_take_our_jobs/,12,1683049328.0,"https://autogpt.net/auto-gpt-vs-chatgpt-how-do-they-differ-and-everything-you-need-to-know/

We are within years of AI handling the entirety of database management, pipelines, transformations, dash-boarding and analysis.

AutoGPT can write and recursively correct/improve its code. 

You convinced me in my last post that data would be the last industry to go, but to me it‚Äôs now looking to be the least safe. 

Data is relatively simple to a machine, and there is less nuance or requirement for a human-touch that software engineering might have (when building software for human application)."
117,12rgc2w,datascience,GPT,relevance,2023-04-19 04:12:07,"GPT-4, my best study buddy!",Somomi_,False,0.7,4,https://www.reddit.com/r/datascience/comments/12rgc2w/gpt4_my_best_study_buddy/,6,1681877527.0," 

Today I find several prompts which could be very helpful for active learning.

**1. Generate Multiple Choice Question**

*Topic: { }*

*Write 3 multiple choice question with 1 correct answer and 3 incorrect distractor answers and let me choose an answer. Later you should let me know if I got it right or wrong and provide me with explanation.*

**2. Generate General Question**

>*Topic: { }*  
*Write 2* *data scientist interview questions* *about this topic and let me answer them. Later you should let me know if I got it right or wrong and provide me with explanation.*

**3. Learning by Teaching**

>*Please act as a data scientist. I will tell you what I l*  
*earn today and you can point out if I miss any step or made any mistake.*  
*Today I learn { }*

You can check my original post with example image here! Thank you!

[https://www.kaggle.com/code/kuixizhu/gpt-4-my-best-study-buddy](https://www.kaggle.com/code/kuixizhu/gpt-4-my-best-study-buddy)"
118,138ckjn,datascience,GPT,relevance,2023-05-05 06:07:33,The language in which GPT 3.5 communicates changes how it thinks.....,NoCartographer4725,False,0.66,13,https://www.reddit.com/r/datascience/comments/138ckjn/the_language_in_which_gpt_35_communicates_changes/,19,1683266853.0,"Seems like a new paper shows that GPT is more patient when talking in Mandarin and German vs when talking in English and Russian.

[https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=4437617](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4437617)

&#x200B;"
119,12sogo9,datascience,GPT,relevance,2023-04-20 06:19:10,Will GPT and its friends lead to a Data Science winter (or have they already)?,AntiqueFigure6,False,0.06,0,https://www.reddit.com/r/datascience/comments/12sogo9/will_gpt_and_its_friends_lead_to_a_data_science/,38,1681971550.0,"Data science was the great hype a few years ago, but a lot of that hype has abated, particularly as businesses found that implementing models in a way that lead to increased profits was significantly more difficult that just making them.  
Now we have the GPT family, and the hype is more than 10x the hype around DS ever was. Whether or not the hype is justified or not, will that mean or has it already meant, that businesses searching for the next way to improve profitability will bypass DS and head straight to GPT, possibly closing DS programs in the process?

Hopefully clearer restatement- will businesses reduce investment in DS in order to invest in implementing GPT style AI solutions?"
120,10mu9ru,datascience,GPT,relevance,2023-01-27 19:53:34,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,1.0,5,https://www.reddit.com/r/datascience/comments/10mu9ru/a_python_module_to_generate_optimized_prompts/,0,1674849214.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features üöÄ

* üßô‚Äç‚ôÄÔ∏è NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* üî® Easily add one-shot, two-shot, or few-shot examples to the prompt
* ‚úå Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* üí• Custom examples and samples can be easily added to the prompt
* üí∞ Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Examples](https://preview.redd.it/x232msli2nea1.png?width=1236&format=png&auto=webp&s=6071efdd8cb12801230af6572991ba8aaff1a9ec)

&#x200B;

https://preview.redd.it/5o5uqk3k2nea1.png?width=1398&format=png&auto=webp&s=96c25820a698a83dfeb0e8f7f37682d9d27c06cb"
121,12avx58,datascience,GPT,relevance,2023-04-03 20:01:07,Has ChatGPT/AI changed anything about your career as a data scientist?,NavidsonsCloset,False,0.56,3,https://www.reddit.com/r/datascience/comments/12avx58/has_chatgptai_changed_anything_about_your_career/,30,1680552067.0,
122,zkqaz5,datascience,GPT,relevance,2022-12-13 08:04:51,ChatGPT business use cases?,danktank138,False,0.5,0,https://www.reddit.com/r/datascience/comments/zkqaz5/chatgpt_business_use_cases/,9,1670918691.0,"What are some good business use cases that you can see the AI model show its strength?

Clear to see that the age of writing essays by trial and error is over. From now on, students have received a tool capable of impressing even the most bitter of essay snobs. But that is hardly anything to be excited for. As I am sure there are tonnes more use cases capable of becoming a full service. 

Thus, what would be some business use cases, which one could use the chatGPT for?

P.S.
I will post this question in both tech oriented and business oriented groups. After a week or two I will sum the best reposnes with my comment üòá"
123,132cvf9,datascience,GPT,relevance,2023-04-28 23:16:15,New ChatGPT features and data science,jehan_gonzales,False,0.54,1,https://www.reddit.com/r/datascience/comments/132cvf9/new_chatgpt_features_and_data_science/,5,1682723775.0,"I'm wondering what impact updates to ChatGPT will have on data science and data scientists. 

I saw a TED talk from one of the OpenAI founders. You can see it here: https://youtu.be/C_78DM8fG6E

It's mind-blowing. He shows some new features that are coming soon to ChatGPT. He asks for a meal and uses the DALL-E integration to get an image of it. He even gets a shopping order put together on Instakart. He just needs to click and the food will be delivered to his house. 

The most impressive thing was the data analysis where he uploaded a CSV and the LLM figured out what the columns referred to on its own and then asked for plots and a prediction etc. The spat out Python code that he could dive into as well. 

It sounds like it could do data cleaning, preprocessing and modelling fairly easily. It would take some iterations, but if you have it the right direction, it would speed up the process 10x or more. 

I think this will basically simplify work for data scientists but will also enable ordinary folks with no quant backgrounds to do sophisticated analysis. 

I'm no longer a data scientist and work in product management. But if I were still in data science, I'd focus on my ability to help people self serve. I think the role will split to expert data scientists who build and productionise ML models and analytics enablers who help people get more out of the tools. 

What do you think? Is this a threat to data scientists? Or is it a productivity booster that will only make life better?"
124,10dfbc7,datascience,GPT,relevance,2023-01-16 14:02:26,Are you using ChatGPT for work?,Conscious-Rush-9646,False,0.58,2,https://www.reddit.com/r/datascience/comments/10dfbc7/are_you_using_chatgpt_for_work/,11,1673877746.0,I started using it just for some silly things like describe what this code does. I'm curios to see if anyone is actually using it on a daily basis and it's making a difference.
125,13gsrkv,datascience,GPT,relevance,2023-05-13 21:06:58,Relying too much on ChatGPT to learn DS programming?,PhisheadS1,False,0.44,0,https://www.reddit.com/r/datascience/comments/13gsrkv/relying_too_much_on_chatgpt_to_learn_ds/,11,1684012018.0,"Hello, I've been learning DS the past few months and I really enjoy it but I find the programming part (Python) quite difficult as I'm coming from a non-DS career (though I had learned SQL pretty well). 

So while doing projects I basically us ChatGPT to tell what code to write for what I want. For example I say ""For each value in the Neighborhood column, get the median home price and rank them so I can assign a numeric value instead of categorical...and while you're add it include the code to do this to the test df""...and voil√† (of course many times I will need to correct it/tinker with the code/or re-explain my request. 

So am I making a big mistake? I mean I know I'll have to learn eventually but I kinda feel like it's like teaching a child to use a calculator without teaching him to do arithmetic. Or is this just going to be the future of learning to program?"
126,137xo27,datascience,GPT,relevance,2023-05-04 19:42:30,"""The new ChatGPT Code Interpreter is like a Data Scientist on steroids""",datasciencepro,False,0.46,0,https://twitter.com/moritzkremb/status/1654107314528612355,23,1683229350.0,
127,128qiu9,datascience,GPT,relevance,2023-04-01 16:02:06,Smarty GPT v2 is out!,usc-ur,False,0.5,0,https://www.reddit.com/r/datascience/comments/128qiu9/smarty_gpt_v2_is_out/,0,1680364926.0,"The second stable version of our library is out. Feel free to check it out! More functionality, simpler to use, support to the official Open AI API (GPT4 included).

Feel free to share, comment, and create PR!

[https://github.com/citiususc/Smarty-GPT](https://github.com/citiususc/Smarty-GPT)"
128,13deekw,datascience,GPT,relevance,2023-05-10 02:58:03,Create Tableau Data Model using GPT-4,wangda-tan,False,0.62,2,https://www.reddit.com/r/datascience/comments/13deekw/create_tableau_data_model_using_gpt4/,1,1683687483.0,"We wrote a blog about how to use GPT-4 and ‚Äúcustom SQL‚Äù to create Tableau data models. Wondering from the experts here, if this is / can be useful - feedback is welcome (and feel free to reach out if you want to try it out!)

Blog: [https://medium.com/querymind/simplify-tableau-data-modeling-with-gpt-4-based-sql-generation-ecb3cf6bfaa5](https://medium.com/querymind/simplify-tableau-data-modeling-with-gpt-4-based-sql-generation-ecb3cf6bfaa5)

Slack community: [https://join.slack.com/t/waiicommunity/shared\_invite/zt-1uslik76c-dKbrUUUuoPBbI4xGH5XCeA](https://join.slack.com/t/waiicommunity/shared_invite/zt-1uslik76c-dKbrUUUuoPBbI4xGH5XCeA)"
129,10fw1a3,datascience,GPT,relevance,2023-01-19 07:54:24,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.94,124,https://www.reddit.com/r/datascience/comments/10fw1a3/gpt4_will_be_500x_smaller_than_people_think_here/,14,1674114864.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/t6m0epzlgyca1.png?width=575&format=png&auto=webp&s=a5972941053f833e76fd3a5009fc68a61f9e5406)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ‚≠ï, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
130,1360rt3,datascience,GPT,relevance,2023-05-02 21:24:46,processing internet data best practice and chatGPT,SomeProfessional,False,0.43,0,https://www.reddit.com/r/datascience/comments/1360rt3/processing_internet_data_best_practice_and_chatgpt/,1,1683062686.0,"Hi, anyone here has had to collect data from the internet and analyze it as part of their jobs. Do you have to constantly going through a lot of websites to find information?

Have you try chatGPT for this purpose? If so, what is your experience?

I developed a tool to automate that process using chatGPT but not sure if it will be useful for anyone?"
131,127qs9h,datascience,GPT,relevance,2023-03-31 16:26:16,ChatGPT Survey: Performance on NLP datasets,matus_pikuliak,False,0.71,3,https://www.reddit.com/r/datascience/comments/127qs9h/chatgpt_survey_performance_on_nlp_datasets/,0,1680279976.0,"I've done a paper survey of how well ChatGPT performs on various NLP tasks as reported in arXiv papers. I have found 19 papers where they compared ChatGPT with fine-tuned models, but they are being published practically daily now. It seems that for the most of the classical NLP tasks, ChatGPT is not actually that strong and smaller fine-tuned models are often much better. According to the API page, GPT-4 is not expected to be much stronger on tasks like these. I think it is an interesting perspective that shows that for many of the tasks we need to solve, GPT models are actually not the right tool.

The full survey is in my blog post: [http://opensamizdat.com/posts/chatgpt\_survey/](http://opensamizdat.com/posts/chatgpt_survey/)

Any feedback is welcomed."
132,13ec41m,datascience,GPT,relevance,2023-05-11 03:43:05,I built a tool to analyze CSVs with ChatGPT!,Gbox4,False,0.47,0,https://v.redd.it/acmm2b1oi4za1,4,1683776585.0,
133,10p9ffb,datascience,GPT,relevance,2023-01-30 18:05:19,Code Nuts and Bolts of Chat GPT,dj_ski_mask,False,0.5,0,https://www.reddit.com/r/datascience/comments/10p9ffb/code_nuts_and_bolts_of_chat_gpt/,3,1675101919.0,"Yo first up I am sorry for YACGPTT (yet another Chat GPT Thread). This is a question about leaning resources.

For some context I did the Coursera NLP Cert a few years ago and really enjoyed it. And it does, in very good detail, hit on the nuts and bolts of manually coding transformer architecture (the ‚ÄòT‚Äô) . 

But like I said it‚Äôs a few years old and I‚Äôm looking for resources on the combination of reinforcement learning + transformer architecture.

Again,  but nuts and bolts I mean code heavy manual demonstrations of how to construct the architecture.

Thanks in advance and if you think I should delete and post in r/learnmachinelearning I will do."
134,12oiyni,datascience,GPT,relevance,2023-04-16 17:57:00,Challenge: Use data science to predict ChatGPT failures,Neurosymbolic,False,0.65,7,https://www.reddit.com/r/datascience/comments/12oiyni/challenge_use_data_science_to_predict_chatgpt/,2,1681667820.0,"Last month, at the AAAI-MAKE conference, we introduced a new challenge problem: predict the failures of ChatGPT when solving math problems.

We have compiled a dataset (based on DRAW-1K) that consists of 1,000 math problem and ChatGPT's response.  We introduced some baseline models at AAAI-MAKE that showed you can predict ChatGPT's failures, we are asking the community to improve on the results.  No need to write a ChatGPT app or anything like that - you can use pure data science techniques.

The challenge results will be due in early 2024 and presented at AAAI-MAKE next year.  You can pre-register here: [https://www.aaai-make.info/next/](https://www.aaai-make.info/next/)

To learn more about the challenge, visit this website: [https://neurosymbolic.asu.edu/chatgpt-mwp-challenge/](https://neurosymbolic.asu.edu/chatgpt-mwp-challenge/)"
135,11vdjat,datascience,GPT,relevance,2023-03-19 06:27:43,datasetGPT - A command-line tool to generate datasets by inferencing LLMs at scale. It can even make two ChatGPT agents talk with one another.,radi-cho,False,0.83,4,https://www.reddit.com/r/datascience/comments/11vdjat/datasetgpt_a_commandline_tool_to_generate/,0,1679207263.0,"GitHub: [https://github.com/radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT)

It can generate texts by varying input parameters and using multiple backends. But, personally, the conversations dataset generation is my favorite: It can produce dialogues between two ChatGPT agents.

Possible use cases may include:

* Constructing textual corpora to train/fine-tune detectors for content written by AI.
* Collecting datasets of LLM-produced conversations for research purposes, analysis of AI performance/impact/ethics, etc.
* Automating a task that a LLM can handle over big amounts of input texts. For example, using GPT-3 to summarize 1000 paragraphs with a single CLI command.
* Leveraging APIs of especially big LLMs to produce diverse texts for a specific task and then fine-tune a smaller model with them.

What would you use it for?"
136,10e1uxn,datascience,GPT,relevance,2023-01-17 03:43:04,How ChatGPT Completely Changes Data Parsing,1dolinski,False,0.67,1,https://vibehut.io/watch/63c5ea59ba3ef10015378e28,1,1673926984.0,
137,107khox,datascience,GPT,relevance,2023-01-09 17:37:48,FYI: GPT-3 & Beyond (Stanford AI Webinar),itedelweiss,False,0.87,22,https://www.reddit.com/r/datascience/comments/107khox/fyi_gpt3_beyond_stanford_ai_webinar/,1,1673285868.0,"The field of Natural Language Processing (NLP) has seen tremendous progress. In particular, algorithms for Natural Language Understanding (NLU) are evolving and improving at a stunning pace. Systems that seemed like science fiction as recently as ten years ago are now commonplace. One such system is GPT-3 (Generative Pretrained Transformer 3), a state-of-the-art model capable of generating human-like text for a wide range of tasks, such as conversation modeling, summarization, and question answering.  

In this live webinar Stanford Professor Christopher Potts will discuss the significance and implications of recent Natural Language Understanding developments including GPT-3. He will outline the fundamental building blocks of these new systems and describe how we can reliably assess and understand them.

Topics will include:

- Technical capabilities, limitations, and applications of new Natural Language Understanding systems

- Analysis of the performance of GPT-3 on various language tasks

- Potential future developments in Natural Language Processing

DATE: January 18, 2023
 	

TIME: 11:00 AM - 12:00 PM (PST)


[Registration link](https://event.on24.com/wcc/r/4076653/D32B5B8B45C4099498D2555AB941504C?mkt_tok=MTk0LU9DUS00ODcAAAGJNpCzioiOp8O-K8z9UB050H0f-EbLnEk9bZVBNI2vmlidmSvXCca2T1T2-BFseDFQyKNr0wv1iH8RGEXMDOj_E1d47fWkanGqcP2AjCkwjYFtaGU)"
138,109r7lq,datascience,GPT,relevance,2023-01-12 05:18:31,How to explain ChatGPT to laypeople / executives?,prawmlhandson,False,0.36,0,https://www.reddit.com/r/datascience/comments/109r7lq/how_to_explain_chatgpt_to_laypeople_executives/,5,1673500711.0,What's the best approach to explain how ChatGPT **works** to high level executives who are not aware of machine learning? Do you talk about how it is essentially doing next word prediction etc? Do you talk about how it was trained? Are there any good resources I can refer to?
139,1361tl0,datascience,GPT,relevance,2023-05-02 22:03:30,127 ChatGPT prompts to 10x your data team's productivity,castor-metadata,False,0.25,0,https://www.reddit.com/r/datascience/comments/1361tl0/127_chatgpt_prompts_to_10x_your_data_teams/,0,1683065010.0,"&#x200B;

[chatgpt data prompts](https://preview.redd.it/ggj6376nqhxa1.png?width=446&format=png&auto=webp&s=0982a03f4911f5e02fe7ec8705b3e7bb6fefa51d)

if you're not leveraging GenAI technologies to write better code, design smarter data pipelines & generate documentation automatically, you are falling behind.  


This leaves opportunities to your competition to do better. If you are:  
\- data engineer  
\- data scientist  
\- data analyst  
\- head of data  
\- analytics engineer  
\- data governance  


Check this out. It will help you:  
\- Focus on high value tasks  
\- Produce higher quality work  
\- Be more productive  
[https://notion.castordoc.com/gpt-prompts](https://notion.castordoc.com/gpt-prompts)  


Have other first-hand experience to leverage ChatGPT to improve data workflows? please add in comments"
140,zaf4bs,datascience,GPT,relevance,2022-12-02 07:35:37,"Cross Entropy, Explained by GPT-3",Ill-Tomato-8400,False,1.0,1,https://www.reddit.com/r/datascience/comments/zaf4bs/cross_entropy_explained_by_gpt3/,0,1669966537.0,"I recently experimented with OpenAI's new Chat GPT3 and used it to generate an explanation of cross entropy. It's impressive what language models are capable of, and it's daunting to imagine what they'll be able to do in the future.

&#x200B;

[https://gradiently.io/cross-entropy-explained-by-gpt3/](https://gradiently.io/cross-entropy-explained-by-gpt3/)

&#x200B;

&#x200B;"
141,znkets,datascience,GPT,relevance,2022-12-16 17:42:17,Can You Generate Realistic Data With GPT-3?,Djinn_Tonic4DataSci,False,0.45,0,https://www.reddit.com/r/datascience/comments/znkets/can_you_generate_realistic_data_with_gpt3/,3,1671212537.0,"ChatGPT has taken the tech world by storm, but its older cousin GPT-3 is still relevant. Being able to connect to the text completion API through python allows you to use the large language model to [generate synthetic data](https://www.tonic.ai/blog/can-you-generate-realistic-data-with-gpt-3) with bespoke distributions and relationships. The application is limited, however, as the lack of on-prem deployment limits your ability to show the model your proprietary data to learn from due to privacy concerns.

Real data is complex, what do people think about using LLMs to generate synthetic data? Should they just stick to writing stories and jokes?"
142,zi11jn,datascience,GPT,relevance,2022-12-10 19:26:15,The code that ChatGPT can't write,philosophicalhacker,False,0.6,1,https://datachimp.app/blog/the-code-chat-gpt-cant-write,1,1670700375.0,
143,137fijm,datascience,GPT,relevance,2023-05-04 09:33:40,Excuse me if this has been asked before but isn‚Äôt ChatGPT going to make this entire field redundant?,data_tryingtist,False,0.25,0,https://www.reddit.com/r/datascience/comments/137fijm/excuse_me_if_this_has_been_asked_before_but_isnt/,30,1683192820.0,"I‚Äôm halfway through a certification in Data Science, and after trying ChatGPT out for a week or two I am suddenly unable to convince myself that it‚Äôs worth becoming certified in this field when AI is going to take all of the jobs. 

Anyone have any thoughts on this? Should I even bother completing the cert?"
144,12in1vt,datascience,GPT,relevance,2023-04-11 15:16:46,Open source libraries for ChatGPT to SQL data analysis?,adunk40,False,0.4,0,/r/dataanalysis/comments/12imui2/open_source_libraries_for_chatgpt_to_sql_data/,0,1681226206.0,
145,11mad6m,datascience,GPT,relevance,2023-03-08 22:30:57,Is it worth using LLMs like GPT-3 for text classification?,pgalgali,False,0.6,1,https://www.reddit.com/r/datascience/comments/11mad6m/is_it_worth_using_llms_like_gpt3_for_text/,5,1678314657.0,"Hello, I am new to LLMs world. I am curious to know whether it is worth using LLMs for text classification problems or they are better suited for text generation use cases?"
146,10987al,datascience,GPT,relevance,2023-01-11 16:01:02,Can GPT-J be used for text summarization?,Monsoon611,False,0.67,1,https://www.reddit.com/r/datascience/comments/10987al/can_gptj_be_used_for_text_summarization/,1,1673452862.0,"I'm trying to find a good alternative for summarizing a large number of comments. I've tried with GPT-3 and was blown away by the results. But it's not free. Looking at alternatives, I found a couple of models specifically Google's T5 and GPT-J. While I was able to do the task with T5, I can't seem to find any resources for performing summarization with GPT-J. Most of it is related to text generation. I thought I would ask here to see if any of you have done the same with either of these models and could give me some advice. Any suggestions for alternative models is also appreciated. For context, the comments are basically customer reviews that need to be summarized. Any help is appreciated.

Thank you!"
147,zcmlp0,datascience,GPT,relevance,2022-12-04 21:39:24,Unofficial Python SDK for OpenAI's ChatGPT,brunneis,False,0.86,15,https://github.com/labteral/chatgpt-python,1,1670189964.0,
148,11tqiab,datascience,GPT,relevance,2023-03-17 13:03:03,An Instruct Version Of GPT-J Using Stanford Alpaca's Dataset,juliensalinas,False,1.0,3,https://www.reddit.com/r/datascience/comments/11tqiab/an_instruct_version_of_gptj_using_stanford/,0,1679058183.0,"I just released an instruct version of GPT-J using Stanford Alpaca's dataset.The result of this experiment is very cool and confirms that, when fine-tuned on the right data, GPT-J is a very powerful AI model!You can download the model from the HuggingFace hub: [https://huggingface.co/nlpcloud/instruct-gpt-j-fp16](https://huggingface.co/nlpcloud/instruct-gpt-j-fp16)

Here is an example:

`from transformers import pipeline import torch`

`generator = pipeline(model=""nlpcloud/instruct-gpt-j-fp16"", torch_dtype=torch.float16, device=0)`

`prompt = ""Correct spelling and grammar from the following text.\nI do not wan to go\n"" print(generator(prompt))`

More details about this experiment here: [https://nlpcloud.com/instruct-version-of-gpt-j-using-stanford-alpaca-dataset.html](https://nlpcloud.com/instruct-version-of-gpt-j-using-stanford-alpaca-dataset.html?utm_source=reddit&utm_campaign=jwu8d596-3816-11ed-a261-0242ac140007)

I hope it will be useful! Please don't hesitate to share some feedbacks!

Julien"
