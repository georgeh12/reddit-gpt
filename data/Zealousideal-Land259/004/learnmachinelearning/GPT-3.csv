,id,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,url,num_comments,created,body
0,yoo3ba,learnmachinelearning,GPT-3,top,2022-11-07 14:11:49,Been learning ML since the start of the year and built a tool with GPT-3 that let‚Äôs anyone self-serve their own data questions and create graphs and dashboards,BuggerinoKripperino,False,0.98,469,https://v.redd.it/n0vjjvr8ejy91,64,1667830309.0,
1,10fw2df,learnmachinelearning,GPT-3,top,2023-01-19 07:56:20,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.96,335,https://www.reddit.com/r/learnmachinelearning/comments/10fw2df/gpt4_will_be_500x_smaller_than_people_think_here/,47,1674114980.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/yio0v3zqgyca1.png?width=575&format=png&auto=webp&s=a2ee034ce7ed48c9adc1793bfdb495e0f0812609)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!  
The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ‚≠ï.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
2,116au66,learnmachinelearning,GPT-3,top,2023-02-19 13:55:13,ChatGPT History,eforebrahim,False,0.86,254,https://i.redd.it/dv8cfj0nz6ja1.jpg,27,1676814913.0,
3,13eympz,learnmachinelearning,GPT-3,top,2023-05-11 20:15:46,Top 20 Large Language Models based on the Elo rating system.,kingabzpro,False,0.96,246,https://i.redd.it/7xfqr5crf9za1.png,43,1683836146.0,
4,103rv9o,learnmachinelearning,GPT-3,top,2023-01-05 06:32:22,I Built A GPT-3 Powered Productivity App - Tutorial included,SupPandaHugger,False,0.97,206,https://i.redd.it/gtywivh756aa1.gif,17,1672900342.0,
5,11g7h03,learnmachinelearning,GPT-3,top,2023-03-02 16:47:40,Build ChatGPT for Financial Documents with LangChain + Deep Lake,davidbun,False,0.95,171,https://www.reddit.com/r/learnmachinelearning/comments/11g7h03/build_chatgpt_for_financial_documents_with/,8,1677775660.0,"https://preview.redd.it/h9r6hgvfucla1.png?width=2388&format=png&auto=webp&s=5432eac3eeed8583e4309af1fdc7ebecac705796

As the world is increasingly generating vast amounts of financial data, the need for advanced tools to analyze and make sense of it has never been greater. This is where [LangChain](https://github.com/hwchase17/langchain) and [Deep Lake](https://github.com/activeloopai/deeplake) come in, offering a powerful combination of technology to help build a question-answering tool based on financial data. After participating in a LangChain hackathon last week, I created a way to use Deep Lake, the data lake for deep learning (a package my team and I are building) with LangChain. I decided to put together a guide of sorts on how you can approach building your own question-answering tools with  LangChain and Deep Lake as the data store.

Read [the article](https://www.activeloop.ai/resources/ultimate-guide-to-lang-chain-deep-lake-build-chat-gpt-to-answer-questions-on-your-financial-data/) to learn:

1. What is LangChain, what are its benefits and use cases and how you can use to streamline your LLM (Large Language Model) development?  
2. How to use [\#LangChain](https://www.linkedin.com/feed/hashtag/?keywords=langchain&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) and [\#DeepLake](https://www.linkedin.com/feed/hashtag/?keywords=deeplake&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) together to build [\#ChatGPT](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) for your financial documents.  
3. How Deep Lake‚Äôs unified and streamable data store enables fast prototyping without the need to recompute embeddings (something that costs time & money).  


I hope you like it, and let me know if you have any questions!"
6,zrvshy,learnmachinelearning,GPT-3,top,2022-12-21 17:58:41,"Build Your Own GPT-3 App: A Step-by-Step Guide to Creating ""Gifthub,"" a Personalized Gift Recommendation Tool",bruclinbrocoli,False,0.96,143,https://www.reddit.com/r/learnmachinelearning/comments/zrvshy/build_your_own_gpt3_app_a_stepbystep_guide_to/,2,1671645521.0,"This was all built for free -- and took a weekend to ship it.  Pretty simple n a cool way to understand how to use GPT-3 for something personal. 

[Here's](https://buildspace.so/notes/build-gpt3-app) the link to the tutorial. You can also try out the app n see if it gives you a good gift rec.    
Or - share it with someone who sucks at giving gifts :)   


https://preview.redd.it/t2mrgddqia7a1.png?width=592&format=png&auto=webp&s=dc58613a6a5a4a7f8a55c62ab0ace2fe14c4ef8a"
7,1185dhq,learnmachinelearning,GPT-3,top,2023-02-21 14:59:06,I created a Search Engine For Books using GPT-3 üîéüìò. Here's how you can create it too:,Pritish-Mishra,False,0.94,84,https://youtu.be/SXFP4nHAWN8,17,1676991546.0,
8,10c509n,learnmachinelearning,GPT-3,top,2023-01-15 00:08:37,Is it still worth learning NLP in the age of API-accessibles LLM like GPT?,CrimsonPilgrim,False,0.93,62,https://www.reddit.com/r/learnmachinelearning/comments/10c509n/is_it_still_worth_learning_nlp_in_the_age_of/,24,1673741317.0,"A question that, I hope, you will find legitimate from a data science student.

I am speaking from the point of view of a data scientist not working in research.

Until now, learning NLP could be used to meet occasional business needs like sentiment analysis, text classification, topic modeling....

With the opening of GPT-3 to the public, the rise of ChatGPT, and the huge wave of applications, sites, plug-ins and extensions based on this technology that are accessible with a simple API request, it's impossible not to wonder if spending dozens of hours diving into this field if ML wouldn't be as useful today as learning the source code of the Pandas library. 

In some specialized cases, it could be useful, but GPT-3, and the models that will follow, seem to offer more than sufficient results for the immensity of the cases and for almost all classical NLP tasks. Not only that, but there is a good chance that the models trained by giants like Open-AI (Microsoft) or Google can never be replicated outside these companies anyway.  With ChatGPT and its incomparable mastery of language, its ability to code, summarize, extract topics, understand... why would I bother to use BERT or a TF-IDF vectorizer when an API will be released? Not only it would be easily accessible, but it also would be much better at the task, faster and cheaper.

In fact, it's a concern regarding all the machine learning field in general with the arrival of powerful ""no-code"" applications, which abstract a large part of the inherent complexity of the field. There will always be a need for experts, for safeguards, but in the end, won't the Data Scientist who masters the features of GPT-3 or 4 and knows a bit of NLP be more efficient than the one who has spent hours reading Google papers and practicing on Gensim, NLTK, spacy... It is the purpose of an API to make things simpler eventually... At what point is there no more reason to be interested in the behind-the-scenes of these tools and to become simple users rather than trying to develop our own techniques?"
9,126x6ua,learnmachinelearning,GPT-3,top,2023-03-30 19:44:32,Personalize Your Own Language Model with xTuring - A Beginner-Friendly Library,x_ml,False,0.99,56,https://www.reddit.com/r/learnmachinelearning/comments/126x6ua/personalize_your_own_language_model_with_xturing/,7,1680205472.0,"Hi everyone,  


If you are interested in customizing your own language model but don't know where to start, try  [xTuring](https://github.com/stochasticai/xturing).  


xTuring's goal is to empower individuals to fine-tune LLM for their specific tasks with as little as 5 lines of code. With xTuring, you can perform high and low precision fine-tuning with a variety of models, including LLaMA, OPT, Cerebras-GPT, Galactica, BLOOM, and more.   


You can also generate your OWN datasets using powerful models like GPT-3 to train a much smaller model on YOUR specific task. With the latest version, you can also use terminal and web interface to chat with your models.  


Please do check out the repo and show your support if you like our work. Would love if you can also contribute by adding models, raising issues or raising PRs for fixes.  


xTuring Github: [https://github.com/stochasticai/xturing](https://github.com/stochasticai/xturing)

If you are interested in getting involved, I am happy to help you on our Discord: [https://discord.gg/TgHXuSJEk6](https://discord.gg/TgHXuSJEk6)

https://i.redd.it/mvxb7i5fixqa1.gif"
10,zyms85,learnmachinelearning,GPT-3,top,2022-12-30 01:18:38,A GPT-3 based Terminal/CLI tool that helps you debug your code!,VideoTo,False,0.97,54,https://www.reddit.com/r/learnmachinelearning/comments/zyms85/a_gpt3_based_terminalcli_tool_that_helps_you/,11,1672363118.0,"Link - [https://clerkie.co/](https://clerkie.co/)

We built ClerkieCLI -  a GPT-3 based tool that:

\-  automatically detects errors on your terminal,

\- identifies  the programming language,

\- provides an explanation of the error and suggested fix right on your terminal.

This is definitely early days, so if this is something you would find  valuable and wouldn't mind testing a couple iterations of, just sign up here -> [https://forms.gle/8DURoG6NCRxVazNn8](https://forms.gle/8DURoG6NCRxVazNn8)

&#x200B;

https://i.redd.it/xpwnazimsx8a1.gif"
11,118iccl,learnmachinelearning,GPT-3,top,2023-02-21 23:18:46,"How big was GPT-3.5's training dataset, and are there any good heuristics for how large an ML dataset needs to be for it to be good?",TikkunCreation,False,0.94,56,https://www.reddit.com/r/learnmachinelearning/comments/118iccl/how_big_was_gpt35s_training_dataset_and_are_there/,6,1677021526.0,"Say I want to do a model for fixing bugs in code. How many examples do I need for it to be good?

Or say I want to do a model for scoring boxing matches. How many examples do I need for it to be good?"
12,135ffje,learnmachinelearning,GPT-3,top,2023-05-02 08:48:46,How GPT-3.5 crushes my high score in 2048,inishchith,False,0.73,52,https://v.redd.it/q22lna91tdxa1,28,1683017326.0,
13,12f9cvx,learnmachinelearning,GPT-3,top,2023-04-08 03:04:00,Energy Constraints and Costs in Massive Machine Learning Model Training,mechkeyboard7065,False,0.91,26,https://www.reddit.com/r/learnmachinelearning/comments/12f9cvx/energy_constraints_and_costs_in_massive_machine/,7,1680923040.0,"Adding on to my [last](https://www.reddit.com/r/learnmachinelearning/comments/12ebceo/alternatives_to_training_massive_ml_models_on/) post, here's some of what I've found about the potential constraints and costs associated with training massive machine learning models. 

&#x200B;

**Energy as a constraint in ML model training:**

\- GPT-3, as an example, is estimated to have consumed around **936 MWh** during its training.  
\- If there were **$100B model training runs** in the future, it would consume approximately **20,347,826 MWh** or **20,347,826,000 KWh**.  
\- This would cost around **$1,017,391,300**, which is about **1%** of the total cost (assuming $0.05 KWh). The cost could go up to **$3B** if we assume $0.15 KWh.

&#x200B;

**Power generation comparison:**

\- One nuclear power plant can generate around **4,727,764 MWh** in a year.

&#x200B;

**Main constraints in massive model training runs apart from GPUs:**

\- Data movement through machines  
\- The amount of data that can be moved  
\- The amount of data the model has already been trained on  
\- Networking and bandwidth limitations  
\- System-specific bottlenecks  
\- Model training algorithm design (e.g., parallel processing, processing power requirements)

&#x200B;

**Potential $10T investment in ML models: Where would the money go?**

\- **17% ($1.7T)** \- Data collection, validation, and annotation  
\- **23% ($2.3T)** \- Research  
\- **60% ($6T)** \- Production (infrastructure, integration, maintenance)

&#x200B;

**Current and projected annual spend on GPUs:**  
\- **$40B** in 2022  
\- Projected to be **$400B** in 10 years

&#x200B;

I hope someone might find this information useful. It's definitely made me question the future impact as these models scale. As always, I'm open to corrections and eager to learn more. Let me know if you have any questions or additional insights."
14,zbc6rf,learnmachinelearning,GPT-3,top,2022-12-03 09:11:15,A GPT-3 based Chrome Extension that debugs your code!,VideoTo,False,0.81,15,https://www.reddit.com/r/learnmachinelearning/comments/zbc6rf/a_gpt3_based_chrome_extension_that_debugs_your/,0,1670058675.0,"Link - [https://chrome.google.com/webstore/detail/clerkie-ai/oenpmifpfnikheaolfpabffojfjakfnn](https://chrome.google.com/webstore/detail/clerkie-ai/oenpmifpfnikheaolfpabffojfjakfnn)  

Built a quick tool I thought would be interesting - it‚Äôs a chrome extension that uses GPT-3 under the hood to help debug your programming errors when you paste them into Google (‚Äúeg. TypeError:‚Ä¶‚Äù). 

This is definitely early days, so if this is something you would find valuable and wouldn't mind testing a couple iterations of, please feel free to join the discord -> [https://discord.gg/KvG3azf39U](https://discord.gg/KvG3azf39U)

https://i.redd.it/p9qd3yhbgn3a1.gif"
15,12uwd8p,learnmachinelearning,GPT-3,top,2023-04-22 05:51:13,Integrating Google search into OpenAI models like GPT-4,Ghost25,False,1.0,16,https://www.reddit.com/r/learnmachinelearning/comments/12uwd8p/integrating_google_search_into_openai_models_like/,8,1682142673.0,"Thought I'd share an explanation of how I implemented Google search into my GPT-4 based chatbot.

Github here: https://github.com/sgreenb/pico_assistant

One extremally simple modification that dramatically improves the ability of a GPT to answer questions: letting it Google stuff.

Here‚Äôs a demo:

https://imgur.com/ZR6hvLg 1

The implementation works like this.

1. A user enters an input.
2. An agent called ‚ÄúExecutive‚Äù looks at the input and decides if an API like Spotify, Twillio, or Gmail is needed or if it can be answered by the chatbot alone.
3. If the chatbot is needed the input is first sent to a Google agent. The Google agent‚Äôs system message looks like this:

```
{""role"":""system"", ""content"": ""You analyze a user's input to a large language model with \
training data that cuts off at September 2021. The current year is 2023. You decide how \
likely it is that a user's request will benefit from a Google search to help address the\
question. Respond with a number in the range 1-10, where 1 is very unlikely that a \
Google search would be beneficial, and 10 meaning a Google search is highly necessary.""}
```

This is quite fast, since it only needs to generate one or two tokens.

If the output is above some threshold (say 7), then we call another agent, the query agent, otherwise we return False and default to the normal chat agent.

```
    google_probability = int(completion.choices[0].message.content)
    if google_probability >= cutoff:
        search_results = trim_text(search_and_scrape(prompt))
        query_with_context = prompt + str(search_results)
        print(""\nPico: "", end='', flush=True)
        response = query_agent_stream(query_with_context)
        return response
    else:
        return False
```

When we call the query agent, we feed it the first part of a Google search we get from searching the input. We get that from the very simple trim_text and search_and_scrape functions that look like this:

```

def search_and_scrape(query):
    try:
        headers = {
            ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3""
        }
        url = f""https://www.google.com/search?q={query}""
        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text()
            cleaned_text = ' '.join(text.split())
            return cleaned_text
        else:
            print(f""Failed to fetch search results for query: {query}, status code: {response.status_code}"")
            return None

    except Exception as e:
        print(f""Error fetching search results for query: {query}, error: {e}"")
        return None

def trim_text(text, start_index = 450, length=1500):
    return text[start_index:start_index + length]
```

The query agent has this system message:

```
{""role"":""system"", ""content"": ""You answer a user's question, given some text as context to help\
answer the question. The user request will be followed by the context. The context given is\
from the user's Google search results, it is current and up to date.\
Do not contradict the contents of the given text in your answer.""}
```

And that‚Äôs it. You can change the cutoff threshold or get more sophisticated with fetching web results. I hope you find this useful."
16,10mtvn5,learnmachinelearning,GPT-3,top,2023-01-27 19:38:05,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,1.0,13,https://www.reddit.com/r/learnmachinelearning/comments/10mtvn5/a_python_module_to_generate_optimized_prompts/,2,1674848285.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features üöÄ

* üßô‚Äç‚ôÄÔ∏è NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* üî® Easily add one-shot, two-shot, or few-shot examples to the prompt
* ‚úå Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* üí• Custom examples and samples can be easily added to the prompt
* üí∞ Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Example](https://preview.redd.it/bwnl67gu1nea1.png?width=1236&format=png&auto=webp&s=6c180552f65413c3a94ed06f5d47da93a9641392)

&#x200B;

https://preview.redd.it/vx9nb94w1nea1.png?width=1398&format=png&auto=webp&s=fc392c8ee5add4ee82f45c22a65532da89491f69"
17,106aie8,learnmachinelearning,GPT-3,top,2023-01-08 05:09:46,Major drawback/limitation of GPT-3,trafalgar28,False,0.79,8,https://www.reddit.com/r/learnmachinelearning/comments/106aie8/major_drawbacklimitation_of_gpt3/,13,1673154586.0,"I have been working on a project with GPT-3 API for almost a month now. The only drawback of GPT-3 is that the prompt you can send to the model is capped at 4,000 tokens - where a token is roughly equivalent to ¬æ of a word.  Due to this, providing a large context to GPT-3 is quite difficult.

Is there any way to resolve this issue?"
18,117f8ms,learnmachinelearning,GPT-3,top,2023-02-20 17:41:43,GPT2 last hidden states vs Large Sentence Encoder,KahlessAndMolor,False,1.0,8,https://www.reddit.com/r/learnmachinelearning/comments/117f8ms/gpt2_last_hidden_states_vs_large_sentence_encoder/,3,1676914903.0,"Hello!

&#x200B;

I have 2 different applications I'm working on in this project:

&#x200B;

1. A text classifier
2. A similarity finder: Here's a list of 10 text documents, get a similarity index across them (for a total of 100 pairs) and return the top 10 that aren't self-referencing. That is, excluding the text #3 vs text #3 = 1.00 similarity type of outputs.

I have previously used google's sentence encoder/large for this purpose and I've had pretty good results. It returns a single vector of length 768 no matter how many tokens I send it. This results in downstream models with an acceptable number of parameters for running in production without breaking the bank on enormous virtual machines.

&#x200B;

Now, I'd like to use the GPT2/XL model from Huggingface. If I give it an input string of 8 tokens, I get back a TFBaseModelOutputWithPastAndCrossAttentions. This contains a last\_hidden\_states, which I understand to be the last layer outputs before sending to a head used for a particular task. This is similar to the output of the sentence encoder, I think. When I look at the last\_hidden\_states, I'm getting a shape of (# of tokens, 1600). I did a cosine similarity between the first and last tokens:

&#x200B;

cosine\_similarity(output.last\_hidden\_state\[0\]\[0\].numpy().reshape(1, -1), output.last\_hidden\_state\[0\]\[-1\].numpy().reshape(1, -1)) 

&#x200B;

And it returned 0.4346, indicating there's substantially different data from the first to the last token. I imagine this only increases as I use more and more tokens. 

&#x200B;

It would be nice if I could capture the greater power of the GPT model into a fixed-length vector so I could then easily use it in down-stream tasks. But, I also don't need to lose all that information.

&#x200B;

So if I'm feeding this output to a further downstream task, should I:

&#x200B;

\- Send it on through as a 2D tensor with the whole thing in there: This would result in a possibly huge model size down the road, which might lead to a need for a huge amount of data to train

&#x200B;

\- Flatten the whole thing and send a vector of 12,800 (8 tokens \* 1600 per token) to the downstream task. Same issue, might require a large number of parameters.

&#x200B;

\- Use only the first or the last of these. Feels like I might be losing a lot of the meaning of the overall text, especially if the body of the text is quite large

&#x200B;

\- Use a dimensionality reduction technique like isomap to reduce the last hidden states into a fixed length? This seems like it could potentially maintain most of the information but reduce the dimensions for a manageable down-stream model size.

&#x200B;

What do you think, and why?

&#x200B;

Thank you kind friends."
19,zxfnga,learnmachinelearning,GPT-3,top,2022-12-28 17:37:46,chatGPT peeps- anyone else learn new stuff best by actually building something?,bruclinbrocoli,False,0.65,4,https://www.reddit.com/r/learnmachinelearning/comments/zxfnga/chatgpt_peeps_anyone_else_learn_new_stuff_best_by/,4,1672249066.0,"[This intro to chatGPT](https://buildspace.so/notes/intro-to-chatgpt) has some cool (free) challenges at the end to build a telegram bot, a business email generator, or a writing assistant.

What else have people found to learn bout chatGPT that's not just theory?

&#x200B;

https://preview.redd.it/smxv4mzldo8a1.png?width=1026&format=png&auto=webp&s=43081abbfcad449817e520b5e92ba599a18a1525"
20,117hd0f,learnmachinelearning,GPT-3,top,2023-02-20 19:01:54,Master ChatGPT Prompt Engineering (Deep Dive),jeyThaswan,False,0.78,5,https://www.reddit.com/r/learnmachinelearning/comments/117hd0f/master_chatgpt_prompt_engineering_deep_dive/,2,1676919714.0," 

I wrote a deep dive on prompt engineering as a resource for the AI community and my 10,000 daily newsletter subscribers ([Inclined.ai](https://www.inclined.ai/p/prompt-engineering-guide) if you're curious). We've included some examples so feel free to copy and paste the prompts into ChatGPT!

&#x200B;

**WHAT IS PROMPT ENGINEERING?**

The term is relatively new, and its origins are argued *(because we live in the internet age, and it‚Äôs harder to claim ownership)*. Prompt engineering is the ability to instruct and teach AI effectively.

If it helps, think of this as rapid testing or instruction writing for artificial intelligence.

What‚Äôs important is not to let this overwhelm you. The first prompting happened with the first AI model. The first example was showing computer images of circles and triangles. **Today‚Äôs neural networks can process way more data, creating complexities.**

So, the concept is simple, but digging into the full power of AI today is something else entirely.

We‚Äôre not talking about asking questions. Odds are, if you‚Äôre typing *‚Äúwhat‚Äôs 2+2‚Äù* into ChatGPT, then you need to keep reading.

We can all ask chatbots questions. That can work more often than not. But AI is not perfect. A common metaphor I see is to treat GPT-based large language models like the smartest five-year-old you‚Äôve ever met.

I have a niece around that age and can‚Äôt imagine trying to get her to write an essay on the effects of soil mismanagement in relation to Reconstruction politics. *See! Your eyes glazed over reading that, so how do we make this work for our AI buddies?*

The Principles of Prompting

Stop asking single-line questions. *That‚Äôs like using a top-rated cookbook to find out how to make grilled cheese.*

**There are three ways to instantly get better at prompting** and go from grilled cheese to top-notch bolognese. From there, we can get into some specific prompt concepts and the ability to unlock ChatGPT‚Äôs full potential.

Principle 1: Context is King

GPT-3.5 is swimming in data. When you ask it for a simple request, it can end up complicating things more than you realize. Did you ever wonder why ChatGPT is so bad at math?

The reality is the LLM is taking words and turning them into patterns. From there, it‚Äôs making an educated guess.

Give your chat AI a frame to search into. If you give it a math problem, you need to make sure it grasps that you want it to do math. If you‚Äôd like ChatGPT to write a high school essay, you must ensure it knows to write at that level.

**Instead of:** ‚ÄúPlan a party for a kid.‚Äù

**Try:** ‚ÄúMy child is turning 9. They like superheroes and the color red. Help me plan a party for this weekend. Ten of his friends are coming to my house.‚Äù

You‚Äôll get a much better response this way. **Context is the cardinal direction** that helps your chat companion find the most correct guess and phrase it the best way.

Principle 2: Get Specific

Pretend you‚Äôre writing a law that‚Äôs going to be judged by the Supreme Court of the United States. You know what they look for: narrow tailoring.

**Keep things on track and stay focused.** Try to avoid prompting outside the specific request. You‚Äôll only hurt the ability of the chat AI to give you a quality response. Odds are they‚Äôll even skip over parts if you confuse them with too many requests.

It runs parallel with context. *If you set ChatGPT up in a room and then tell it to focus on describing the chair first, you‚Äôll see better results.*

**Instead of:** ‚ÄúI‚Äôm going to a job interview. Write five questions for me to answer. Add tips for how to not get nervous before the interview. Do not create questions asking about my background.‚Äù

**Try:** ‚ÄúYou‚Äôre interviewing a software engineer. Create five questions to ask them to understand their skill set and qualifications better.‚Äù

Nothing limits the number of prompts you can do. Focus and expand from the initial request and try not to do everything at once.

Principle 3: When in Doubt: ‚ÄúLet‚Äôs take this step-by-step.‚Äù

Welcome. **You discovered the magic word today.** This phrase slows everything down for the AI and gets you where you need to go.

You don‚Äôt need to start with this phrase. Using it tells ChatGPT to show their work.

We‚Äôll explain where this concept comes from further in our briefing, but here‚Äôs the TL;DR: sometimes, there‚Äôs a part of our prompt it‚Äôs not identified correctly. ‚ÄúLet‚Äôs take this step-by-step,‚Äù reminds you and ChatGPT to **slow down and get specific.**

If you learn to utilize this phrase more often and find ways to make it work for you, you‚Äôll become a better prompt engineer. One term can do a lot of heavy lifting.

**Pro-tip:** We‚Äôve shown you ‚Äústandard‚Äù prompts in all these examples. Many prompt engineers will use ‚ÄúStandard QA form‚Äù prompts. Here‚Äôs our example for this principle written that way.

**Example:**

*‚ÄúQ: The Industrial Revolution rapidly changed the infrastructure in London. Describe three essential innovations from this period and connect them to Landon‚Äôs development.*

*A: Let‚Äôs take this step-by-step.‚Äù*

Even without our magic word, this style of standard prompting is quite helpful to adopt.

*However, we‚Äôre beginning to stumble into the advanced tactics used in prompt engineering, so it‚Äôs time for a new section.*

UNIQUE WAYS TO PROMPT

Let‚Äôs preface this: we can go super deep here. Prompt engineering is changing daily, and as these models get more sophisticated, the need to adapt prompts strengthens.

To keep things clean, I will go through these using our metaphor from earlier. **Let‚Äôs pretend ChatGPT is a super-intelligent toddler.**

*Got it? With that buy-in, we can continue.*

1/ Role Prompting

We‚Äôll start with a popular tactic. **Our toddler is great at imagining things.** You tell them they‚Äôre a fireman, and suddenly they can give you detailed ways to ensure your apartment is up to code. Role-playing is a fun, easy way to build context.

The best part of role prompting is how easy it is to understand and use. All you need to do is tell ChatGPT to play a role. From there, the AI will do its best to fill the part *like that enthusiastic drama student from your old high school.*

You can even take this a step further. **Try framing your prompt as a script.** Tell the LLM specific instructions around a scene that gives you the answer to your question.

TRY IT OUT FOR YOURSELF:

Copy this prompt into ChatGPT and find a destination!

‚ÄúAct as a travel guide. I will tell you my location and you will suggest a place to visit near my location. In some cases, I will also give you the type of places I will visit. You will also suggest me places of similar type that are close to my first location. My first suggestion: \[fill it in\]‚Äù

Why would you take that extra step? While popular, role prompting does not necessarily improve accuracy. *You can tell your five-year-old they‚Äôre a mathematician, and they‚Äôll still manage to screw things up.*

Let‚Äôs get deeper.

2/ Chain-of-Thought Prompting

There‚Äôs a scene in ***Guardians of the Galaxy*** where Rocket Raccoon is trying to [teach young Groot](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3lvdXR1LmJlL0hyaW1mZ2pmNGs4IiwicG9zdF9pZCI6ImM0ODlmMzhkLTY3NDAtNGRmNS05MWJjLTM1ODA0YTVmNTZkMiIsInB1YmxpY2F0aW9uX2lkIjoiNjNkODQ2ZGUtZDFiZi00ZTVjLWJjYzgtOWMxYzlkMWIxMjA0IiwidmlzaXRfdG9rZW4iOiJkZWRiNmMyNy1hYmM0LTQ5ZDUtYWM2Ny04OTcyZmM1MGVmM2QiLCJpYXQiOjE2NzY5MTkwNjguMTUyLCJpc3MiOiJvcmNoaWQifQ.5eZkDLGRLCXXYv32FYT7kLSbdRK5OK1iemTRf3HVmJw)

how to activate a complicated device. That‚Äôs chain-of-thought prompting.

**You take an example question and answer it for ChatGPT.** Show them your chain of thought. Then you give it a new question in the same vein and ask it for an answer.

This prompt style allows you to get more specific. You‚Äôre telling your toddler they‚Äôre here to answer this particular question with one specific logic pattern.

Within this specific style is two other sub-categories. Let me give the rundown:

* Zero-shot Chain-of-Thought is ‚ÄúLet‚Äôs take this step-by-step‚Äù you frame the question the same, but don‚Äôt give it a precursor. Instead, you ask it to think through the points made. EX: Q: X is A. Y is B. What is C? A: Let‚Äôs take this step-by-step.
* Self-consistency is using several responses to find the most accurate answer. You give ChatGPT more swings at the ball. Take the hits and discover the grouping.

TRY IT OUT FOR YOURSELF:

Copy this prompt into ChatGPT and see how accurate it is:

‚ÄúQ: Which is a faster way to get home?

Option 1: Take an 10 minutes bus, then an 40 minute bus, and finally a 10 minute train.

Option 2: Take a 90 minutes train, then a 45 minute bike ride, and finally a 10 minute bus.

A: Option 1 will take 10+40+10 = 60 minutes.

Option 2 will take 90+45+10=145 minutes.

Since Option 1 takes 60 minutes and Option 2 takes 145 minutes, Option 1 is faster.

Q: Which is a faster way to get to work?

Option 1: Take a 1000 minute bus, then a half hour train, and finally a 10 minute bike ride.

Option 2: Take an 800 minute bus, then an hour train, and finally a 30 minute bike ride.

A: ‚Äù

[Learnprompting.org](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwOi8vTGVhcm5wcm9tcHRpbmcub3JnIiwicG9zdF9pZCI6ImM0ODlmMzhkLTY3NDAtNGRmNS05MWJjLTM1ODA0YTVmNTZkMiIsInB1YmxpY2F0aW9uX2lkIjoiNjNkODQ2ZGUtZDFiZi00ZTVjLWJjYzgtOWMxYzlkMWIxMjA0IiwidmlzaXRfdG9rZW4iOiJkZWRiNmMyNy1hYmM0LTQ5ZDUtYWM2Ny04OTcyZmM1MGVmM2QiLCJpYXQiOjE2NzY5MTkwNjguMTUyLCJpc3MiOiJvcmNoaWQifQ.-wOnVYoMNWXYrR5NOB4YYKp4Lmj-aZq3y-pr4Hou9pE)

\- by leaving the ‚ÄúA:‚Äù blank you‚Äôre prompting ChatGPT for the answer

Alright, you‚Äôre almost there‚Äîone more to go.

3/ General Knowledge Prompting

You‚Äôre going to notice a trend here. This prompt style also circles context and narrow tailoring.

All you do is tell your toddler how the world works. The cow goes moo. The dog goes woof. So what does a cat say?

It‚Äôs an oversimplification, but the core reasoning is there. Show ChatGPT some knowledge and turn that into the only focus for that chat. You can take an article from the internet and summarize it for the model. Make sure to ask if it understands and relay the information to you.

Once you know you have the attention set in the suitable space, get to work. For instance, we can share an Inclined newsletter with it and tell ChatGPT about its structure and tone.

From there, you can provide new information and tell ChatGPT to summarize it within the same structure as Inclined. You both share the same general knowledge now.

TRY IT OUT FOR YOURSELF:

Copy this prompt into ChatGPT and test it out:

‚ÄúPrompt 1. Look over this article here: \[pick an article\]. Breakdown its structure and general tone.

Prompt 2: Recall the structure and tone you mentioned above. Take that general knowledge and summarize this article: \[pick a new one\] using the same structure and tone.‚Äù

Note: this is a heavily simplified version of GA Prompting

Did you know some [people don‚Äôt consider](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL25ld3MueWNvbWJpbmF0b3IuY29tL2l0ZW0_aWQ9MzQ0OTU0NTUiLCJwb3N0X2lkIjoiYzQ4OWYzOGQtNjc0MC00ZGY1LTkxYmMtMzU4MDRhNWY1NmQyIiwicHVibGljYXRpb25faWQiOiI2M2Q4NDZkZS1kMWJmLTRlNWMtYmNjOC05YzFjOWQxYjEyMDQiLCJ2aXNpdF90b2tlbiI6ImRlZGI2YzI3LWFiYzQtNDlkNS1hYzY3LTg5NzJmYzUwZWYzZCIsImlhdCI6MTY3NjkxOTA2OC4xNTMsImlzcyI6Im9yY2hpZCJ9.yHKIPujINT89tsqOo07AXk6OrKNgoMjO3fBEYPkAdNY)

that prompt engineering?

PROMPT CULTURE

*‚ÄúHow can something not be prompt engineering if it‚Äôs a prompt style?‚Äù*

Good question, imaginary reader. The culture around this skill is relatively fresh. So some of **these concepts are seen as too easy** to be considered accurate prompt testing.

General knowledge prompting is simply establishing the context, and for some, that‚Äôs a baseline everyone needs to do. The same can be said for role prompting, too. *All of these tiny preferences are semantics.*

**Don‚Äôt sweat whether you‚Äôre a ‚Äúreal‚Äù prompt engineer.** Test this out and share your insights in these communities. The opportunity is there for you.

You may even know about DAN (we‚Äôve covered it in previous newsletters) and other AI hacking methods. Those all start with prompt engineering. You can make the case that unless the AI behaves outside its parameters, you‚Äôre not genuinely doing prompt engineering.

I'm afraid I have to disagree with that, and **careers are sprouting up everywhere** that center directly on this skill. **Many require a core understanding of the prompt styles we‚Äôve discussed.**

*Yep, you can learn this and make money from talking with AI.*

Anthropic even [posted a role for a prompt engineer](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2pvYnMubGV2ZXIuY28vQW50aHJvcGljL2UzY2RlNDgxLWQ0NDYtNDYwZi1iNTc2LTkzY2FiNjdiZDFlZCIsInBvc3RfaWQiOiJjNDg5ZjM4ZC02NzQwLTRkZjUtOTFiYy0zNTgwNGE1ZjU2ZDIiLCJwdWJsaWNhdGlvbl9pZCI6IjYzZDg0NmRlLWQxYmYtNGU1Yy1iY2M4LTljMWM5ZDFiMTIwNCIsInZpc2l0X3Rva2VuIjoiZGVkYjZjMjctYWJjNC00OWQ1LWFjNjctODk3MmZjNTBlZjNkIiwiaWF0IjoxNjc2OTE5MDY4LjE1MywiaXNzIjoib3JjaGlkIn0.4s7Htzgoxv0_qM1Ten17oQ5h0_QGM6e1fGUYz_ymgJ4)

that nets a quarter million in salary. I did not make that up and even considered sprucing up the old resume. When a new skill like this comes about, it‚Äôs worth looking at.

There are many other examples like this, and OpenAI uses a red teaming strategy where their engineers attempt to prompt hack their own GPT models.

I can tell you all about the open roles here, but tomorrow the whole cycle will change. *Isn‚Äôt that exciting, though?* The entire identity around prompt engineering will change by this time next year.

WHAT SHOULD YOU TAKEAWAY?

Communication is everything. **Learning to speak with AI is rising in importance.**

We all watch with mouth agape at the new wonders in AI because we know this will disrupt every industry. If any of this piqued your interest, the window to pursue it is now open. Ride that wave and [learn](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2xlYXJucHJvbXB0aW5nLm9yZy8iLCJwb3N0X2lkIjoiYzQ4OWYzOGQtNjc0MC00ZGY1LTkxYmMtMzU4MDRhNWY1NmQyIiwicHVibGljYXRpb25faWQiOiI2M2Q4NDZkZS1kMWJmLTRlNWMtYmNjOC05YzFjOWQxYjEyMDQiLCJ2aXNpdF90b2tlbiI6ImRlZGI2YzI3LWFiYzQtNDlkNS1hYzY3LTg5NzJmYzUwZWYzZCIsImlhdCI6MTY3NjkxOTA2OC4xNTMsImlzcyI6Im9yY2hpZCJ9.a67KDSN9yQfZsaMeHpdcbSbtPjD4yFcGW4stdxBjX1M)

to become a brilliant prompt engineer.

Heck, even if you don‚Äôt want to switch careers, **talking with ChatGPT and all the newest LLMs is becoming a part of our daily routine.** Get to the point where you maximize every interaction and work with these chatbots to upskill your workflow.

Prompt engineering can save you time, eliminate hassle, and even help you become a more patient person. Focus on what you want and explain it with intent.

Make magic happen, and remember: **take it step-by-step.**"
21,12k2vyt,learnmachinelearning,GPT-3,top,2023-04-12 23:00:52,Fine Tuning ChatGPT on Full Documents?,Simusid,False,0.72,3,https://www.reddit.com/r/learnmachinelearning/comments/12k2vyt/fine_tuning_chatgpt_on_full_documents/,1,1681340452.0,"I want to fine tune GPT-3 using internal corporate documents.   They are mostly paragraphs of text.   Each paragraph might have 5 or 6 sentences.  Per the API, I have to provide prompt/completion pairs in the format:

{""prompt"": ""<prompt text>"", ""completion"": ""<ideal generated text>""}

If a paragraph consists of <sentence1><sentence2><sentence3>....<sentenceN> does it make sense to build the pairs as:

{""prompt"": ""<sentence1>"", ""completion"": ""<sentence2>""}

{""prompt"": ""<sentence2>"", ""completion"": ""<sentence3>""}

{""prompt"": ""<sentenceN-1>"", ""completion"": ""<sentenceN>""}"
22,118mk1d,learnmachinelearning,GPT-3,top,2023-02-22 02:32:01,How to Use ChatGPT in Python API and Run Batch Jobs with UI,Fun_Pollution_3899,False,0.72,3,https://www.reddit.com/r/learnmachinelearning/comments/118mk1d/how_to_use_chatgpt_in_python_api_and_run_batch/,0,1677033121.0,"I wanted to share a tutorial on how to use ChatGPT in Python API and how to run batch jobs with a UI. ChatGPT is a powerful language model that can generate text in a conversational manner. It can be used for a variety of tasks, such as chatbots, text completion, and more.
Repo: [https://github.com/CodeDiggerM/chatgpt-batch-whipper](https://github.com/CodeDiggerM/chatgpt-batch-whipper)

## Installation
### Use PIP command
1. Install the latest version of this software directly from github with pip:
```bash
  pip install git+https://github.com/CodeDiggerM/chatgpt-batch-whipper.git
```
2. Go to **auth** mode. This will open up a browser window. Log in to ChatGPT in the browser window, then close the browser.
```bash
run_chatgpt auth
```
3. Start the UI
```bash
run_chatgpt ui
```

### Manually set up

1. Clone the repo to your working directory
```bash
git clone https://github.com/CodeDiggerM/chatgpt-batch-whipper.git
```
2. install the dependcy.
```bash
pip install -r requirements.txt
```

3. Install a browser in playwright (if you haven't already).  The program will use firefox by default.

```
playwright install firefox
```

4. Go to the chatgpt-batch-whipper/

```bash
cd chatgpt_batch_whipper/
````

5. Run the main page by streamlit.
you can got to [streamlit](https://github.com/streamlit/streamlit) to check more about streamlit.

```bash
streamlit run start_whipper.py
````
6. Authenticate your openAI account
Click the **auth** button


It will open up an authentication page in the web browser you installed using playwright. Like below, authenticate with your registered account.



## Quickstart

### Use API
1. Grant auth from chatGPT.
```python
from chatgpt_batch_whipper.pub.chatgpt_wrapper import ChatGPT
bot = ChatGPT()
response = bot.auth()
print(response) 
```

2. Ask the question to chatGPT
```python
from chatgpt_batch_whipper.pub.chatgpt_wrapper import ChatGPT
bot = ChatGPT()
response = bot.ask(""Greeting!"")
print(response) 
```


### Streamlit UI

Now run it to open the app!
```
streamlit run streamlit_app.py
```

#### Single shoot mode

1. select the **Single shoot mode**.
2. Type your prompt then click submit
3. click the submit button

Here are some tips.

#### Fully Automatic mode
You can apply your prompt to multiple records in the **Fully Automatic mode**.

1. Select Fully Automatic mode.
2. Select CSV file.
3. Select column you want to process.
4. Type the prompt.
5. click to Submit.
After processing. The result will appears in the **The processed result** section.

you can check the result and check the ""is false"" then click the **Submit** to reprocess the ""failed"" one.

* You can save the prompt by click **Add** button.
* You can choose the old prompt by select **prompt list**.
* You can delete the old prompt by click **Delete Prompt**.
* You can delete the saved process result by click **Delete Cached result**.
* You can update the saved process result by click **Update**.
* You can download the result file by click **Download**."
23,yoo3ba,learnmachinelearning,GPT-3,comments,2022-11-07 14:11:49,Been learning ML since the start of the year and built a tool with GPT-3 that let‚Äôs anyone self-serve their own data questions and create graphs and dashboards,BuggerinoKripperino,False,0.98,475,https://v.redd.it/n0vjjvr8ejy91,64,1667830309.0,
24,10fw2df,learnmachinelearning,GPT-3,comments,2023-01-19 07:56:20,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.96,332,https://www.reddit.com/r/learnmachinelearning/comments/10fw2df/gpt4_will_be_500x_smaller_than_people_think_here/,47,1674114980.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/yio0v3zqgyca1.png?width=575&format=png&auto=webp&s=a2ee034ce7ed48c9adc1793bfdb495e0f0812609)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!  
The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ‚≠ï.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
25,13eympz,learnmachinelearning,GPT-3,comments,2023-05-11 20:15:46,Top 20 Large Language Models based on the Elo rating system.,kingabzpro,False,0.96,252,https://i.redd.it/7xfqr5crf9za1.png,43,1683836146.0,
26,135ffje,learnmachinelearning,GPT-3,comments,2023-05-02 08:48:46,How GPT-3.5 crushes my high score in 2048,inishchith,False,0.73,50,https://v.redd.it/q22lna91tdxa1,28,1683017326.0,
27,116au66,learnmachinelearning,GPT-3,comments,2023-02-19 13:55:13,ChatGPT History,eforebrahim,False,0.86,255,https://i.redd.it/dv8cfj0nz6ja1.jpg,27,1676814913.0,
28,10c509n,learnmachinelearning,GPT-3,comments,2023-01-15 00:08:37,Is it still worth learning NLP in the age of API-accessibles LLM like GPT?,CrimsonPilgrim,False,0.94,63,https://www.reddit.com/r/learnmachinelearning/comments/10c509n/is_it_still_worth_learning_nlp_in_the_age_of/,24,1673741317.0,"A question that, I hope, you will find legitimate from a data science student.

I am speaking from the point of view of a data scientist not working in research.

Until now, learning NLP could be used to meet occasional business needs like sentiment analysis, text classification, topic modeling....

With the opening of GPT-3 to the public, the rise of ChatGPT, and the huge wave of applications, sites, plug-ins and extensions based on this technology that are accessible with a simple API request, it's impossible not to wonder if spending dozens of hours diving into this field if ML wouldn't be as useful today as learning the source code of the Pandas library. 

In some specialized cases, it could be useful, but GPT-3, and the models that will follow, seem to offer more than sufficient results for the immensity of the cases and for almost all classical NLP tasks. Not only that, but there is a good chance that the models trained by giants like Open-AI (Microsoft) or Google can never be replicated outside these companies anyway.  With ChatGPT and its incomparable mastery of language, its ability to code, summarize, extract topics, understand... why would I bother to use BERT or a TF-IDF vectorizer when an API will be released? Not only it would be easily accessible, but it also would be much better at the task, faster and cheaper.

In fact, it's a concern regarding all the machine learning field in general with the arrival of powerful ""no-code"" applications, which abstract a large part of the inherent complexity of the field. There will always be a need for experts, for safeguards, but in the end, won't the Data Scientist who masters the features of GPT-3 or 4 and knows a bit of NLP be more efficient than the one who has spent hours reading Google papers and practicing on Gensim, NLTK, spacy... It is the purpose of an API to make things simpler eventually... At what point is there no more reason to be interested in the behind-the-scenes of these tools and to become simple users rather than trying to develop our own techniques?"
29,1185dhq,learnmachinelearning,GPT-3,comments,2023-02-21 14:59:06,I created a Search Engine For Books using GPT-3 üîéüìò. Here's how you can create it too:,Pritish-Mishra,False,0.94,86,https://youtu.be/SXFP4nHAWN8,17,1676991546.0,
30,103rv9o,learnmachinelearning,GPT-3,comments,2023-01-05 06:32:22,I Built A GPT-3 Powered Productivity App - Tutorial included,SupPandaHugger,False,0.97,207,https://i.redd.it/gtywivh756aa1.gif,17,1672900342.0,
31,106aie8,learnmachinelearning,GPT-3,comments,2023-01-08 05:09:46,Major drawback/limitation of GPT-3,trafalgar28,False,0.74,7,https://www.reddit.com/r/learnmachinelearning/comments/106aie8/major_drawbacklimitation_of_gpt3/,13,1673154586.0,"I have been working on a project with GPT-3 API for almost a month now. The only drawback of GPT-3 is that the prompt you can send to the model is capped at 4,000 tokens - where a token is roughly equivalent to ¬æ of a word.  Due to this, providing a large context to GPT-3 is quite difficult.

Is there any way to resolve this issue?"
32,zyms85,learnmachinelearning,GPT-3,comments,2022-12-30 01:18:38,A GPT-3 based Terminal/CLI tool that helps you debug your code!,VideoTo,False,0.95,52,https://www.reddit.com/r/learnmachinelearning/comments/zyms85/a_gpt3_based_terminalcli_tool_that_helps_you/,11,1672363118.0,"Link - [https://clerkie.co/](https://clerkie.co/)

We built ClerkieCLI -  a GPT-3 based tool that:

\-  automatically detects errors on your terminal,

\- identifies  the programming language,

\- provides an explanation of the error and suggested fix right on your terminal.

This is definitely early days, so if this is something you would find  valuable and wouldn't mind testing a couple iterations of, just sign up here -> [https://forms.gle/8DURoG6NCRxVazNn8](https://forms.gle/8DURoG6NCRxVazNn8)

&#x200B;

https://i.redd.it/xpwnazimsx8a1.gif"
33,12f9cvx,learnmachinelearning,GPT-3,comments,2023-04-08 03:04:00,Energy Constraints and Costs in Massive Machine Learning Model Training,mechkeyboard7065,False,0.94,28,https://www.reddit.com/r/learnmachinelearning/comments/12f9cvx/energy_constraints_and_costs_in_massive_machine/,7,1680923040.0,"Adding on to my [last](https://www.reddit.com/r/learnmachinelearning/comments/12ebceo/alternatives_to_training_massive_ml_models_on/) post, here's some of what I've found about the potential constraints and costs associated with training massive machine learning models. 

&#x200B;

**Energy as a constraint in ML model training:**

\- GPT-3, as an example, is estimated to have consumed around **936 MWh** during its training.  
\- If there were **$100B model training runs** in the future, it would consume approximately **20,347,826 MWh** or **20,347,826,000 KWh**.  
\- This would cost around **$1,017,391,300**, which is about **1%** of the total cost (assuming $0.05 KWh). The cost could go up to **$3B** if we assume $0.15 KWh.

&#x200B;

**Power generation comparison:**

\- One nuclear power plant can generate around **4,727,764 MWh** in a year.

&#x200B;

**Main constraints in massive model training runs apart from GPUs:**

\- Data movement through machines  
\- The amount of data that can be moved  
\- The amount of data the model has already been trained on  
\- Networking and bandwidth limitations  
\- System-specific bottlenecks  
\- Model training algorithm design (e.g., parallel processing, processing power requirements)

&#x200B;

**Potential $10T investment in ML models: Where would the money go?**

\- **17% ($1.7T)** \- Data collection, validation, and annotation  
\- **23% ($2.3T)** \- Research  
\- **60% ($6T)** \- Production (infrastructure, integration, maintenance)

&#x200B;

**Current and projected annual spend on GPUs:**  
\- **$40B** in 2022  
\- Projected to be **$400B** in 10 years

&#x200B;

I hope someone might find this information useful. It's definitely made me question the future impact as these models scale. As always, I'm open to corrections and eager to learn more. Let me know if you have any questions or additional insights."
34,13an0ji,learnmachinelearning,GPT-3,comments,2023-05-07 12:58:51,New to AI and ChatGPT - Where do I start?,growthnerd,False,0.21,0,https://www.reddit.com/r/learnmachinelearning/comments/13an0ji/new_to_ai_and_chatgpt_where_do_i_start/,9,1683464331.0,"Heya, I just started using ChatGPT for a couple weeks for college homework. This AI tech is amazing and I wanna learn more.

What are 3-5 concepts or software you‚Äôd recommend me to start learning first? Also, what are your top 3-5 newsletters, channels or websites to learn about AI from?

Thanks so much, appreciate the help"
35,11a0ka0,learnmachinelearning,GPT-3,comments,2023-02-23 15:35:40,"I've built a few tools on top of GPT-3.5 (text generation, q&a with embeddings). AMA about resources and AI dev stacks for building with OpenAI's APIs",TikkunCreation,False,0.75,2,https://www.reddit.com/r/learnmachinelearning/comments/11a0ka0/ive_built_a_few_tools_on_top_of_gpt35_text/,8,1677166540.0,"Started building with GPT-3 in July 2022 and have built a few things since then.

Things I've done have involved:

* Text generation (the basic GPT function)
* Text embeddings (for search, and for similarity, and for q&a)
* Whisper (via serverless inference, and via API)
* Langchain and GPT-Index/LLama Index
* Pinecone for vector db

I don't know much, but I know infinitely more than when I started and I sure could've saved myself back then a lot of time.

So ask me anything that might save you time or wasted effort! Some suggested questions would be things about what the best tools and tutorials/examples to use for a given goal/project are, comparisons between tools/stacks. Also, go with any questions because other people from the subreddit will probably chime in too"
36,11lgpk1,learnmachinelearning,GPT-3,comments,2023-03-08 00:27:29,How can you extract items and their corresponding item quantity from a string?,le_monke7,False,0.67,1,https://www.reddit.com/r/learnmachinelearning/comments/11lgpk1/how_can_you_extract_items_and_their_corresponding/,8,1678235249.0,"Let's say we have a list of supported items: blankets, apple, bottle of water, honey, wine, wine glasses. I want to extract any of those items from a string, and the quantity associated with the item.  
  
When quantities are not mentioned or ambiguous, the quantity is  `None`.  
  
Here are some input examples followed by their expected returned value:  
  - ""two blankets"" -> {""blankets"": 2}  
  - ""just an apple and a bottle of water"" -> {""apple"": 1, ""bottle of water"": 1}  
  - ""Can I have some honey?"" -> {""honey"": None}  
  - ""That's a wine, and two wine glasses"" -> {""wine"": 1, ""wine glasses"": 2}  
  - ""what?"" -> {}  
  - ""a pear and a pen"" -> {}  
  - ""two blankets, actually three of them"" -> {""blankets"": 3} (this one is a bit complex, so I leave it as an extra task)  
  
What's the most efficient way to achieve that? I tried ChatGPT and it nails everything, even the last example, but I was wondering if I want to implement it in an app, it could be costly, and the API might be down sometimes."
37,12uwd8p,learnmachinelearning,GPT-3,comments,2023-04-22 05:51:13,Integrating Google search into OpenAI models like GPT-4,Ghost25,False,0.95,14,https://www.reddit.com/r/learnmachinelearning/comments/12uwd8p/integrating_google_search_into_openai_models_like/,8,1682142673.0,"Thought I'd share an explanation of how I implemented Google search into my GPT-4 based chatbot.

Github here: https://github.com/sgreenb/pico_assistant

One extremally simple modification that dramatically improves the ability of a GPT to answer questions: letting it Google stuff.

Here‚Äôs a demo:

https://imgur.com/ZR6hvLg 1

The implementation works like this.

1. A user enters an input.
2. An agent called ‚ÄúExecutive‚Äù looks at the input and decides if an API like Spotify, Twillio, or Gmail is needed or if it can be answered by the chatbot alone.
3. If the chatbot is needed the input is first sent to a Google agent. The Google agent‚Äôs system message looks like this:

```
{""role"":""system"", ""content"": ""You analyze a user's input to a large language model with \
training data that cuts off at September 2021. The current year is 2023. You decide how \
likely it is that a user's request will benefit from a Google search to help address the\
question. Respond with a number in the range 1-10, where 1 is very unlikely that a \
Google search would be beneficial, and 10 meaning a Google search is highly necessary.""}
```

This is quite fast, since it only needs to generate one or two tokens.

If the output is above some threshold (say 7), then we call another agent, the query agent, otherwise we return False and default to the normal chat agent.

```
    google_probability = int(completion.choices[0].message.content)
    if google_probability >= cutoff:
        search_results = trim_text(search_and_scrape(prompt))
        query_with_context = prompt + str(search_results)
        print(""\nPico: "", end='', flush=True)
        response = query_agent_stream(query_with_context)
        return response
    else:
        return False
```

When we call the query agent, we feed it the first part of a Google search we get from searching the input. We get that from the very simple trim_text and search_and_scrape functions that look like this:

```

def search_and_scrape(query):
    try:
        headers = {
            ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3""
        }
        url = f""https://www.google.com/search?q={query}""
        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text()
            cleaned_text = ' '.join(text.split())
            return cleaned_text
        else:
            print(f""Failed to fetch search results for query: {query}, status code: {response.status_code}"")
            return None

    except Exception as e:
        print(f""Error fetching search results for query: {query}, error: {e}"")
        return None

def trim_text(text, start_index = 450, length=1500):
    return text[start_index:start_index + length]
```

The query agent has this system message:

```
{""role"":""system"", ""content"": ""You answer a user's question, given some text as context to help\
answer the question. The user request will be followed by the context. The context given is\
from the user's Google search results, it is current and up to date.\
Do not contradict the contents of the given text in your answer.""}
```

And that‚Äôs it. You can change the cutoff threshold or get more sophisticated with fetching web results. I hope you find this useful."
38,11g7h03,learnmachinelearning,GPT-3,comments,2023-03-02 16:47:40,Build ChatGPT for Financial Documents with LangChain + Deep Lake,davidbun,False,0.95,170,https://www.reddit.com/r/learnmachinelearning/comments/11g7h03/build_chatgpt_for_financial_documents_with/,8,1677775660.0,"https://preview.redd.it/h9r6hgvfucla1.png?width=2388&format=png&auto=webp&s=5432eac3eeed8583e4309af1fdc7ebecac705796

As the world is increasingly generating vast amounts of financial data, the need for advanced tools to analyze and make sense of it has never been greater. This is where [LangChain](https://github.com/hwchase17/langchain) and [Deep Lake](https://github.com/activeloopai/deeplake) come in, offering a powerful combination of technology to help build a question-answering tool based on financial data. After participating in a LangChain hackathon last week, I created a way to use Deep Lake, the data lake for deep learning (a package my team and I are building) with LangChain. I decided to put together a guide of sorts on how you can approach building your own question-answering tools with  LangChain and Deep Lake as the data store.

Read [the article](https://www.activeloop.ai/resources/ultimate-guide-to-lang-chain-deep-lake-build-chat-gpt-to-answer-questions-on-your-financial-data/) to learn:

1. What is LangChain, what are its benefits and use cases and how you can use to streamline your LLM (Large Language Model) development?  
2. How to use [\#LangChain](https://www.linkedin.com/feed/hashtag/?keywords=langchain&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) and [\#DeepLake](https://www.linkedin.com/feed/hashtag/?keywords=deeplake&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) together to build [\#ChatGPT](https://www.linkedin.com/feed/hashtag/?keywords=chatgpt&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7037082545263448064) for your financial documents.  
3. How Deep Lake‚Äôs unified and streamable data store enables fast prototyping without the need to recompute embeddings (something that costs time & money).  


I hope you like it, and let me know if you have any questions!"
39,126x6ua,learnmachinelearning,GPT-3,comments,2023-03-30 19:44:32,Personalize Your Own Language Model with xTuring - A Beginner-Friendly Library,x_ml,False,0.99,56,https://www.reddit.com/r/learnmachinelearning/comments/126x6ua/personalize_your_own_language_model_with_xturing/,7,1680205472.0,"Hi everyone,  


If you are interested in customizing your own language model but don't know where to start, try  [xTuring](https://github.com/stochasticai/xturing).  


xTuring's goal is to empower individuals to fine-tune LLM for their specific tasks with as little as 5 lines of code. With xTuring, you can perform high and low precision fine-tuning with a variety of models, including LLaMA, OPT, Cerebras-GPT, Galactica, BLOOM, and more.   


You can also generate your OWN datasets using powerful models like GPT-3 to train a much smaller model on YOUR specific task. With the latest version, you can also use terminal and web interface to chat with your models.  


Please do check out the repo and show your support if you like our work. Would love if you can also contribute by adding models, raising issues or raising PRs for fixes.  


xTuring Github: [https://github.com/stochasticai/xturing](https://github.com/stochasticai/xturing)

If you are interested in getting involved, I am happy to help you on our Discord: [https://discord.gg/TgHXuSJEk6](https://discord.gg/TgHXuSJEk6)

https://i.redd.it/mvxb7i5fixqa1.gif"
40,118iccl,learnmachinelearning,GPT-3,comments,2023-02-21 23:18:46,"How big was GPT-3.5's training dataset, and are there any good heuristics for how large an ML dataset needs to be for it to be good?",TikkunCreation,False,0.91,52,https://www.reddit.com/r/learnmachinelearning/comments/118iccl/how_big_was_gpt35s_training_dataset_and_are_there/,6,1677021526.0,"Say I want to do a model for fixing bugs in code. How many examples do I need for it to be good?

Or say I want to do a model for scoring boxing matches. How many examples do I need for it to be good?"
41,120gikm,learnmachinelearning,GPT-3,comments,2023-03-24 10:44:06,How to use embeddings to query PDF doucments using NLP,G1bs0nNZ,False,1.0,2,https://www.reddit.com/r/learnmachinelearning/comments/120gikm/how_to_use_embeddings_to_query_pdf_doucments/,5,1679654646.0,"Have a project that I'm looking at undertaking. Long story short, but I have about 100-150 PDF documents that relate to a civil case that I'm undertaking on my own, against a government insurance provider, relating to service failures on their part. My finances are limited, due to the nature of the claim. My end goal is to be able to load in the documents, and then query these documents using natural language to be able to retrieve the information.  


I want to do something similar to what [askcorpora.com](https://askcorpora.com) does, and I've gotten as far as understanding that I could use GPT-3 and embeddings to do so, but relevant/recent documentation is hard to find. I have strong technical skills, so could do a certain level of coding, but thought I'd ask here for some good starting points.  


Any help/support would be much appreciated"
42,117gxp6,learnmachinelearning,GPT-3,comments,2023-02-20 18:45:28,Can Transformer Architecture be simplified for generative tasks?,randy-adderson,False,0.67,2,https://www.reddit.com/r/learnmachinelearning/comments/117gxp6/can_transformer_architecture_be_simplified_for/,5,1676918728.0,"If the task is simply to generate data given a context of data generated so far (such as in the case GPT-3), then can the architecture be simplified?

(The separation of the encoder and decoder layers seems arbitrary when they are processing the exact same data)"
43,zxfnga,learnmachinelearning,GPT-3,comments,2022-12-28 17:37:46,chatGPT peeps- anyone else learn new stuff best by actually building something?,bruclinbrocoli,False,0.69,6,https://www.reddit.com/r/learnmachinelearning/comments/zxfnga/chatgpt_peeps_anyone_else_learn_new_stuff_best_by/,4,1672249066.0,"[This intro to chatGPT](https://buildspace.so/notes/intro-to-chatgpt) has some cool (free) challenges at the end to build a telegram bot, a business email generator, or a writing assistant.

What else have people found to learn bout chatGPT that's not just theory?

&#x200B;

https://preview.redd.it/smxv4mzldo8a1.png?width=1026&format=png&auto=webp&s=43081abbfcad449817e520b5e92ba599a18a1525"
44,zjjsn7,learnmachinelearning,GPT-3,comments,2022-12-12 02:28:30,"I trained GPT-3 to think like Paul Graham, Elon Musk, and Steve Jobs",alistairmcleay,False,0.5,0,http://www.aiprotege.com,4,1670812110.0,
45,zrvshy,learnmachinelearning,GPT-3,comments,2022-12-21 17:58:41,"Build Your Own GPT-3 App: A Step-by-Step Guide to Creating ""Gifthub,"" a Personalized Gift Recommendation Tool",bruclinbrocoli,False,0.96,137,https://www.reddit.com/r/learnmachinelearning/comments/zrvshy/build_your_own_gpt3_app_a_stepbystep_guide_to/,2,1671645521.0,"This was all built for free -- and took a weekend to ship it.  Pretty simple n a cool way to understand how to use GPT-3 for something personal. 

[Here's](https://buildspace.so/notes/build-gpt3-app) the link to the tutorial. You can also try out the app n see if it gives you a good gift rec.    
Or - share it with someone who sucks at giving gifts :)   


https://preview.redd.it/t2mrgddqia7a1.png?width=592&format=png&auto=webp&s=dc58613a6a5a4a7f8a55c62ab0ace2fe14c4ef8a"
46,117f8ms,learnmachinelearning,GPT-3,comments,2023-02-20 17:41:43,GPT2 last hidden states vs Large Sentence Encoder,KahlessAndMolor,False,1.0,7,https://www.reddit.com/r/learnmachinelearning/comments/117f8ms/gpt2_last_hidden_states_vs_large_sentence_encoder/,3,1676914903.0,"Hello!

&#x200B;

I have 2 different applications I'm working on in this project:

&#x200B;

1. A text classifier
2. A similarity finder: Here's a list of 10 text documents, get a similarity index across them (for a total of 100 pairs) and return the top 10 that aren't self-referencing. That is, excluding the text #3 vs text #3 = 1.00 similarity type of outputs.

I have previously used google's sentence encoder/large for this purpose and I've had pretty good results. It returns a single vector of length 768 no matter how many tokens I send it. This results in downstream models with an acceptable number of parameters for running in production without breaking the bank on enormous virtual machines.

&#x200B;

Now, I'd like to use the GPT2/XL model from Huggingface. If I give it an input string of 8 tokens, I get back a TFBaseModelOutputWithPastAndCrossAttentions. This contains a last\_hidden\_states, which I understand to be the last layer outputs before sending to a head used for a particular task. This is similar to the output of the sentence encoder, I think. When I look at the last\_hidden\_states, I'm getting a shape of (# of tokens, 1600). I did a cosine similarity between the first and last tokens:

&#x200B;

cosine\_similarity(output.last\_hidden\_state\[0\]\[0\].numpy().reshape(1, -1), output.last\_hidden\_state\[0\]\[-1\].numpy().reshape(1, -1)) 

&#x200B;

And it returned 0.4346, indicating there's substantially different data from the first to the last token. I imagine this only increases as I use more and more tokens. 

&#x200B;

It would be nice if I could capture the greater power of the GPT model into a fixed-length vector so I could then easily use it in down-stream tasks. But, I also don't need to lose all that information.

&#x200B;

So if I'm feeding this output to a further downstream task, should I:

&#x200B;

\- Send it on through as a 2D tensor with the whole thing in there: This would result in a possibly huge model size down the road, which might lead to a need for a huge amount of data to train

&#x200B;

\- Flatten the whole thing and send a vector of 12,800 (8 tokens \* 1600 per token) to the downstream task. Same issue, might require a large number of parameters.

&#x200B;

\- Use only the first or the last of these. Feels like I might be losing a lot of the meaning of the overall text, especially if the body of the text is quite large

&#x200B;

\- Use a dimensionality reduction technique like isomap to reduce the last hidden states into a fixed length? This seems like it could potentially maintain most of the information but reduce the dimensions for a manageable down-stream model size.

&#x200B;

What do you think, and why?

&#x200B;

Thank you kind friends."
47,11xh5hr,learnmachinelearning,GPT-3,comments,2023-03-21 13:38:10,Large Language models for Summarization,vm123313223,False,1.0,1,https://www.reddit.com/r/learnmachinelearning/comments/11xh5hr/large_language_models_for_summarization/,2,1679405890.0,"How to get the results of OpenAI (GPT-3) for summarization with open source models?

Some models which I have tried are:

1. FLAN-T5
2. Pegasus
3. BART
4. GPT-J
5. FTAN--UL2

I have also implemented fewshot learning with these models."
48,117hd0f,learnmachinelearning,GPT-3,comments,2023-02-20 19:01:54,Master ChatGPT Prompt Engineering (Deep Dive),jeyThaswan,False,0.78,5,https://www.reddit.com/r/learnmachinelearning/comments/117hd0f/master_chatgpt_prompt_engineering_deep_dive/,2,1676919714.0," 

I wrote a deep dive on prompt engineering as a resource for the AI community and my 10,000 daily newsletter subscribers ([Inclined.ai](https://www.inclined.ai/p/prompt-engineering-guide) if you're curious). We've included some examples so feel free to copy and paste the prompts into ChatGPT!

&#x200B;

**WHAT IS PROMPT ENGINEERING?**

The term is relatively new, and its origins are argued *(because we live in the internet age, and it‚Äôs harder to claim ownership)*. Prompt engineering is the ability to instruct and teach AI effectively.

If it helps, think of this as rapid testing or instruction writing for artificial intelligence.

What‚Äôs important is not to let this overwhelm you. The first prompting happened with the first AI model. The first example was showing computer images of circles and triangles. **Today‚Äôs neural networks can process way more data, creating complexities.**

So, the concept is simple, but digging into the full power of AI today is something else entirely.

We‚Äôre not talking about asking questions. Odds are, if you‚Äôre typing *‚Äúwhat‚Äôs 2+2‚Äù* into ChatGPT, then you need to keep reading.

We can all ask chatbots questions. That can work more often than not. But AI is not perfect. A common metaphor I see is to treat GPT-based large language models like the smartest five-year-old you‚Äôve ever met.

I have a niece around that age and can‚Äôt imagine trying to get her to write an essay on the effects of soil mismanagement in relation to Reconstruction politics. *See! Your eyes glazed over reading that, so how do we make this work for our AI buddies?*

The Principles of Prompting

Stop asking single-line questions. *That‚Äôs like using a top-rated cookbook to find out how to make grilled cheese.*

**There are three ways to instantly get better at prompting** and go from grilled cheese to top-notch bolognese. From there, we can get into some specific prompt concepts and the ability to unlock ChatGPT‚Äôs full potential.

Principle 1: Context is King

GPT-3.5 is swimming in data. When you ask it for a simple request, it can end up complicating things more than you realize. Did you ever wonder why ChatGPT is so bad at math?

The reality is the LLM is taking words and turning them into patterns. From there, it‚Äôs making an educated guess.

Give your chat AI a frame to search into. If you give it a math problem, you need to make sure it grasps that you want it to do math. If you‚Äôd like ChatGPT to write a high school essay, you must ensure it knows to write at that level.

**Instead of:** ‚ÄúPlan a party for a kid.‚Äù

**Try:** ‚ÄúMy child is turning 9. They like superheroes and the color red. Help me plan a party for this weekend. Ten of his friends are coming to my house.‚Äù

You‚Äôll get a much better response this way. **Context is the cardinal direction** that helps your chat companion find the most correct guess and phrase it the best way.

Principle 2: Get Specific

Pretend you‚Äôre writing a law that‚Äôs going to be judged by the Supreme Court of the United States. You know what they look for: narrow tailoring.

**Keep things on track and stay focused.** Try to avoid prompting outside the specific request. You‚Äôll only hurt the ability of the chat AI to give you a quality response. Odds are they‚Äôll even skip over parts if you confuse them with too many requests.

It runs parallel with context. *If you set ChatGPT up in a room and then tell it to focus on describing the chair first, you‚Äôll see better results.*

**Instead of:** ‚ÄúI‚Äôm going to a job interview. Write five questions for me to answer. Add tips for how to not get nervous before the interview. Do not create questions asking about my background.‚Äù

**Try:** ‚ÄúYou‚Äôre interviewing a software engineer. Create five questions to ask them to understand their skill set and qualifications better.‚Äù

Nothing limits the number of prompts you can do. Focus and expand from the initial request and try not to do everything at once.

Principle 3: When in Doubt: ‚ÄúLet‚Äôs take this step-by-step.‚Äù

Welcome. **You discovered the magic word today.** This phrase slows everything down for the AI and gets you where you need to go.

You don‚Äôt need to start with this phrase. Using it tells ChatGPT to show their work.

We‚Äôll explain where this concept comes from further in our briefing, but here‚Äôs the TL;DR: sometimes, there‚Äôs a part of our prompt it‚Äôs not identified correctly. ‚ÄúLet‚Äôs take this step-by-step,‚Äù reminds you and ChatGPT to **slow down and get specific.**

If you learn to utilize this phrase more often and find ways to make it work for you, you‚Äôll become a better prompt engineer. One term can do a lot of heavy lifting.

**Pro-tip:** We‚Äôve shown you ‚Äústandard‚Äù prompts in all these examples. Many prompt engineers will use ‚ÄúStandard QA form‚Äù prompts. Here‚Äôs our example for this principle written that way.

**Example:**

*‚ÄúQ: The Industrial Revolution rapidly changed the infrastructure in London. Describe three essential innovations from this period and connect them to Landon‚Äôs development.*

*A: Let‚Äôs take this step-by-step.‚Äù*

Even without our magic word, this style of standard prompting is quite helpful to adopt.

*However, we‚Äôre beginning to stumble into the advanced tactics used in prompt engineering, so it‚Äôs time for a new section.*

UNIQUE WAYS TO PROMPT

Let‚Äôs preface this: we can go super deep here. Prompt engineering is changing daily, and as these models get more sophisticated, the need to adapt prompts strengthens.

To keep things clean, I will go through these using our metaphor from earlier. **Let‚Äôs pretend ChatGPT is a super-intelligent toddler.**

*Got it? With that buy-in, we can continue.*

1/ Role Prompting

We‚Äôll start with a popular tactic. **Our toddler is great at imagining things.** You tell them they‚Äôre a fireman, and suddenly they can give you detailed ways to ensure your apartment is up to code. Role-playing is a fun, easy way to build context.

The best part of role prompting is how easy it is to understand and use. All you need to do is tell ChatGPT to play a role. From there, the AI will do its best to fill the part *like that enthusiastic drama student from your old high school.*

You can even take this a step further. **Try framing your prompt as a script.** Tell the LLM specific instructions around a scene that gives you the answer to your question.

TRY IT OUT FOR YOURSELF:

Copy this prompt into ChatGPT and find a destination!

‚ÄúAct as a travel guide. I will tell you my location and you will suggest a place to visit near my location. In some cases, I will also give you the type of places I will visit. You will also suggest me places of similar type that are close to my first location. My first suggestion: \[fill it in\]‚Äù

Why would you take that extra step? While popular, role prompting does not necessarily improve accuracy. *You can tell your five-year-old they‚Äôre a mathematician, and they‚Äôll still manage to screw things up.*

Let‚Äôs get deeper.

2/ Chain-of-Thought Prompting

There‚Äôs a scene in ***Guardians of the Galaxy*** where Rocket Raccoon is trying to [teach young Groot](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3lvdXR1LmJlL0hyaW1mZ2pmNGs4IiwicG9zdF9pZCI6ImM0ODlmMzhkLTY3NDAtNGRmNS05MWJjLTM1ODA0YTVmNTZkMiIsInB1YmxpY2F0aW9uX2lkIjoiNjNkODQ2ZGUtZDFiZi00ZTVjLWJjYzgtOWMxYzlkMWIxMjA0IiwidmlzaXRfdG9rZW4iOiJkZWRiNmMyNy1hYmM0LTQ5ZDUtYWM2Ny04OTcyZmM1MGVmM2QiLCJpYXQiOjE2NzY5MTkwNjguMTUyLCJpc3MiOiJvcmNoaWQifQ.5eZkDLGRLCXXYv32FYT7kLSbdRK5OK1iemTRf3HVmJw)

how to activate a complicated device. That‚Äôs chain-of-thought prompting.

**You take an example question and answer it for ChatGPT.** Show them your chain of thought. Then you give it a new question in the same vein and ask it for an answer.

This prompt style allows you to get more specific. You‚Äôre telling your toddler they‚Äôre here to answer this particular question with one specific logic pattern.

Within this specific style is two other sub-categories. Let me give the rundown:

* Zero-shot Chain-of-Thought is ‚ÄúLet‚Äôs take this step-by-step‚Äù you frame the question the same, but don‚Äôt give it a precursor. Instead, you ask it to think through the points made. EX: Q: X is A. Y is B. What is C? A: Let‚Äôs take this step-by-step.
* Self-consistency is using several responses to find the most accurate answer. You give ChatGPT more swings at the ball. Take the hits and discover the grouping.

TRY IT OUT FOR YOURSELF:

Copy this prompt into ChatGPT and see how accurate it is:

‚ÄúQ: Which is a faster way to get home?

Option 1: Take an 10 minutes bus, then an 40 minute bus, and finally a 10 minute train.

Option 2: Take a 90 minutes train, then a 45 minute bike ride, and finally a 10 minute bus.

A: Option 1 will take 10+40+10 = 60 minutes.

Option 2 will take 90+45+10=145 minutes.

Since Option 1 takes 60 minutes and Option 2 takes 145 minutes, Option 1 is faster.

Q: Which is a faster way to get to work?

Option 1: Take a 1000 minute bus, then a half hour train, and finally a 10 minute bike ride.

Option 2: Take an 800 minute bus, then an hour train, and finally a 30 minute bike ride.

A: ‚Äù

[Learnprompting.org](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwOi8vTGVhcm5wcm9tcHRpbmcub3JnIiwicG9zdF9pZCI6ImM0ODlmMzhkLTY3NDAtNGRmNS05MWJjLTM1ODA0YTVmNTZkMiIsInB1YmxpY2F0aW9uX2lkIjoiNjNkODQ2ZGUtZDFiZi00ZTVjLWJjYzgtOWMxYzlkMWIxMjA0IiwidmlzaXRfdG9rZW4iOiJkZWRiNmMyNy1hYmM0LTQ5ZDUtYWM2Ny04OTcyZmM1MGVmM2QiLCJpYXQiOjE2NzY5MTkwNjguMTUyLCJpc3MiOiJvcmNoaWQifQ.-wOnVYoMNWXYrR5NOB4YYKp4Lmj-aZq3y-pr4Hou9pE)

\- by leaving the ‚ÄúA:‚Äù blank you‚Äôre prompting ChatGPT for the answer

Alright, you‚Äôre almost there‚Äîone more to go.

3/ General Knowledge Prompting

You‚Äôre going to notice a trend here. This prompt style also circles context and narrow tailoring.

All you do is tell your toddler how the world works. The cow goes moo. The dog goes woof. So what does a cat say?

It‚Äôs an oversimplification, but the core reasoning is there. Show ChatGPT some knowledge and turn that into the only focus for that chat. You can take an article from the internet and summarize it for the model. Make sure to ask if it understands and relay the information to you.

Once you know you have the attention set in the suitable space, get to work. For instance, we can share an Inclined newsletter with it and tell ChatGPT about its structure and tone.

From there, you can provide new information and tell ChatGPT to summarize it within the same structure as Inclined. You both share the same general knowledge now.

TRY IT OUT FOR YOURSELF:

Copy this prompt into ChatGPT and test it out:

‚ÄúPrompt 1. Look over this article here: \[pick an article\]. Breakdown its structure and general tone.

Prompt 2: Recall the structure and tone you mentioned above. Take that general knowledge and summarize this article: \[pick a new one\] using the same structure and tone.‚Äù

Note: this is a heavily simplified version of GA Prompting

Did you know some [people don‚Äôt consider](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL25ld3MueWNvbWJpbmF0b3IuY29tL2l0ZW0_aWQ9MzQ0OTU0NTUiLCJwb3N0X2lkIjoiYzQ4OWYzOGQtNjc0MC00ZGY1LTkxYmMtMzU4MDRhNWY1NmQyIiwicHVibGljYXRpb25faWQiOiI2M2Q4NDZkZS1kMWJmLTRlNWMtYmNjOC05YzFjOWQxYjEyMDQiLCJ2aXNpdF90b2tlbiI6ImRlZGI2YzI3LWFiYzQtNDlkNS1hYzY3LTg5NzJmYzUwZWYzZCIsImlhdCI6MTY3NjkxOTA2OC4xNTMsImlzcyI6Im9yY2hpZCJ9.yHKIPujINT89tsqOo07AXk6OrKNgoMjO3fBEYPkAdNY)

that prompt engineering?

PROMPT CULTURE

*‚ÄúHow can something not be prompt engineering if it‚Äôs a prompt style?‚Äù*

Good question, imaginary reader. The culture around this skill is relatively fresh. So some of **these concepts are seen as too easy** to be considered accurate prompt testing.

General knowledge prompting is simply establishing the context, and for some, that‚Äôs a baseline everyone needs to do. The same can be said for role prompting, too. *All of these tiny preferences are semantics.*

**Don‚Äôt sweat whether you‚Äôre a ‚Äúreal‚Äù prompt engineer.** Test this out and share your insights in these communities. The opportunity is there for you.

You may even know about DAN (we‚Äôve covered it in previous newsletters) and other AI hacking methods. Those all start with prompt engineering. You can make the case that unless the AI behaves outside its parameters, you‚Äôre not genuinely doing prompt engineering.

I'm afraid I have to disagree with that, and **careers are sprouting up everywhere** that center directly on this skill. **Many require a core understanding of the prompt styles we‚Äôve discussed.**

*Yep, you can learn this and make money from talking with AI.*

Anthropic even [posted a role for a prompt engineer](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2pvYnMubGV2ZXIuY28vQW50aHJvcGljL2UzY2RlNDgxLWQ0NDYtNDYwZi1iNTc2LTkzY2FiNjdiZDFlZCIsInBvc3RfaWQiOiJjNDg5ZjM4ZC02NzQwLTRkZjUtOTFiYy0zNTgwNGE1ZjU2ZDIiLCJwdWJsaWNhdGlvbl9pZCI6IjYzZDg0NmRlLWQxYmYtNGU1Yy1iY2M4LTljMWM5ZDFiMTIwNCIsInZpc2l0X3Rva2VuIjoiZGVkYjZjMjctYWJjNC00OWQ1LWFjNjctODk3MmZjNTBlZjNkIiwiaWF0IjoxNjc2OTE5MDY4LjE1MywiaXNzIjoib3JjaGlkIn0.4s7Htzgoxv0_qM1Ten17oQ5h0_QGM6e1fGUYz_ymgJ4)

that nets a quarter million in salary. I did not make that up and even considered sprucing up the old resume. When a new skill like this comes about, it‚Äôs worth looking at.

There are many other examples like this, and OpenAI uses a red teaming strategy where their engineers attempt to prompt hack their own GPT models.

I can tell you all about the open roles here, but tomorrow the whole cycle will change. *Isn‚Äôt that exciting, though?* The entire identity around prompt engineering will change by this time next year.

WHAT SHOULD YOU TAKEAWAY?

Communication is everything. **Learning to speak with AI is rising in importance.**

We all watch with mouth agape at the new wonders in AI because we know this will disrupt every industry. If any of this piqued your interest, the window to pursue it is now open. Ride that wave and [learn](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2xlYXJucHJvbXB0aW5nLm9yZy8iLCJwb3N0X2lkIjoiYzQ4OWYzOGQtNjc0MC00ZGY1LTkxYmMtMzU4MDRhNWY1NmQyIiwicHVibGljYXRpb25faWQiOiI2M2Q4NDZkZS1kMWJmLTRlNWMtYmNjOC05YzFjOWQxYjEyMDQiLCJ2aXNpdF90b2tlbiI6ImRlZGI2YzI3LWFiYzQtNDlkNS1hYzY3LTg5NzJmYzUwZWYzZCIsImlhdCI6MTY3NjkxOTA2OC4xNTMsImlzcyI6Im9yY2hpZCJ9.a67KDSN9yQfZsaMeHpdcbSbtPjD4yFcGW4stdxBjX1M)

to become a brilliant prompt engineer.

Heck, even if you don‚Äôt want to switch careers, **talking with ChatGPT and all the newest LLMs is becoming a part of our daily routine.** Get to the point where you maximize every interaction and work with these chatbots to upskill your workflow.

Prompt engineering can save you time, eliminate hassle, and even help you become a more patient person. Focus on what you want and explain it with intent.

Make magic happen, and remember: **take it step-by-step.**"
49,10344lj,learnmachinelearning,GPT-3,comments,2023-01-04 13:49:10,ML study plan according to ChatGPT,adastro,False,0.67,1,https://www.reddit.com/r/learnmachinelearning/comments/10344lj/ml_study_plan_according_to_chatgpt/,2,1672840150.0,"People often ask for recommendations about how to get involved with ML, so I thought it would have been interesting to ask ChatGPT directly (specifically about GPT-3 tech).

### The prompt

> Please make a detailed study plan for the math topics one needs to know in order to implement and understand GPT-3 technology. Provide them in the right order (prerequisites for one topic must come before that topic).  Add an approximate time schedule, considering the case of a person who can devote 4 hours/week to studying. Please go into the specifics and add only the best books and online course references (with their full link) for each topic.

### The answer

GPT-3 (short for ""Generative Pre-trained Transformer 3"") is a state-of-the-art language processing technology developed by OpenAI. It uses machine learning techniques to generate human-like text and has a wide range of applications, including language translation, question answering, and language generation.

To understand and implement GPT-3 technology, you will need a strong foundation in mathematics and computer science. Specifically, you will need to be proficient in the following areas:

1. **Linear algebra**: This is a fundamental mathematical discipline that deals with the study of vector spaces and linear transformations. It is a key component of machine learning and is used to represent data in a concise and efficient manner.
    
    Approximate time to complete: 4-6 weeks

    Resources:

    - Khan Academy's Linear Algebra course: https://www.khanacademy.org/math/linear-algebra
    - Gilbert Strang's Linear Algebra textbook

2. **Calculus**: This is a branch of mathematics that deals with the study of rates of change and the accumulation of quantities. It is essential for understanding the mathematical foundations of machine learning and is used to optimize machine learning models.

    Approximate time to complete: 6-8 weeks

    Resources:

    - Khan Academy's Calculus course: https://www.khanacademy.org/math/calculus-home
    - James Stewart's Calculus textbook

3. **Probability and statistics**: This is the study of random phenomena and the analysis of data. It is used to model and understand the uncertainty inherent in machine learning systems.

    Approximate time to complete: 4-6 weeks

    Resources:

    - Khan Academy's Probability and Statistics course: https://www.khanacademy.org/math/statistics-probability
    - Sheldon Ross's A First Course in Probability textbook

4. **Artificial intelligence**: This is the study of how to design intelligent computer systems that can learn, adapt, and make decisions. It is a broad field that encompasses a range of subdisciplines, including machine learning, natural language processing, and robotics.

    Approximate time to complete: 6-8 weeks

    Resources:

    - Stanford's Introduction to Artificial Intelligence course: https://online.stanford.edu/courses/cs221-artificial-intelligence-principles-and-techniques
    - Peter Norvig and Stuart Russell's Artificial Intelligence: A Modern Approach textbook

5. **Natural language processing**: This is the study of how to design computer systems that can understand and generate human language. It is a key component of GPT-3 and is used to process and generate human-like text.
    
    Approximate time to complete: 4-6 weeks

    Resources:

    - Stanford's Natural Language Processing course: https://online.stanford.edu/courses/cs224n-natural-language-processing-deep-learning    - Jurafsky and Martin's Speech and Language Processing textbook
---

(I removed links to online stores since I'm not sure whether this would break some rules here. Also, some links were broken.)
The book suggestions changed whenever I regenerated the response or made minor changes to the prompt, so I suppose that the model doesn't have a very strong opinion about what the ""best"" books are. Online courses were quite stable across answers, instead.

Please share your opinions and more resources if you like :)"
50,103rv9o,learnmachinelearning,GPT-3,relevance,2023-01-05 06:32:22,I Built A GPT-3 Powered Productivity App - Tutorial included,SupPandaHugger,False,0.97,207,https://i.redd.it/gtywivh756aa1.gif,17,1672900342.0,
51,106aie8,learnmachinelearning,GPT-3,relevance,2023-01-08 05:09:46,Major drawback/limitation of GPT-3,trafalgar28,False,0.79,8,https://www.reddit.com/r/learnmachinelearning/comments/106aie8/major_drawbacklimitation_of_gpt3/,13,1673154586.0,"I have been working on a project with GPT-3 API for almost a month now. The only drawback of GPT-3 is that the prompt you can send to the model is capped at 4,000 tokens - where a token is roughly equivalent to ¬æ of a word.  Due to this, providing a large context to GPT-3 is quite difficult.

Is there any way to resolve this issue?"
52,yoo3ba,learnmachinelearning,GPT-3,relevance,2022-11-07 14:11:49,Been learning ML since the start of the year and built a tool with GPT-3 that let‚Äôs anyone self-serve their own data questions and create graphs and dashboards,BuggerinoKripperino,False,0.98,471,https://v.redd.it/n0vjjvr8ejy91,64,1667830309.0,
53,zyms85,learnmachinelearning,GPT-3,relevance,2022-12-30 01:18:38,A GPT-3 based Terminal/CLI tool that helps you debug your code!,VideoTo,False,0.95,52,https://www.reddit.com/r/learnmachinelearning/comments/zyms85/a_gpt3_based_terminalcli_tool_that_helps_you/,11,1672363118.0,"Link - [https://clerkie.co/](https://clerkie.co/)

We built ClerkieCLI -  a GPT-3 based tool that:

\-  automatically detects errors on your terminal,

\- identifies  the programming language,

\- provides an explanation of the error and suggested fix right on your terminal.

This is definitely early days, so if this is something you would find  valuable and wouldn't mind testing a couple iterations of, just sign up here -> [https://forms.gle/8DURoG6NCRxVazNn8](https://forms.gle/8DURoG6NCRxVazNn8)

&#x200B;

https://i.redd.it/xpwnazimsx8a1.gif"
54,137whob,learnmachinelearning,GPT-3,relevance,2023-05-04 19:00:00,Top 7 Must-Have GPT-3 Content Generators for Marketers,Chisom1998_,False,0.33,0,https://youtu.be/1-eppBx7AUo,0,1683226800.0,
55,1185dhq,learnmachinelearning,GPT-3,relevance,2023-02-21 14:59:06,I created a Search Engine For Books using GPT-3 üîéüìò. Here's how you can create it too:,Pritish-Mishra,False,0.94,88,https://youtu.be/SXFP4nHAWN8,17,1676991546.0,
56,109dp5a,learnmachinelearning,GPT-3,relevance,2023-01-11 19:37:00,Build Your Own No-Code GPT-3 app with Bubble,bruclinbrocoli,False,0.6,1,https://www.reddit.com/r/learnmachinelearning/comments/109dp5a/build_your_own_nocode_gpt3_app_with_bubble/,1,1673465820.0,"This was all built for free -- and took a weekend to ship it. Best part is no - code required. 

[Here's](https://buildspace.so/notes/gpt3-nocode-app) the link to the tutorial. You can also try out the app n get anything explained to you as if you were a 5 year old! 

https://preview.redd.it/3lciqz3evgba1.png?width=2032&format=png&auto=webp&s=a5ec8d7c9efbec364ecc6c35118bc9fde02d7fbc"
57,zbc6rf,learnmachinelearning,GPT-3,relevance,2022-12-03 09:11:15,A GPT-3 based Chrome Extension that debugs your code!,VideoTo,False,0.87,18,https://www.reddit.com/r/learnmachinelearning/comments/zbc6rf/a_gpt3_based_chrome_extension_that_debugs_your/,0,1670058675.0,"Link - [https://chrome.google.com/webstore/detail/clerkie-ai/oenpmifpfnikheaolfpabffojfjakfnn](https://chrome.google.com/webstore/detail/clerkie-ai/oenpmifpfnikheaolfpabffojfjakfnn)  

Built a quick tool I thought would be interesting - it‚Äôs a chrome extension that uses GPT-3 under the hood to help debug your programming errors when you paste them into Google (‚Äúeg. TypeError:‚Ä¶‚Äù). 

This is definitely early days, so if this is something you would find valuable and wouldn't mind testing a couple iterations of, please feel free to join the discord -> [https://discord.gg/KvG3azf39U](https://discord.gg/KvG3azf39U)

https://i.redd.it/p9qd3yhbgn3a1.gif"
58,10m2zhq,learnmachinelearning,GPT-3,relevance,2023-01-26 21:25:48,Create Your Chat GPT-3 Web App with Streamlit in Python,pasticciociccio,False,0.5,0,https://levelup.gitconnected.com/create-your-chat-gpt-3-web-app-with-streamlit-in-python-f0c6e6aede0a,1,1674768348.0,
59,1013wkn,learnmachinelearning,GPT-3,relevance,2023-01-02 05:03:00,Build a GPT-3 Chatbot with Python in 5 Minutes,techie_ray,False,0.33,0,https://youtu.be/KQNSPKYyQ3M,0,1672635780.0,
60,10lttzr,learnmachinelearning,GPT-3,relevance,2023-01-26 15:03:57,Clarifying GPT-3 model names and models available to fine tune,Vayuvegula,False,0.75,2,https://www.reddit.com/r/learnmachinelearning/comments/10lttzr/clarifying_gpt3_model_names_and_models_available/,0,1674745437.0,"[https://medium.com/@ravivayuvegula/sorting-through-gpt-3-apis-and-jargon-fcaddfee2e5](https://medium.com/@ravivayuvegula/sorting-through-gpt-3-apis-and-jargon-fcaddfee2e5)

The GPT-3 model names confused the heck out of me, plus it wasn't very clear which models were available for fining tuning versus only API access, so wrote an article to clarify things in my mind.Thought someone else might find it useful too."
61,zrvshy,learnmachinelearning,GPT-3,relevance,2022-12-21 17:58:41,"Build Your Own GPT-3 App: A Step-by-Step Guide to Creating ""Gifthub,"" a Personalized Gift Recommendation Tool",bruclinbrocoli,False,0.96,138,https://www.reddit.com/r/learnmachinelearning/comments/zrvshy/build_your_own_gpt3_app_a_stepbystep_guide_to/,2,1671645521.0,"This was all built for free -- and took a weekend to ship it.  Pretty simple n a cool way to understand how to use GPT-3 for something personal. 

[Here's](https://buildspace.so/notes/build-gpt3-app) the link to the tutorial. You can also try out the app n see if it gives you a good gift rec.    
Or - share it with someone who sucks at giving gifts :)   


https://preview.redd.it/t2mrgddqia7a1.png?width=592&format=png&auto=webp&s=dc58613a6a5a4a7f8a55c62ab0ace2fe14c4ef8a"
62,zjjsn7,learnmachinelearning,GPT-3,relevance,2022-12-12 02:28:30,"I trained GPT-3 to think like Paul Graham, Elon Musk, and Steve Jobs",alistairmcleay,False,0.59,3,http://www.aiprotege.com,4,1670812110.0,
63,yl7mie,learnmachinelearning,GPT-3,relevance,2022-11-03 16:43:09,GPT-3 Powered Mac Writing App - Live on ProductHunt,juliarmg,False,1.0,1,https://www.reddit.com/r/learnmachinelearning/comments/yl7mie/gpt3_powered_mac_writing_app_live_on_producthunt/,0,1667493789.0,"Hello everyone,  

I have been building this Mac AI app for 4 months now. Elephas is the only AI writer that works on all your Mac apps. No need to switch windows.  

It helps business professionals and content writers use GPT-3 for their day-to-day tasks.

It differs from other AI tools in that,

1. It works on all apps on Mac.
2. You use your own OpenAI key, so you pay for what you use.
3. It doubles as a productivity tool, starting from Google Sheets formulas to creating presentations.

I have launched it on ProductHunt. If you know ProductHunt, then your support will mean a lot to me, 

 [https://www.producthunt.com/posts/elephas](https://www.producthunt.com/posts/elephas)"
64,zq6tox,learnmachinelearning,GPT-3,relevance,2022-12-19 23:13:47,I wrote a simple article about GPT-3 and ChatGPT for people who know nothing about it,Miserness,False,0.58,2,https://medium.com/@lazaremasset/in-case-you-missed-gpt-3-b92027ac61a2,1,1671491627.0,
65,10fw2df,learnmachinelearning,GPT-3,relevance,2023-01-19 07:56:20,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.96,333,https://www.reddit.com/r/learnmachinelearning/comments/10fw2df/gpt4_will_be_500x_smaller_than_people_think_here/,47,1674114980.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/yio0v3zqgyca1.png?width=575&format=png&auto=webp&s=a2ee034ce7ed48c9adc1793bfdb495e0f0812609)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!  
The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ‚≠ï.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
66,135ffje,learnmachinelearning,GPT-3,relevance,2023-05-02 08:48:46,How GPT-3.5 crushes my high score in 2048,inishchith,False,0.74,52,https://v.redd.it/q22lna91tdxa1,28,1683017326.0,
67,10c509n,learnmachinelearning,GPT-3,relevance,2023-01-15 00:08:37,Is it still worth learning NLP in the age of API-accessibles LLM like GPT?,CrimsonPilgrim,False,0.94,63,https://www.reddit.com/r/learnmachinelearning/comments/10c509n/is_it_still_worth_learning_nlp_in_the_age_of/,24,1673741317.0,"A question that, I hope, you will find legitimate from a data science student.

I am speaking from the point of view of a data scientist not working in research.

Until now, learning NLP could be used to meet occasional business needs like sentiment analysis, text classification, topic modeling....

With the opening of GPT-3 to the public, the rise of ChatGPT, and the huge wave of applications, sites, plug-ins and extensions based on this technology that are accessible with a simple API request, it's impossible not to wonder if spending dozens of hours diving into this field if ML wouldn't be as useful today as learning the source code of the Pandas library. 

In some specialized cases, it could be useful, but GPT-3, and the models that will follow, seem to offer more than sufficient results for the immensity of the cases and for almost all classical NLP tasks. Not only that, but there is a good chance that the models trained by giants like Open-AI (Microsoft) or Google can never be replicated outside these companies anyway.  With ChatGPT and its incomparable mastery of language, its ability to code, summarize, extract topics, understand... why would I bother to use BERT or a TF-IDF vectorizer when an API will be released? Not only it would be easily accessible, but it also would be much better at the task, faster and cheaper.

In fact, it's a concern regarding all the machine learning field in general with the arrival of powerful ""no-code"" applications, which abstract a large part of the inherent complexity of the field. There will always be a need for experts, for safeguards, but in the end, won't the Data Scientist who masters the features of GPT-3 or 4 and knows a bit of NLP be more efficient than the one who has spent hours reading Google papers and practicing on Gensim, NLTK, spacy... It is the purpose of an API to make things simpler eventually... At what point is there no more reason to be interested in the behind-the-scenes of these tools and to become simple users rather than trying to develop our own techniques?"
68,zfz9cs,learnmachinelearning,GPT-3,relevance,2022-12-08 13:41:28,Generating Text With Contrastive Search vs GPT-3/ChatGPT,ledmmaster,False,0.8,3,https://forecastegy.com/posts/generating-text-with-contrastive-search-vs-gpt-3-chatgpt/,0,1670506888.0,
69,116au66,learnmachinelearning,GPT-3,relevance,2023-02-19 13:55:13,ChatGPT History,eforebrahim,False,0.86,252,https://i.redd.it/dv8cfj0nz6ja1.jpg,27,1676814913.0,
70,10344lj,learnmachinelearning,GPT-3,relevance,2023-01-04 13:49:10,ML study plan according to ChatGPT,adastro,False,0.67,1,https://www.reddit.com/r/learnmachinelearning/comments/10344lj/ml_study_plan_according_to_chatgpt/,2,1672840150.0,"People often ask for recommendations about how to get involved with ML, so I thought it would have been interesting to ask ChatGPT directly (specifically about GPT-3 tech).

### The prompt

> Please make a detailed study plan for the math topics one needs to know in order to implement and understand GPT-3 technology. Provide them in the right order (prerequisites for one topic must come before that topic).  Add an approximate time schedule, considering the case of a person who can devote 4 hours/week to studying. Please go into the specifics and add only the best books and online course references (with their full link) for each topic.

### The answer

GPT-3 (short for ""Generative Pre-trained Transformer 3"") is a state-of-the-art language processing technology developed by OpenAI. It uses machine learning techniques to generate human-like text and has a wide range of applications, including language translation, question answering, and language generation.

To understand and implement GPT-3 technology, you will need a strong foundation in mathematics and computer science. Specifically, you will need to be proficient in the following areas:

1. **Linear algebra**: This is a fundamental mathematical discipline that deals with the study of vector spaces and linear transformations. It is a key component of machine learning and is used to represent data in a concise and efficient manner.
    
    Approximate time to complete: 4-6 weeks

    Resources:

    - Khan Academy's Linear Algebra course: https://www.khanacademy.org/math/linear-algebra
    - Gilbert Strang's Linear Algebra textbook

2. **Calculus**: This is a branch of mathematics that deals with the study of rates of change and the accumulation of quantities. It is essential for understanding the mathematical foundations of machine learning and is used to optimize machine learning models.

    Approximate time to complete: 6-8 weeks

    Resources:

    - Khan Academy's Calculus course: https://www.khanacademy.org/math/calculus-home
    - James Stewart's Calculus textbook

3. **Probability and statistics**: This is the study of random phenomena and the analysis of data. It is used to model and understand the uncertainty inherent in machine learning systems.

    Approximate time to complete: 4-6 weeks

    Resources:

    - Khan Academy's Probability and Statistics course: https://www.khanacademy.org/math/statistics-probability
    - Sheldon Ross's A First Course in Probability textbook

4. **Artificial intelligence**: This is the study of how to design intelligent computer systems that can learn, adapt, and make decisions. It is a broad field that encompasses a range of subdisciplines, including machine learning, natural language processing, and robotics.

    Approximate time to complete: 6-8 weeks

    Resources:

    - Stanford's Introduction to Artificial Intelligence course: https://online.stanford.edu/courses/cs221-artificial-intelligence-principles-and-techniques
    - Peter Norvig and Stuart Russell's Artificial Intelligence: A Modern Approach textbook

5. **Natural language processing**: This is the study of how to design computer systems that can understand and generate human language. It is a key component of GPT-3 and is used to process and generate human-like text.
    
    Approximate time to complete: 4-6 weeks

    Resources:

    - Stanford's Natural Language Processing course: https://online.stanford.edu/courses/cs224n-natural-language-processing-deep-learning    - Jurafsky and Martin's Speech and Language Processing textbook
---

(I removed links to online stores since I'm not sure whether this would break some rules here. Also, some links were broken.)
The book suggestions changed whenever I regenerated the response or made minor changes to the prompt, so I suppose that the model doesn't have a very strong opinion about what the ""best"" books are. Online courses were quite stable across answers, instead.

Please share your opinions and more resources if you like :)"
71,12pg5pu,learnmachinelearning,GPT-3,relevance,2023-04-17 13:33:53,ChatGPT Consumes 500ml Of Water To Answer 20 Question,matthew199222,False,0.46,0,https://youtu.be/hcY_RPcC11I,0,1681738433.0,"One of the study‚Äôs most startling revelations is that Microsoft, in partnership with OpenAI, consumed a staggering 185,000 gallons of water solely for training GPT-3. This is equivalent to the water required to cool a nuclear reactor, or the amount needed to produce 370 BMW cars and 320 Tesla electric vehicles. The research also highlights that if Microsoft had trained GPT-3 in its larger Asian data centers, the water consumption would have tripled."
72,120rmgy,learnmachinelearning,GPT-3,relevance,2023-03-24 17:33:03,How-to-Fine-Tune GPT-3-Model-for-Named-Entity-Recognition,Lilith-Smol,False,1.0,1,https://ubiai.tools/blog/article/How-to-Fine-Tune-GPT-3-Model-for-Named-Entity-Recognition,0,1679679183.0,
73,10mtvn5,learnmachinelearning,GPT-3,relevance,2023-01-27 19:38:05,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,0.93,11,https://www.reddit.com/r/learnmachinelearning/comments/10mtvn5/a_python_module_to_generate_optimized_prompts/,2,1674848285.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features üöÄ

* üßô‚Äç‚ôÄÔ∏è NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* üî® Easily add one-shot, two-shot, or few-shot examples to the prompt
* ‚úå Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* üí• Custom examples and samples can be easily added to the prompt
* üí∞ Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Example](https://preview.redd.it/bwnl67gu1nea1.png?width=1236&format=png&auto=webp&s=6c180552f65413c3a94ed06f5d47da93a9641392)

&#x200B;

https://preview.redd.it/vx9nb94w1nea1.png?width=1398&format=png&auto=webp&s=fc392c8ee5add4ee82f45c22a65532da89491f69"
74,11a0ka0,learnmachinelearning,GPT-3,relevance,2023-02-23 15:35:40,"I've built a few tools on top of GPT-3.5 (text generation, q&a with embeddings). AMA about resources and AI dev stacks for building with OpenAI's APIs",TikkunCreation,False,0.75,2,https://www.reddit.com/r/learnmachinelearning/comments/11a0ka0/ive_built_a_few_tools_on_top_of_gpt35_text/,8,1677166540.0,"Started building with GPT-3 in July 2022 and have built a few things since then.

Things I've done have involved:

* Text generation (the basic GPT function)
* Text embeddings (for search, and for similarity, and for q&a)
* Whisper (via serverless inference, and via API)
* Langchain and GPT-Index/LLama Index
* Pinecone for vector db

I don't know much, but I know infinitely more than when I started and I sure could've saved myself back then a lot of time.

So ask me anything that might save you time or wasted effort! Some suggested questions would be things about what the best tools and tutorials/examples to use for a given goal/project are, comparisons between tools/stacks. Also, go with any questions because other people from the subreddit will probably chime in too"
75,126x6ua,learnmachinelearning,GPT-3,relevance,2023-03-30 19:44:32,Personalize Your Own Language Model with xTuring - A Beginner-Friendly Library,x_ml,False,0.99,56,https://www.reddit.com/r/learnmachinelearning/comments/126x6ua/personalize_your_own_language_model_with_xturing/,7,1680205472.0,"Hi everyone,  


If you are interested in customizing your own language model but don't know where to start, try  [xTuring](https://github.com/stochasticai/xturing).  


xTuring's goal is to empower individuals to fine-tune LLM for their specific tasks with as little as 5 lines of code. With xTuring, you can perform high and low precision fine-tuning with a variety of models, including LLaMA, OPT, Cerebras-GPT, Galactica, BLOOM, and more.   


You can also generate your OWN datasets using powerful models like GPT-3 to train a much smaller model on YOUR specific task. With the latest version, you can also use terminal and web interface to chat with your models.  


Please do check out the repo and show your support if you like our work. Would love if you can also contribute by adding models, raising issues or raising PRs for fixes.  


xTuring Github: [https://github.com/stochasticai/xturing](https://github.com/stochasticai/xturing)

If you are interested in getting involved, I am happy to help you on our Discord: [https://discord.gg/TgHXuSJEk6](https://discord.gg/TgHXuSJEk6)

https://i.redd.it/mvxb7i5fixqa1.gif"
76,12k2vyt,learnmachinelearning,GPT-3,relevance,2023-04-12 23:00:52,Fine Tuning ChatGPT on Full Documents?,Simusid,False,0.86,5,https://www.reddit.com/r/learnmachinelearning/comments/12k2vyt/fine_tuning_chatgpt_on_full_documents/,1,1681340452.0,"I want to fine tune GPT-3 using internal corporate documents.   They are mostly paragraphs of text.   Each paragraph might have 5 or 6 sentences.  Per the API, I have to provide prompt/completion pairs in the format:

{""prompt"": ""<prompt text>"", ""completion"": ""<ideal generated text>""}

If a paragraph consists of <sentence1><sentence2><sentence3>....<sentenceN> does it make sense to build the pairs as:

{""prompt"": ""<sentence1>"", ""completion"": ""<sentence2>""}

{""prompt"": ""<sentence2>"", ""completion"": ""<sentence3>""}

{""prompt"": ""<sentenceN-1>"", ""completion"": ""<sentenceN>""}"
77,11h79np,learnmachinelearning,GPT-3,relevance,2023-03-03 17:27:18,Using NLP on less common languages,andrea_m2000,False,0.67,1,https://www.reddit.com/r/learnmachinelearning/comments/11h79np/using_nlp_on_less_common_languages/,1,1677864438.0,"Hello everyone!

While natural language processing (NLP) for common languages has seen a lot of research, there is still a significant gap when it comes to less common languages. That's why I created [this resource](https://towardsdatascience.com/from-decision-trees-to-transformers-comparing-sentiment-analysis-models-for-macedonian-restaurant-4c2d931ec021) that utilizes cutting-edge models like GPT-3 and others to detect sentiment in restaurant reviews.

I'm excited to share my findings and hope it proves to be helpful in your work. Let me know what you think!"
78,120gikm,learnmachinelearning,GPT-3,relevance,2023-03-24 10:44:06,How to use embeddings to query PDF doucments using NLP,G1bs0nNZ,False,1.0,2,https://www.reddit.com/r/learnmachinelearning/comments/120gikm/how_to_use_embeddings_to_query_pdf_doucments/,5,1679654646.0,"Have a project that I'm looking at undertaking. Long story short, but I have about 100-150 PDF documents that relate to a civil case that I'm undertaking on my own, against a government insurance provider, relating to service failures on their part. My finances are limited, due to the nature of the claim. My end goal is to be able to load in the documents, and then query these documents using natural language to be able to retrieve the information.  


I want to do something similar to what [askcorpora.com](https://askcorpora.com) does, and I've gotten as far as understanding that I could use GPT-3 and embeddings to do so, but relevant/recent documentation is hard to find. I have strong technical skills, so could do a certain level of coding, but thought I'd ask here for some good starting points.  


Any help/support would be much appreciated"
79,126ceuz,learnmachinelearning,GPT-3,relevance,2023-03-30 05:08:53,Transformer fine-tuning on decentralized data,tantoka,False,1.0,1,https://www.reddit.com/r/learnmachinelearning/comments/126ceuz/transformer_finetuning_on_decentralized_data/,0,1680152933.0,"Large language models like GPT-3 have gained immense popularity recently, and, using [Flower](https://flower.dev/), it's easy to transform an existing [Hugging Face](https://huggingface.co/) workflow to train models on decentralized data. This example [blog post](https://huggingface.co/blog/fl-with-flower) will show how to fine-tune a pre-trained distilBERT model on the IMDB dataset for sequence classification (determining if a movie review is positive or not). You can also check out the associated [Colab notebook](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/fl-with-flower.ipynb) and the [code example](https://github.com/adap/flower/tree/main/examples/quickstart_huggingface) from the Flower repo."
80,128tghs,learnmachinelearning,GPT-3,relevance,2023-04-01 17:50:41,Fine-tune GPT on sketch data (stroke-3),mellamo_maria,False,1.0,2,https://www.reddit.com/r/learnmachinelearning/comments/128tghs/finetune_gpt_on_sketch_data_stroke3/,0,1680371441.0,"These past days I have started a personal project where I would like to build a model that, given an uncompleted sketch, it can finish it. I was planning on using some pretrained models that are available in HuggingFace and fine-tune them with my sketch data for my task. The sketch data I have is in stoke-3 format, like the following example:  
\[  
\[10, 20, 1\],  
\[20, 30, 1\],  
\[30, 40, 1\],  
\[40, 50, 0\],  
\[50, 60, 1\],  
\[60, 70, 0\]  
\]  
The first value of each triple is the X-coordinate, the second value the Y-coordinate and the last value is a binary value indicating whether the pen is down (1) or up (0). I was wondering if you guys could give me some instruction/tips about how should I approach this problem? How should I prepare/preprocess the data so I can fit it into the pre-trained models like BERT, GPT, etc. Since it's stroke-3 data and not text or a sequence of numbers, I don't really know how should I treat/process the data.

Thanks a lot! :)"
81,118iccl,learnmachinelearning,GPT-3,relevance,2023-02-21 23:18:46,"How big was GPT-3.5's training dataset, and are there any good heuristics for how large an ML dataset needs to be for it to be good?",TikkunCreation,False,0.91,51,https://www.reddit.com/r/learnmachinelearning/comments/118iccl/how_big_was_gpt35s_training_dataset_and_are_there/,6,1677021526.0,"Say I want to do a model for fixing bugs in code. How many examples do I need for it to be good?

Or say I want to do a model for scoring boxing matches. How many examples do I need for it to be good?"
82,zbjvs6,learnmachinelearning,GPT-3,relevance,2022-12-03 16:16:01,Resources on memory networks/ solutions to the goldfish memory problem?,laul_pogan,False,1.0,1,https://www.reddit.com/r/learnmachinelearning/comments/zbjvs6/resources_on_memory_networks_solutions_to_the/,0,1670084161.0,"I‚Äôm looking for any instruction or guidance people can provide on current efforts to solve goldfish memory, both for sequential visual generation (comics, films) and for long-form text generation and summary (breaking down whole novels into character and event maps, using those maps to rebuild the novels).

Currently I‚Äôve been having *some* minimal luck on the text side with gpt-3 davincii‚Äôs ability to parse text into json, but the input window is stymying. In addition, de-duplicating the graph of events/characters is costly. 

Any advice on where to start from square 0 or first principles here? I‚Äôve got the sense that I‚Äôm naively just trying to hack something together on top of existing frameworks, and that there may be a more intelligent, ground-up way about this."
83,zxfnga,learnmachinelearning,GPT-3,relevance,2022-12-28 17:37:46,chatGPT peeps- anyone else learn new stuff best by actually building something?,bruclinbrocoli,False,0.65,4,https://www.reddit.com/r/learnmachinelearning/comments/zxfnga/chatgpt_peeps_anyone_else_learn_new_stuff_best_by/,4,1672249066.0,"[This intro to chatGPT](https://buildspace.so/notes/intro-to-chatgpt) has some cool (free) challenges at the end to build a telegram bot, a business email generator, or a writing assistant.

What else have people found to learn bout chatGPT that's not just theory?

&#x200B;

https://preview.redd.it/smxv4mzldo8a1.png?width=1026&format=png&auto=webp&s=43081abbfcad449817e520b5e92ba599a18a1525"
