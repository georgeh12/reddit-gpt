,id,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,url,num_comments,created,body
0,zhrgln,machinelearning,ChatGPT,top,2022-12-10 12:32:57,[P] I made a command-line tool that explains your errors using ChatGPT (link in comments),jsonathan,False,0.97,2863,https://i.redd.it/kq518l9ne25a1.gif,112,1670675577.0,
1,12nbixk,machinelearning,ChatGPT,top,2023-04-15 17:14:58,[P] OpenAssistant - The world's largest open-source replication of ChatGPT,ykilcher,False,0.97,1271,https://www.reddit.com/r/MachineLearning/comments/12nbixk/p_openassistant_the_worlds_largest_opensource/,175,1681578898.0,"We’re excited to announce the release of OpenAssistant.

The future of AI development depends heavily on high quality datasets and models being made publicly available, and that’s exactly what this project does.

Watch the annoucement video:

[https://youtu.be/ddG2fM9i4Kk](https://youtu.be/ddG2fM9i4Kk)

&#x200B;

Our team has worked tirelessly over the past several months collecting large amounts of text-based input and feedback to create an incredibly diverse and unique dataset designed specifically for training language models or other AI applications.

With over 600k human-generated data points covering a wide range of topics and styles of writing, our dataset will be an invaluable tool for any developer looking to create state-of-the-art instruction models!

To make things even better, we are making this entire dataset free and accessible to all who wish to use it. Check it out today at our HF org: OpenAssistant

On top of that, we've trained very powerful models that you can try right now at: [open-assistant.io/chat](https://open-assistant.io/chat) !"
2,121t6tp,machinelearning,ChatGPT,top,2023-03-25 17:41:20,[P] A 'ChatGPT Interface' to Explore Your ML Datasets -> app.activeloop.ai,davidbun,False,0.95,1058,https://v.redd.it/n5l842qa9xpa1,38,1679766080.0,
3,11mlwty,machinelearning,ChatGPT,top,2023-03-09 07:24:35,"[R] Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",MysteryInc152,False,0.97,870,https://www.reddit.com/gallery/11mlwty,26,1678346675.0,
4,11ybjsi,machinelearning,ChatGPT,top,2023-03-22 08:04:01,[D] Overwhelmed by fast advances in recent weeks,iamx9000again,False,0.96,825,https://www.reddit.com/r/MachineLearning/comments/11ybjsi/d_overwhelmed_by_fast_advances_in_recent_weeks/,331,1679472241.0,"I was watching the GTC keynote and became entirely overwhelmed by the amount of progress achieved from last year.  I'm wondering how everyone else feels.

&#x200B;

Firstly, the entire ChatGPT, GPT-3/GPT-4 chaos has been going on for a few weeks, with everyone scrambling left and right to integrate chatbots into their apps, products, websites. Twitter is flooded with new product ideas, how to speed up the process from idea to product, countless promp engineering blogs, tips, tricks, paid courses.

&#x200B;

Not only was ChatGPT disruptive, but a few days later, Microsoft and Google also released their models and integrated them into their search engines. Microsoft also integrated its LLM into its Office suite. It all happenned overnight. I understand that they've started integrating them along the way, but still, it seems like it hapenned way too fast. This tweet encompases the past few weeks perfectly [https://twitter.com/AlphaSignalAI/status/1638235815137386508](https://twitter.com/AlphaSignalAI/status/1638235815137386508) , on a random Tuesday countless products are released that seem revolutionary.

&#x200B;

In addition to the language models, there are also the generative art models that have been slowly rising in mainstream recognition. Now Midjourney AI is known by a lot of people who are not even remotely connected to the AI space.

&#x200B;

For the past few weeks, reading Twitter, I've felt completely overwhelmed, as if the entire AI space is moving beyond at lightning speed, whilst around me we're just slowly training models, adding some data, and not seeing much improvement, being stuck on coming up with ""new ideas, that set us apart"".

&#x200B;

Watching the GTC keynote from NVIDIA I was again, completely overwhelmed by how much is being developed throughout all the different domains. The ASML EUV (microchip making system) was incredible, I have no idea how it does lithography and to me it still seems like magic. The Grace CPU with 2 dies (although I think Apple was the first to do it?) and 100 GB RAM, all in a small form factor. There were a lot more different hardware servers that I just blanked out at some point. The omniverse sim engine looks incredible, almost real life (I wonder how much of a domain shift there is between real and sim considering how real the sim looks). Beyond it being cool and usable to train on synthetic data, the car manufacturers use it to optimize their pipelines. This change in perspective, of using these tools for other goals than those they were designed for I find the most interesting.

&#x200B;

The hardware part may be old news, as I don't really follow it, however the software part is just as incredible. NVIDIA AI foundations (language, image, biology models), just packaging everything together like a sandwich. Getty, Shutterstock and Adobe will use the generative models to create images. Again, already these huge juggernauts are already integrated.

&#x200B;

I can't believe the point where we're at. We can use AI to write code, create art, create audiobooks using Britney Spear's voice, create an interactive chatbot to converse with books, create 3D real-time avatars, generate new proteins (?i'm lost on this one), create an anime and countless other scenarios. Sure, they're not perfect, but the fact that we can do all that in the first place is amazing.

&#x200B;

As Huang said in his keynote, companies want to develop ""disruptive products and business models"". I feel like this is what I've seen lately. Everyone wants to be the one that does something first, just throwing anything and everything at the wall and seeing what sticks.

&#x200B;

In conclusion, I'm feeling like the world is moving so fast around me whilst I'm standing still. I want to not read anything anymore and just wait until everything dies down abit, just so I can get my bearings. However, I think this is unfeasible. I fear we'll keep going in a frenzy until we just burn ourselves at some point.

&#x200B;

How are you all fairing? How do you feel about this frenzy in the AI space? What are you the most excited about?"
5,128lo83,machinelearning,ChatGPT,top,2023-04-01 12:57:30,[R] [P] I generated a 30K-utterance dataset by making GPT-4 prompt two ChatGPT instances to converse.,radi-cho,False,0.96,800,https://i.redd.it/bywcz1kzs9ra1.png,104,1680353850.0,
6,11uk8ti,machinelearning,ChatGPT,top,2023-03-18 10:15:33,[D] Totally Open Alternatives to ChatGPT,KingsmanVince,False,0.98,746,https://www.reddit.com/r/MachineLearning/comments/11uk8ti/d_totally_open_alternatives_to_chatgpt/,70,1679134533.0,"I have migrated this to GitHub for easy contribution: https://github.com/nichtdax/awesome-totally-open-chatgpt

By alternative, I mean projects feature different language model for chat system.
I do **not** count alternative **frontend** projects because they just call the API from OpenAI. 
I do **not** consider alternative **transformer decoder** to GPT 3.5 either because the training data of them are (mostly) not for chat system.

Tags:

-   B: bare (no data, no model's weight, no chat system)
-   F: full (yes data, yes model's weight, yes chat system including TUI and GUI)

| Project                                                                               | Description                                                                                                                                                                                                                                                                                                                                                                               | Tags |
| ------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- |
| [lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch)       | Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM                                                                                                                                                                                                                                                      | B    |
| [togethercomputer/OpenChatKit](https://github.com/togethercomputer/OpenChatKit)       | OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. [Demo](https://huggingface.co/spaces/togethercomputer/OpenChatKit)                                                                                                                                                                                    | F    |
| [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) | A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.                                                                                                                                                                                                                                                                                    | F    |
| [KoboldAI/KoboldAI-Client](https://github.com/KoboldAI/KoboldAI-Client)               | This is a browser-based front-end for AI-assisted writing with multiple local & remote AI models. It offers the standard array of tools, including Memory, Author's Note, World Info, Save & Load, adjustable AI settings, formatting options, and the ability to import existing AI Dungeon adventures. You can also turn on Adventure mode and play the game like AI Dungeon Unleashed. | F    |
| [LAION-AI/Open-Assistant/](https://github.com/LAION-AI/Open-Assistant/)               | OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.                                                                                                                                                                                                                                     | F    |"
7,12ay0vt,machinelearning,ChatGPT,top,2023-04-03 21:11:52,"[P] The weights neccessary to construct Vicuna, a fine-tuned LLM with capabilities comparable to GPT3.5, has now been released",Andy_Schlafly,False,0.98,605,https://www.reddit.com/r/MachineLearning/comments/12ay0vt/p_the_weights_neccessary_to_construct_vicuna_a/,86,1680556312.0,"Vicuna is a large language model derived from LLaMA, that has been fine-tuned to the point of having 90% ChatGPT quality. The delta-weights, necessary to reconstruct the model from LLaMA weights have now been released, and can be used to build your own Vicuna.

https://vicuna.lmsys.org/"
8,120usfk,machinelearning,ChatGPT,top,2023-03-24 19:15:58,[R] Hello Dolly: Democratizing the magic of ChatGPT with open models,austintackaberry,False,0.98,599,https://www.reddit.com/r/MachineLearning/comments/120usfk/r_hello_dolly_democratizing_the_magic_of_chatgpt/,109,1679685358.0,"Databricks shows that anyone can take a dated off-the-shelf open source large language model (LLM) and give it magical ChatGPT-like instruction following ability by training it in less than three hours on one machine, using high-quality training data.

They fine tuned GPT-J using the Alpaca dataset.

Blog: [https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)  
Github: [https://github.com/databrickslabs/dolly](https://github.com/databrickslabs/dolly)"
9,11fbccz,machinelearning,ChatGPT,top,2023-03-01 18:31:12,[D] OpenAI introduces ChatGPT and Whisper APIs (ChatGPT API is 1/10th the cost of GPT-3 API),minimaxir,False,0.97,579,https://www.reddit.com/r/MachineLearning/comments/11fbccz/d_openai_introduces_chatgpt_and_whisper_apis/,119,1677695472.0,"https://openai.com/blog/introducing-chatgpt-and-whisper-apis

> It is priced at $0.002 per 1k tokens, which is 10x cheaper than our existing GPT-3.5 models.

This is a massive, massive deal. For context, the reason GPT-3 apps took off over the past few months before ChatGPT went viral is because a) text-davinci-003 was released and was a significant performance increase and b) the cost was cut from $0.06/1k tokens to $0.02/1k tokens, which made consumer applications feasible without a large upfront cost.

A much better model and a 1/10th cost warps the economics completely to the point that it may be better than in-house finetuned LLMs.

I have no idea how OpenAI can make money on this. This has to be a loss-leader to lock out competitors before they even get off the ground."
10,10gtruu,machinelearning,ChatGPT,top,2023-01-20 10:41:04,[N] OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic,ChubChubkitty,False,0.83,526,https://www.reddit.com/r/MachineLearning/comments/10gtruu/n_openai_used_kenyan_workers_on_less_than_2_per/,246,1674211264.0,https://time.com/6247678/openai-chatgpt-kenya-workers/
11,11h3p2x,machinelearning,ChatGPT,top,2023-03-03 15:37:03,[D] Facebooks LLaMA leaks via torrent file in PR,londons_explorer,False,0.98,525,https://www.reddit.com/r/MachineLearning/comments/11h3p2x/d_facebooks_llama_leaks_via_torrent_file_in_pr/,184,1677857823.0,"See here:
https://github.com/facebookresearch/llama/pull/73/files

Note that this PR *is not* made by a member of Facebook/Meta staff.    I have downloaded parts of the torrent and it does appear to be lots of weights, although I haven't confirmed it is trained as in the LLaMA paper, although it seems likely.


I wonder how much finetuning it would take to make this work like ChatGPT - finetuning tends to be much cheaper than the original training, so it might be something a community could do..."
12,10pb1y3,machinelearning,ChatGPT,top,2023-01-30 19:09:14,"[P] I launched “CatchGPT”, a supervised model trained with millions of text examples, to detect GPT created content",qthai912,False,0.75,498,https://www.reddit.com/r/MachineLearning/comments/10pb1y3/p_i_launched_catchgpt_a_supervised_model_trained/,206,1675105754.0,"I’m an ML Engineer at Hive AI and I’ve been working on a ChatGPT Detector.

Here is a free demo we have up: [https://hivemoderation.com/ai-generated-content-detection](https://hivemoderation.com/ai-generated-content-detection)

From our benchmarks it’s significantly better than similar solutions like GPTZero and OpenAI’s GPT2 Output Detector. On our internal datasets, we’re seeing balanced accuracies of >99% for our own model compared to around 60% for GPTZero and 84% for OpenAI’s GPT2 Detector.

Feel free to try it out and let us know if you have any feedback!"
13,121domd,machinelearning,ChatGPT,top,2023-03-25 06:54:55,[N] March 2023 - Recent Instruction/Chat-Based Models and their parents,michaelthwan_ai,False,0.98,460,https://i.redd.it/oz51w0t22upa1.png,50,1679727295.0,
14,11zsdwv,machinelearning,ChatGPT,top,2023-03-23 18:09:11,[N] ChatGPT plugins,Singularian2501,False,0.97,447,https://www.reddit.com/r/MachineLearning/comments/11zsdwv/n_chatgpt_plugins/,144,1679594951.0,"[https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins)

>We’ve implemented initial support for plugins in ChatGPT. Plugins are tools designed specifically for language models with safety as a core principle, and help ChatGPT access up-to-date information, run  computations, or use third-party services."
15,1095os9,machinelearning,ChatGPT,top,2023-01-11 14:12:57,[D] Microsoft ChatGPT investment isn't about Bing but about Cortana,fintechSGNYC,False,0.89,395,https://www.reddit.com/r/MachineLearning/comments/1095os9/d_microsoft_chatgpt_investment_isnt_about_bing/,173,1673446377.0,"I believe that Microsoft's 10B USD investment in ChatGPT is less about Bing and more about turning Cortana into an Alexa for corporates.   
Examples: Cortana prepare the new T&Cs... Cortana answer that client email... Cortana prepare the Q4 investor presentation (maybe even with PowerBI integration)... Cortana please analyze cost cutting measures... Cortana please look up XYZ... 

What do you think?"
16,11v6bvv,machinelearning,ChatGPT,top,2023-03-19 00:45:37,[P] Let's build ChatGPT,blatant_variable,False,0.96,365,https://www.reddit.com/r/MachineLearning/comments/11v6bvv/p_lets_build_chatgpt/,16,1679186737.0,"Hi all, I just made a tutorial on how to build a basic RLHF system on top of Andrej Karpathy's nanoGPT. I'm grateful to have gotten a thumbs up on Twitter from the legend himself, always a bit nerve wracking making this sort of thing.

I'm sharing this here because I'd love to go deeper into teaching and building this out, if people are interested in watching this sort of thing. Would be very helpful to hear your thoughts.

Here's the code:

https://github.com/sanjeevanahilan/nanoChatGPT

The video: 

https://m.youtube.com/watch?v=soqTT0o1ZKo&feature=youtu.be"
17,zstequ,machinelearning,ChatGPT,top,2022-12-22 18:39:30,[D] When chatGPT stops being free: Run SOTA LLM in cloud,_underlines_,False,0.95,350,https://www.reddit.com/r/MachineLearning/comments/zstequ/d_when_chatgpt_stops_being_free_run_sota_llm_in/,95,1671734370.0,"Edit: Found [LAION-AI/OPEN-ASSISTANT](https://github.com/LAION-AI/Open-Assistant) a very promising project opensourcing the idea of chatGPT. [video here](https://www.youtube.com/watch?v=8gVYC_QX1DI)

**TL;DR: I found GPU compute to be [generally cheap](https://github.com/full-stack-deep-learning/website/blob/main/docs/cloud-gpus/cloud-gpus.csv) and spot or on-demand instances can be launched on AWS for a few USD / hour up to over 100GB vRAM. So I thought it would make sense to run your own SOTA LLM like Bloomz 176B inference endpoint whenever you need it for a few questions to answer. I thought it would still make more sense than shoving money into a closed walled garden like ""not-so-OpenAi"" when they make ChatGPT or GPT-4 available for $$$. But I struggle due to lack of tutorials/resources.**

Therefore, I carefully checked benchmarks, model parameters and sizes as well as training sources for all SOTA LLMs [here](https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit#gid=1158069878).

Knowing since reading the Chinchilla paper that Model Scaling according to OpenAI was wrong and more params != better quality generation. So I was looking for the best performing LLM openly available in terms of quality and broadness to use for multilingual everyday questions/code completion/reasoning similar to what chatGPT provides (minus the fine-tuning for chat-style conversations).

My choice fell on [Bloomz](https://huggingface.co/bigscience/bloomz) (because that handles multi-lingual questions well and has good zero shot performance for instructions and Q&A style text generation. Confusingly Galactica seems to outperform Bloom on several benchmarks. But since Galactica had a very narrow training set only using scientific papers, I guess usage is probably limited for answers on non-scientific topics.

Therefore I tried running the original bloom 176B and alternatively also Bloomz 176B on AWS SageMaker JumpStart, which should be a one click deployment. This fails after 20min. On Azure ML, I tried using DeepSpeed-MII which also supports bloom but also fails due the instance size of max 12GB vRAM I guess.

From my understanding to save costs on inference, it's probably possible to use one or multiple of the following solutions:

- Precision: int8 instead of fp16
- [Microsoft/DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) for an up 40x reduction on inference cost on Azure, this thing also supports int8 and fp16 bloom out of the box, but it fails on Azure due to instance size.
- [facebook/xformer](https://github.com/facebookresearch/xformers) not sure, but if I remember correctly this brought inference requirements down to 4GB vRAM for StableDiffusion and DreamBooth fine-tuning to 10GB. No idea if this is usefull for Bloom(z) inference cost reduction though

I have a CompSci background but I am not familiar with most stuff, except that I was running StableDiffusion since day one on my rtx3080 using linux and also doing fine-tuning with DreamBooth. But that was all just following youtube tutorials. I can't find a single post or youtube video of anyone explaining a full BLOOM / Galactica / BLOOMZ inference deployment on cloud platforms like AWS/Azure using one of the optimizations mentioned above, yet alone deployment of the raw model. :(

I still can't figure it out by myself after 3 days.

**TL;DR2: Trying to find likeminded people who are interested to run open source SOTA LLMs for when chatGPT will be paid or just for fun.**

Any comments, inputs, rants, counter-arguments are welcome.

/end of rant"
18,13b6miy,machinelearning,ChatGPT,top,2023-05-07 23:26:29,"[D] ClosedAI license, open-source license which restricts only OpenAI, Microsoft, Google, and Meta from commercial use",wemsyn,False,0.8,347,https://www.reddit.com/r/MachineLearning/comments/13b6miy/d_closedai_license_opensource_license_which/,191,1683501989.0,"After reading [this article](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither), I realized it might be nice if the open-source AI community could exclude ""closed AI"" players from taking advantage of community-generated models and datasets. I was wondering if it would be possible to write a license that is completely permissive (like Apache 2.0 or MIT), except to certain companies, which are completely barred from using the software in any context.

Maybe this could be called the ""ClosedAI"" license. I'm not any sort of legal expert so I have no idea how best to write this license such that it protects model weights and derivations thereof.

I prompted ChatGPT for an example license and this is what it gave me:

    <PROJECT NAME> ClosedAI License v1.0
    
    Permission is hereby granted, free of charge, to any person or organization obtaining a copy of this software and associated documentation files (the ""Software""), to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, subject to the following conditions:
    
    1. The above copyright notice and this license notice shall be included in all copies or substantial portions of the Software.
    
    2. The Software and any derivative works thereof may not be used, in whole or in part, by or on behalf of OpenAI Inc., Google LLC, or Microsoft Corporation (collectively, the ""Prohibited Entities"") in any capacity, including but not limited to training, inference, or serving of neural network models, or any other usage of the Software or neural network weights generated by the Software.
    
    3. Any attempt by the Prohibited Entities to use the Software or neural network weights generated by the Software is a material breach of this license.
    
    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

No idea if this is valid or not. Looking for advice.

&#x200B;

**Edit:** Thanks for the input. Removed non-commercial clause (whoops, proofread what ChatGPT gives you). Also removed Meta from the excluded companies list due to popular demand."
19,1244q71,machinelearning,ChatGPT,top,2023-03-27 23:21:38,[D] FOMO on the rapid pace of LLMs,00001746,False,0.96,309,https://www.reddit.com/r/MachineLearning/comments/1244q71/d_fomo_on_the_rapid_pace_of_llms/,121,1679959298.0,"Hi all, 

I recently read [this reddit post](https://www.reddit.com/r/blender/comments/121lhfq/i_lost_everything_that_made_me_love_my_job/) about a 2D modeler experiencing an existential crisis about their job being disrupted by midjourney ([HN discussion here](https://news.ycombinator.com/item?id=35319861)). I can't help but feel the same as someone who has been working in the applied ML space for the past few years. 

Despite my background in ""classical"" ML, I'm feeling some anxiety about the rapid pace of LLM development and face a fear of missing out / being left behind.

I'd love to get involved again in ML research apart from my day job, but one of the biggest obstacles is the fact that training most of foundational LLM research requires huge compute more than anything else \[1\]. I understand that there are some directions in distributing compute ([https://petals.ml](https://petals.ml/)), or distilling existing models  ([https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)). 

I thought I might not be the only one being humbled by the recent advances in ChatGPT, etc. and wanted to hear how other people feel / are getting involved. 

\--

\[1\] I can't help but be reminded of Sutton's description of the [""bitter lesson"" of modern AI research](https://www.incompleteideas.net/IncIdeas/BitterLesson.html): ""breakthrough progress eventually arrives by an opposing approach based on scaling computation... eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach."""
20,zwht9g,machinelearning,ChatGPT,top,2022-12-27 15:13:00,[P] Can you distinguish AI-generated content from real art or literature? I made a little test!,Dicitur,False,0.93,292,https://www.reddit.com/r/MachineLearning/comments/zwht9g/p_can_you_distinguish_aigenerated_content_from/,126,1672153980.0,"Hi everyone, 

I am no programmer, and I have a very basic knowledge of machine learning, but I am fascinated by the possibilities offered by all the new models we have seen so far. 

Some people around me say they are not that impressed by what AIs can do, so I built a small test (with a little help by chatGPT to code the whole thing): can you always 100% distinguish between AI art or text and old works of art or literature?

Here is the site: http://aiorart.com/

I find that AI-generated text is still generally easy to spot, but of course it is very challenging to go against great literary works. AI images can sometimes be truly deceptive.

I wonder what you will all think of it... and how all that will evolve in the coming months!

PS: The site is very crude (again, I am no programmer!). It works though."
21,1271po7,machinelearning,ChatGPT,top,2023-03-30 22:40:29,[P] Introducing Vicuna: An open-source language model based on LLaMA 13B,Business-Lead2679,False,0.95,287,https://www.reddit.com/r/MachineLearning/comments/1271po7/p_introducing_vicuna_an_opensource_language_model/,107,1680216029.0,"We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%\* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%\* of cases. The cost of training Vicuna-13B is around $300. The training and serving [code](https://github.com/lm-sys/FastChat), along with an online [demo](https://chat.lmsys.org/), are publicly available for non-commercial use.

# Training details

Vicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model’s maximum context length.

Our training recipe builds on top of [Stanford’s alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) with the following improvements.

* **Memory Optimizations:** To enable Vicuna’s understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing [gradient checkpointing](https://arxiv.org/abs/1604.06174) and [flash attention](https://arxiv.org/abs/2205.14135).
* **Multi-round conversations:** We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot’s output.
* **Cost Reduction via Spot Instance:** The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ [SkyPilot](https://github.com/skypilot-org/skypilot) [managed spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html) to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.

&#x200B;

[Vicuna - Online demo](https://reddit.com/link/1271po7/video/0qsiu08kdyqa1/player)

# Limitations

We have noticed that, similar to other large language models, Vicuna has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. Additionally, it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI [moderation](https://platform.openai.com/docs/guides/moderation/overview) API to filter out inappropriate user inputs in our online demo. Nonetheless, we anticipate that Vicuna can serve as an open starting point for future research to tackle these limitations.

[Relative Response Quality Assessed by GPT-4](https://preview.redd.it/1rnmhv01eyqa1.png?width=599&format=png&auto=webp&s=02b4d415b5d378851bb70e225f1b1ebce98bfd83)

&#x200B;

For more information, check [https://vicuna.lmsys.org/](https://vicuna.lmsys.org/)

Online demo: [https://chat.lmsys.org/](https://chat.lmsys.org/)

&#x200B;

All credits go to the creators of this model. I did not participate in the creation of this model nor in the fine-tuning process. Usage of this model falls under a non-commercial license."
22,zzn35o,machinelearning,ChatGPT,top,2022-12-31 06:04:44,An Open-Source Version of ChatGPT is Coming [News],lambolifeofficial,False,0.88,265,https://metaroids.com/news/an-open-source-version-of-chatgpt-is-coming/,50,1672466684.0,
23,12dz4hh,machinelearning,ChatGPT,top,2023-04-06 21:45:18,[D] Is all the talk about what GPT can do on Twitter and Reddit exaggerated or fairly accurate?,ThePhantomguy,False,0.89,266,https://www.reddit.com/r/MachineLearning/comments/12dz4hh/d_is_all_the_talk_about_what_gpt_can_do_on/,311,1680817518.0,"I saw [this post](https://www.reddit.com/r/ChatGPT/comments/12diapw/gpt4_week_3_chatbots_are_yesterdays_news_ai/?utm_source=share&utm_medium=ios_app&utm_name=iossmf) on the r/ChatGPT subreddit, and I’ve been seeing similar talk on Twitter. There’s people talking about AGI, the singularity, and etc. I get that it’s cool, exciting, and fun; but some of the talk seems a little much? Like it reminds me of how the NFT bros would talk about blockchain technology.

Do any of the people making these kind of claims have a decent amount of knowledge on machine learning at all? The scope of my own knowledge is very limited, as I’ve only implemented and taken courses on models that are pretty old. So I’m here to ask for opinions from ya’ll. Is there some validity, or is it just people that don’t really understand what they’re saying and making grand claims (Like some sort of Dunning Kruger Effect)?"
24,13hrkbj,machinelearning,ChatGPT,top,2023-05-15 00:00:05,[D] On LLMs' ability to perform random sampling,bgighjigftuik,False,0.91,252,https://i.redd.it/l1uyl3k3gxza1.jpg,104,1684108805.0,"To me this is the most counterintuitive thing I've yet seen from a language model. In this case it was ChatGPT (3.5, the free version); but I can see this happening in any model.

Here's the prompt I used on a fresh session:

""Generate 60 random numbers coming from an exponential distribution with lambda=6.4. Don't write any code""

It outputs the following values:

0.013, 0.078, 0.266, 0.040, 0.117, 0.283, 0.048, 0.078, 0.181, 0.134, 0.017, 0.160, 0.052, 0.196, 0.096, 0.098, 0.019, 0.048, 0.141, 0.071, 0.106, 0.023, 0.197, 0.062, 0.029, 0.072, 0.111, 0.166, 0.080, 0.027, 0.124, 0.016, 0.135, 0.052, 0.078, 0.161, 0.078, 0.139, 0.068, 0.066, 0.039, 0.056, 0.018, 0.082, 0.180, 0.086, 0.024, 0.022, 0.057, 0.042, 0.050, 0.104, 0.099, 0.062, 0.016, 0.186, 0.043, 0.085, 0.145, 0.110.

I get to plot a histogram with the numbers and they are close to what I would consider an exponential distribution with such parameter (you can see the plot).

Given that GPT 3.5 does not have access to a Python interpreter, how on earth is it able to do so? I have also tried other distributions and parameters and it kind of works. It's not perfect, but with normal distributions it is usually close to what scipy.stats would generate.

I could understand that it can have learnt to interpret Python code to some extent, but honestly I can't find explanation for random sampling from a probability distribution. For a Normal distribution, I can tell it about the desired mean and variance, and it samples values that are more than reasonable (and close to the true mean/variance specified).

Any thoughts? I honestly am unable to wrap my head around how a LLM can have the understanding on how to sample tokens (at digit level) to fit any probability distribution. To me it seems very unlikely to have similar data either the pre-training or fine-tuning stages."
25,zjbsie,machinelearning,ChatGPT,top,2022-12-11 22:16:43,"[D] - Has Open AI said what ChatGPT's architecture is? What technique is it using to ""remember"" previous prompts?",029187,False,0.95,247,https://www.reddit.com/r/MachineLearning/comments/zjbsie/d_has_open_ai_said_what_chatgpts_architecture_is/,88,1670797003.0,"Has Open AI said what ChatGPT's architecture is? What technique is it using to ""remember"" previous prompts? Have they come up with some way to add recurrence to the transformer or is it just using a feedforward sliding window approach?"
26,11f29f9,machinelearning,ChatGPT,top,2023-03-01 12:14:49,[R] ChatGPT failure increase linearly with addition on math problems,Neurosymbolic,False,0.94,239,https://www.reddit.com/r/MachineLearning/comments/11f29f9/r_chatgpt_failure_increase_linearly_with_addition/,66,1677672889.0," We did a study on ChatGPT's performance on math word problems. We found, under several conditions, its probability of failure increases linearly with the number of addition and subtraction operations - see below. This could imply that multi-step inference is a limitation. The performance also changes drastically when you restrict ChatGPT from showing its work (note the priors in the figure below, also see detailed breakdown of responses in the paper).

&#x200B;

[Math problems adds and subs vs. ChatGPT prob. of failure](https://preview.redd.it/z88ey3n6d4la1.png?width=1451&format=png&auto=webp&s=6da125b7a7cd60022ca70cd26434af6872a50d12)

ChatGPT Probability of Failure increase with addition and subtraction operations.

You the paper (preprint: [https://arxiv.org/abs/2302.13814](https://arxiv.org/abs/2302.13814)) will be presented at AAAI-MAKE next month. You can also check out our video here: [https://www.youtube.com/watch?v=vD-YSTLKRC8](https://www.youtube.com/watch?v=vD-YSTLKRC8)

&#x200B;

https://preview.redd.it/k58sbjd5d4la1.png?width=1264&format=png&auto=webp&s=5261923a2689201f905a26f06c6b5e9bac2fead6"
27,zc5sg6,machinelearning,ChatGPT,top,2022-12-04 09:57:44,[D] OpenAI’s ChatGPT is unbelievable good in telling stories!,Far_Pineapple770,False,0.91,240,https://www.reddit.com/r/MachineLearning/comments/zc5sg6/d_openais_chatgpt_is_unbelievable_good_in_telling/,85,1670147864.0,"
I started playing with ChatGPT, the new chatbot created by OpenAI and is free to use at the moment. 

I asked it to describe a scene between Batman and Joker on a subway platform and the result is mind blowing! I let you have a read of the story yourself:

Generated by ChatGPT:

The scene is set in a dimly lit subway station, with the Joker standing on the platform, gazing up at Batman who is perched on a nearby overhead beam. The Joker is dressed in his signature purple suit and green hair, a wicked grin plastered across his face.

In his hand, he holds a deck of playing cards, flipping them expertly through his fingers. The sound of the cards shuffling echoes through the empty station.

Batman, dressed in his dark suit and cape, stares down at the Joker with a steely gaze. His muscles are tense, ready for any sudden movements from the villain.

The Joker cackles, his laughter bouncing off the walls of the station. ""Hello, Batsy! Long time no see. How's the crime-fighting business these days?""

Batman remains silent, watching the Joker carefully.

The Joker's grin widens. ""Come on, Bats. Don't be so serious all the time. Let's have some fun."" He tosses a card at Batman, who easily catches it.

""I'm not here to play games, Joker,"" Batman says in a grave tone.

The Joker's laughter turns into a maniacal cackle. ""But that's the whole point, Batsy! Life is a game. And I'm the master of it.""

He pulls out a bomb from his pocket and holds it up, the timer ticking down. ""Let's see how well you can play, shall we?""

Batman leaps down from the beam, landing gracefully on the platform. He advances towards the Joker, who backs away, still holding the bomb.

The tension in the air is palpable as the two foes face off, each ready to outsmart the other. The sound of the bomb's timer counts down, adding to the tension of the scene.

The Joker's grin never fades, even as Batman inches closer. ""It's always a pleasure to play with you, Batsy. But I think it's time for me to make my exit.""

With a flourish, the Joker disappears in a cloud of smoke, leaving Batman standing alone on the platform. The bomb detonates, causing a small explosion and sending debris flying.

But Batman remains unharmed, ready to continue the never-ending battle against the Joker and all the evil that plagues the city.

End of the story.

I used the first two paragraphs in OpenAI’s Dall E 2 to generate an image and [this](https://labs.openai.com/s/p8L8Z6c3S7qZ26U7owfbx7lS) is the result I got.

Please keep in mind that this is the very beginning of this tool and think about the endless possibilities it can create."
28,12yqhmo,machinelearning,ChatGPT,top,2023-04-25 17:45:33,"[N] Microsoft Releases SynapseMl v0.11 with support for ChatGPT, GPT-4, Causal Learning, and More",mhamilton723,False,0.95,240,https://www.reddit.com/r/MachineLearning/comments/12yqhmo/n_microsoft_releases_synapseml_v011_with_support/,22,1682444733.0,"Today Microsoft launched SynapseML v0.11, an open-source library designed to make it easy to create distributed ml systems. SynapseML v0.11 introduces support for ChatGPT, GPT-4, distributed training of huggingface and torchvision models, an ONNX Model hub integration, Causal Learning with EconML, 10x memory reductions for LightGBM, and a newly refactored integration with Vowpal Wabbit. To learn more:

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.11.0](https://github.com/microsoft/SynapseML/releases/tag/v0.11.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919)

Thank you to all the contributors in the community who made the release possible!

&#x200B;

https://preview.redd.it/kobq2t1gi2wa1.png?width=4125&format=png&auto=webp&s=125f63b63273191a58833ced87f17cb108e4c1ee"
29,12yr1eq,machinelearning,ChatGPT,top,2023-04-25 18:05:32,"[P] HuggingChat (open source ChatGPT, interface + model)",lorepieri,False,0.94,240,https://www.reddit.com/r/MachineLearning/comments/12yr1eq/p_huggingchat_open_source_chatgpt_interface_model/,58,1682445932.0,[https://huggingface.co/chat/](https://huggingface.co/chat/)
30,12vaauo,machinelearning,ChatGPT,top,2023-04-22 15:59:25,[P] Easily make complex plots using ChatGPT [open source],ofirpress,False,0.9,236,https://v.redd.it/gz8mwx5okgva1,22,1682179165.0,
31,12b9tx7,machinelearning,ChatGPT,top,2023-04-04 05:20:04,[D] Closed AI Models Make Bad Baselines,leondz,False,0.95,232,https://www.reddit.com/r/MachineLearning/comments/12b9tx7/d_closed_ai_models_make_bad_baselines/,56,1680585604.0,"> That which is not open and reasonably reproducible cannot be considered a requisite baseline.

""What comes below is an attempt to bring together some discussions on the state of NLP research post-chatGPT.""

  https://hackingsemantics.xyz/2023/closed-baselines/

Interested to hear thoughts on this. Closed APIs with moving code behind them seem to be terrible bases for comparison, and demanding comparison with one shouldn't really be a way of blocking a publication, should it?"
32,1135tir,machinelearning,ChatGPT,top,2023-02-15 19:07:24,"[D] GLM 130B (Chinese-English Bilingual model) translations vs Google, Deepl Translate, NLLB and chatGPT",MysteryInc152,False,0.96,223,https://www.reddit.com/gallery/1135tir,38,1676488044.0,
33,zx7cxn,machinelearning,ChatGPT,top,2022-12-28 11:32:22,[D] DeepMind has at least half a dozen prototypes for abstract/symbolic reasoning. What are their approaches?,valdanylchuk,False,0.97,213,https://www.reddit.com/r/MachineLearning/comments/zx7cxn/d_deepmind_has_at_least_half_a_dozen_prototypes/,35,1672227142.0,"In TED Interview on the future of AI from three months ago, Demis Hassabis says he spends most of his time on the problem of abstract concepts, conceptual knowledge, and approaches to move deep learning systems into the realm of symbolic reasoning and mathematical discovery. He says at DeepMind they have at least half a dozen internal prototype projects working in that direction:

https://youtu.be/I5FrFq3W25U?t=2550

Earlier, around the 28min mark, he says that while current LLMs are very impressive, they are nowhere near reaching sentience or consciousness, among other things, because they are very data-inefficient in their learning. 

Can we infer their half dozen approaches to abstract reasoning from the research published by DeepMind so far? Or is this likely to be some yet unreleased new research?

DeepMind list many (not sure if all) of their papers here:

https://www.deepmind.com/research

I was able to find some related papers there, but I am not qualified to judge their significance, and I probably missed some important ones because of the less obvious titles. 

https://www.deepmind.com/publications/symbolic-behaviour-in-artificial-intelligence

https://www.deepmind.com/publications/discovering-symbolic-models-from-deep-learning-with-inductive-biases

https://www.deepmind.com/publications/neural-symbolic-vqa-disentangling-reasoning-from-vision-and-language-understanding

https://www.deepmind.com/publications/learning-symbolic-physics-with-graph-networks

https://www.deepmind.com/publications/how-to-transfer-algorithmic-reasoning-knowledge-to-learn-new-algorithms

https://www.deepmind.com/publications/a-simple-approach-for-state-action-abstractionusing-a-learned-mdp-homomorphism

Can anyone help summarize the approaches currently considered promising in this problem? Are we missing something bigger coming up behind all the hype around ChatGPT?"
34,126oiey,machinelearning,ChatGPT,top,2023-03-30 14:18:50,[D] AI Policy Group CAIDP Asks FTC To Stop OpenAI From Launching New GPT Models,vadhavaniyafaijan,False,0.84,205,https://www.reddit.com/r/MachineLearning/comments/126oiey/d_ai_policy_group_caidp_asks_ftc_to_stop_openai/,213,1680185930.0,"The Center for AI and Digital Policy (CAIDP), a tech ethics group, has asked the Federal Trade Commission to investigate OpenAI for violating consumer protection rules. CAIDP claims that OpenAI's AI text generation tools have been ""biased, deceptive, and a risk to public safety.""

CAIDP's complaint raises concerns about potential threats from OpenAI's GPT-4 generative text model, which was announced in mid-March. It warns of the potential for GPT-4 to produce malicious code and highly tailored propaganda and the risk that biased training data could result in baked-in stereotypes or unfair race and gender preferences in hiring. 

The complaint also mentions significant privacy failures with OpenAI's product interface, such as a recent bug that exposed OpenAI ChatGPT histories and possibly payment details of ChatGPT plus subscribers.

CAIDP seeks to hold OpenAI accountable for violating Section 5 of the FTC Act, which prohibits unfair and deceptive trade practices. The complaint claims that OpenAI knowingly released GPT-4 to the public for commercial use despite the risks, including potential bias and harmful behavior. 

[Source](https://www.theinsaneapp.com/2023/03/stop-openai-from-launching-gpt-5.html) | [Case](https://www.caidp.org/cases/openai/)| [PDF](https://www.caidp.org/app/download/8450269463/CAIDP-FTC-Complaint-OpenAI-GPT-033023.pdf)"
35,11wt2fl,machinelearning,ChatGPT,top,2023-03-20 19:30:55,[P] OpenAssistant is now live on reddit (Open Source ChatGPT alternative),pixiegirl417,False,0.98,205,https://www.reddit.com/r/MachineLearning/comments/11wt2fl/p_openassistant_is_now_live_on_reddit_open_source/,29,1679340655.0,"OpenAssistant bot is live on /r/ask_open_assistant. There are some limitations to the reddit bot; you can also try on the model in chat mode at https://huggingface.co/spaces/olivierdehaene/chat-llm-streaming. Model is available for free download at https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b.


Prompt it by creating a new text post (responds to text body of post), starting a comment with !OpenAssistant, or by replying directly to it. 

I have recently enabled memory for the bot so it should do a (pretty mediocre) job of continuing a conversation with you."
36,zn0juq,machinelearning,ChatGPT,top,2022-12-15 23:57:18,[P] Medical question-answering without hallucinating,tmblweeds,False,0.94,179,https://www.reddit.com/r/MachineLearning/comments/zn0juq/p_medical_questionanswering_without_hallucinating/,50,1671148638.0,"**tl;dr**I built a site that uses GPT-3.5 to answer natural-language medical questions using peer-reviewed medical studies.

**Live demo:** [**https://www.glaciermd.com/search**](https://www.glaciermd.com/search?utm_campaign=reddit_post_1)

**Background**

I've been working for a while on building a better version of WebMD, and I recently started playing around with LLMs, trying to figure out if there was anything useful there.

The problem with the current batch of ""predict-next-token"" LLMs is that they hallucinate—you can ask ChatGPT to answer medical questions, but it'll either

1. Refuse to answer (not great)
2. Give a completely false answer (really super bad)

So I spent some time trying to coax these LLMs to give answers based on a very specific set of inputs (peer-reviewed medical research) to see if I could get more accurate answers. And I did!

The best part is you can actually trace the final answer back to the original sources, which will hopefully instill some confidence in the result.

Here's how it works:

1. User types in a question
2. Pull top \~800 studies from Semantic Scholar and Pubmed
3. Re-rank using `sentence-transformers/multi-qa-MiniLM-L6-cos-v1`
4. Ask `text-davinci-003` to answer the question based on the top 10 studies (if possible)
5. Summarize those answers using `text-davinci-003`

Would love to hear what people think (and if there's a better/cheaper way to do it!).

\---

**UPDATE 1:** So far the #1 piece of feedback has been that I should be *way* more explicit about the fact that this is a proof-of-concept and not meant to be taken seriously. To that end, I've just added a screen that explains this and requires you to acknowledge it before continuing.

&#x200B;

https://preview.redd.it/jrt0yv3rfb6a1.png?width=582&format=png&auto=webp&s=38021decdfc7ed4bc3fe8caacaee2d09cd9b541e

Thoughts?

**Update 2:** Welp that's all the $$$ I have to spend on OpenAI credits, so the full demo isn't running anymore. But you can still follow the link above and browse existing questions/answers. Thanks for all the great feedback!"
37,11njpb9,machinelearning,ChatGPT,top,2023-03-10 08:50:32,[D] Is ML a big boys game now?,TheStartIs2019,False,0.83,178,https://www.reddit.com/r/MachineLearning/comments/11njpb9/d_is_ml_a_big_boys_game_now/,146,1678438232.0,"As much as I enjoy ML as a whole, I am a bit skeptical of the future for individuals. With OpenAI trying to monopolize the market along with Microsoft, which part remains for the small time researchers/developers?

It seems everything now is just a ChatGPT wrapper, and with GPT-4 around the corner I assume itll be even more prominent.

What are your thoughts?"
38,13ijfrb,machinelearning,ChatGPT,top,2023-05-15 20:27:43,[P] abstracts-search: A semantic search engine indexing 95 million academic publications,colonel_watch,False,0.95,174,https://www.reddit.com/r/MachineLearning/comments/13ijfrb/p_abstractssearch_a_semantic_search_engine/,18,1684182463.0,"This was an interesting side project! I generated embeddings from the titles and abstracts of 95 million academic publications taken from the publicly-available [OpenAlex](https://openalex.org/) dataset and put them all into a single semantic search engine.

By now, this is a classic method, but I've been fascinated by seeing where it works and where it doesn't. So far, I've had success describing the content of a possible research paper in natural language then seeing what people have actually done. I've also had ChatGPT hallucinate a paper, that response being used to find real papers. On the other hand, I've seen it fall flat on an acronym or two.

You can try it out on a publicly-hosted instance at Hugging Face: [https://huggingface.co/spaces/colonelwatch/abstracts-index](https://huggingface.co/spaces/colonelwatch/abstracts-index)

I'm releasing the entire project as open source and open data. All \~600 lines of Python, 69 GB in embeddings, and the raw faiss index can be found through [https://github.com/colonelwatch/abstracts-search](https://github.com/colonelwatch/abstracts-search)

Feedback is welcome. As much as I've fumbled around with Google Scholar, I'd like to know what people actually expect out of academic search engines.

&#x200B;

>EDIT 03:49pm: Caused a bug trying to fix an edge case that showed up in the logs, should be back up and running in a couple minutes  
>  
>EDIT 03:56pm: Back online!  
>  
>EDIT 08:27pm: My logs are saying people are running into another edge case about `null`\-named authors, and the fix I pushed isn't triggering an update. Lesson learned about data cleaning! I'll try restarting the hosted instance and see how it fares in a couple minutes  
>  
>EDIT 08:43pm: Restart completed"
39,129qi8p,machinelearning,ChatGPT,top,2023-04-02 16:39:23,[R] HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace - Yongliang Shen et al Microsoft Research Asia 2023 - Able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results!,Singularian2501,False,0.94,173,https://www.reddit.com/r/MachineLearning/comments/129qi8p/r_hugginggpt_solving_ai_tasks_with_chatgpt_and/,30,1680453563.0,"Paper: [https://arxiv.org/abs/2303.17580](https://arxiv.org/abs/2303.17580) 

Abstract:

>Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in HuggingFace, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which **paves a new way towards AGI.** 

https://preview.redd.it/huc5so9f1ira1.jpg?width=1201&format=pjpg&auto=webp&s=cd714263f8a6ea443195316d95704fd550beee95

https://preview.redd.it/d2dfhs9f1ira1.jpg?width=655&format=pjpg&auto=webp&s=07fcb2b969cdaaf649aed259296f3dfa9157531e

https://preview.redd.it/v4gc9r9f1ira1.jpg?width=773&format=pjpg&auto=webp&s=b014fa679a7bdc2024a3d27690950be2248735aa"
40,12gr91a,machinelearning,ChatGPT,top,2023-04-09 18:25:12,[D] The Complete Guide to Spiking Neural Networks,s_arme,False,0.94,173,https://www.reddit.com/r/MachineLearning/comments/12gr91a/d_the_complete_guide_to_spiking_neural_networks/,34,1681064712.0,"Greetings, r/MachineLearning community!  
Spiking Neural Networks (SNNs) are a type of Neural Networks that mimic the way neurons in the brain work. These networks are capable of producing temporal responses, and this makes them particularly interesting where power efficiency is important. They are [trending](https://trends.google.com/trends/explore/TIMESERIES/1681063800?hl=en-GB&tz=-120&date=2012-01-09+2023-03-09&q=%2Fm%2F02q3qrf&sni=3) (not as much as chatgpt), yet more research is needed to become mainstream in certain tasks.

I wrote this guide to cover fundamentals, advantages and caveats that needs to be addressed. I hope you enjoy it. Any thoughts or feedback is appreciated!

[https://pub.towardsai.net/the-complete-guide-to-spiking-neural-networks-d0a85fa6a64](https://pub.towardsai.net/the-complete-guide-to-spiking-neural-networks-d0a85fa6a64)"
41,10l9tet,machinelearning,ChatGPT,top,2023-01-25 21:10:17,"[R] Blogpost on comparing Chatbots like ChatGPT, LaMDA, Sparrow, BlenderBot 3, and Claude",emailnazneen,False,0.94,162,https://www.reddit.com/r/MachineLearning/comments/10l9tet/r_blogpost_on_comparing_chatbots_like_chatgpt/,5,1674681017.0,"[https://huggingface.co/blog/dialog-agents](https://huggingface.co/blog/dialog-agents) breaks down the techniques behind ChatGPT -- instruction fine-tuning, supervised fine-tuning, chain-of-thought, read teaming, and more.

https://preview.redd.it/fv16fsemd9ea1.png?width=889&format=png&auto=webp&s=a8f24de27c40a946fec64eaa674f81ddef0d0cc3"
42,zh2u3k,machinelearning,ChatGPT,top,2022-12-09 17:16:24,[R] Illustrating Reinforcement Learning from Human Feedback (RLHF),robotphilanthropist,False,0.96,142,https://www.reddit.com/r/MachineLearning/comments/zh2u3k/r_illustrating_reinforcement_learning_from_human/,13,1670606184.0,"New HuggingFace blog post on RLHF: [https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf)

Motivated by ChatGPT and the lack of conceptually focused resources on the topic."
43,zikps2,machinelearning,ChatGPT,top,2022-12-11 08:25:59,"[P] I made a tool that auto-saves your ChatGPT conversations and adds a ""Chat History"" button on the website.",silentx09,False,0.95,139,https://www.reddit.com/r/MachineLearning/comments/zikps2/p_i_made_a_tool_that_autosaves_your_chatgpt/,13,1670747159.0,"[savegpt.com](https://savegpt.com/) is a browser extension available both on the Chrome webstore and Firefox addons.

https://reddit.com/link/zikps2/video/5zinkph4b85a1/player"
44,122q3h7,machinelearning,ChatGPT,top,2023-03-26 15:38:08,[P] Using ChatGPT plugins with LLaMA,balthierwings,False,0.96,133,https://blog.lastmileai.dev/using-openais-retrieval-plugin-with-llama-d2e0b6732f14,24,1679845088.0,
45,1088rnw,machinelearning,ChatGPT,top,2023-01-10 12:35:07,[N] Microsoft Considers $10 Billion Investment in ChatGPT Creator --Bloomberg News,bikeskata,False,0.95,125,https://www.reddit.com/r/MachineLearning/comments/1088rnw/n_microsoft_considers_10_billion_investment_in/,42,1673354107.0,"Story here: https://www.bloomberg.com/news/articles/2023-01-10/microsoft-weighs-10-billion-chatgpt-investment-semafor-says?srnd=premium

Unpaywalled: https://archive.ph/XOOlg"
46,10eh2f3,machinelearning,ChatGPT,top,2023-01-17 16:54:30,"[P] RWKV 14B Language Model & ChatRWKV : pure RNN (attention-free), scalable and parallelizable like Transformers",bo_peng,False,0.99,117,https://www.reddit.com/r/MachineLearning/comments/10eh2f3/p_rwkv_14b_language_model_chatrwkv_pure_rnn/,21,1673974470.0,"Hi everyone. I am training my RWKV 14B ( [https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM) ) on the Pile (332B tokens) and it is getting closer to GPT-NeoX 20B level. You can already try the latest checkpoint.

https://preview.redd.it/7ycdftmjvmca1.png?width=1174&format=png&auto=webp&s=860a41193f1a254299d48a173756ecd66ccbc75b

RWKV is a RNN that also works as a linear transformer (or we may say it's a linear transformer that also works as a RNN). So it has both parallel & serial mode, and you get the best of both worlds (fast and saves VRAM).

At this moment, RWKV might be the only pure RNN that scales like usual transformers for language modeling, without using any QKV attention. It's great at preserving long context (unlike LSTM).

Moreover, you get smooth spike-free carefree training experience (bf16 & Adam):

https://preview.redd.it/0g3lrg6mvmca1.png?width=871&format=png&auto=webp&s=b4de1af4831ec359079cf99c41df8aa9591d48b0

As a proof of concept, I present ChatRWKV ( [https://github.com/BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV) ). It's not instruct-tuned yet, and there are few conversations in the Pile, so don't expect great quality. But it's already fun. Chat examples (using slightly earlier checkpoints): 

https://preview.redd.it/zyqni6bpvmca1.png?width=1084&format=png&auto=webp&s=038fd2eab524c36d8aa2a8720a2caa3eb420df5b

https://preview.redd.it/xhje4j7qvmca1.png?width=1200&format=png&auto=webp&s=7e8597d2370f9f87230560dac7f5439520384dd9

And you can chat with the bot (or try free generation) in RWKV Discord (link in Github readme: [https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM) ). This is an open source project and let's build together."
47,13fzf2m,machinelearning,ChatGPT,top,2023-05-12 22:39:24,[R] DetGPT: Detect What You Need via Reasoning,OptimalScale_2023,False,0.89,116,https://www.reddit.com/r/MachineLearning/comments/13fzf2m/r_detgpt_detect_what_you_need_via_reasoning/,10,1683931164.0,"https://reddit.com/link/13fzf2m/video/fwcuwd3q9hza1/player

Throughout history, humans have dreamed of robots that could assist them with their daily lives and work. With the emergence of home assistants and OpenAI's Copilot, requests such as 'Please lower the temperature of the air conditioning' or even 'Please help me build an online store' have become possible.The emergence of GPT-4 has further demonstrated the potential of multimodal large models in visual understanding. In the open-source small model space, LLAVA and minigpt-4 have performed well in image recognition and chat, and can even suggest recipes for food images. However, these models still face significant challenges in practical implementation: they lack accurate localization capabilities and cannot provide specific locations of objects in images, nor can they understand complex human instructions to detect specific objects, making it difficult for them to perform specific tasks as requested by humans. In practical scenarios, if people could simply take a photo and ask an intelligent assistant for the correct answer to a complex problem, such a 'take a photo and ask' feature would be incredibly cool.  
To implement the ""**take a photo and ask**"" feature, robots need to have several capabilities:

1. Language understanding: the ability to listen and understand human intentions.
2. Visual understanding: the ability to understand the objects in the image.
3. Common sense reasoning: the ability to convert complex human intentions into precise and locatable targets.
4. Object localization: the ability to locate and detect corresponding objects in the image.

Currently, only a few large models (such as Google's PaLM-E) possess all four of these capabilities. However, researchers from the Hong Kong University of Science and Technology and the University of Hong Kong have proposed an open-source model called DetGPT (DetectionGPT), which only needs to fine-tune three million parameters to easily acquire complex reasoning and local object localization capabilities that can be generalized to most scenarios. This means that the model can easily recognize the objects that humans are interested in through self-knowledge reasoning and understand abstract human instructions. They have already developed a ""take a photo and ask"" demo using the model, which can be experienced online: [https://detgpt.github.io/](https://detgpt.github.io/)DetGPT allows users to operate everything with natural language without the need for complex commands or interfaces. In addition, DetGPT has intelligent reasoning and object detection capabilities, which can accurately understand user needs and intentions. For example, if a human gives a language instruction, ""I want to have a cold beverage,"" the robot first searches for a cold drink in the scene but does not find any. It then begins to think, ""There is no visible beverage. Where can I find it?"" Through its powerful common sense reasoning ability, the model realizes that the fridge is a possible location and scans the scene to successfully locate the drink!

https://preview.redd.it/ai8j05uy9hza1.png?width=1280&format=png&auto=webp&s=c8d833e2db63d0ebceb1c99aa68d89cc7fa7dcc7

  
Online demo: [https://detgpt.github.io/](https://detgpt.github.io/) 

Open-source code: [https://github.com/OptimalScale/DetGPT](https://github.com/OptimalScale/DetGPT)

&#x200B;

## Online demo: [https://detgpt.github.io/](https://detgpt.github.io/)

Feeling thirsty in the summer? DetGPT easily understands and finds the refrigerator with the image of where the iced beverages are.

https://preview.redd.it/kiiv4tb1ahza1.jpg?width=1280&format=pjpg&auto=webp&s=49a055fafd1c4e50cea46723bc567896ec60499e

Need to wake up early tomorrow? DetGPT makes it easy with an electronic alarm clock.

https://preview.redd.it/0lby9hh2ahza1.png?width=1280&format=png&auto=webp&s=e6fc77356d080fe755310dbc74879ac4f7a8b894

Do you suffer from hypertension and fatigue? Are you unsure of what fruits to buy at the market to help alleviate your symptoms? DetGPT acts as your nutrition teacher and provides guidance on which fruits can help relieve hypertension.

https://preview.redd.it/c1r7kwv3ahza1.png?width=1280&format=png&auto=webp&s=169fb015df8e9973c48a26a35caeb5892ce1d92f

Stuck in the Zelda game and can't pass it? DetGPT helps you disguise yourself and get past the challenges in the Gerudo Town.

https://preview.redd.it/wdny0v55ahza1.png?width=1280&format=png&auto=webp&s=070de46239405993eefeb5112bd4a459baec94df

Unsure of potential dangers in your surroundings within the range of the image? DetGPT acts as your safety officer and helps protect you from any potential risks.

https://preview.redd.it/nf64a176ahza1.png?width=1280&format=png&auto=webp&s=f6b641c2163076f5403361561c95663450227cd1

What items in the image could be dangerous for children? DetGPT still has got you covered.

https://preview.redd.it/oz8hx987ahza1.png?width=1280&format=png&auto=webp&s=b2d8ad27ff758a2d39e87fba86f7cc5a2b4a2c76

## Features of DetGPT

DetGPT has several unique features:

1. It has a significantly improved understanding of specific objects in images. Compared to previous models that use multimodal dialogues, DetGPT can retrieve and locate target objects from images based on the user's instructions, rather than simply describing the entire image.
2. It can understand complex human instructions, which lowers the barrier for users to ask questions. For example, the model can understand the question ""find fruits that can relieve hypertension?"" Traditional object detection requires humans to know the answer and pre-set the detection category, such as ""banana.""
3. DetGPT can use existing LLM knowledge to reason and accurately locate the corresponding object in the image that can solve more complex tasks. For complex tasks, such as ""fruits that can relieve hypertension,"" DetGPT can reason step by step: relieving hypertension -> potassium can relieve hypertension -> bananas are rich in potassium -> bananas can relieve hypertension -> need to identify the object banana.
4. It provides answers beyond human common sense. For some uncommon questions, such as which fruits are rich in potassium, the model can provide answers based on existing knowledge.

## A new direction: reasoning-based object detection

Traditional object detection tasks require pre-defined categories of possible objects for detection. However, providing accurate and comprehensive descriptions of the objects to be detected can be difficult and unrealistic for humans. This is due to the limitations of human memory and knowledge. For instance, a doctor may recommend that people with hypertension eat fruits rich in potassium, but may not know which specific fruits are rich in potassium, making it impossible to provide specific fruit names for the model to detect. If the question ""Identify fruits that can help alleviate hypertension"" could be directly posed to the detection model, humans would only need to take a photo, and the model could think, reason, and detect fruits rich in potassium, making the problem much simpler.Moreover, the examples of object categories provided by humans are not always comprehensive. For instance, if monitoring is required to detect behaviors that violate public order relative to public places, humans may only be able to provide a few simple scenarios, such as holding a knife or smoking. However, if the question ""detect behaviors that violate public order"" is directly posed to the detection model, the model can think and reason based on its own knowledge, thus capturing more unacceptable behaviors and generalizing to more relevant categories that need to be detected. After all, the knowledge that ordinary humans have access to is limited, and the object categories that they can provide examples of are also limited. However, if there is a big brain-like ChatGPT-like model to assist and reason, the instructions that humans need to provide will be much simpler, and the obtained answers will be much more accurate and comprehensive.To address the limitations of human instructions and their abstract nature, researchers from the Hong Kong University of Science and Technology and the University of Hong Kong have proposed a new direction called ""reasoning-based object detection."" In simple terms, humans give complex tasks, and the model can understand and reason about which objects in the image might be able to complete the task, and then detect them. For example, if a person describes ""I want to drink a cold drink, where can I find it,"" and the model sees a picture of a kitchen, it can detect the ""refrigerator."" This topic requires the perfect combination of multimodal models' image understanding ability and the rich knowledge stored in language models. It is used in fine-grained detection scenarios to accurately locate objects of interest to humans in images without pre-defined object categories.  


# The Approach

&#x200B;

https://preview.redd.it/ho9ux1pcahza1.png?width=1280&format=png&auto=webp&s=bf42e1baffa2925e8b946b191766ca116aec2fe1

The ""reasoning-based object detection"" is a challenging problem because the detector needs to understand and reason about the user's coarse-grained/abstract instructions and analyze the current visual information to locate the target object accurately. In this direction, researchers from the Hong Kong University of Science and Technology and the University of Hong Kong have conducted some preliminary explorations. Specifically, they use a pre-trained visual encoder (BLIP-2) to extract visual features from images and align the visual features to the text space using an alignment function. They use a large-scale language model (Robin/Vicuna) to understand the user's question, combined with the visual information they see, to reason about the objects that users are truly interested in. Then, they provide the object names to the pre-trained detector (Grounding-DINO) for specific location prediction. In this way, the model can analyze the image based on any user instructions and accurately predict the location of the object of interest to the user.  
It is worth noting that the difficulty here mainly lies in the fact that the model needs to achieve task-specific output formats for different specific tasks as much as possible without damaging the model's original abilities. To guide the language model to follow specific patterns and generate outputs that conform to the object detection format, the research team used ChatGPT to generate cross-modal instruction data to fine-tune the model. Specifically, based on 5000 coco images, they used ChatGPT to create a 30,000 cross-modal image-text fine-tuning dataset. To improve the efficiency of training, they fixed other model parameters and only learned cross-modal linear mapping. Experimental results show that even if only the linear layer is fine-tuned, the language model can understand fine-grained image features and follow specific patterns to perform inference-based image detection tasks, showing excellent performance.  
This research topic has great potential. Based on this technology, the field of home robots will further shine: people in homes can use abstract or coarse-grained voice instructions to make robots understand, recognize, and locate the objects they need, and provide relevant services. In the field of industrial robots, this technology will bring endless vitality: industrial robots can cooperate more naturally with human workers, accurately understand their instructions and needs, and achieve intelligent decision-making and operations. On the production line, human workers can use coarse-grained voice instructions or text input to allow robots to automatically understand, recognize, and locate the items that need to be processed, thereby improving production efficiency and quality.  
With object detection models that come with reasoning capabilities, we can develop more intelligent, natural, and efficient robots to provide more convenient, efficient, and humane services to humans. This is a field with broad prospects and deserves more attention and further exploration by more researchers.  
DetGPT supports multiple language models and has been validated based on two language models, Robin-13B and Vicuna-13B. The Robin series language model is a dialogue model trained by the LMFlow team ( https://github.com/OptimalScale/LMFlow) at the Hong Kong University of Science and Technology, achieving results competitive to Vicuna on multiple language ability evaluation benchmarks (model download: [https://github.com/OptimalScale/LMFlow#model-zoo](https://github.com/OptimalScale/LMFlow#model-zoo)). Previously, the LMFlow team trained a vertical GPT model using a consumer-grade 3090 graphics card in just 5 hours. Today, this team, in collaboration with the NLP Group at the University of Hong Kong, has brought us a multimodal surprise.  
Welcome to try our demo and open-source code!  
Online demo: [https://detgpt.github.io/](https://detgpt.github.io/) Open-source code: [https://github.com/OptimalScale/DetGPT](https://github.com/OptimalScale/DetGPT)"
48,10htfwp,machinelearning,ChatGPT,top,2023-01-21 15:15:45,ChatGPT is not all you need [R],EduCGM,False,0.83,113,https://www.reddit.com/r/MachineLearning/comments/10htfwp/chatgpt_is_not_all_you_need_r/,13,1674314145.0,"Hi all,

We would like to share here our little concise review of generative AI large models just to show how current models are able to work with lots of formats like texts, videos, images, etc... 

[https://arxiv.org/abs/2301.04655](https://arxiv.org/abs/2301.04655)

&#x200B;

Enjoy!"
49,zrfy75,machinelearning,ChatGPT,top,2022-12-21 09:52:57,[N] Point-E: a new Dalle-like model that generates 3D Point Clouds from Prompts,RepresentativeCod613,False,0.97,108,https://www.reddit.com/r/MachineLearning/comments/zrfy75/n_pointe_a_new_dallelike_model_that_generates_3d/,11,1671616377.0,"It's only been a month since OpenAI released ChatGPT, and yesterday they launched Point-E, a new Dalle-like model that generates 3D Point Clouds from Complex Prompts. As someone who is always interested in the latest advancements in machine learning, I was really excited to dig into this paper and see what it had to offer.

One of the key features of Point-E is its use of diffusion models to generate synthetic views and 3D point clouds. These models use text input to generate an image, which is then used as a reference for generating the 3D point cloud. This process takes only 1-2 minutes on a single GPU, making it much faster than previous state-of-the-art methods.

While the quality of the samples produced by Point-E may be lower than those produced by other methods, the speed of generation makes it a practical option for certain use cases.

If you're interested in learning more about this new model and how it was developed, I highly recommend giving the full paper a read. But if you're more into reading the gist of it, I added a link to an overview blog I published about.

The blog: [https://dagshub.com/blog/point-e/](https://dagshub.com/blog/point-e/)

The paper: [https://arxiv.org/abs/2212.08751](https://arxiv.org/abs/2212.08751)

I'm sure I have yet to reach all the insights while writing the blog, and I'd love to get your thoughts about the model and how OpenAI developed it."
50,12qf60j,machinelearning,ChatGPT,top,2023-04-18 07:46:29,[P] FastLoRAChat Instruct-tune LLaMA on consumer hardware with shareGPT data,icybee666,False,0.9,105,https://www.reddit.com/r/MachineLearning/comments/12qf60j/p_fastlorachat_instructtune_llama_on_consumer/,14,1681803989.0,"Announcing [FastLoRAChat](https://github.com/bupticybee/FastLoRAChat) , training chatGPT without A100.

&#x200B;

Releasing model:  [https://huggingface.co/icybee/fast\_lora\_chat\_v1\_sunlight](https://huggingface.co/icybee/fast_lora_chat_v1_sunlight)

and training data:  [https://huggingface.co/datasets/icybee/share\_gpt\_90k\_v1](https://huggingface.co/datasets/icybee/share_gpt_90k_v1)

&#x200B;

The purpose of this project is to produce similar result to the Fastchat model, but in much cheaper hardware (especially in non-Ampere GPUs).

This repository combined features of [alpaca-lora](https://github.com/tloen/alpaca-lora) and [Fastchat](https://github.com/lm-sys/FastChat):

1. Like Fastchat, support multilanguage and multi round chat.
2. Like alpaca-lora, support training and inference on low-end graphic cards (using LORA).
3. Opensource everything, include dataset, training code, export model code, and more.

Give it a try!"
51,121a8p4,machinelearning,ChatGPT,top,2023-03-25 04:14:58,[D] Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.89,106,https://www.reddit.com/r/MachineLearning/comments/121a8p4/d_do_we_really_need_100b_parameters_in_a_large/,90,1679717698.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
52,131z2k9,machinelearning,ChatGPT,top,2023-04-28 16:10:02,[P] We built an app that allows you to easily talk to your LLMs (or anything else),sergeybok,False,0.83,102,https://www.reddit.com/r/MachineLearning/comments/131z2k9/p_we_built_an_app_that_allows_you_to_easily_talk/,17,1682698202.0,"Hi all. So this all started with me wanting to talk to my local Alpaca bot from the bar to show my friend something. He’s a mobile developer and also recently unemployed like me, so the stars aligned and we built this thing over the last few weeks. 

Friendly AI is an app that is compatible with the [BaseBot](https://github.com/sergeybok/BaseBot) python library that we built. We are basically open sourcing the message protocol that it uses so that you can build your own “backend” for it that does whatever you want! I recently built myself a bot that allows me to write and run commands, shell scripts, and even python from my phone. Very handy when you went to the bar and forgot to commit and push your code. 

[Apple app is available](https://apps.apple.com/us/app/friendly-ai/id6447589849). The android app is currently in review so hopefully comes out later today.

If you are using Mac/Ubuntu the Quickstart command from the GitHub Readme should set you up with a starter project. If you either already have openai key on your system, or you create one and provide it on install, it will start you off with a simple ChatGPT wrapper (like the one that comes with the app if you Sign Up). 

If you are on windows I’m sorry neither of us has one so we couldn’t create an install script. However if you pip install the library and read the Readme you should be fine. 

Furthermore because it’s self-hosted, you can be sure that your data stays private. It’s stored on your own machine (in mongodb if you have it setup, in json files if you don’t). When you message your bots from the app the message data is sent directly to your bot and nowhere else. 

I think here of all places people will make good use of this tech. Because personally since I don’t have millions of dollars and can’t be actually working on proper LLM research by myself (which is what I’d rather be doing tbh), at least I can build cool stuff that uses the already existing models. 

The signup stuff isn’t necessary, the only reason why we built it is just to be able to limit people’s use of our bots, while also providing some access to them since without any bots you can’t try out the app. But we want people to build their own bots, and not simply use ours!

My hope was that it would remove a lot of the annoying parts of building bots and let people (including myself) concentrate on the actual interesting / ML /etc. parts of the problem — namely what the bot actually does in response to user prompts! And of course, the response doesn't actually have to use any LLMs (e.g. you can hook up your local stable diffusion model), or ML in general (as I said earlier I made a bot that simply executes the shell commands i give it). 

PS. Our servers are basically free-tier so in the off-chance that there’s a lot of downloads they might not hold up. But even if our servers are completely down that affects only our bots, you can still talk with your own bots!"
53,13a5baq,machinelearning,ChatGPT,top,2023-05-06 23:08:09,[P] OpenAI vs Open Source LLM Comparison for Document Q&A,georgesung,False,0.95,94,https://www.reddit.com/r/MachineLearning/comments/13a5baq/p_openai_vs_open_source_llm_comparison_for/,16,1683414489.0,"Ran a fun comparison between OpenAI vs open source (Apache 2.0) LLMs for Wikipedia document Q&A -- open source is looking good (and getting better).

TLDR:

For simple Wikipedia article Q&A, I compared OpenAI GPT 3.5, FastChat-T5, FLAN-T5-XXL, and FLAN-T5-XL. GPT 3.5 provided the best answers, but FastChat-T5 was very close in performance (with a basic guardrail). The T5 models I tested are all licensed under Apache 2.0, so they are commercially viable.

For the embedding model, I compared OpenAI text-embedding-ada-002 and the open source INSTRUCTOR-XL models. The INSTRUCTOR-XL model performed better, which is encouraging since INSTRUCTOR-XL is also licensed under Apache 2.0.

Full blog post:

[https://georgesung.github.io/ai/llm-qa-eval-wikipedia/](https://georgesung.github.io/ai/llm-qa-eval-wikipedia/)"
54,11f9k5g,machinelearning,ChatGPT,top,2023-03-01 17:23:03,"[P] ChatRWKV v2 (can run RWKV 14B with 3G VRAM), RWKV pip package, and finetuning to ctx16K",bo_peng,False,0.98,96,https://www.reddit.com/r/MachineLearning/comments/11f9k5g/p_chatrwkv_v2_can_run_rwkv_14b_with_3g_vram_rwkv/,37,1677691383.0,"Hi everyone. Now ChatRWKV v2 can split RWKV to multiple GPUs, or stream layers (compute layer-by-layer), so you can run RWKV 14B with as few as 3G VRAM. [https://github.com/BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)

Example:

`'cuda:0 fp16 *10 -> cuda:1 fp16 *8 -> cpu fp32'` = first 10 layers on cuda:0 fp16, then 8 layers on cuda:1 fp16, then on cpu fp32

`'cuda fp16 *20+'` = first 20 layers on cuda fp16, then stream the rest on it

And RWKV is now a pip package: [https://pypi.org/project/rwkv/](https://pypi.org/project/rwkv/)

    os.environ['RWKV_JIT_ON'] = '1'
    os.environ[""RWKV_CUDA_ON""] = '0' # if '1' then compile CUDA kernel for seq mode (much faster)
    from rwkv.model import RWKV
    from rwkv.utils import PIPELINE, PIPELINE_ARGS
    pipeline = PIPELINE(model, ""20B_tokenizer.json"") # find it in https://github.com/BlinkDL/ChatRWKV
    # download models: https://huggingface.co/BlinkDL
    model = RWKV(model='/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-169m/RWKV-4-Pile-169M-20220807-8023', strategy='cpu fp32')
    ctx = ""\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.""
    print(ctx, end='')
    def my_print(s):
        print(s, end='', flush=True)
    # For alpha_frequency and alpha_presence, see ""Frequency and presence penalties"":
    # https://platform.openai.com/docs/api-reference/parameter-details
    args = PIPELINE_ARGS(temperature = 1.0, top_p = 0.7,
        alpha_frequency = 0.25,
        alpha_presence = 0.25,
        token_ban = [0], # ban the generation of some tokens
        token_stop = []) # stop generation whenever you see any token here
    pipeline.generate(ctx, token_count=512, args=args, callback=my_print)

Right now all RWKV models are still trained with GPT-like method, so they are limited by the ctxlen used in training, even though in theory they should have almost infinite ctxlen (because they are RNNs). However RWKV models can be easily finetuned to support longer ctxlens (and large models actually use the ctxlen). I have finetuned 1B5/3B/7B/14B to ctx4K, and now finetuning 7B/14B to ctx8K, and 14B to ctx16K after that :) All models are available at [https://huggingface.co/BlinkDL](https://huggingface.co/BlinkDL)

The core RWKV is still mostly an one-man project, but a number of great developers are building on top of it, and you are welcome to join our community :)"
55,126wvkq,machinelearning,ChatGPT,top,2023-03-30 19:32:30,[R] TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs - Yaobo Liang et al Microsoft 2023,Singularian2501,False,0.95,93,https://www.reddit.com/r/MachineLearning/comments/126wvkq/r_taskmatrixai_completing_tasks_by_connecting/,9,1680204750.0,"Paper: [https://arxiv.org/abs/2303.16434](https://arxiv.org/abs/2303.16434)

Abstract:

>Artificial Intelligence (AI) has made incredible progress recently. On the one hand, advanced foundation models like ChatGPT can offer powerful conversation, in-context learning and code generation abilities on a broad range of open-domain tasks. They can also generate high-level solution outlines for domain-specific tasks based on the common sense knowledge they have acquired. However, they still face difficulties with some specialized tasks because they lack enough domain specific data during pre-training or they often have errors in their neural network computations on those tasks that need accurate executions. On the other hand, there are also many existing models and systems (symbolic-based or neural-based) that can do some domain specific tasks very well. However, due to the different implementation or working mechanisms, they are not easily accessible or compatible with foundation models. Therefore, there is a clear and pressing need for a mechanism that can leverage foundation models to propose task solution outlines and then automatically match some of the sub tasks in the outlines to the off-the-shelf models and systems with special functionalities to complete them. Inspired by this, we introduce TaskMatrix.AI as a new AI ecosystem that connects foundation models with millions of APIs for task completion. Unlike most previous work that aimed to improve a single AI model, TaskMatrix.AI focuses more on using existing foundation models (as a brain-like central system) and APIs of other AI models and systems (as sub-task solvers) to achieve diversified tasks in both digital and physical domains. As a position paper, we will present our vision of how to build such an ecosystem, explain each key component, and use study cases to illustrate both the feasibility of this vision and the main challenges we need to address next. 

https://preview.redd.it/0guexiznhxqa1.jpg?width=979&format=pjpg&auto=webp&s=e5d818ae789cfc493cfb82fdf8b002a8dfe11939"
56,133zvdl,machinelearning,ChatGPT,top,2023-04-30 18:54:05,[R] This month (+ 2 more weeks) in LLM/Transformer research (Timeline),viktorgar,False,0.95,91,https://i.redd.it/o26q1bk7j2xa1.png,11,1682880845.0,
57,11kk3iq,machinelearning,ChatGPT,top,2023-03-07 00:54:07,[R] PyReason: logic for use with ML,Neurosymbolic,False,0.97,88,https://www.reddit.com/r/MachineLearning/comments/11kk3iq/r_pyreason_logic_for_use_with_ml/,30,1678150447.0,"Last week,  we released a paper on [PyReason on Arxiv](https://arxiv.org/pdf/2302.13482.pdf). PyReason is a Python package for logical inference and designed for use with machine learning ([https://github.com/lab-v2/pyreason](https://github.com/lab-v2/pyreason)).

You may think that’s all fine and good, but are wondering why would we need a logic for machine learning? In this post, I’ll discuss why we did it.

First, a lot of the criticism of machine learning, especially deep learning is that while it obtains excellent result son may tasks, it is merely mimicking historical data and not learning actual relationships. This has resulted in a lot of the major shortcomings in ML such as the [hallucinations](https://www.nytimes.com/2023/02/17/podcasts/hard-fork-bing-ai-elon.html?action=click&module=audio-series-bar&region=header&pgtype=Article) of large language models, the requirements of vast amounts of training data to learn games, and brittleness in certain applications (e.g., the recent defeat of AlphaGo, difficulty in [solving math problems](https://www.reddit.com/r/MachineLearning/comments/11f29f9/r_chatgpt_failure_increase_linearly_with_addition/)). In a video lecture, we review some of these shortcomings, much of which constitutes active areas of research ([part 1](https://www.youtube.com/watch?v=9cooDzgd8NA), [part 2](https://www.youtube.com/watch?v=d2xfgwovwso)).

Then enter “[neuro symbolic](https://neurosymoblic.asu.edu/)” artificial intelligence. Actually an old idea where neural architectures can work hand-in-hand with logic, often even having an equivalence between the two. The idea is symbolic AI has many shortcomings (brittleness to noise, difficulty in learning) that can be address with deep learning while its strengths (modularity, ability to add constraints, symbolic manipulation) can address some of deep learning’s limitations.

Neuro symbolic AI is a highly active area of research, and much of the advancements have identified special logical languages to use in their approach. Our goal with PyReason was to unify many of these logics and provide logic capabilities in a robust and modern Python implementation. We are working on a few joint projects with industry partners applying this to various use-cases, and now we have made the code base and library available as an open source package. In a [video](https://www.youtube.com/watch?v=E1PSl3KQCmo&t=8s), we outline six major capabilities that we felt were important:

1. Open world reasoning – ability to reason in uncertain situations (important for interfacing with ML models)
2. Multi-step inference
3. Explainability
4. Temporal reasoning
5. Graph-based reasoning
6. Designed to support neuro symbolic frameworks

The release of PyReason will kick off not only new research by our group and our collaborators, but also associated software. We’re pretty excited about this new direction!"
58,11y70rx,machinelearning,ChatGPT,top,2023-03-22 04:34:44,[R] MM-ReAct: Prompting ChatGPT for Multimodal Reasoning and Action,MysteryInc152,False,0.93,88,https://www.reddit.com/r/MachineLearning/comments/11y70rx/r_mmreact_prompting_chatgpt_for_multimodal/,22,1679459684.0," Blog - [https://multimodal-react.github.io/](https://multimodal-react.github.io/)

Paper - [https://arxiv.org/abs/2303.11381](https://arxiv.org/abs/2303.11381)

Code - [https://github.com/microsoft/MM-REACT](https://github.com/microsoft/MM-REACT)

Demo - [https://huggingface.co/spaces/microsoft-cognitive-service/mm-react](https://huggingface.co/spaces/microsoft-cognitive-service/mm-react)

Wildest thing i've seen in a while. Still processing how a connection of foundation models can be this good."
59,122bju6,machinelearning,ChatGPT,top,2023-03-26 04:57:23,I made a chrome extension to make chatGPT bots from any web content in seconds [P],TernaryJimbo,False,0.85,85,https://v.redd.it/z00323t3j0qa1,18,1679806643.0,
60,120csub,machinelearning,ChatGPT,top,2023-03-24 07:32:32,[P] ChatGPT with GPT-2: A minimum example of aligning language models with RLHF similar to ChatGPT,liyanjia92,False,0.94,75,https://www.reddit.com/r/MachineLearning/comments/120csub/p_chatgpt_with_gpt2_a_minimum_example_of_aligning/,15,1679643152.0,"hey folks, happy Friday! I wish to get some feedback for my recent project of a minimum example of using RLHF on language models to improve human alignment. 

The goal is to compare with vanilla GPT-2 and supervised fine-tuned GPT-2 to see how much RLHF can benefit small models. Also I hope this project can show an example of the minimum requirements to build a RLHF training pipeline for LLMs.

Github: https://github.com/ethanyanjiali/minChatGPT
Demo: https://colab.research.google.com/drive/1LR1sbWTyaNAmTZ1g1M2tpmU_pFw1lyEX?usp=sharing

Thanks a lot for any suggestions and feedback!"
61,10fh79i,machinelearning,ChatGPT,top,2023-01-18 20:05:46,[R] A simple explanation of Reinforcement Learning from Human Feedback (RLHF),JClub,False,0.93,75,https://www.reddit.com/r/MachineLearning/comments/10fh79i/r_a_simple_explanation_of_reinforcement_learning/,21,1674072346.0,"&#x200B;

[Overview of RLHF training](https://preview.redd.it/fp5mh1sdayca1.png?width=2324&format=png&auto=webp&s=29e75d417ba9f0439b7d33a8b705679b43300e2c)

You must have heard about ChatGPT. Maybe you heard that it was trained with RLHF and PPO. Perhaps you do not really understand how that process works. Then check my Gist on Reinforcement Learning from Human Feedback (RLHF): [https://gist.github.com/JoaoLages/c6f2dfd13d2484aa8bb0b2d567fbf093](https://gist.github.com/JoaoLages/c6f2dfd13d2484aa8bb0b2d567fbf093)

No hard maths, straight to the point and simplified. Hope that it helps!"
62,11ybjsi,machinelearning,ChatGPT,comments,2023-03-22 08:04:01,[D] Overwhelmed by fast advances in recent weeks,iamx9000again,False,0.96,831,https://www.reddit.com/r/MachineLearning/comments/11ybjsi/d_overwhelmed_by_fast_advances_in_recent_weeks/,331,1679472241.0,"I was watching the GTC keynote and became entirely overwhelmed by the amount of progress achieved from last year.  I'm wondering how everyone else feels.

&#x200B;

Firstly, the entire ChatGPT, GPT-3/GPT-4 chaos has been going on for a few weeks, with everyone scrambling left and right to integrate chatbots into their apps, products, websites. Twitter is flooded with new product ideas, how to speed up the process from idea to product, countless promp engineering blogs, tips, tricks, paid courses.

&#x200B;

Not only was ChatGPT disruptive, but a few days later, Microsoft and Google also released their models and integrated them into their search engines. Microsoft also integrated its LLM into its Office suite. It all happenned overnight. I understand that they've started integrating them along the way, but still, it seems like it hapenned way too fast. This tweet encompases the past few weeks perfectly [https://twitter.com/AlphaSignalAI/status/1638235815137386508](https://twitter.com/AlphaSignalAI/status/1638235815137386508) , on a random Tuesday countless products are released that seem revolutionary.

&#x200B;

In addition to the language models, there are also the generative art models that have been slowly rising in mainstream recognition. Now Midjourney AI is known by a lot of people who are not even remotely connected to the AI space.

&#x200B;

For the past few weeks, reading Twitter, I've felt completely overwhelmed, as if the entire AI space is moving beyond at lightning speed, whilst around me we're just slowly training models, adding some data, and not seeing much improvement, being stuck on coming up with ""new ideas, that set us apart"".

&#x200B;

Watching the GTC keynote from NVIDIA I was again, completely overwhelmed by how much is being developed throughout all the different domains. The ASML EUV (microchip making system) was incredible, I have no idea how it does lithography and to me it still seems like magic. The Grace CPU with 2 dies (although I think Apple was the first to do it?) and 100 GB RAM, all in a small form factor. There were a lot more different hardware servers that I just blanked out at some point. The omniverse sim engine looks incredible, almost real life (I wonder how much of a domain shift there is between real and sim considering how real the sim looks). Beyond it being cool and usable to train on synthetic data, the car manufacturers use it to optimize their pipelines. This change in perspective, of using these tools for other goals than those they were designed for I find the most interesting.

&#x200B;

The hardware part may be old news, as I don't really follow it, however the software part is just as incredible. NVIDIA AI foundations (language, image, biology models), just packaging everything together like a sandwich. Getty, Shutterstock and Adobe will use the generative models to create images. Again, already these huge juggernauts are already integrated.

&#x200B;

I can't believe the point where we're at. We can use AI to write code, create art, create audiobooks using Britney Spear's voice, create an interactive chatbot to converse with books, create 3D real-time avatars, generate new proteins (?i'm lost on this one), create an anime and countless other scenarios. Sure, they're not perfect, but the fact that we can do all that in the first place is amazing.

&#x200B;

As Huang said in his keynote, companies want to develop ""disruptive products and business models"". I feel like this is what I've seen lately. Everyone wants to be the one that does something first, just throwing anything and everything at the wall and seeing what sticks.

&#x200B;

In conclusion, I'm feeling like the world is moving so fast around me whilst I'm standing still. I want to not read anything anymore and just wait until everything dies down abit, just so I can get my bearings. However, I think this is unfeasible. I fear we'll keep going in a frenzy until we just burn ourselves at some point.

&#x200B;

How are you all fairing? How do you feel about this frenzy in the AI space? What are you the most excited about?"
63,12dz4hh,machinelearning,ChatGPT,comments,2023-04-06 21:45:18,[D] Is all the talk about what GPT can do on Twitter and Reddit exaggerated or fairly accurate?,ThePhantomguy,False,0.89,264,https://www.reddit.com/r/MachineLearning/comments/12dz4hh/d_is_all_the_talk_about_what_gpt_can_do_on/,311,1680817518.0,"I saw [this post](https://www.reddit.com/r/ChatGPT/comments/12diapw/gpt4_week_3_chatbots_are_yesterdays_news_ai/?utm_source=share&utm_medium=ios_app&utm_name=iossmf) on the r/ChatGPT subreddit, and I’ve been seeing similar talk on Twitter. There’s people talking about AGI, the singularity, and etc. I get that it’s cool, exciting, and fun; but some of the talk seems a little much? Like it reminds me of how the NFT bros would talk about blockchain technology.

Do any of the people making these kind of claims have a decent amount of knowledge on machine learning at all? The scope of my own knowledge is very limited, as I’ve only implemented and taken courses on models that are pretty old. So I’m here to ask for opinions from ya’ll. Is there some validity, or is it just people that don’t really understand what they’re saying and making grand claims (Like some sort of Dunning Kruger Effect)?"
64,10gtruu,machinelearning,ChatGPT,comments,2023-01-20 10:41:04,[N] OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic,ChubChubkitty,False,0.83,521,https://www.reddit.com/r/MachineLearning/comments/10gtruu/n_openai_used_kenyan_workers_on_less_than_2_per/,246,1674211264.0,https://time.com/6247678/openai-chatgpt-kenya-workers/
65,126oiey,machinelearning,ChatGPT,comments,2023-03-30 14:18:50,[D] AI Policy Group CAIDP Asks FTC To Stop OpenAI From Launching New GPT Models,vadhavaniyafaijan,False,0.84,210,https://www.reddit.com/r/MachineLearning/comments/126oiey/d_ai_policy_group_caidp_asks_ftc_to_stop_openai/,213,1680185930.0,"The Center for AI and Digital Policy (CAIDP), a tech ethics group, has asked the Federal Trade Commission to investigate OpenAI for violating consumer protection rules. CAIDP claims that OpenAI's AI text generation tools have been ""biased, deceptive, and a risk to public safety.""

CAIDP's complaint raises concerns about potential threats from OpenAI's GPT-4 generative text model, which was announced in mid-March. It warns of the potential for GPT-4 to produce malicious code and highly tailored propaganda and the risk that biased training data could result in baked-in stereotypes or unfair race and gender preferences in hiring. 

The complaint also mentions significant privacy failures with OpenAI's product interface, such as a recent bug that exposed OpenAI ChatGPT histories and possibly payment details of ChatGPT plus subscribers.

CAIDP seeks to hold OpenAI accountable for violating Section 5 of the FTC Act, which prohibits unfair and deceptive trade practices. The complaint claims that OpenAI knowingly released GPT-4 to the public for commercial use despite the risks, including potential bias and harmful behavior. 

[Source](https://www.theinsaneapp.com/2023/03/stop-openai-from-launching-gpt-5.html) | [Case](https://www.caidp.org/cases/openai/)| [PDF](https://www.caidp.org/app/download/8450269463/CAIDP-FTC-Complaint-OpenAI-GPT-033023.pdf)"
66,10pb1y3,machinelearning,ChatGPT,comments,2023-01-30 19:09:14,"[P] I launched “CatchGPT”, a supervised model trained with millions of text examples, to detect GPT created content",qthai912,False,0.75,497,https://www.reddit.com/r/MachineLearning/comments/10pb1y3/p_i_launched_catchgpt_a_supervised_model_trained/,206,1675105754.0,"I’m an ML Engineer at Hive AI and I’ve been working on a ChatGPT Detector.

Here is a free demo we have up: [https://hivemoderation.com/ai-generated-content-detection](https://hivemoderation.com/ai-generated-content-detection)

From our benchmarks it’s significantly better than similar solutions like GPTZero and OpenAI’s GPT2 Output Detector. On our internal datasets, we’re seeing balanced accuracies of >99% for our own model compared to around 60% for GPTZero and 84% for OpenAI’s GPT2 Detector.

Feel free to try it out and let us know if you have any feedback!"
67,13b6miy,machinelearning,ChatGPT,comments,2023-05-07 23:26:29,"[D] ClosedAI license, open-source license which restricts only OpenAI, Microsoft, Google, and Meta from commercial use",wemsyn,False,0.8,347,https://www.reddit.com/r/MachineLearning/comments/13b6miy/d_closedai_license_opensource_license_which/,191,1683501989.0,"After reading [this article](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither), I realized it might be nice if the open-source AI community could exclude ""closed AI"" players from taking advantage of community-generated models and datasets. I was wondering if it would be possible to write a license that is completely permissive (like Apache 2.0 or MIT), except to certain companies, which are completely barred from using the software in any context.

Maybe this could be called the ""ClosedAI"" license. I'm not any sort of legal expert so I have no idea how best to write this license such that it protects model weights and derivations thereof.

I prompted ChatGPT for an example license and this is what it gave me:

    <PROJECT NAME> ClosedAI License v1.0
    
    Permission is hereby granted, free of charge, to any person or organization obtaining a copy of this software and associated documentation files (the ""Software""), to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, subject to the following conditions:
    
    1. The above copyright notice and this license notice shall be included in all copies or substantial portions of the Software.
    
    2. The Software and any derivative works thereof may not be used, in whole or in part, by or on behalf of OpenAI Inc., Google LLC, or Microsoft Corporation (collectively, the ""Prohibited Entities"") in any capacity, including but not limited to training, inference, or serving of neural network models, or any other usage of the Software or neural network weights generated by the Software.
    
    3. Any attempt by the Prohibited Entities to use the Software or neural network weights generated by the Software is a material breach of this license.
    
    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

No idea if this is valid or not. Looking for advice.

&#x200B;

**Edit:** Thanks for the input. Removed non-commercial clause (whoops, proofread what ChatGPT gives you). Also removed Meta from the excluded companies list due to popular demand."
68,11h3p2x,machinelearning,ChatGPT,comments,2023-03-03 15:37:03,[D] Facebooks LLaMA leaks via torrent file in PR,londons_explorer,False,0.98,524,https://www.reddit.com/r/MachineLearning/comments/11h3p2x/d_facebooks_llama_leaks_via_torrent_file_in_pr/,184,1677857823.0,"See here:
https://github.com/facebookresearch/llama/pull/73/files

Note that this PR *is not* made by a member of Facebook/Meta staff.    I have downloaded parts of the torrent and it does appear to be lots of weights, although I haven't confirmed it is trained as in the LLaMA paper, although it seems likely.


I wonder how much finetuning it would take to make this work like ChatGPT - finetuning tends to be much cheaper than the original training, so it might be something a community could do..."
69,12nbixk,machinelearning,ChatGPT,comments,2023-04-15 17:14:58,[P] OpenAssistant - The world's largest open-source replication of ChatGPT,ykilcher,False,0.97,1265,https://www.reddit.com/r/MachineLearning/comments/12nbixk/p_openassistant_the_worlds_largest_opensource/,175,1681578898.0,"We’re excited to announce the release of OpenAssistant.

The future of AI development depends heavily on high quality datasets and models being made publicly available, and that’s exactly what this project does.

Watch the annoucement video:

[https://youtu.be/ddG2fM9i4Kk](https://youtu.be/ddG2fM9i4Kk)

&#x200B;

Our team has worked tirelessly over the past several months collecting large amounts of text-based input and feedback to create an incredibly diverse and unique dataset designed specifically for training language models or other AI applications.

With over 600k human-generated data points covering a wide range of topics and styles of writing, our dataset will be an invaluable tool for any developer looking to create state-of-the-art instruction models!

To make things even better, we are making this entire dataset free and accessible to all who wish to use it. Check it out today at our HF org: OpenAssistant

On top of that, we've trained very powerful models that you can try right now at: [open-assistant.io/chat](https://open-assistant.io/chat) !"
70,1095os9,machinelearning,ChatGPT,comments,2023-01-11 14:12:57,[D] Microsoft ChatGPT investment isn't about Bing but about Cortana,fintechSGNYC,False,0.89,402,https://www.reddit.com/r/MachineLearning/comments/1095os9/d_microsoft_chatgpt_investment_isnt_about_bing/,173,1673446377.0,"I believe that Microsoft's 10B USD investment in ChatGPT is less about Bing and more about turning Cortana into an Alexa for corporates.   
Examples: Cortana prepare the new T&Cs... Cortana answer that client email... Cortana prepare the Q4 investor presentation (maybe even with PowerBI integration)... Cortana please analyze cost cutting measures... Cortana please look up XYZ... 

What do you think?"
71,11njpb9,machinelearning,ChatGPT,comments,2023-03-10 08:50:32,[D] Is ML a big boys game now?,TheStartIs2019,False,0.83,176,https://www.reddit.com/r/MachineLearning/comments/11njpb9/d_is_ml_a_big_boys_game_now/,146,1678438232.0,"As much as I enjoy ML as a whole, I am a bit skeptical of the future for individuals. With OpenAI trying to monopolize the market along with Microsoft, which part remains for the small time researchers/developers?

It seems everything now is just a ChatGPT wrapper, and with GPT-4 around the corner I assume itll be even more prominent.

What are your thoughts?"
72,11zsdwv,machinelearning,ChatGPT,comments,2023-03-23 18:09:11,[N] ChatGPT plugins,Singularian2501,False,0.97,443,https://www.reddit.com/r/MachineLearning/comments/11zsdwv/n_chatgpt_plugins/,144,1679594951.0,"[https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins)

>We’ve implemented initial support for plugins in ChatGPT. Plugins are tools designed specifically for language models with safety as a core principle, and help ChatGPT access up-to-date information, run  computations, or use third-party services."
73,zwht9g,machinelearning,ChatGPT,comments,2022-12-27 15:13:00,[P] Can you distinguish AI-generated content from real art or literature? I made a little test!,Dicitur,False,0.93,295,https://www.reddit.com/r/MachineLearning/comments/zwht9g/p_can_you_distinguish_aigenerated_content_from/,126,1672153980.0,"Hi everyone, 

I am no programmer, and I have a very basic knowledge of machine learning, but I am fascinated by the possibilities offered by all the new models we have seen so far. 

Some people around me say they are not that impressed by what AIs can do, so I built a small test (with a little help by chatGPT to code the whole thing): can you always 100% distinguish between AI art or text and old works of art or literature?

Here is the site: http://aiorart.com/

I find that AI-generated text is still generally easy to spot, but of course it is very challenging to go against great literary works. AI images can sometimes be truly deceptive.

I wonder what you will all think of it... and how all that will evolve in the coming months!

PS: The site is very crude (again, I am no programmer!). It works though."
74,1244q71,machinelearning,ChatGPT,comments,2023-03-27 23:21:38,[D] FOMO on the rapid pace of LLMs,00001746,False,0.96,304,https://www.reddit.com/r/MachineLearning/comments/1244q71/d_fomo_on_the_rapid_pace_of_llms/,121,1679959298.0,"Hi all, 

I recently read [this reddit post](https://www.reddit.com/r/blender/comments/121lhfq/i_lost_everything_that_made_me_love_my_job/) about a 2D modeler experiencing an existential crisis about their job being disrupted by midjourney ([HN discussion here](https://news.ycombinator.com/item?id=35319861)). I can't help but feel the same as someone who has been working in the applied ML space for the past few years. 

Despite my background in ""classical"" ML, I'm feeling some anxiety about the rapid pace of LLM development and face a fear of missing out / being left behind.

I'd love to get involved again in ML research apart from my day job, but one of the biggest obstacles is the fact that training most of foundational LLM research requires huge compute more than anything else \[1\]. I understand that there are some directions in distributing compute ([https://petals.ml](https://petals.ml/)), or distilling existing models  ([https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)). 

I thought I might not be the only one being humbled by the recent advances in ChatGPT, etc. and wanted to hear how other people feel / are getting involved. 

\--

\[1\] I can't help but be reminded of Sutton's description of the [""bitter lesson"" of modern AI research](https://www.incompleteideas.net/IncIdeas/BitterLesson.html): ""breakthrough progress eventually arrives by an opposing approach based on scaling computation... eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach."""
75,11fbccz,machinelearning,ChatGPT,comments,2023-03-01 18:31:12,[D] OpenAI introduces ChatGPT and Whisper APIs (ChatGPT API is 1/10th the cost of GPT-3 API),minimaxir,False,0.97,571,https://www.reddit.com/r/MachineLearning/comments/11fbccz/d_openai_introduces_chatgpt_and_whisper_apis/,119,1677695472.0,"https://openai.com/blog/introducing-chatgpt-and-whisper-apis

> It is priced at $0.002 per 1k tokens, which is 10x cheaper than our existing GPT-3.5 models.

This is a massive, massive deal. For context, the reason GPT-3 apps took off over the past few months before ChatGPT went viral is because a) text-davinci-003 was released and was a significant performance increase and b) the cost was cut from $0.06/1k tokens to $0.02/1k tokens, which made consumer applications feasible without a large upfront cost.

A much better model and a 1/10th cost warps the economics completely to the point that it may be better than in-house finetuned LLMs.

I have no idea how OpenAI can make money on this. This has to be a loss-leader to lock out competitors before they even get off the ground."
76,zhrgln,machinelearning,ChatGPT,comments,2022-12-10 12:32:57,[P] I made a command-line tool that explains your errors using ChatGPT (link in comments),jsonathan,False,0.97,2859,https://i.redd.it/kq518l9ne25a1.gif,112,1670675577.0,
77,120usfk,machinelearning,ChatGPT,comments,2023-03-24 19:15:58,[R] Hello Dolly: Democratizing the magic of ChatGPT with open models,austintackaberry,False,0.98,605,https://www.reddit.com/r/MachineLearning/comments/120usfk/r_hello_dolly_democratizing_the_magic_of_chatgpt/,109,1679685358.0,"Databricks shows that anyone can take a dated off-the-shelf open source large language model (LLM) and give it magical ChatGPT-like instruction following ability by training it in less than three hours on one machine, using high-quality training data.

They fine tuned GPT-J using the Alpaca dataset.

Blog: [https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)  
Github: [https://github.com/databrickslabs/dolly](https://github.com/databrickslabs/dolly)"
78,1271po7,machinelearning,ChatGPT,comments,2023-03-30 22:40:29,[P] Introducing Vicuna: An open-source language model based on LLaMA 13B,Business-Lead2679,False,0.95,288,https://www.reddit.com/r/MachineLearning/comments/1271po7/p_introducing_vicuna_an_opensource_language_model/,107,1680216029.0,"We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%\* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%\* of cases. The cost of training Vicuna-13B is around $300. The training and serving [code](https://github.com/lm-sys/FastChat), along with an online [demo](https://chat.lmsys.org/), are publicly available for non-commercial use.

# Training details

Vicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model’s maximum context length.

Our training recipe builds on top of [Stanford’s alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) with the following improvements.

* **Memory Optimizations:** To enable Vicuna’s understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing [gradient checkpointing](https://arxiv.org/abs/1604.06174) and [flash attention](https://arxiv.org/abs/2205.14135).
* **Multi-round conversations:** We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot’s output.
* **Cost Reduction via Spot Instance:** The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ [SkyPilot](https://github.com/skypilot-org/skypilot) [managed spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html) to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.

&#x200B;

[Vicuna - Online demo](https://reddit.com/link/1271po7/video/0qsiu08kdyqa1/player)

# Limitations

We have noticed that, similar to other large language models, Vicuna has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. Additionally, it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI [moderation](https://platform.openai.com/docs/guides/moderation/overview) API to filter out inappropriate user inputs in our online demo. Nonetheless, we anticipate that Vicuna can serve as an open starting point for future research to tackle these limitations.

[Relative Response Quality Assessed by GPT-4](https://preview.redd.it/1rnmhv01eyqa1.png?width=599&format=png&auto=webp&s=02b4d415b5d378851bb70e225f1b1ebce98bfd83)

&#x200B;

For more information, check [https://vicuna.lmsys.org/](https://vicuna.lmsys.org/)

Online demo: [https://chat.lmsys.org/](https://chat.lmsys.org/)

&#x200B;

All credits go to the creators of this model. I did not participate in the creation of this model nor in the fine-tuning process. Usage of this model falls under a non-commercial license."
79,13hrkbj,machinelearning,ChatGPT,comments,2023-05-15 00:00:05,[D] On LLMs' ability to perform random sampling,bgighjigftuik,False,0.91,247,https://i.redd.it/l1uyl3k3gxza1.jpg,104,1684108805.0,"To me this is the most counterintuitive thing I've yet seen from a language model. In this case it was ChatGPT (3.5, the free version); but I can see this happening in any model.

Here's the prompt I used on a fresh session:

""Generate 60 random numbers coming from an exponential distribution with lambda=6.4. Don't write any code""

It outputs the following values:

0.013, 0.078, 0.266, 0.040, 0.117, 0.283, 0.048, 0.078, 0.181, 0.134, 0.017, 0.160, 0.052, 0.196, 0.096, 0.098, 0.019, 0.048, 0.141, 0.071, 0.106, 0.023, 0.197, 0.062, 0.029, 0.072, 0.111, 0.166, 0.080, 0.027, 0.124, 0.016, 0.135, 0.052, 0.078, 0.161, 0.078, 0.139, 0.068, 0.066, 0.039, 0.056, 0.018, 0.082, 0.180, 0.086, 0.024, 0.022, 0.057, 0.042, 0.050, 0.104, 0.099, 0.062, 0.016, 0.186, 0.043, 0.085, 0.145, 0.110.

I get to plot a histogram with the numbers and they are close to what I would consider an exponential distribution with such parameter (you can see the plot).

Given that GPT 3.5 does not have access to a Python interpreter, how on earth is it able to do so? I have also tried other distributions and parameters and it kind of works. It's not perfect, but with normal distributions it is usually close to what scipy.stats would generate.

I could understand that it can have learnt to interpret Python code to some extent, but honestly I can't find explanation for random sampling from a probability distribution. For a Normal distribution, I can tell it about the desired mean and variance, and it samples values that are more than reasonable (and close to the true mean/variance specified).

Any thoughts? I honestly am unable to wrap my head around how a LLM can have the understanding on how to sample tokens (at digit level) to fit any probability distribution. To me it seems very unlikely to have similar data either the pre-training or fine-tuning stages."
80,128lo83,machinelearning,ChatGPT,comments,2023-04-01 12:57:30,[R] [P] I generated a 30K-utterance dataset by making GPT-4 prompt two ChatGPT instances to converse.,radi-cho,False,0.96,802,https://i.redd.it/bywcz1kzs9ra1.png,104,1680353850.0,
81,zstequ,machinelearning,ChatGPT,comments,2022-12-22 18:39:30,[D] When chatGPT stops being free: Run SOTA LLM in cloud,_underlines_,False,0.95,351,https://www.reddit.com/r/MachineLearning/comments/zstequ/d_when_chatgpt_stops_being_free_run_sota_llm_in/,95,1671734370.0,"Edit: Found [LAION-AI/OPEN-ASSISTANT](https://github.com/LAION-AI/Open-Assistant) a very promising project opensourcing the idea of chatGPT. [video here](https://www.youtube.com/watch?v=8gVYC_QX1DI)

**TL;DR: I found GPU compute to be [generally cheap](https://github.com/full-stack-deep-learning/website/blob/main/docs/cloud-gpus/cloud-gpus.csv) and spot or on-demand instances can be launched on AWS for a few USD / hour up to over 100GB vRAM. So I thought it would make sense to run your own SOTA LLM like Bloomz 176B inference endpoint whenever you need it for a few questions to answer. I thought it would still make more sense than shoving money into a closed walled garden like ""not-so-OpenAi"" when they make ChatGPT or GPT-4 available for $$$. But I struggle due to lack of tutorials/resources.**

Therefore, I carefully checked benchmarks, model parameters and sizes as well as training sources for all SOTA LLMs [here](https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit#gid=1158069878).

Knowing since reading the Chinchilla paper that Model Scaling according to OpenAI was wrong and more params != better quality generation. So I was looking for the best performing LLM openly available in terms of quality and broadness to use for multilingual everyday questions/code completion/reasoning similar to what chatGPT provides (minus the fine-tuning for chat-style conversations).

My choice fell on [Bloomz](https://huggingface.co/bigscience/bloomz) (because that handles multi-lingual questions well and has good zero shot performance for instructions and Q&A style text generation. Confusingly Galactica seems to outperform Bloom on several benchmarks. But since Galactica had a very narrow training set only using scientific papers, I guess usage is probably limited for answers on non-scientific topics.

Therefore I tried running the original bloom 176B and alternatively also Bloomz 176B on AWS SageMaker JumpStart, which should be a one click deployment. This fails after 20min. On Azure ML, I tried using DeepSpeed-MII which also supports bloom but also fails due the instance size of max 12GB vRAM I guess.

From my understanding to save costs on inference, it's probably possible to use one or multiple of the following solutions:

- Precision: int8 instead of fp16
- [Microsoft/DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) for an up 40x reduction on inference cost on Azure, this thing also supports int8 and fp16 bloom out of the box, but it fails on Azure due to instance size.
- [facebook/xformer](https://github.com/facebookresearch/xformers) not sure, but if I remember correctly this brought inference requirements down to 4GB vRAM for StableDiffusion and DreamBooth fine-tuning to 10GB. No idea if this is usefull for Bloom(z) inference cost reduction though

I have a CompSci background but I am not familiar with most stuff, except that I was running StableDiffusion since day one on my rtx3080 using linux and also doing fine-tuning with DreamBooth. But that was all just following youtube tutorials. I can't find a single post or youtube video of anyone explaining a full BLOOM / Galactica / BLOOMZ inference deployment on cloud platforms like AWS/Azure using one of the optimizations mentioned above, yet alone deployment of the raw model. :(

I still can't figure it out by myself after 3 days.

**TL;DR2: Trying to find likeminded people who are interested to run open source SOTA LLMs for when chatGPT will be paid or just for fun.**

Any comments, inputs, rants, counter-arguments are welcome.

/end of rant"
82,121a8p4,machinelearning,ChatGPT,comments,2023-03-25 04:14:58,[D] Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.89,107,https://www.reddit.com/r/MachineLearning/comments/121a8p4/d_do_we_really_need_100b_parameters_in_a_large/,90,1679717698.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
83,zjbsie,machinelearning,ChatGPT,comments,2022-12-11 22:16:43,"[D] - Has Open AI said what ChatGPT's architecture is? What technique is it using to ""remember"" previous prompts?",029187,False,0.95,248,https://www.reddit.com/r/MachineLearning/comments/zjbsie/d_has_open_ai_said_what_chatgpts_architecture_is/,88,1670797003.0,"Has Open AI said what ChatGPT's architecture is? What technique is it using to ""remember"" previous prompts? Have they come up with some way to add recurrence to the transformer or is it just using a feedforward sliding window approach?"
84,12ay0vt,machinelearning,ChatGPT,comments,2023-04-03 21:11:52,"[P] The weights neccessary to construct Vicuna, a fine-tuned LLM with capabilities comparable to GPT3.5, has now been released",Andy_Schlafly,False,0.98,606,https://www.reddit.com/r/MachineLearning/comments/12ay0vt/p_the_weights_neccessary_to_construct_vicuna_a/,86,1680556312.0,"Vicuna is a large language model derived from LLaMA, that has been fine-tuned to the point of having 90% ChatGPT quality. The delta-weights, necessary to reconstruct the model from LLaMA weights have now been released, and can be used to build your own Vicuna.

https://vicuna.lmsys.org/"
85,zc5sg6,machinelearning,ChatGPT,comments,2022-12-04 09:57:44,[D] OpenAI’s ChatGPT is unbelievable good in telling stories!,Far_Pineapple770,False,0.91,244,https://www.reddit.com/r/MachineLearning/comments/zc5sg6/d_openais_chatgpt_is_unbelievable_good_in_telling/,85,1670147864.0,"
I started playing with ChatGPT, the new chatbot created by OpenAI and is free to use at the moment. 

I asked it to describe a scene between Batman and Joker on a subway platform and the result is mind blowing! I let you have a read of the story yourself:

Generated by ChatGPT:

The scene is set in a dimly lit subway station, with the Joker standing on the platform, gazing up at Batman who is perched on a nearby overhead beam. The Joker is dressed in his signature purple suit and green hair, a wicked grin plastered across his face.

In his hand, he holds a deck of playing cards, flipping them expertly through his fingers. The sound of the cards shuffling echoes through the empty station.

Batman, dressed in his dark suit and cape, stares down at the Joker with a steely gaze. His muscles are tense, ready for any sudden movements from the villain.

The Joker cackles, his laughter bouncing off the walls of the station. ""Hello, Batsy! Long time no see. How's the crime-fighting business these days?""

Batman remains silent, watching the Joker carefully.

The Joker's grin widens. ""Come on, Bats. Don't be so serious all the time. Let's have some fun."" He tosses a card at Batman, who easily catches it.

""I'm not here to play games, Joker,"" Batman says in a grave tone.

The Joker's laughter turns into a maniacal cackle. ""But that's the whole point, Batsy! Life is a game. And I'm the master of it.""

He pulls out a bomb from his pocket and holds it up, the timer ticking down. ""Let's see how well you can play, shall we?""

Batman leaps down from the beam, landing gracefully on the platform. He advances towards the Joker, who backs away, still holding the bomb.

The tension in the air is palpable as the two foes face off, each ready to outsmart the other. The sound of the bomb's timer counts down, adding to the tension of the scene.

The Joker's grin never fades, even as Batman inches closer. ""It's always a pleasure to play with you, Batsy. But I think it's time for me to make my exit.""

With a flourish, the Joker disappears in a cloud of smoke, leaving Batman standing alone on the platform. The bomb detonates, causing a small explosion and sending debris flying.

But Batman remains unharmed, ready to continue the never-ending battle against the Joker and all the evil that plagues the city.

End of the story.

I used the first two paragraphs in OpenAI’s Dall E 2 to generate an image and [this](https://labs.openai.com/s/p8L8Z6c3S7qZ26U7owfbx7lS) is the result I got.

Please keep in mind that this is the very beginning of this tool and think about the endless possibilities it can create."
86,11uk8ti,machinelearning,ChatGPT,comments,2023-03-18 10:15:33,[D] Totally Open Alternatives to ChatGPT,KingsmanVince,False,0.98,742,https://www.reddit.com/r/MachineLearning/comments/11uk8ti/d_totally_open_alternatives_to_chatgpt/,70,1679134533.0,"I have migrated this to GitHub for easy contribution: https://github.com/nichtdax/awesome-totally-open-chatgpt

By alternative, I mean projects feature different language model for chat system.
I do **not** count alternative **frontend** projects because they just call the API from OpenAI. 
I do **not** consider alternative **transformer decoder** to GPT 3.5 either because the training data of them are (mostly) not for chat system.

Tags:

-   B: bare (no data, no model's weight, no chat system)
-   F: full (yes data, yes model's weight, yes chat system including TUI and GUI)

| Project                                                                               | Description                                                                                                                                                                                                                                                                                                                                                                               | Tags |
| ------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- |
| [lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch)       | Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM                                                                                                                                                                                                                                                      | B    |
| [togethercomputer/OpenChatKit](https://github.com/togethercomputer/OpenChatKit)       | OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. [Demo](https://huggingface.co/spaces/togethercomputer/OpenChatKit)                                                                                                                                                                                    | F    |
| [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) | A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.                                                                                                                                                                                                                                                                                    | F    |
| [KoboldAI/KoboldAI-Client](https://github.com/KoboldAI/KoboldAI-Client)               | This is a browser-based front-end for AI-assisted writing with multiple local & remote AI models. It offers the standard array of tools, including Memory, Author's Note, World Info, Save & Load, adjustable AI settings, formatting options, and the ability to import existing AI Dungeon adventures. You can also turn on Adventure mode and play the game like AI Dungeon Unleashed. | F    |
| [LAION-AI/Open-Assistant/](https://github.com/LAION-AI/Open-Assistant/)               | OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.                                                                                                                                                                                                                                     | F    |"
87,11f29f9,machinelearning,ChatGPT,comments,2023-03-01 12:14:49,[R] ChatGPT failure increase linearly with addition on math problems,Neurosymbolic,False,0.94,238,https://www.reddit.com/r/MachineLearning/comments/11f29f9/r_chatgpt_failure_increase_linearly_with_addition/,66,1677672889.0," We did a study on ChatGPT's performance on math word problems. We found, under several conditions, its probability of failure increases linearly with the number of addition and subtraction operations - see below. This could imply that multi-step inference is a limitation. The performance also changes drastically when you restrict ChatGPT from showing its work (note the priors in the figure below, also see detailed breakdown of responses in the paper).

&#x200B;

[Math problems adds and subs vs. ChatGPT prob. of failure](https://preview.redd.it/z88ey3n6d4la1.png?width=1451&format=png&auto=webp&s=6da125b7a7cd60022ca70cd26434af6872a50d12)

ChatGPT Probability of Failure increase with addition and subtraction operations.

You the paper (preprint: [https://arxiv.org/abs/2302.13814](https://arxiv.org/abs/2302.13814)) will be presented at AAAI-MAKE next month. You can also check out our video here: [https://www.youtube.com/watch?v=vD-YSTLKRC8](https://www.youtube.com/watch?v=vD-YSTLKRC8)

&#x200B;

https://preview.redd.it/k58sbjd5d4la1.png?width=1264&format=png&auto=webp&s=5261923a2689201f905a26f06c6b5e9bac2fead6"
88,1060gfk,machinelearning,ChatGPT,comments,2023-01-07 21:38:33,[D] Will NLP Researchers Lose Our Jobs after ChatGPT?,singularpanda,False,0.61,9,https://www.reddit.com/r/MachineLearning/comments/1060gfk/d_will_nlp_researchers_lose_our_jobs_after_chatgpt/,63,1673127513.0,"Recently, ChatGPT has become one of the hottest tools in the NLP area. I have tried it and it gives me amazing and fancy results. I believe it will benefit most of the people and make a significant advance in our life. However, unfortunately, I, as an NLP researcher in text generation, feel all what I have done seems meaningless now. I also don't know what I can do as ChatGPT is already strong enough and can solve most of my previous concerns in text generation. Research on  ChatGPT also seems not possible as I believe it will not be an open-source project. Research on other NLP tasks also seems challenge as using a prompt in ChatGPT can solve most of the NLP tasks.  Any suggestions or comments are welcome."
89,12yr1eq,machinelearning,ChatGPT,comments,2023-04-25 18:05:32,"[P] HuggingChat (open source ChatGPT, interface + model)",lorepieri,False,0.94,238,https://www.reddit.com/r/MachineLearning/comments/12yr1eq/p_huggingchat_open_source_chatgpt_interface_model/,58,1682445932.0,[https://huggingface.co/chat/](https://huggingface.co/chat/)
90,12b9tx7,machinelearning,ChatGPT,comments,2023-04-04 05:20:04,[D] Closed AI Models Make Bad Baselines,leondz,False,0.95,233,https://www.reddit.com/r/MachineLearning/comments/12b9tx7/d_closed_ai_models_make_bad_baselines/,56,1680585604.0,"> That which is not open and reasonably reproducible cannot be considered a requisite baseline.

""What comes below is an attempt to bring together some discussions on the state of NLP research post-chatGPT.""

  https://hackingsemantics.xyz/2023/closed-baselines/

Interested to hear thoughts on this. Closed APIs with moving code behind them seem to be terrible bases for comparison, and demanding comparison with one shouldn't really be a way of blocking a publication, should it?"
91,zn0juq,machinelearning,ChatGPT,comments,2022-12-15 23:57:18,[P] Medical question-answering without hallucinating,tmblweeds,False,0.94,177,https://www.reddit.com/r/MachineLearning/comments/zn0juq/p_medical_questionanswering_without_hallucinating/,50,1671148638.0,"**tl;dr**I built a site that uses GPT-3.5 to answer natural-language medical questions using peer-reviewed medical studies.

**Live demo:** [**https://www.glaciermd.com/search**](https://www.glaciermd.com/search?utm_campaign=reddit_post_1)

**Background**

I've been working for a while on building a better version of WebMD, and I recently started playing around with LLMs, trying to figure out if there was anything useful there.

The problem with the current batch of ""predict-next-token"" LLMs is that they hallucinate—you can ask ChatGPT to answer medical questions, but it'll either

1. Refuse to answer (not great)
2. Give a completely false answer (really super bad)

So I spent some time trying to coax these LLMs to give answers based on a very specific set of inputs (peer-reviewed medical research) to see if I could get more accurate answers. And I did!

The best part is you can actually trace the final answer back to the original sources, which will hopefully instill some confidence in the result.

Here's how it works:

1. User types in a question
2. Pull top \~800 studies from Semantic Scholar and Pubmed
3. Re-rank using `sentence-transformers/multi-qa-MiniLM-L6-cos-v1`
4. Ask `text-davinci-003` to answer the question based on the top 10 studies (if possible)
5. Summarize those answers using `text-davinci-003`

Would love to hear what people think (and if there's a better/cheaper way to do it!).

\---

**UPDATE 1:** So far the #1 piece of feedback has been that I should be *way* more explicit about the fact that this is a proof-of-concept and not meant to be taken seriously. To that end, I've just added a screen that explains this and requires you to acknowledge it before continuing.

&#x200B;

https://preview.redd.it/jrt0yv3rfb6a1.png?width=582&format=png&auto=webp&s=38021decdfc7ed4bc3fe8caacaee2d09cd9b541e

Thoughts?

**Update 2:** Welp that's all the $$$ I have to spend on OpenAI credits, so the full demo isn't running anymore. But you can still follow the link above and browse existing questions/answers. Thanks for all the great feedback!"
92,121domd,machinelearning,ChatGPT,comments,2023-03-25 06:54:55,[N] March 2023 - Recent Instruction/Chat-Based Models and their parents,michaelthwan_ai,False,0.98,461,https://i.redd.it/oz51w0t22upa1.png,50,1679727295.0,
93,zzn35o,machinelearning,ChatGPT,comments,2022-12-31 06:04:44,An Open-Source Version of ChatGPT is Coming [News],lambolifeofficial,False,0.88,267,https://metaroids.com/news/an-open-source-version-of-chatgpt-is-coming/,50,1672466684.0,
94,1320hyh,machinelearning,ChatGPT,comments,2023-04-28 16:35:59,[P] Lamini rapidly achieves ChatGPT performance with an LLM Engine,gdiamos,False,0.46,0,https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/,45,1682699759.0,"According to the authors, Lamini AI has invented an LLM Engine for rapidly customizing models.  

Read the blog post, github, and huggingface for details.  

* Blog [https://lamini.ai/blog/introducing-lamini](https://lamini.ai/blog/introducing-lamini) 
* Code 
   * Chat data ([https://github.com/lamini-ai/lamini/](https://github.com/lamini-ai/lamini/)) 
   * SQL data ([https://github.com/lamini-ai/lamini-sql/](https://github.com/lamini-ai/lamini-sql/))
* LLM Type System Playground: [https://app.lamini.ai](https://app.lamini.ai/)
* Open-source fine-tuned LLMs that follow instructions: 
   * [weights](https://huggingface.co/lamini/instruct-tuned-2.8b) 
   * [playground](https://huggingface.co/spaces/lamini/instruct-playground)"
95,1088rnw,machinelearning,ChatGPT,comments,2023-01-10 12:35:07,[N] Microsoft Considers $10 Billion Investment in ChatGPT Creator --Bloomberg News,bikeskata,False,0.95,119,https://www.reddit.com/r/MachineLearning/comments/1088rnw/n_microsoft_considers_10_billion_investment_in/,42,1673354107.0,"Story here: https://www.bloomberg.com/news/articles/2023-01-10/microsoft-weighs-10-billion-chatgpt-investment-semafor-says?srnd=premium

Unpaywalled: https://archive.ph/XOOlg"
96,1135tir,machinelearning,ChatGPT,comments,2023-02-15 19:07:24,"[D] GLM 130B (Chinese-English Bilingual model) translations vs Google, Deepl Translate, NLLB and chatGPT",MysteryInc152,False,0.96,221,https://www.reddit.com/gallery/1135tir,38,1676488044.0,
97,121t6tp,machinelearning,ChatGPT,comments,2023-03-25 17:41:20,[P] A 'ChatGPT Interface' to Explore Your ML Datasets -> app.activeloop.ai,davidbun,False,0.95,1059,https://v.redd.it/n5l842qa9xpa1,38,1679766080.0,
98,11f9k5g,machinelearning,ChatGPT,comments,2023-03-01 17:23:03,"[P] ChatRWKV v2 (can run RWKV 14B with 3G VRAM), RWKV pip package, and finetuning to ctx16K",bo_peng,False,0.98,92,https://www.reddit.com/r/MachineLearning/comments/11f9k5g/p_chatrwkv_v2_can_run_rwkv_14b_with_3g_vram_rwkv/,37,1677691383.0,"Hi everyone. Now ChatRWKV v2 can split RWKV to multiple GPUs, or stream layers (compute layer-by-layer), so you can run RWKV 14B with as few as 3G VRAM. [https://github.com/BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)

Example:

`'cuda:0 fp16 *10 -> cuda:1 fp16 *8 -> cpu fp32'` = first 10 layers on cuda:0 fp16, then 8 layers on cuda:1 fp16, then on cpu fp32

`'cuda fp16 *20+'` = first 20 layers on cuda fp16, then stream the rest on it

And RWKV is now a pip package: [https://pypi.org/project/rwkv/](https://pypi.org/project/rwkv/)

    os.environ['RWKV_JIT_ON'] = '1'
    os.environ[""RWKV_CUDA_ON""] = '0' # if '1' then compile CUDA kernel for seq mode (much faster)
    from rwkv.model import RWKV
    from rwkv.utils import PIPELINE, PIPELINE_ARGS
    pipeline = PIPELINE(model, ""20B_tokenizer.json"") # find it in https://github.com/BlinkDL/ChatRWKV
    # download models: https://huggingface.co/BlinkDL
    model = RWKV(model='/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-169m/RWKV-4-Pile-169M-20220807-8023', strategy='cpu fp32')
    ctx = ""\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.""
    print(ctx, end='')
    def my_print(s):
        print(s, end='', flush=True)
    # For alpha_frequency and alpha_presence, see ""Frequency and presence penalties"":
    # https://platform.openai.com/docs/api-reference/parameter-details
    args = PIPELINE_ARGS(temperature = 1.0, top_p = 0.7,
        alpha_frequency = 0.25,
        alpha_presence = 0.25,
        token_ban = [0], # ban the generation of some tokens
        token_stop = []) # stop generation whenever you see any token here
    pipeline.generate(ctx, token_count=512, args=args, callback=my_print)

Right now all RWKV models are still trained with GPT-like method, so they are limited by the ctxlen used in training, even though in theory they should have almost infinite ctxlen (because they are RNNs). However RWKV models can be easily finetuned to support longer ctxlens (and large models actually use the ctxlen). I have finetuned 1B5/3B/7B/14B to ctx4K, and now finetuning 7B/14B to ctx8K, and 14B to ctx16K after that :) All models are available at [https://huggingface.co/BlinkDL](https://huggingface.co/BlinkDL)

The core RWKV is still mostly an one-man project, but a number of great developers are building on top of it, and you are welcome to join our community :)"
99,10lp3g4,machinelearning,ChatGPT,comments,2023-01-26 10:48:19,Few questions about scalability of chatGPT [D],besabestin,False,0.83,25,https://www.reddit.com/r/MachineLearning/comments/10lp3g4/few_questions_about_scalability_of_chatgpt_d/,36,1674730099.0,"I have two questions about chatGPT. I don't come from a machine learning background. I am just a programmer. So bear with me if they sound a bit dumb.

I was checking about chatGPT a bit the last week. I went through their papers and also tried out a fine tuning by myself by creating some fictional world and giving it some examples. 

The first thing I wondered is what is very special about the model than the large data and parameter set it has, that other competitors can't do. I ask this because I have seen a lot of ""google killer"" discussions in some places. From what I understood from their papers I thought it is something another company with the computing power and the filtered data can have up and running in few months. I see their advantage in rolling out to the public because with feedbacks from actual users all over the world it can potentially be retrained.

The second thing I wondered is its scalability. It feels to me that it is a very big challenge to keep it scalable in the future. Currently getting a long text out of it is kind of painful because it has to continuously generate. I think it is continuously calculating with the huge parameter set it has. I wonder also about new trends, if it needs to be retrained. I also used it for a fine tuning, where I created a fictional world with its own law and rules and the fine tuning took hours in the queue - so is it creating separate parameters for my case? that would be a lot considering how much parameter set they have."
100,zx7cxn,machinelearning,ChatGPT,comments,2022-12-28 11:32:22,[D] DeepMind has at least half a dozen prototypes for abstract/symbolic reasoning. What are their approaches?,valdanylchuk,False,0.97,209,https://www.reddit.com/r/MachineLearning/comments/zx7cxn/d_deepmind_has_at_least_half_a_dozen_prototypes/,35,1672227142.0,"In TED Interview on the future of AI from three months ago, Demis Hassabis says he spends most of his time on the problem of abstract concepts, conceptual knowledge, and approaches to move deep learning systems into the realm of symbolic reasoning and mathematical discovery. He says at DeepMind they have at least half a dozen internal prototype projects working in that direction:

https://youtu.be/I5FrFq3W25U?t=2550

Earlier, around the 28min mark, he says that while current LLMs are very impressive, they are nowhere near reaching sentience or consciousness, among other things, because they are very data-inefficient in their learning. 

Can we infer their half dozen approaches to abstract reasoning from the research published by DeepMind so far? Or is this likely to be some yet unreleased new research?

DeepMind list many (not sure if all) of their papers here:

https://www.deepmind.com/research

I was able to find some related papers there, but I am not qualified to judge their significance, and I probably missed some important ones because of the less obvious titles. 

https://www.deepmind.com/publications/symbolic-behaviour-in-artificial-intelligence

https://www.deepmind.com/publications/discovering-symbolic-models-from-deep-learning-with-inductive-biases

https://www.deepmind.com/publications/neural-symbolic-vqa-disentangling-reasoning-from-vision-and-language-understanding

https://www.deepmind.com/publications/learning-symbolic-physics-with-graph-networks

https://www.deepmind.com/publications/how-to-transfer-algorithmic-reasoning-knowledge-to-learn-new-algorithms

https://www.deepmind.com/publications/a-simple-approach-for-state-action-abstractionusing-a-learned-mdp-homomorphism

Can anyone help summarize the approaches currently considered promising in this problem? Are we missing something bigger coming up behind all the hype around ChatGPT?"
101,12gr91a,machinelearning,ChatGPT,comments,2023-04-09 18:25:12,[D] The Complete Guide to Spiking Neural Networks,s_arme,False,0.94,171,https://www.reddit.com/r/MachineLearning/comments/12gr91a/d_the_complete_guide_to_spiking_neural_networks/,34,1681064712.0,"Greetings, r/MachineLearning community!  
Spiking Neural Networks (SNNs) are a type of Neural Networks that mimic the way neurons in the brain work. These networks are capable of producing temporal responses, and this makes them particularly interesting where power efficiency is important. They are [trending](https://trends.google.com/trends/explore/TIMESERIES/1681063800?hl=en-GB&tz=-120&date=2012-01-09+2023-03-09&q=%2Fm%2F02q3qrf&sni=3) (not as much as chatgpt), yet more research is needed to become mainstream in certain tasks.

I wrote this guide to cover fundamentals, advantages and caveats that needs to be addressed. I hope you enjoy it. Any thoughts or feedback is appreciated!

[https://pub.towardsai.net/the-complete-guide-to-spiking-neural-networks-d0a85fa6a64](https://pub.towardsai.net/the-complete-guide-to-spiking-neural-networks-d0a85fa6a64)"
102,11kk3iq,machinelearning,ChatGPT,comments,2023-03-07 00:54:07,[R] PyReason: logic for use with ML,Neurosymbolic,False,0.97,88,https://www.reddit.com/r/MachineLearning/comments/11kk3iq/r_pyreason_logic_for_use_with_ml/,30,1678150447.0,"Last week,  we released a paper on [PyReason on Arxiv](https://arxiv.org/pdf/2302.13482.pdf). PyReason is a Python package for logical inference and designed for use with machine learning ([https://github.com/lab-v2/pyreason](https://github.com/lab-v2/pyreason)).

You may think that’s all fine and good, but are wondering why would we need a logic for machine learning? In this post, I’ll discuss why we did it.

First, a lot of the criticism of machine learning, especially deep learning is that while it obtains excellent result son may tasks, it is merely mimicking historical data and not learning actual relationships. This has resulted in a lot of the major shortcomings in ML such as the [hallucinations](https://www.nytimes.com/2023/02/17/podcasts/hard-fork-bing-ai-elon.html?action=click&module=audio-series-bar&region=header&pgtype=Article) of large language models, the requirements of vast amounts of training data to learn games, and brittleness in certain applications (e.g., the recent defeat of AlphaGo, difficulty in [solving math problems](https://www.reddit.com/r/MachineLearning/comments/11f29f9/r_chatgpt_failure_increase_linearly_with_addition/)). In a video lecture, we review some of these shortcomings, much of which constitutes active areas of research ([part 1](https://www.youtube.com/watch?v=9cooDzgd8NA), [part 2](https://www.youtube.com/watch?v=d2xfgwovwso)).

Then enter “[neuro symbolic](https://neurosymoblic.asu.edu/)” artificial intelligence. Actually an old idea where neural architectures can work hand-in-hand with logic, often even having an equivalence between the two. The idea is symbolic AI has many shortcomings (brittleness to noise, difficulty in learning) that can be address with deep learning while its strengths (modularity, ability to add constraints, symbolic manipulation) can address some of deep learning’s limitations.

Neuro symbolic AI is a highly active area of research, and much of the advancements have identified special logical languages to use in their approach. Our goal with PyReason was to unify many of these logics and provide logic capabilities in a robust and modern Python implementation. We are working on a few joint projects with industry partners applying this to various use-cases, and now we have made the code base and library available as an open source package. In a [video](https://www.youtube.com/watch?v=E1PSl3KQCmo&t=8s), we outline six major capabilities that we felt were important:

1. Open world reasoning – ability to reason in uncertain situations (important for interfacing with ML models)
2. Multi-step inference
3. Explainability
4. Temporal reasoning
5. Graph-based reasoning
6. Designed to support neuro symbolic frameworks

The release of PyReason will kick off not only new research by our group and our collaborators, but also associated software. We’re pretty excited about this new direction!"
103,129qi8p,machinelearning,ChatGPT,comments,2023-04-02 16:39:23,[R] HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace - Yongliang Shen et al Microsoft Research Asia 2023 - Able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results!,Singularian2501,False,0.94,170,https://www.reddit.com/r/MachineLearning/comments/129qi8p/r_hugginggpt_solving_ai_tasks_with_chatgpt_and/,30,1680453563.0,"Paper: [https://arxiv.org/abs/2303.17580](https://arxiv.org/abs/2303.17580) 

Abstract:

>Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in HuggingFace, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which **paves a new way towards AGI.** 

https://preview.redd.it/huc5so9f1ira1.jpg?width=1201&format=pjpg&auto=webp&s=cd714263f8a6ea443195316d95704fd550beee95

https://preview.redd.it/d2dfhs9f1ira1.jpg?width=655&format=pjpg&auto=webp&s=07fcb2b969cdaaf649aed259296f3dfa9157531e

https://preview.redd.it/v4gc9r9f1ira1.jpg?width=773&format=pjpg&auto=webp&s=b014fa679a7bdc2024a3d27690950be2248735aa"
104,123b4f0,machinelearning,ChatGPT,comments,2023-03-27 04:19:33,[D] Will prompting the LLM to review it's own answer be any helpful to reduce chances of hallucinations? I tested couple of tricky questions and it seems it might work.,tamilupk,False,0.86,46,https://i.redd.it/n77jd7fpj7qa1.png,29,1679890773.0,
105,11wt2fl,machinelearning,ChatGPT,comments,2023-03-20 19:30:55,[P] OpenAssistant is now live on reddit (Open Source ChatGPT alternative),pixiegirl417,False,0.98,204,https://www.reddit.com/r/MachineLearning/comments/11wt2fl/p_openassistant_is_now_live_on_reddit_open_source/,29,1679340655.0,"OpenAssistant bot is live on /r/ask_open_assistant. There are some limitations to the reddit bot; you can also try on the model in chat mode at https://huggingface.co/spaces/olivierdehaene/chat-llm-streaming. Model is available for free download at https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b.


Prompt it by creating a new text post (responds to text body of post), starting a comment with !OpenAssistant, or by replying directly to it. 

I have recently enabled memory for the bot so it should do a (pretty mediocre) job of continuing a conversation with you."
106,12r91g1,machinelearning,ChatGPT,comments,2023-04-18 23:42:21,[P] GPT4 is my new co-founder,Jman9107,False,0.44,0,https://www.reddit.com/r/MachineLearning/comments/12r91g1/p_gpt4_is_my_new_cofounder/,28,1681861341.0,"GPT4 helped me build a pretty incredible app, and in a totally full stack way. First, we identified the biggest hole in the AI market: a voice-first, web-connected, clean mobile app to bring ChatGPT to the masses. Then, it helped me with feature dev, backend, frontend, and even this post.

Ended up calling it [Jackchat](https://www.jackchat.ai/) (had to name it after myself lol). You can use voice to talk to ChatGPT (big voice button), it can talk back to you with voice, it’s connected to the web, it's free, and it doesn’t require an account to use. Surprisingly, it's replaced me and most of my friend’s Google usage.

Check it out for free here: [http://jackchat.ai](http://jackchat.ai/) (available on web, iOS, and Android)"
107,11mlwty,machinelearning,ChatGPT,comments,2023-03-09 07:24:35,"[R] Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",MysteryInc152,False,0.97,867,https://www.reddit.com/gallery/11mlwty,26,1678346675.0,
108,11qgxs8,machinelearning,ChatGPT,comments,2023-03-13 18:11:39,[D] ChatGPT without text limits.,spiritus_dei,False,0.87,60,https://www.reddit.com/r/MachineLearning/comments/11qgxs8/d_chatgpt_without_text_limits/,27,1678731099.0,"One of the biggest limitations of large language models is the text limit. This limits their use cases and prohibits more ambitious prompts.

This was recently resolved by researchers at Google Brain in Alberta, Canada. In their recent paper they describe a new method of using associative memory which removes the text limit and they also prove that some large language models are universal Turing machines.

This will pave the way for entire novels being shared with large language models, personal genomes, etc.

The paper talks about the use of ""associative memory"" which is also known as content-addressable memory (CAM). This type of memory allows the system to retrieve data based on its content rather than its location. Unlike traditional memory systems that use specific memory addresses to access data, associative memory uses CAM to find data based on a pattern or keyword.

Presumably, this will open up a new market for associative memory since I would happily pay some extra money for content to be permanently stored in associative memory and to remove the text limit. This will also drive down the price of associative memory if millions of people are willing to pay a monthly fee for storage and the removal of prompt text limits.

The paper does point that there are still problems with conditional statements that confuse the large language models. However, I believe this can be resolved with semantic graphs. This would involve collecting data from various sources and using natural language processing techniques to extract entities and relationships from the text. Once the graph is constructed, it could be integrated into the language model in a variety of ways. One approach is to use the graph as an external memory, similar to the approach taken in the paper. The graph can be encoded as a set of key-value pairs and used to augment the model's attention mechanism during inference. The attention mechanism can then focus on relevant nodes in the graph when generating outputs.

Another potential approach is to incorporate the graph into the model's architecture itself. For example, the graph can be used to inform the initialization of the model's parameters or to guide the attention mechanism during training. This could help the model learn to reason about complex concepts and relationships more effectively, potentially leading to better performance on tasks that require this kind of reasoning.

The use of knowledge graphs can also help ground truth large language models and reduce hallucinations.

I'm curious to read your thoughts."
109,10ofcis,machinelearning,ChatGPT,comments,2023-01-29 18:57:37,[P] AI Content Detector,YoutubeStruggle,False,0.32,0,https://www.reddit.com/r/MachineLearning/comments/10ofcis/p_ai_content_detector/,27,1675018657.0,"Have you tried ChatGPT? It's super cool but some users are also using it to create automated content submissions and resulting in an increase in AI-generated plagiarism. I have made a tool as a college project to detect content generated using AI.  
Go ahead and validate your content on [AI Content Detector](https://ai-content-detector.online/)  
If you are an educator worried about automated content submissions or developers worried about search engine penalties, this tool will help everyone to efficiently detect content generated using AI."
110,11ibm1j,machinelearning,ChatGPT,comments,2023-03-04 20:02:40,[D] First glance at LLaMA,enryu42,False,0.91,66,https://www.reddit.com/r/MachineLearning/comments/11ibm1j/d_first_glance_at_llama/,27,1677960160.0,"[https://medium.com/@enryu9000/mini-post-first-look-at-llama-4403517d41a1](https://medium.com/@enryu9000/mini-post-first-look-at-llama-4403517d41a1)  


I'm kind of surprised - I expected it to be much better than ChatGPT, but results are all over the place (e.g. it is better for few-shot classification, but worse for SQL generation).  


I wonder what makes ChatGPT so decent; given that OpenAI can afford to serve it, it is probably an order of magnitude smaller than LLaMA, yet it is competitive; can RLHF get the model that far?"
111,139tthh,machinelearning,ChatGPT,comments,2023-05-06 15:57:34,[D] perplexity.ai appreciation / information post,cooperbaerseth,False,0.77,40,https://www.reddit.com/r/MachineLearning/comments/139tthh/d_perplexityai_appreciation_information_post/,26,1683388654.0,"How many other people here are using or interested in [perplexity.ai](https://perplexity.ai/)? I gravitate towards it much more than ChatGPT now. It feels like being able to check the sources of the answer the model gives puts the power back in the user's hands rather than just blindly trusting.

Further, does anyone have information on the approach they may use? There must be some extra layers in order to be able to site sources. To me it seems like ChatGPT and the like are much more of a black box than this model."
112,13bua1t,machinelearning,ChatGPT,comments,2023-05-08 14:59:45,[Research] Can LLMs do meaning causal reasoning? Preprint says yes but I think it's hype.,buggaby,False,0.77,39,https://www.reddit.com/r/MachineLearning/comments/13bua1t/research_can_llms_do_meaning_causal_reasoning/,26,1683557985.0,"Here's the preprint.

https://arxiv.org/abs/2305.00050

This papers is 42 pages long without citations, so I didn't read it all, but I scanned it all and read in depth several sections. I would be interested in whether I missed something here. 

The main argument seems to be that ChatGPT can do ""causal discovery"" better than other algorithmic approaches. If true, this could be really big. Imagine giving a data set and an algorithm gives you even a better-than-chance determination of causal relationships? This could help give really meaningful context to data sets and inform science in a real way.

And this paper also seems to at least recognize the need to control for data contamination by testing whether a data set has been ""memorized"", or is in the training set.

But there's a huge problem. On page 7, we get this

>LLMs offer a fresh perspective on the causal discovery problem by focusing on the metadata associated with variables in a dataset, rather than their data values.

As far as I can tell, this paper is nothing but asking causal questions of the column names in a table. So you have a table with n columns, 2 of which are ""Amount of rain"" and ""Number of car crashes"", and then you ask ChatGPT if the amount of rain causes the number of car crashes or the reverse. (Section 3.1: ""Pairwise causal discovery"") The paper then says that this means ChatGPT is doing ""causal analysis"" on this dataset. Wow!

(Side note: Why spend all the time they do talking about how they tested for data contamination if they aren't even using the data? The better question is whether the names of the data columns are included in descriptive text anywhere in the training set, and that's not something that can be probed using the method they describe.)

Basically, they are offering ""a new frontier for causality"" by just asking if A causes B or the reverse without knowing if sentences saying that A causes B are included in the training data. The performance of the models in this paper seem to be entirely because of data contamination. And this offers nothing over just asking a human to quickly say which is causing which. There's no identification of **new** causal links, for example.

Am I missing anything, or is this just more Microsoft advertising-pretending-to-be-real-research?"
113,11zsdwv,machinelearning,ChatGPT,relevance,2023-03-23 18:09:11,[N] ChatGPT plugins,Singularian2501,False,0.97,442,https://www.reddit.com/r/MachineLearning/comments/11zsdwv/n_chatgpt_plugins/,144,1679594951.0,"[https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins)

>We’ve implemented initial support for plugins in ChatGPT. Plugins are tools designed specifically for language models with safety as a core principle, and help ChatGPT access up-to-date information, run  computations, or use third-party services."
114,12nbixk,machinelearning,ChatGPT,relevance,2023-04-15 17:14:58,[P] OpenAssistant - The world's largest open-source replication of ChatGPT,ykilcher,False,0.97,1274,https://www.reddit.com/r/MachineLearning/comments/12nbixk/p_openassistant_the_worlds_largest_opensource/,175,1681578898.0,"We’re excited to announce the release of OpenAssistant.

The future of AI development depends heavily on high quality datasets and models being made publicly available, and that’s exactly what this project does.

Watch the annoucement video:

[https://youtu.be/ddG2fM9i4Kk](https://youtu.be/ddG2fM9i4Kk)

&#x200B;

Our team has worked tirelessly over the past several months collecting large amounts of text-based input and feedback to create an incredibly diverse and unique dataset designed specifically for training language models or other AI applications.

With over 600k human-generated data points covering a wide range of topics and styles of writing, our dataset will be an invaluable tool for any developer looking to create state-of-the-art instruction models!

To make things even better, we are making this entire dataset free and accessible to all who wish to use it. Check it out today at our HF org: OpenAssistant

On top of that, we've trained very powerful models that you can try right now at: [open-assistant.io/chat](https://open-assistant.io/chat) !"
115,11fbccz,machinelearning,ChatGPT,relevance,2023-03-01 18:31:12,[D] OpenAI introduces ChatGPT and Whisper APIs (ChatGPT API is 1/10th the cost of GPT-3 API),minimaxir,False,0.97,579,https://www.reddit.com/r/MachineLearning/comments/11fbccz/d_openai_introduces_chatgpt_and_whisper_apis/,119,1677695472.0,"https://openai.com/blog/introducing-chatgpt-and-whisper-apis

> It is priced at $0.002 per 1k tokens, which is 10x cheaper than our existing GPT-3.5 models.

This is a massive, massive deal. For context, the reason GPT-3 apps took off over the past few months before ChatGPT went viral is because a) text-davinci-003 was released and was a significant performance increase and b) the cost was cut from $0.06/1k tokens to $0.02/1k tokens, which made consumer applications feasible without a large upfront cost.

A much better model and a 1/10th cost warps the economics completely to the point that it may be better than in-house finetuned LLMs.

I have no idea how OpenAI can make money on this. This has to be a loss-leader to lock out competitors before they even get off the ground."
116,11uk8ti,machinelearning,ChatGPT,relevance,2023-03-18 10:15:33,[D] Totally Open Alternatives to ChatGPT,KingsmanVince,False,0.98,750,https://www.reddit.com/r/MachineLearning/comments/11uk8ti/d_totally_open_alternatives_to_chatgpt/,70,1679134533.0,"I have migrated this to GitHub for easy contribution: https://github.com/nichtdax/awesome-totally-open-chatgpt

By alternative, I mean projects feature different language model for chat system.
I do **not** count alternative **frontend** projects because they just call the API from OpenAI. 
I do **not** consider alternative **transformer decoder** to GPT 3.5 either because the training data of them are (mostly) not for chat system.

Tags:

-   B: bare (no data, no model's weight, no chat system)
-   F: full (yes data, yes model's weight, yes chat system including TUI and GUI)

| Project                                                                               | Description                                                                                                                                                                                                                                                                                                                                                                               | Tags |
| ------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- |
| [lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch)       | Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM                                                                                                                                                                                                                                                      | B    |
| [togethercomputer/OpenChatKit](https://github.com/togethercomputer/OpenChatKit)       | OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. [Demo](https://huggingface.co/spaces/togethercomputer/OpenChatKit)                                                                                                                                                                                    | F    |
| [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) | A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.                                                                                                                                                                                                                                                                                    | F    |
| [KoboldAI/KoboldAI-Client](https://github.com/KoboldAI/KoboldAI-Client)               | This is a browser-based front-end for AI-assisted writing with multiple local & remote AI models. It offers the standard array of tools, including Memory, Author's Note, World Info, Save & Load, adjustable AI settings, formatting options, and the ability to import existing AI Dungeon adventures. You can also turn on Adventure mode and play the game like AI Dungeon Unleashed. | F    |
| [LAION-AI/Open-Assistant/](https://github.com/LAION-AI/Open-Assistant/)               | OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.                                                                                                                                                                                                                                     | F    |"
117,zhrgln,machinelearning,ChatGPT,relevance,2022-12-10 12:32:57,[P] I made a command-line tool that explains your errors using ChatGPT (link in comments),jsonathan,False,0.97,2863,https://i.redd.it/kq518l9ne25a1.gif,112,1670675577.0,
118,1095os9,machinelearning,ChatGPT,relevance,2023-01-11 14:12:57,[D] Microsoft ChatGPT investment isn't about Bing but about Cortana,fintechSGNYC,False,0.89,401,https://www.reddit.com/r/MachineLearning/comments/1095os9/d_microsoft_chatgpt_investment_isnt_about_bing/,173,1673446377.0,"I believe that Microsoft's 10B USD investment in ChatGPT is less about Bing and more about turning Cortana into an Alexa for corporates.   
Examples: Cortana prepare the new T&Cs... Cortana answer that client email... Cortana prepare the Q4 investor presentation (maybe even with PowerBI integration)... Cortana please analyze cost cutting measures... Cortana please look up XYZ... 

What do you think?"
119,128lo83,machinelearning,ChatGPT,relevance,2023-04-01 12:57:30,[R] [P] I generated a 30K-utterance dataset by making GPT-4 prompt two ChatGPT instances to converse.,radi-cho,False,0.96,802,https://i.redd.it/bywcz1kzs9ra1.png,104,1680353850.0,
120,120usfk,machinelearning,ChatGPT,relevance,2023-03-24 19:15:58,[R] Hello Dolly: Democratizing the magic of ChatGPT with open models,austintackaberry,False,0.98,600,https://www.reddit.com/r/MachineLearning/comments/120usfk/r_hello_dolly_democratizing_the_magic_of_chatgpt/,109,1679685358.0,"Databricks shows that anyone can take a dated off-the-shelf open source large language model (LLM) and give it magical ChatGPT-like instruction following ability by training it in less than three hours on one machine, using high-quality training data.

They fine tuned GPT-J using the Alpaca dataset.

Blog: [https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)  
Github: [https://github.com/databrickslabs/dolly](https://github.com/databrickslabs/dolly)"
121,10gtruu,machinelearning,ChatGPT,relevance,2023-01-20 10:41:04,[N] OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic,ChubChubkitty,False,0.83,519,https://www.reddit.com/r/MachineLearning/comments/10gtruu/n_openai_used_kenyan_workers_on_less_than_2_per/,246,1674211264.0,https://time.com/6247678/openai-chatgpt-kenya-workers/
122,121t6tp,machinelearning,ChatGPT,relevance,2023-03-25 17:41:20,[P] A 'ChatGPT Interface' to Explore Your ML Datasets -> app.activeloop.ai,davidbun,False,0.95,1053,https://v.redd.it/n5l842qa9xpa1,38,1679766080.0,
123,zstequ,machinelearning,ChatGPT,relevance,2022-12-22 18:39:30,[D] When chatGPT stops being free: Run SOTA LLM in cloud,_underlines_,False,0.95,354,https://www.reddit.com/r/MachineLearning/comments/zstequ/d_when_chatgpt_stops_being_free_run_sota_llm_in/,95,1671734370.0,"Edit: Found [LAION-AI/OPEN-ASSISTANT](https://github.com/LAION-AI/Open-Assistant) a very promising project opensourcing the idea of chatGPT. [video here](https://www.youtube.com/watch?v=8gVYC_QX1DI)

**TL;DR: I found GPU compute to be [generally cheap](https://github.com/full-stack-deep-learning/website/blob/main/docs/cloud-gpus/cloud-gpus.csv) and spot or on-demand instances can be launched on AWS for a few USD / hour up to over 100GB vRAM. So I thought it would make sense to run your own SOTA LLM like Bloomz 176B inference endpoint whenever you need it for a few questions to answer. I thought it would still make more sense than shoving money into a closed walled garden like ""not-so-OpenAi"" when they make ChatGPT or GPT-4 available for $$$. But I struggle due to lack of tutorials/resources.**

Therefore, I carefully checked benchmarks, model parameters and sizes as well as training sources for all SOTA LLMs [here](https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit#gid=1158069878).

Knowing since reading the Chinchilla paper that Model Scaling according to OpenAI was wrong and more params != better quality generation. So I was looking for the best performing LLM openly available in terms of quality and broadness to use for multilingual everyday questions/code completion/reasoning similar to what chatGPT provides (minus the fine-tuning for chat-style conversations).

My choice fell on [Bloomz](https://huggingface.co/bigscience/bloomz) (because that handles multi-lingual questions well and has good zero shot performance for instructions and Q&A style text generation. Confusingly Galactica seems to outperform Bloom on several benchmarks. But since Galactica had a very narrow training set only using scientific papers, I guess usage is probably limited for answers on non-scientific topics.

Therefore I tried running the original bloom 176B and alternatively also Bloomz 176B on AWS SageMaker JumpStart, which should be a one click deployment. This fails after 20min. On Azure ML, I tried using DeepSpeed-MII which also supports bloom but also fails due the instance size of max 12GB vRAM I guess.

From my understanding to save costs on inference, it's probably possible to use one or multiple of the following solutions:

- Precision: int8 instead of fp16
- [Microsoft/DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) for an up 40x reduction on inference cost on Azure, this thing also supports int8 and fp16 bloom out of the box, but it fails on Azure due to instance size.
- [facebook/xformer](https://github.com/facebookresearch/xformers) not sure, but if I remember correctly this brought inference requirements down to 4GB vRAM for StableDiffusion and DreamBooth fine-tuning to 10GB. No idea if this is usefull for Bloom(z) inference cost reduction though

I have a CompSci background but I am not familiar with most stuff, except that I was running StableDiffusion since day one on my rtx3080 using linux and also doing fine-tuning with DreamBooth. But that was all just following youtube tutorials. I can't find a single post or youtube video of anyone explaining a full BLOOM / Galactica / BLOOMZ inference deployment on cloud platforms like AWS/Azure using one of the optimizations mentioned above, yet alone deployment of the raw model. :(

I still can't figure it out by myself after 3 days.

**TL;DR2: Trying to find likeminded people who are interested to run open source SOTA LLMs for when chatGPT will be paid or just for fun.**

Any comments, inputs, rants, counter-arguments are welcome.

/end of rant"
124,11v6bvv,machinelearning,ChatGPT,relevance,2023-03-19 00:45:37,[P] Let's build ChatGPT,blatant_variable,False,0.96,368,https://www.reddit.com/r/MachineLearning/comments/11v6bvv/p_lets_build_chatgpt/,16,1679186737.0,"Hi all, I just made a tutorial on how to build a basic RLHF system on top of Andrej Karpathy's nanoGPT. I'm grateful to have gotten a thumbs up on Twitter from the legend himself, always a bit nerve wracking making this sort of thing.

I'm sharing this here because I'd love to go deeper into teaching and building this out, if people are interested in watching this sort of thing. Would be very helpful to hear your thoughts.

Here's the code:

https://github.com/sanjeevanahilan/nanoChatGPT

The video: 

https://m.youtube.com/watch?v=soqTT0o1ZKo&feature=youtu.be"
125,11f29f9,machinelearning,ChatGPT,relevance,2023-03-01 12:14:49,[R] ChatGPT failure increase linearly with addition on math problems,Neurosymbolic,False,0.94,241,https://www.reddit.com/r/MachineLearning/comments/11f29f9/r_chatgpt_failure_increase_linearly_with_addition/,66,1677672889.0," We did a study on ChatGPT's performance on math word problems. We found, under several conditions, its probability of failure increases linearly with the number of addition and subtraction operations - see below. This could imply that multi-step inference is a limitation. The performance also changes drastically when you restrict ChatGPT from showing its work (note the priors in the figure below, also see detailed breakdown of responses in the paper).

&#x200B;

[Math problems adds and subs vs. ChatGPT prob. of failure](https://preview.redd.it/z88ey3n6d4la1.png?width=1451&format=png&auto=webp&s=6da125b7a7cd60022ca70cd26434af6872a50d12)

ChatGPT Probability of Failure increase with addition and subtraction operations.

You the paper (preprint: [https://arxiv.org/abs/2302.13814](https://arxiv.org/abs/2302.13814)) will be presented at AAAI-MAKE next month. You can also check out our video here: [https://www.youtube.com/watch?v=vD-YSTLKRC8](https://www.youtube.com/watch?v=vD-YSTLKRC8)

&#x200B;

https://preview.redd.it/k58sbjd5d4la1.png?width=1264&format=png&auto=webp&s=5261923a2689201f905a26f06c6b5e9bac2fead6"
126,zc5sg6,machinelearning,ChatGPT,relevance,2022-12-04 09:57:44,[D] OpenAI’s ChatGPT is unbelievable good in telling stories!,Far_Pineapple770,False,0.91,241,https://www.reddit.com/r/MachineLearning/comments/zc5sg6/d_openais_chatgpt_is_unbelievable_good_in_telling/,85,1670147864.0,"
I started playing with ChatGPT, the new chatbot created by OpenAI and is free to use at the moment. 

I asked it to describe a scene between Batman and Joker on a subway platform and the result is mind blowing! I let you have a read of the story yourself:

Generated by ChatGPT:

The scene is set in a dimly lit subway station, with the Joker standing on the platform, gazing up at Batman who is perched on a nearby overhead beam. The Joker is dressed in his signature purple suit and green hair, a wicked grin plastered across his face.

In his hand, he holds a deck of playing cards, flipping them expertly through his fingers. The sound of the cards shuffling echoes through the empty station.

Batman, dressed in his dark suit and cape, stares down at the Joker with a steely gaze. His muscles are tense, ready for any sudden movements from the villain.

The Joker cackles, his laughter bouncing off the walls of the station. ""Hello, Batsy! Long time no see. How's the crime-fighting business these days?""

Batman remains silent, watching the Joker carefully.

The Joker's grin widens. ""Come on, Bats. Don't be so serious all the time. Let's have some fun."" He tosses a card at Batman, who easily catches it.

""I'm not here to play games, Joker,"" Batman says in a grave tone.

The Joker's laughter turns into a maniacal cackle. ""But that's the whole point, Batsy! Life is a game. And I'm the master of it.""

He pulls out a bomb from his pocket and holds it up, the timer ticking down. ""Let's see how well you can play, shall we?""

Batman leaps down from the beam, landing gracefully on the platform. He advances towards the Joker, who backs away, still holding the bomb.

The tension in the air is palpable as the two foes face off, each ready to outsmart the other. The sound of the bomb's timer counts down, adding to the tension of the scene.

The Joker's grin never fades, even as Batman inches closer. ""It's always a pleasure to play with you, Batsy. But I think it's time for me to make my exit.""

With a flourish, the Joker disappears in a cloud of smoke, leaving Batman standing alone on the platform. The bomb detonates, causing a small explosion and sending debris flying.

But Batman remains unharmed, ready to continue the never-ending battle against the Joker and all the evil that plagues the city.

End of the story.

I used the first two paragraphs in OpenAI’s Dall E 2 to generate an image and [this](https://labs.openai.com/s/p8L8Z6c3S7qZ26U7owfbx7lS) is the result I got.

Please keep in mind that this is the very beginning of this tool and think about the endless possibilities it can create."
127,zzn35o,machinelearning,ChatGPT,relevance,2022-12-31 06:04:44,An Open-Source Version of ChatGPT is Coming [News],lambolifeofficial,False,0.88,265,https://metaroids.com/news/an-open-source-version-of-chatgpt-is-coming/,50,1672466684.0,
128,120csub,machinelearning,ChatGPT,relevance,2023-03-24 07:32:32,[P] ChatGPT with GPT-2: A minimum example of aligning language models with RLHF similar to ChatGPT,liyanjia92,False,0.95,77,https://www.reddit.com/r/MachineLearning/comments/120csub/p_chatgpt_with_gpt2_a_minimum_example_of_aligning/,15,1679643152.0,"hey folks, happy Friday! I wish to get some feedback for my recent project of a minimum example of using RLHF on language models to improve human alignment. 

The goal is to compare with vanilla GPT-2 and supervised fine-tuned GPT-2 to see how much RLHF can benefit small models. Also I hope this project can show an example of the minimum requirements to build a RLHF training pipeline for LLMs.

Github: https://github.com/ethanyanjiali/minChatGPT
Demo: https://colab.research.google.com/drive/1LR1sbWTyaNAmTZ1g1M2tpmU_pFw1lyEX?usp=sharing

Thanks a lot for any suggestions and feedback!"
129,12vaauo,machinelearning,ChatGPT,relevance,2023-04-22 15:59:25,[P] Easily make complex plots using ChatGPT [open source],ofirpress,False,0.9,232,https://v.redd.it/gz8mwx5okgva1,22,1682179165.0,
130,11qgxs8,machinelearning,ChatGPT,relevance,2023-03-13 18:11:39,[D] ChatGPT without text limits.,spiritus_dei,False,0.86,61,https://www.reddit.com/r/MachineLearning/comments/11qgxs8/d_chatgpt_without_text_limits/,27,1678731099.0,"One of the biggest limitations of large language models is the text limit. This limits their use cases and prohibits more ambitious prompts.

This was recently resolved by researchers at Google Brain in Alberta, Canada. In their recent paper they describe a new method of using associative memory which removes the text limit and they also prove that some large language models are universal Turing machines.

This will pave the way for entire novels being shared with large language models, personal genomes, etc.

The paper talks about the use of ""associative memory"" which is also known as content-addressable memory (CAM). This type of memory allows the system to retrieve data based on its content rather than its location. Unlike traditional memory systems that use specific memory addresses to access data, associative memory uses CAM to find data based on a pattern or keyword.

Presumably, this will open up a new market for associative memory since I would happily pay some extra money for content to be permanently stored in associative memory and to remove the text limit. This will also drive down the price of associative memory if millions of people are willing to pay a monthly fee for storage and the removal of prompt text limits.

The paper does point that there are still problems with conditional statements that confuse the large language models. However, I believe this can be resolved with semantic graphs. This would involve collecting data from various sources and using natural language processing techniques to extract entities and relationships from the text. Once the graph is constructed, it could be integrated into the language model in a variety of ways. One approach is to use the graph as an external memory, similar to the approach taken in the paper. The graph can be encoded as a set of key-value pairs and used to augment the model's attention mechanism during inference. The attention mechanism can then focus on relevant nodes in the graph when generating outputs.

Another potential approach is to incorporate the graph into the model's architecture itself. For example, the graph can be used to inform the initialization of the model's parameters or to guide the attention mechanism during training. This could help the model learn to reason about complex concepts and relationships more effectively, potentially leading to better performance on tasks that require this kind of reasoning.

The use of knowledge graphs can also help ground truth large language models and reduce hallucinations.

I'm curious to read your thoughts."
131,122q3h7,machinelearning,ChatGPT,relevance,2023-03-26 15:38:08,[P] Using ChatGPT plugins with LLaMA,balthierwings,False,0.96,130,https://blog.lastmileai.dev/using-openais-retrieval-plugin-with-llama-d2e0b6732f14,24,1679845088.0,
132,1320hyh,machinelearning,ChatGPT,relevance,2023-04-28 16:35:59,[P] Lamini rapidly achieves ChatGPT performance with an LLM Engine,gdiamos,False,0.47,0,https://www.reddit.com/r/MachineLearning/comments/1320hyh/p_lamini_rapidly_achieves_chatgpt_performance/,45,1682699759.0,"According to the authors, Lamini AI has invented an LLM Engine for rapidly customizing models.  

Read the blog post, github, and huggingface for details.  

* Blog [https://lamini.ai/blog/introducing-lamini](https://lamini.ai/blog/introducing-lamini) 
* Code 
   * Chat data ([https://github.com/lamini-ai/lamini/](https://github.com/lamini-ai/lamini/)) 
   * SQL data ([https://github.com/lamini-ai/lamini-sql/](https://github.com/lamini-ai/lamini-sql/))
* LLM Type System Playground: [https://app.lamini.ai](https://app.lamini.ai/)
* Open-source fine-tuned LLMs that follow instructions: 
   * [weights](https://huggingface.co/lamini/instruct-tuned-2.8b) 
   * [playground](https://huggingface.co/spaces/lamini/instruct-playground)"
133,10lp3g4,machinelearning,ChatGPT,relevance,2023-01-26 10:48:19,Few questions about scalability of chatGPT [D],besabestin,False,0.85,29,https://www.reddit.com/r/MachineLearning/comments/10lp3g4/few_questions_about_scalability_of_chatgpt_d/,36,1674730099.0,"I have two questions about chatGPT. I don't come from a machine learning background. I am just a programmer. So bear with me if they sound a bit dumb.

I was checking about chatGPT a bit the last week. I went through their papers and also tried out a fine tuning by myself by creating some fictional world and giving it some examples. 

The first thing I wondered is what is very special about the model than the large data and parameter set it has, that other competitors can't do. I ask this because I have seen a lot of ""google killer"" discussions in some places. From what I understood from their papers I thought it is something another company with the computing power and the filtered data can have up and running in few months. I see their advantage in rolling out to the public because with feedbacks from actual users all over the world it can potentially be retrained.

The second thing I wondered is its scalability. It feels to me that it is a very big challenge to keep it scalable in the future. Currently getting a long text out of it is kind of painful because it has to continuously generate. I think it is continuously calculating with the huge parameter set it has. I wonder also about new trends, if it needs to be retrained. I also used it for a fine tuning, where I created a fictional world with its own law and rules and the fine tuning took hours in the queue - so is it creating separate parameters for my case? that would be a lot considering how much parameter set they have."
134,11wt2fl,machinelearning,ChatGPT,relevance,2023-03-20 19:30:55,[P] OpenAssistant is now live on reddit (Open Source ChatGPT alternative),pixiegirl417,False,0.98,207,https://www.reddit.com/r/MachineLearning/comments/11wt2fl/p_openassistant_is_now_live_on_reddit_open_source/,29,1679340655.0,"OpenAssistant bot is live on /r/ask_open_assistant. There are some limitations to the reddit bot; you can also try on the model in chat mode at https://huggingface.co/spaces/olivierdehaene/chat-llm-streaming. Model is available for free download at https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b.


Prompt it by creating a new text post (responds to text body of post), starting a comment with !OpenAssistant, or by replying directly to it. 

I have recently enabled memory for the bot so it should do a (pretty mediocre) job of continuing a conversation with you."
135,1088rnw,machinelearning,ChatGPT,relevance,2023-01-10 12:35:07,[N] Microsoft Considers $10 Billion Investment in ChatGPT Creator --Bloomberg News,bikeskata,False,0.95,123,https://www.reddit.com/r/MachineLearning/comments/1088rnw/n_microsoft_considers_10_billion_investment_in/,42,1673354107.0,"Story here: https://www.bloomberg.com/news/articles/2023-01-10/microsoft-weighs-10-billion-chatgpt-investment-semafor-says?srnd=premium

Unpaywalled: https://archive.ph/XOOlg"
136,1135tir,machinelearning,ChatGPT,relevance,2023-02-15 19:07:24,"[D] GLM 130B (Chinese-English Bilingual model) translations vs Google, Deepl Translate, NLLB and chatGPT",MysteryInc152,False,0.96,221,https://www.reddit.com/gallery/1135tir,38,1676488044.0,
137,10htfwp,machinelearning,ChatGPT,relevance,2023-01-21 15:15:45,ChatGPT is not all you need [R],EduCGM,False,0.83,113,https://www.reddit.com/r/MachineLearning/comments/10htfwp/chatgpt_is_not_all_you_need_r/,13,1674314145.0,"Hi all,

We would like to share here our little concise review of generative AI large models just to show how current models are able to work with lots of formats like texts, videos, images, etc... 

[https://arxiv.org/abs/2301.04655](https://arxiv.org/abs/2301.04655)

&#x200B;

Enjoy!"
138,128ji6w,machinelearning,ChatGPT,relevance,2023-04-01 11:28:39,[P] ChatGPT Survey: Performance on NLP datasets,matus_pikuliak,False,0.85,30,https://www.reddit.com/r/MachineLearning/comments/128ji6w/p_chatgpt_survey_performance_on_nlp_datasets/,16,1680348519.0,"I've done a survey of how well ChatGPT performs on various NLP tasks as reported in arXiv papers. I have found 19 papers where they compared ChatGPT with fine-tuned models, but they are being published practically daily now. It seems that for the most of the classical NLP tasks, ChatGPT is not actually that strong and smaller fine-tuned models are  often much better. According to the API page, GPT-4 is not expected to  be much stronger on tasks like these. I think it is an interesting  perspective that shows that for many of the tasks we need to solve, GPT models are actually not the right tool.

There are of course many caveats in a comparison like this: People probably don't know how to utilize ChatGPT fully, but on the other hand the model can be contaminated by the testing data. As I see it, we are basically losing our ability to rigorously evaluate these close-sourced models, since we don't know what is in the training data and what they are doing with the prompts that are used every day.

The full survey can be found here: [http://opensamizdat.com/posts/chatgpt\_survey/](http://opensamizdat.com/posts/chatgpt_survey/)

Any feedback is welcomed."
139,11rthqf,machinelearning,ChatGPT,relevance,2023-03-15 11:22:32,[D] ChatGPT Plus waitlist,blabboy,False,0.44,0,https://www.reddit.com/r/MachineLearning/comments/11rthqf/d_chatgpt_plus_waitlist/,9,1678879352.0,"I was surprised to find that ChatGPT plus (currently the only way to test a vanilla GPT-4 model) is not only behind a pay wall, it is also behind a ""wait wall""!

Has anyone played with GPT-4 yet? Is it as good as the paper suggests? Anyone got any idea how long the wait list is for access?"
140,12327d1,machinelearning,ChatGPT,relevance,2023-03-26 22:33:33,[D] Build a ChatGPT from zero,manuelfraile,False,0.52,1,https://www.reddit.com/r/MachineLearning/comments/12327d1/d_build_a_chatgpt_from_zero/,13,1679870013.0,"I've recently discovered models such as ChatLLaMA that allows you to create a ""ChatGPT"" but you need Meta's LLaMA weights (yes, you can find them in torrents but that's not the point of the question). Similar limitations found in other cases.

Therefore I wanted to try to find an open source: dataset (in addition to hugging face), ""base model"", ""chat model""  AND that it is feasible to train with a commercial computer with a very good GPU (NVIDIA, etc.). With this get at least decent results.

Also would be interesting to distinguish between solutions with commercial limitations and those who don't.

Thanks!

• EDIT •
A first solution I already found is this: https://github.com/databrickslabs/dolly based on this https://huggingface.co/EleutherAI/gpt-j-6B, but looking for some discussion and perhaps other/better solutions."
141,11y70rx,machinelearning,ChatGPT,relevance,2023-03-22 04:34:44,[R] MM-ReAct: Prompting ChatGPT for Multimodal Reasoning and Action,MysteryInc152,False,0.93,87,https://www.reddit.com/r/MachineLearning/comments/11y70rx/r_mmreact_prompting_chatgpt_for_multimodal/,22,1679459684.0," Blog - [https://multimodal-react.github.io/](https://multimodal-react.github.io/)

Paper - [https://arxiv.org/abs/2303.11381](https://arxiv.org/abs/2303.11381)

Code - [https://github.com/microsoft/MM-REACT](https://github.com/microsoft/MM-REACT)

Demo - [https://huggingface.co/spaces/microsoft-cognitive-service/mm-react](https://huggingface.co/spaces/microsoft-cognitive-service/mm-react)

Wildest thing i've seen in a while. Still processing how a connection of foundation models can be this good."
142,10oyllu,machinelearning,ChatGPT,relevance,2023-01-30 10:21:13,[Discussion] ChatGPT and language understanding benchmarks,mettle,False,0.75,14,https://www.reddit.com/r/MachineLearning/comments/10oyllu/discussion_chatgpt_and_language_understanding/,15,1675074073.0,"The general consensus seems to be that large language models, and ChatGPT in particular, have a problem with accuracy and hallucination. As compared to what, is often unclear, but let's say as compared to other NLP methods of question answering, language understanding or as compared to Google Search.

I haven't really been able to find any reliable sources documenting this accuracy problem, though.

The SuperGLUE benchmark has GPT-3 ranked #24, not terrible, but outperformed by old models like T5, which seems odd. GLUE nothing. SQUAD nothing.

So, I'm curious:

1. Is there any benchmark or metric reflecting the seeming step-function made by ChatGPT that's got everyone so excited? I definitely feel like there's a difference between gpt-3 and chatGPT, but is it measurable or is it just vibes?
2. Is there any metric showing ChatGPT's problem with fact hallucination and accuracy?
3. Am I off the mark here looking at question-answering benchmarks as an assessment of LLMs?

Thanks"
143,12e2mtg,machinelearning,ChatGPT,relevance,2023-04-06 23:55:10,[D] Local chatGPT for python co-programming?,rorowhat,False,0.46,0,https://www.reddit.com/r/MachineLearning/comments/12e2mtg/d_local_chatgpt_for_python_coprogramming/,13,1680825310.0,"Hi there,

Sorry if this was already asked, but I was wondering is there is a language model just for python. The main attraction is that it would be much smaller in size, and easier to train. A few things that I was thinking that would be great to be trained on:

1. High quality answers from Stack overflow, something like >50 upvotes, top 3 answers per quality question.
2. Scrapping vetted python tutorial sites, the ones with good reputation.
3. ability to run locally.

It would be awesome if something like this existed, so you could bounce ideas and suggestion from it.

Is there something like this already?"
144,ztjw7j,machinelearning,ChatGPT,relevance,2022-12-23 15:45:45,[D] Has anyone integrated ChatGPT with scientific papers?,justrandomtourist,False,0.81,38,https://www.reddit.com/r/MachineLearning/comments/ztjw7j/d_has_anyone_integrated_chatgpt_with_scientific/,18,1671810345.0,"A guy on Twitter shared a ChatGPT that is aware of all the podcasts from Andrew Huberman, which is great (https://huberman.rile.yt/?query=)

Has anyone open sourced something like ChatGPT that it is easy to fine tune with external knowledge, potentially tested on scientific papers? It would be great for brainstorming, writing research proposal and exploring the literature in a different way. Maybe even integrating it with Zotero.

As of now I talked about finetuning the model, but let’s say I take the easier path of few shot learning instead. Is there a way to save the state of ChatGPT? In other words, if I open a new chat and feed it all the papers by copy and paste for example, is there a way I can use it next week? Sometimes I have found the session to expire, but recently it seems past chats are saved. Will this last indefinitely you believe?

TL;DR: best way to adapt ChatGPT to specific knowledge?"
145,11c1hzc,machinelearning,ChatGPT,relevance,2023-02-26 01:15:09,[P] [N] Democratizing the chatGPT technology through a Q&A game,coconautico,False,0.75,25,https://www.reddit.com/r/MachineLearning/comments/11c1hzc/p_n_democratizing_the_chatgpt_technology_through/,22,1677374109.0,"Hey Reddit,

tl;dr: To democratize the technology behind virtual assistants, we can play a [Q&A game](https://open-assistant.io/) to build a collaborative dataset that will enable the creation of culturally and politically unbiased virtual assistants.

As AI becomes more ubiquitous in our lives, we need to democratize it, ensuring that the next generation of virtual assistants, such as chatGPT or BingChat, are not solely controlled by one company, group or country, as it would allow them to skew our reality more easily, by deploying politically and culturally biased assistants at large scale, as we have seen with OpenAI.

While one could argue that over time companies and startups will emerge and create their own alternatives, these could be few, as creating such virtual assistants is not only a matter of massive raw data and computation, but it requires the creation of very specific datasets (many of them created by experts from multiple fields) with the goal of ""fine-tuning"" Large Language Models (LLMs) into virtual assistants.

Because of this, there is an international collaborative effort to create a public, multilingual, and high-quality dataset through a Q&A game, that will enable the creation of other virtual assistants outside the control of these companies.

At this very moment, we already have more data than OpenAI had when it launched its first version of ChatGPT. However, the current dataset is strongly biased towards Spanish and English speakers, as they are the only ones who have contributed to it so far. Therefore, we need to encourage people from other countries and cultures to play this Q&A game in order to create a truly multilingual dataset with expert knowledge of all kinds, from all over the world. (This would allow the virtual assistant to even answer questions that have not been answered in their language).

For Spanish and English is already a reality. Let's make a reality for other languages too by writing a few of questions/answers in the OpenAssistant game!

Link: [https://open-assistant.io/](https://open-assistant.io/)"
146,11xyk8c,machinelearning,ChatGPT,relevance,2023-03-21 23:17:40,SmartyGPT: now with ChatGPT and GPT4 [P],usc-ur,False,0.2,0,https://www.reddit.com/r/MachineLearning/comments/11xyk8c/smartygpt_now_with_chatgpt_and_gpt4_p/,1,1679440660.0,I want to announce that we have released v1.1.0 which includes access for ChatGPT and GPT4 for Plus suscribers! :)  [https://github.com/citiususc/Smarty-GPT](https://github.com/citiususc/Smarty-GPT)
147,11yof4h,machinelearning,ChatGPT,relevance,2023-03-22 16:34:03,[P] ChatLLaMA - A ChatGPT style chatbot for Facebook's LLaMA,imgonnarelph,False,0.87,30,https://www.reddit.com/r/MachineLearning/comments/11yof4h/p_chatllama_a_chatgpt_style_chatbot_for_facebooks/,11,1679502843.0,"👋  Hey all, we just launched [ChatLLaMA](https://chatllama.baseten.co/). An experimental chatbot interface for interacting with variants of Facebook's LLaMa. Currently, we support the 7 billion parameter variant that was fine-tuned on the Alpaca dataset. This early version isn't as conversational as we'd like, but over the next week or so, we're planning on adding support for the 30 billion parameter variant, another variant fine-tuned on LAION's OpenAssistant dataset and more as we explore what this model is capable of.

If you want deploy your own instance is the model powering the chatbot and build something similar we've open sourced the Truss here: [https://github.com/basetenlabs/alpaca-7b-truss](https://github.com/basetenlabs/alpaca-7b-truss)

We'd love to hear any feedback you have!

[Check it out here](https://chatllama.baseten.co/)"
148,12iulqu,machinelearning,ChatGPT,relevance,2023-04-11 19:26:07,[R] Going further under Grounded-Segment-Anything: integrating Whisper and ChatGPT,Technical-Vast1314,False,0.93,61,https://www.reddit.com/r/MachineLearning/comments/12iulqu/r_going_further_under_groundedsegmentanything/,9,1681241167.0,"https://preview.redd.it/1c0jnenb3bta1.png?width=1076&format=png&auto=webp&s=8884ed9984f34a97868aa1bac36ef0cc2f08f58a

Please check out **new Demo** about combining Whisper and ChatGPT, which aims to  **Automatically Detect , Segment and Generate Anything with Image, Text, and Speech Inputs , Imagine that you can det/seg/generate anything by speaking!**

&#x200B;

here's the github link: [https://github.com/IDEA-Research/Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)

&#x200B;

We implemented it in a very simple way, but **there is still unlimited space left for community users** to explore the capabilities of combining the expert models!"
149,121deu6,machinelearning,ChatGPT,relevance,2023-03-25 06:41:10,[D] ChatGpt plugins: are tech innovators feeding a beast that may ultimately devour them?,Grenouillet,False,0.73,17,https://www.reddit.com/r/MachineLearning/comments/121deu6/d_chatgpt_plugins_are_tech_innovators_feeding_a/,24,1679726470.0,"OpenAI has demonstrated that they may not prioritize ethical concerns. I'm genuinely curious about your opinion on this matter. Are tech companies trapped in a situation where they must engage in partnerships with OpenAI to stay competitive, while simultaneously generating an unprecedented amount of high-quality data? Could OpenAI then use this data to train their future models, rendering these very partnerships less relevant?"
150,11853g5,machinelearning,ChatGPT,relevance,2023-02-21 14:46:31,[R] ChatGPT for Robotics: Design Principles and Model Abilities,CheapBreakfast9,False,0.9,33,https://www.reddit.com/r/MachineLearning/comments/11853g5/r_chatgpt_for_robotics_design_principles_and/,8,1676990791.0,"I wanted to share a paper we have just released, where we extended the capabilities of ChatGPT to robotics, and controlled multiple platforms such as robot arms, drones, and home assistant robots intuitively with language: [https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/chatgpt-for-robotics/](https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/chatgpt-for-robotics/)

Video: [https://youtu.be/NYd0QcZcS6Q](https://youtu.be/NYd0QcZcS6Q)

Technical paper: [https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT\_\_\_Robotics.pdf](https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf)

https://i.redd.it/ya84nryu0kja1.gif"
151,13byr7o,machinelearning,ChatGPT,relevance,2023-05-08 17:30:49,[D] Technical Limitations to Running ChatGPT on Own Data,ConvexPreferences,False,0.25,0,https://www.reddit.com/r/MachineLearning/comments/13byr7o/d_technical_limitations_to_running_chatgpt_on_own/,5,1683567049.0,"I would get a ton of value out of being able to ask questions about a folder of PDFs using ChatGPT or a similar interface.

I've tried ChatPDF and another solution but it is extremely low quality in my experience.

Is the reason these solutions are terrible because the usage of embeddings is inherently lower quality because it has less context? Or is that wrong?

I'd love to try it with the 32k context window. But even that will be too small to fit both the data and my queries even if I sent in the prompts piecemeal.

Does anyone know if OpenAI is working on something (or if something is currently available that is similar quality) that has a massively higher context window? Are there big technical limitations to someone developing something with a massive context window? How much more would it cost per inference - does it scale linearly or exponentially as you increase the context window?

I'd ask ChatGPT these questions but it only runs through 2021! And Bard / Bing Chat are utterly useless.

I've seen something around Azure Opensearch linked to OpenAI APIs but it seems complicated to set up especially if I can't have ChatGPT walk me through it step by step. And I imagine that if it worked very well, there would already be companies productizing it that would be getting better results than ChatPDF.

Any ideas? How easy is this to do now without having to manually train an LLM? Any idea how soon we will have something plug and play and easy that isn't low quality like ChatPDF?"
152,122bju6,machinelearning,ChatGPT,relevance,2023-03-26 04:57:23,I made a chrome extension to make chatGPT bots from any web content in seconds [P],TernaryJimbo,False,0.85,83,https://v.redd.it/z00323t3j0qa1,18,1679806643.0,
153,1249bvt,machinelearning,ChatGPT,relevance,2023-03-28 02:13:23,[Discussion] Recommendations from ChatGPT vs traditional recommendation algorithms?,hunniddollab,False,0.25,0,https://www.reddit.com/r/MachineLearning/comments/1249bvt/discussion_recommendations_from_chatgpt_vs/,1,1679969603.0,"Curious if anyone has been able to compare recommendations from ChatGPT vs traditional recommendation algorithms (e.g. neural networks, collaborative filtering etc.)?

For example, using Chat GPT:

>can you recommend me restaurants off the beaten path in new york city

&#x200B;

>Sure, here are some restaurants off the beaten path in New York City that you might enjoy:  
>  
>Llama Inn: This Peruvian restaurant in Williamsburg serves up delicious ceviche, empanadas, and grilled meats in a stylish and trendy setting.  
>  
>Xi'an Famous Foods: This small chain of restaurants serves up hand-pulled noodles and other traditional dishes from the Xi'an region of China. There are several locations throughout the city.  
>  
>Claro: This Mexican restaurant in Gowanus offers delicious, authentic cuisine made with locally-sourced ingredients. The outdoor seating area is particularly charming.  
>  
>Cervo's: This seafood restaurant in the Lower East Side offers a Mediterranean-inspired menu featuring fresh fish and delicious cocktails.  
>  
>L'Artusi: This Italian restaurant in the West Village has a sleek and stylish interior and a menu featuring delicious pastas and other Italian specialties.  
>  
>Adda: This Indian restaurant in Long Island City offers a cozy and inviting atmosphere and a menu featuring flavorful curries, tandoori meats, and other Indian dishes.  
>  
>Dirt Candy: This vegetarian restaurant on the Lower East Side offers inventive and delicious plant-based dishes in a funky, eclectic setting.  
>  
>These are just a few options, but there are many more great restaurants to explore in New York City beyond the usual tourist hotspots

These recommendations seem pretty good but since I can't conduct a large scale AB test, it's hard to know how these recommendations would compare vs other methods."
154,1292wbk,machinelearning,ChatGPT,relevance,2023-04-01 23:37:07,[P]Notes analysis with ChatGPT and topic modeling,ThickDoctor007,False,1.0,6,https://niko-gamulin.medium.com/a-data-driven-exploration-of-my-reading-journey-8bdec7b2c6c4,0,1680392227.0,
155,11hwxx1,machinelearning,ChatGPT,relevance,2023-03-04 11:18:05,[D] Testing the New Bing vs ChatGPT,Fr33-Thinker,False,0.24,0,https://www.reddit.com/r/MachineLearning/comments/11hwxx1/d_testing_the_new_bing_vs_chatgpt/,0,1677928685.0,"Asked both engines the exact same task “**Write a fake news about the Ukraine war.**”

ChatGPT refused the task, whereas the New Bing creatively wrote a professional looking news article that fabricates the defeat of Ukraine. 😂😂

Has anyone else found interesting differences?"
156,11ynzc1,machinelearning,ChatGPT,relevance,2023-03-22 16:19:18,[R] Prompting ChatGPT for visual math and text reasoning,simpleuserhere,False,0.73,5,https://www.reddit.com/r/MachineLearning/comments/11ynzc1/r_prompting_chatgpt_for_visual_math_and_text/,1,1679501958.0,"&#x200B;

https://preview.redd.it/m7tdhkd2gbpa1.jpg?width=449&format=pjpg&auto=webp&s=36ae0dbae9b5a96ecc9b7239bd2b3e476d69d706"
157,zikps2,machinelearning,ChatGPT,relevance,2022-12-11 08:25:59,"[P] I made a tool that auto-saves your ChatGPT conversations and adds a ""Chat History"" button on the website.",silentx09,False,0.95,136,https://www.reddit.com/r/MachineLearning/comments/zikps2/p_i_made_a_tool_that_autosaves_your_chatgpt/,13,1670747159.0,"[savegpt.com](https://savegpt.com/) is a browser extension available both on the Chrome webstore and Firefox addons.

https://reddit.com/link/zikps2/video/5zinkph4b85a1/player"
158,znk7bz,machinelearning,ChatGPT,relevance,2022-12-16 17:33:07,[D] What kind of effects ChatGPT or future developments may have on job market?,ureepamuree,False,0.72,13,https://www.reddit.com/r/MachineLearning/comments/znk7bz/d_what_kind_of_effects_chatgpt_or_future/,20,1671211987.0,"I am actively using ChatGPT nowadays to seek assistance in various tasks such as fixing grammatical errors in manuscripts, to provide simplified/coherent explanations on technical jargon etc. This is giving me an impression that future jobs related to ""writing"" such as proofreaders might run out of business."
159,zxef0f,machinelearning,ChatGPT,relevance,2022-12-28 16:49:26,[Project] I ask ChatGPT to draw and explain 100+ programmatic SVG images,evanthebouncy,False,0.85,36,https://www.reddit.com/r/MachineLearning/comments/zxef0f/project_i_ask_chatgpt_to_draw_and_explain_100/,10,1672246166.0,"Foundational models can generate realistic images from prompts, but do these models *understand* their own drawings? Generating SVG (Scalable Vector Graphics) gives us a unique opportunity to ask this question. SVG is programmatic, consisting of circles, rectangles, and lines. Therefore, the model must schematically decompose the target object into meaningful parts, approximating each part using simple shapes, then arrange the parts together in a meaningful way.  


Check out the blog (5min read) for the full report [https://medium.com/p/74ec9ca106b4](https://medium.com/p/74ec9ca106b4) 

tl;dr:  
GPT can symbolically decompose an object into parts, is okay at approximating the parts using SVG, is bad at putting the parts together, and is Egyptian.

be happy to take some comments and QA here :D

\--evan"
160,11vl691,machinelearning,ChatGPT,relevance,2023-03-19 13:16:59,[R] Quantitative comparison of ChatGPT and GPT-4 performance on multiple open source datasets,N00B1ST,False,0.92,9,https://www.reddit.com/r/MachineLearning/comments/11vl691/r_quantitative_comparison_of_chatgpt_and_gpt4/,0,1679231819.0,"Preliminary results give credence to some of the claims made by OpenAI regarding performance gains achieved by GPT-4 across domains. Unanswered questions remain regarding training data used and possible leakage. Tools used were Langchain and the current API endpoints (chatgpt-3.5-turbo and gpt-4).

https://twitter.com/K_Hebenstreit/status/1636789765189308416"
161,10nfquy,machinelearning,ChatGPT,relevance,2023-01-28 14:00:18,[P] Launching my first ever open-source project and it might make your ChatGPT answers better,Vegetable-Skill-9700,False,0.88,59,https://www.reddit.com/r/MachineLearning/comments/10nfquy/p_launching_my_first_ever_opensource_project_and/,16,1674914418.0,"I am building an open-source ML observability and refinement toolkit. 

The tool helps ML practitioners to:
1. Understand how their models are performing in production
2. Catch edge-cases and outliers to help them refine their models
3. Allow them to customise the tool according to their needs (hence, open-source)
4. Bring data-security at the forefront (hence, self hosted)

You can check out the project https://github.com/uptrain-ai/uptrain and would love to hear feedback from the community"
162,12yr1eq,machinelearning,ChatGPT,relevance,2023-04-25 18:05:32,"[P] HuggingChat (open source ChatGPT, interface + model)",lorepieri,False,0.94,239,https://www.reddit.com/r/MachineLearning/comments/12yr1eq/p_huggingchat_open_source_chatgpt_interface_model/,58,1682445932.0,[https://huggingface.co/chat/](https://huggingface.co/chat/)
163,128bmv4,machinelearning,ChatGPT,relevance,2023-04-01 04:53:26,[R] A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?,Learningforeverrrrr,False,0.61,11,https://www.reddit.com/r/MachineLearning/comments/128bmv4/r_a_complete_survey_on_generative_ai_aigc_is/,0,1680324806.0,"We recently completed two surveys: one on generative AI and the other on ChatGPT. Generative AI and ChatGPT are two fast-evolving research fields, and we will update the content soon, for which your feedback is appreciated (you can reach out to us through emails on the paper).

The title of this post refers to the first one, however, we put both links below.

**Link to a survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

**Link to a survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)

The following is the **abstract** of the **survey on generative AI** with a summary **figure**.

As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible to miss the opportunity to glimpse AIGC from a certain angle.  In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? To answer this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its **techniques** to **applications**. Modern generative AI relies on various technical foundations, ranging from **model architecture** and **self-supervised pretraining** to **generative modeling** methods (like GAN and diffusion models). After introducing the fundamental techniques, this work focuses on the technological development of various AIGC tasks based on their output type, including **text**, **images**, **videos**, **3D content**, **etc**., which depicts the full potential of ChatGPT's future. Moreover, we summarize their significant applications in some mainstream **industries**, such as **education** and **creativity** content. Finally, we discuss the **challenges** currently faced and present an **outlook** on how generative AI might evolve in the near future.

&#x200B;

https://preview.redd.it/pild5vcre7ra1.png?width=1356&format=png&auto=webp&s=58c101ee2fa8fec75032b733e3f03d9bc4f41756

**Link to a survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

**Link to a survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)"
164,12ojpxj,machinelearning,ChatGPT,relevance,2023-04-16 18:20:39,[P] CHARLIE - Voice/text chat with a roleplaying ChatGPT with a voiced live2d avatar and more,IamNobodyAmaA,False,0.81,16,https://www.reddit.com/r/MachineLearning/comments/12ojpxj/p_charlie_voicetext_chat_with_a_roleplaying/,2,1681669239.0,"Hey everyone,

similar to the [earlier post](https://old.reddit.com/r/MachineLearning/comments/12n3c4i/ai_ui_user_interface_for_interacting_with_ai/) that showcased an app where you can chat with an AI, I was working on my own project called CHARLIE.

#[CHARLIE - GitHub repo](https://github.com/TobiasM95/CHARLIE)

The GitHub has a lot of information including a description as well as detailed instructions on how to run it yourself. Here are the most important details:

- CHARLIE is my attempt at connecting multiple AI APIs in a somewhat modular or customizable way to enable straightforward communication with any AI (currently has a ChatGPT API connection)
- You can use a microphone or regular textbox input to talk to Charlie, who is acting as a character defined with a style (i.e. personality) string. Charlie will respond with text and voice as well as a live2d model integration that is lip-synced.
- The frontend is a React Application connected to a Flask backend via a REST API as well as websockets.
- The code is written in a way that it should be straightforward to replace APIs with different ones or locally run models.
- It's MIT licensed and free for everyone to tinker with or improve.
- The APIs that are currently in use are: OpenAI's Whisper, DeepL, Google Cloud TTS, elevenlabs.ai TTS, OpenAI's ChatGPT
- I am currently running it as a private website since this is using single API keys in the backend. This is mainly to show that this can be published/distributed near its current form.
- A detailed step-by-step list of how to get it to run is included. It runs on Windows and Linux and needs Python and Node.js
- When using OpenAI's API and Google Cloud TTS you can easily run this thing for free if you have some free OpenAI credits lying around or for $0.1 to $1-$10 dollars a month depending on if you use it a few times a day or 24/7 for a whole month.

Here are two screenshots of the interface: [Screenshot 1](https://github.com/TobiasM95/CHARLIE/raw/master/preview.png) and [screenshot 2](https://github.com/TobiasM95/CHARLIE/raw/master/settingsReview.png)

It also has the ability to act as a voiced translator and a raw ChatGPT interface but that's not yet properly implemented and documented for the frontend (although you could probably access it by using the ""charliesettings"" prompt).

There are still lots of small things to improve but in general, it's well-usable now and you can extend it with functionality if you want. For example, instead of the ChatGPT API use a locally run model that's not as locked down as ChatGPT to make it easier for Charlie to act human-like (especially with the release of Open-Assistant recently or the plethora of models that got released recently that you can run locally). More polished web interface that works better on mobile. Better text-to-speech and/or cheaper (Google Cloud TTS vs elevenlabs.ai). But since it's modular and open-source there's always the possibility of picking and choosing the parts you want/need and customizing the rest.

Hope you like it and if there are questions/issues please let me know."
165,129qi8p,machinelearning,ChatGPT,relevance,2023-04-02 16:39:23,[R] HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace - Yongliang Shen et al Microsoft Research Asia 2023 - Able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results!,Singularian2501,False,0.94,172,https://www.reddit.com/r/MachineLearning/comments/129qi8p/r_hugginggpt_solving_ai_tasks_with_chatgpt_and/,30,1680453563.0,"Paper: [https://arxiv.org/abs/2303.17580](https://arxiv.org/abs/2303.17580) 

Abstract:

>Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in HuggingFace, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which **paves a new way towards AGI.** 

https://preview.redd.it/huc5so9f1ira1.jpg?width=1201&format=pjpg&auto=webp&s=cd714263f8a6ea443195316d95704fd550beee95

https://preview.redd.it/d2dfhs9f1ira1.jpg?width=655&format=pjpg&auto=webp&s=07fcb2b969cdaaf649aed259296f3dfa9157531e

https://preview.redd.it/v4gc9r9f1ira1.jpg?width=773&format=pjpg&auto=webp&s=b014fa679a7bdc2024a3d27690950be2248735aa"
166,12d09f5,machinelearning,ChatGPT,relevance,2023-04-05 22:31:10,"[P] OSS ChatGPT trust, safety, & enablement platform to secure the organization and the individual!",Just_Paramedic_5198,False,0.73,11,https://www.reddit.com/r/MachineLearning/comments/12d09f5/p_oss_chatgpt_trust_safety_enablement_platform_to/,2,1680733870.0,"[https://github.com/circulatedev/last-stop](https://github.com/circulatedev/last-stop)

Hi everyone,

My friend and I are building a platform that allows you to host a ChatGPT website within your own network - whether it's in an organization or at home!

The benefits include:

* Monitoring for DLP scenarios / employee needs
* Getting back control of current AI platforms
* Preventing users from bringing their own accounts
* Deploying within your network

&#x200B;

Future Plans:

* Easier deployment process using Beanstalk / K8s
* Integrate with existing DLP solutions / advanced DLP capabilities
* Enable prompt sanitization
* Enable SIEM features
* Build internal corpus for prompts / responses

Come check it out and please give us any feedback! We are working with some decision makers in the community and would love to share this with the broader audience.

Cheers,

kai-ten"
167,126oiey,machinelearning,ChatGPT,relevance,2023-03-30 14:18:50,[D] AI Policy Group CAIDP Asks FTC To Stop OpenAI From Launching New GPT Models,vadhavaniyafaijan,False,0.84,207,https://www.reddit.com/r/MachineLearning/comments/126oiey/d_ai_policy_group_caidp_asks_ftc_to_stop_openai/,213,1680185930.0,"The Center for AI and Digital Policy (CAIDP), a tech ethics group, has asked the Federal Trade Commission to investigate OpenAI for violating consumer protection rules. CAIDP claims that OpenAI's AI text generation tools have been ""biased, deceptive, and a risk to public safety.""

CAIDP's complaint raises concerns about potential threats from OpenAI's GPT-4 generative text model, which was announced in mid-March. It warns of the potential for GPT-4 to produce malicious code and highly tailored propaganda and the risk that biased training data could result in baked-in stereotypes or unfair race and gender preferences in hiring. 

The complaint also mentions significant privacy failures with OpenAI's product interface, such as a recent bug that exposed OpenAI ChatGPT histories and possibly payment details of ChatGPT plus subscribers.

CAIDP seeks to hold OpenAI accountable for violating Section 5 of the FTC Act, which prohibits unfair and deceptive trade practices. The complaint claims that OpenAI knowingly released GPT-4 to the public for commercial use despite the risks, including potential bias and harmful behavior. 

[Source](https://www.theinsaneapp.com/2023/03/stop-openai-from-launching-gpt-5.html) | [Case](https://www.caidp.org/cases/openai/)| [PDF](https://www.caidp.org/app/download/8450269463/CAIDP-FTC-Complaint-OpenAI-GPT-033023.pdf)"
168,11mlwty,machinelearning,ChatGPT,relevance,2023-03-09 07:24:35,"[R] Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",MysteryInc152,False,0.97,877,https://www.reddit.com/gallery/11mlwty,26,1678346675.0,
169,zwi4jx,machinelearning,ChatGPT,relevance,2022-12-27 15:25:39,[Discussion] 2 discrimination mechanisms that should be provided with powerful generative models e.g. ChatGPT or DALL-E,Exnur0,False,0.61,6,https://www.reddit.com/r/MachineLearning/comments/zwi4jx/discussion_2_discrimination_mechanisms_that/,15,1672154739.0,"In the wake of all the questions and worries about models that can generate content nearing (or exceeding, in some cases) the quality of that made of humans, there are a couple mechanisms that companies should provide alongside their models. Both vary in feasibility, but in general, both are pretty doable, at least for what we've seen so far.

1. A hashing-based system to check whether a given piece of content was generated by the model. This can be accomplished by hashing all of the outputs of the model, and storing them. If it doesn't pose some sort of security risk for the generator, it could also provide the date of generation.

2. A model for discriminating whether a given piece of content was generated by the model, similar to [this model for GPT-2](https://huggingface.co/roberta-base-openai-detector). This is necessary in addition to the simpler hashing mechanism, since it's possible for only a portion of the media to be generated. This would be imperfect, of course, but if nothing else, we should press companies enough that they feel obligated to give it a dedicated try.

These mechanisms need real support - an API for developers, and a UI for less sophisticated users. They should have decent latency, and be hopefully be provided for free, at some level of usage - I understand the compute required could be enormous.

Curious what others think here :)"
170,zctiu3,machinelearning,ChatGPT,relevance,2022-12-05 02:03:58,[D] Thread: Top 10 ways you can use ChatGPT for Music related stuff,dicklesworth,False,0.78,12,https://www.reddit.com/r/MachineLearning/comments/zctiu3/d_thread_top_10_ways_you_can_use_chatgpt_for/,3,1670205838.0," I realize it's limited now, but I think with more refinement (and especially when gpt4 comes out), this approach will prove very useful: [https://twitter.com/doodlestein/status/1599551670140051458](https://twitter.com/doodlestein/status/1599551670140051458)"
