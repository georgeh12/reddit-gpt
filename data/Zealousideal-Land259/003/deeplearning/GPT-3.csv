,id,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,url,num_comments,created,body
0,za73dc,deeplearning,GPT-3,top,2022-12-02 01:35:02,GPT-3 Generated Rap Battle between Yann LeCun & Gary Marcus,hayAbhay,False,0.99,141,https://i.redd.it/ybfcfvez1e3a1.png,16,1669944902.0,
1,10mhyek,deeplearning,GPT-3,top,2023-01-27 10:45:48,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.95,119,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
2,10fw22o,deeplearning,GPT-3,top,2023-01-19 07:55:49,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.89,69,https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/,11,1674114949.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=webp&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
3,121agx4,deeplearning,GPT-3,top,2023-03-25 04:24:49,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.91,47,https://www.reddit.com/r/deeplearning/comments/121agx4/do_we_really_need_100b_parameters_in_a_large/,54,1679718289.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
4,zaxg5m,deeplearning,GPT-3,top,2022-12-02 20:59:47,Everyone: AI will make it easy to spread misinformation; Me: Stop hitting yourself GPT3!,hayAbhay,False,0.9,39,https://i.redd.it/mrf9rz0ltj3a1.png,0,1670014787.0,
5,ylj1ux,deeplearning,GPT-3,top,2022-11-03 23:55:15,BlogNLP: AI Writing Tool,britdev,False,1.0,37,https://www.reddit.com/r/deeplearning/comments/ylj1ux/blognlp_ai_writing_tool/,9,1667519715.0,"Hey everyone,

I created this web app using Open AI's GPT-3 (Davinci model). The purpose here is to provide a free tool to allow people to generate blog content/outlines/headlines and help with writer's block. Will continue to improve it over time, but just a side project I figured would provide some value to you all. Hope you all enjoy and please share ❤️

[https://www.blognlp.com/](https://www.blognlp.com/)"
6,11ium8l,deeplearning,GPT-3,top,2023-03-05 11:10:56,LLaMA model parallelization and server configuration,ChristmasInOct,False,1.0,25,https://www.reddit.com/r/deeplearning/comments/11ium8l/llama_model_parallelization_and_server/,8,1678014656.0,"Hey everyone,

First of all, tldr at bottom, typed more than expected here.  

Please excuse the rather naive perspective I have here.  I've followed along with great interest, but this is not my industry.

Regardless, I have spent the past 3-4 days falling down a brutally obsessive rabbit hole, and I cannot seem to find this information.  I'm assuming it's just that I am missing context of course, and regardless of whether there is a clear answer, I'm trying to get a better understanding of this topic so that I could better appraise the situation myself.

Really I suppose I have two questions.  **The first** is regarding model parallelization.

I'm assuming this is not generic whatsoever.  What is the typical process engineers go about for designing such a pipeline?  Specifically in regards to these new LLaMA models, is something like ALPA relevant?  Deepspeed?

More importantly, what information should I be seeking to determine this myself?

This roughly segues to my **second inquiry**.

The reason I'm curious about splitting the model pipeline etc., is that I am potentially in interested in standing a server up for this.  Although I don't have much of a budget for this build (\~$30-40K is the rough top-end, but I'd be a lot happier around $20-25K), the money is there if I can genuinely satisfy my use-case.

I work at a small, but borderline manic startup working on enterprise software; 90% of the work we're doing based in the react/node ecosystem, some low-level work for backend services, and some very interesting database work that I have very little to do with.  I am a fullstack engineer that grew up playing with C++ => C#, and somehow ended up spending all of my time r/w'ing javascript.  Lol.  Anyways.

Part of our roadmap since GPT-3 and the playground were made publicly accessible, involves usage of these transformer models, and their ability to interpret natural language inputs, whether from user inputs, or scraped input values generated somewhere in a chain of requests / operations.

Seeing GPT-3 in action made me specifically realize that my estimations on this technology had been wildly off.  Seeing ChatGPT in action and uptick, the API's becoming available, has me further panicked.

Running our inference through their API has never really been an option for us.  I haven't even really looked that far into it, but bottom line the data running through our platform is all back-office, highly sensitive business information, and many have agreements explicitly restricting the movement of data to or from any cloud services, with Microsoft, Amazon, and Google all specifically mentioned.

Regardless of the reasoning for these contracts, the LLaMA release has had me obsessed over this topic in more detail than before, and whether or not I would be able to get this setup privately, for our use-case.

**To get to the actual second inquiry**:

Say I want to throw a budget rig together for this in a server cabinet.  Am I able to effectively parallelize the LLaMA model, well enough to justify going with 24GB VRAM 4090's in the rig?  Say I do so with DeepSpeed, or some of the standard model parallelization libraries.

Is the performance cost low enough to justify taking the extra compute here over 1/3 - 1/2 as many RTX6000 ADA's?

Or should I be grabbing the 48GB ADA's?

Like I said, I apologize for the naivety, I'm really looking for more information so that I can start to put this picture together better on my own.  It really isn't the easiest topic to research with how quickly things seem to move, and the giant gap between conversation depths (gamer || phd in a lot of the most interesting or niche discussions, little between).

Thank you very much for your time.

TL;DR - Any information on LLaMA model parallelization at the moment?  Will it be compatible with things like zero or alpa?  How about for throwing a rig together right now for fine-tuning and then running inference on the LLaMA models?  48GB 6000 ADA's, or 24GB 4090's?

Planning on putting it in a mostly empty 42U cabinet that also houses our primary web server and networking hardware, so if there is a sales pitch for 4090's across multiple nodes here, I do have a massive bias as the kind of nerd that finds that kind of hardware borderline erotic.

Hydro and cooling are not an issue, just usage of the budget and understanding the requirements / approach given memory limitations, and how to avoid communication bottlenecks or even balance them against raw compute.

Thanks again everyone!"
7,zb2kkc,deeplearning,GPT-3,top,2022-12-03 00:17:31,A GPT-3 based Chrome Extension that debugs your code!,VideoTo,False,0.97,22,https://www.reddit.com/r/deeplearning/comments/zb2kkc/a_gpt3_based_chrome_extension_that_debugs_your/,0,1670026651.0,"Link - [https://chrome.google.com/webstore/detail/clerkie-ai/oenpmifpfnikheaolfpabffojfjakfnn](https://chrome.google.com/webstore/detail/clerkie-ai/oenpmifpfnikheaolfpabffojfjakfnn)

Built  a quick tool I thought would be interesting - it’s a chrome extension  that uses GPT-3 under the hood to help debug your programming errors  when you paste them into Google (“eg. TypeError:…”).

This is definitely early days, so **if   this is something you would find valuable and wouldn't mind testing a   couple iterations of, please feel free to join the discord** \-> [https://discord.gg/KvG3azf39U](https://discord.gg/KvG3azf39U)

&#x200B;

https://i.redd.it/tt6hcqn2tk3a1.gif"
8,10irh5u,deeplearning,GPT-3,top,2023-01-22 19:11:36,Apple M2 Max 96 GB unified memory for larger models vs multiple 24GB GPUs or 40GB A100s?,lol-its-funny,False,0.93,20,https://www.reddit.com/r/deeplearning/comments/10irh5u/apple_m2_max_96_gb_unified_memory_for_larger/,14,1674414696.0,"How feasible is it to use an Apple Silicon M2 Max, which has about [96 GB unified memory](https://www.apple.com/shop/buy-mac/macbook-pro/16-inch-space-gray-apple-m2-max-with-12-core-cpu-and-38-core-gpu-1tb) for ""large model"" deep learning? I'm inspired by the the [Chinchilla](https://arxiv.org/abs/2203.15556) paper that shows a lot of promise at 70B parameters. Outperforming ultra large models like Gopher (280B) or GPT-3 (175B) there is hope for working with < 70B parameters without needing a super computer. At least for fine tuning. I've been working with GPT-J but want to scale/tinker with larger open-sourced models.

However, I don't know how clearly the CompSci theory (M2 Max's 38-core GPU, 16 core Neural Engine accessing 96 GB unified memory) maps out to the IT reality (toolkits and libraries on macOS actually using it). My exposure is mostly around Jupyter books on Colab Pro+ (A100s) and nvidia 3080 GPUs (locally). 

I appreciate your guidance."
9,10n8c80,deeplearning,GPT-3,top,2023-01-28 06:34:28,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,0.96,17,https://www.reddit.com/r/deeplearning/comments/10n8c80/a_python_module_to_generate_optimized_prompts/,0,1674887668.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features 🚀

* 🧙‍♀️ NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* 🔨 Easily add one-shot, two-shot, or few-shot examples to the prompt
* ✌ Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* 💥 Custom examples and samples can be easily added to the prompt
* 💰 Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Example](https://preview.redd.it/sjvhtd8b3nea1.png?width=1236&format=png&auto=webp&s=e9a3a28f41f59cd25fe8e95bd1fca56b15f27a6e)

&#x200B;

https://preview.redd.it/fnb05bys3nea1.png?width=1398&format=png&auto=webp&s=096f4e2cbd0a71e795f30cc5e3720316b5e5caf6"
10,11q8tir,deeplearning,GPT-3,top,2023-03-13 12:50:10,Learning logical relationships with neural networks with differential ILP,Neurosymbolic,False,1.0,14,https://www.reddit.com/r/deeplearning/comments/11q8tir/learning_logical_relationships_with_neural/,3,1678711810.0,"Since last week’s post on my lab’s software package [PyReason](https://neurosymoblic.asu.edu/pyreason/), I got a lot of questions on if it would be possible for a neural network to learn logical relationships from data. After all, ChatGPT seems to be able to generate Python code, and Meta released a NeurIPS paper to show you can learn math equations from data using a transformer-based model, so why not logic? In this article, we will one line of research in this area – differential ILP.

The research in this area really kicked off with a 2018 [paper from DeepMind](https://www.reddit.com/r/deeplearning/jair.org/index.php/jair/article/view/11172) where Richard Evans and Edward Grefenstette showed that you could adapt techniques from “inductive logic programming” to use gradient descent, and learn logical rules from data. Previous (non-neural) work on inductive logic programming was generally not designed to work with noisy data and instead fit the historical examples in a precise manner. Evans and Grefenstette utilized a neural architecture and a loss function – and they showed they could handle noisy data and even do some level of integration with CNN’s. Their neural architecture mimicked a set of candidate logical rules – and the rules assigned higher weights by gradient descent would be thought to best fit the data. However, a downside to this approach is that the neural network was [quintic in the size of the input](https://www.youtube.com/watch?v=SOnAE0EyX8c&list=PLpqh-PUKX-i7URwnkTqpAkSchJHvbxZHB&index=5). This is why they only applied their approach on very small problems – it did not see very wide adoption.

That said, in the last two years, there have been some notable follow-ons to this work. Researchers out of Kyoto University and NTT introduced a manner to learn rules that are more expressive in a different manner by allowing function symbols in the logical language ([Shindo et al., AAAI 2021](https://ojs.aaai.org/index.php/AAAI/article/view/16637/16444)). They leverage a clause search and refinement process to limit the number of candidate rules – hence limiting the size of the neural network. A student team from ASU created a presentation on their work for our recent seminar course on neuro symbolic AI. We released a three part video series from their talk:

[Part 1: Review of differentiable inductive logic programming](https://www.youtube.com/watch?v=JIS78a40q8U&t=270s)

[Part 2: Clause search and refinement In our recent video series](https://www.youtube.com/watch?v=nzfbxlHUwuE&t=345s)

[Part 3: Experiments](https://www.youtube.com/watch?v=-fKWNtHUIN0&t=27s)

[Slides](https://labs.engineering.asu.edu/labv2/wp-content/uploads/sites/82/2022/10/Shindo_dILP.pdf)

Some think that the ability to learn such relationships will represent a significant advancement in ML, specifically addressing shortcomings in areas such as knowledge graph completion and reasoning about scene graphs. However, the gap still remains wide, and ILP techniques, including differentiable ILP still have a ways to go. Really interested in what your thoughts are, feel free to comment below."
11,zth8rl,deeplearning,GPT-3,top,2022-12-23 14:35:17,How to change career trajectory to NLP engineer,Creative-Milk-8266,False,0.88,13,https://www.reddit.com/r/deeplearning/comments/zth8rl/how_to_change_career_trajectory_to_nlp_engineer/,3,1671806117.0," A little of my background - 5 years experience in data science. Mostly related to prototyping statistical models and optimization problems, bringing them into production. Some experience in building pipeline and orchestration flow with AWS services.

I have basic understanding on Transformers, BERT, GPT. Did my first NLP Kaggle competition the first time recently.

I'd like my next job to be a NLP engineer. How should I prepare myself for it?

Here's some of the items I'm thinking

&#x200B;

1. More hands on projects I can put on resume, including integration with cloud services. Any recommendations on what kinds of projects I should pick?  
 
2. Tryout techniques of speeding up models like distilled model, dynamic shape, quantization. Anything else that would be helpful?  
 
3. Understand lower level of GPU programming knowledges. Not sure if this is helpful for me finding a NLP job. If so, what kind of things I can do to go deeper on this subject. I'm currently taking [Intro to Parallel Programming](https://classroom.udacity.com/courses/cs344) CS344 course on Udemy (highly recommend btw).  
 
4. Grind leetcode :/  
 

Please point out other important directions I missed."
12,zboc8w,deeplearning,GPT-3,top,2022-12-03 19:29:01,BlogNLP: AI Blog Writing Tool,britdev,False,0.88,12,https://www.reddit.com/r/deeplearning/comments/zboc8w/blognlp_ai_blog_writing_tool/,7,1670095741.0,"Hey everyone,

I developed this web app with Open AI's GPT-3 to provide a free, helpful resource for generating blog content, outlines, and more - so you can beat writer's block! I'm sure you'll find it useful and I'd really appreciate it if you shared it with others ❤️.

[https://www.blognlp.com/](https://www.blognlp.com/)"
13,129k24i,deeplearning,GPT-3,top,2023-04-02 12:37:38,[N] Software 3.0 Blog Post Release 🔥,DragonLord9,False,0.77,12,https://www.reddit.com/r/deeplearning/comments/129k24i/n_software_30_blog_post_release/,3,1680439058.0,"Hi all, excited to share my blog post on [**Software 3.0**](https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm_campaign=post&utm_medium=web)

https://preview.redd.it/9b4hjkkhugra1.png?width=1500&format=png&auto=webp&s=e341f3ab4c3c8abb206df8daa17428a297ff61e2

The blog post offers an insightful read on the new GPT-powered programming paradigm where the new programming language is simply ""*English*"", as well as recent developments in AI.

The post was originally written before GPT-4 release, and the predictions seem to have held surprisingly well. Knowledge cutoff date 28 Feb 2023.

Please read and share!! Happy to answer any follow-ups here or on DM 😊

Tweet: [https://twitter.com/DivGarg9/status/1642229948185280521?s=20](https://twitter.com/DivGarg9/status/1642229948185280521?s=20)

Blog: [https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm\_campaign=post&utm\_medium=web](https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm_campaign=post&utm_medium=web)"
14,10bq685,deeplearning,GPT-3,top,2023-01-14 14:48:43,Scaling Language Models Shines Light On The Future Of AI ⭕,LesleyFair,False,0.75,8,https://www.reddit.com/r/deeplearning/comments/10bq685/scaling_language_models_shines_light_on_the/,1,1673707723.0,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs might allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* reduce hallucination and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example is words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (a proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/jlh726eku0ca1.png?width=800&format=png&auto=webp&s=55d170251a967f31b36f01864af6bb7e2dbda253)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week.No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)"
15,yzwzp0,deeplearning,GPT-3,top,2022-11-20 06:20:53,How do various content-generating services work?,th3luck,False,0.71,4,https://www.reddit.com/r/deeplearning/comments/yzwzp0/how_do_various_contentgenerating_services_work/,1,1668925253.0,"Right now sites like [https://www.jasper.ai/](https://www.jasper.ai/) offer text generation for emails, ads, social media posts and etc. I wonder, do they simply tune a separate gpt-3-like model for each of these tasks? Or there is a new approach to solving this?"
16,10igecg,deeplearning,GPT-3,top,2023-01-22 10:12:08,"BigScience BLOOM, how should we use it?",Haghiri75,False,1.0,5,https://www.reddit.com/r/deeplearning/comments/10igecg/bigscience_bloom_how_should_we_use_it/,1,1674382328.0,"Since the release of BLOOM, I always wanted to test it the way GPT-3 (and newly released ChatGPT) are tested. Having a playground with the ability to explore settings and even generating codes and stuff. But I don't know how long was it (I guess almost a year) and the only thing *close to playground* it had was the huggingface model card.

So is there any reliable way to use BLOOM in a proper way?"
17,1180x0e,deeplearning,GPT-3,top,2023-02-21 11:06:33,I created a Search Engine For Books using GPT-3 🔎📘. Here's how you can create it too:,Pritish-Mishra,False,0.67,5,https://youtu.be/SXFP4nHAWN8,8,1676977593.0,
18,zk2ser,deeplearning,GPT-3,top,2022-12-12 15:53:41,"GPT-Rex: A chrome extension to plug GPT-3 directly into Medium. Hit ""Ctrl + >"" to trigger auto-complete while writing. Available on Chrome web store. Support for other platforms coming soon.",hayAbhay,False,0.67,2,https://github.com/hayabhay/gpt-go,2,1670860421.0,
19,128tfvc,deeplearning,GPT-3,top,2023-04-01 17:50:06,Fine-tune GPT on sketch data (stroke-3),mellamo_maria,False,1.0,1,https://www.reddit.com/r/deeplearning/comments/128tfvc/finetune_gpt_on_sketch_data_stroke3/,0,1680371406.0," These past days I have started a personal project where I would like to build a model that, given an uncompleted sketch, it can finish it. I was planning on using some pretrained models that are available in HuggingFace and fine-tune them with my sketch data for my task. The sketch data I have is in stoke-3 format, like the following example:  
\[  
\[10, 20, 1\],  
\[20, 30, 1\],  
\[30, 40, 1\],  
\[40, 50, 0\],  
\[50, 60, 1\],  
\[60, 70, 0\]  
\]  
The first value of each triple is the X-coordinate, the second value the Y-coordinate and the last value is a binary value indicating whether the pen is down (1) or up (0). I was wondering if you guys could give me some instruction/tips about how should I approach this problem? How should I prepare/preprocess the data so I can fit it into the pre-trained models like BERT, GPT, etc. Since it's stroke-3 data and not text or a sequence of numbers, I don't really know how should I treat/process the data.

Thanks a lot! :)"
20,zsics7,deeplearning,GPT-3,top,2022-12-22 09:57:14,"Show ChatGPT's response next to the search results from Google, Bing, and DuckDuckGo.",Harrypham22,False,1.0,1,https://www.reddit.com/r/deeplearning/comments/zsics7/show_chatgpts_response_next_to_the_search_results/,0,1671703034.0," **Do you know about ChatGPT?** 

It is a variant of the GPT-3 language model developed by OpenAI specifically designed for generating responses to user input in chat or messaging applications. It is trained on a large dataset of conversation data and is able to understand the context of a conversation and generate appropriate responses. ChatGPT is particularly well-suited for use in chat or messaging applications, but it can also be used for a wide range of other natural language processing tasks, such as language translation, summarization, and question answering.

**Display ChatGPT response alongside other search engine results (Google,Bing,Duck go go,..)**

***ChatGPT for Search Engines*** is an AI-based extension that could potentially become a real threat to any search engine.

It basically shows results to all sorts of queries next to the Google results (or other search engine). The precision is very impressive. This extension makes it possible everywhere you browse

This is a simple extension that show response from ChatGPT alongside Google and other search engines

Features:

\* Markdown rendering

\* Code hightlights

\* Feedback buttons

\* Custom trigger mode

Maybe you should try this extension: [shorturl.at/eqZ78](https://shorturl.at/eqZ78)

Watch this video to see how it work: [https://www.tiktok.com/@ai\_life26/video/7179865803003579674](https://www.tiktok.com/@ai_life26/video/7179865803003579674)

https://preview.redd.it/ojjxcy2q9f7a1.png?width=1294&format=png&auto=webp&s=e3b02aba9ba03f37dbdcd0a2c6265a95b9f11ed8"
21,yu8oru,deeplearning,GPT-3,top,2022-11-13 17:50:42,"Can we possibly get access to large language models (PaLM 540B, etc) like GPT-3 but no cost?",NLP2829,False,0.6,2,https://www.reddit.com/r/deeplearning/comments/yu8oru/can_we_possibly_get_access_to_large_language/,3,1668361842.0,"(I only want to do inference, I don't need to finetune it.)

I want to use very-large language model (#parameters > 100B) to do some experiments, is that true the only very-large language model we can get access to is GPT3 API? Can we possibly get access to PaLM and Flan-PaLM 540B with no cost by chance?

I have searched over the internet but can't find a definite answer. As GPT-3 pricing for text-davinci-2 is not cheap, I am wondering if there's a chance to use other models.

Also, I can request up to 372GB VRAM, is there any large language model (#parameters > 100B) that I can actually download and run ""locally""?"
22,121agx4,deeplearning,GPT-3,comments,2023-03-25 04:24:49,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.92,47,https://www.reddit.com/r/deeplearning/comments/121agx4/do_we_really_need_100b_parameters_in_a_large/,54,1679718289.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
23,10mhyek,deeplearning,GPT-3,comments,2023-01-27 10:45:48,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.95,117,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
24,za73dc,deeplearning,GPT-3,comments,2022-12-02 01:35:02,GPT-3 Generated Rap Battle between Yann LeCun & Gary Marcus,hayAbhay,False,0.99,140,https://i.redd.it/ybfcfvez1e3a1.png,16,1669944902.0,
25,10irh5u,deeplearning,GPT-3,comments,2023-01-22 19:11:36,Apple M2 Max 96 GB unified memory for larger models vs multiple 24GB GPUs or 40GB A100s?,lol-its-funny,False,1.0,23,https://www.reddit.com/r/deeplearning/comments/10irh5u/apple_m2_max_96_gb_unified_memory_for_larger/,14,1674414696.0,"How feasible is it to use an Apple Silicon M2 Max, which has about [96 GB unified memory](https://www.apple.com/shop/buy-mac/macbook-pro/16-inch-space-gray-apple-m2-max-with-12-core-cpu-and-38-core-gpu-1tb) for ""large model"" deep learning? I'm inspired by the the [Chinchilla](https://arxiv.org/abs/2203.15556) paper that shows a lot of promise at 70B parameters. Outperforming ultra large models like Gopher (280B) or GPT-3 (175B) there is hope for working with < 70B parameters without needing a super computer. At least for fine tuning. I've been working with GPT-J but want to scale/tinker with larger open-sourced models.

However, I don't know how clearly the CompSci theory (M2 Max's 38-core GPU, 16 core Neural Engine accessing 96 GB unified memory) maps out to the IT reality (toolkits and libraries on macOS actually using it). My exposure is mostly around Jupyter books on Colab Pro+ (A100s) and nvidia 3080 GPUs (locally). 

I appreciate your guidance."
26,10fw22o,deeplearning,GPT-3,comments,2023-01-19 07:55:49,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.9,70,https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/,11,1674114949.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=webp&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
27,ylj1ux,deeplearning,GPT-3,comments,2022-11-03 23:55:15,BlogNLP: AI Writing Tool,britdev,False,1.0,36,https://www.reddit.com/r/deeplearning/comments/ylj1ux/blognlp_ai_writing_tool/,9,1667519715.0,"Hey everyone,

I created this web app using Open AI's GPT-3 (Davinci model). The purpose here is to provide a free tool to allow people to generate blog content/outlines/headlines and help with writer's block. Will continue to improve it over time, but just a side project I figured would provide some value to you all. Hope you all enjoy and please share ❤️

[https://www.blognlp.com/](https://www.blognlp.com/)"
28,1180x0e,deeplearning,GPT-3,comments,2023-02-21 11:06:33,I created a Search Engine For Books using GPT-3 🔎📘. Here's how you can create it too:,Pritish-Mishra,False,0.71,6,https://youtu.be/SXFP4nHAWN8,8,1676977593.0,
29,12ff87f,deeplearning,GPT-3,comments,2023-04-08 07:55:07,need help. GPT-3.5 can't solve it.,ryanultralifeio,False,0.25,0,https://www.reddit.com/r/deeplearning/comments/12ff87f/need_help_gpt35_cant_solve_it/,8,1680940507.0,"Trying to make a schedule for the league, here are the constraints.  I think it should be tailormade for AI.


Schedule May 2023 Games.

￼￼

I need you to schedule games between 7 teams, on 4 fields, beginning on Monday May 1st for the whole month of May 2023. 

The 4 fields are; Quincy, Portola, Chester and Loyalton. Fields in Quincy, Portola and Loyalton are available beginning May 1st. The field in Chester is available beginning May 8th.

 Saturdays can have 3 games per day at either 10am, 1pm, or 4 pm. 

No games on Sunday. 

Monday, Tuesday, Wednesday, Thursday, and Friday games are at 5:00. 

Mondays, Tuesdays, Wednesdays, Thursdays, and Fridays can have games played on 3 different fields at the same time. 

There are 7 teams. Quincy Red, Quincy Blue, Quincy Grey, Portola Padres, Portola Dodgers, Chester Giants and Loyalton. 

All teams can only play each other 2 times in May with the exceptions of Quincy Grey and Quincy Red, Quincy Grey and Quincy Blue, and Quincy Grey and Chester Giants, who can only play each other 1 time in May. 

Only Loyalton cannot play on May 3,4, or 5 for Sierra Nevada Journeys. 

All teams are unavailable to play May 26,27,29 for Memorial Day Weekend. 

All teams are unavailable to play May 17,18,19 for 6th grade field trip. 

Each team will play one home game against each other, except for the teams only playing one game. 

Quincy Blue only plays home games on Quincy field on Mondays, and Thursdays. 

Quincy Grey only plays home games on Quincy field on Wednesdays, and Fridays. 

Quincy Red only plays home games on Quincy Field on Tuesdays, Thursdays, and Fridays. 

Loyalton only plays home games on Loyalton field. 

Chester Giants only play Home Games on Chester field. 

Portola Padres only play home games on Portola field. 

Portola Dodgers only play home games on Portola field. 

Each team can play a maximum of two games per week. 

A team cannot play without two calenders days between games. 

A team cannot play two games on consecutive days.

A team cannot play two games on the same day. 

Teams must have at least 9 games.

Put the total number of games played per team at the bottom of the whole months schedule.

2+ hours a no good results........."
30,11ium8l,deeplearning,GPT-3,comments,2023-03-05 11:10:56,LLaMA model parallelization and server configuration,ChristmasInOct,False,1.0,25,https://www.reddit.com/r/deeplearning/comments/11ium8l/llama_model_parallelization_and_server/,8,1678014656.0,"Hey everyone,

First of all, tldr at bottom, typed more than expected here.  

Please excuse the rather naive perspective I have here.  I've followed along with great interest, but this is not my industry.

Regardless, I have spent the past 3-4 days falling down a brutally obsessive rabbit hole, and I cannot seem to find this information.  I'm assuming it's just that I am missing context of course, and regardless of whether there is a clear answer, I'm trying to get a better understanding of this topic so that I could better appraise the situation myself.

Really I suppose I have two questions.  **The first** is regarding model parallelization.

I'm assuming this is not generic whatsoever.  What is the typical process engineers go about for designing such a pipeline?  Specifically in regards to these new LLaMA models, is something like ALPA relevant?  Deepspeed?

More importantly, what information should I be seeking to determine this myself?

This roughly segues to my **second inquiry**.

The reason I'm curious about splitting the model pipeline etc., is that I am potentially in interested in standing a server up for this.  Although I don't have much of a budget for this build (\~$30-40K is the rough top-end, but I'd be a lot happier around $20-25K), the money is there if I can genuinely satisfy my use-case.

I work at a small, but borderline manic startup working on enterprise software; 90% of the work we're doing based in the react/node ecosystem, some low-level work for backend services, and some very interesting database work that I have very little to do with.  I am a fullstack engineer that grew up playing with C++ => C#, and somehow ended up spending all of my time r/w'ing javascript.  Lol.  Anyways.

Part of our roadmap since GPT-3 and the playground were made publicly accessible, involves usage of these transformer models, and their ability to interpret natural language inputs, whether from user inputs, or scraped input values generated somewhere in a chain of requests / operations.

Seeing GPT-3 in action made me specifically realize that my estimations on this technology had been wildly off.  Seeing ChatGPT in action and uptick, the API's becoming available, has me further panicked.

Running our inference through their API has never really been an option for us.  I haven't even really looked that far into it, but bottom line the data running through our platform is all back-office, highly sensitive business information, and many have agreements explicitly restricting the movement of data to or from any cloud services, with Microsoft, Amazon, and Google all specifically mentioned.

Regardless of the reasoning for these contracts, the LLaMA release has had me obsessed over this topic in more detail than before, and whether or not I would be able to get this setup privately, for our use-case.

**To get to the actual second inquiry**:

Say I want to throw a budget rig together for this in a server cabinet.  Am I able to effectively parallelize the LLaMA model, well enough to justify going with 24GB VRAM 4090's in the rig?  Say I do so with DeepSpeed, or some of the standard model parallelization libraries.

Is the performance cost low enough to justify taking the extra compute here over 1/3 - 1/2 as many RTX6000 ADA's?

Or should I be grabbing the 48GB ADA's?

Like I said, I apologize for the naivety, I'm really looking for more information so that I can start to put this picture together better on my own.  It really isn't the easiest topic to research with how quickly things seem to move, and the giant gap between conversation depths (gamer || phd in a lot of the most interesting or niche discussions, little between).

Thank you very much for your time.

TL;DR - Any information on LLaMA model parallelization at the moment?  Will it be compatible with things like zero or alpa?  How about for throwing a rig together right now for fine-tuning and then running inference on the LLaMA models?  48GB 6000 ADA's, or 24GB 4090's?

Planning on putting it in a mostly empty 42U cabinet that also houses our primary web server and networking hardware, so if there is a sales pitch for 4090's across multiple nodes here, I do have a massive bias as the kind of nerd that finds that kind of hardware borderline erotic.

Hydro and cooling are not an issue, just usage of the budget and understanding the requirements / approach given memory limitations, and how to avoid communication bottlenecks or even balance them against raw compute.

Thanks again everyone!"
31,zboc8w,deeplearning,GPT-3,comments,2022-12-03 19:29:01,BlogNLP: AI Blog Writing Tool,britdev,False,0.88,12,https://www.reddit.com/r/deeplearning/comments/zboc8w/blognlp_ai_blog_writing_tool/,7,1670095741.0,"Hey everyone,

I developed this web app with Open AI's GPT-3 to provide a free, helpful resource for generating blog content, outlines, and more - so you can beat writer's block! I'm sure you'll find it useful and I'd really appreciate it if you shared it with others ❤️.

[https://www.blognlp.com/](https://www.blognlp.com/)"
32,129k24i,deeplearning,GPT-3,comments,2023-04-02 12:37:38,[N] Software 3.0 Blog Post Release 🔥,DragonLord9,False,0.69,8,https://www.reddit.com/r/deeplearning/comments/129k24i/n_software_30_blog_post_release/,3,1680439058.0,"Hi all, excited to share my blog post on [**Software 3.0**](https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm_campaign=post&utm_medium=web)

https://preview.redd.it/9b4hjkkhugra1.png?width=1500&format=png&auto=webp&s=e341f3ab4c3c8abb206df8daa17428a297ff61e2

The blog post offers an insightful read on the new GPT-powered programming paradigm where the new programming language is simply ""*English*"", as well as recent developments in AI.

The post was originally written before GPT-4 release, and the predictions seem to have held surprisingly well. Knowledge cutoff date 28 Feb 2023.

Please read and share!! Happy to answer any follow-ups here or on DM 😊

Tweet: [https://twitter.com/DivGarg9/status/1642229948185280521?s=20](https://twitter.com/DivGarg9/status/1642229948185280521?s=20)

Blog: [https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm\_campaign=post&utm\_medium=web](https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm_campaign=post&utm_medium=web)"
33,11q8tir,deeplearning,GPT-3,comments,2023-03-13 12:50:10,Learning logical relationships with neural networks with differential ILP,Neurosymbolic,False,1.0,14,https://www.reddit.com/r/deeplearning/comments/11q8tir/learning_logical_relationships_with_neural/,3,1678711810.0,"Since last week’s post on my lab’s software package [PyReason](https://neurosymoblic.asu.edu/pyreason/), I got a lot of questions on if it would be possible for a neural network to learn logical relationships from data. After all, ChatGPT seems to be able to generate Python code, and Meta released a NeurIPS paper to show you can learn math equations from data using a transformer-based model, so why not logic? In this article, we will one line of research in this area – differential ILP.

The research in this area really kicked off with a 2018 [paper from DeepMind](https://www.reddit.com/r/deeplearning/jair.org/index.php/jair/article/view/11172) where Richard Evans and Edward Grefenstette showed that you could adapt techniques from “inductive logic programming” to use gradient descent, and learn logical rules from data. Previous (non-neural) work on inductive logic programming was generally not designed to work with noisy data and instead fit the historical examples in a precise manner. Evans and Grefenstette utilized a neural architecture and a loss function – and they showed they could handle noisy data and even do some level of integration with CNN’s. Their neural architecture mimicked a set of candidate logical rules – and the rules assigned higher weights by gradient descent would be thought to best fit the data. However, a downside to this approach is that the neural network was [quintic in the size of the input](https://www.youtube.com/watch?v=SOnAE0EyX8c&list=PLpqh-PUKX-i7URwnkTqpAkSchJHvbxZHB&index=5). This is why they only applied their approach on very small problems – it did not see very wide adoption.

That said, in the last two years, there have been some notable follow-ons to this work. Researchers out of Kyoto University and NTT introduced a manner to learn rules that are more expressive in a different manner by allowing function symbols in the logical language ([Shindo et al., AAAI 2021](https://ojs.aaai.org/index.php/AAAI/article/view/16637/16444)). They leverage a clause search and refinement process to limit the number of candidate rules – hence limiting the size of the neural network. A student team from ASU created a presentation on their work for our recent seminar course on neuro symbolic AI. We released a three part video series from their talk:

[Part 1: Review of differentiable inductive logic programming](https://www.youtube.com/watch?v=JIS78a40q8U&t=270s)

[Part 2: Clause search and refinement In our recent video series](https://www.youtube.com/watch?v=nzfbxlHUwuE&t=345s)

[Part 3: Experiments](https://www.youtube.com/watch?v=-fKWNtHUIN0&t=27s)

[Slides](https://labs.engineering.asu.edu/labv2/wp-content/uploads/sites/82/2022/10/Shindo_dILP.pdf)

Some think that the ability to learn such relationships will represent a significant advancement in ML, specifically addressing shortcomings in areas such as knowledge graph completion and reasoning about scene graphs. However, the gap still remains wide, and ILP techniques, including differentiable ILP still have a ways to go. Really interested in what your thoughts are, feel free to comment below."
34,yu8oru,deeplearning,GPT-3,comments,2022-11-13 17:50:42,"Can we possibly get access to large language models (PaLM 540B, etc) like GPT-3 but no cost?",NLP2829,False,0.56,1,https://www.reddit.com/r/deeplearning/comments/yu8oru/can_we_possibly_get_access_to_large_language/,3,1668361842.0,"(I only want to do inference, I don't need to finetune it.)

I want to use very-large language model (#parameters > 100B) to do some experiments, is that true the only very-large language model we can get access to is GPT3 API? Can we possibly get access to PaLM and Flan-PaLM 540B with no cost by chance?

I have searched over the internet but can't find a definite answer. As GPT-3 pricing for text-davinci-2 is not cheap, I am wondering if there's a chance to use other models.

Also, I can request up to 372GB VRAM, is there any large language model (#parameters > 100B) that I can actually download and run ""locally""?"
35,zth8rl,deeplearning,GPT-3,comments,2022-12-23 14:35:17,How to change career trajectory to NLP engineer,Creative-Milk-8266,False,0.84,12,https://www.reddit.com/r/deeplearning/comments/zth8rl/how_to_change_career_trajectory_to_nlp_engineer/,3,1671806117.0," A little of my background - 5 years experience in data science. Mostly related to prototyping statistical models and optimization problems, bringing them into production. Some experience in building pipeline and orchestration flow with AWS services.

I have basic understanding on Transformers, BERT, GPT. Did my first NLP Kaggle competition the first time recently.

I'd like my next job to be a NLP engineer. How should I prepare myself for it?

Here's some of the items I'm thinking

&#x200B;

1. More hands on projects I can put on resume, including integration with cloud services. Any recommendations on what kinds of projects I should pick?  
 
2. Tryout techniques of speeding up models like distilled model, dynamic shape, quantization. Anything else that would be helpful?  
 
3. Understand lower level of GPU programming knowledges. Not sure if this is helpful for me finding a NLP job. If so, what kind of things I can do to go deeper on this subject. I'm currently taking [Intro to Parallel Programming](https://classroom.udacity.com/courses/cs344) CS344 course on Udemy (highly recommend btw).  
 
4. Grind leetcode :/  
 

Please point out other important directions I missed."
36,10g2npf,deeplearning,GPT-3,comments,2023-01-19 14:10:45,BlogNLP: AI Blog Writing Tool,britdev,False,0.5,0,https://www.reddit.com/r/deeplearning/comments/10g2npf/blognlp_ai_blog_writing_tool/,2,1674137445.0,"Hey everyone,

I developed this web app with Open AI's GPT-3 to provide a helpful resource for generating blog content, outlines, and more - so you can beat writer's block! I'd really appreciate it if you shared it with others.

[https://www.blognlp.com/](https://www.blognlp.com/)"
37,zk2ser,deeplearning,GPT-3,comments,2022-12-12 15:53:41,"GPT-Rex: A chrome extension to plug GPT-3 directly into Medium. Hit ""Ctrl + >"" to trigger auto-complete while writing. Available on Chrome web store. Support for other platforms coming soon.",hayAbhay,False,0.57,1,https://github.com/hayabhay/gpt-go,2,1670860421.0,
38,yzwzp0,deeplearning,GPT-3,comments,2022-11-20 06:20:53,How do various content-generating services work?,th3luck,False,0.89,7,https://www.reddit.com/r/deeplearning/comments/yzwzp0/how_do_various_contentgenerating_services_work/,1,1668925253.0,"Right now sites like [https://www.jasper.ai/](https://www.jasper.ai/) offer text generation for emails, ads, social media posts and etc. I wonder, do they simply tune a separate gpt-3-like model for each of these tasks? Or there is a new approach to solving this?"
39,10bq685,deeplearning,GPT-3,comments,2023-01-14 14:48:43,Scaling Language Models Shines Light On The Future Of AI ⭕,LesleyFair,False,0.72,8,https://www.reddit.com/r/deeplearning/comments/10bq685/scaling_language_models_shines_light_on_the/,1,1673707723.0,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs might allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* reduce hallucination and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example is words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (a proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/jlh726eku0ca1.png?width=800&format=png&auto=webp&s=55d170251a967f31b36f01864af6bb7e2dbda253)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week.No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)"
40,10igecg,deeplearning,GPT-3,comments,2023-01-22 10:12:08,"BigScience BLOOM, how should we use it?",Haghiri75,False,1.0,5,https://www.reddit.com/r/deeplearning/comments/10igecg/bigscience_bloom_how_should_we_use_it/,1,1674382328.0,"Since the release of BLOOM, I always wanted to test it the way GPT-3 (and newly released ChatGPT) are tested. Having a playground with the ability to explore settings and even generating codes and stuff. But I don't know how long was it (I guess almost a year) and the only thing *close to playground* it had was the huggingface model card.

So is there any reliable way to use BLOOM in a proper way?"
41,12uv0og,deeplearning,GPT-3,comments,2023-04-22 04:48:57,Help give feedback on an AI generated comic system,laa_k,False,0.25,0,https://www.reddit.com/r/deeplearning/comments/12uv0og/help_give_feedback_on_an_ai_generated_comic_system/,0,1682138937.0," Over the past few months, some colleagues and I have put together a system based on ChatGPT, Stable Diffusion, and other AI tools to create simple 3-panel comic strips. We would appreciate 5 minutes of your time to help us evaluate the system and its outputs by taking the following survey:

[https://qfreeaccountssjc1.az1.qualtrics.com/jfe/form/SV\_5haZc4idQ7mkbUG](https://qfreeaccountssjc1.az1.qualtrics.com/jfe/form/SV_5haZc4idQ7mkbUG)

Thank You!

&#x200B;

https://preview.redd.it/hkhwwtm59dva1.png?width=1556&format=png&auto=webp&s=c9b10687d93f3d6d475da8feb3ad978d304cd9db"
42,10n8c80,deeplearning,GPT-3,comments,2023-01-28 06:34:28,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,0.95,16,https://www.reddit.com/r/deeplearning/comments/10n8c80/a_python_module_to_generate_optimized_prompts/,0,1674887668.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features 🚀

* 🧙‍♀️ NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* 🔨 Easily add one-shot, two-shot, or few-shot examples to the prompt
* ✌ Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* 💥 Custom examples and samples can be easily added to the prompt
* 💰 Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Example](https://preview.redd.it/sjvhtd8b3nea1.png?width=1236&format=png&auto=webp&s=e9a3a28f41f59cd25fe8e95bd1fca56b15f27a6e)

&#x200B;

https://preview.redd.it/fnb05bys3nea1.png?width=1398&format=png&auto=webp&s=096f4e2cbd0a71e795f30cc5e3720316b5e5caf6"
43,128tfvc,deeplearning,GPT-3,comments,2023-04-01 17:50:06,Fine-tune GPT on sketch data (stroke-3),mellamo_maria,False,1.0,1,https://www.reddit.com/r/deeplearning/comments/128tfvc/finetune_gpt_on_sketch_data_stroke3/,0,1680371406.0," These past days I have started a personal project where I would like to build a model that, given an uncompleted sketch, it can finish it. I was planning on using some pretrained models that are available in HuggingFace and fine-tune them with my sketch data for my task. The sketch data I have is in stoke-3 format, like the following example:  
\[  
\[10, 20, 1\],  
\[20, 30, 1\],  
\[30, 40, 1\],  
\[40, 50, 0\],  
\[50, 60, 1\],  
\[60, 70, 0\]  
\]  
The first value of each triple is the X-coordinate, the second value the Y-coordinate and the last value is a binary value indicating whether the pen is down (1) or up (0). I was wondering if you guys could give me some instruction/tips about how should I approach this problem? How should I prepare/preprocess the data so I can fit it into the pre-trained models like BERT, GPT, etc. Since it's stroke-3 data and not text or a sequence of numbers, I don't really know how should I treat/process the data.

Thanks a lot! :)"
44,zsics7,deeplearning,GPT-3,comments,2022-12-22 09:57:14,"Show ChatGPT's response next to the search results from Google, Bing, and DuckDuckGo.",Harrypham22,False,1.0,1,https://www.reddit.com/r/deeplearning/comments/zsics7/show_chatgpts_response_next_to_the_search_results/,0,1671703034.0," **Do you know about ChatGPT?** 

It is a variant of the GPT-3 language model developed by OpenAI specifically designed for generating responses to user input in chat or messaging applications. It is trained on a large dataset of conversation data and is able to understand the context of a conversation and generate appropriate responses. ChatGPT is particularly well-suited for use in chat or messaging applications, but it can also be used for a wide range of other natural language processing tasks, such as language translation, summarization, and question answering.

**Display ChatGPT response alongside other search engine results (Google,Bing,Duck go go,..)**

***ChatGPT for Search Engines*** is an AI-based extension that could potentially become a real threat to any search engine.

It basically shows results to all sorts of queries next to the Google results (or other search engine). The precision is very impressive. This extension makes it possible everywhere you browse

This is a simple extension that show response from ChatGPT alongside Google and other search engines

Features:

\* Markdown rendering

\* Code hightlights

\* Feedback buttons

\* Custom trigger mode

Maybe you should try this extension: [shorturl.at/eqZ78](https://shorturl.at/eqZ78)

Watch this video to see how it work: [https://www.tiktok.com/@ai\_life26/video/7179865803003579674](https://www.tiktok.com/@ai_life26/video/7179865803003579674)

https://preview.redd.it/ojjxcy2q9f7a1.png?width=1294&format=png&auto=webp&s=e3b02aba9ba03f37dbdcd0a2c6265a95b9f11ed8"
45,10m3034,deeplearning,GPT-3,comments,2023-01-26 21:26:30,Create Your Chat GPT-3 Web App with Streamlit in Python,pasticciociccio,False,0.43,0,https://levelup.gitconnected.com/create-your-chat-gpt-3-web-app-with-streamlit-in-python-f0c6e6aede0a,0,1674768390.0,
46,zaxg5m,deeplearning,GPT-3,comments,2022-12-02 20:59:47,Everyone: AI will make it easy to spread misinformation; Me: Stop hitting yourself GPT3!,hayAbhay,False,0.89,37,https://i.redd.it/mrf9rz0ltj3a1.png,0,1670014787.0,
47,128nbfn,deeplearning,GPT-3,comments,2023-04-01 14:01:42,Revolutionizing Content Creation: Moji AI's Impact on Social Media and Beyond,Large_Rush9013,False,0.25,0,https://www.reddit.com/r/deeplearning/comments/128nbfn/revolutionizing_content_creation_moji_ais_impact/,0,1680357702.0,"Hey fellow Redditors, I recently stumbled upon a summary of an incredible new AI content tool called Moji AI, and I just had to share my thoughts about it. I think it has the potential to be a game-changer for content creators!

Moji AI is designed to make content creation easier by using the power of GPT-4 to generate text and Stable Diffusion Models to create eye-catching images. It offers icons and image assets that can significantly boost social media engagement. As a Reddit user, I'm always trying to find new ways to share content and start conversations, and I think the potential benefits of this tool are undeniable.

I've been aware of GPT-3 for a while now, and the thought of GPT-4 being a more powerful version gets me excited about what it could mean for the future of AI-generated content. The fact that Moji AI can not only generate text, but also customize images and icons, makes it seem like a must-have tool for anyone serious about making an impact on social media platforms.

The Stable Diffusion Models used by Moji AI allow it to create visually stunning images that are bound to catch the attention of users as they're scrolling through their feeds. It's not just about the text anymore - visuals are crucial in today's social media landscape, and Moji AI is tackling that aspect head-on.

I can already think of countless ways to apply Moji AI in both personal and professional projects. Imagine effortlessly creating engaging blog posts, social media posts, and digital marketing campaigns without the hassle of finding a graphic designer or a copywriter. This tool seems too good to be true!

For those of you who are interested in learning more about Moji AI and how it can elevate your content creation game, I urge you to check out their website at [mojiai.io](https://mojiai.io). I'm excited to see the applications of this tool, and I believe that it'll revolutionize how we create and share content moving forward.

Indeed, it's exciting to be part of a community that is always at the forefront of groundbreaking innovations like Moji AI! Feel free to share your thoughts and ideas about how you think Moji AI could impact the world of content creation. Let's start a conversation!"
48,za73dc,deeplearning,GPT-3,relevance,2022-12-02 01:35:02,GPT-3 Generated Rap Battle between Yann LeCun & Gary Marcus,hayAbhay,False,0.99,137,https://i.redd.it/ybfcfvez1e3a1.png,16,1669944902.0,
49,zb2kkc,deeplearning,GPT-3,relevance,2022-12-03 00:17:31,A GPT-3 based Chrome Extension that debugs your code!,VideoTo,False,0.97,21,https://www.reddit.com/r/deeplearning/comments/zb2kkc/a_gpt3_based_chrome_extension_that_debugs_your/,0,1670026651.0,"Link - [https://chrome.google.com/webstore/detail/clerkie-ai/oenpmifpfnikheaolfpabffojfjakfnn](https://chrome.google.com/webstore/detail/clerkie-ai/oenpmifpfnikheaolfpabffojfjakfnn)

Built  a quick tool I thought would be interesting - it’s a chrome extension  that uses GPT-3 under the hood to help debug your programming errors  when you paste them into Google (“eg. TypeError:…”).

This is definitely early days, so **if   this is something you would find valuable and wouldn't mind testing a   couple iterations of, please feel free to join the discord** \-> [https://discord.gg/KvG3azf39U](https://discord.gg/KvG3azf39U)

&#x200B;

https://i.redd.it/tt6hcqn2tk3a1.gif"
50,10m3034,deeplearning,GPT-3,relevance,2023-01-26 21:26:30,Create Your Chat GPT-3 Web App with Streamlit in Python,pasticciociccio,False,0.43,0,https://levelup.gitconnected.com/create-your-chat-gpt-3-web-app-with-streamlit-in-python-f0c6e6aede0a,0,1674768390.0,
51,1180x0e,deeplearning,GPT-3,relevance,2023-02-21 11:06:33,I created a Search Engine For Books using GPT-3 🔎📘. Here's how you can create it too:,Pritish-Mishra,False,0.64,4,https://youtu.be/SXFP4nHAWN8,8,1676977593.0,
52,yu8oru,deeplearning,GPT-3,relevance,2022-11-13 17:50:42,"Can we possibly get access to large language models (PaLM 540B, etc) like GPT-3 but no cost?",NLP2829,False,0.5,0,https://www.reddit.com/r/deeplearning/comments/yu8oru/can_we_possibly_get_access_to_large_language/,3,1668361842.0,"(I only want to do inference, I don't need to finetune it.)

I want to use very-large language model (#parameters > 100B) to do some experiments, is that true the only very-large language model we can get access to is GPT3 API? Can we possibly get access to PaLM and Flan-PaLM 540B with no cost by chance?

I have searched over the internet but can't find a definite answer. As GPT-3 pricing for text-davinci-2 is not cheap, I am wondering if there's a chance to use other models.

Also, I can request up to 372GB VRAM, is there any large language model (#parameters > 100B) that I can actually download and run ""locally""?"
53,zk2ser,deeplearning,GPT-3,relevance,2022-12-12 15:53:41,"GPT-Rex: A chrome extension to plug GPT-3 directly into Medium. Hit ""Ctrl + >"" to trigger auto-complete while writing. Available on Chrome web store. Support for other platforms coming soon.",hayAbhay,False,0.63,2,https://github.com/hayabhay/gpt-go,2,1670860421.0,
54,10fw22o,deeplearning,GPT-3,relevance,2023-01-19 07:55:49,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.9,70,https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/,11,1674114949.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=webp&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
55,12ff87f,deeplearning,GPT-3,relevance,2023-04-08 07:55:07,need help. GPT-3.5 can't solve it.,ryanultralifeio,False,0.25,0,https://www.reddit.com/r/deeplearning/comments/12ff87f/need_help_gpt35_cant_solve_it/,8,1680940507.0,"Trying to make a schedule for the league, here are the constraints.  I think it should be tailormade for AI.


Schedule May 2023 Games.

￼￼

I need you to schedule games between 7 teams, on 4 fields, beginning on Monday May 1st for the whole month of May 2023. 

The 4 fields are; Quincy, Portola, Chester and Loyalton. Fields in Quincy, Portola and Loyalton are available beginning May 1st. The field in Chester is available beginning May 8th.

 Saturdays can have 3 games per day at either 10am, 1pm, or 4 pm. 

No games on Sunday. 

Monday, Tuesday, Wednesday, Thursday, and Friday games are at 5:00. 

Mondays, Tuesdays, Wednesdays, Thursdays, and Fridays can have games played on 3 different fields at the same time. 

There are 7 teams. Quincy Red, Quincy Blue, Quincy Grey, Portola Padres, Portola Dodgers, Chester Giants and Loyalton. 

All teams can only play each other 2 times in May with the exceptions of Quincy Grey and Quincy Red, Quincy Grey and Quincy Blue, and Quincy Grey and Chester Giants, who can only play each other 1 time in May. 

Only Loyalton cannot play on May 3,4, or 5 for Sierra Nevada Journeys. 

All teams are unavailable to play May 26,27,29 for Memorial Day Weekend. 

All teams are unavailable to play May 17,18,19 for 6th grade field trip. 

Each team will play one home game against each other, except for the teams only playing one game. 

Quincy Blue only plays home games on Quincy field on Mondays, and Thursdays. 

Quincy Grey only plays home games on Quincy field on Wednesdays, and Fridays. 

Quincy Red only plays home games on Quincy Field on Tuesdays, Thursdays, and Fridays. 

Loyalton only plays home games on Loyalton field. 

Chester Giants only play Home Games on Chester field. 

Portola Padres only play home games on Portola field. 

Portola Dodgers only play home games on Portola field. 

Each team can play a maximum of two games per week. 

A team cannot play without two calenders days between games. 

A team cannot play two games on consecutive days.

A team cannot play two games on the same day. 

Teams must have at least 9 games.

Put the total number of games played per team at the bottom of the whole months schedule.

2+ hours a no good results........."
56,10n8c80,deeplearning,GPT-3,relevance,2023-01-28 06:34:28,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,1.0,17,https://www.reddit.com/r/deeplearning/comments/10n8c80/a_python_module_to_generate_optimized_prompts/,0,1674887668.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features 🚀

* 🧙‍♀️ NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* 🔨 Easily add one-shot, two-shot, or few-shot examples to the prompt
* ✌ Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* 💥 Custom examples and samples can be easily added to the prompt
* 💰 Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Example](https://preview.redd.it/sjvhtd8b3nea1.png?width=1236&format=png&auto=webp&s=e9a3a28f41f59cd25fe8e95bd1fca56b15f27a6e)

&#x200B;

https://preview.redd.it/fnb05bys3nea1.png?width=1398&format=png&auto=webp&s=096f4e2cbd0a71e795f30cc5e3720316b5e5caf6"
57,zboc8w,deeplearning,GPT-3,relevance,2022-12-03 19:29:01,BlogNLP: AI Blog Writing Tool,britdev,False,0.88,12,https://www.reddit.com/r/deeplearning/comments/zboc8w/blognlp_ai_blog_writing_tool/,7,1670095741.0,"Hey everyone,

I developed this web app with Open AI's GPT-3 to provide a free, helpful resource for generating blog content, outlines, and more - so you can beat writer's block! I'm sure you'll find it useful and I'd really appreciate it if you shared it with others ❤️.

[https://www.blognlp.com/](https://www.blognlp.com/)"
58,10igecg,deeplearning,GPT-3,relevance,2023-01-22 10:12:08,"BigScience BLOOM, how should we use it?",Haghiri75,False,1.0,6,https://www.reddit.com/r/deeplearning/comments/10igecg/bigscience_bloom_how_should_we_use_it/,1,1674382328.0,"Since the release of BLOOM, I always wanted to test it the way GPT-3 (and newly released ChatGPT) are tested. Having a playground with the ability to explore settings and even generating codes and stuff. But I don't know how long was it (I guess almost a year) and the only thing *close to playground* it had was the huggingface model card.

So is there any reliable way to use BLOOM in a proper way?"
59,ylj1ux,deeplearning,GPT-3,relevance,2022-11-03 23:55:15,BlogNLP: AI Writing Tool,britdev,False,0.98,35,https://www.reddit.com/r/deeplearning/comments/ylj1ux/blognlp_ai_writing_tool/,9,1667519715.0,"Hey everyone,

I created this web app using Open AI's GPT-3 (Davinci model). The purpose here is to provide a free tool to allow people to generate blog content/outlines/headlines and help with writer's block. Will continue to improve it over time, but just a side project I figured would provide some value to you all. Hope you all enjoy and please share ❤️

[https://www.blognlp.com/](https://www.blognlp.com/)"
60,10g2npf,deeplearning,GPT-3,relevance,2023-01-19 14:10:45,BlogNLP: AI Blog Writing Tool,britdev,False,0.5,0,https://www.reddit.com/r/deeplearning/comments/10g2npf/blognlp_ai_blog_writing_tool/,2,1674137445.0,"Hey everyone,

I developed this web app with Open AI's GPT-3 to provide a helpful resource for generating blog content, outlines, and more - so you can beat writer's block! I'd really appreciate it if you shared it with others.

[https://www.blognlp.com/](https://www.blognlp.com/)"
61,10irh5u,deeplearning,GPT-3,relevance,2023-01-22 19:11:36,Apple M2 Max 96 GB unified memory for larger models vs multiple 24GB GPUs or 40GB A100s?,lol-its-funny,False,0.96,20,https://www.reddit.com/r/deeplearning/comments/10irh5u/apple_m2_max_96_gb_unified_memory_for_larger/,14,1674414696.0,"How feasible is it to use an Apple Silicon M2 Max, which has about [96 GB unified memory](https://www.apple.com/shop/buy-mac/macbook-pro/16-inch-space-gray-apple-m2-max-with-12-core-cpu-and-38-core-gpu-1tb) for ""large model"" deep learning? I'm inspired by the the [Chinchilla](https://arxiv.org/abs/2203.15556) paper that shows a lot of promise at 70B parameters. Outperforming ultra large models like Gopher (280B) or GPT-3 (175B) there is hope for working with < 70B parameters without needing a super computer. At least for fine tuning. I've been working with GPT-J but want to scale/tinker with larger open-sourced models.

However, I don't know how clearly the CompSci theory (M2 Max's 38-core GPU, 16 core Neural Engine accessing 96 GB unified memory) maps out to the IT reality (toolkits and libraries on macOS actually using it). My exposure is mostly around Jupyter books on Colab Pro+ (A100s) and nvidia 3080 GPUs (locally). 

I appreciate your guidance."
62,128tfvc,deeplearning,GPT-3,relevance,2023-04-01 17:50:06,Fine-tune GPT on sketch data (stroke-3),mellamo_maria,False,1.0,1,https://www.reddit.com/r/deeplearning/comments/128tfvc/finetune_gpt_on_sketch_data_stroke3/,0,1680371406.0," These past days I have started a personal project where I would like to build a model that, given an uncompleted sketch, it can finish it. I was planning on using some pretrained models that are available in HuggingFace and fine-tune them with my sketch data for my task. The sketch data I have is in stoke-3 format, like the following example:  
\[  
\[10, 20, 1\],  
\[20, 30, 1\],  
\[30, 40, 1\],  
\[40, 50, 0\],  
\[50, 60, 1\],  
\[60, 70, 0\]  
\]  
The first value of each triple is the X-coordinate, the second value the Y-coordinate and the last value is a binary value indicating whether the pen is down (1) or up (0). I was wondering if you guys could give me some instruction/tips about how should I approach this problem? How should I prepare/preprocess the data so I can fit it into the pre-trained models like BERT, GPT, etc. Since it's stroke-3 data and not text or a sequence of numbers, I don't really know how should I treat/process the data.

Thanks a lot! :)"
63,zsics7,deeplearning,GPT-3,relevance,2022-12-22 09:57:14,"Show ChatGPT's response next to the search results from Google, Bing, and DuckDuckGo.",Harrypham22,False,1.0,1,https://www.reddit.com/r/deeplearning/comments/zsics7/show_chatgpts_response_next_to_the_search_results/,0,1671703034.0," **Do you know about ChatGPT?** 

It is a variant of the GPT-3 language model developed by OpenAI specifically designed for generating responses to user input in chat or messaging applications. It is trained on a large dataset of conversation data and is able to understand the context of a conversation and generate appropriate responses. ChatGPT is particularly well-suited for use in chat or messaging applications, but it can also be used for a wide range of other natural language processing tasks, such as language translation, summarization, and question answering.

**Display ChatGPT response alongside other search engine results (Google,Bing,Duck go go,..)**

***ChatGPT for Search Engines*** is an AI-based extension that could potentially become a real threat to any search engine.

It basically shows results to all sorts of queries next to the Google results (or other search engine). The precision is very impressive. This extension makes it possible everywhere you browse

This is a simple extension that show response from ChatGPT alongside Google and other search engines

Features:

\* Markdown rendering

\* Code hightlights

\* Feedback buttons

\* Custom trigger mode

Maybe you should try this extension: [shorturl.at/eqZ78](https://shorturl.at/eqZ78)

Watch this video to see how it work: [https://www.tiktok.com/@ai\_life26/video/7179865803003579674](https://www.tiktok.com/@ai_life26/video/7179865803003579674)

https://preview.redd.it/ojjxcy2q9f7a1.png?width=1294&format=png&auto=webp&s=e3b02aba9ba03f37dbdcd0a2c6265a95b9f11ed8"
64,11ium8l,deeplearning,GPT-3,relevance,2023-03-05 11:10:56,LLaMA model parallelization and server configuration,ChristmasInOct,False,1.0,25,https://www.reddit.com/r/deeplearning/comments/11ium8l/llama_model_parallelization_and_server/,8,1678014656.0,"Hey everyone,

First of all, tldr at bottom, typed more than expected here.  

Please excuse the rather naive perspective I have here.  I've followed along with great interest, but this is not my industry.

Regardless, I have spent the past 3-4 days falling down a brutally obsessive rabbit hole, and I cannot seem to find this information.  I'm assuming it's just that I am missing context of course, and regardless of whether there is a clear answer, I'm trying to get a better understanding of this topic so that I could better appraise the situation myself.

Really I suppose I have two questions.  **The first** is regarding model parallelization.

I'm assuming this is not generic whatsoever.  What is the typical process engineers go about for designing such a pipeline?  Specifically in regards to these new LLaMA models, is something like ALPA relevant?  Deepspeed?

More importantly, what information should I be seeking to determine this myself?

This roughly segues to my **second inquiry**.

The reason I'm curious about splitting the model pipeline etc., is that I am potentially in interested in standing a server up for this.  Although I don't have much of a budget for this build (\~$30-40K is the rough top-end, but I'd be a lot happier around $20-25K), the money is there if I can genuinely satisfy my use-case.

I work at a small, but borderline manic startup working on enterprise software; 90% of the work we're doing based in the react/node ecosystem, some low-level work for backend services, and some very interesting database work that I have very little to do with.  I am a fullstack engineer that grew up playing with C++ => C#, and somehow ended up spending all of my time r/w'ing javascript.  Lol.  Anyways.

Part of our roadmap since GPT-3 and the playground were made publicly accessible, involves usage of these transformer models, and their ability to interpret natural language inputs, whether from user inputs, or scraped input values generated somewhere in a chain of requests / operations.

Seeing GPT-3 in action made me specifically realize that my estimations on this technology had been wildly off.  Seeing ChatGPT in action and uptick, the API's becoming available, has me further panicked.

Running our inference through their API has never really been an option for us.  I haven't even really looked that far into it, but bottom line the data running through our platform is all back-office, highly sensitive business information, and many have agreements explicitly restricting the movement of data to or from any cloud services, with Microsoft, Amazon, and Google all specifically mentioned.

Regardless of the reasoning for these contracts, the LLaMA release has had me obsessed over this topic in more detail than before, and whether or not I would be able to get this setup privately, for our use-case.

**To get to the actual second inquiry**:

Say I want to throw a budget rig together for this in a server cabinet.  Am I able to effectively parallelize the LLaMA model, well enough to justify going with 24GB VRAM 4090's in the rig?  Say I do so with DeepSpeed, or some of the standard model parallelization libraries.

Is the performance cost low enough to justify taking the extra compute here over 1/3 - 1/2 as many RTX6000 ADA's?

Or should I be grabbing the 48GB ADA's?

Like I said, I apologize for the naivety, I'm really looking for more information so that I can start to put this picture together better on my own.  It really isn't the easiest topic to research with how quickly things seem to move, and the giant gap between conversation depths (gamer || phd in a lot of the most interesting or niche discussions, little between).

Thank you very much for your time.

TL;DR - Any information on LLaMA model parallelization at the moment?  Will it be compatible with things like zero or alpa?  How about for throwing a rig together right now for fine-tuning and then running inference on the LLaMA models?  48GB 6000 ADA's, or 24GB 4090's?

Planning on putting it in a mostly empty 42U cabinet that also houses our primary web server and networking hardware, so if there is a sales pitch for 4090's across multiple nodes here, I do have a massive bias as the kind of nerd that finds that kind of hardware borderline erotic.

Hydro and cooling are not an issue, just usage of the budget and understanding the requirements / approach given memory limitations, and how to avoid communication bottlenecks or even balance them against raw compute.

Thanks again everyone!"
65,128nbfn,deeplearning,GPT-3,relevance,2023-04-01 14:01:42,Revolutionizing Content Creation: Moji AI's Impact on Social Media and Beyond,Large_Rush9013,False,0.25,0,https://www.reddit.com/r/deeplearning/comments/128nbfn/revolutionizing_content_creation_moji_ais_impact/,0,1680357702.0,"Hey fellow Redditors, I recently stumbled upon a summary of an incredible new AI content tool called Moji AI, and I just had to share my thoughts about it. I think it has the potential to be a game-changer for content creators!

Moji AI is designed to make content creation easier by using the power of GPT-4 to generate text and Stable Diffusion Models to create eye-catching images. It offers icons and image assets that can significantly boost social media engagement. As a Reddit user, I'm always trying to find new ways to share content and start conversations, and I think the potential benefits of this tool are undeniable.

I've been aware of GPT-3 for a while now, and the thought of GPT-4 being a more powerful version gets me excited about what it could mean for the future of AI-generated content. The fact that Moji AI can not only generate text, but also customize images and icons, makes it seem like a must-have tool for anyone serious about making an impact on social media platforms.

The Stable Diffusion Models used by Moji AI allow it to create visually stunning images that are bound to catch the attention of users as they're scrolling through their feeds. It's not just about the text anymore - visuals are crucial in today's social media landscape, and Moji AI is tackling that aspect head-on.

I can already think of countless ways to apply Moji AI in both personal and professional projects. Imagine effortlessly creating engaging blog posts, social media posts, and digital marketing campaigns without the hassle of finding a graphic designer or a copywriter. This tool seems too good to be true!

For those of you who are interested in learning more about Moji AI and how it can elevate your content creation game, I urge you to check out their website at [mojiai.io](https://mojiai.io). I'm excited to see the applications of this tool, and I believe that it'll revolutionize how we create and share content moving forward.

Indeed, it's exciting to be part of a community that is always at the forefront of groundbreaking innovations like Moji AI! Feel free to share your thoughts and ideas about how you think Moji AI could impact the world of content creation. Let's start a conversation!"
66,121agx4,deeplearning,GPT-3,relevance,2023-03-25 04:24:49,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.93,47,https://www.reddit.com/r/deeplearning/comments/121agx4/do_we_really_need_100b_parameters_in_a_large/,54,1679718289.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
67,10bq685,deeplearning,GPT-3,relevance,2023-01-14 14:48:43,Scaling Language Models Shines Light On The Future Of AI ⭕,LesleyFair,False,0.71,7,https://www.reddit.com/r/deeplearning/comments/10bq685/scaling_language_models_shines_light_on_the/,1,1673707723.0,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs might allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* reduce hallucination and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example is words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (a proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/jlh726eku0ca1.png?width=800&format=png&auto=webp&s=55d170251a967f31b36f01864af6bb7e2dbda253)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week.No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)"
