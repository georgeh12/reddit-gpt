,id,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,url,num_comments,created,body
0,11u1xb7,datascience,LLM,top,2023-03-17 19:57:47,I hire for super senior data scientists (30+ years of experience). These are some question I ask (be prepared!).,purplebrown_updown,False,0.86,878,https://www.reddit.com/r/datascience/comments/11u1xb7/i_hire_for_super_senior_data_scientists_30_years/,227,1679083067.0,"First, I always ask facts about the Sun. How many miles is it from the Earth? Circumference? Mass, etc. Typical DS questions anyone should know. 

Next, I go into a deep discussion about harmonic means and whats the difference between + and -, multiplication and division. 

Third-of-ly, I go into specifics about garbage collection and null reference pointers in Python, since, as a DS expert, those will be super relevant and important.  

Last, but not least, need someone who not only knows Python and SQL, but also COBALT and BASIC. 

To give some context, I work in the field of screwing in light bulbs. So we definitely want someone who knows NLP, LLM, CV, CNNs, random forests regression, mixed integer programming, optimization, etc. 

I would love to hear your thoughts. Good luck!

..."
1,137gt1i,datascience,LLM,top,2023-05-04 10:40:20,"""Experienced in GenAI""!? Let me guess 5+ years of experience?",MorningDarkMountain,False,0.85,185,https://i.redd.it/9wiziprb4uxa1.jpg,77,1683196820.0,
2,10a1mik,datascience,LLM,top,2023-01-12 14:59:04,New Research From Google Shines Light On The Future Of Language Models ⭕,LesleyFair,False,0.84,119,https://www.reddit.com/r/datascience/comments/10a1mik/new_research_from_google_shines_light_on_the/,18,1673535544.0,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs will allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* Stop hallucinating and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example are words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/w7xffqjimmba1.png?width=800&format=png&auto=webp&s=e2d9cb63f750efcbfa45c4bb7a985d4dcb5b0319)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.  
At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week.  
No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)"
3,10fw1a3,datascience,LLM,top,2023-01-19 07:54:24,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.94,122,https://www.reddit.com/r/datascience/comments/10fw1a3/gpt4_will_be_500x_smaller_than_people_think_here/,14,1674114864.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/t6m0epzlgyca1.png?width=575&format=png&auto=webp&s=a5972941053f833e76fd3a5009fc68a61f9e5406)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
4,11b5xb2,datascience,LLM,top,2023-02-24 23:46:48,Is there any self hosting LLM option that offers GPT3 level of performance?,roylv22,False,0.86,29,https://www.reddit.com/r/datascience/comments/11b5xb2/is_there_any_self_hosting_llm_option_that_offers/,25,1677282408.0,"GPT3 is great but not cheap, I also have privacy concerns using their API. Is there any LLM offers similar performance but allows self hosting?"
5,1289p4d,datascience,LLM,top,2023-04-01 03:24:17,Do you think NLP will increase with LLM models?,Muted_Standard175,False,0.86,14,https://www.reddit.com/r/datascience/comments/1289p4d/do_you_think_nlp_will_increase_with_llm_models/,7,1680319457.0,"I am thinking in studying this, some say NLP will decrease as GPT can beat most of NLP tasks in a low cost. What do you say?"
6,zkii2l,datascience,LLM,top,2022-12-13 01:37:48,HellaSwag or HellaBad? 36% of this popular LLM benchmark contains errors,maximumpineapple27,False,0.73,8,https://www.reddit.com/r/datascience/comments/zkii2l/hellaswag_or_hellabad_36_of_this_popular_llm/,1,1670895468.0,"Continuing a previous blog post analyzing errors in popular LLM benchmarks (post on Google’s GoEmotions [here](https://www.reddit.com/r/MachineLearning/comments/vye69k/30_of_googles_reddit_emotions_dataset_is/)) — I analyzed HellaSwag and found 36% contains errors.

  
For example, here’s a prompt and set of possible completions from the dataset. Which completion do you think is most appropriate? See if you can figure it out through the haze of typos and generally non-sensical writing.

  
*Men are standing in a large green field playing lacrosse. People* *is* *around the field watching the game. men*

* *are holding tshirts watching* *int* *lacrosse playing.*
* *are being interviewed in a podium in front of a large group and a gymnast is holding a microphone for the announcers.*
* *are running side to side* *of* *the* *ield* *playing lacrosse trying to score.*
* *are in a field running around playing lacrosse.*

I’ll keep it spoiler-free here, but the full blog post goes into detail on this example (and others) and explains why they are so problematic.

  
Link: [https://www.surgehq.ai/blog/hellaswag-or-hellabad-36-of-this-popular-llm-benchmark-contains-errors](https://www.surgehq.ai/blog/hellaswag-or-hellabad-36-of-this-popular-llm-benchmark-contains-errors)"
7,13ai0wp,datascience,LLM,top,2023-05-07 08:54:50,What are your thoughts on the LLM fever going on right now?,Samirio,False,0.75,8,https://www.reddit.com/r/datascience/comments/13ai0wp/what_are_your_thoughts_on_the_llm_fever_going_on/,36,1683449690.0,"The hype is strong with this one, but do you think it is justified? 

Do you think that this will actually change our day to day in ways other than using chatgpt or other LLMs as personal assistants?

I look around and see people left and right reaching out for creating applications using LLMs, but so far I didn’t see anything other than feeding documents to an LLM and having it summarize them, which doesn’t seem that ground breaking to me.

What are your thoughts on this topic?

Edit: I understand that I have over simplified my view of LLMs just “summarizing” text, when instead I should be asking something like, do you think LLMs are being effective now in the way they are being hyped for?"
8,134o5fe,datascience,LLM,top,2023-05-01 14:33:50,"[colabdog.com] I build an aggregator for AI News that combines OpenAI, Google AI, MIT, BAIR.",colabDog,False,0.86,5,https://i.redd.it/rlizo23od8xa1.png,1,1682951630.0,
9,10ndm39,datascience,LLM,top,2023-01-28 12:08:28,Implementing GPTZero from scratch,BurhanUlTayyab,False,0.75,4,https://www.reddit.com/r/datascience/comments/10ndm39/implementing_gptzero_from_scratch/,0,1674907708.0,"We've gone through the original implementation of GPTZero and successfully reverse engineer it. (it gives the same results as original GPTZero). We've also recorded the implementation process which can be found below.

Youtube Implementation Video: [https://youtu.be/x9H-aY5sCDA](https://youtu.be/x9H-aY5sCDA)  
Github: [https://github.com/BurhanUlTayyab/GPTZero](https://github.com/BurhanUlTayyab/GPTZero)  
Website: [https://gptzero.sg](https://gptzero.sg)  
Discord: [https://discord.com/invite/F3kFan28vH](https://discord.com/invite/F3kFan28vH)

We're also working on a GPTZerov2 (inspired by LLM based transformers and GANs), which would be more accurate, and can detect lines changed by humans.  


Please give some feedback on our work.

Thanks"
10,13bnz10,datascience,LLM,top,2023-05-08 12:42:59,Ontology vs. LLM for Query Expansion (or both?),FlimsyYou6861,False,0.83,4,https://www.reddit.com/r/datascience/comments/13bnz10/ontology_vs_llm_for_query_expansion_or_both/,6,1683549779.0,"
I work at a recruitment agency where as a job seeker you can search for jobs and as a recruiter you can search for candidates in our candidate pool that might fit the job description. Currently both search engines are based on Elastic Search with some handling of synonyms, but we still have problems with showing all relevant search results if the search term doesn't fit the job description or CV (for example if some specific frontend framework is required for a job, a candidate with experience in a similar framework should still be shown in the results but with slightly lower relevancy.

Without much consideration for different approaches (because we don't have much NLP Expertise in the company and have a quite new data science department), we already experimented with building an ontology based on external ontologies and our own data (e.g. Python is used in Data Science) to find closely related terms and expand the search queries based on those relationships. While this approach seems to work somewhat, it feels kind of cumbersome, outdated and will probably need a lot of maintenance in the long run. For example using a prompt in GPT yielded very similar results in a matter of seconds, which raises the question if, for example, just using the embeddings of the search terms would already be enough to expand a users search query with additional relevant terms.

What approach would you suggest when dealing with the problem of query expansion? Or would a combination of both approaches make sense (e.g. using an LLM to automate building an ontology). Are ontologies regarding that use case outdated or am i just falling for the ChatGPT hype?

I would very much appreciate your insights!"
11,11vdjat,datascience,LLM,top,2023-03-19 06:27:43,datasetGPT - A command-line tool to generate datasets by inferencing LLMs at scale. It can even make two ChatGPT agents talk with one another.,radi-cho,False,0.8,3,https://www.reddit.com/r/datascience/comments/11vdjat/datasetgpt_a_commandline_tool_to_generate/,0,1679207263.0,"GitHub: [https://github.com/radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT)

It can generate texts by varying input parameters and using multiple backends. But, personally, the conversations dataset generation is my favorite: It can produce dialogues between two ChatGPT agents.

Possible use cases may include:

* Constructing textual corpora to train/fine-tune detectors for content written by AI.
* Collecting datasets of LLM-produced conversations for research purposes, analysis of AI performance/impact/ethics, etc.
* Automating a task that a LLM can handle over big amounts of input texts. For example, using GPT-3 to summarize 1000 paragraphs with a single CLI command.
* Leveraging APIs of especially big LLMs to produce diverse texts for a specific task and then fine-tune a smaller model with them.

What would you use it for?"
12,11yvv2h,datascience,LLM,top,2023-03-22 20:39:09,Fully AI generated data science mock interview,sang89,False,0.63,2,https://www.reddit.com/r/datascience/comments/11yvv2h/fully_ai_generated_data_science_mock_interview/,1,1679517549.0,"hey everyone, sharing this fun project around using LLMs to generate mock interviews. 

its not there yet, but trying to simulate a real case study interview as means to help with interview preparation and answering open case study questions  (and also keep up with LLM landscape).  at the least, hope its an engaging lunch-time read. what do you all think? 

i made it a free daily newsletter so please subscribe if you find it interesting. and please do share ideas to improve it.

newsletter- [https://open.substack.com/pub/sangy/p/online-shopping-behavior-prediction?r=1ecjtr&utm\_campaign=post&utm\_medium=web](https://open.substack.com/pub/sangy/p/online-shopping-behavior-prediction?r=1ecjtr&utm_campaign=post&utm_medium=web)

https://preview.redd.it/8djzffaxpcpa1.png?width=964&format=png&auto=webp&s=c4acbc3e7cf02b67ba9c199a0218cee4b9b188a3"
13,132cvf9,datascience,LLM,top,2023-04-28 23:16:15,New ChatGPT features and data science,jehan_gonzales,False,0.53,1,https://www.reddit.com/r/datascience/comments/132cvf9/new_chatgpt_features_and_data_science/,5,1682723775.0,"I'm wondering what impact updates to ChatGPT will have on data science and data scientists. 

I saw a TED talk from one of the OpenAI founders. You can see it here: https://youtu.be/C_78DM8fG6E

It's mind-blowing. He shows some new features that are coming soon to ChatGPT. He asks for a meal and uses the DALL-E integration to get an image of it. He even gets a shopping order put together on Instakart. He just needs to click and the food will be delivered to his house. 

The most impressive thing was the data analysis where he uploaded a CSV and the LLM figured out what the columns referred to on its own and then asked for plots and a prediction etc. The spat out Python code that he could dive into as well. 

It sounds like it could do data cleaning, preprocessing and modelling fairly easily. It would take some iterations, but if you have it the right direction, it would speed up the process 10x or more. 

I think this will basically simplify work for data scientists but will also enable ordinary folks with no quant backgrounds to do sophisticated analysis. 

I'm no longer a data scientist and work in product management. But if I were still in data science, I'd focus on my ability to help people self serve. I think the role will split to expert data scientists who build and productionise ML models and analytics enablers who help people get more out of the tools. 

What do you think? Is this a threat to data scientists? Or is it a productivity booster that will only make life better?"
14,11reoww,datascience,LLM,top,2023-03-14 18:46:51,(non neural net) Parameter fine tuning,mysterybasil,False,0.6,1,https://www.reddit.com/r/datascience/comments/11reoww/non_neural_net_parameter_fine_tuning/,3,1678819611.0,"Hi everyone, pretty new here, good to meet you.

In LLM's/neural networks there is a concept of parameter fine tuning - e.g., you start with the Bert model and then further train it on a more specific domain.

I'm wondering if the same idea has/can be applied to more standard ML techniques, such as random forest. The idea here is that you don't have the original data, you just have the model itself and want to fit it to the new data. Maybe the new data, is itself, insufficient for producing a strong model.

Thanks."
15,12coioi,datascience,LLM,top,2023-04-05 15:46:14,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.63,2,https://www.reddit.com/r/datascience/comments/12coioi/do_we_really_need_100b_parameters_in_a_large/,2,1680709574.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back!

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
16,11u1xb7,datascience,LLM,comments,2023-03-17 19:57:47,I hire for super senior data scientists (30+ years of experience). These are some question I ask (be prepared!).,purplebrown_updown,False,0.86,877,https://www.reddit.com/r/datascience/comments/11u1xb7/i_hire_for_super_senior_data_scientists_30_years/,227,1679083067.0,"First, I always ask facts about the Sun. How many miles is it from the Earth? Circumference? Mass, etc. Typical DS questions anyone should know. 

Next, I go into a deep discussion about harmonic means and whats the difference between + and -, multiplication and division. 

Third-of-ly, I go into specifics about garbage collection and null reference pointers in Python, since, as a DS expert, those will be super relevant and important.  

Last, but not least, need someone who not only knows Python and SQL, but also COBALT and BASIC. 

To give some context, I work in the field of screwing in light bulbs. So we definitely want someone who knows NLP, LLM, CV, CNNs, random forests regression, mixed integer programming, optimization, etc. 

I would love to hear your thoughts. Good luck!

..."
17,137gt1i,datascience,LLM,comments,2023-05-04 10:40:20,"""Experienced in GenAI""!? Let me guess 5+ years of experience?",MorningDarkMountain,False,0.85,187,https://i.redd.it/9wiziprb4uxa1.jpg,77,1683196820.0,
18,10ert4y,datascience,LLM,comments,2023-01-17 23:53:49,Would you buy new MBP M2 Max 96GB (V)RAM to run LLM inference?,eugenehp,False,0.4,0,https://i.redd.it/niebhlucgqca1.jpg,50,1673999629.0,
19,10nk3pf,datascience,LLM,comments,2023-01-28 17:13:48,will openAI make data scientists obsolete?,fabzo100,False,0.14,0,https://www.reddit.com/r/datascience/comments/10nk3pf/will_openai_make_data_scientists_obsolete/,40,1674926028.0,"I started to think that openAI (or other advanced AI tech from other big tech) may eventually make data scientists obsolete faster before any other job. Here's why.

First of all, machine learning is a bit useless if everybody just leverage openAI (or Google) LLM. I mean why would companies ever need any other AI if they can all just pay google or microsoft to do the AI-related jobs for them?

And even when it comes to data reasoning, AI will be able to do that job much faster as compared to data scientists. If you check azure openAI landing page, they literally mentioned code generation and data reasoning as their selling points.

Thoughts?"
20,13ai0wp,datascience,LLM,comments,2023-05-07 08:54:50,What are your thoughts on the LLM fever going on right now?,Samirio,False,0.78,10,https://www.reddit.com/r/datascience/comments/13ai0wp/what_are_your_thoughts_on_the_llm_fever_going_on/,36,1683449690.0,"The hype is strong with this one, but do you think it is justified? 

Do you think that this will actually change our day to day in ways other than using chatgpt or other LLMs as personal assistants?

I look around and see people left and right reaching out for creating applications using LLMs, but so far I didn’t see anything other than feeding documents to an LLM and having it summarize them, which doesn’t seem that ground breaking to me.

What are your thoughts on this topic?

Edit: I understand that I have over simplified my view of LLMs just “summarizing” text, when instead I should be asking something like, do you think LLMs are being effective now in the way they are being hyped for?"
21,114zcik,datascience,LLM,comments,2023-02-17 22:19:56,"What is something ChatGPT (or any LLM) could do, that it can’t currently, that would actually worry you about the future of data science?",cjrook,False,0.43,0,https://www.reddit.com/r/datascience/comments/114zcik/what_is_something_chatgpt_or_any_llm_could_do/,26,1676672396.0,"Lately on this sub there have been many “sky is falling” posts related to ChatGPT. Most of the posts have drastically overestimated ChatGPT’s current use cases in the industry. What is a capability that if ChatGPT could do it, you would actually worry about the future of the data science field? More specifically worried about mass job loss within the field, if you foresee that."
22,11b5xb2,datascience,LLM,comments,2023-02-24 23:46:48,Is there any self hosting LLM option that offers GPT3 level of performance?,roylv22,False,0.86,31,https://www.reddit.com/r/datascience/comments/11b5xb2/is_there_any_self_hosting_llm_option_that_offers/,25,1677282408.0,"GPT3 is great but not cheap, I also have privacy concerns using their API. Is there any LLM offers similar performance but allows self hosting?"
23,10a1mik,datascience,LLM,comments,2023-01-12 14:59:04,New Research From Google Shines Light On The Future Of Language Models ⭕,LesleyFair,False,0.84,122,https://www.reddit.com/r/datascience/comments/10a1mik/new_research_from_google_shines_light_on_the/,18,1673535544.0,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs will allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* Stop hallucinating and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example are words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/w7xffqjimmba1.png?width=800&format=png&auto=webp&s=e2d9cb63f750efcbfa45c4bb7a985d4dcb5b0319)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.  
At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week.  
No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)"
24,10fw1a3,datascience,LLM,comments,2023-01-19 07:54:24,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.94,119,https://www.reddit.com/r/datascience/comments/10fw1a3/gpt4_will_be_500x_smaller_than_people_think_here/,14,1674114864.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/t6m0epzlgyca1.png?width=575&format=png&auto=webp&s=a5972941053f833e76fd3a5009fc68a61f9e5406)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
25,1289p4d,datascience,LLM,comments,2023-04-01 03:24:17,Do you think NLP will increase with LLM models?,Muted_Standard175,False,0.86,14,https://www.reddit.com/r/datascience/comments/1289p4d/do_you_think_nlp_will_increase_with_llm_models/,7,1680319457.0,"I am thinking in studying this, some say NLP will decrease as GPT can beat most of NLP tasks in a low cost. What do you say?"
26,13dld9y,datascience,LLM,comments,2023-05-10 09:29:46,LLM to analyze earning reports ?,Lobbel1992,False,0.43,0,https://www.reddit.com/r/datascience/comments/13dld9y/llm_to_analyze_earning_reports/,7,1683710986.0,"Hi,
I am interested to know if their are open source LLM that can analyze an earnings report.

I love to analyze stock but reading multiple reports is time consuming.

Do you have any advice/tips to analyze reports faster ?

T.I.A."
27,13bnz10,datascience,LLM,comments,2023-05-08 12:42:59,Ontology vs. LLM for Query Expansion (or both?),FlimsyYou6861,False,0.72,3,https://www.reddit.com/r/datascience/comments/13bnz10/ontology_vs_llm_for_query_expansion_or_both/,6,1683549779.0,"
I work at a recruitment agency where as a job seeker you can search for jobs and as a recruiter you can search for candidates in our candidate pool that might fit the job description. Currently both search engines are based on Elastic Search with some handling of synonyms, but we still have problems with showing all relevant search results if the search term doesn't fit the job description or CV (for example if some specific frontend framework is required for a job, a candidate with experience in a similar framework should still be shown in the results but with slightly lower relevancy.

Without much consideration for different approaches (because we don't have much NLP Expertise in the company and have a quite new data science department), we already experimented with building an ontology based on external ontologies and our own data (e.g. Python is used in Data Science) to find closely related terms and expand the search queries based on those relationships. While this approach seems to work somewhat, it feels kind of cumbersome, outdated and will probably need a lot of maintenance in the long run. For example using a prompt in GPT yielded very similar results in a matter of seconds, which raises the question if, for example, just using the embeddings of the search terms would already be enough to expand a users search query with additional relevant terms.

What approach would you suggest when dealing with the problem of query expansion? Or would a combination of both approaches make sense (e.g. using an LLM to automate building an ontology). Are ontologies regarding that use case outdated or am i just falling for the ChatGPT hype?

I would very much appreciate your insights!"
28,13htnet,datascience,LLM,comments,2023-05-15 01:32:07,Reverse engineer credit score algorithms with LLM + Code Interpreter,worldprowler,False,0.17,0,https://www.reddit.com/r/datascience/comments/13htnet/reverse_engineer_credit_score_algorithms_with_llm/,5,1684114327.0,"Could you take time series data log of changes in credit score and related transactions for some amount of users, hand it of to an LLM + Code Interpreter engine and reverse engineer the algorithm for credit scores ?"
29,132cvf9,datascience,LLM,comments,2023-04-28 23:16:15,New ChatGPT features and data science,jehan_gonzales,False,0.57,2,https://www.reddit.com/r/datascience/comments/132cvf9/new_chatgpt_features_and_data_science/,5,1682723775.0,"I'm wondering what impact updates to ChatGPT will have on data science and data scientists. 

I saw a TED talk from one of the OpenAI founders. You can see it here: https://youtu.be/C_78DM8fG6E

It's mind-blowing. He shows some new features that are coming soon to ChatGPT. He asks for a meal and uses the DALL-E integration to get an image of it. He even gets a shopping order put together on Instakart. He just needs to click and the food will be delivered to his house. 

The most impressive thing was the data analysis where he uploaded a CSV and the LLM figured out what the columns referred to on its own and then asked for plots and a prediction etc. The spat out Python code that he could dive into as well. 

It sounds like it could do data cleaning, preprocessing and modelling fairly easily. It would take some iterations, but if you have it the right direction, it would speed up the process 10x or more. 

I think this will basically simplify work for data scientists but will also enable ordinary folks with no quant backgrounds to do sophisticated analysis. 

I'm no longer a data scientist and work in product management. But if I were still in data science, I'd focus on my ability to help people self serve. I think the role will split to expert data scientists who build and productionise ML models and analytics enablers who help people get more out of the tools. 

What do you think? Is this a threat to data scientists? Or is it a productivity booster that will only make life better?"
30,11reoww,datascience,LLM,comments,2023-03-14 18:46:51,(non neural net) Parameter fine tuning,mysterybasil,False,0.6,1,https://www.reddit.com/r/datascience/comments/11reoww/non_neural_net_parameter_fine_tuning/,3,1678819611.0,"Hi everyone, pretty new here, good to meet you.

In LLM's/neural networks there is a concept of parameter fine tuning - e.g., you start with the Bert model and then further train it on a more specific domain.

I'm wondering if the same idea has/can be applied to more standard ML techniques, such as random forest. The idea here is that you don't have the original data, you just have the model itself and want to fit it to the new data. Maybe the new data, is itself, insufficient for producing a strong model.

Thanks."
31,12coioi,datascience,LLM,comments,2023-04-05 15:46:14,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.6,2,https://www.reddit.com/r/datascience/comments/12coioi/do_we_really_need_100b_parameters_in_a_large/,2,1680709574.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back!

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
32,134o5fe,datascience,LLM,comments,2023-05-01 14:33:50,"[colabdog.com] I build an aggregator for AI News that combines OpenAI, Google AI, MIT, BAIR.",colabDog,False,0.86,5,https://i.redd.it/rlizo23od8xa1.png,1,1682951630.0,
33,10i6kg4,datascience,LLM,comments,2023-01-22 00:44:58,Large Language Model,ashishtele,False,0.14,0,https://www.reddit.com/r/datascience/comments/10i6kg4/large_language_model/,1,1674348298.0,"Hi,

I am looking for a course to learn the Large Language Model. Please provide some sources.

\#LLM #ChatGPT"
34,13ai0wp,datascience,LLM,relevance,2023-05-07 08:54:50,What are your thoughts on the LLM fever going on right now?,Samirio,False,0.75,8,https://www.reddit.com/r/datascience/comments/13ai0wp/what_are_your_thoughts_on_the_llm_fever_going_on/,36,1683449690.0,"The hype is strong with this one, but do you think it is justified? 

Do you think that this will actually change our day to day in ways other than using chatgpt or other LLMs as personal assistants?

I look around and see people left and right reaching out for creating applications using LLMs, but so far I didn’t see anything other than feeding documents to an LLM and having it summarize them, which doesn’t seem that ground breaking to me.

What are your thoughts on this topic?

Edit: I understand that I have over simplified my view of LLMs just “summarizing” text, when instead I should be asking something like, do you think LLMs are being effective now in the way they are being hyped for?"
35,13dld9y,datascience,LLM,relevance,2023-05-10 09:29:46,LLM to analyze earning reports ?,Lobbel1992,False,0.43,0,https://www.reddit.com/r/datascience/comments/13dld9y/llm_to_analyze_earning_reports/,7,1683710986.0,"Hi,
I am interested to know if their are open source LLM that can analyze an earnings report.

I love to analyze stock but reading multiple reports is time consuming.

Do you have any advice/tips to analyze reports faster ?

T.I.A."
36,10ert4y,datascience,LLM,relevance,2023-01-17 23:53:49,Would you buy new MBP M2 Max 96GB (V)RAM to run LLM inference?,eugenehp,False,0.41,0,https://i.redd.it/niebhlucgqca1.jpg,50,1673999629.0,
37,11b5xb2,datascience,LLM,relevance,2023-02-24 23:46:48,Is there any self hosting LLM option that offers GPT3 level of performance?,roylv22,False,0.84,28,https://www.reddit.com/r/datascience/comments/11b5xb2/is_there_any_self_hosting_llm_option_that_offers/,25,1677282408.0,"GPT3 is great but not cheap, I also have privacy concerns using their API. Is there any LLM offers similar performance but allows self hosting?"
38,13bnz10,datascience,LLM,relevance,2023-05-08 12:42:59,Ontology vs. LLM for Query Expansion (or both?),FlimsyYou6861,False,0.83,4,https://www.reddit.com/r/datascience/comments/13bnz10/ontology_vs_llm_for_query_expansion_or_both/,6,1683549779.0,"
I work at a recruitment agency where as a job seeker you can search for jobs and as a recruiter you can search for candidates in our candidate pool that might fit the job description. Currently both search engines are based on Elastic Search with some handling of synonyms, but we still have problems with showing all relevant search results if the search term doesn't fit the job description or CV (for example if some specific frontend framework is required for a job, a candidate with experience in a similar framework should still be shown in the results but with slightly lower relevancy.

Without much consideration for different approaches (because we don't have much NLP Expertise in the company and have a quite new data science department), we already experimented with building an ontology based on external ontologies and our own data (e.g. Python is used in Data Science) to find closely related terms and expand the search queries based on those relationships. While this approach seems to work somewhat, it feels kind of cumbersome, outdated and will probably need a lot of maintenance in the long run. For example using a prompt in GPT yielded very similar results in a matter of seconds, which raises the question if, for example, just using the embeddings of the search terms would already be enough to expand a users search query with additional relevant terms.

What approach would you suggest when dealing with the problem of query expansion? Or would a combination of both approaches make sense (e.g. using an LLM to automate building an ontology). Are ontologies regarding that use case outdated or am i just falling for the ChatGPT hype?

I would very much appreciate your insights!"
39,1289p4d,datascience,LLM,relevance,2023-04-01 03:24:17,Do you think NLP will increase with LLM models?,Muted_Standard175,False,0.81,12,https://www.reddit.com/r/datascience/comments/1289p4d/do_you_think_nlp_will_increase_with_llm_models/,7,1680319457.0,"I am thinking in studying this, some say NLP will decrease as GPT can beat most of NLP tasks in a low cost. What do you say?"
40,13htnet,datascience,LLM,relevance,2023-05-15 01:32:07,Reverse engineer credit score algorithms with LLM + Code Interpreter,worldprowler,False,0.14,0,https://www.reddit.com/r/datascience/comments/13htnet/reverse_engineer_credit_score_algorithms_with_llm/,5,1684114327.0,"Could you take time series data log of changes in credit score and related transactions for some amount of users, hand it of to an LLM + Code Interpreter engine and reverse engineer the algorithm for credit scores ?"
41,zkii2l,datascience,LLM,relevance,2022-12-13 01:37:48,HellaSwag or HellaBad? 36% of this popular LLM benchmark contains errors,maximumpineapple27,False,0.79,11,https://www.reddit.com/r/datascience/comments/zkii2l/hellaswag_or_hellabad_36_of_this_popular_llm/,1,1670895468.0,"Continuing a previous blog post analyzing errors in popular LLM benchmarks (post on Google’s GoEmotions [here](https://www.reddit.com/r/MachineLearning/comments/vye69k/30_of_googles_reddit_emotions_dataset_is/)) — I analyzed HellaSwag and found 36% contains errors.

  
For example, here’s a prompt and set of possible completions from the dataset. Which completion do you think is most appropriate? See if you can figure it out through the haze of typos and generally non-sensical writing.

  
*Men are standing in a large green field playing lacrosse. People* *is* *around the field watching the game. men*

* *are holding tshirts watching* *int* *lacrosse playing.*
* *are being interviewed in a podium in front of a large group and a gymnast is holding a microphone for the announcers.*
* *are running side to side* *of* *the* *ield* *playing lacrosse trying to score.*
* *are in a field running around playing lacrosse.*

I’ll keep it spoiler-free here, but the full blog post goes into detail on this example (and others) and explains why they are so problematic.

  
Link: [https://www.surgehq.ai/blog/hellaswag-or-hellabad-36-of-this-popular-llm-benchmark-contains-errors](https://www.surgehq.ai/blog/hellaswag-or-hellabad-36-of-this-popular-llm-benchmark-contains-errors)"
42,114zcik,datascience,LLM,relevance,2023-02-17 22:19:56,"What is something ChatGPT (or any LLM) could do, that it can’t currently, that would actually worry you about the future of data science?",cjrook,False,0.44,0,https://www.reddit.com/r/datascience/comments/114zcik/what_is_something_chatgpt_or_any_llm_could_do/,26,1676672396.0,"Lately on this sub there have been many “sky is falling” posts related to ChatGPT. Most of the posts have drastically overestimated ChatGPT’s current use cases in the industry. What is a capability that if ChatGPT could do it, you would actually worry about the future of the data science field? More specifically worried about mass job loss within the field, if you foresee that."
43,11u1xb7,datascience,LLM,relevance,2023-03-17 19:57:47,I hire for super senior data scientists (30+ years of experience). These are some question I ask (be prepared!).,purplebrown_updown,False,0.86,880,https://www.reddit.com/r/datascience/comments/11u1xb7/i_hire_for_super_senior_data_scientists_30_years/,227,1679083067.0,"First, I always ask facts about the Sun. How many miles is it from the Earth? Circumference? Mass, etc. Typical DS questions anyone should know. 

Next, I go into a deep discussion about harmonic means and whats the difference between + and -, multiplication and division. 

Third-of-ly, I go into specifics about garbage collection and null reference pointers in Python, since, as a DS expert, those will be super relevant and important.  

Last, but not least, need someone who not only knows Python and SQL, but also COBALT and BASIC. 

To give some context, I work in the field of screwing in light bulbs. So we definitely want someone who knows NLP, LLM, CV, CNNs, random forests regression, mixed integer programming, optimization, etc. 

I would love to hear your thoughts. Good luck!

..."
44,11vdjat,datascience,LLM,relevance,2023-03-19 06:27:43,datasetGPT - A command-line tool to generate datasets by inferencing LLMs at scale. It can even make two ChatGPT agents talk with one another.,radi-cho,False,0.8,3,https://www.reddit.com/r/datascience/comments/11vdjat/datasetgpt_a_commandline_tool_to_generate/,0,1679207263.0,"GitHub: [https://github.com/radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT)

It can generate texts by varying input parameters and using multiple backends. But, personally, the conversations dataset generation is my favorite: It can produce dialogues between two ChatGPT agents.

Possible use cases may include:

* Constructing textual corpora to train/fine-tune detectors for content written by AI.
* Collecting datasets of LLM-produced conversations for research purposes, analysis of AI performance/impact/ethics, etc.
* Automating a task that a LLM can handle over big amounts of input texts. For example, using GPT-3 to summarize 1000 paragraphs with a single CLI command.
* Leveraging APIs of especially big LLMs to produce diverse texts for a specific task and then fine-tune a smaller model with them.

What would you use it for?"
45,10nk3pf,datascience,LLM,relevance,2023-01-28 17:13:48,will openAI make data scientists obsolete?,fabzo100,False,0.14,0,https://www.reddit.com/r/datascience/comments/10nk3pf/will_openai_make_data_scientists_obsolete/,40,1674926028.0,"I started to think that openAI (or other advanced AI tech from other big tech) may eventually make data scientists obsolete faster before any other job. Here's why.

First of all, machine learning is a bit useless if everybody just leverage openAI (or Google) LLM. I mean why would companies ever need any other AI if they can all just pay google or microsoft to do the AI-related jobs for them?

And even when it comes to data reasoning, AI will be able to do that job much faster as compared to data scientists. If you check azure openAI landing page, they literally mentioned code generation and data reasoning as their selling points.

Thoughts?"
46,137gt1i,datascience,LLM,relevance,2023-05-04 10:40:20,"""Experienced in GenAI""!? Let me guess 5+ years of experience?",MorningDarkMountain,False,0.85,192,https://i.redd.it/9wiziprb4uxa1.jpg,77,1683196820.0,
