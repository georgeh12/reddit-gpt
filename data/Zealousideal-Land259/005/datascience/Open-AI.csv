,id,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,url,num_comments,created,body
0,13cpckb,datascience,Open-AI,top,2023-05-09 13:07:55,PSA: You don't need fancy stuff to do good work.,Bitwise_Gamgee,False,0.88,368,https://www.reddit.com/r/datascience/comments/13cpckb/psa_you_dont_need_fancy_stuff_to_do_good_work/,63,1683637675.0,"I've been reading a lot of posts on r/datascience and several seem to orbit the subject of how to use the latest tool or tweak, I understand that it can be easy to get caught up in the whirlwind of tools, frameworks, and cutting-edge technologies. While these advancements can undoubtedly enhance our work, it's important to remember that data science isn't about using the most advanced or expensive tools; it's about extracting valuable insights from data to drive informed decision-making.

Data Collection and Categorization

Before diving into advanced machine learning algorithms or statistical models, we need to start with the basics: collecting and organizing data. Fortunately, both Python and R offer a wealth of libraries that make it easy to collect data from a variety of sources, including web scraping, APIs, and reading from files. Key libraries in Python include [requests](https://requests.readthedocs.io/en/latest/), [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/), and [pandas](https://pandas.pydata.org/), while R has [httr](https://cran.r-project.org/web/packages/httr/index.html), [rvest](https://rvest.tidyverse.org/), and [dplyr](https://dplyr.tidyverse.org/).

These libraries not only make it easy to collect data but also to clean and structure it for analysis. With just a few lines of code, you can filter, sort, and transform data into a format that's ready for exploration and modeling.

Data Analysis and Visualization

Once your data is collected and organized, the next step is to analyze and visualize it. Both Python and R excel in this area, providing a wide range of libraries and packages for exploratory data analysis and visualization.

Python's pandas, [NumPy](https://numpy.org/), and [SciPy](https://scipy.org/) libraries offer powerful functionality for data manipulation, while [matplotlib](https://matplotlib.org/), [seaborn](https://seaborn.pydata.org/), and [plotly](https://plotly.com/) provide versatile tools for creating visualizations. Similarly, in R, you can use dplyr, [tidyverse](https://www.tidyverse.org/), and [data.table](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html) for data manipulation, and [ggplot2](https://ggplot2.tidyverse.org/), [lattice](https://cran.r-project.org/web/packages/lattice/index.html), and [shiny](https://shiny.rstudio.com/) for visualization. These packages enable you to create insightful visualizations and perform statistical analyses without relying on expensive or proprietary software.

Modeling and Prediction

Finally, when it comes to building models and making predictions, Python and R have a plethora of options available. Libraries like [scikit-learn](https://scikit-learn.org), [statsmodels](https://www.statsmodels.org/stable/index.html), and [TensorFlow](https://www.tensorflow.org/)in Python, or [caret](https://topepo.github.io/caret/), [randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf), and [xgboost](https://xgboost.readthedocs.io/en/stable/)in R, provide powerful machine learning algorithms and statistical models that can be applied to a wide range of problems. What's more, these libraries are open-source and have extensive documentation and community support, making it easy to learn and apply new techniques without needing specialized training or expensive software licenses.

Simplicity is key, embrace it and you'll learn a lot faster than trying to glean insights from some poorly trained AI model.

&#x200B;

ps. Any ""IDE"" more extensive than VIM/EMACS/~~nano~~ are unnecessary :)"
1,10fw1a3,datascience,Open-AI,top,2023-01-19 07:54:24,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.94,125,https://www.reddit.com/r/datascience/comments/10fw1a3/gpt4_will_be_500x_smaller_than_people_think_here/,14,1674114864.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/t6m0epzlgyca1.png?width=575&format=png&auto=webp&s=a5972941053f833e76fd3a5009fc68a61f9e5406)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ‚≠ï, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
2,1000yz9,datascience,Open-AI,top,2022-12-31 18:55:53,swe vs ds,Impossible-Ask4646,False,0.88,86,https://www.reddit.com/r/datascience/comments/1000yz9/swe_vs_ds/,116,1672512953.0,"I'm a 29yr old dairy farm manager in Colorado, being paid well (+- 150k/yr) for working extremely long hours on the farm managing people. For the past 5 years I've been locked into this job with a workvisa, but I got my greencard approved a couple weeks ago and finally have some more freedom and am looking into making a complete career switch.

I don't have the best people skills (although it improved managing 20+ employees for 5 years), but have good technical and math skills. I grew up in Belgium where every year in high school I made it to the national Math Olympics final. I got a Bachelor of Science degree in Bioscience Engineering and a Masters of Science degree in Management, Economics and Consumer Sciences. I always felt I was learning things faster than others, was always best in class, but spent the majority of my time helping my parents on their farm until I moved to the US.

While managing this dairy in the US, I did a lot of little things on the side.

* I played around with some crypto, was arbitraging bets on the US elections on different crypto betting websites and protocols (eg. receiving odds of 1.9x for Biden to win, while receiving odds above 3x for Trump to win election)
* Buying and selling large amounts of crypto for cash for a 10-15% mark-up
* Buying bitcoin miners from China after their crypto ban and selling them locally for a profit
* I saw publicly traded bitcoin mining companies were way overvalued, but shorting them is risky since it's hard to predict what will happen to the bitcoin-price so I started to run efficient bitcoin miners in a facility with cheap electricity, while shorting stocks like RIOT to eliminate the risk of the bitcoinprice going up. I made a copy of a % of RIOT for a 10th of what their stock was worth and shorted them at the same time.
* Buying SPY at the stock market while shorting mSPY (mirrored SPY) on mirror protocol (DeFi - Decentralized Finance) with aUST (acnhored UST) as collateral, leveraging this up many times to get yields around +100% APY on USD (by taking insurance for a UST-depeg through Unslashed (who did pay us out through a Kleros-court case). I lost 300k $ on this after making 600k $ with it because of SPY pricing jumping up by 4% to come back down 4% a bleep of a second afterwards on the actual stock market (dark pool after hours). [see here](https://forum.mirror.finance/t/liquidations-caused-by-unrepresentative-oracle-pricing-of-mspy-on-jan-3-2022/2569)

All of this together made some good amount of money, but right now I'm trying to figure out what to do with our future. The biggest reason I want to quit my current job is that I have a wife and 3 little kids who I don't see enough. I want to spend more time with them, but it's not working out in my current position. I also feel like I want to use my technical/logical/math skills more, but after all this time it's hard to figure out what to do exactly and how to even start on getting there. 

We are thinking of either:

* Running our own small business, but we can't seem to figure out what exactly.
* Software Engineering
* Data Scientist/AI/ML
* Other managerial jobs I could get, although I don't think I ""love"" managing people
* ...

&#x200B;

I'm open to any advice, on positions, on who to talk to, on which path to take. Thanks in advance!"
3,11is6oq,datascience,Open-AI,top,2023-03-05 08:48:09,Beating OpenAI CLIP in Image retrieval with 100x less data and compute,vov_or,False,0.92,83,https://www.reddit.com/r/datascience/comments/11is6oq/beating_openai_clip_in_image_retrieval_with_100x/,7,1678006089.0,"Hello from the Unum AI team! We have been silently pre-training numerous Multi-Modal Models for Semantic Search for the last year!  
We are releasing several extremely performant checkpoints on the [HuggingFace portal](https://huggingface.co/unum-cloud/uform)!  
In addition there is the blog post about efficient Vision-Language pre-training:  
[https://www.unum.cloud/blog/2023-02-20-efficient-multimodality](https://www.unum.cloud/blog/2023-02-20-efficient-multimodality)"
4,zcgpbp,datascience,Open-AI,top,2022-12-04 18:05:07,What do you guys think of OpenAI‚Äôs ChatGPT?,Loud_Ad_6272,False,0.95,56,https://www.reddit.com/r/datascience/comments/zcgpbp/what_do_you_guys_think_of_openais_chatgpt/,77,1670177107.0,"As the title goes, what do you guys think of this and what effect do you think it would have on the discipline and field going forward?"
5,ykybpj,datascience,Open-AI,top,2022-11-03 10:17:29,Testing OpenAI GPT3 in Airtable. Fine-tuning gpt3 even with a very small dataset (50 in this case) appears to work pretty well. Any interesting use cases that you'd recommend testing with?,igornefedovi,False,0.91,47,https://twitter.com/igornefedovi/status/1588032734315704320,0,1667470649.0,
6,126ndlu,datascience,Open-AI,top,2023-03-30 13:41:51,Seeing a lot of job openings for high-level AI and Data Analytics positions...,fingin,False,0.82,29,https://www.reddit.com/r/datascience/comments/126ndlu/seeing_a_lot_of_job_openings_for_highlevel_ai_and/,20,1680183711.0,"I have noticed an uptick in jobs for things like prompt engineer, AI ethics lead, AI manager. When you look at these requirements it looks like relatively low entry: a familiarity with general AI and AI regulations (not that there is a ton of expertise to be had in this latter category). They don't require much or any technical skill. 

I'll admit, I find myself frustrated as I work in a highly technical role and feel like these opportunities are really 'low hanging fruit', due to the vagueness of the requirements. I'm sure many of us wear not just technical hats but also those of product management, coaching and training, etc. 

What do you think? Is it just a fad stemming from ChatGPT and Image gen promotion? Are you going to make a job switch and apply for these roles?"
7,zixqgn,datascience,Open-AI,top,2022-12-11 16:06:53,Personal project for PhDs and scientists,Cyalas,False,0.73,18,https://www.reddit.com/r/datascience/comments/zixqgn/personal_project_for_phds_and_scientists/,10,1670774813.0," Hello!

I've developed a project [NaimAI](https://www.naimai.fr/), to help PhDs and scientists in their scientific literaure review. To describe it brievely, it has 3 main features : 1 search in papers, 2 structures abstracts into objectives, methods and results and 3 generates automatically a (pseudo) literature review.

I wrote a [medium article](https://medium.com/@yaassinekaddi/literature-review-with-naimai-open-sourced-fcbdb36762de) that goes through the details.

Github repos : [https://github.com/yassinekdi/naimai](https://github.com/yassinekdi/naimai)

I've created a subreddit in case : [r/naimai4science](https://www.reddit.com/r/naimai4science/)

I'd be happy to have your opinion about it and hopefully this could be useful!"
8,10mi1x8,datascience,Open-AI,top,2023-01-27 10:52:18,‚≠ï What People Are Missing About Microsoft‚Äôs $10B Investment In OpenAI,LesleyFair,False,0.74,17,https://www.reddit.com/r/datascience/comments/10mi1x8/what_people_are_missing_about_microsofts_10b/,3,1674816738.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/35vtxrwnekea1.png?width=720&format=png&auto=webp&s=a61dd557e1d00c96448c429c9f9bb78516205a6f)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI‚Äôs models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let‚Äôs first look at Sam Altman‚Äôs backstory.

*Let‚Äôs go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI ‚Äî one of the most exciting and impactful organizations in all of tech.

However, OpenAI ‚Äî the rocket ship of AI innovation ‚Äî is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn‚Äôt take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can‚Äôt they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT‚Äôs outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That‚Äôs gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI‚Äôs profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI‚Äôs API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don‚Äôt have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman‚Äôs foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft‚Äôs sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI ‚Äî whatever that looks like ‚Äî the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ‚≠ï.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)‚Äã

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)‚Äã

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)‚Äã

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)‚Äã

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)‚Äã

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
9,zcmlp0,datascience,Open-AI,top,2022-12-04 21:39:24,Unofficial Python SDK for OpenAI's ChatGPT,brunneis,False,0.82,13,https://github.com/labteral/chatgpt-python,1,1670189964.0,
10,12jb54e,datascience,Open-AI,top,2023-04-12 05:21:27,Is OpenAI‚Äôs Study On The Labor Market Impacts Of AI Flawed?,LesleyFair,False,0.75,8,https://www.reddit.com/r/datascience/comments/12jb54e/is_openais_study_on_the_labor_market_impacts_of/,0,1681276887.0,"[Example img\_name](https://preview.redd.it/wzz3wtwu1eta1.png?width=1451&format=png&auto=webp&s=9a10cc08b28effc9cbda57b43d625bfcc5c03be2)

We all have heard an uncountable amount of predictions about how AI will¬†***terk err jerbs!***

However, here we have a proper study on the topic from OpenAI and the University of Pennsylvania. They investigate how Generative Pre-trained Transformers (GPTs) could automate tasks across different occupations \[1\].

Although I‚Äôm going to discuss how the study comes with a set of ‚Äúimperfections‚Äù, the findings still make me really excited. The findings suggest that machine learning is going to deliver some serious productivity gains.

People in the data science world fought tooth and nail for years to squeeze some value out of incomplete data sets from scattered sources while hand-holding people on their way toward a data-driven organization. At the same time, the media was flooded with predictions of omniscient AI right around the corner.

*Let‚Äôs dive in and take an*¬†exciting glimpse into the future of labor markets\*!\*

# What They Did

The study looks at all US occupations. It breaks them down into tasks and assesses the possible level of for each task. They use that to estimate how much automation is possible for a given occupation.

The researchers used the¬†[O\*NET database,](https://www.onetcenter.org/database.html)¬†which is an occupation database specifically for the U.S. market. It lists 1,016 occupations along with its standardized descriptions of tasks.

The researchers annotated each task once manually and once using GPT-4. Thereby, each task was labeled as either somewhat (<50%) or significantly (>50%) automatable through LLMs. In their judgment, they considered both the direct ‚Äúexposure‚Äù of a task to GPT as well as to a secondary GPT-powered system, e.g. LLMs integrated with image generation systems.

To reiterate, a higher ‚Äúexposure‚Äù means that an occupation is more likely to get automated.

Lastly, they enriched the occupation data with wages and demographic information. This was used to determine whether e. g. high or low-paying jobs are at higher risk to be automated.

So far so good. This all sounds pretty decent. Sure, there is a lot of qualitative judgment going into their data acquisition process. However, we gotta cut them some slag. These kinds of studies always struggle to get any hard data and so far they did a good job.

However, there are a few obvious things to criticize. But before we get to that let‚Äôs look at their results.

# Key Findings

The study finds that 80% of the US workforce, across all industries, could have at least some tasks affected. Even more significantly, 19% of occupations are expected to have at least half of their tasks significantly automated!

Furthermore, they find that higher levels of automation exposure are associated with:

* Programming and writing skills
* Higher wages (contrary to previous research!)
* Higher levels of education (Bachelor‚Äôs and up)

Lower levels of exposure are associated with:

* Science and critical thinking skills
* Manual work and tasks that might potentially be done using physical robots

This is somewhat unsurprising. We of course know that LLMs will likely not increase productivity in the plumbing business. However, their findings underline again how different this wave is. In the past, simple and repetitive tasks fell prey to automation.

*This time it‚Äôs the suits!*

If we took this study at face value, many of us could start thinking about life as full-time pensioners.

But not so fast! This, like all the other studies on the topic, has a number of flaws.

# Necessary Criticism

First, let‚Äôs address the elephant in the room!

OpenAI co-authored the study. They have a vested interest in the hype around AI, both for commercial and regulatory reasons. Even if the external researchers performed their work with the utmost thoroughness and integrity, which I am sure they did, the involvement of OpenAI could have introduced an unconscious bias.

*But there‚Äôs more!*

The occupation database contains over 1000 occupations broken down into tasks. Neither GPT-4 nor the human labelers can possibly have a complete understanding of all the tasks across all occupations. Hence, their judgment about how much a certain task can be automated has to be rather hand-wavy in many cases.

Flaws in the data also arise from the GPT-based labeling itself.

The internet is flooded with countless sensationalist articles about how AI will replace jobs. It is hard to gauge whether this actually causes GPT models to be more optimistic when it comes to their own impact on society. However, it is possible and should not be neglected.

The authors do also not really distinguish between labor-augmenting and labor-displacing effects and it is hard to know what ‚Äúaffected by‚Äù or ‚Äúexposed to LLMs‚Äù actually means. Will people be replaced or will they just be able to do more?

Last but not least, lists of tasks most likely do not capture all requirements in a given occupation. For instance ""making someone feel cared for"" can be an essential part of a job but might be neglected in such a list.

# Take-Away And Implications

GPT models have the world in a frenzy - rightfully so.

Nobody knows whether 19% of knowledge work gets heavily automated or if it is only 10%.

As the dust settles, we will begin to see how the ecosystem develops and how productivity in different industries can be increased. Time will tell whether foundational LLMs, specialized smaller models, or vertical tools built on top of APIs will be having the biggest impact.

In any case, these technologies have the potential to create unimaginable value for the world. At the same time, change rarely happens without pain. I strongly believe in human ingenuity and our ability to adapt to change. All in all, the study - flaws aside - represents an honest attempt at gauging the future.

Efforts like this and their scrutiny are our best shot at navigating the future. Well, or we all get chased out of the city by pitchforks.

Jokes aside!

What an exciting time for science and humanity!

As always, I really enjoyed making this for you and I sincerely hope you found value in it!

If you are not subscribed to the newsletter yet,¬†[click here to sign up](https://thedecoding.net/)! I send out a thoughtful 5-minute email every week to keep you in the loop about machine learning research and the data economy.

*Thank you for reading and I see you next week ‚≠ï!*

**References:**

\[1\]¬†[https://arxiv.org/abs/2303.10130](https://arxiv.org/abs/2303.10130)"
11,11gbjgm,datascience,Open-AI,top,2023-03-02 19:30:12,ActiveLab: Active Learning with Data Re-Labeling,cmauck10,False,0.88,6,https://www.reddit.com/r/datascience/comments/11gbjgm/activelab_active_learning_with_data_relabeling/,3,1677785412.0,"I‚Äôm excited to share **ActiveLab**, a better algorithm for practical active learning.

https://preview.redd.it/j2payaxlndla1.png?width=1544&format=png&auto=webp&s=04bbeeb05a717602c4be9d97847b56747bcfec91

We recently published a [paper](https://arxiv.org/abs/2301.11856) introducing this novel method and an [open-source](https://github.com/cleanlab/cleanlab) Python implementation that is easy-to-use for all data types (image, text, tabular, audio, etc). For data scientists, we've made a quick [Jupyter tutorial](https://github.com/cleanlab/examples/blob/master/active_learning_multiannotator/active_learning.ipynb) to run ActiveLab on your own data. For ML researchers, we've made all of our [benchmarking code](https://github.com/cleanlab/multiannotator-benchmarks/tree/main/active_learning_benchmarks) available for reproducibility so you can see for yourself how effective ActiveLab is in practice.

Labeled data is key to train models, but data annotators often make mistakes. One can collect multiple annotations per datapoint to get a more reliable consensus label, but this is expensive! To train the best ML model with the least data labeling, a key question is: **which new data should I label, or which of my current labels should be checked again?**

https://preview.redd.it/txcqiokmndla1.png?width=960&format=png&auto=webp&s=d202b8ffabd1174d81616bcbcddea5eea9c93203

ActiveLab automatically answers this question for you, allowing you to train the most accurate ML model via a smaller number of total annotations than required to reach similar accuracy with popular active learning methods.  ActiveLab is highly practical ‚Äî it runs quickly and works with: any type of ML model, batch settings where many examples are (re)labeled before model retraining, and settings where multiple annotators can label an example (or just one annotator).

If you're interested in reading more, check out our blogpost: [https://cleanlab.ai/blog/active-learning/](https://cleanlab.ai/blog/active-learning/)"
12,zuh1de,datascience,Open-AI,top,2022-12-24 19:37:45,Bootcamp isn't great,smothry,False,0.8,6,https://www.reddit.com/r/datascience/comments/zuh1de/bootcamp_isnt_great/,7,1671910665.0,"Ugh. So, I went through all the lectures and examples provided for Central Michigan's ML and AI boot camp  which, although much more expensive than a Udemy class, are not as high quality. The classes are hosted on Ed2go. Now, for a capstone project, to pass the class, I need to design models to take video input of from driver POV and overlay on an output video the current speed of the vehicle, boxes around the signs with sign classification, and road edges / centerline lines. I am starting with the speed detection using a a dataset I found by commaai on GitHub. Thing is, there was never any discussion about how to preprocess video to get it into an RNN. We discussed how RNN's work but not much preprocessing. Are there any keras preprocessing layers anyone would suggest? Speed detection is not really an image classification problem. I have at least split the video into an array of frame data using openCV already. This capstone seems very advanced compared to the instruction given. 

P.s. I only took the class because the state offered to pay for it and it sounded interesting. After it started they informed me that I I don't pass the final I will have to pay it all back. So, let's just say there is strong monetary motivation to figure this out."
13,134o5fe,datascience,Open-AI,top,2023-05-01 14:33:50,"[colabdog.com] I build an aggregator for AI News that combines OpenAI, Google AI, MIT, BAIR.",colabDog,False,0.87,6,https://i.redd.it/rlizo23od8xa1.png,1,1682951630.0,
14,10mu9ru,datascience,Open-AI,top,2023-01-27 19:53:34,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,0.84,4,https://www.reddit.com/r/datascience/comments/10mu9ru/a_python_module_to_generate_optimized_prompts/,0,1674849214.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features üöÄ

* üßô‚Äç‚ôÄÔ∏è NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* üî® Easily add one-shot, two-shot, or few-shot examples to the prompt
* ‚úå Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* üí• Custom examples and samples can be easily added to the prompt
* üí∞ Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Examples](https://preview.redd.it/x232msli2nea1.png?width=1236&format=png&auto=webp&s=6071efdd8cb12801230af6572991ba8aaff1a9ec)

&#x200B;

https://preview.redd.it/5o5uqk3k2nea1.png?width=1398&format=png&auto=webp&s=96c25820a698a83dfeb0e8f7f37682d9d27c06cb"
15,yq3pdr,datascience,Open-AI,top,2022-11-09 00:39:02,Modern Forecasting in Practice with Jan Gasthaus (AWS) and Tim Januschowski (Zalando),lorenzo_1999,False,0.7,4,https://www.reddit.com/r/datascience/comments/yq3pdr/modern_forecasting_in_practice_with_jan_gasthaus/,4,1667954342.0,"Just wanted to give a heads up that we‚Äôve got an upcoming course on Time Series & Forecasting. The goal is to help you solve complex business problems by making more accurate predictions with modern forecasting techniques.

¬†This course will be led by two industry leaders: Jan Gasthaus (AWS) and Tim Januschowski (ex-AWS, Zalando).

In the past, Tim and his team built multiple AI services for AWS such as SageMaker, Forecast, Lookout for Metrics, and DevOps Guru. Jan was part of the teams pushing these projects forward, and also co-created the open-source deep learning forecasting library Gluon TS.

Plus, like all of our courses, Time Series & Forecasting qualifies for coverage from your org‚Äôs L&D budget or personal learning stipend.

Come join Tim and Jan live for 5-days of hands-on training. You can learn more about the course by clicking here: https://www.getsphere.com/cohorts/modern-forecasting-in-practice?source=Sphere-Communities-r-datascience"
16,yqocx2,datascience,Open-AI,top,2022-11-09 16:48:59,"Is AGI, as defined by Sam Altman from OpenAI, a real possibility in the near future?",deepfuckingbass,False,0.61,4,https://www.reddit.com/r/datascience/comments/yqocx2/is_agi_as_defined_by_sam_altman_from_openai_a/,14,1668012539.0,"I saw [this interview](https://m.youtube.com/watch?v=WHoWGNQRXb0) with Sam Altman, CEO of OpenAI, and I was surprised to hear several bold claims about the capabilities of AGI in the near future. He defines AGI as an AI/ML model with human-level intelligence. He seems to imply that current neural network architectures and techniques will get us there. At one point there‚Äôs an aside about whether we (humans) should still have kids with AGI an inevitability.

I‚Äôm struggling to understand how he can make the leap from where we are today to this sci-fi-like AGI future he describes. I‚Äôm very impressed by the work at OpenAI, so maybe Sam has access to tech that most practitioners in data science haven‚Äôt seen yet. With that being said, his interview rang a couple hype alarm bells with me.

What am I missing? Is AGI really around the corner?"
17,zrvydv,datascience,Open-AI,top,2022-12-21 18:04:50,Advice for a recent college graduate who majored in Computer Science and Statistics looking to start a career in data? How does job security look?,OGlogicgate,False,0.69,5,https://www.reddit.com/r/datascience/comments/zrvydv/advice_for_a_recent_college_graduate_who_majored/,3,1671645890.0,"Hello, I graduated from university (not elite but notable) this semester with a major in Computer Science and Statistics. My internship experience is limited to front end development but as I was accruing credits towards my statistics major over the past year and a half and taking a course on machine learning, I've decided that I want to go into the field of data science. Unfortunately I don't have any internship experience in the field but after alot of consideration, a career in data science seems to be what I have the most interest in.

Through my coursework and projects I've had alot of exposure to python and data and ml frameworks, namely numpy and pyTorch. I also have experience in SQL and backend query languages.  

My question is, what should I focus on doing to make myself a more appealing candidate to get into the field of data science? Are there any certificate programs like the TensorFlow developer certificate that would help in me getting a job in this field? I should note that I had my fair share of personal issues in college and that my gpa is a 2.7 which really concerns me about my chances.

I apologize if this question is too open-ended or lacks basic research on my part, I've been struggling on what career I wanna go into and just recently decided I want to orient myself towards data science. Any advice would be greatly appreciated.

Also, as a side if you can touch a little bit on what job security looks like in the field I would greatly appreciate it. I've been paying attention to the openAI language model and like I'm sure many others, was frightened by what it could do. I understand there's no way it could replace a data scientist in it's current state however who knows what it can do in future iterations? How likely is it that large language models like chatGPT will either replace or displace a large percentage of data scientists in the field?"
18,z81m6m,datascience,Open-AI,top,2022-11-29 18:19:43,Automatically Detect Annotation Errors in Image/Text Tagging Datasets,cmauck10,False,1.0,5,https://www.reddit.com/r/datascience/comments/z81m6m/automatically_detect_annotation_errors_in/,0,1669745983.0,"Hey guys! Many of us in the data science and ML space work with **multi-label data**, where the image or text is tagged with multiple labels. Often these datasets contain **frequent label errors** and/or **missing tags** (check what we found below in the CelebA dataset) that make it hard to train highly accurate ML models. Support for multi-label data was one of the top features requested ‚Äî so we [added it](https://docs.cleanlab.ai/stable/tutorials/multilabel_classification.html), [benchmarked it](https://cleanlab.ai/blog/multilabel/), and published all of the [research](https://cleanlab.ai/blog/multilabel/).

[Find errors and missing labels in multi-label datasets.](https://preview.redd.it/tn0m9lg8mx2a1.png?width=1250&format=png&auto=webp&s=80d4d09a24b6929894a5ce994042f491a6b8f544)

We are excited to share this newest research on algorithms to automatically find label errors in multi-label classification datasets. Image/document tagging represents important instances of¬†**multi-label classification**¬†tasks, where each example can belong to multiple (or none) of K possible classes. Because annotating such data requires many decisions for each example, often multi-label classification datasets contain tons of label errors, which harm the performance of ML models.

We‚Äôve open-sourced our algorithms in the [recent release of cleanlab v2.2](https://github.com/cleanlab/cleanlab/releases/tag/v2.2.0). All you need to do to use them is write one line of open-source code via [cleanlab.filter.find\_label\_issues](https://docs.cleanlab.ai/stable/tutorials/multilabel_classification.html).

    from cleanlab.filter import find_label_issues
    
    ranked_label_issues = find_label_issues(
        labels=labels,
        pred_probs=pred_probs,
        multi_label=True,
        return_indices_ranked_by=""self_confidence"",
    )
    # labels: list of lists of (multiple) labels of each example
    # pred_probs: predicted class probabilities from any trained classifier

Running the new `find_label_issues()`function on the [CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) image tagging dataset reveals around **30,000 mislabeled images**! Check out a few of them in the blog post!

Resources:

* Blog post: [https://cleanlab.ai/blog/multilabel/](https://cleanlab.ai/blog/multilabel/)
* Paper: [https://arxiv.org/abs/2211.13895](https://arxiv.org/abs/2211.13895)
* Tutorial: [https://docs.cleanlab.ai/stable/tutorials/multilabel\_classification.html](https://docs.cleanlab.ai/stable/tutorials/multilabel_classification.html)
* Benchmarks: [https://github.com/cleanlab/multilabel-error-detection-benchmarks](https://github.com/cleanlab/multilabel-error-detection-benchmarks)
* Code: [https://github.com/cleanlab/cleanlab](https://github.com/cleanlab/cleanlab)

Hope you find these practical tools useful in your real-world data science and ML applications!"
19,12huu9r,datascience,Open-AI,top,2023-04-10 20:19:40,Make History And Win 1 Million Dollars On This Fascinating AI Treasure Hunt,LesleyFair,False,0.62,3,https://www.reddit.com/r/datascience/comments/12huu9r/make_history_and_win_1_million_dollars_on_this/,1,1681157980.0,"[Example img\_name](https://preview.redd.it/7up7o8s984ta1.png?width=683&format=png&auto=webp&s=a9d32fed069c82e8a26d62932cbf7791e411b760)

This week‚Äôs story sounds like it was taken straight from a science fiction novel.

The leaders of the Church are shaking in fear because of what AI could bring to light.

Thousands of years ago, a massive volcanic eruption wiped out a monumental city in a matter of hours. Among the thousands of destroyed houses was one very special estate. It belonged to a close relative of the most powerful Kaiser that ever lived.

On his estate was a vast library filled with thousands of papyrus scrolls of unspeakable value.

The scrolls contain texts from long-lost secrets about philosophy, science, and possibly even about the origins of modern religions. When the house was destroyed along with the library, the conditions under the scorching hot lava miraculously preserved the scrolls. Under the stone, the scrolls survived for thousand of years.

The scrolls were discovered but have become so fragile that they cannot be opened anymore without destroying them. So, scientists are using modern particle accelerators and AI to unlock the secrets hidden in them. A price of $1M will go to whoever manages to read the scrolls first.

*Pretty good no?*

The best part about this story is that it is not made up. Okay, I might have been adding some drama in my depiction of church leaders shaking under their cassocks. I am pretty sure they neither know what is going on nor are they reading this newsletter.

In this week's edition, we will look at a spine-tingling story behind the [Vesuvius Challenge](https://scrollprize.org/) and see how computer vision can help to unlock the secrets of the past.

Let‚Äôs jump in!

**What Actually Happened**

In 79 AD the Vesuvius volcano erupted and buried the city of Pompeii. What very few people know is that multiple cities were also destroyed in the incident. One of these cities was Herculaneum.

We can think of Herculaneum as the Beverly Hills of Pompeii.

The city was full of marvelous villas and estates. One of the more impressive ones belonged to Caesar‚Äôs father-in-law. It goes without saying, the guy was very powerful, well-connected, and super-rich.

[Example img\_name](https://preview.redd.it/3nqjl5y984ta1.png?width=422&format=png&auto=webp&s=2270aa0f571e9d59c8e2f8fb5ca7b5bbc5d3dfa4)

Estate of Caesar‚Äôs father-in-law

Inside his estate was a giant library full of scrolls from the Greek and Roman times.

When the villa was destroyed, the heat of the lava carbonized (turning to charcoal without burning) the scrolls. This has preserved them for almost 2000 years. Since the 18th century, different groups tried to dig up the scrolls.

To date, more than 1800 scrolls have been excavated and most-likely there are many more under ground.

Some people speculate that his library might even contain scrolls from the library of Alexandria that burned down a few years before. From these scrolls, we might discover completely new philosophical schools, scientific secrets of the Greeks, and *heck!* maybe drafts of the bible with GPT watermarks on them.

However, there is a catch!find

Quite frankly, the scrolls have more resemblance with a cigarette bud than a roll of papyrus.

[Example img\_name](https://preview.redd.it/77ir7zz984ta1.png?width=474&format=png&auto=webp&s=7883343757ff1f75036489cac7c1304b09d3e2f9)

Herculaneum Scroll

Looking at the image above, it is needless to say that simply unrolling them is not really an option.

In the 17 hundreds, an Italian monk painstakingly tried to unroll some of the scrolls over several decades. The result was mostly papyrus confetti. He managed to uncover a few intact fragments that had philosophical texts written in Greek on them.

This is obviously not scalable and would destroy most of the texts. However, if we could read the scrolls this would more than double the amount of text that was handed down to us from the Greek and Roman times. The value of that is obviously hard to overstate!

*But, if we cannot unroll the scrolls, how are we supposed to find what‚Äôs written on them?*

**How To Read The Scrolls Without Opening Them**

The Herculaneum scrolls are not the first carbonized scrolls to be found.

In 1970, a number of 2000-year-old scrolls were discovered in the En-Gedi Oasis close to the Dead Sea. With no Italian monks at hand and the foresight that opening the scrolls would destroy them Dr. Seals from the University of Kentucky pioneered a method called *virtual unwrapping.*

It allows us to read the scrolls without opening them.

First, a high-resolution CT scan is created of each scroll. The scan creates digital slices from the scroll. The slices are created lengthwise, similar to how a cucumber is cut. Now, in order to perform the virtual unwrapping a sheet of the scroll is traced along the cross-sections.

[Example img\_name](https://preview.redd.it/3fdm424a84ta1.png?width=945&format=png&auto=webp&s=d63e7ee8e4b8794f955e6d88898b2b9d9cb2f893)

In the image above, you can see an animation of how this is done cross-section by cross-section until a connected piece of the scroll is extracted. These connected pieces are then virtually flattened in order to read the text (see video below).

[https://scrollprize.org/img/landing/engedi5.webm](https://scrollprize.org/img/landing/engedi5.webm)

*So far so good. Why can we not just do the same with the Scrolls from Herculaneum?*

There are a few challenges with applying this technique to the Herculaneum scrolls. On the one hand, the scrolls are very tightly wrapped and generally in pretty bad shape. On the other hand, the ink in the Herculaneum scrolls is radiolucent. This means that X-rays pass through the ink the same way they pass through the papyrus.

As a result, the ink, in the CT scans, is not visible to the human eye.

But there is good news. It has been shown that neural networks can pick up on subtle patterns in the scans that are created by the ink \[2\]. Next, we will look at how neural networks are being trained on the scans and how to win the price. *Read on!*

**The Challenge of Training On The Fragments**

As mentioned above, a few of the scrolls were unrolled by an exceptionally patient Italian monk.

[Example img\_name](https://preview.redd.it/27vy1n5a84ta1.png?width=485&format=png&auto=webp&s=8be2c7c802d0048ca8d0515472055166e3816f9a)

Scroll Fragments With Ink

Some of the resulting fragments have legible ink on them.

So, people created training datasets from them. First, a 4¬µm 3D X-ray scan was created for the fragments. Second, an additional infrared image was taken of the scroll fragments to make the ink more visible. Then, the ink on ht IR images was hand-labeled. The labeled images are then aligned with the scans in order to create input and label pairs.

Next, the areas with ink were hand-labeled. Finally, the labeled images were aligned with the scans in order to create input and label pairs.

[Example img\_name](https://preview.redd.it/rki6yy7a84ta1.png?width=910&format=png&auto=webp&s=a9a51cb28206fd2c8e446bee23332dfcb215c0d6)

Overview Of Data Acquisition Process For Scroll Fragments

The [data paper](https://raw.githubusercontent.com/educelab/EduceLab-Scrolls/main/paper/EduceLab-Scrolls.pdf), in which they trained a model on the fragments, reports a pretty low recall (in the 40% range).

However, their approach appears to be quite basic. They formulated the problem as a patch-wise binary classification. So, for each patch, their model predicted ink vs. no ink. Furthermore, the final accuracy might not need to be very high to make the text readable.

Most likely, translating the model to the full scrolls will be a tough nut to crack.

[Example img\_name](https://preview.redd.it/9m6pbfaa84ta1.png?width=1200&format=png&auto=webp&s=6f93b4925db8c55b637954c835656dae992d6ff4)

The Two Scrolls To Be Read

Alongside the fragment datasets, we are provided with 8¬µm 3D X-ray scans of two full scrolls. As a matter of fact, we are only given half of the scan data for each of the two scrolls. The other half is held out as a validation set. Each half-scroll scan consists of 14,000 .tif files with 120MB each. Since each slice is 8¬µm tall, the scroll half is 11.2cm tall.

The two scrolls need to be virtually unwrapped first.

The software to do the unwrapping is provided. Some manual work is required to get it going, but all the pieces are there. I dearly hope that the challenge attracts many brilliant minds from all over the world!

If you have some time on your hand, or you simply want to make some money to buy a few A100 GPUs go and [check out the challenge](https://scrollprize.org/)!

The best ink detection model gets $100K and whoever is the first to read four separate passages on one of the full scrolls wins $700K. An additional $200K of prices will be announced in the coming months.

Money aside, the thought that some guy or girl with a cup of coffee and a laptop could create a model which unlocks this trove of wisdom makes me excited about the present and the future alike.

What an exciting time for science and humanity!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

If you did find it useful and are not subscribed to the newsletter yet, [click here to sign up](https://thedecoding.net/)! I send out a thoughtful 5-minute email every week to keep you in the loop about machine learning research and the data economy.

*Thank you for reading and I see you next week ‚≠ï!*

**References:**

\[1\] [https://en.wikipedia.org/wiki/Herculaneum\_papyri](https://en.wikipedia.org/wiki/Herculaneum_papyri)

\[2\] [https://raw.githubusercontent.com/educelab/EduceLab-Scrolls/main/paper/EduceLab-Scrolls.pdf](https://raw.githubusercontent.com/educelab/EduceLab-Scrolls/main/paper/EduceLab-Scrolls.pdf)"
20,1000yz9,datascience,Open-AI,comments,2022-12-31 18:55:53,swe vs ds,Impossible-Ask4646,False,0.88,90,https://www.reddit.com/r/datascience/comments/1000yz9/swe_vs_ds/,116,1672512953.0,"I'm a 29yr old dairy farm manager in Colorado, being paid well (+- 150k/yr) for working extremely long hours on the farm managing people. For the past 5 years I've been locked into this job with a workvisa, but I got my greencard approved a couple weeks ago and finally have some more freedom and am looking into making a complete career switch.

I don't have the best people skills (although it improved managing 20+ employees for 5 years), but have good technical and math skills. I grew up in Belgium where every year in high school I made it to the national Math Olympics final. I got a Bachelor of Science degree in Bioscience Engineering and a Masters of Science degree in Management, Economics and Consumer Sciences. I always felt I was learning things faster than others, was always best in class, but spent the majority of my time helping my parents on their farm until I moved to the US.

While managing this dairy in the US, I did a lot of little things on the side.

* I played around with some crypto, was arbitraging bets on the US elections on different crypto betting websites and protocols (eg. receiving odds of 1.9x for Biden to win, while receiving odds above 3x for Trump to win election)
* Buying and selling large amounts of crypto for cash for a 10-15% mark-up
* Buying bitcoin miners from China after their crypto ban and selling them locally for a profit
* I saw publicly traded bitcoin mining companies were way overvalued, but shorting them is risky since it's hard to predict what will happen to the bitcoin-price so I started to run efficient bitcoin miners in a facility with cheap electricity, while shorting stocks like RIOT to eliminate the risk of the bitcoinprice going up. I made a copy of a % of RIOT for a 10th of what their stock was worth and shorted them at the same time.
* Buying SPY at the stock market while shorting mSPY (mirrored SPY) on mirror protocol (DeFi - Decentralized Finance) with aUST (acnhored UST) as collateral, leveraging this up many times to get yields around +100% APY on USD (by taking insurance for a UST-depeg through Unslashed (who did pay us out through a Kleros-court case). I lost 300k $ on this after making 600k $ with it because of SPY pricing jumping up by 4% to come back down 4% a bleep of a second afterwards on the actual stock market (dark pool after hours). [see here](https://forum.mirror.finance/t/liquidations-caused-by-unrepresentative-oracle-pricing-of-mspy-on-jan-3-2022/2569)

All of this together made some good amount of money, but right now I'm trying to figure out what to do with our future. The biggest reason I want to quit my current job is that I have a wife and 3 little kids who I don't see enough. I want to spend more time with them, but it's not working out in my current position. I also feel like I want to use my technical/logical/math skills more, but after all this time it's hard to figure out what to do exactly and how to even start on getting there. 

We are thinking of either:

* Running our own small business, but we can't seem to figure out what exactly.
* Software Engineering
* Data Scientist/AI/ML
* Other managerial jobs I could get, although I don't think I ""love"" managing people
* ...

&#x200B;

I'm open to any advice, on positions, on who to talk to, on which path to take. Thanks in advance!"
21,zcgpbp,datascience,Open-AI,comments,2022-12-04 18:05:07,What do you guys think of OpenAI‚Äôs ChatGPT?,Loud_Ad_6272,False,0.95,56,https://www.reddit.com/r/datascience/comments/zcgpbp/what_do_you_guys_think_of_openais_chatgpt/,77,1670177107.0,"As the title goes, what do you guys think of this and what effect do you think it would have on the discipline and field going forward?"
22,114qfkp,datascience,Open-AI,comments,2023-02-17 16:14:47,I am worried about the future of working as a Data Scientist in industry,Slumi,False,0.51,3,https://www.reddit.com/r/datascience/comments/114qfkp/i_am_worried_about_the_future_of_working_as_a/,71,1676650487.0,"I once watched a video where someone argued that Michael Jordan had ruined basketball. As a person whose only notions of basketball come from Space Jam, this surprised me. Wasn't the guy the best out there? How could someone who's best in his field ruin it? After hearing the argument out though, I began to understand what the video was really getting at: to them, Michael Jordan had ruined the field BECAUSE he was so good. A sport that was once full of diverse personalities and strategies then turned into multiple teams trying to produce their own Michael Jordan copycat with copycat moves and copycat strategies.

I'm not well versed enough in basketball to know how right or wrong that guy was. But this concept that something could ruin an entire field simply by being too good stuck with me. And all these years later, I consistently think about it while doing my job as a Data Scientist. And the more GPT-X and ChatGPT advance, the more this concept haunts me.

I've been working in this field for 5 years. During the first 4 years, the job was exciting. Every new problem required extensive research on my part: looking for papers, datasets, implementations, implementing my own stuff, experimenting, comparing the results... That was the biggest part of my job. Yeah I still had to do some software engineering stuff here and there, but I felt like I had a well defined and specialized role at my company. Even in the NLP field, the variety of technologies I had to work with was a lot of fun: sometimes I used RNNs, sometimes embedding based similarity functions, sometimes more classic approaches, and then towards the end it was transformer after transformer, but at least I still had to finetune them myself.

Over the past few months and years, this has started to change, however. GPT-1 and 2 were promising, but were more of a proof of concept than anything realistically usable. But with GPT3's and ChatGPT's latest performance, it feels like NLP is becoming more and more standardized. I wouldn't call it a solved field, far from it. But, sadly, unless you work for a big tech company,  I think the days of exploratory research-type work in the NLP field are over.

Even in academia, the latest NLP papers I've seen come out of prestigious universities went from actual engineering/mathematical advances to ""Look, if you ""engineer"" the prompt like this, it works better sometimes!""

And now, with the exploding popularity of ChatGPT, any random lambda would come to the conclusion that if you need to use AI, ChatGPT is the answer.

Now, some of you may already be thinking ""but hold on, with my finetuned transformer I get a 1% increase in accuracy over out-of-the-box GPT-3!"". The thing is: even if you can, it doesn't really matter. I really doubt the sales team of your company is gonna pay much attention to your pleas to give you 6 months to work on a product rather than 6 minutes. Because yes, that's the increase in productivity we're dealing with: What took me months a few years ago only takes minutes now. The range of problems I can apply NLP too has also increased, as out-of-the-box GPT-3 works well enough not to require a dataset for more ""general"" tasks. And the results are either comparable or even better.

In the span of a year, NLP went from my favorite ML field to one I never see myself working in again if given the choice. And while a lot of non NLP fields are, for now, untouched by the exploding popularity of the GPT family, I'm afraid that what happened to the NLP field will be replicated in other fields, and that in a few years, only super specialized issues won't have a solution somewhere in the form of a ridiculously big model accessible via an API key provided by a big tech company. And even if GPT3 was open source, the sheer size of it makes it close to impossible to realistically train for anything less than the biggest of tech companies out there. Even finetuning it would be a challenge.

I'm not saying the job of data scientist will disappear, in fact, I'd say it will be asked more than ever since boomers who don't understand what the title actually implies will want some in their company. But we won't be doing nearly as much actual data sciency stuff as we did in the past. Instead we'll have to become even bigger software engineer/data analyst/DevOps/project manager hybrids than we already were. The only exception is for people who either work in big tech companies or on very specialized problems that require an in-house model.

It's not that much of a problem for people who like the roles I listed above. But for people like me, whose enjoyment came out of the exploratory side of the job, I believe the future to be bleak."
23,13cpckb,datascience,Open-AI,comments,2023-05-09 13:07:55,PSA: You don't need fancy stuff to do good work.,Bitwise_Gamgee,False,0.88,368,https://www.reddit.com/r/datascience/comments/13cpckb/psa_you_dont_need_fancy_stuff_to_do_good_work/,63,1683637675.0,"I've been reading a lot of posts on r/datascience and several seem to orbit the subject of how to use the latest tool or tweak, I understand that it can be easy to get caught up in the whirlwind of tools, frameworks, and cutting-edge technologies. While these advancements can undoubtedly enhance our work, it's important to remember that data science isn't about using the most advanced or expensive tools; it's about extracting valuable insights from data to drive informed decision-making.

Data Collection and Categorization

Before diving into advanced machine learning algorithms or statistical models, we need to start with the basics: collecting and organizing data. Fortunately, both Python and R offer a wealth of libraries that make it easy to collect data from a variety of sources, including web scraping, APIs, and reading from files. Key libraries in Python include [requests](https://requests.readthedocs.io/en/latest/), [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/), and [pandas](https://pandas.pydata.org/), while R has [httr](https://cran.r-project.org/web/packages/httr/index.html), [rvest](https://rvest.tidyverse.org/), and [dplyr](https://dplyr.tidyverse.org/).

These libraries not only make it easy to collect data but also to clean and structure it for analysis. With just a few lines of code, you can filter, sort, and transform data into a format that's ready for exploration and modeling.

Data Analysis and Visualization

Once your data is collected and organized, the next step is to analyze and visualize it. Both Python and R excel in this area, providing a wide range of libraries and packages for exploratory data analysis and visualization.

Python's pandas, [NumPy](https://numpy.org/), and [SciPy](https://scipy.org/) libraries offer powerful functionality for data manipulation, while [matplotlib](https://matplotlib.org/), [seaborn](https://seaborn.pydata.org/), and [plotly](https://plotly.com/) provide versatile tools for creating visualizations. Similarly, in R, you can use dplyr, [tidyverse](https://www.tidyverse.org/), and [data.table](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html) for data manipulation, and [ggplot2](https://ggplot2.tidyverse.org/), [lattice](https://cran.r-project.org/web/packages/lattice/index.html), and [shiny](https://shiny.rstudio.com/) for visualization. These packages enable you to create insightful visualizations and perform statistical analyses without relying on expensive or proprietary software.

Modeling and Prediction

Finally, when it comes to building models and making predictions, Python and R have a plethora of options available. Libraries like [scikit-learn](https://scikit-learn.org), [statsmodels](https://www.statsmodels.org/stable/index.html), and [TensorFlow](https://www.tensorflow.org/)in Python, or [caret](https://topepo.github.io/caret/), [randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf), and [xgboost](https://xgboost.readthedocs.io/en/stable/)in R, provide powerful machine learning algorithms and statistical models that can be applied to a wide range of problems. What's more, these libraries are open-source and have extensive documentation and community support, making it easy to learn and apply new techniques without needing specialized training or expensive software licenses.

Simplicity is key, embrace it and you'll learn a lot faster than trying to glean insights from some poorly trained AI model.

&#x200B;

ps. Any ""IDE"" more extensive than VIM/EMACS/~~nano~~ are unnecessary :)"
24,10nk3pf,datascience,Open-AI,comments,2023-01-28 17:13:48,will openAI make data scientists obsolete?,fabzo100,False,0.16,0,https://www.reddit.com/r/datascience/comments/10nk3pf/will_openai_make_data_scientists_obsolete/,40,1674926028.0,"I started to think that openAI (or other advanced AI tech from other big tech) may eventually make data scientists obsolete faster before any other job. Here's why.

First of all, machine learning is a bit useless if everybody just leverage openAI (or Google) LLM. I mean why would companies ever need any other AI if they can all just pay google or microsoft to do the AI-related jobs for them?

And even when it comes to data reasoning, AI will be able to do that job much faster as compared to data scientists. If you check azure openAI landing page, they literally mentioned code generation and data reasoning as their selling points.

Thoughts?"
25,126ndlu,datascience,Open-AI,comments,2023-03-30 13:41:51,Seeing a lot of job openings for high-level AI and Data Analytics positions...,fingin,False,0.81,26,https://www.reddit.com/r/datascience/comments/126ndlu/seeing_a_lot_of_job_openings_for_highlevel_ai_and/,20,1680183711.0,"I have noticed an uptick in jobs for things like prompt engineer, AI ethics lead, AI manager. When you look at these requirements it looks like relatively low entry: a familiarity with general AI and AI regulations (not that there is a ton of expertise to be had in this latter category). They don't require much or any technical skill. 

I'll admit, I find myself frustrated as I work in a highly technical role and feel like these opportunities are really 'low hanging fruit', due to the vagueness of the requirements. I'm sure many of us wear not just technical hats but also those of product management, coaching and training, etc. 

What do you think? Is it just a fad stemming from ChatGPT and Image gen promotion? Are you going to make a job switch and apply for these roles?"
26,124p2uv,datascience,Open-AI,comments,2023-03-28 13:38:56,"How will companies go about integrating GPT into their ecosystem (databases, documents, websites, etc.)? Is this possible already or not yet?",KidzKlub,False,0.62,3,https://www.reddit.com/r/datascience/comments/124p2uv/how_will_companies_go_about_integrating_gpt_into/,20,1680010736.0,"I would love for GPT to have the context of our ecosystem and be able to ask it questions about our data, have it write code that works with the rest of our infrastructure, analyze documents that we have stored, and more. Would this involve training a bespoke model on a company's data? Will OpenAI offer enterprise solutions where they help set you up with a model that meets your needs? I'm curious for my own purposes, but also I think this would be a major way that companies will start using this technology in the near future."
27,z918vu,datascience,Open-AI,comments,2022-11-30 19:50:56,"How hot will I be after I finish this Data Science, ML, and AI learning and certificate plan?",HappyCamperS5,False,0.31,0,https://www.reddit.com/r/datascience/comments/z918vu/how_hot_will_i_be_after_i_finish_this_data/,19,1669837856.0,"I have used mathematics, mathematical programming--Project Euler and OpenFOAM as my psychological mindfulness activity since 2016, because I am medically retired. Now, I think I want to concentrate on data science, machine learning (ML) and artificial intelligence (AI). Specifically, I am interested in prediction and online social network analysis. I suspect my main interests will be in the area of AI/ML.

Eventually, after I finish my refreshing of MIT single-variable calculus and MIT multivariable calculus relearning, I will be taking MIT Linear Algebra (3 months); MIT Python programming (3 months); MIT Micromaster program in statistics and data science audit (18 months); MIT Micromasters in statistics and data science for certificate (12 months; shorter because 2nd time); Machine learning by Stanford (3 months); MIT AI Products and Services (2 months);  graph theory (3 months); and Harvard or University of Canterbury text analytics (3 months).I believe the above path will allow me to freelance as a data scientist, ML engineer and/or AI engineer. As I learn and complete projects, I will build my GitHub portfolio for potential contracts. I also hope to volunteer at several non-profits to help and to learn. Specifically, climate crisis prediction, whistleblower retaliation analysis and mental health.

I am a simple man, but I excelled in mathematics and mathematical programming--96% average-in MIT single variable, multivariable and differential equation calculus, which got me recognized by the MIT mathematics department, and top 12.97% at Project Euler mathematical programming on an international scale. I also earned a B+ in MIT classical mechanics. I did quite well in chemical engineering and finished with a B+ average even though I doubled up on my chemical engineering and engineering courses. Not because I love pain, but because I was accepted into the professional school of chemical engineering from a community college, and I did not have the sophomore chemical engineering and engineering courses finished. It was quite difficult, but it was worth it as chemical engineering is a unique thought process that has opened doors for me.

I also did well as a chemical engineer in the pharmaceutical industry. I optimized 25 processes, and was awarded three vice president's awards from vice president of research, development and validation. In total, I worked for 5 corporations and the the government.

If I succeed with my coursework, will I be competitive? I have read, in r/datascience, that some believe coding is more important than mathematics. Meanwhile, MIT, Berkeley, Princeton, Harvard and Northwestern University, as a few examples, concentrate on mathematics for a strong foundation in the above mentioned subjects. Princeton says that one should take as much probability as possible, and Berkeley emphasized that an excellent foundation in math is important. What is your opinion?"
28,yqocx2,datascience,Open-AI,comments,2022-11-09 16:48:59,"Is AGI, as defined by Sam Altman from OpenAI, a real possibility in the near future?",deepfuckingbass,False,0.61,4,https://www.reddit.com/r/datascience/comments/yqocx2/is_agi_as_defined_by_sam_altman_from_openai_a/,14,1668012539.0,"I saw [this interview](https://m.youtube.com/watch?v=WHoWGNQRXb0) with Sam Altman, CEO of OpenAI, and I was surprised to hear several bold claims about the capabilities of AGI in the near future. He defines AGI as an AI/ML model with human-level intelligence. He seems to imply that current neural network architectures and techniques will get us there. At one point there‚Äôs an aside about whether we (humans) should still have kids with AGI an inevitability.

I‚Äôm struggling to understand how he can make the leap from where we are today to this sci-fi-like AGI future he describes. I‚Äôm very impressed by the work at OpenAI, so maybe Sam has access to tech that most practitioners in data science haven‚Äôt seen yet. With that being said, his interview rang a couple hype alarm bells with me.

What am I missing? Is AGI really around the corner?"
29,zfb19n,datascience,Open-AI,comments,2022-12-07 19:30:37,AI to improve revenue of liquor/wine retail stores,jko1701284,False,0.5,0,https://www.reddit.com/r/datascience/comments/zfb19n/ai_to_improve_revenue_of_liquorwine_retail_stores/,17,1670441437.0,"I'm a infra/devops/full-stack dev whose family owns a liquor store. They spend a ton of time analyzing their inventory and making decisions such as:

\- Reduce stock of this low performing product and make room for this one

\- Order more of this product at this time based on sales and stock

\- Increase stock of this category/type of product based on the upcoming holiday, event, time of year

A lot of their decisions are based on intuition, and I'd like to make it more data driven. They need some business intelligence that we see utilized in other industries.

What steps do I need to take to build what they need? I have no experience in ML, AI, etc. I see there are services such as [datacamp.com](https://datacamp.com)

Also, I'm interested in turning this into a business. If any of you are interested in partnering up, my inbox is open."
30,123nr31,datascience,Open-AI,comments,2023-03-27 14:00:59,Is object recognition now a trivial task because of OpenAI?,throwitfaarawayy,False,0.17,0,https://www.reddit.com/r/datascience/comments/123nr31/is_object_recognition_now_a_trivial_task_because/,16,1679925659.0,"I'm working on a project where we are tasked with classifying different types of vehicles. I am thinking that now because of OpenAI models especially GPT-4 with vision, this is now a redundant effort. In a few weeks to months this will be available to everyone and for really cheap. Then why am I building this?"
31,10fw1a3,datascience,Open-AI,comments,2023-01-19 07:54:24,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.94,120,https://www.reddit.com/r/datascience/comments/10fw1a3/gpt4_will_be_500x_smaller_than_people_think_here/,14,1674114864.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/t6m0epzlgyca1.png?width=575&format=png&auto=webp&s=a5972941053f833e76fd3a5009fc68a61f9e5406)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ‚≠ï, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
32,12sxxbq,datascience,Open-AI,comments,2023-04-20 13:01:57,AI eating the world,xmagedo,False,0.33,0,https://www.reddit.com/r/datascience/comments/12sxxbq/ai_eating_the_world/,9,1681995717.0,"Hello, 

I hope is all well with you all. 

This post to seek some guidance and advice from professional data scientists, ML developers and big data developers. With the latest AI tech that have been automated some jobs such chatgpt and others. It feels like that my effort of studying, reading books, understanding the match in some algorithms have been useless. A lot of things have came out to automate those things, I would be afraid to open IT company nowadays just that something might come and automated this thing. My point is that, what skills should one focus on in data science and ML? I need to keep improving and keep learning but what skills ? What will make me ahead of the curve? What books do you guys recommend? 



Thank you so much"
33,zixqgn,datascience,Open-AI,comments,2022-12-11 16:06:53,Personal project for PhDs and scientists,Cyalas,False,0.75,20,https://www.reddit.com/r/datascience/comments/zixqgn/personal_project_for_phds_and_scientists/,10,1670774813.0," Hello!

I've developed a project [NaimAI](https://www.naimai.fr/), to help PhDs and scientists in their scientific literaure review. To describe it brievely, it has 3 main features : 1 search in papers, 2 structures abstracts into objectives, methods and results and 3 generates automatically a (pseudo) literature review.

I wrote a [medium article](https://medium.com/@yaassinekaddi/literature-review-with-naimai-open-sourced-fcbdb36762de) that goes through the details.

Github repos : [https://github.com/yassinekdi/naimai](https://github.com/yassinekdi/naimai)

I've created a subreddit in case : [r/naimai4science](https://www.reddit.com/r/naimai4science/)

I'd be happy to have your opinion about it and hopefully this could be useful!"
34,zuh1de,datascience,Open-AI,comments,2022-12-24 19:37:45,Bootcamp isn't great,smothry,False,0.73,5,https://www.reddit.com/r/datascience/comments/zuh1de/bootcamp_isnt_great/,7,1671910665.0,"Ugh. So, I went through all the lectures and examples provided for Central Michigan's ML and AI boot camp  which, although much more expensive than a Udemy class, are not as high quality. The classes are hosted on Ed2go. Now, for a capstone project, to pass the class, I need to design models to take video input of from driver POV and overlay on an output video the current speed of the vehicle, boxes around the signs with sign classification, and road edges / centerline lines. I am starting with the speed detection using a a dataset I found by commaai on GitHub. Thing is, there was never any discussion about how to preprocess video to get it into an RNN. We discussed how RNN's work but not much preprocessing. Are there any keras preprocessing layers anyone would suggest? Speed detection is not really an image classification problem. I have at least split the video into an array of frame data using openCV already. This capstone seems very advanced compared to the instruction given. 

P.s. I only took the class because the state offered to pay for it and it sounded interesting. After it started they informed me that I I don't pass the final I will have to pay it all back. So, let's just say there is strong monetary motivation to figure this out."
35,zmwyxs,datascience,Open-AI,comments,2022-12-15 21:42:15,laptop for Data Science and Scientific Computing: proart vs legion 7i vs thinkpad p16/p1-gen5,macORnvidia,False,0.75,2,https://www.reddit.com/r/datascience/comments/zmwyxs/laptop_for_data_science_and_scientific_computing/,7,1671140535.0,"
laptop for Data Science and Scientific Computing: proart vs legion 7i vs thinkpad p16/p1-gen5

I'm looking at four laptop for DS. Not really interested in gaming, just the gpu, good cpu and massive ram. So that kind of brings me to the gaming laptop segment. 

**Main uses:**

- Data preprocessing, Prototyping cuda, rapids ai for accelerating classical data science and machine learning, DL inferencing, building conda enabled containers, 3D modeling/rendering and simulations using python, NLP, openCV, pytorch



1. Thinkpad p16:  4200$/3900$ (64 vs 32 gb ram)

64gb/32gb ddr5, i9 12900hx, rtx a4500 16gb vram, 1 TB, 3480 vs 2400, 230W power adapter 



2. Thinkpad p1 gen5:  3900$

32gb ddr5, i9 12900h vpro, rtx 3080ti 16gb vram, 1 TB, 2560 vs 1600, 230W power adapter



3. Asus Proart studiobook: 2999$

32gb ddr5, i7 12700h, rtx 3080ti 16gb vram, 2 TB, 3840 vs 2400 4K OLED, 330W power adaptor 



4. Legion 7i: 3500$

32gb ddr5, i9 12900hx, rtx 3080ti 16gb vram, 2 TB, 2560 vs 1600 165hz,  300W power adaptor



I love how beautiful and robust legion 7i is but based on the price difference I'm also leaning towards asus proart in case i7 12th gen isn't too bad to work with."
36,zmyhve,datascience,Open-AI,comments,2022-12-15 22:38:06,Text to SQL,ljh78,False,0.6,1,https://www.reddit.com/r/datascience/comments/zmyhve/text_to_sql/,7,1671143886.0,"Hi!

Looking to start a project to build an in-house text to SQL (which I then use to query a relational DB) tool for some of the engineers (not software...no experience with SQL). Have any of you folks done something of the sort before and would be able to recommend a decent starting point? For the starter version it'd be very simple.. most likely one dataset and simple questions (i.e. no joins, only SELECT FROM WHERE).  

I have heard of OpenAI's GPT-3, but am not looking to ask my org to pay for any third party systems at the moment (still very much an exploratory project). Additionally, I'm looking to further my experience and expertise in DS/AI, so this might be a good way to do just that (as far as NL processing goes).

Thank you!"
37,11is6oq,datascience,Open-AI,comments,2023-03-05 08:48:09,Beating OpenAI CLIP in Image retrieval with 100x less data and compute,vov_or,False,0.92,79,https://www.reddit.com/r/datascience/comments/11is6oq/beating_openai_clip_in_image_retrieval_with_100x/,7,1678006089.0,"Hello from the Unum AI team! We have been silently pre-training numerous Multi-Modal Models for Semantic Search for the last year!  
We are releasing several extremely performant checkpoints on the [HuggingFace portal](https://huggingface.co/unum-cloud/uform)!  
In addition there is the blog post about efficient Vision-Language pre-training:  
[https://www.unum.cloud/blog/2023-02-20-efficient-multimodality](https://www.unum.cloud/blog/2023-02-20-efficient-multimodality)"
38,zxemyn,datascience,Open-AI,comments,2022-12-28 16:58:16,Problem with installation of OpenCV !,MustafaAlnjar,False,0.17,0,https://i.redd.it/sr26qq3znp8a1.jpg,6,1672246696.0,"Could you guys help me with it ?? I want to use it with visual studio code (python)
What should i do ? 
Btw i‚Äôm new here"
39,1168sew,datascience,Open-AI,comments,2023-02-19 12:06:24,"Google's Response To OpenAI and ChatGPT is Coming, And They've Named It...",Kolownik,False,0.09,0,https://www.technews.city/2023/02/googles-response-to-openai-and-chatgpt.html?=read,5,1676808384.0,
40,132cvf9,datascience,Open-AI,comments,2023-04-28 23:16:15,New ChatGPT features and data science,jehan_gonzales,False,0.57,2,https://www.reddit.com/r/datascience/comments/132cvf9/new_chatgpt_features_and_data_science/,5,1682723775.0,"I'm wondering what impact updates to ChatGPT will have on data science and data scientists. 

I saw a TED talk from one of the OpenAI founders. You can see it here: https://youtu.be/C_78DM8fG6E

It's mind-blowing. He shows some new features that are coming soon to ChatGPT. He asks for a meal and uses the DALL-E integration to get an image of it. He even gets a shopping order put together on Instakart. He just needs to click and the food will be delivered to his house. 

The most impressive thing was the data analysis where he uploaded a CSV and the LLM figured out what the columns referred to on its own and then asked for plots and a prediction etc. The spat out Python code that he could dive into as well. 

It sounds like it could do data cleaning, preprocessing and modelling fairly easily. It would take some iterations, but if you have it the right direction, it would speed up the process 10x or more. 

I think this will basically simplify work for data scientists but will also enable ordinary folks with no quant backgrounds to do sophisticated analysis. 

I'm no longer a data scientist and work in product management. But if I were still in data science, I'd focus on my ability to help people self serve. I think the role will split to expert data scientists who build and productionise ML models and analytics enablers who help people get more out of the tools. 

What do you think? Is this a threat to data scientists? Or is it a productivity booster that will only make life better?"
41,yusu7c,datascience,Open-AI,comments,2022-11-14 08:23:52,R/Python usage in the supply chain industry?,levenshteinn,False,0.6,1,https://www.reddit.com/r/datascience/comments/yusu7c/rpython_usage_in_the_supply_chain_industry/,5,1668414232.0,"From my reading, the supply chain industry has a lot of fully integrated supply chain management solutions. These solution typically have some modules related to data science out of the box. Take for example Kinaxis, which has its Planning.AI within its ecosystem. Others like Blue Yonder, SAP and O9 also feature some readily built-in data science solutions to tackle supply chain problems. 

Sure other industries also have their own popular proprietary solutions bought from the market.

However, for supply chain specifically, I kinda have the impression that the usage of open source solutions like R/Python/Julia is lesser known. Python has PuLP, R has ROI package and Julia has JuliaOpt. But choosing which one over the other is not always clear. You have more resources online debating the merits of R vs Python, say for deep learning with concrete examples of how they are being used in the industry. 

I just joined the supply chain industry and there is a lot of focus on getting certified on these paid solutions (Kinaxis, Blue Yonder, O9, etc). While the opportunity to learn the proprietary systems is great, I hope to ensure my previously acquired skills in R/Python remain relevant in this industry. For example, previously I used R forecast package to perform demand forecasting. This was because the client was mainly using Excel tool and the SAP system has very basic forecasting feature. 

But now it seems that this proprietary systems are gettibg more sophisticated with the supply chain offering that you can run some level of AI within their integrated systems.

So how is R/Python being used alongside the proprietary solution in the supply chain industry?"
42,zo2pj1,datascience,Open-AI,comments,2022-12-17 09:31:36,Was ChatGPT trained on Kaggle and other DS coding platforms?,ikke89,False,0.5,0,https://www.reddit.com/r/datascience/comments/zo2pj1/was_chatgpt_trained_on_kaggle_and_other_ds_coding/,5,1671269496.0,"Hey everyone, does anyone know if chatGPT has been trained on Kaggle projects? If so, it should already be pretty good at a lot of DS stuff, right?

If not, I think it's only a matter of time before they will include that, which could create a very powerful DS personal assistant.

I guess it could be challenging to train it on large datasets specifically, but I'm sure there are some smart ways to make that part more efficient, like only using a sample of each data set. Plus, there is the legal question if Kaggle would allow openAI to use their data.

Do you guys have any thoughts?"
43,yq3pdr,datascience,Open-AI,comments,2022-11-09 00:39:02,Modern Forecasting in Practice with Jan Gasthaus (AWS) and Tim Januschowski (Zalando),lorenzo_1999,False,0.7,4,https://www.reddit.com/r/datascience/comments/yq3pdr/modern_forecasting_in_practice_with_jan_gasthaus/,4,1667954342.0,"Just wanted to give a heads up that we‚Äôve got an upcoming course on Time Series & Forecasting. The goal is to help you solve complex business problems by making more accurate predictions with modern forecasting techniques.

¬†This course will be led by two industry leaders: Jan Gasthaus (AWS) and Tim Januschowski (ex-AWS, Zalando).

In the past, Tim and his team built multiple AI services for AWS such as SageMaker, Forecast, Lookout for Metrics, and DevOps Guru. Jan was part of the teams pushing these projects forward, and also co-created the open-source deep learning forecasting library Gluon TS.

Plus, like all of our courses, Time Series & Forecasting qualifies for coverage from your org‚Äôs L&D budget or personal learning stipend.

Come join Tim and Jan live for 5-days of hands-on training. You can learn more about the course by clicking here: https://www.getsphere.com/cohorts/modern-forecasting-in-practice?source=Sphere-Communities-r-datascience"
44,zcgpbp,datascience,Open-AI,relevance,2022-12-04 18:05:07,What do you guys think of OpenAI‚Äôs ChatGPT?,Loud_Ad_6272,False,0.94,54,https://www.reddit.com/r/datascience/comments/zcgpbp/what_do_you_guys_think_of_openais_chatgpt/,77,1670177107.0,"As the title goes, what do you guys think of this and what effect do you think it would have on the discipline and field going forward?"
45,10nk3pf,datascience,Open-AI,relevance,2023-01-28 17:13:48,will openAI make data scientists obsolete?,fabzo100,False,0.14,0,https://www.reddit.com/r/datascience/comments/10nk3pf/will_openai_make_data_scientists_obsolete/,40,1674926028.0,"I started to think that openAI (or other advanced AI tech from other big tech) may eventually make data scientists obsolete faster before any other job. Here's why.

First of all, machine learning is a bit useless if everybody just leverage openAI (or Google) LLM. I mean why would companies ever need any other AI if they can all just pay google or microsoft to do the AI-related jobs for them?

And even when it comes to data reasoning, AI will be able to do that job much faster as compared to data scientists. If you check azure openAI landing page, they literally mentioned code generation and data reasoning as their selling points.

Thoughts?"
46,10p6zj6,datascience,Open-AI,relevance,2023-01-30 16:25:42,Open AI,Suspicious-Win-2889,False,0.33,0,https://medium.com/p/2730fbca7a43,0,1675095942.0,
47,134o5fe,datascience,Open-AI,relevance,2023-05-01 14:33:50,"[colabdog.com] I build an aggregator for AI News that combines OpenAI, Google AI, MIT, BAIR.",colabDog,False,0.75,4,https://i.redd.it/rlizo23od8xa1.png,1,1682951630.0,
48,123nr31,datascience,Open-AI,relevance,2023-03-27 14:00:59,Is object recognition now a trivial task because of OpenAI?,throwitfaarawayy,False,0.25,0,https://www.reddit.com/r/datascience/comments/123nr31/is_object_recognition_now_a_trivial_task_because/,16,1679925659.0,"I'm working on a project where we are tasked with classifying different types of vehicles. I am thinking that now because of OpenAI models especially GPT-4 with vision, this is now a redundant effort. In a few weeks to months this will be available to everyone and for really cheap. Then why am I building this?"
49,12jb54e,datascience,Open-AI,relevance,2023-04-12 05:21:27,Is OpenAI‚Äôs Study On The Labor Market Impacts Of AI Flawed?,LesleyFair,False,0.81,10,https://www.reddit.com/r/datascience/comments/12jb54e/is_openais_study_on_the_labor_market_impacts_of/,0,1681276887.0,"[Example img\_name](https://preview.redd.it/wzz3wtwu1eta1.png?width=1451&format=png&auto=webp&s=9a10cc08b28effc9cbda57b43d625bfcc5c03be2)

We all have heard an uncountable amount of predictions about how AI will¬†***terk err jerbs!***

However, here we have a proper study on the topic from OpenAI and the University of Pennsylvania. They investigate how Generative Pre-trained Transformers (GPTs) could automate tasks across different occupations \[1\].

Although I‚Äôm going to discuss how the study comes with a set of ‚Äúimperfections‚Äù, the findings still make me really excited. The findings suggest that machine learning is going to deliver some serious productivity gains.

People in the data science world fought tooth and nail for years to squeeze some value out of incomplete data sets from scattered sources while hand-holding people on their way toward a data-driven organization. At the same time, the media was flooded with predictions of omniscient AI right around the corner.

*Let‚Äôs dive in and take an*¬†exciting glimpse into the future of labor markets\*!\*

# What They Did

The study looks at all US occupations. It breaks them down into tasks and assesses the possible level of for each task. They use that to estimate how much automation is possible for a given occupation.

The researchers used the¬†[O\*NET database,](https://www.onetcenter.org/database.html)¬†which is an occupation database specifically for the U.S. market. It lists 1,016 occupations along with its standardized descriptions of tasks.

The researchers annotated each task once manually and once using GPT-4. Thereby, each task was labeled as either somewhat (<50%) or significantly (>50%) automatable through LLMs. In their judgment, they considered both the direct ‚Äúexposure‚Äù of a task to GPT as well as to a secondary GPT-powered system, e.g. LLMs integrated with image generation systems.

To reiterate, a higher ‚Äúexposure‚Äù means that an occupation is more likely to get automated.

Lastly, they enriched the occupation data with wages and demographic information. This was used to determine whether e. g. high or low-paying jobs are at higher risk to be automated.

So far so good. This all sounds pretty decent. Sure, there is a lot of qualitative judgment going into their data acquisition process. However, we gotta cut them some slag. These kinds of studies always struggle to get any hard data and so far they did a good job.

However, there are a few obvious things to criticize. But before we get to that let‚Äôs look at their results.

# Key Findings

The study finds that 80% of the US workforce, across all industries, could have at least some tasks affected. Even more significantly, 19% of occupations are expected to have at least half of their tasks significantly automated!

Furthermore, they find that higher levels of automation exposure are associated with:

* Programming and writing skills
* Higher wages (contrary to previous research!)
* Higher levels of education (Bachelor‚Äôs and up)

Lower levels of exposure are associated with:

* Science and critical thinking skills
* Manual work and tasks that might potentially be done using physical robots

This is somewhat unsurprising. We of course know that LLMs will likely not increase productivity in the plumbing business. However, their findings underline again how different this wave is. In the past, simple and repetitive tasks fell prey to automation.

*This time it‚Äôs the suits!*

If we took this study at face value, many of us could start thinking about life as full-time pensioners.

But not so fast! This, like all the other studies on the topic, has a number of flaws.

# Necessary Criticism

First, let‚Äôs address the elephant in the room!

OpenAI co-authored the study. They have a vested interest in the hype around AI, both for commercial and regulatory reasons. Even if the external researchers performed their work with the utmost thoroughness and integrity, which I am sure they did, the involvement of OpenAI could have introduced an unconscious bias.

*But there‚Äôs more!*

The occupation database contains over 1000 occupations broken down into tasks. Neither GPT-4 nor the human labelers can possibly have a complete understanding of all the tasks across all occupations. Hence, their judgment about how much a certain task can be automated has to be rather hand-wavy in many cases.

Flaws in the data also arise from the GPT-based labeling itself.

The internet is flooded with countless sensationalist articles about how AI will replace jobs. It is hard to gauge whether this actually causes GPT models to be more optimistic when it comes to their own impact on society. However, it is possible and should not be neglected.

The authors do also not really distinguish between labor-augmenting and labor-displacing effects and it is hard to know what ‚Äúaffected by‚Äù or ‚Äúexposed to LLMs‚Äù actually means. Will people be replaced or will they just be able to do more?

Last but not least, lists of tasks most likely do not capture all requirements in a given occupation. For instance ""making someone feel cared for"" can be an essential part of a job but might be neglected in such a list.

# Take-Away And Implications

GPT models have the world in a frenzy - rightfully so.

Nobody knows whether 19% of knowledge work gets heavily automated or if it is only 10%.

As the dust settles, we will begin to see how the ecosystem develops and how productivity in different industries can be increased. Time will tell whether foundational LLMs, specialized smaller models, or vertical tools built on top of APIs will be having the biggest impact.

In any case, these technologies have the potential to create unimaginable value for the world. At the same time, change rarely happens without pain. I strongly believe in human ingenuity and our ability to adapt to change. All in all, the study - flaws aside - represents an honest attempt at gauging the future.

Efforts like this and their scrutiny are our best shot at navigating the future. Well, or we all get chased out of the city by pitchforks.

Jokes aside!

What an exciting time for science and humanity!

As always, I really enjoyed making this for you and I sincerely hope you found value in it!

If you are not subscribed to the newsletter yet,¬†[click here to sign up](https://thedecoding.net/)! I send out a thoughtful 5-minute email every week to keep you in the loop about machine learning research and the data economy.

*Thank you for reading and I see you next week ‚≠ï!*

**References:**

\[1\]¬†[https://arxiv.org/abs/2303.10130](https://arxiv.org/abs/2303.10130)"
50,133zi37,datascience,Open-AI,relevance,2023-04-30 18:38:55,Why does the OpenAI key expire so fast?,vm123313223,False,0.5,0,https://www.reddit.com/r/datascience/comments/133zi37/why_does_the_openai_key_expire_so_fast/,2,1682879935.0,It seems the OpenAI key will expire on 1st May UTC. Any way around that?
51,11is6oq,datascience,Open-AI,relevance,2023-03-05 08:48:09,Beating OpenAI CLIP in Image retrieval with 100x less data and compute,vov_or,False,0.92,83,https://www.reddit.com/r/datascience/comments/11is6oq/beating_openai_clip_in_image_retrieval_with_100x/,7,1678006089.0,"Hello from the Unum AI team! We have been silently pre-training numerous Multi-Modal Models for Semantic Search for the last year!  
We are releasing several extremely performant checkpoints on the [HuggingFace portal](https://huggingface.co/unum-cloud/uform)!  
In addition there is the blog post about efficient Vision-Language pre-training:  
[https://www.unum.cloud/blog/2023-02-20-efficient-multimodality](https://www.unum.cloud/blog/2023-02-20-efficient-multimodality)"
52,zlovfw,datascience,Open-AI,relevance,2022-12-14 11:46:22,"Optimization, zoo and OpenAI chatgpt",AlexFleischer2,False,0.33,0,https://www.linkedin.com/pulse/optimization-zoo-openai-chatgpt-alex-fleischer/,0,1671018382.0,
53,z9ba36,datascience,Open-AI,relevance,2022-12-01 02:21:12,OpenAI debuts ChatGPT: a conversational AI on GPT-3.5,Opitmus_Prime,False,0.6,1,https://ithinkbot.com/openai-debuts-chatgpt-50dd611278a4,0,1669861272.0,
54,102jnm2,datascience,Open-AI,relevance,2023-01-03 21:14:56,OpenAI has been blowing my mind. Anyone else?,Curious-Baby7671,False,0.13,0,https://www.reddit.com/r/datascience/comments/102jnm2/openai_has_been_blowing_my_mind_anyone_else/,2,1672780496.0,[https://medium.com/@davidsalmela/openai-review-how-ai-normalization-will-shape-2023-48201d809fe1](https://medium.com/@davidsalmela/openai-review-how-ai-normalization-will-shape-2023-48201d809fe1)
55,1097d5c,datascience,Open-AI,relevance,2023-01-11 15:26:10,Do you use OpenAI's API in production?,VarietyElderberry,False,0.5,0,https://www.reddit.com/r/datascience/comments/1097d5c/do_you_use_openais_api_in_production/,1,1673450770.0,"With the release of Dall-E and now ChatGPT, OpenAI has been getting a lot of attention. I expect that more and more companies are starting to use their API. I am considering doing so myself in my work, but have a few doubts that I am interested to hear your opinion on. Specifically, I am worried that OpenAI will retire the current `text-embedding-ada-002` model at some point in the future. When that happens, we need to switch to the newer model, but I doubt that the embedding vectors of the older and newer model will be aligned with each other. That would require a significant amount of work to align whatever layer you built to process the embedding vectors to adapt to the new model. This is less of an issue for generative models, such as ChatGPT and Dall-E, where switching to a newer model should not impact much of the rest of your application. But something as low-level of an embedding vector cannot be subject to regular change.

What are your insights into this? Would this prevent you from using the OpenAI API (or alternatives) in your work, or do you have trust in OpenAI that they wouldn't haphazardly change/retire their models?"
56,zcmlp0,datascience,Open-AI,relevance,2022-12-04 21:39:24,Unofficial Python SDK for OpenAI's ChatGPT,brunneis,False,0.83,14,https://github.com/labteral/chatgpt-python,1,1670189964.0,
57,10mi1x8,datascience,Open-AI,relevance,2023-01-27 10:52:18,‚≠ï What People Are Missing About Microsoft‚Äôs $10B Investment In OpenAI,LesleyFair,False,0.74,16,https://www.reddit.com/r/datascience/comments/10mi1x8/what_people_are_missing_about_microsofts_10b/,3,1674816738.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/35vtxrwnekea1.png?width=720&format=png&auto=webp&s=a61dd557e1d00c96448c429c9f9bb78516205a6f)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI‚Äôs models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let‚Äôs first look at Sam Altman‚Äôs backstory.

*Let‚Äôs go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI ‚Äî one of the most exciting and impactful organizations in all of tech.

However, OpenAI ‚Äî the rocket ship of AI innovation ‚Äî is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn‚Äôt take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can‚Äôt they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT‚Äôs outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That‚Äôs gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI‚Äôs profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI‚Äôs API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don‚Äôt have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman‚Äôs foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft‚Äôs sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI ‚Äî whatever that looks like ‚Äî the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ‚≠ï.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)‚Äã

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)‚Äã

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)‚Äã

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)‚Äã

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)‚Äã

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
58,1168sew,datascience,Open-AI,relevance,2023-02-19 12:06:24,"Google's Response To OpenAI and ChatGPT is Coming, And They've Named It...",Kolownik,False,0.1,0,https://www.technews.city/2023/02/googles-response-to-openai-and-chatgpt.html?=read,5,1676808384.0,
59,yqocx2,datascience,Open-AI,relevance,2022-11-09 16:48:59,"Is AGI, as defined by Sam Altman from OpenAI, a real possibility in the near future?",deepfuckingbass,False,0.63,5,https://www.reddit.com/r/datascience/comments/yqocx2/is_agi_as_defined_by_sam_altman_from_openai_a/,14,1668012539.0,"I saw [this interview](https://m.youtube.com/watch?v=WHoWGNQRXb0) with Sam Altman, CEO of OpenAI, and I was surprised to hear several bold claims about the capabilities of AGI in the near future. He defines AGI as an AI/ML model with human-level intelligence. He seems to imply that current neural network architectures and techniques will get us there. At one point there‚Äôs an aside about whether we (humans) should still have kids with AGI an inevitability.

I‚Äôm struggling to understand how he can make the leap from where we are today to this sci-fi-like AGI future he describes. I‚Äôm very impressed by the work at OpenAI, so maybe Sam has access to tech that most practitioners in data science haven‚Äôt seen yet. With that being said, his interview rang a couple hype alarm bells with me.

What am I missing? Is AGI really around the corner?"
60,zcbvqf,datascience,Open-AI,relevance,2022-12-04 15:01:28,OpenAI ChatGPT and DaVinci-003 experiments by me,Opitmus_Prime,False,0.29,0,https://www.reddit.com/r/datascience/comments/zcbvqf/openai_chatgpt_and_davinci003_experiments_by_me/,0,1670166088.0,"I did some experiments with both chatGPI and GPT-3 davinci release 003. The answers by AI are really impressive! Give it a read 

[https://ithinkbot.com/openai-debuts-chatgpt-50dd611278a4](https://ithinkbot.com/openai-debuts-chatgpt-50dd611278a4)

[https://pub.towardsai.net/openai-just-released-gpt-3-text-davinci-003-i-compared-it-with-002-the-results-are-impressive-dced9aed0cba](https://pub.towardsai.net/openai-just-released-gpt-3-text-davinci-003-i-compared-it-with-002-the-results-are-impressive-dced9aed0cba)"
61,126ndlu,datascience,Open-AI,relevance,2023-03-30 13:41:51,Seeing a lot of job openings for high-level AI and Data Analytics positions...,fingin,False,0.79,27,https://www.reddit.com/r/datascience/comments/126ndlu/seeing_a_lot_of_job_openings_for_highlevel_ai_and/,20,1680183711.0,"I have noticed an uptick in jobs for things like prompt engineer, AI ethics lead, AI manager. When you look at these requirements it looks like relatively low entry: a familiarity with general AI and AI regulations (not that there is a ton of expertise to be had in this latter category). They don't require much or any technical skill. 

I'll admit, I find myself frustrated as I work in a highly technical role and feel like these opportunities are really 'low hanging fruit', due to the vagueness of the requirements. I'm sure many of us wear not just technical hats but also those of product management, coaching and training, etc. 

What do you think? Is it just a fad stemming from ChatGPT and Image gen promotion? Are you going to make a job switch and apply for these roles?"
62,13ea5ru,datascience,Open-AI,relevance,2023-05-11 02:04:31,"Finally acquired Azure/OpenAI enterprise product list, what they are and detailed product costs, post here if anyone is interested :)",digital-bolkonsky,False,0.5,0,/r/OpenAIDev/comments/13dturq/finally_acquired_azureopenai_enterprise_product/,0,1683770671.0,
63,11l3r81,datascience,Open-AI,relevance,2023-03-07 16:23:54,"We tracked mentions of OpenAI, Bing, and Bard across social media to find out who's the most talked about in Silicon Valley",yachay_ai,False,0.5,0,https://www.reddit.com/r/datascience/comments/11l3r81/we_tracked_mentions_of_openai_bing_and_bard/,0,1678206234.0,"[Posts about OpenAI, Bing, and Bard in the San Francisco Bay Area and Silicon Valley](https://preview.redd.it/fnhre7h3fcma1.png?width=1286&format=png&auto=webp&s=1bd01c38f158752d9eaf058ab1f679fd4e9f73c7)

Have you been following the news on the conversational AI race? We used social media data and [geolocation models](https://github.com/1712n/yachay-public/tree/master/conf_geotagging_model) to find posts about OpenAI, Bing, and Bard in the Silicon Valley and San Francisco Bay Area for the last two weeks to see which one received the most mentions.

First, we filtered social media data with the keywords ""openai,"" ""bing,"" ""bard,"" and then we predicted coordinates for the social media posts by using our text-based geolocation models. After selecting texts which received a confidence score higher than 0.8, we plotted their coordinates as company logos on a leaflet map using Python and the folium library, restricting the map to the bounding box of the San Francisco Bay Area and Silicon Valley.

We analyzed over 300 social media posts and found that roughly 54.5% of the time, OpenAI was the most talked about. Bing made second place with around 27.2%, and then Bard came in last with 18.3%.

You can see the full map [here](https://1712n.github.io/yachay-public/maps/chatbots/).

OpenAI may be winning the AI race at the moment, but it's not the end yet. Let us know what other AI projects you're following, and we'll check them out."
64,ykybpj,datascience,Open-AI,relevance,2022-11-03 10:17:29,Testing OpenAI GPT3 in Airtable. Fine-tuning gpt3 even with a very small dataset (50 in this case) appears to work pretty well. Any interesting use cases that you'd recommend testing with?,igornefedovi,False,0.91,50,https://twitter.com/igornefedovi/status/1588032734315704320,0,1667470649.0,
65,zn1pi5,datascience,Open-AI,relevance,2022-12-16 00:49:54,Is it too late for me to get into tech?,iguesswhatevs,False,0.14,0,https://www.reddit.com/r/datascience/comments/zn1pi5/is_it_too_late_for_me_to_get_into_tech/,4,1671151794.0,"I‚Äôve been trying to get into tech. I‚Äôve been teaching myself Python through videos and dataquest. 

I do enjoy coding. My goal is to eventually maybe be a data engineer and then data scientist and potentially a ML engineer or something.

But then recently I‚Äôve been hearing more and more about open AI and chatGPT. Seems to be hitting the tech industry like a storm.

And it makes me wonder if maybe the time to get into tech has passed. The time when tech was seen as difficult and high paying maybe coming to an end in the next few years as that becomes more and more prevalent.

I work for a Fortune 500 company and during a townhall, even the senior management in IT talked about open AI and how they had personally used it. Seems like even company executives are taking notice in this. I can imagine it won‚Äôt be long before they begin to use that instead of tech workers. 

Even if the open AI isn‚Äôt super advanced right now. I can see it developing quite a bit in the next few years to a point where it can be just as effective as a programmer or software engineer"
66,130jy9r,datascience,Open-AI,relevance,2023-04-27 12:06:18,Create pictures from lyrics,PalmTurtle,False,1.0,1,https://www.reddit.com/r/datascience/comments/130jy9r/create_pictures_from_lyrics/,1,1682597178.0,"Hey Folks,
I want to create Images with an pretrained AI model out of song lyrics. I wanted to use DALL-E but the openAI Models doesn‚Äôt seem to be free with the API. Do you have some recommendations of libraries, which I can use? 
Google and ChatGPT doesn‚Äôt helped me with my search‚Ä¶

Thank you very much!"
67,12coioi,datascience,Open-AI,relevance,2023-04-05 15:46:14,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.56,1,https://www.reddit.com/r/datascience/comments/12coioi/do_we_really_need_100b_parameters_in_a_large/,2,1680709574.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back!

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
68,10fw1a3,datascience,Open-AI,relevance,2023-01-19 07:54:24,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.94,124,https://www.reddit.com/r/datascience/comments/10fw1a3/gpt4_will_be_500x_smaller_than_people_think_here/,14,1674114864.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/t6m0epzlgyca1.png?width=575&format=png&auto=webp&s=a5972941053f833e76fd3a5009fc68a61f9e5406)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That‚Äôs a *trillion* with a ‚Äút‚Äù.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI‚Äôs new brainchild will certainly be mind-bending and language models have been getting bigger ‚Äî fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let‚Äôs go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): ‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters‚Äù.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there‚Äôs a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community‚Äôs understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: ‚ÄúScaling Laws For Neural Language Models‚Äù.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind‚Äôs 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): ‚ÄúTraining Compute-Optimal Large Language Models‚Äù

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ‚Äã[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails‚Äã

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ‚≠ï, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,‚Ä¶ & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
69,11zk3ru,datascience,Open-AI,relevance,2023-03-23 13:16:27,Cheshire Cat - Open source layer on top of any language model (extendible via plugins),pieroit,False,0.5,0,https://www.reddit.com/r/datascience/comments/11zk3ru/cheshire_cat_open_source_layer_on_top_of_any/,0,1679577387.0,"&#x200B;

 \^.\_.\^

&#x200B;

The Cheshire Cat is an open source, customizable AI architecture:

&#x200B;

\- language model agnosatic (works with OpenAI, Cohere, HuggingFace models, custom)

\- long term memory

\- can use external tools (APIs, other models)

\- can ingest documents (.pdf, .txt)

\- 100% dockerized

\- extendible via plugins

&#x200B;

Waiting for you to try it out and contribute with tutorials, code, and whatever makes you happy

&#x200B;

\#opensource #artificialintelligence #cognitivecomputing #deeplearning #cheshirecat

&#x200B;

Tutorial:

&#x200B;

[https://www.youtube.com/watch?v=srsaYy0xmkc](https://www.youtube.com/watch?v=srsaYy0xmkc)

&#x200B;

Repo:

&#x200B;

[https://github.com/pieroit/cheshire-cat](https://github.com/pieroit/cheshire-cat)"
70,11qaizm,datascience,Open-AI,relevance,2023-03-13 14:00:48,"Open-ended questions, and do feel free to comment your opinion on the matter below. But do you think that the launch of increasingly ""Smart"" AIs will be seen in the future as akin to the beginning of the industrial revolution (AKA a turning point in history) ?",Oldthriftmaan,False,0.55,1,https://www.reddit.com/r/datascience/comments/11qaizm/openended_questions_and_do_feel_free_to_comment/,1,1678716048.0,"

[View Poll](https://www.reddit.com/poll/11qaizm)"
71,11xemex,datascience,Open-AI,relevance,2023-03-21 11:55:57,Large Language Models For Summarization,vm123313223,False,1.0,1,https://www.reddit.com/r/datascience/comments/11xemex/large_language_models_for_summarization/,0,1679399757.0,"How to get the results of OpenAI (GPT-3) for summarization with open source models?

Some models which I have tried are:

1) FLAN-T5

2) Pegasus

3) BART

4) GPT-J

5) FTAN--UL2

I have also implemented fewshot learning with these models."
72,124p2uv,datascience,Open-AI,relevance,2023-03-28 13:38:56,"How will companies go about integrating GPT into their ecosystem (databases, documents, websites, etc.)? Is this possible already or not yet?",KidzKlub,False,0.62,3,https://www.reddit.com/r/datascience/comments/124p2uv/how_will_companies_go_about_integrating_gpt_into/,20,1680010736.0,"I would love for GPT to have the context of our ecosystem and be able to ask it questions about our data, have it write code that works with the rest of our infrastructure, analyze documents that we have stored, and more. Would this involve training a bespoke model on a company's data? Will OpenAI offer enterprise solutions where they help set you up with a model that meets your needs? I'm curious for my own purposes, but also I think this would be a major way that companies will start using this technology in the near future."
73,128qiu9,datascience,Open-AI,relevance,2023-04-01 16:02:06,Smarty GPT v2 is out!,usc-ur,False,0.5,0,https://www.reddit.com/r/datascience/comments/128qiu9/smarty_gpt_v2_is_out/,0,1680364926.0,"The second stable version of our library is out. Feel free to check it out! More functionality, simpler to use, support to the official Open AI API (GPT4 included).

Feel free to share, comment, and create PR!

[https://github.com/citiususc/Smarty-GPT](https://github.com/citiususc/Smarty-GPT)"
74,11sp0yn,datascience,Open-AI,relevance,2023-03-16 09:21:40,Smarty-GPT: library of prompts/contexts (connected with Awesome Prompts Chat GPT),usc-ur,False,0.57,1,https://www.reddit.com/r/datascience/comments/11sp0yn/smartygpt_library_of_promptscontexts_connected/,1,1678958500.0,"This is a simple wrapper that introduces any imaginable complex context to each question submitted to Open AI API. The main goal is to enhance the accuracy obtained in its answers in a **TRANSPARENT** way to end users.

[https://github.com/citiususc/Smarty-GPT](https://github.com/citiususc/Smarty-GPT)"
75,zmyhve,datascience,Open-AI,relevance,2022-12-15 22:38:06,Text to SQL,ljh78,False,0.6,1,https://www.reddit.com/r/datascience/comments/zmyhve/text_to_sql/,7,1671143886.0,"Hi!

Looking to start a project to build an in-house text to SQL (which I then use to query a relational DB) tool for some of the engineers (not software...no experience with SQL). Have any of you folks done something of the sort before and would be able to recommend a decent starting point? For the starter version it'd be very simple.. most likely one dataset and simple questions (i.e. no joins, only SELECT FROM WHERE).  

I have heard of OpenAI's GPT-3, but am not looking to ask my org to pay for any third party systems at the moment (still very much an exploratory project). Additionally, I'm looking to further my experience and expertise in DS/AI, so this might be a good way to do just that (as far as NL processing goes).

Thank you!"
