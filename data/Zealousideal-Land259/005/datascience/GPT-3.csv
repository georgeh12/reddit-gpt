,id,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,url,num_comments,created,body
0,13ilm03,datascience,GPT-3,top,2023-05-15 21:45:36,I investigated the Underground Economy of Glassdoor Reviews,ibsurvivors,False,0.99,1163,https://www.reddit.com/r/datascience/comments/13ilm03/i_investigated_the_underground_economy_of/,63,1684187136.0,"Online company reviews are high stakes.

Top reviews on sites like Glassdoor and Google can get thousands of impressions each month and are major drivers of brand perception.

Employers know this. And when I come across multiple 5 star reviews left with no cons, or a Pulitzer worthy essay from a former intern, I become suspicious.

These reviews start to resemble 30 under 30 lists: so artificially constructed that you begin to question their credibility in the first place.

The scrutiny around company reviews is well documented; some companies file lawsuits worth over a million dollars to reveal anonymous reviewers that complain about their jobs.

Whilst it's the flashy lawsuits that make the headlines, there also exists an underground economy of company reviews operating quietly every single day.

In this underground economy, some companies pay over $150 to freelancers to try and get a negative review removed. If they want “better” results, they go to the plethora of Online Reputation Management services (ORMs) in the United States that can charge retainers worth thousands of dollars.

The supply of positive reviews exists too. My research led me to find companies, including a prominent Y-Combinator backed startup, that solicit fake positive reviews from online freelancers to improve their rating.

Many of these mercenary fake reviewers, often based in South East Asia, make a full time living doing this, netting over $2,000 per month.

Some of these run such sophisticated operations that they’ve even created their own pricing tiers (e.g $35 per original review, $20 to post an already created review from an email address), a la SaaS offering.

Others operate on a contingency fee agreement model, where they only get paid if they’re able to take a negative review down.

The underground economy of company reviews is well and truly alive. And today we’re going to find out how it operates.

***Note***: For more content like this, [*subscribe*](https://www.careerfair.io/subscribe) *to my newsletter. In a couple of weeks, I'll be releasing my guide to writing a killer resume.*

**Adding reviews**

The barriers to entry for adding fake reviews are much lower than for getting reviews removed, so that’s where we’ll start.

To write an employer review, all you really need is the ability to create an email address. For most sites, you don’t need any proof of employment (say like a company specific email address).

I went on a gig marketplace site and posted a pretty vague post related to wanting to find out more on how to improve a company’s online presence.

Within minutes of posting a gig, my inbox was flooded with proposals:

https://preview.redd.it/esx3904qa20b1.png?width=3064&format=png&auto=webp&s=2ff3a2f8528fee99aabb830f27ea71a7569ebb2e

After a bit of chatting, I narrowed the scope of their services and summarized their rates into the table below:

|Channel|Cost|Timeline|Model|
|:-|:-|:-|:-|
|Freelancer #1|$10 per review|Monthly|Unlimited|
|Freelancer #2|$35 per original review, $20 per already created review|Monthly|Unlimited|
|Freelancer #3|$25 per review|Monthly|Unlimited|
|Freelancer #4|$25 per review|Monthly|10 reviews|
|Freelancer #5|$20 per review|Monthly|Unlimited|
|Online Reputation Management Agency|$300 subscription|Monthly|8 reviews|

Let’s dive a bit deeper into the services that Freelancer #5 offered.

Freelancer #5 explained to me he had been writing reviews for one particular company for the past 4 months now. Each month he wrote them 10 reviews.

&#x200B;

https://preview.redd.it/n1ddox6cb20b1.png?width=2684&format=png&auto=webp&s=5c271d0eec4328cb78d7d2cb85dfffa3f9eb72f8

In another message, he tells me he’s offering the same services to 5 other companies. Doing some quick math:

5 companies x 10 reviews per company x $25 per review = $1,250 per month

Considering the average person in Pakistan earns $150 per month, that’s not bad change at all.

One of the companies that he’s offering his services to includes a Y-Combinator backed startup. I won’t name the company, but here’s what its average Glassdoor review rating distribution looks like:

https://preview.redd.it/2np5b6fdb20b1.png?width=2420&format=png&auto=webp&s=f8cafaa85453b0933a18eb5c30f931b3bb893c46

5 star reviews account for over 77% of the company’s total reviews. Obviously, no one is buying fake reviews that make them look bad.

But here’s the thing: freelancers are getting quite smart when it comes to writing reviews that don’t look too fishy. They tend to do this by spacing the reviews out (so that they don’t come in “spikes” – more on this later) and they also make sure that they’re not always leaving the “cons” section blank.

Don’t get me wrong, if you come across this company’s reviews, it’d be pretty easy to tell they’re quite strange. In fact, I can’t even post some screenshots here because it’d give the company away immediately.

But it would be challenging to conclude that the above company is buying reviews just by analyzing review volume and distribution without actually reading some of the reviews.

The same company is also buying reviews on Google Reviews.

Sidenote: I got curious about how he’s been writing 50 reviews from 50 different emails per month. Would he actually create 50 different email addresses? And what about the IP address – doesn’t Glassdoor flag multiple reviews from the same IP?

One of the freelancers answered my question:

&#x200B;

https://preview.redd.it/g4id2yqeb20b1.png?width=2572&format=png&auto=webp&s=c2a77fdea8834a6d90f02b8b3eb67b3a874f3df2

Moving on – another company that seems to buy fake reviews seems to be having some more trouble. Approximately a month after a freelancer linked me to fake reviews he had written for this company, all five reviews that he had linked me to had been removed:

&#x200B;

https://preview.redd.it/99fdvcgfb20b1.png?width=3116&format=png&auto=webp&s=b7e244529fc62b5c824d925feb61fd2cc16cbfd5

Based on this [Glassdoor webinar](https://youtu.be/3iy0JWOS1gs) from 2018, “if it is found that a user has created multiple email accounts to submit reviews, then ALL submissions from that user are deleted” – so likely Glassdoor’s content moderation team flagged one of the initial reviews and the same freelancer who was writing reviews for that company had all the fake reviews deleted.

So far, it looks like the key to an effective fake review creation strategy lies in:

* Spacing the fake reviews out
* Writing each review from a different IP address (i.e benefit of being part of a team)
* Using language that isn’t an obvious giveaway

On that third point: the reality is that many of these freelancers’ first language is not English.

As an experiment, I turned to everybody’s favorite new toy, ChatGPT, and asked it to write me a positive Glassdoor review:

https://preview.redd.it/8w7cal9gb20b1.png?width=3164&format=png&auto=webp&s=860c39b11c5813e8b7fabdbb038d73c565cc98cf

And I’d say that the above answer was better than 95% of the fake reviews I came across.

**Removing reviews**

The process for removing an employer review usually works like this:

1. You identify one or multiple reviews that you want removed
2. You verify whether the review violates the site's Guidelines, or whether there’s something else about the review(s) that could get it removed.
3. You file an appeal to get it removed.

As an example, Glassdoor’s Review guidelines can be found [here](https://help.glassdoor.com/s/article/Community-Guidelines?language=en_US#:~:text=See%20More-,Review%C2%A0Guidelines,-Millions%20of%20job). Mainly, they forbid mentioning anyone by name who’s not an executive and revealing proprietary or confidential information, amongst a host of other things.

Sounds simple enough right? Well, according to one of the freelancers I messaged:

&#x200B;

https://preview.redd.it/x6s8hsyac20b1.png?width=2036&format=png&auto=webp&s=f86c386f864198dc43faeb41faea378090c20107

After some research, I summarized the different vendors and prices in the table below:

&#x200B;

|Channel|Cost|Timeline|Model|Self reported success rate|
|:-|:-|:-|:-|:-|
|Freelancer #1|$100 per review|3 days|Contingency Agreement Model|100%|
|Freelancer #2|$30 per review|7 days|Contingency Agreement Model|100%|
|Reputation management service #2|$450 per review|21 business days|Contingency Agreement Model|Unknown|
|Reputation management service #3|$1000 per review|Undefined|Contingency Agreement Model|100%|
|Reputation management service #4 Plan 1|$550 per review|5-6 weeks|Contingency Agreement Model|50-75%|
|Reputation management service #4 Plan 2|$300 Subscription + $100 per each review removed|Monthly service|Subscription plan|50-75%|
|Freelancer #3|$20|Undefined|Pay regardless|Undefined|
|Freelancer #4|$500|Undefined|Contingency Agreement Model|Undefined|

As you can see, unlike the fake review generation market, the prices vary quite a bit for getting reviews removed.

At one end, you have freelancers on gig marketplaces that will attempt to remove a review for less than $100. And then on the other end, you have ORMs (Online Reputation Management Agencies) that have multiple employees and more comprehensive packages in place. The one constant seems to be that most companies operate on a contingency agreement model (i.e pay only if review gets removed).

**Analyzing reviews**

ReviewMeta is a site that analyzes Amazon reviews and tells you how many are legitimate. The creator of the site, Tommy Noonan, mentions in an [interview with NPR](https://www.npr.org/sections/money/2018/06/27/623990036/episode-850-the-fake-review-hunter) that the main giveaway that a product is soliciting fake reviews is:

* A large, suspicious flood of positive reviews at the exact same time. For example, a 3 day stretch of time constituting 30% of total reviews.
* Phrases and words that are constantly repeated, especially in the section with no cons
* Brand monogamists (only review products from one company)

Whilst the last two bullets are hard to track, the first can be used to analyze different companies’ reviews and to check if there might be some funky business going on.

After a couple of days, I have the ability to track review volume and review ratings over time for any company that I specify:

https://preview.redd.it/ehcbw2oje20b1.png?width=1653&format=png&auto=webp&s=b448ff35eb9878fbb1686de2fa8cf031e4ed3e05

Let the games begin.

## Voluntary Response Bias

One of the biggest challenges that review platforms face is the Voluntary Response bias.

Research shows many of today’s most popular online review platforms (e.g Amazon) have a distribution of opinion that is highly polarized, with many extreme positive and/or negative reviews, and few moderate opinions.

Think about it: have you ever felt moderately satisfied at your job and thought to yourself, now would be a great time to leave a Glassdoor review? Probably not.

On the other hand, if you’ve had a terrible experience or even just had one thing really flip you off, you might be quite likely to leave an angry review.

Consider when a company goes through layoffs. You’re going to have a flood of angry reviews coming your way and are likely going to experience a “spike” in reviews.

**Note:** Just like the Wall Street Journal’s methodology described [here](https://archive.is/20201016094732/https://www.wsj.com/articles/companies-manipulate-glassdoor-by-inflating-rankings-and-pressuring-employees-11548171977#selection-3965.0-3968.0), I considered there to be a spike if the total number of reviews in the month was greater than three standard deviations above the mean of the surrounding months.

Let’s take the company below. Here’s a graph of of their review volume since Jan 2020, including when they announced one of their first round of layoffs in June 2022:

https://preview.redd.it/n6kd9ejle20b1.png?width=3216&format=png&auto=webp&s=9eea2f3836617feca37eb88b1d3f67c8fa1b6fe2

In June 2022, approximately 19% of this company's 52 reviews were 1 star reviews (compared to an overall average of around 10%). This is what we could call a statistically significant spike in reviews. It also illustrates how the employees most likely to leave reviews are the ones that obviously had a bad experience (i.e getting laid off).

Here’s another company that had a similar spike in negative reviews due to layoffs in November 2022:

https://preview.redd.it/4vcnr1ine20b1.png?width=2408&format=png&auto=webp&s=f3877fb315ccc5d9a9294306a9f86616cb0fabd2

This company had an approximate 20% 1 star review rate (compared to an overall average of 12%) in November 2022, as well as an Avg Rating of 2.96 that month (compared to an overall average rating of 3.73).Unless HR is proactive, their reviews page risks succumbing to an echochamber of negative reviews that can really tilt one way.

**Note:** Glassdoor does state (based on [this video](https://www.youtube.com/watch?v=3iy0JWOS1gs) from 2017) that about 75% of the reviews on their platform are neutral. Their “give to get policy” has helped in keeping the platform from becoming too polarized.

I can understand why HR teams, like the ones that Nader talked to me about earlier, take a proactive stance towards managing their reviews. If they don’t try to control their reputation themselves, then their reputation risks getting controlled by the employees that had the worst possible experience.

## Goodhart’s Law

Goodhart’s law states the following:

*""When a measure becomes a target, it ceases to be a good measure""*

Every October, Glassdoor publishes their Best Places To Work ranking.

In a [report](https://www.wsj.com/articles/companies-manipulate-glassdoor-by-inflating-rankings-and-pressuring-employees-11548171977) that the WSJ did a couple of years ago, they found large spikes in the number of reviews that some companies (e.g SpaceX, Bain & Co, etc) got in September. The logic here is that some companies try to artificially inflate their Glassdoor reviews right before the October deadline.

I decided to revisit some of this analysis with Glassdoor’s 2023 Best Places To Work Ranking.

One of the companies I examined is rated as one of the best places to work in 2023. Let’s refer to this company as FunPlaceToWork.

Here is how their review volume looks like for all of 2022:

https://preview.redd.it/4e656zkqe20b1.png?width=2516&format=png&auto=webp&s=07141a66c56be7a6818efb9b1a4d912ee0021c91

FunPlaceToWork got around 50 reviews in September 2022. Of those 50 reviews, 96% were 5 star reviews.

FunPlaceToWork averaged 12 reviews per month up till then in 2022. Also, in the prior six months, the average percent of 5 star reviews received every month was \~75%.

Both the spike in volume of reviews and the spike in percentage of five star reviews are statistically significant.

I find it strange that Glassdoor’s proprietary algorithm and/or Human Content Moderation team did not find a spike of this nature unusual. If we look at Glassdoor’s eligibility criteria for the award, it’s as follows:

https://preview.redd.it/hag04y7se20b1.png?width=2868&format=png&auto=webp&s=ec2b920e126a8ea42b40d35aaa55d5341e69d022

The goal, according to Glassdoor, is to collect “authentic and unbiased reviews”.

Whilst there’s nothing against the rules for asking your employees to leave you reviews, I find the statistically significant spike of reviews at odds with the goal of collecting ""unbiased and authentic"" reviews (which Glassdoor states is the purpose of the awards).

Glassdoor states that an employer is allowed to ask its employees to leave reviews, but that they are not allowed to “coerce” them. Examples of what you can’t do:

* Offer incentives like Gift Cards in exchange for positive reviews.
* Withholding their reference letter unless they leave you a positive review.
* Anything that leads you to require proof for the employee to show you that they wrote a review.

It is possible to play by the rules (i.e not break any of the above rules) and to still in my opinion not collect authentic and unbiased reviews.

They say that you shouldn’t hate the player but the game – I think **FunPlaceToWork** played by the rules, won fair and square, and that this is simply a perfect example of Goodhart’s Law.

I reached out to Glassdoor ([awards@glassdoor.com](mailto:awards@glassdoor.com)) about the above and this is the reply I got:

https://preview.redd.it/x0dqq39ue20b1.png?width=4800&format=png&auto=webp&s=c0102c963be9486370b340f2f473cbc6650fc48a

**Conclusion**

When I was 22, on an [F1 visa with 3 months to find work](https://www.careerfair.io/job-hunt-story), I didn’t give a damn about bad reviews. I needed a job and I’d sign any piece of paper you put in front of me.

Compare that to someone at the peak of their career, someone with optionality and a multitude of job offers; an “A-Player”, as the experts call it, would absolutely have the luxury of choice and discard a job offer based on bad company reviews.

For most people, the impact of online company reviews lies somewhere in the middle. In marketing, there’s a concept of a “marketing touchpoint” - an interaction with the brand over the course of the whole buying journey.

Company reviews are one of the many touchpoints a job seeker experiences over their interview process. And with the technology industry booming the past couple of years, companies couldn’t afford to slack on any touchpoints, including this one.

After all, when others start to game the system, you’re at a disadvantage if you don’t. The rewards can be quite high. Certainly higher than just trying to be as transparent as possible.

HR leaders are often more incentivized to inflate their metrics than to get honest feedback. Fake review writers have bills to pay. ORMs know that companies are desperate. And the platforms, well, aren’t always paying attention.

The result is a potluck of interests that leads to an underground economy.

One that ends up hurting the job seeker.

\*\*\*

Whew. That took a while (about 3 months in fact). Thanks for reading. For more content like this, [subscribe](https://www.careerfair.io/subscribe) to my newsletter. It's my best content delivered to your inbox once every 2 weeks."
1,zpraee,datascience,GPT-3,top,2022-12-19 13:15:11,The real reason ChatGPT was created,xdonvanx,False,0.94,734,https://i.redd.it/g5z2t4zeuu6a1.png,73,1671455711.0,
2,10fw1a3,datascience,GPT-3,top,2023-01-19 07:54:24,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.94,122,https://www.reddit.com/r/datascience/comments/10fw1a3/gpt4_will_be_500x_smaller_than_people_think_here/,14,1674114864.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/t6m0epzlgyca1.png?width=575&format=png&auto=webp&s=a5972941053f833e76fd3a5009fc68a61f9e5406)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
3,10a1mik,datascience,GPT-3,top,2023-01-12 14:59:04,New Research From Google Shines Light On The Future Of Language Models ⭕,LesleyFair,False,0.84,122,https://www.reddit.com/r/datascience/comments/10a1mik/new_research_from_google_shines_light_on_the/,18,1673535544.0,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs will allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* Stop hallucinating and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example are words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/w7xffqjimmba1.png?width=800&format=png&auto=webp&s=e2d9cb63f750efcbfa45c4bb7a985d4dcb5b0319)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.  
At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week.  
No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)"
4,11szca1,datascience,GPT-3,top,2023-03-16 16:45:29,"What is your digital workspace, tools, setup, etc. for ETL, research, production?",BreathAether,False,0.92,36,https://www.reddit.com/r/datascience/comments/11szca1/what_is_your_digital_workspace_tools_setup_etc/,16,1678985129.0,"I'm new to this and so I've been wanting to know what other people have been using to make their work feel as smooth as butter. Since I've been learning lots and not just the industry standard stuff, I wanted to share what little I found to be valuable which others may want to try. **The main goal of this post is to share, critique, and provide suggestions so that we can all find the setup we like most. I  am also looking for new, up and coming tech, and definitely not afraid to try new things!**

IDE: **VSCode with the Jupyter Notebook Extension**. What I like about it is that I can view data structures like series/dataframes in a table format by clicking the variable in the Jupyter: Variables pane at the bottom. I started with plain vanilla jupyter notebooks from Anaconda so this was pretty nice. I have seen demos that **Jupyter Lab** has something like this, so if anyone has used both VSCode's notebooks and used Lab, your input would be appreciated. I hear good things about **PyCharm and Spyder**. Some people also use **Google Collab, DataSpell, and DeepNote** but I don't know enough about it. I did play around with DeepNote, and it was very cool but I didn't feel compelled to switch (and you have to pay for it!). 

Tools:

* A code helper: A few months back I was googling everything and I would've listed **Stackoverflow**. I might actually use that occasionally, but these days I use **ChatGPT** and **Bing AI**. For more current info or news-based I'll use Bing AI since it uses live search results, and for information that is knowledge based I might use ChatGPT. ChatGPT saves conversations so it's great for exploring topics in depth and referencing that conversation later. For those who have used both, maybe you know what I'm talking about and can provide a better explanation as to which is better for what purpose.
* Software: **Excel** is an obvious one. For instance, if I have a huge dataset and I just want to delete out columns that I don't need with Ctrl+click to select, it's easier and quicker than copy + pasting or typing out each of the string column names I want to ""df.drop()"". Excel is great for quick and simple stuff. Some software I have been learning about are I guess what I would consider as no- or low-code data analytics platforms, such as **Alteryx, KNIME, and Orange**. These software let you practically run an entire ETL pipeline. I believe Alteryx and KNIME are the gold-standard in this category, and Orange is a ""lite"" version of the two and is available in Anaconda. I think these are pretty cool, and I personally haven't found a huge use case for them since I've been chugging away in my notebooks with Python, but I can see the value. Would love for someone to chime in on these tools and how they compare to manually doing stuff in code, especially for large datasets.
* Version Control: This is where I'm primarily lacking, but I know that **Github** is the go-to. I don't use this but I know that a ton of people do. I don't even know where to start to be honest. I usually just create a new .ipynb file for each analysis or phase of an ETL pipeline haha. I'm also not too aware of what other innovative tools for version control exist.
* Python Libraries: Besides the obvious stuff like Pandas/NumPy, MatplotLib/Seaborn, and your popular ML libraries, I've recently found out about this library called **Polars**. It's basically a Rust version of Pandas, and it's super powerful. Some operations that I've run, that would've taken hours with Pandas, took me minutes. But I've been hearing that **Pandas 2.0** which will be released some time this month, has been looking at using PyArrow dtypes (if I recall correctly) and the speed is comparable to Polars. I mean these two are FAST. Another contender is **DuckDB** but I think the new Pandas and Polars are still faster. I mostly use Pandas but if there is some heavy lifting, I'll swap the dataframe to a polars one with a quick function, run it with polars, then back to pandas.

Anyway, that's just some things I can immediately think of. Looking forward to your suggestions! Bonus points for anything new and innovative. Cheers.

https://preview.redd.it/qj2cywt1r4oa1.png?width=1920&format=png&auto=webp&s=c4e02f5fec0b3768df336c7a3f63cc382b3954a8"
5,107khox,datascience,GPT-3,top,2023-01-09 17:37:48,FYI: GPT-3 & Beyond (Stanford AI Webinar),itedelweiss,False,0.84,22,https://www.reddit.com/r/datascience/comments/107khox/fyi_gpt3_beyond_stanford_ai_webinar/,1,1673285868.0,"The field of Natural Language Processing (NLP) has seen tremendous progress. In particular, algorithms for Natural Language Understanding (NLU) are evolving and improving at a stunning pace. Systems that seemed like science fiction as recently as ten years ago are now commonplace. One such system is GPT-3 (Generative Pretrained Transformer 3), a state-of-the-art model capable of generating human-like text for a wide range of tasks, such as conversation modeling, summarization, and question answering.  

In this live webinar Stanford Professor Christopher Potts will discuss the significance and implications of recent Natural Language Understanding developments including GPT-3. He will outline the fundamental building blocks of these new systems and describe how we can reliably assess and understand them.

Topics will include:

- Technical capabilities, limitations, and applications of new Natural Language Understanding systems

- Analysis of the performance of GPT-3 on various language tasks

- Potential future developments in Natural Language Processing

DATE: January 18, 2023
 	

TIME: 11:00 AM - 12:00 PM (PST)


[Registration link](https://event.on24.com/wcc/r/4076653/D32B5B8B45C4099498D2555AB941504C?mkt_tok=MTk0LU9DUS00ODcAAAGJNpCzioiOp8O-K8z9UB050H0f-EbLnEk9bZVBNI2vmlidmSvXCca2T1T2-BFseDFQyKNr0wv1iH8RGEXMDOj_E1d47fWkanGqcP2AjCkwjYFtaGU)"
6,10mi1x8,datascience,GPT-3,top,2023-01-27 10:52:18,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.78,19,https://www.reddit.com/r/datascience/comments/10mi1x8/what_people_are_missing_about_microsofts_10b/,3,1674816738.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/35vtxrwnekea1.png?width=720&format=png&auto=webp&s=a61dd557e1d00c96448c429c9f9bb78516205a6f)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
7,10iue4e,datascience,GPT-3,top,2023-01-22 21:12:11,"I wrote a tiny library this morning for openai's (suddenly really good, really cheap) embeddings to search stuff - Semantic search, smarter replies w/ GPT-3, easier.",morganpartee,False,0.77,11,https://www.sensibledefaults.io/blog/chatgpt/easy-python-embeddings,14,1674421931.0,"I've had a few buddies in the community ask about something like this in the last week, and then a reader did today so I finally took the time to write it up. It's basically a generic interface to try embeddings, and it's less than a hundred lines of code. Built this so you can just authenticate and test without having to fuss with numpy, pinecone, etc.

The only thing you do is put dictionaries with a name and text key, then search your embeddings instance. We return anything you stick in there with it too for your programming pleasure - think dates, page numbers, file names, whatever.

You can just... Build chat bots that are pretty smart like this for next to nothing by reading in your docs or whatever. It's cheap and incredibly effective. I know next to nothing about how to preprocess data for this thing, I've tried different sizes of chunks, lines, sentences and haven't found a one size fits all solution (other than more gpt to summarize) but - just try it, it's cheap as hell and there's so much you can do. Like the rest of the current state of gpt - it just kinda works. Start with one to five sentences (or lines for code) and experiment!

For you proper data scientists (nerds!) - you can just get the numpy array out and do regular embeddings stuff with it. If you do, I'd love to hear about it! Clustering, classification, etc are suddenly super easy, and the raw data is stored in json so you can upload it to something better when you're ready.

I know it's small, but little abstractions make life better. Anything we can do to lower the barrier to experimentation is worth doing. Hope it helps someone!"
8,138ckjn,datascience,GPT-3,top,2023-05-05 06:07:33,The language in which GPT 3.5 communicates changes how it thinks.....,NoCartographer4725,False,0.64,12,https://www.reddit.com/r/datascience/comments/138ckjn/the_language_in_which_gpt_35_communicates_changes/,19,1683266853.0,"Seems like a new paper shows that GPT is more patient when talking in Mandarin and German vs when talking in English and Russian.

[https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=4437617](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4437617)

&#x200B;"
9,123yr9p,datascience,GPT-3,top,2023-03-27 20:09:45,ChatGPT and made up citations,its_the_llama,False,0.65,5,https://www.reddit.com/r/datascience/comments/123yr9p/chatgpt_and_made_up_citations/,10,1679947785.0,"Hopefully this hasn't been posted here before. I was using ChatGPT to try and find a few reviews to read on parameter search strategies in system biology. Helpfully, it returned a list of seminal papers: 

 

1. ""Parameter estimation and inference for differential equations"" by P.J. Haario et al. (2006). This review article discusses various methods for parameter estimation in differential equations, including Bayesian methods, gradient-based optimization, and particle filtering.
2. ""Parameter estimation in differential equations: a Bayesian perspective"" by S. Särkkä (2013). This review article focuses on Bayesian methods for parameter estimation in differential equations, including Markov chain Monte Carlo (MCMC) methods and sequential Monte Carlo (SMC) methods.
3. ""Optimization methods for parameter estimation in nonlinear differential equation models"" by C.L. Lawson et al. (2013). This review article discusses various optimization methods for parameter estimation in nonlinear differential equation models, including gradient-based methods, genetic algorithms, and particle swarm optimization.
4. ""Parameter estimation and sensitivity analysis in computational systems biology"" by D. Ge and J.J. Liu (2019). This review article provides an overview of various methods for parameter estimation and sensitivity analysis in computational systems biology, including optimization-based methods, Bayesian methods, and global sensitivity analysis.

Great, right? Except, **none of these papers actually exist**. The authors sound similar to people in the field (""P.J. Haario is probably ""inspired"" by Heikki Haario who's well known in the field, and Simo Särkkä is an actual author who's published on this), but the work does not exist. 

In hindsight, this makes sense considering how chatGPT works. It's still pretty interesting though, and I wonder how many people have turned in college assays with completely fabricated references."
10,13c8ewn,datascience,GPT-3,top,2023-05-08 23:21:30,I asked ChatGPT4 to do some stats modelling - it was okay…ish,AFL_gains,False,0.65,5,https://www.reddit.com/r/datascience/comments/13c8ewn/i_asked_chatgpt4_to_do_some_stats_modelling_it/,11,1683588090.0,"Hi guys ! There’s been some debate, especially on here, about the “future of data science” and “whose job is going to be taken” etc etc. Imo I don’t know the answer, but I think LLMs have definitely changed the landscape.

One of the really interesting things ChatGPT has unlocked is that people can now code without really knowing how to. I think if you already are familiar with coding, using ChatGPT to improve productivity is awesome. But if you’re just starting out and use it generate code you can’t explain, then I think you can get into lots of trouble. 

And I think this is especially true when there’s a mathematical modelling choice aspect to your code. My thought was that just because something works / compiles, doesn’t mean it’s a very good model and doesn’t mean that the explicit choices / assumptions make sense. This, of course, isn’t chatGPTs fault, it’s the users fault for not checking! 

Anyway, to investigate this point, I recently tested ChatGPT to write a Stan code (bayesian inference ) to predict premier league matches. My feeling was that the task simple enough for it to do an okay job, but not so generic it there’s a million examples on the internet.

I put the results on YouTube (link below), but in summary I found the following: 

1. ChatGPT made a decent model, but with some really weird choices. Eg It decided to use a normal distribution to model goal differences , where I think a Skellam would have been better. It also decided not to model the variance of this distribution , instead deciding that it was 1. Super weird!

2. It wasn’t able to rationalise about things like over parameterisation. The model it build had way too many parameters, unnecessarily. The idea of parsimony wasn’t really there. Maybe with better prompts it would have, but out of the box it made the model overly complex


3. Prompt engineering really makes a difference. I think with better prompts, the model Could have been better. There was even a point where I spotted an error and prompted chatGPT to fix it and it did! But again, this all relied on me being able to read Stan code and know what was good and bad. 


For me, I learnt that at least for tasks where lots of modelling choices need to be made, humans still beat GPT. But perhaps in the future, those that win will be the data scientists/ engineers that know what they are doing but are able to prompt GPT optimally to maximise their productivity boost.



The videos are here : 
Part 1: https://m.youtube.com/watch?v=4LTUYTxKuIk&t=66s&pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D
Part 2: https://m.youtube.com/watch?v=XjQpV6c9K5g&t=1s&pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D"
11,134o5fe,datascience,GPT-3,top,2023-05-01 14:33:50,"[colabdog.com] I build an aggregator for AI News that combines OpenAI, Google AI, MIT, BAIR.",colabDog,False,0.75,4,https://i.redd.it/rlizo23od8xa1.png,1,1682951630.0,
12,10mu9ru,datascience,GPT-3,top,2023-01-27 19:53:34,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,0.84,4,https://www.reddit.com/r/datascience/comments/10mu9ru/a_python_module_to_generate_optimized_prompts/,0,1674849214.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features 🚀

* 🧙‍♀️ NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* 🔨 Easily add one-shot, two-shot, or few-shot examples to the prompt
* ✌ Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* 💥 Custom examples and samples can be easily added to the prompt
* 💰 Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Examples](https://preview.redd.it/x232msli2nea1.png?width=1236&format=png&auto=webp&s=6071efdd8cb12801230af6572991ba8aaff1a9ec)

&#x200B;

https://preview.redd.it/5o5uqk3k2nea1.png?width=1398&format=png&auto=webp&s=96c25820a698a83dfeb0e8f7f37682d9d27c06cb"
13,12rgc2w,datascience,GPT-3,top,2023-04-19 04:12:07,"GPT-4, my best study buddy!",Somomi_,False,0.7,4,https://www.reddit.com/r/datascience/comments/12rgc2w/gpt4_my_best_study_buddy/,6,1681877527.0," 

Today I find several prompts which could be very helpful for active learning.

**1. Generate Multiple Choice Question**

*Topic: { }*

*Write 3 multiple choice question with 1 correct answer and 3 incorrect distractor answers and let me choose an answer. Later you should let me know if I got it right or wrong and provide me with explanation.*

**2. Generate General Question**

>*Topic: { }*  
*Write 2* *data scientist interview questions* *about this topic and let me answer them. Later you should let me know if I got it right or wrong and provide me with explanation.*

**3. Learning by Teaching**

>*Please act as a data scientist. I will tell you what I l*  
*earn today and you can point out if I miss any step or made any mistake.*  
*Today I learn { }*

You can check my original post with example image here! Thank you!

[https://www.kaggle.com/code/kuixizhu/gpt-4-my-best-study-buddy](https://www.kaggle.com/code/kuixizhu/gpt-4-my-best-study-buddy)"
14,114qfkp,datascience,GPT-3,top,2023-02-17 16:14:47,I am worried about the future of working as a Data Scientist in industry,Slumi,False,0.51,3,https://www.reddit.com/r/datascience/comments/114qfkp/i_am_worried_about_the_future_of_working_as_a/,71,1676650487.0,"I once watched a video where someone argued that Michael Jordan had ruined basketball. As a person whose only notions of basketball come from Space Jam, this surprised me. Wasn't the guy the best out there? How could someone who's best in his field ruin it? After hearing the argument out though, I began to understand what the video was really getting at: to them, Michael Jordan had ruined the field BECAUSE he was so good. A sport that was once full of diverse personalities and strategies then turned into multiple teams trying to produce their own Michael Jordan copycat with copycat moves and copycat strategies.

I'm not well versed enough in basketball to know how right or wrong that guy was. But this concept that something could ruin an entire field simply by being too good stuck with me. And all these years later, I consistently think about it while doing my job as a Data Scientist. And the more GPT-X and ChatGPT advance, the more this concept haunts me.

I've been working in this field for 5 years. During the first 4 years, the job was exciting. Every new problem required extensive research on my part: looking for papers, datasets, implementations, implementing my own stuff, experimenting, comparing the results... That was the biggest part of my job. Yeah I still had to do some software engineering stuff here and there, but I felt like I had a well defined and specialized role at my company. Even in the NLP field, the variety of technologies I had to work with was a lot of fun: sometimes I used RNNs, sometimes embedding based similarity functions, sometimes more classic approaches, and then towards the end it was transformer after transformer, but at least I still had to finetune them myself.

Over the past few months and years, this has started to change, however. GPT-1 and 2 were promising, but were more of a proof of concept than anything realistically usable. But with GPT3's and ChatGPT's latest performance, it feels like NLP is becoming more and more standardized. I wouldn't call it a solved field, far from it. But, sadly, unless you work for a big tech company,  I think the days of exploratory research-type work in the NLP field are over.

Even in academia, the latest NLP papers I've seen come out of prestigious universities went from actual engineering/mathematical advances to ""Look, if you ""engineer"" the prompt like this, it works better sometimes!""

And now, with the exploding popularity of ChatGPT, any random lambda would come to the conclusion that if you need to use AI, ChatGPT is the answer.

Now, some of you may already be thinking ""but hold on, with my finetuned transformer I get a 1% increase in accuracy over out-of-the-box GPT-3!"". The thing is: even if you can, it doesn't really matter. I really doubt the sales team of your company is gonna pay much attention to your pleas to give you 6 months to work on a product rather than 6 minutes. Because yes, that's the increase in productivity we're dealing with: What took me months a few years ago only takes minutes now. The range of problems I can apply NLP too has also increased, as out-of-the-box GPT-3 works well enough not to require a dataset for more ""general"" tasks. And the results are either comparable or even better.

In the span of a year, NLP went from my favorite ML field to one I never see myself working in again if given the choice. And while a lot of non NLP fields are, for now, untouched by the exploding popularity of the GPT family, I'm afraid that what happened to the NLP field will be replicated in other fields, and that in a few years, only super specialized issues won't have a solution somewhere in the form of a ridiculously big model accessible via an API key provided by a big tech company. And even if GPT3 was open source, the sheer size of it makes it close to impossible to realistically train for anything less than the biggest of tech companies out there. Even finetuning it would be a challenge.

I'm not saying the job of data scientist will disappear, in fact, I'd say it will be asked more than ever since boomers who don't understand what the title actually implies will want some in their company. But we won't be doing nearly as much actual data sciency stuff as we did in the past. Instead we'll have to become even bigger software engineer/data analyst/DevOps/project manager hybrids than we already were. The only exception is for people who either work in big tech companies or on very specialized problems that require an in-house model.

It's not that much of a problem for people who like the roles I listed above. But for people like me, whose enjoyment came out of the exploratory side of the job, I believe the future to be bleak."
15,11vdjat,datascience,GPT-3,top,2023-03-19 06:27:43,datasetGPT - A command-line tool to generate datasets by inferencing LLMs at scale. It can even make two ChatGPT agents talk with one another.,radi-cho,False,0.83,4,https://www.reddit.com/r/datascience/comments/11vdjat/datasetgpt_a_commandline_tool_to_generate/,0,1679207263.0,"GitHub: [https://github.com/radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT)

It can generate texts by varying input parameters and using multiple backends. But, personally, the conversations dataset generation is my favorite: It can produce dialogues between two ChatGPT agents.

Possible use cases may include:

* Constructing textual corpora to train/fine-tune detectors for content written by AI.
* Collecting datasets of LLM-produced conversations for research purposes, analysis of AI performance/impact/ethics, etc.
* Automating a task that a LLM can handle over big amounts of input texts. For example, using GPT-3 to summarize 1000 paragraphs with a single CLI command.
* Leveraging APIs of especially big LLMs to produce diverse texts for a specific task and then fine-tune a smaller model with them.

What would you use it for?"
16,zrr7zz,datascience,GPT-3,top,2022-12-21 15:41:57,Sample Peyote: generate multi-table synthetic data on any topic using GPT-3,abegong,False,0.63,2,/r/datasets/comments/zrr2yr/sample_peyote_generate_multitable_synthetic_data/,0,1671637317.0,
17,11xemex,datascience,GPT-3,top,2023-03-21 11:55:57,Large Language Models For Summarization,vm123313223,False,1.0,1,https://www.reddit.com/r/datascience/comments/11xemex/large_language_models_for_summarization/,0,1679399757.0,"How to get the results of OpenAI (GPT-3) for summarization with open source models?

Some models which I have tried are:

1) FLAN-T5

2) Pegasus

3) BART

4) GPT-J

5) FTAN--UL2

I have also implemented fewshot learning with these models."
18,11fpp9o,datascience,GPT-3,top,2023-03-02 02:08:10,"Working on a report for school, can someone answer a few ethics related data science questions for me.",VelvetRevolver_,False,0.6,1,https://www.reddit.com/r/datascience/comments/11fpp9o/working_on_a_report_for_school_can_someone_answer/,5,1677722890.0,"I have a report for school where I need to ask a data science professional some questions and then write a report on it. You don't have to answer all of them and the responses don't have to be that long they can be on the shorter side. Any help would be greatly appreciated. Thank you.

1) As AI advances do you see any possible risk of mass job displacement in the future? Such as self driving cars replacing truck drivers, taxi drivers, etc. Do you think this will be a problem in the future or will it be the same as all the times we've automated something in the past and people will just adapt to the job market?


2) Recently there has been some controversy around algorithmic bias, especially in data science. Such as automating parole decisions, job application screening, or applying for credit cards. Some people argue these machine learning algorithms have been somewhat racist/sexist in the past. Do you think data scientists should be doing their best to eliminate bias?


3) With the recent release of chat bots like chatGPT students have been using it to help them write papers or help with their programming assignments. Do you see anything wrong with claiming chatGPT's responses as your own work? Do you consider it plagiarism?


4) Art generation has gotten very popular recently and there have actually been some lawsuits against these AI's claiming they were trained on copyrighted images. Do you believe AI shouldn't be legally allowed to train on copyrighted material or will doing so significantly hinder AI's development as data scientists will have to be much more careful about how they obtain their datasets?


5) As AI and image detection/recognition algorithms become more withspread do you think adversarial attacks will ever be a significant and practical problem?"
19,123jgeh,datascience,GPT-3,top,2023-03-27 11:00:57,Name classification with ChatGPT: How does it compare to ML language models?,tabacof,False,0.57,1,https://www.reddit.com/r/datascience/comments/123jgeh/name_classification_with_chatgpt_how_does_it/,4,1679914857.0,"In this [blog post](https://tabacof.github.io/posts/name_classification/name_classification.html), I explore the problem of name classification with ChatGPT and 3 ML models of increasing complexity (logistic regression, FastAI LSTM, and HuggingFace DistilBERT).

ChatGPT delivers the best accuracy of them all with no model training, just prompt engineering. It classifies 100k names in 18 minutes for under $5.

We see a lot of ChatGPT chat examples, but here I show how to use its API to solve an actual text classification problem (albeit a simple one).

GPT is transforming tasks that required deep machine learning knowledge into software + prompt engineering problems. As a data scientist, I’m not worried about it taking over my job, as predictive modelling is only a small aspect of what a data scientist does.

Feel free to share any feedback or questions."
20,11mad6m,datascience,GPT-3,top,2023-03-08 22:30:57,Is it worth using LLMs like GPT-3 for text classification?,pgalgali,False,0.6,1,https://www.reddit.com/r/datascience/comments/11mad6m/is_it_worth_using_llms_like_gpt3_for_text/,5,1678314657.0,"Hello, I am new to LLMs world. I am curious to know whether it is worth using LLMs for text classification problems or they are better suited for text generation use cases?"
21,z9ba36,datascience,GPT-3,top,2022-12-01 02:21:12,OpenAI debuts ChatGPT: a conversational AI on GPT-3.5,Opitmus_Prime,False,0.6,1,https://ithinkbot.com/openai-debuts-chatgpt-50dd611278a4,0,1669861272.0,
22,120rm54,datascience,GPT-3,top,2023-03-24 17:32:45,How-to-Fine-Tune GPT-3-Model-for-Named-Entity-Recognition,Lilith-Smol,False,1.0,1,https://ubiai.tools/blog/article/How-to-Fine-Tune-GPT-3-Model-for-Named-Entity-Recognition,0,1679679165.0,
23,zaf4bs,datascience,GPT-3,top,2022-12-02 07:35:37,"Cross Entropy, Explained by GPT-3",Ill-Tomato-8400,False,1.0,1,https://www.reddit.com/r/datascience/comments/zaf4bs/cross_entropy_explained_by_gpt3/,0,1669966537.0,"I recently experimented with OpenAI's new Chat GPT3 and used it to generate an explanation of cross entropy. It's impressive what language models are capable of, and it's daunting to imagine what they'll be able to do in the future.

&#x200B;

[https://gradiently.io/cross-entropy-explained-by-gpt3/](https://gradiently.io/cross-entropy-explained-by-gpt3/)

&#x200B;

&#x200B;"
24,12coioi,datascience,GPT-3,top,2023-04-05 15:46:14,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.5,0,https://www.reddit.com/r/datascience/comments/12coioi/do_we_really_need_100b_parameters_in_a_large/,2,1680709574.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back!

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
25,zmyhve,datascience,GPT-3,top,2022-12-15 22:38:06,Text to SQL,ljh78,False,0.6,1,https://www.reddit.com/r/datascience/comments/zmyhve/text_to_sql/,7,1671143886.0,"Hi!

Looking to start a project to build an in-house text to SQL (which I then use to query a relational DB) tool for some of the engineers (not software...no experience with SQL). Have any of you folks done something of the sort before and would be able to recommend a decent starting point? For the starter version it'd be very simple.. most likely one dataset and simple questions (i.e. no joins, only SELECT FROM WHERE).  

I have heard of OpenAI's GPT-3, but am not looking to ask my org to pay for any third party systems at the moment (still very much an exploratory project). Additionally, I'm looking to further my experience and expertise in DS/AI, so this might be a good way to do just that (as far as NL processing goes).

Thank you!"
26,10987al,datascience,GPT-3,top,2023-01-11 16:01:02,Can GPT-J be used for text summarization?,Monsoon611,False,0.67,1,https://www.reddit.com/r/datascience/comments/10987al/can_gptj_be_used_for_text_summarization/,1,1673452862.0,"I'm trying to find a good alternative for summarizing a large number of comments. I've tried with GPT-3 and was blown away by the results. But it's not free. Looking at alternatives, I found a couple of models specifically Google's T5 and GPT-J. While I was able to do the task with T5, I can't seem to find any resources for performing summarization with GPT-J. Most of it is related to text generation. I thought I would ask here to see if any of you have done the same with either of these models and could give me some advice. Any suggestions for alternative models is also appreciated. For context, the comments are basically customer reviews that need to be summarized. Any help is appreciated.

Thank you!"
27,znav39,datascience,GPT-3,top,2022-12-16 09:39:06,Ask a question and GPT-3 will fetch the data and generate a graph - What do you think?,Miserness,False,0.33,0,https://www.youtube.com/watch?v=Lh09uxa4BV0,1,1671183546.0,
28,zfvm7r,datascience,GPT-3,top,2022-12-08 10:50:12,Explainable and responsible AI,bilalak,False,0.4,0,https://www.reddit.com/r/datascience/comments/zfvm7r/explainable_and_responsible_ai/,1,1670496612.0,"While everybody is talking about #ChatGPT, the market looks with awe towards such innovations. Yet, the market needs different tools and much simpler enablement algorithms. 
 
After trying with several algorithms for explainable #XAI, the best option would be to design an explainable solution instead of ex-post explanation of the un-explainable networks.

Many data scientists have been trained to acquire a bunch of data, prepare, through it on a network then squeeze in or out some insights. 

Rigorous testing would fail in most cases. Explaining the results to the lay customer will be much harder even. 

The adoption of #ML solutions in every day life (other than #redommendation_system and #chatbot) requires clarity. 

I tried several libraries and tools such as: #LIME, #SHAP, #GIRP, #CEM … 

GIRP is promising to simplify the understanding. Yet, more work is required to get newer tools that incorporate the explainable component by design. 

One final note for further discussions:
1. Explainability facilitates the adoption of AI at the bottom of the pyramid (the industries that are slowly adopting AI)
2. Offering monetize-able AI solutions for conventional decision makers 
3. XAI might become part of compliance process with the spread of #responsible #AI laws and regulations."
29,znkets,datascience,GPT-3,top,2022-12-16 17:42:17,Can You Generate Realistic Data With GPT-3?,Djinn_Tonic4DataSci,False,0.4,0,https://www.reddit.com/r/datascience/comments/znkets/can_you_generate_realistic_data_with_gpt3/,3,1671212537.0,"ChatGPT has taken the tech world by storm, but its older cousin GPT-3 is still relevant. Being able to connect to the text completion API through python allows you to use the large language model to [generate synthetic data](https://www.tonic.ai/blog/can-you-generate-realistic-data-with-gpt-3) with bespoke distributions and relationships. The application is limited, however, as the lack of on-prem deployment limits your ability to show the model your proprietary data to learn from due to privacy concerns.

Real data is complex, what do people think about using LLMs to generate synthetic data? Should they just stick to writing stories and jokes?"
30,13foosi,datascience,GPT-3,top,2023-05-12 15:44:16,"AI-Powered Accommodation Search: Harnessing the Power of Hadoop, MongoDB, Spark, GPT-3, React, and Flask",Jealous_Ad6059,False,0.17,0,https://www.reddit.com/r/datascience/comments/13foosi/aipowered_accommodation_search_harnessing_the/,0,1683906256.0,"[https://medium.com/@stefentaime\_10958/ai-powered-accommodation-search-harnessing-the-power-of-hadoop-mongodb-spark-gpt-3-react-and-7e0bfc41bf26](https://medium.com/@stefentaime_10958/ai-powered-accommodation-search-harnessing-the-power-of-hadoop-mongodb-spark-gpt-3-react-and-7e0bfc41bf26)

&#x200B;

https://preview.redd.it/xtx2l6z48fza1.png?width=2000&format=png&auto=webp&s=a0d38eba791b545a7bce6768a43241f8a3d92372"
31,zcbvqf,datascience,GPT-3,top,2022-12-04 15:01:28,OpenAI ChatGPT and DaVinci-003 experiments by me,Opitmus_Prime,False,0.29,0,https://www.reddit.com/r/datascience/comments/zcbvqf/openai_chatgpt_and_davinci003_experiments_by_me/,0,1670166088.0,"I did some experiments with both chatGPI and GPT-3 davinci release 003. The answers by AI are really impressive! Give it a read 

[https://ithinkbot.com/openai-debuts-chatgpt-50dd611278a4](https://ithinkbot.com/openai-debuts-chatgpt-50dd611278a4)

[https://pub.towardsai.net/openai-just-released-gpt-3-text-davinci-003-i-compared-it-with-002-the-results-are-impressive-dced9aed0cba](https://pub.towardsai.net/openai-just-released-gpt-3-text-davinci-003-i-compared-it-with-002-the-results-are-impressive-dced9aed0cba)"
32,10m2zqe,datascience,GPT-3,top,2023-01-26 21:26:06,Create Your Chat GPT-3 Web App with Streamlit in Python,pasticciociccio,False,0.2,0,https://levelup.gitconnected.com/create-your-chat-gpt-3-web-app-with-streamlit-in-python-f0c6e6aede0a,0,1674768366.0,
33,10j94hn,datascience,GPT-3,top,2023-01-23 10:10:23,How can I get my application tested by data teams?,Miserness,False,0.17,0,https://www.reddit.com/r/datascience/comments/10j94hn/how_can_i_get_my_application_tested_by_data_teams/,0,1674468623.0,"I have a project to give everyone access to data. For more info, watch the videos posted on my Reddit. 

&#x200B;

I built an AI based on GPT-3 and other models to increase the quality of results.

&#x200B;

I would like to test it with some data scientists working in companies. It's quite complex to ask them for access to their db.

&#x200B;

My model only needs to know the tables and column names.

&#x200B;

With a SQL query we can easily get this information out but do you think it is possible for them to give it to me? I don't see what's sensitive about the table and column names but you never know.

&#x200B;

I designed a Slack app so he can give me his information there. 

&#x200B;

What do you think of it?"
34,zvsc8h,datascience,GPT-3,top,2022-12-26 17:43:47,Chat GPT taking over,BlackLotus8888,False,0.4,0,https://www.reddit.com/r/datascience/comments/zvsc8h/chat_gpt_taking_over/,29,1672076627.0,"Have y'all tried using chat GPT?  It's ridiculous!

1) I asked it to write a definition that takes in a df, imputes missing values, and returns a df and boom!

2) do t like the model it used, just ask it to use a different model

3) need cross validation?  No problem.  

4) need to get data using xx API?  You got it!  

It can do anything.  You just need to talk to it to get it right."
35,zpraee,datascience,GPT-3,comments,2022-12-19 13:15:11,The real reason ChatGPT was created,xdonvanx,False,0.94,736,https://i.redd.it/g5z2t4zeuu6a1.png,73,1671455711.0,
36,114qfkp,datascience,GPT-3,comments,2023-02-17 16:14:47,I am worried about the future of working as a Data Scientist in industry,Slumi,False,0.51,4,https://www.reddit.com/r/datascience/comments/114qfkp/i_am_worried_about_the_future_of_working_as_a/,71,1676650487.0,"I once watched a video where someone argued that Michael Jordan had ruined basketball. As a person whose only notions of basketball come from Space Jam, this surprised me. Wasn't the guy the best out there? How could someone who's best in his field ruin it? After hearing the argument out though, I began to understand what the video was really getting at: to them, Michael Jordan had ruined the field BECAUSE he was so good. A sport that was once full of diverse personalities and strategies then turned into multiple teams trying to produce their own Michael Jordan copycat with copycat moves and copycat strategies.

I'm not well versed enough in basketball to know how right or wrong that guy was. But this concept that something could ruin an entire field simply by being too good stuck with me. And all these years later, I consistently think about it while doing my job as a Data Scientist. And the more GPT-X and ChatGPT advance, the more this concept haunts me.

I've been working in this field for 5 years. During the first 4 years, the job was exciting. Every new problem required extensive research on my part: looking for papers, datasets, implementations, implementing my own stuff, experimenting, comparing the results... That was the biggest part of my job. Yeah I still had to do some software engineering stuff here and there, but I felt like I had a well defined and specialized role at my company. Even in the NLP field, the variety of technologies I had to work with was a lot of fun: sometimes I used RNNs, sometimes embedding based similarity functions, sometimes more classic approaches, and then towards the end it was transformer after transformer, but at least I still had to finetune them myself.

Over the past few months and years, this has started to change, however. GPT-1 and 2 were promising, but were more of a proof of concept than anything realistically usable. But with GPT3's and ChatGPT's latest performance, it feels like NLP is becoming more and more standardized. I wouldn't call it a solved field, far from it. But, sadly, unless you work for a big tech company,  I think the days of exploratory research-type work in the NLP field are over.

Even in academia, the latest NLP papers I've seen come out of prestigious universities went from actual engineering/mathematical advances to ""Look, if you ""engineer"" the prompt like this, it works better sometimes!""

And now, with the exploding popularity of ChatGPT, any random lambda would come to the conclusion that if you need to use AI, ChatGPT is the answer.

Now, some of you may already be thinking ""but hold on, with my finetuned transformer I get a 1% increase in accuracy over out-of-the-box GPT-3!"". The thing is: even if you can, it doesn't really matter. I really doubt the sales team of your company is gonna pay much attention to your pleas to give you 6 months to work on a product rather than 6 minutes. Because yes, that's the increase in productivity we're dealing with: What took me months a few years ago only takes minutes now. The range of problems I can apply NLP too has also increased, as out-of-the-box GPT-3 works well enough not to require a dataset for more ""general"" tasks. And the results are either comparable or even better.

In the span of a year, NLP went from my favorite ML field to one I never see myself working in again if given the choice. And while a lot of non NLP fields are, for now, untouched by the exploding popularity of the GPT family, I'm afraid that what happened to the NLP field will be replicated in other fields, and that in a few years, only super specialized issues won't have a solution somewhere in the form of a ridiculously big model accessible via an API key provided by a big tech company. And even if GPT3 was open source, the sheer size of it makes it close to impossible to realistically train for anything less than the biggest of tech companies out there. Even finetuning it would be a challenge.

I'm not saying the job of data scientist will disappear, in fact, I'd say it will be asked more than ever since boomers who don't understand what the title actually implies will want some in their company. But we won't be doing nearly as much actual data sciency stuff as we did in the past. Instead we'll have to become even bigger software engineer/data analyst/DevOps/project manager hybrids than we already were. The only exception is for people who either work in big tech companies or on very specialized problems that require an in-house model.

It's not that much of a problem for people who like the roles I listed above. But for people like me, whose enjoyment came out of the exploratory side of the job, I believe the future to be bleak."
37,13ilm03,datascience,GPT-3,comments,2023-05-15 21:45:36,I investigated the Underground Economy of Glassdoor Reviews,ibsurvivors,False,0.99,1159,https://www.reddit.com/r/datascience/comments/13ilm03/i_investigated_the_underground_economy_of/,63,1684187136.0,"Online company reviews are high stakes.

Top reviews on sites like Glassdoor and Google can get thousands of impressions each month and are major drivers of brand perception.

Employers know this. And when I come across multiple 5 star reviews left with no cons, or a Pulitzer worthy essay from a former intern, I become suspicious.

These reviews start to resemble 30 under 30 lists: so artificially constructed that you begin to question their credibility in the first place.

The scrutiny around company reviews is well documented; some companies file lawsuits worth over a million dollars to reveal anonymous reviewers that complain about their jobs.

Whilst it's the flashy lawsuits that make the headlines, there also exists an underground economy of company reviews operating quietly every single day.

In this underground economy, some companies pay over $150 to freelancers to try and get a negative review removed. If they want “better” results, they go to the plethora of Online Reputation Management services (ORMs) in the United States that can charge retainers worth thousands of dollars.

The supply of positive reviews exists too. My research led me to find companies, including a prominent Y-Combinator backed startup, that solicit fake positive reviews from online freelancers to improve their rating.

Many of these mercenary fake reviewers, often based in South East Asia, make a full time living doing this, netting over $2,000 per month.

Some of these run such sophisticated operations that they’ve even created their own pricing tiers (e.g $35 per original review, $20 to post an already created review from an email address), a la SaaS offering.

Others operate on a contingency fee agreement model, where they only get paid if they’re able to take a negative review down.

The underground economy of company reviews is well and truly alive. And today we’re going to find out how it operates.

***Note***: For more content like this, [*subscribe*](https://www.careerfair.io/subscribe) *to my newsletter. In a couple of weeks, I'll be releasing my guide to writing a killer resume.*

**Adding reviews**

The barriers to entry for adding fake reviews are much lower than for getting reviews removed, so that’s where we’ll start.

To write an employer review, all you really need is the ability to create an email address. For most sites, you don’t need any proof of employment (say like a company specific email address).

I went on a gig marketplace site and posted a pretty vague post related to wanting to find out more on how to improve a company’s online presence.

Within minutes of posting a gig, my inbox was flooded with proposals:

https://preview.redd.it/esx3904qa20b1.png?width=3064&format=png&auto=webp&s=2ff3a2f8528fee99aabb830f27ea71a7569ebb2e

After a bit of chatting, I narrowed the scope of their services and summarized their rates into the table below:

|Channel|Cost|Timeline|Model|
|:-|:-|:-|:-|
|Freelancer #1|$10 per review|Monthly|Unlimited|
|Freelancer #2|$35 per original review, $20 per already created review|Monthly|Unlimited|
|Freelancer #3|$25 per review|Monthly|Unlimited|
|Freelancer #4|$25 per review|Monthly|10 reviews|
|Freelancer #5|$20 per review|Monthly|Unlimited|
|Online Reputation Management Agency|$300 subscription|Monthly|8 reviews|

Let’s dive a bit deeper into the services that Freelancer #5 offered.

Freelancer #5 explained to me he had been writing reviews for one particular company for the past 4 months now. Each month he wrote them 10 reviews.

&#x200B;

https://preview.redd.it/n1ddox6cb20b1.png?width=2684&format=png&auto=webp&s=5c271d0eec4328cb78d7d2cb85dfffa3f9eb72f8

In another message, he tells me he’s offering the same services to 5 other companies. Doing some quick math:

5 companies x 10 reviews per company x $25 per review = $1,250 per month

Considering the average person in Pakistan earns $150 per month, that’s not bad change at all.

One of the companies that he’s offering his services to includes a Y-Combinator backed startup. I won’t name the company, but here’s what its average Glassdoor review rating distribution looks like:

https://preview.redd.it/2np5b6fdb20b1.png?width=2420&format=png&auto=webp&s=f8cafaa85453b0933a18eb5c30f931b3bb893c46

5 star reviews account for over 77% of the company’s total reviews. Obviously, no one is buying fake reviews that make them look bad.

But here’s the thing: freelancers are getting quite smart when it comes to writing reviews that don’t look too fishy. They tend to do this by spacing the reviews out (so that they don’t come in “spikes” – more on this later) and they also make sure that they’re not always leaving the “cons” section blank.

Don’t get me wrong, if you come across this company’s reviews, it’d be pretty easy to tell they’re quite strange. In fact, I can’t even post some screenshots here because it’d give the company away immediately.

But it would be challenging to conclude that the above company is buying reviews just by analyzing review volume and distribution without actually reading some of the reviews.

The same company is also buying reviews on Google Reviews.

Sidenote: I got curious about how he’s been writing 50 reviews from 50 different emails per month. Would he actually create 50 different email addresses? And what about the IP address – doesn’t Glassdoor flag multiple reviews from the same IP?

One of the freelancers answered my question:

&#x200B;

https://preview.redd.it/g4id2yqeb20b1.png?width=2572&format=png&auto=webp&s=c2a77fdea8834a6d90f02b8b3eb67b3a874f3df2

Moving on – another company that seems to buy fake reviews seems to be having some more trouble. Approximately a month after a freelancer linked me to fake reviews he had written for this company, all five reviews that he had linked me to had been removed:

&#x200B;

https://preview.redd.it/99fdvcgfb20b1.png?width=3116&format=png&auto=webp&s=b7e244529fc62b5c824d925feb61fd2cc16cbfd5

Based on this [Glassdoor webinar](https://youtu.be/3iy0JWOS1gs) from 2018, “if it is found that a user has created multiple email accounts to submit reviews, then ALL submissions from that user are deleted” – so likely Glassdoor’s content moderation team flagged one of the initial reviews and the same freelancer who was writing reviews for that company had all the fake reviews deleted.

So far, it looks like the key to an effective fake review creation strategy lies in:

* Spacing the fake reviews out
* Writing each review from a different IP address (i.e benefit of being part of a team)
* Using language that isn’t an obvious giveaway

On that third point: the reality is that many of these freelancers’ first language is not English.

As an experiment, I turned to everybody’s favorite new toy, ChatGPT, and asked it to write me a positive Glassdoor review:

https://preview.redd.it/8w7cal9gb20b1.png?width=3164&format=png&auto=webp&s=860c39b11c5813e8b7fabdbb038d73c565cc98cf

And I’d say that the above answer was better than 95% of the fake reviews I came across.

**Removing reviews**

The process for removing an employer review usually works like this:

1. You identify one or multiple reviews that you want removed
2. You verify whether the review violates the site's Guidelines, or whether there’s something else about the review(s) that could get it removed.
3. You file an appeal to get it removed.

As an example, Glassdoor’s Review guidelines can be found [here](https://help.glassdoor.com/s/article/Community-Guidelines?language=en_US#:~:text=See%20More-,Review%C2%A0Guidelines,-Millions%20of%20job). Mainly, they forbid mentioning anyone by name who’s not an executive and revealing proprietary or confidential information, amongst a host of other things.

Sounds simple enough right? Well, according to one of the freelancers I messaged:

&#x200B;

https://preview.redd.it/x6s8hsyac20b1.png?width=2036&format=png&auto=webp&s=f86c386f864198dc43faeb41faea378090c20107

After some research, I summarized the different vendors and prices in the table below:

&#x200B;

|Channel|Cost|Timeline|Model|Self reported success rate|
|:-|:-|:-|:-|:-|
|Freelancer #1|$100 per review|3 days|Contingency Agreement Model|100%|
|Freelancer #2|$30 per review|7 days|Contingency Agreement Model|100%|
|Reputation management service #2|$450 per review|21 business days|Contingency Agreement Model|Unknown|
|Reputation management service #3|$1000 per review|Undefined|Contingency Agreement Model|100%|
|Reputation management service #4 Plan 1|$550 per review|5-6 weeks|Contingency Agreement Model|50-75%|
|Reputation management service #4 Plan 2|$300 Subscription + $100 per each review removed|Monthly service|Subscription plan|50-75%|
|Freelancer #3|$20|Undefined|Pay regardless|Undefined|
|Freelancer #4|$500|Undefined|Contingency Agreement Model|Undefined|

As you can see, unlike the fake review generation market, the prices vary quite a bit for getting reviews removed.

At one end, you have freelancers on gig marketplaces that will attempt to remove a review for less than $100. And then on the other end, you have ORMs (Online Reputation Management Agencies) that have multiple employees and more comprehensive packages in place. The one constant seems to be that most companies operate on a contingency agreement model (i.e pay only if review gets removed).

**Analyzing reviews**

ReviewMeta is a site that analyzes Amazon reviews and tells you how many are legitimate. The creator of the site, Tommy Noonan, mentions in an [interview with NPR](https://www.npr.org/sections/money/2018/06/27/623990036/episode-850-the-fake-review-hunter) that the main giveaway that a product is soliciting fake reviews is:

* A large, suspicious flood of positive reviews at the exact same time. For example, a 3 day stretch of time constituting 30% of total reviews.
* Phrases and words that are constantly repeated, especially in the section with no cons
* Brand monogamists (only review products from one company)

Whilst the last two bullets are hard to track, the first can be used to analyze different companies’ reviews and to check if there might be some funky business going on.

After a couple of days, I have the ability to track review volume and review ratings over time for any company that I specify:

https://preview.redd.it/ehcbw2oje20b1.png?width=1653&format=png&auto=webp&s=b448ff35eb9878fbb1686de2fa8cf031e4ed3e05

Let the games begin.

## Voluntary Response Bias

One of the biggest challenges that review platforms face is the Voluntary Response bias.

Research shows many of today’s most popular online review platforms (e.g Amazon) have a distribution of opinion that is highly polarized, with many extreme positive and/or negative reviews, and few moderate opinions.

Think about it: have you ever felt moderately satisfied at your job and thought to yourself, now would be a great time to leave a Glassdoor review? Probably not.

On the other hand, if you’ve had a terrible experience or even just had one thing really flip you off, you might be quite likely to leave an angry review.

Consider when a company goes through layoffs. You’re going to have a flood of angry reviews coming your way and are likely going to experience a “spike” in reviews.

**Note:** Just like the Wall Street Journal’s methodology described [here](https://archive.is/20201016094732/https://www.wsj.com/articles/companies-manipulate-glassdoor-by-inflating-rankings-and-pressuring-employees-11548171977#selection-3965.0-3968.0), I considered there to be a spike if the total number of reviews in the month was greater than three standard deviations above the mean of the surrounding months.

Let’s take the company below. Here’s a graph of of their review volume since Jan 2020, including when they announced one of their first round of layoffs in June 2022:

https://preview.redd.it/n6kd9ejle20b1.png?width=3216&format=png&auto=webp&s=9eea2f3836617feca37eb88b1d3f67c8fa1b6fe2

In June 2022, approximately 19% of this company's 52 reviews were 1 star reviews (compared to an overall average of around 10%). This is what we could call a statistically significant spike in reviews. It also illustrates how the employees most likely to leave reviews are the ones that obviously had a bad experience (i.e getting laid off).

Here’s another company that had a similar spike in negative reviews due to layoffs in November 2022:

https://preview.redd.it/4vcnr1ine20b1.png?width=2408&format=png&auto=webp&s=f3877fb315ccc5d9a9294306a9f86616cb0fabd2

This company had an approximate 20% 1 star review rate (compared to an overall average of 12%) in November 2022, as well as an Avg Rating of 2.96 that month (compared to an overall average rating of 3.73).Unless HR is proactive, their reviews page risks succumbing to an echochamber of negative reviews that can really tilt one way.

**Note:** Glassdoor does state (based on [this video](https://www.youtube.com/watch?v=3iy0JWOS1gs) from 2017) that about 75% of the reviews on their platform are neutral. Their “give to get policy” has helped in keeping the platform from becoming too polarized.

I can understand why HR teams, like the ones that Nader talked to me about earlier, take a proactive stance towards managing their reviews. If they don’t try to control their reputation themselves, then their reputation risks getting controlled by the employees that had the worst possible experience.

## Goodhart’s Law

Goodhart’s law states the following:

*""When a measure becomes a target, it ceases to be a good measure""*

Every October, Glassdoor publishes their Best Places To Work ranking.

In a [report](https://www.wsj.com/articles/companies-manipulate-glassdoor-by-inflating-rankings-and-pressuring-employees-11548171977) that the WSJ did a couple of years ago, they found large spikes in the number of reviews that some companies (e.g SpaceX, Bain & Co, etc) got in September. The logic here is that some companies try to artificially inflate their Glassdoor reviews right before the October deadline.

I decided to revisit some of this analysis with Glassdoor’s 2023 Best Places To Work Ranking.

One of the companies I examined is rated as one of the best places to work in 2023. Let’s refer to this company as FunPlaceToWork.

Here is how their review volume looks like for all of 2022:

https://preview.redd.it/4e656zkqe20b1.png?width=2516&format=png&auto=webp&s=07141a66c56be7a6818efb9b1a4d912ee0021c91

FunPlaceToWork got around 50 reviews in September 2022. Of those 50 reviews, 96% were 5 star reviews.

FunPlaceToWork averaged 12 reviews per month up till then in 2022. Also, in the prior six months, the average percent of 5 star reviews received every month was \~75%.

Both the spike in volume of reviews and the spike in percentage of five star reviews are statistically significant.

I find it strange that Glassdoor’s proprietary algorithm and/or Human Content Moderation team did not find a spike of this nature unusual. If we look at Glassdoor’s eligibility criteria for the award, it’s as follows:

https://preview.redd.it/hag04y7se20b1.png?width=2868&format=png&auto=webp&s=ec2b920e126a8ea42b40d35aaa55d5341e69d022

The goal, according to Glassdoor, is to collect “authentic and unbiased reviews”.

Whilst there’s nothing against the rules for asking your employees to leave you reviews, I find the statistically significant spike of reviews at odds with the goal of collecting ""unbiased and authentic"" reviews (which Glassdoor states is the purpose of the awards).

Glassdoor states that an employer is allowed to ask its employees to leave reviews, but that they are not allowed to “coerce” them. Examples of what you can’t do:

* Offer incentives like Gift Cards in exchange for positive reviews.
* Withholding their reference letter unless they leave you a positive review.
* Anything that leads you to require proof for the employee to show you that they wrote a review.

It is possible to play by the rules (i.e not break any of the above rules) and to still in my opinion not collect authentic and unbiased reviews.

They say that you shouldn’t hate the player but the game – I think **FunPlaceToWork** played by the rules, won fair and square, and that this is simply a perfect example of Goodhart’s Law.

I reached out to Glassdoor ([awards@glassdoor.com](mailto:awards@glassdoor.com)) about the above and this is the reply I got:

https://preview.redd.it/x0dqq39ue20b1.png?width=4800&format=png&auto=webp&s=c0102c963be9486370b340f2f473cbc6650fc48a

**Conclusion**

When I was 22, on an [F1 visa with 3 months to find work](https://www.careerfair.io/job-hunt-story), I didn’t give a damn about bad reviews. I needed a job and I’d sign any piece of paper you put in front of me.

Compare that to someone at the peak of their career, someone with optionality and a multitude of job offers; an “A-Player”, as the experts call it, would absolutely have the luxury of choice and discard a job offer based on bad company reviews.

For most people, the impact of online company reviews lies somewhere in the middle. In marketing, there’s a concept of a “marketing touchpoint” - an interaction with the brand over the course of the whole buying journey.

Company reviews are one of the many touchpoints a job seeker experiences over their interview process. And with the technology industry booming the past couple of years, companies couldn’t afford to slack on any touchpoints, including this one.

After all, when others start to game the system, you’re at a disadvantage if you don’t. The rewards can be quite high. Certainly higher than just trying to be as transparent as possible.

HR leaders are often more incentivized to inflate their metrics than to get honest feedback. Fake review writers have bills to pay. ORMs know that companies are desperate. And the platforms, well, aren’t always paying attention.

The result is a potluck of interests that leads to an underground economy.

One that ends up hurting the job seeker.

\*\*\*

Whew. That took a while (about 3 months in fact). Thanks for reading. For more content like this, [subscribe](https://www.careerfair.io/subscribe) to my newsletter. It's my best content delivered to your inbox once every 2 weeks."
38,zvsc8h,datascience,GPT-3,comments,2022-12-26 17:43:47,Chat GPT taking over,BlackLotus8888,False,0.41,0,https://www.reddit.com/r/datascience/comments/zvsc8h/chat_gpt_taking_over/,29,1672076627.0,"Have y'all tried using chat GPT?  It's ridiculous!

1) I asked it to write a definition that takes in a df, imputes missing values, and returns a df and boom!

2) do t like the model it used, just ask it to use a different model

3) need cross validation?  No problem.  

4) need to get data using xx API?  You got it!  

It can do anything.  You just need to talk to it to get it right."
39,138ckjn,datascience,GPT-3,comments,2023-05-05 06:07:33,The language in which GPT 3.5 communicates changes how it thinks.....,NoCartographer4725,False,0.64,12,https://www.reddit.com/r/datascience/comments/138ckjn/the_language_in_which_gpt_35_communicates_changes/,19,1683266853.0,"Seems like a new paper shows that GPT is more patient when talking in Mandarin and German vs when talking in English and Russian.

[https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=4437617](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4437617)

&#x200B;"
40,10a1mik,datascience,GPT-3,comments,2023-01-12 14:59:04,New Research From Google Shines Light On The Future Of Language Models ⭕,LesleyFair,False,0.84,120,https://www.reddit.com/r/datascience/comments/10a1mik/new_research_from_google_shines_light_on_the/,18,1673535544.0,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs will allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* Stop hallucinating and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example are words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/w7xffqjimmba1.png?width=800&format=png&auto=webp&s=e2d9cb63f750efcbfa45c4bb7a985d4dcb5b0319)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.  
At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week.  
No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)"
41,11szca1,datascience,GPT-3,comments,2023-03-16 16:45:29,"What is your digital workspace, tools, setup, etc. for ETL, research, production?",BreathAether,False,0.94,35,https://www.reddit.com/r/datascience/comments/11szca1/what_is_your_digital_workspace_tools_setup_etc/,16,1678985129.0,"I'm new to this and so I've been wanting to know what other people have been using to make their work feel as smooth as butter. Since I've been learning lots and not just the industry standard stuff, I wanted to share what little I found to be valuable which others may want to try. **The main goal of this post is to share, critique, and provide suggestions so that we can all find the setup we like most. I  am also looking for new, up and coming tech, and definitely not afraid to try new things!**

IDE: **VSCode with the Jupyter Notebook Extension**. What I like about it is that I can view data structures like series/dataframes in a table format by clicking the variable in the Jupyter: Variables pane at the bottom. I started with plain vanilla jupyter notebooks from Anaconda so this was pretty nice. I have seen demos that **Jupyter Lab** has something like this, so if anyone has used both VSCode's notebooks and used Lab, your input would be appreciated. I hear good things about **PyCharm and Spyder**. Some people also use **Google Collab, DataSpell, and DeepNote** but I don't know enough about it. I did play around with DeepNote, and it was very cool but I didn't feel compelled to switch (and you have to pay for it!). 

Tools:

* A code helper: A few months back I was googling everything and I would've listed **Stackoverflow**. I might actually use that occasionally, but these days I use **ChatGPT** and **Bing AI**. For more current info or news-based I'll use Bing AI since it uses live search results, and for information that is knowledge based I might use ChatGPT. ChatGPT saves conversations so it's great for exploring topics in depth and referencing that conversation later. For those who have used both, maybe you know what I'm talking about and can provide a better explanation as to which is better for what purpose.
* Software: **Excel** is an obvious one. For instance, if I have a huge dataset and I just want to delete out columns that I don't need with Ctrl+click to select, it's easier and quicker than copy + pasting or typing out each of the string column names I want to ""df.drop()"". Excel is great for quick and simple stuff. Some software I have been learning about are I guess what I would consider as no- or low-code data analytics platforms, such as **Alteryx, KNIME, and Orange**. These software let you practically run an entire ETL pipeline. I believe Alteryx and KNIME are the gold-standard in this category, and Orange is a ""lite"" version of the two and is available in Anaconda. I think these are pretty cool, and I personally haven't found a huge use case for them since I've been chugging away in my notebooks with Python, but I can see the value. Would love for someone to chime in on these tools and how they compare to manually doing stuff in code, especially for large datasets.
* Version Control: This is where I'm primarily lacking, but I know that **Github** is the go-to. I don't use this but I know that a ton of people do. I don't even know where to start to be honest. I usually just create a new .ipynb file for each analysis or phase of an ETL pipeline haha. I'm also not too aware of what other innovative tools for version control exist.
* Python Libraries: Besides the obvious stuff like Pandas/NumPy, MatplotLib/Seaborn, and your popular ML libraries, I've recently found out about this library called **Polars**. It's basically a Rust version of Pandas, and it's super powerful. Some operations that I've run, that would've taken hours with Pandas, took me minutes. But I've been hearing that **Pandas 2.0** which will be released some time this month, has been looking at using PyArrow dtypes (if I recall correctly) and the speed is comparable to Polars. I mean these two are FAST. Another contender is **DuckDB** but I think the new Pandas and Polars are still faster. I mostly use Pandas but if there is some heavy lifting, I'll swap the dataframe to a polars one with a quick function, run it with polars, then back to pandas.

Anyway, that's just some things I can immediately think of. Looking forward to your suggestions! Bonus points for anything new and innovative. Cheers.

https://preview.redd.it/qj2cywt1r4oa1.png?width=1920&format=png&auto=webp&s=c4e02f5fec0b3768df336c7a3f63cc382b3954a8"
42,10fw1a3,datascience,GPT-3,comments,2023-01-19 07:54:24,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.94,125,https://www.reddit.com/r/datascience/comments/10fw1a3/gpt4_will_be_500x_smaller_than_people_think_here/,14,1674114864.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/t6m0epzlgyca1.png?width=575&format=png&auto=webp&s=a5972941053f833e76fd3a5009fc68a61f9e5406)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
43,10iue4e,datascience,GPT-3,comments,2023-01-22 21:12:11,"I wrote a tiny library this morning for openai's (suddenly really good, really cheap) embeddings to search stuff - Semantic search, smarter replies w/ GPT-3, easier.",morganpartee,False,0.81,12,https://www.sensibledefaults.io/blog/chatgpt/easy-python-embeddings,14,1674421931.0,"I've had a few buddies in the community ask about something like this in the last week, and then a reader did today so I finally took the time to write it up. It's basically a generic interface to try embeddings, and it's less than a hundred lines of code. Built this so you can just authenticate and test without having to fuss with numpy, pinecone, etc.

The only thing you do is put dictionaries with a name and text key, then search your embeddings instance. We return anything you stick in there with it too for your programming pleasure - think dates, page numbers, file names, whatever.

You can just... Build chat bots that are pretty smart like this for next to nothing by reading in your docs or whatever. It's cheap and incredibly effective. I know next to nothing about how to preprocess data for this thing, I've tried different sizes of chunks, lines, sentences and haven't found a one size fits all solution (other than more gpt to summarize) but - just try it, it's cheap as hell and there's so much you can do. Like the rest of the current state of gpt - it just kinda works. Start with one to five sentences (or lines for code) and experiment!

For you proper data scientists (nerds!) - you can just get the numpy array out and do regular embeddings stuff with it. If you do, I'd love to hear about it! Clustering, classification, etc are suddenly super easy, and the raw data is stored in json so you can upload it to something better when you're ready.

I know it's small, but little abstractions make life better. Anything we can do to lower the barrier to experimentation is worth doing. Hope it helps someone!"
44,13c8ewn,datascience,GPT-3,comments,2023-05-08 23:21:30,I asked ChatGPT4 to do some stats modelling - it was okay…ish,AFL_gains,False,0.65,5,https://www.reddit.com/r/datascience/comments/13c8ewn/i_asked_chatgpt4_to_do_some_stats_modelling_it/,11,1683588090.0,"Hi guys ! There’s been some debate, especially on here, about the “future of data science” and “whose job is going to be taken” etc etc. Imo I don’t know the answer, but I think LLMs have definitely changed the landscape.

One of the really interesting things ChatGPT has unlocked is that people can now code without really knowing how to. I think if you already are familiar with coding, using ChatGPT to improve productivity is awesome. But if you’re just starting out and use it generate code you can’t explain, then I think you can get into lots of trouble. 

And I think this is especially true when there’s a mathematical modelling choice aspect to your code. My thought was that just because something works / compiles, doesn’t mean it’s a very good model and doesn’t mean that the explicit choices / assumptions make sense. This, of course, isn’t chatGPTs fault, it’s the users fault for not checking! 

Anyway, to investigate this point, I recently tested ChatGPT to write a Stan code (bayesian inference ) to predict premier league matches. My feeling was that the task simple enough for it to do an okay job, but not so generic it there’s a million examples on the internet.

I put the results on YouTube (link below), but in summary I found the following: 

1. ChatGPT made a decent model, but with some really weird choices. Eg It decided to use a normal distribution to model goal differences , where I think a Skellam would have been better. It also decided not to model the variance of this distribution , instead deciding that it was 1. Super weird!

2. It wasn’t able to rationalise about things like over parameterisation. The model it build had way too many parameters, unnecessarily. The idea of parsimony wasn’t really there. Maybe with better prompts it would have, but out of the box it made the model overly complex


3. Prompt engineering really makes a difference. I think with better prompts, the model Could have been better. There was even a point where I spotted an error and prompted chatGPT to fix it and it did! But again, this all relied on me being able to read Stan code and know what was good and bad. 


For me, I learnt that at least for tasks where lots of modelling choices need to be made, humans still beat GPT. But perhaps in the future, those that win will be the data scientists/ engineers that know what they are doing but are able to prompt GPT optimally to maximise their productivity boost.



The videos are here : 
Part 1: https://m.youtube.com/watch?v=4LTUYTxKuIk&t=66s&pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D
Part 2: https://m.youtube.com/watch?v=XjQpV6c9K5g&t=1s&pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D"
45,123yr9p,datascience,GPT-3,comments,2023-03-27 20:09:45,ChatGPT and made up citations,its_the_llama,False,0.7,7,https://www.reddit.com/r/datascience/comments/123yr9p/chatgpt_and_made_up_citations/,10,1679947785.0,"Hopefully this hasn't been posted here before. I was using ChatGPT to try and find a few reviews to read on parameter search strategies in system biology. Helpfully, it returned a list of seminal papers: 

 

1. ""Parameter estimation and inference for differential equations"" by P.J. Haario et al. (2006). This review article discusses various methods for parameter estimation in differential equations, including Bayesian methods, gradient-based optimization, and particle filtering.
2. ""Parameter estimation in differential equations: a Bayesian perspective"" by S. Särkkä (2013). This review article focuses on Bayesian methods for parameter estimation in differential equations, including Markov chain Monte Carlo (MCMC) methods and sequential Monte Carlo (SMC) methods.
3. ""Optimization methods for parameter estimation in nonlinear differential equation models"" by C.L. Lawson et al. (2013). This review article discusses various optimization methods for parameter estimation in nonlinear differential equation models, including gradient-based methods, genetic algorithms, and particle swarm optimization.
4. ""Parameter estimation and sensitivity analysis in computational systems biology"" by D. Ge and J.J. Liu (2019). This review article provides an overview of various methods for parameter estimation and sensitivity analysis in computational systems biology, including optimization-based methods, Bayesian methods, and global sensitivity analysis.

Great, right? Except, **none of these papers actually exist**. The authors sound similar to people in the field (""P.J. Haario is probably ""inspired"" by Heikki Haario who's well known in the field, and Simo Särkkä is an actual author who's published on this), but the work does not exist. 

In hindsight, this makes sense considering how chatGPT works. It's still pretty interesting though, and I wonder how many people have turned in college assays with completely fabricated references."
46,zmyhve,datascience,GPT-3,comments,2022-12-15 22:38:06,Text to SQL,ljh78,False,0.6,1,https://www.reddit.com/r/datascience/comments/zmyhve/text_to_sql/,7,1671143886.0,"Hi!

Looking to start a project to build an in-house text to SQL (which I then use to query a relational DB) tool for some of the engineers (not software...no experience with SQL). Have any of you folks done something of the sort before and would be able to recommend a decent starting point? For the starter version it'd be very simple.. most likely one dataset and simple questions (i.e. no joins, only SELECT FROM WHERE).  

I have heard of OpenAI's GPT-3, but am not looking to ask my org to pay for any third party systems at the moment (still very much an exploratory project). Additionally, I'm looking to further my experience and expertise in DS/AI, so this might be a good way to do just that (as far as NL processing goes).

Thank you!"
47,12rgc2w,datascience,GPT-3,comments,2023-04-19 04:12:07,"GPT-4, my best study buddy!",Somomi_,False,0.64,3,https://www.reddit.com/r/datascience/comments/12rgc2w/gpt4_my_best_study_buddy/,6,1681877527.0," 

Today I find several prompts which could be very helpful for active learning.

**1. Generate Multiple Choice Question**

*Topic: { }*

*Write 3 multiple choice question with 1 correct answer and 3 incorrect distractor answers and let me choose an answer. Later you should let me know if I got it right or wrong and provide me with explanation.*

**2. Generate General Question**

>*Topic: { }*  
*Write 2* *data scientist interview questions* *about this topic and let me answer them. Later you should let me know if I got it right or wrong and provide me with explanation.*

**3. Learning by Teaching**

>*Please act as a data scientist. I will tell you what I l*  
*earn today and you can point out if I miss any step or made any mistake.*  
*Today I learn { }*

You can check my original post with example image here! Thank you!

[https://www.kaggle.com/code/kuixizhu/gpt-4-my-best-study-buddy](https://www.kaggle.com/code/kuixizhu/gpt-4-my-best-study-buddy)"
48,11fpp9o,datascience,GPT-3,comments,2023-03-02 02:08:10,"Working on a report for school, can someone answer a few ethics related data science questions for me.",VelvetRevolver_,False,0.6,1,https://www.reddit.com/r/datascience/comments/11fpp9o/working_on_a_report_for_school_can_someone_answer/,5,1677722890.0,"I have a report for school where I need to ask a data science professional some questions and then write a report on it. You don't have to answer all of them and the responses don't have to be that long they can be on the shorter side. Any help would be greatly appreciated. Thank you.

1) As AI advances do you see any possible risk of mass job displacement in the future? Such as self driving cars replacing truck drivers, taxi drivers, etc. Do you think this will be a problem in the future or will it be the same as all the times we've automated something in the past and people will just adapt to the job market?


2) Recently there has been some controversy around algorithmic bias, especially in data science. Such as automating parole decisions, job application screening, or applying for credit cards. Some people argue these machine learning algorithms have been somewhat racist/sexist in the past. Do you think data scientists should be doing their best to eliminate bias?


3) With the recent release of chat bots like chatGPT students have been using it to help them write papers or help with their programming assignments. Do you see anything wrong with claiming chatGPT's responses as your own work? Do you consider it plagiarism?


4) Art generation has gotten very popular recently and there have actually been some lawsuits against these AI's claiming they were trained on copyrighted images. Do you believe AI shouldn't be legally allowed to train on copyrighted material or will doing so significantly hinder AI's development as data scientists will have to be much more careful about how they obtain their datasets?


5) As AI and image detection/recognition algorithms become more withspread do you think adversarial attacks will ever be a significant and practical problem?"
49,11mad6m,datascience,GPT-3,comments,2023-03-08 22:30:57,Is it worth using LLMs like GPT-3 for text classification?,pgalgali,False,0.6,1,https://www.reddit.com/r/datascience/comments/11mad6m/is_it_worth_using_llms_like_gpt3_for_text/,5,1678314657.0,"Hello, I am new to LLMs world. I am curious to know whether it is worth using LLMs for text classification problems or they are better suited for text generation use cases?"
50,123jgeh,datascience,GPT-3,comments,2023-03-27 11:00:57,Name classification with ChatGPT: How does it compare to ML language models?,tabacof,False,0.57,1,https://www.reddit.com/r/datascience/comments/123jgeh/name_classification_with_chatgpt_how_does_it/,4,1679914857.0,"In this [blog post](https://tabacof.github.io/posts/name_classification/name_classification.html), I explore the problem of name classification with ChatGPT and 3 ML models of increasing complexity (logistic regression, FastAI LSTM, and HuggingFace DistilBERT).

ChatGPT delivers the best accuracy of them all with no model training, just prompt engineering. It classifies 100k names in 18 minutes for under $5.

We see a lot of ChatGPT chat examples, but here I show how to use its API to solve an actual text classification problem (albeit a simple one).

GPT is transforming tasks that required deep machine learning knowledge into software + prompt engineering problems. As a data scientist, I’m not worried about it taking over my job, as predictive modelling is only a small aspect of what a data scientist does.

Feel free to share any feedback or questions."
51,znkets,datascience,GPT-3,comments,2022-12-16 17:42:17,Can You Generate Realistic Data With GPT-3?,Djinn_Tonic4DataSci,False,0.4,0,https://www.reddit.com/r/datascience/comments/znkets/can_you_generate_realistic_data_with_gpt3/,3,1671212537.0,"ChatGPT has taken the tech world by storm, but its older cousin GPT-3 is still relevant. Being able to connect to the text completion API through python allows you to use the large language model to [generate synthetic data](https://www.tonic.ai/blog/can-you-generate-realistic-data-with-gpt-3) with bespoke distributions and relationships. The application is limited, however, as the lack of on-prem deployment limits your ability to show the model your proprietary data to learn from due to privacy concerns.

Real data is complex, what do people think about using LLMs to generate synthetic data? Should they just stick to writing stories and jokes?"
52,10mi1x8,datascience,GPT-3,comments,2023-01-27 10:52:18,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.74,17,https://www.reddit.com/r/datascience/comments/10mi1x8/what_people_are_missing_about_microsofts_10b/,3,1674816738.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/35vtxrwnekea1.png?width=720&format=png&auto=webp&s=a61dd557e1d00c96448c429c9f9bb78516205a6f)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
53,12coioi,datascience,GPT-3,comments,2023-04-05 15:46:14,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.55,1,https://www.reddit.com/r/datascience/comments/12coioi/do_we_really_need_100b_parameters_in_a_large/,2,1680709574.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back!

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
54,zfvm7r,datascience,GPT-3,comments,2022-12-08 10:50:12,Explainable and responsible AI,bilalak,False,0.36,0,https://www.reddit.com/r/datascience/comments/zfvm7r/explainable_and_responsible_ai/,1,1670496612.0,"While everybody is talking about #ChatGPT, the market looks with awe towards such innovations. Yet, the market needs different tools and much simpler enablement algorithms. 
 
After trying with several algorithms for explainable #XAI, the best option would be to design an explainable solution instead of ex-post explanation of the un-explainable networks.

Many data scientists have been trained to acquire a bunch of data, prepare, through it on a network then squeeze in or out some insights. 

Rigorous testing would fail in most cases. Explaining the results to the lay customer will be much harder even. 

The adoption of #ML solutions in every day life (other than #redommendation_system and #chatbot) requires clarity. 

I tried several libraries and tools such as: #LIME, #SHAP, #GIRP, #CEM … 

GIRP is promising to simplify the understanding. Yet, more work is required to get newer tools that incorporate the explainable component by design. 

One final note for further discussions:
1. Explainability facilitates the adoption of AI at the bottom of the pyramid (the industries that are slowly adopting AI)
2. Offering monetize-able AI solutions for conventional decision makers 
3. XAI might become part of compliance process with the spread of #responsible #AI laws and regulations."
55,134o5fe,datascience,GPT-3,comments,2023-05-01 14:33:50,"[colabdog.com] I build an aggregator for AI News that combines OpenAI, Google AI, MIT, BAIR.",colabDog,False,0.87,6,https://i.redd.it/rlizo23od8xa1.png,1,1682951630.0,
56,107khox,datascience,GPT-3,comments,2023-01-09 17:37:48,FYI: GPT-3 & Beyond (Stanford AI Webinar),itedelweiss,False,0.86,23,https://www.reddit.com/r/datascience/comments/107khox/fyi_gpt3_beyond_stanford_ai_webinar/,1,1673285868.0,"The field of Natural Language Processing (NLP) has seen tremendous progress. In particular, algorithms for Natural Language Understanding (NLU) are evolving and improving at a stunning pace. Systems that seemed like science fiction as recently as ten years ago are now commonplace. One such system is GPT-3 (Generative Pretrained Transformer 3), a state-of-the-art model capable of generating human-like text for a wide range of tasks, such as conversation modeling, summarization, and question answering.  

In this live webinar Stanford Professor Christopher Potts will discuss the significance and implications of recent Natural Language Understanding developments including GPT-3. He will outline the fundamental building blocks of these new systems and describe how we can reliably assess and understand them.

Topics will include:

- Technical capabilities, limitations, and applications of new Natural Language Understanding systems

- Analysis of the performance of GPT-3 on various language tasks

- Potential future developments in Natural Language Processing

DATE: January 18, 2023
 	

TIME: 11:00 AM - 12:00 PM (PST)


[Registration link](https://event.on24.com/wcc/r/4076653/D32B5B8B45C4099498D2555AB941504C?mkt_tok=MTk0LU9DUS00ODcAAAGJNpCzioiOp8O-K8z9UB050H0f-EbLnEk9bZVBNI2vmlidmSvXCca2T1T2-BFseDFQyKNr0wv1iH8RGEXMDOj_E1d47fWkanGqcP2AjCkwjYFtaGU)"
57,10987al,datascience,GPT-3,comments,2023-01-11 16:01:02,Can GPT-J be used for text summarization?,Monsoon611,False,0.67,1,https://www.reddit.com/r/datascience/comments/10987al/can_gptj_be_used_for_text_summarization/,1,1673452862.0,"I'm trying to find a good alternative for summarizing a large number of comments. I've tried with GPT-3 and was blown away by the results. But it's not free. Looking at alternatives, I found a couple of models specifically Google's T5 and GPT-J. While I was able to do the task with T5, I can't seem to find any resources for performing summarization with GPT-J. Most of it is related to text generation. I thought I would ask here to see if any of you have done the same with either of these models and could give me some advice. Any suggestions for alternative models is also appreciated. For context, the comments are basically customer reviews that need to be summarized. Any help is appreciated.

Thank you!"
58,znav39,datascience,GPT-3,comments,2022-12-16 09:39:06,Ask a question and GPT-3 will fetch the data and generate a graph - What do you think?,Miserness,False,0.33,0,https://www.youtube.com/watch?v=Lh09uxa4BV0,1,1671183546.0,
59,11xemex,datascience,GPT-3,comments,2023-03-21 11:55:57,Large Language Models For Summarization,vm123313223,False,1.0,1,https://www.reddit.com/r/datascience/comments/11xemex/large_language_models_for_summarization/,0,1679399757.0,"How to get the results of OpenAI (GPT-3) for summarization with open source models?

Some models which I have tried are:

1) FLAN-T5

2) Pegasus

3) BART

4) GPT-J

5) FTAN--UL2

I have also implemented fewshot learning with these models."
60,10m2zqe,datascience,GPT-3,comments,2023-01-26 21:26:06,Create Your Chat GPT-3 Web App with Streamlit in Python,pasticciociccio,False,0.2,0,https://levelup.gitconnected.com/create-your-chat-gpt-3-web-app-with-streamlit-in-python-f0c6e6aede0a,0,1674768366.0,
61,120rm54,datascience,GPT-3,comments,2023-03-24 17:32:45,How-to-Fine-Tune GPT-3-Model-for-Named-Entity-Recognition,Lilith-Smol,False,1.0,1,https://ubiai.tools/blog/article/How-to-Fine-Tune-GPT-3-Model-for-Named-Entity-Recognition,0,1679679165.0,
62,11vdjat,datascience,GPT-3,comments,2023-03-19 06:27:43,datasetGPT - A command-line tool to generate datasets by inferencing LLMs at scale. It can even make two ChatGPT agents talk with one another.,radi-cho,False,0.83,4,https://www.reddit.com/r/datascience/comments/11vdjat/datasetgpt_a_commandline_tool_to_generate/,0,1679207263.0,"GitHub: [https://github.com/radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT)

It can generate texts by varying input parameters and using multiple backends. But, personally, the conversations dataset generation is my favorite: It can produce dialogues between two ChatGPT agents.

Possible use cases may include:

* Constructing textual corpora to train/fine-tune detectors for content written by AI.
* Collecting datasets of LLM-produced conversations for research purposes, analysis of AI performance/impact/ethics, etc.
* Automating a task that a LLM can handle over big amounts of input texts. For example, using GPT-3 to summarize 1000 paragraphs with a single CLI command.
* Leveraging APIs of especially big LLMs to produce diverse texts for a specific task and then fine-tune a smaller model with them.

What would you use it for?"
63,zcbvqf,datascience,GPT-3,comments,2022-12-04 15:01:28,OpenAI ChatGPT and DaVinci-003 experiments by me,Opitmus_Prime,False,0.29,0,https://www.reddit.com/r/datascience/comments/zcbvqf/openai_chatgpt_and_davinci003_experiments_by_me/,0,1670166088.0,"I did some experiments with both chatGPI and GPT-3 davinci release 003. The answers by AI are really impressive! Give it a read 

[https://ithinkbot.com/openai-debuts-chatgpt-50dd611278a4](https://ithinkbot.com/openai-debuts-chatgpt-50dd611278a4)

[https://pub.towardsai.net/openai-just-released-gpt-3-text-davinci-003-i-compared-it-with-002-the-results-are-impressive-dced9aed0cba](https://pub.towardsai.net/openai-just-released-gpt-3-text-davinci-003-i-compared-it-with-002-the-results-are-impressive-dced9aed0cba)"
64,zrr7zz,datascience,GPT-3,comments,2022-12-21 15:41:57,Sample Peyote: generate multi-table synthetic data on any topic using GPT-3,abegong,False,0.63,2,/r/datasets/comments/zrr2yr/sample_peyote_generate_multitable_synthetic_data/,0,1671637317.0,
65,zaf4bs,datascience,GPT-3,comments,2022-12-02 07:35:37,"Cross Entropy, Explained by GPT-3",Ill-Tomato-8400,False,1.0,1,https://www.reddit.com/r/datascience/comments/zaf4bs/cross_entropy_explained_by_gpt3/,0,1669966537.0,"I recently experimented with OpenAI's new Chat GPT3 and used it to generate an explanation of cross entropy. It's impressive what language models are capable of, and it's daunting to imagine what they'll be able to do in the future.

&#x200B;

[https://gradiently.io/cross-entropy-explained-by-gpt3/](https://gradiently.io/cross-entropy-explained-by-gpt3/)

&#x200B;

&#x200B;"
66,z9ba36,datascience,GPT-3,comments,2022-12-01 02:21:12,OpenAI debuts ChatGPT: a conversational AI on GPT-3.5,Opitmus_Prime,False,0.6,1,https://ithinkbot.com/openai-debuts-chatgpt-50dd611278a4,0,1669861272.0,
67,13foosi,datascience,GPT-3,comments,2023-05-12 15:44:16,"AI-Powered Accommodation Search: Harnessing the Power of Hadoop, MongoDB, Spark, GPT-3, React, and Flask",Jealous_Ad6059,False,0.17,0,https://www.reddit.com/r/datascience/comments/13foosi/aipowered_accommodation_search_harnessing_the/,0,1683906256.0,"[https://medium.com/@stefentaime\_10958/ai-powered-accommodation-search-harnessing-the-power-of-hadoop-mongodb-spark-gpt-3-react-and-7e0bfc41bf26](https://medium.com/@stefentaime_10958/ai-powered-accommodation-search-harnessing-the-power-of-hadoop-mongodb-spark-gpt-3-react-and-7e0bfc41bf26)

&#x200B;

https://preview.redd.it/xtx2l6z48fza1.png?width=2000&format=png&auto=webp&s=a0d38eba791b545a7bce6768a43241f8a3d92372"
68,10j94hn,datascience,GPT-3,comments,2023-01-23 10:10:23,How can I get my application tested by data teams?,Miserness,False,0.17,0,https://www.reddit.com/r/datascience/comments/10j94hn/how_can_i_get_my_application_tested_by_data_teams/,0,1674468623.0,"I have a project to give everyone access to data. For more info, watch the videos posted on my Reddit. 

&#x200B;

I built an AI based on GPT-3 and other models to increase the quality of results.

&#x200B;

I would like to test it with some data scientists working in companies. It's quite complex to ask them for access to their db.

&#x200B;

My model only needs to know the tables and column names.

&#x200B;

With a SQL query we can easily get this information out but do you think it is possible for them to give it to me? I don't see what's sensitive about the table and column names but you never know.

&#x200B;

I designed a Slack app so he can give me his information there. 

&#x200B;

What do you think of it?"
69,10mu9ru,datascience,GPT-3,comments,2023-01-27 19:53:34,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,1.0,5,https://www.reddit.com/r/datascience/comments/10mu9ru/a_python_module_to_generate_optimized_prompts/,0,1674849214.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features 🚀

* 🧙‍♀️ NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* 🔨 Easily add one-shot, two-shot, or few-shot examples to the prompt
* ✌ Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* 💥 Custom examples and samples can be easily added to the prompt
* 💰 Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Examples](https://preview.redd.it/x232msli2nea1.png?width=1236&format=png&auto=webp&s=6071efdd8cb12801230af6572991ba8aaff1a9ec)

&#x200B;

https://preview.redd.it/5o5uqk3k2nea1.png?width=1398&format=png&auto=webp&s=96c25820a698a83dfeb0e8f7f37682d9d27c06cb"
70,zaf4bs,datascience,GPT-3,relevance,2022-12-02 07:35:37,"Cross Entropy, Explained by GPT-3",Ill-Tomato-8400,False,1.0,1,https://www.reddit.com/r/datascience/comments/zaf4bs/cross_entropy_explained_by_gpt3/,0,1669966537.0,"I recently experimented with OpenAI's new Chat GPT3 and used it to generate an explanation of cross entropy. It's impressive what language models are capable of, and it's daunting to imagine what they'll be able to do in the future.

&#x200B;

[https://gradiently.io/cross-entropy-explained-by-gpt3/](https://gradiently.io/cross-entropy-explained-by-gpt3/)

&#x200B;

&#x200B;"
71,107khox,datascience,GPT-3,relevance,2023-01-09 17:37:48,FYI: GPT-3 & Beyond (Stanford AI Webinar),itedelweiss,False,0.87,22,https://www.reddit.com/r/datascience/comments/107khox/fyi_gpt3_beyond_stanford_ai_webinar/,1,1673285868.0,"The field of Natural Language Processing (NLP) has seen tremendous progress. In particular, algorithms for Natural Language Understanding (NLU) are evolving and improving at a stunning pace. Systems that seemed like science fiction as recently as ten years ago are now commonplace. One such system is GPT-3 (Generative Pretrained Transformer 3), a state-of-the-art model capable of generating human-like text for a wide range of tasks, such as conversation modeling, summarization, and question answering.  

In this live webinar Stanford Professor Christopher Potts will discuss the significance and implications of recent Natural Language Understanding developments including GPT-3. He will outline the fundamental building blocks of these new systems and describe how we can reliably assess and understand them.

Topics will include:

- Technical capabilities, limitations, and applications of new Natural Language Understanding systems

- Analysis of the performance of GPT-3 on various language tasks

- Potential future developments in Natural Language Processing

DATE: January 18, 2023
 	

TIME: 11:00 AM - 12:00 PM (PST)


[Registration link](https://event.on24.com/wcc/r/4076653/D32B5B8B45C4099498D2555AB941504C?mkt_tok=MTk0LU9DUS00ODcAAAGJNpCzioiOp8O-K8z9UB050H0f-EbLnEk9bZVBNI2vmlidmSvXCca2T1T2-BFseDFQyKNr0wv1iH8RGEXMDOj_E1d47fWkanGqcP2AjCkwjYFtaGU)"
72,11mad6m,datascience,GPT-3,relevance,2023-03-08 22:30:57,Is it worth using LLMs like GPT-3 for text classification?,pgalgali,False,0.6,1,https://www.reddit.com/r/datascience/comments/11mad6m/is_it_worth_using_llms_like_gpt3_for_text/,5,1678314657.0,"Hello, I am new to LLMs world. I am curious to know whether it is worth using LLMs for text classification problems or they are better suited for text generation use cases?"
73,10m2zqe,datascience,GPT-3,relevance,2023-01-26 21:26:06,Create Your Chat GPT-3 Web App with Streamlit in Python,pasticciociccio,False,0.2,0,https://levelup.gitconnected.com/create-your-chat-gpt-3-web-app-with-streamlit-in-python-f0c6e6aede0a,0,1674768366.0,
74,zrr7zz,datascience,GPT-3,relevance,2022-12-21 15:41:57,Sample Peyote: generate multi-table synthetic data on any topic using GPT-3,abegong,False,0.63,2,/r/datasets/comments/zrr2yr/sample_peyote_generate_multitable_synthetic_data/,0,1671637317.0,
75,znav39,datascience,GPT-3,relevance,2022-12-16 09:39:06,Ask a question and GPT-3 will fetch the data and generate a graph - What do you think?,Miserness,False,0.33,0,https://www.youtube.com/watch?v=Lh09uxa4BV0,1,1671183546.0,
76,10fw1a3,datascience,GPT-3,relevance,2023-01-19 07:54:24,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.94,123,https://www.reddit.com/r/datascience/comments/10fw1a3/gpt4_will_be_500x_smaller_than_people_think_here/,14,1674114864.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/t6m0epzlgyca1.png?width=575&format=png&auto=webp&s=a5972941053f833e76fd3a5009fc68a61f9e5406)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
77,138ckjn,datascience,GPT-3,relevance,2023-05-05 06:07:33,The language in which GPT 3.5 communicates changes how it thinks.....,NoCartographer4725,False,0.66,13,https://www.reddit.com/r/datascience/comments/138ckjn/the_language_in_which_gpt_35_communicates_changes/,19,1683266853.0,"Seems like a new paper shows that GPT is more patient when talking in Mandarin and German vs when talking in English and Russian.

[https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=4437617](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4437617)

&#x200B;"
78,10mu9ru,datascience,GPT-3,relevance,2023-01-27 19:53:34,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,1.0,5,https://www.reddit.com/r/datascience/comments/10mu9ru/a_python_module_to_generate_optimized_prompts/,0,1674849214.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features 🚀

* 🧙‍♀️ NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* 🔨 Easily add one-shot, two-shot, or few-shot examples to the prompt
* ✌ Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* 💥 Custom examples and samples can be easily added to the prompt
* 💰 Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Examples](https://preview.redd.it/x232msli2nea1.png?width=1236&format=png&auto=webp&s=6071efdd8cb12801230af6572991ba8aaff1a9ec)

&#x200B;

https://preview.redd.it/5o5uqk3k2nea1.png?width=1398&format=png&auto=webp&s=96c25820a698a83dfeb0e8f7f37682d9d27c06cb"
79,z9ba36,datascience,GPT-3,relevance,2022-12-01 02:21:12,OpenAI debuts ChatGPT: a conversational AI on GPT-3.5,Opitmus_Prime,False,0.6,1,https://ithinkbot.com/openai-debuts-chatgpt-50dd611278a4,0,1669861272.0,
80,znkets,datascience,GPT-3,relevance,2022-12-16 17:42:17,Can You Generate Realistic Data With GPT-3?,Djinn_Tonic4DataSci,False,0.45,0,https://www.reddit.com/r/datascience/comments/znkets/can_you_generate_realistic_data_with_gpt3/,3,1671212537.0,"ChatGPT has taken the tech world by storm, but its older cousin GPT-3 is still relevant. Being able to connect to the text completion API through python allows you to use the large language model to [generate synthetic data](https://www.tonic.ai/blog/can-you-generate-realistic-data-with-gpt-3) with bespoke distributions and relationships. The application is limited, however, as the lack of on-prem deployment limits your ability to show the model your proprietary data to learn from due to privacy concerns.

Real data is complex, what do people think about using LLMs to generate synthetic data? Should they just stick to writing stories and jokes?"
81,120rm54,datascience,GPT-3,relevance,2023-03-24 17:32:45,How-to-Fine-Tune GPT-3-Model-for-Named-Entity-Recognition,Lilith-Smol,False,1.0,1,https://ubiai.tools/blog/article/How-to-Fine-Tune-GPT-3-Model-for-Named-Entity-Recognition,0,1679679165.0,
82,zcbvqf,datascience,GPT-3,relevance,2022-12-04 15:01:28,OpenAI ChatGPT and DaVinci-003 experiments by me,Opitmus_Prime,False,0.33,0,https://www.reddit.com/r/datascience/comments/zcbvqf/openai_chatgpt_and_davinci003_experiments_by_me/,0,1670166088.0,"I did some experiments with both chatGPI and GPT-3 davinci release 003. The answers by AI are really impressive! Give it a read 

[https://ithinkbot.com/openai-debuts-chatgpt-50dd611278a4](https://ithinkbot.com/openai-debuts-chatgpt-50dd611278a4)

[https://pub.towardsai.net/openai-just-released-gpt-3-text-davinci-003-i-compared-it-with-002-the-results-are-impressive-dced9aed0cba](https://pub.towardsai.net/openai-just-released-gpt-3-text-davinci-003-i-compared-it-with-002-the-results-are-impressive-dced9aed0cba)"
83,11vdjat,datascience,GPT-3,relevance,2023-03-19 06:27:43,datasetGPT - A command-line tool to generate datasets by inferencing LLMs at scale. It can even make two ChatGPT agents talk with one another.,radi-cho,False,0.67,2,https://www.reddit.com/r/datascience/comments/11vdjat/datasetgpt_a_commandline_tool_to_generate/,0,1679207263.0,"GitHub: [https://github.com/radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT)

It can generate texts by varying input parameters and using multiple backends. But, personally, the conversations dataset generation is my favorite: It can produce dialogues between two ChatGPT agents.

Possible use cases may include:

* Constructing textual corpora to train/fine-tune detectors for content written by AI.
* Collecting datasets of LLM-produced conversations for research purposes, analysis of AI performance/impact/ethics, etc.
* Automating a task that a LLM can handle over big amounts of input texts. For example, using GPT-3 to summarize 1000 paragraphs with a single CLI command.
* Leveraging APIs of especially big LLMs to produce diverse texts for a specific task and then fine-tune a smaller model with them.

What would you use it for?"
84,13foosi,datascience,GPT-3,relevance,2023-05-12 15:44:16,"AI-Powered Accommodation Search: Harnessing the Power of Hadoop, MongoDB, Spark, GPT-3, React, and Flask",Jealous_Ad6059,False,0.29,0,https://www.reddit.com/r/datascience/comments/13foosi/aipowered_accommodation_search_harnessing_the/,0,1683906256.0,"[https://medium.com/@stefentaime\_10958/ai-powered-accommodation-search-harnessing-the-power-of-hadoop-mongodb-spark-gpt-3-react-and-7e0bfc41bf26](https://medium.com/@stefentaime_10958/ai-powered-accommodation-search-harnessing-the-power-of-hadoop-mongodb-spark-gpt-3-react-and-7e0bfc41bf26)

&#x200B;

https://preview.redd.it/xtx2l6z48fza1.png?width=2000&format=png&auto=webp&s=a0d38eba791b545a7bce6768a43241f8a3d92372"
85,10987al,datascience,GPT-3,relevance,2023-01-11 16:01:02,Can GPT-J be used for text summarization?,Monsoon611,False,0.67,1,https://www.reddit.com/r/datascience/comments/10987al/can_gptj_be_used_for_text_summarization/,1,1673452862.0,"I'm trying to find a good alternative for summarizing a large number of comments. I've tried with GPT-3 and was blown away by the results. But it's not free. Looking at alternatives, I found a couple of models specifically Google's T5 and GPT-J. While I was able to do the task with T5, I can't seem to find any resources for performing summarization with GPT-J. Most of it is related to text generation. I thought I would ask here to see if any of you have done the same with either of these models and could give me some advice. Any suggestions for alternative models is also appreciated. For context, the comments are basically customer reviews that need to be summarized. Any help is appreciated.

Thank you!"
86,114qfkp,datascience,GPT-3,relevance,2023-02-17 16:14:47,I am worried about the future of working as a Data Scientist in industry,Slumi,False,0.51,3,https://www.reddit.com/r/datascience/comments/114qfkp/i_am_worried_about_the_future_of_working_as_a/,71,1676650487.0,"I once watched a video where someone argued that Michael Jordan had ruined basketball. As a person whose only notions of basketball come from Space Jam, this surprised me. Wasn't the guy the best out there? How could someone who's best in his field ruin it? After hearing the argument out though, I began to understand what the video was really getting at: to them, Michael Jordan had ruined the field BECAUSE he was so good. A sport that was once full of diverse personalities and strategies then turned into multiple teams trying to produce their own Michael Jordan copycat with copycat moves and copycat strategies.

I'm not well versed enough in basketball to know how right or wrong that guy was. But this concept that something could ruin an entire field simply by being too good stuck with me. And all these years later, I consistently think about it while doing my job as a Data Scientist. And the more GPT-X and ChatGPT advance, the more this concept haunts me.

I've been working in this field for 5 years. During the first 4 years, the job was exciting. Every new problem required extensive research on my part: looking for papers, datasets, implementations, implementing my own stuff, experimenting, comparing the results... That was the biggest part of my job. Yeah I still had to do some software engineering stuff here and there, but I felt like I had a well defined and specialized role at my company. Even in the NLP field, the variety of technologies I had to work with was a lot of fun: sometimes I used RNNs, sometimes embedding based similarity functions, sometimes more classic approaches, and then towards the end it was transformer after transformer, but at least I still had to finetune them myself.

Over the past few months and years, this has started to change, however. GPT-1 and 2 were promising, but were more of a proof of concept than anything realistically usable. But with GPT3's and ChatGPT's latest performance, it feels like NLP is becoming more and more standardized. I wouldn't call it a solved field, far from it. But, sadly, unless you work for a big tech company,  I think the days of exploratory research-type work in the NLP field are over.

Even in academia, the latest NLP papers I've seen come out of prestigious universities went from actual engineering/mathematical advances to ""Look, if you ""engineer"" the prompt like this, it works better sometimes!""

And now, with the exploding popularity of ChatGPT, any random lambda would come to the conclusion that if you need to use AI, ChatGPT is the answer.

Now, some of you may already be thinking ""but hold on, with my finetuned transformer I get a 1% increase in accuracy over out-of-the-box GPT-3!"". The thing is: even if you can, it doesn't really matter. I really doubt the sales team of your company is gonna pay much attention to your pleas to give you 6 months to work on a product rather than 6 minutes. Because yes, that's the increase in productivity we're dealing with: What took me months a few years ago only takes minutes now. The range of problems I can apply NLP too has also increased, as out-of-the-box GPT-3 works well enough not to require a dataset for more ""general"" tasks. And the results are either comparable or even better.

In the span of a year, NLP went from my favorite ML field to one I never see myself working in again if given the choice. And while a lot of non NLP fields are, for now, untouched by the exploding popularity of the GPT family, I'm afraid that what happened to the NLP field will be replicated in other fields, and that in a few years, only super specialized issues won't have a solution somewhere in the form of a ridiculously big model accessible via an API key provided by a big tech company. And even if GPT3 was open source, the sheer size of it makes it close to impossible to realistically train for anything less than the biggest of tech companies out there. Even finetuning it would be a challenge.

I'm not saying the job of data scientist will disappear, in fact, I'd say it will be asked more than ever since boomers who don't understand what the title actually implies will want some in their company. But we won't be doing nearly as much actual data sciency stuff as we did in the past. Instead we'll have to become even bigger software engineer/data analyst/DevOps/project manager hybrids than we already were. The only exception is for people who either work in big tech companies or on very specialized problems that require an in-house model.

It's not that much of a problem for people who like the roles I listed above. But for people like me, whose enjoyment came out of the exploratory side of the job, I believe the future to be bleak."
87,10iue4e,datascience,GPT-3,relevance,2023-01-22 21:12:11,"I wrote a tiny library this morning for openai's (suddenly really good, really cheap) embeddings to search stuff - Semantic search, smarter replies w/ GPT-3, easier.",morganpartee,False,0.81,12,https://www.sensibledefaults.io/blog/chatgpt/easy-python-embeddings,14,1674421931.0,"I've had a few buddies in the community ask about something like this in the last week, and then a reader did today so I finally took the time to write it up. It's basically a generic interface to try embeddings, and it's less than a hundred lines of code. Built this so you can just authenticate and test without having to fuss with numpy, pinecone, etc.

The only thing you do is put dictionaries with a name and text key, then search your embeddings instance. We return anything you stick in there with it too for your programming pleasure - think dates, page numbers, file names, whatever.

You can just... Build chat bots that are pretty smart like this for next to nothing by reading in your docs or whatever. It's cheap and incredibly effective. I know next to nothing about how to preprocess data for this thing, I've tried different sizes of chunks, lines, sentences and haven't found a one size fits all solution (other than more gpt to summarize) but - just try it, it's cheap as hell and there's so much you can do. Like the rest of the current state of gpt - it just kinda works. Start with one to five sentences (or lines for code) and experiment!

For you proper data scientists (nerds!) - you can just get the numpy array out and do regular embeddings stuff with it. If you do, I'd love to hear about it! Clustering, classification, etc are suddenly super easy, and the raw data is stored in json so you can upload it to something better when you're ready.

I know it's small, but little abstractions make life better. Anything we can do to lower the barrier to experimentation is worth doing. Hope it helps someone!"
88,10j94hn,datascience,GPT-3,relevance,2023-01-23 10:10:23,How can I get my application tested by data teams?,Miserness,False,0.17,0,https://www.reddit.com/r/datascience/comments/10j94hn/how_can_i_get_my_application_tested_by_data_teams/,0,1674468623.0,"I have a project to give everyone access to data. For more info, watch the videos posted on my Reddit. 

&#x200B;

I built an AI based on GPT-3 and other models to increase the quality of results.

&#x200B;

I would like to test it with some data scientists working in companies. It's quite complex to ask them for access to their db.

&#x200B;

My model only needs to know the tables and column names.

&#x200B;

With a SQL query we can easily get this information out but do you think it is possible for them to give it to me? I don't see what's sensitive about the table and column names but you never know.

&#x200B;

I designed a Slack app so he can give me his information there. 

&#x200B;

What do you think of it?"
89,10a1mik,datascience,GPT-3,relevance,2023-01-12 14:59:04,New Research From Google Shines Light On The Future Of Language Models ⭕,LesleyFair,False,0.84,122,https://www.reddit.com/r/datascience/comments/10a1mik/new_research_from_google_shines_light_on_the/,18,1673535544.0,"Last year, large language models (LLM) have broken record after record. ChatGPT got to 1 million users faster than Facebook, Spotify, and Instagram did. They helped create [billion-dollar companies](https://www.marketsgermany.com/translation-tool-deepl-is-now-a-unicorn/#:~:text=Cologne%2Dbased%20artificial%20neural%20network,sources%20close%20to%20the%20company), and most notably they helped us recognize the [divine nature of ducks](https://twitter.com/drnelk/status/1598048054724423681?t=LWzI2RdbSO0CcY9zuJ-4lQ&s=08).

2023 has started and ML progress is likely to continue at a break-neck speed. This is a great time to take a look at one of the most interesting papers from last year.

Emergent Abilities in LLMs

In a recent [paper from Google Brain](https://arxiv.org/pdf/2206.07682.pdf), Jason Wei and his colleagues allowed us a peak into the future. This beautiful research showed how scaling LLMs will allow them, among other things, to:

* Become better at math
* Understand even more subtleties of human language
* Stop hallucinating and answer truthfully
* ...

(See the plot on break-out performance below for a full list)

**Some Context:**

If you played around with ChatGPT or any of the other LLMs, you will likely have been as impressed as I was. However, you have probably also seen the models go off the rails here and there. The model might hallucinate gibberish, give untrue answers, or fail at performing math.

**Why does this happen?**

LLMs are commonly trained by [maximizing the likelihood](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) over all tokens in a body of text. Put more simply, they learn to predict the next word in a sequence of words.

Hence, if such a model learns to do any math at all, it learns it by figuring concepts present in human language (and thereby math).

Let's look at the following sentence.

""The sum of two plus two is ...""

The model figures out that the most likely missing word is ""four"".

The fact that LLMs learn this at all is mind-bending to me! However, once the math gets more complicated [LLMs begin to struggle](https://twitter.com/Richvn/status/1598714487711756288?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598714487711756288%7Ctwgr%5E478ce47357ad71a72873d1a482af5e5ff73d228f%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Ffreaky-chatgpt-fails-that-caught-our-eyes%2F).

There are many other cases where the models fail to capture the elaborate interactions and meanings behind words. One other example are words that change their meaning with context. When the model encounters the word ""bed"", it needs to figure out from the context, if the text is talking about a ""river bed"" or a ""bed"" to sleep in.

**What they discovered:**

For smaller models, the performance on the challenging tasks outline above remains approximately random. However, the performance shoots up once a certain number of training FLOPs (proxy for model size) is reached.

The figure below visualizes this effect on eight benchmarks. The critical number of training FLOPs is around 10\^23. The big version of GPT-3 already lies to the right of this point, but we seem to be at the beginning stages of performance increases.

&#x200B;

[Break-Out Performance At Critical Scale](https://preview.redd.it/w7xffqjimmba1.png?width=800&format=png&auto=webp&s=e2d9cb63f750efcbfa45c4bb7a985d4dcb5b0319)

They observed similar improvements on (few-shot) prompting strategies, such as multi-step reasoning and instruction following. If you are interested, I also encourage you to check out Jason Wei's personal blog. There he [listed a total of 137](https://www.jasonwei.net/blog/emergence) emergent abilities observable in LLMs.

Looking at the results, one could be forgiven for thinking: simply making models bigger will make them more powerful. That would only be half the story.

(Language) models are primarily scaled along three dimensions: number of parameters, amount of training compute, and dataset size. Hence, emergent abilities are likely to also occur with e.g. bigger and/or cleaner datasets.

There is [other research](https://arxiv.org/abs/2203.15556) suggesting that current models, such as GPT-3, are undertrained. Therefore, scaling datasets promises to boost performance in the near-term, without using more parameters.

**So what does this mean exactly?**

This beautiful paper shines a light on the fact that our understanding of how to train these large models is still very limited. The lack of understanding is largely due to the sheer cost of training LLMs. Running the same number of experiments as people do for smaller models would cost in the hundreds of millions.

However, the results strongly hint that further scaling will continue the exhilarating performance gains of the last years.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you.  
At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week.  
No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)"
90,11xemex,datascience,GPT-3,relevance,2023-03-21 11:55:57,Large Language Models For Summarization,vm123313223,False,1.0,1,https://www.reddit.com/r/datascience/comments/11xemex/large_language_models_for_summarization/,0,1679399757.0,"How to get the results of OpenAI (GPT-3) for summarization with open source models?

Some models which I have tried are:

1) FLAN-T5

2) Pegasus

3) BART

4) GPT-J

5) FTAN--UL2

I have also implemented fewshot learning with these models."
91,12coioi,datascience,GPT-3,relevance,2023-04-05 15:46:14,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.5,0,https://www.reddit.com/r/datascience/comments/12coioi/do_we_really_need_100b_parameters_in_a_large/,2,1680709574.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back!

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
92,123jgeh,datascience,GPT-3,relevance,2023-03-27 11:00:57,Name classification with ChatGPT: How does it compare to ML language models?,tabacof,False,0.57,1,https://www.reddit.com/r/datascience/comments/123jgeh/name_classification_with_chatgpt_how_does_it/,4,1679914857.0,"In this [blog post](https://tabacof.github.io/posts/name_classification/name_classification.html), I explore the problem of name classification with ChatGPT and 3 ML models of increasing complexity (logistic regression, FastAI LSTM, and HuggingFace DistilBERT).

ChatGPT delivers the best accuracy of them all with no model training, just prompt engineering. It classifies 100k names in 18 minutes for under $5.

We see a lot of ChatGPT chat examples, but here I show how to use its API to solve an actual text classification problem (albeit a simple one).

GPT is transforming tasks that required deep machine learning knowledge into software + prompt engineering problems. As a data scientist, I’m not worried about it taking over my job, as predictive modelling is only a small aspect of what a data scientist does.

Feel free to share any feedback or questions."
93,13c8ewn,datascience,GPT-3,relevance,2023-05-08 23:21:30,I asked ChatGPT4 to do some stats modelling - it was okay…ish,AFL_gains,False,0.65,5,https://www.reddit.com/r/datascience/comments/13c8ewn/i_asked_chatgpt4_to_do_some_stats_modelling_it/,11,1683588090.0,"Hi guys ! There’s been some debate, especially on here, about the “future of data science” and “whose job is going to be taken” etc etc. Imo I don’t know the answer, but I think LLMs have definitely changed the landscape.

One of the really interesting things ChatGPT has unlocked is that people can now code without really knowing how to. I think if you already are familiar with coding, using ChatGPT to improve productivity is awesome. But if you’re just starting out and use it generate code you can’t explain, then I think you can get into lots of trouble. 

And I think this is especially true when there’s a mathematical modelling choice aspect to your code. My thought was that just because something works / compiles, doesn’t mean it’s a very good model and doesn’t mean that the explicit choices / assumptions make sense. This, of course, isn’t chatGPTs fault, it’s the users fault for not checking! 

Anyway, to investigate this point, I recently tested ChatGPT to write a Stan code (bayesian inference ) to predict premier league matches. My feeling was that the task simple enough for it to do an okay job, but not so generic it there’s a million examples on the internet.

I put the results on YouTube (link below), but in summary I found the following: 

1. ChatGPT made a decent model, but with some really weird choices. Eg It decided to use a normal distribution to model goal differences , where I think a Skellam would have been better. It also decided not to model the variance of this distribution , instead deciding that it was 1. Super weird!

2. It wasn’t able to rationalise about things like over parameterisation. The model it build had way too many parameters, unnecessarily. The idea of parsimony wasn’t really there. Maybe with better prompts it would have, but out of the box it made the model overly complex


3. Prompt engineering really makes a difference. I think with better prompts, the model Could have been better. There was even a point where I spotted an error and prompted chatGPT to fix it and it did! But again, this all relied on me being able to read Stan code and know what was good and bad. 


For me, I learnt that at least for tasks where lots of modelling choices need to be made, humans still beat GPT. But perhaps in the future, those that win will be the data scientists/ engineers that know what they are doing but are able to prompt GPT optimally to maximise their productivity boost.



The videos are here : 
Part 1: https://m.youtube.com/watch?v=4LTUYTxKuIk&t=66s&pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D
Part 2: https://m.youtube.com/watch?v=XjQpV6c9K5g&t=1s&pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D"
94,12rgc2w,datascience,GPT-3,relevance,2023-04-19 04:12:07,"GPT-4, my best study buddy!",Somomi_,False,0.7,4,https://www.reddit.com/r/datascience/comments/12rgc2w/gpt4_my_best_study_buddy/,6,1681877527.0," 

Today I find several prompts which could be very helpful for active learning.

**1. Generate Multiple Choice Question**

*Topic: { }*

*Write 3 multiple choice question with 1 correct answer and 3 incorrect distractor answers and let me choose an answer. Later you should let me know if I got it right or wrong and provide me with explanation.*

**2. Generate General Question**

>*Topic: { }*  
*Write 2* *data scientist interview questions* *about this topic and let me answer them. Later you should let me know if I got it right or wrong and provide me with explanation.*

**3. Learning by Teaching**

>*Please act as a data scientist. I will tell you what I l*  
*earn today and you can point out if I miss any step or made any mistake.*  
*Today I learn { }*

You can check my original post with example image here! Thank you!

[https://www.kaggle.com/code/kuixizhu/gpt-4-my-best-study-buddy](https://www.kaggle.com/code/kuixizhu/gpt-4-my-best-study-buddy)"
95,zmyhve,datascience,GPT-3,relevance,2022-12-15 22:38:06,Text to SQL,ljh78,False,0.6,1,https://www.reddit.com/r/datascience/comments/zmyhve/text_to_sql/,7,1671143886.0,"Hi!

Looking to start a project to build an in-house text to SQL (which I then use to query a relational DB) tool for some of the engineers (not software...no experience with SQL). Have any of you folks done something of the sort before and would be able to recommend a decent starting point? For the starter version it'd be very simple.. most likely one dataset and simple questions (i.e. no joins, only SELECT FROM WHERE).  

I have heard of OpenAI's GPT-3, but am not looking to ask my org to pay for any third party systems at the moment (still very much an exploratory project). Additionally, I'm looking to further my experience and expertise in DS/AI, so this might be a good way to do just that (as far as NL processing goes).

Thank you!"
96,zvsc8h,datascience,GPT-3,relevance,2022-12-26 17:43:47,Chat GPT taking over,BlackLotus8888,False,0.41,0,https://www.reddit.com/r/datascience/comments/zvsc8h/chat_gpt_taking_over/,29,1672076627.0,"Have y'all tried using chat GPT?  It's ridiculous!

1) I asked it to write a definition that takes in a df, imputes missing values, and returns a df and boom!

2) do t like the model it used, just ask it to use a different model

3) need cross validation?  No problem.  

4) need to get data using xx API?  You got it!  

It can do anything.  You just need to talk to it to get it right."
97,123yr9p,datascience,GPT-3,relevance,2023-03-27 20:09:45,ChatGPT and made up citations,its_the_llama,False,0.69,6,https://www.reddit.com/r/datascience/comments/123yr9p/chatgpt_and_made_up_citations/,10,1679947785.0,"Hopefully this hasn't been posted here before. I was using ChatGPT to try and find a few reviews to read on parameter search strategies in system biology. Helpfully, it returned a list of seminal papers: 

 

1. ""Parameter estimation and inference for differential equations"" by P.J. Haario et al. (2006). This review article discusses various methods for parameter estimation in differential equations, including Bayesian methods, gradient-based optimization, and particle filtering.
2. ""Parameter estimation in differential equations: a Bayesian perspective"" by S. Särkkä (2013). This review article focuses on Bayesian methods for parameter estimation in differential equations, including Markov chain Monte Carlo (MCMC) methods and sequential Monte Carlo (SMC) methods.
3. ""Optimization methods for parameter estimation in nonlinear differential equation models"" by C.L. Lawson et al. (2013). This review article discusses various optimization methods for parameter estimation in nonlinear differential equation models, including gradient-based methods, genetic algorithms, and particle swarm optimization.
4. ""Parameter estimation and sensitivity analysis in computational systems biology"" by D. Ge and J.J. Liu (2019). This review article provides an overview of various methods for parameter estimation and sensitivity analysis in computational systems biology, including optimization-based methods, Bayesian methods, and global sensitivity analysis.

Great, right? Except, **none of these papers actually exist**. The authors sound similar to people in the field (""P.J. Haario is probably ""inspired"" by Heikki Haario who's well known in the field, and Simo Särkkä is an actual author who's published on this), but the work does not exist. 

In hindsight, this makes sense considering how chatGPT works. It's still pretty interesting though, and I wonder how many people have turned in college assays with completely fabricated references."
98,10mi1x8,datascience,GPT-3,relevance,2023-01-27 10:52:18,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.76,18,https://www.reddit.com/r/datascience/comments/10mi1x8/what_people_are_missing_about_microsofts_10b/,3,1674816738.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/35vtxrwnekea1.png?width=720&format=png&auto=webp&s=a61dd557e1d00c96448c429c9f9bb78516205a6f)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
99,zpraee,datascience,GPT-3,relevance,2022-12-19 13:15:11,The real reason ChatGPT was created,xdonvanx,False,0.94,739,https://i.redd.it/g5z2t4zeuu6a1.png,73,1671455711.0,
100,zfvm7r,datascience,GPT-3,relevance,2022-12-08 10:50:12,Explainable and responsible AI,bilalak,False,0.4,0,https://www.reddit.com/r/datascience/comments/zfvm7r/explainable_and_responsible_ai/,1,1670496612.0,"While everybody is talking about #ChatGPT, the market looks with awe towards such innovations. Yet, the market needs different tools and much simpler enablement algorithms. 
 
After trying with several algorithms for explainable #XAI, the best option would be to design an explainable solution instead of ex-post explanation of the un-explainable networks.

Many data scientists have been trained to acquire a bunch of data, prepare, through it on a network then squeeze in or out some insights. 

Rigorous testing would fail in most cases. Explaining the results to the lay customer will be much harder even. 

The adoption of #ML solutions in every day life (other than #redommendation_system and #chatbot) requires clarity. 

I tried several libraries and tools such as: #LIME, #SHAP, #GIRP, #CEM … 

GIRP is promising to simplify the understanding. Yet, more work is required to get newer tools that incorporate the explainable component by design. 

One final note for further discussions:
1. Explainability facilitates the adoption of AI at the bottom of the pyramid (the industries that are slowly adopting AI)
2. Offering monetize-able AI solutions for conventional decision makers 
3. XAI might become part of compliance process with the spread of #responsible #AI laws and regulations."
101,11fpp9o,datascience,GPT-3,relevance,2023-03-02 02:08:10,"Working on a report for school, can someone answer a few ethics related data science questions for me.",VelvetRevolver_,False,0.6,1,https://www.reddit.com/r/datascience/comments/11fpp9o/working_on_a_report_for_school_can_someone_answer/,5,1677722890.0,"I have a report for school where I need to ask a data science professional some questions and then write a report on it. You don't have to answer all of them and the responses don't have to be that long they can be on the shorter side. Any help would be greatly appreciated. Thank you.

1) As AI advances do you see any possible risk of mass job displacement in the future? Such as self driving cars replacing truck drivers, taxi drivers, etc. Do you think this will be a problem in the future or will it be the same as all the times we've automated something in the past and people will just adapt to the job market?


2) Recently there has been some controversy around algorithmic bias, especially in data science. Such as automating parole decisions, job application screening, or applying for credit cards. Some people argue these machine learning algorithms have been somewhat racist/sexist in the past. Do you think data scientists should be doing their best to eliminate bias?


3) With the recent release of chat bots like chatGPT students have been using it to help them write papers or help with their programming assignments. Do you see anything wrong with claiming chatGPT's responses as your own work? Do you consider it plagiarism?


4) Art generation has gotten very popular recently and there have actually been some lawsuits against these AI's claiming they were trained on copyrighted images. Do you believe AI shouldn't be legally allowed to train on copyrighted material or will doing so significantly hinder AI's development as data scientists will have to be much more careful about how they obtain their datasets?


5) As AI and image detection/recognition algorithms become more withspread do you think adversarial attacks will ever be a significant and practical problem?"
102,134o5fe,datascience,GPT-3,relevance,2023-05-01 14:33:50,"[colabdog.com] I build an aggregator for AI News that combines OpenAI, Google AI, MIT, BAIR.",colabDog,False,0.78,5,https://i.redd.it/rlizo23od8xa1.png,1,1682951630.0,
103,13ilm03,datascience,GPT-3,relevance,2023-05-15 21:45:36,I investigated the Underground Economy of Glassdoor Reviews,ibsurvivors,False,0.99,1165,https://www.reddit.com/r/datascience/comments/13ilm03/i_investigated_the_underground_economy_of/,63,1684187136.0,"Online company reviews are high stakes.

Top reviews on sites like Glassdoor and Google can get thousands of impressions each month and are major drivers of brand perception.

Employers know this. And when I come across multiple 5 star reviews left with no cons, or a Pulitzer worthy essay from a former intern, I become suspicious.

These reviews start to resemble 30 under 30 lists: so artificially constructed that you begin to question their credibility in the first place.

The scrutiny around company reviews is well documented; some companies file lawsuits worth over a million dollars to reveal anonymous reviewers that complain about their jobs.

Whilst it's the flashy lawsuits that make the headlines, there also exists an underground economy of company reviews operating quietly every single day.

In this underground economy, some companies pay over $150 to freelancers to try and get a negative review removed. If they want “better” results, they go to the plethora of Online Reputation Management services (ORMs) in the United States that can charge retainers worth thousands of dollars.

The supply of positive reviews exists too. My research led me to find companies, including a prominent Y-Combinator backed startup, that solicit fake positive reviews from online freelancers to improve their rating.

Many of these mercenary fake reviewers, often based in South East Asia, make a full time living doing this, netting over $2,000 per month.

Some of these run such sophisticated operations that they’ve even created their own pricing tiers (e.g $35 per original review, $20 to post an already created review from an email address), a la SaaS offering.

Others operate on a contingency fee agreement model, where they only get paid if they’re able to take a negative review down.

The underground economy of company reviews is well and truly alive. And today we’re going to find out how it operates.

***Note***: For more content like this, [*subscribe*](https://www.careerfair.io/subscribe) *to my newsletter. In a couple of weeks, I'll be releasing my guide to writing a killer resume.*

**Adding reviews**

The barriers to entry for adding fake reviews are much lower than for getting reviews removed, so that’s where we’ll start.

To write an employer review, all you really need is the ability to create an email address. For most sites, you don’t need any proof of employment (say like a company specific email address).

I went on a gig marketplace site and posted a pretty vague post related to wanting to find out more on how to improve a company’s online presence.

Within minutes of posting a gig, my inbox was flooded with proposals:

https://preview.redd.it/esx3904qa20b1.png?width=3064&format=png&auto=webp&s=2ff3a2f8528fee99aabb830f27ea71a7569ebb2e

After a bit of chatting, I narrowed the scope of their services and summarized their rates into the table below:

|Channel|Cost|Timeline|Model|
|:-|:-|:-|:-|
|Freelancer #1|$10 per review|Monthly|Unlimited|
|Freelancer #2|$35 per original review, $20 per already created review|Monthly|Unlimited|
|Freelancer #3|$25 per review|Monthly|Unlimited|
|Freelancer #4|$25 per review|Monthly|10 reviews|
|Freelancer #5|$20 per review|Monthly|Unlimited|
|Online Reputation Management Agency|$300 subscription|Monthly|8 reviews|

Let’s dive a bit deeper into the services that Freelancer #5 offered.

Freelancer #5 explained to me he had been writing reviews for one particular company for the past 4 months now. Each month he wrote them 10 reviews.

&#x200B;

https://preview.redd.it/n1ddox6cb20b1.png?width=2684&format=png&auto=webp&s=5c271d0eec4328cb78d7d2cb85dfffa3f9eb72f8

In another message, he tells me he’s offering the same services to 5 other companies. Doing some quick math:

5 companies x 10 reviews per company x $25 per review = $1,250 per month

Considering the average person in Pakistan earns $150 per month, that’s not bad change at all.

One of the companies that he’s offering his services to includes a Y-Combinator backed startup. I won’t name the company, but here’s what its average Glassdoor review rating distribution looks like:

https://preview.redd.it/2np5b6fdb20b1.png?width=2420&format=png&auto=webp&s=f8cafaa85453b0933a18eb5c30f931b3bb893c46

5 star reviews account for over 77% of the company’s total reviews. Obviously, no one is buying fake reviews that make them look bad.

But here’s the thing: freelancers are getting quite smart when it comes to writing reviews that don’t look too fishy. They tend to do this by spacing the reviews out (so that they don’t come in “spikes” – more on this later) and they also make sure that they’re not always leaving the “cons” section blank.

Don’t get me wrong, if you come across this company’s reviews, it’d be pretty easy to tell they’re quite strange. In fact, I can’t even post some screenshots here because it’d give the company away immediately.

But it would be challenging to conclude that the above company is buying reviews just by analyzing review volume and distribution without actually reading some of the reviews.

The same company is also buying reviews on Google Reviews.

Sidenote: I got curious about how he’s been writing 50 reviews from 50 different emails per month. Would he actually create 50 different email addresses? And what about the IP address – doesn’t Glassdoor flag multiple reviews from the same IP?

One of the freelancers answered my question:

&#x200B;

https://preview.redd.it/g4id2yqeb20b1.png?width=2572&format=png&auto=webp&s=c2a77fdea8834a6d90f02b8b3eb67b3a874f3df2

Moving on – another company that seems to buy fake reviews seems to be having some more trouble. Approximately a month after a freelancer linked me to fake reviews he had written for this company, all five reviews that he had linked me to had been removed:

&#x200B;

https://preview.redd.it/99fdvcgfb20b1.png?width=3116&format=png&auto=webp&s=b7e244529fc62b5c824d925feb61fd2cc16cbfd5

Based on this [Glassdoor webinar](https://youtu.be/3iy0JWOS1gs) from 2018, “if it is found that a user has created multiple email accounts to submit reviews, then ALL submissions from that user are deleted” – so likely Glassdoor’s content moderation team flagged one of the initial reviews and the same freelancer who was writing reviews for that company had all the fake reviews deleted.

So far, it looks like the key to an effective fake review creation strategy lies in:

* Spacing the fake reviews out
* Writing each review from a different IP address (i.e benefit of being part of a team)
* Using language that isn’t an obvious giveaway

On that third point: the reality is that many of these freelancers’ first language is not English.

As an experiment, I turned to everybody’s favorite new toy, ChatGPT, and asked it to write me a positive Glassdoor review:

https://preview.redd.it/8w7cal9gb20b1.png?width=3164&format=png&auto=webp&s=860c39b11c5813e8b7fabdbb038d73c565cc98cf

And I’d say that the above answer was better than 95% of the fake reviews I came across.

**Removing reviews**

The process for removing an employer review usually works like this:

1. You identify one or multiple reviews that you want removed
2. You verify whether the review violates the site's Guidelines, or whether there’s something else about the review(s) that could get it removed.
3. You file an appeal to get it removed.

As an example, Glassdoor’s Review guidelines can be found [here](https://help.glassdoor.com/s/article/Community-Guidelines?language=en_US#:~:text=See%20More-,Review%C2%A0Guidelines,-Millions%20of%20job). Mainly, they forbid mentioning anyone by name who’s not an executive and revealing proprietary or confidential information, amongst a host of other things.

Sounds simple enough right? Well, according to one of the freelancers I messaged:

&#x200B;

https://preview.redd.it/x6s8hsyac20b1.png?width=2036&format=png&auto=webp&s=f86c386f864198dc43faeb41faea378090c20107

After some research, I summarized the different vendors and prices in the table below:

&#x200B;

|Channel|Cost|Timeline|Model|Self reported success rate|
|:-|:-|:-|:-|:-|
|Freelancer #1|$100 per review|3 days|Contingency Agreement Model|100%|
|Freelancer #2|$30 per review|7 days|Contingency Agreement Model|100%|
|Reputation management service #2|$450 per review|21 business days|Contingency Agreement Model|Unknown|
|Reputation management service #3|$1000 per review|Undefined|Contingency Agreement Model|100%|
|Reputation management service #4 Plan 1|$550 per review|5-6 weeks|Contingency Agreement Model|50-75%|
|Reputation management service #4 Plan 2|$300 Subscription + $100 per each review removed|Monthly service|Subscription plan|50-75%|
|Freelancer #3|$20|Undefined|Pay regardless|Undefined|
|Freelancer #4|$500|Undefined|Contingency Agreement Model|Undefined|

As you can see, unlike the fake review generation market, the prices vary quite a bit for getting reviews removed.

At one end, you have freelancers on gig marketplaces that will attempt to remove a review for less than $100. And then on the other end, you have ORMs (Online Reputation Management Agencies) that have multiple employees and more comprehensive packages in place. The one constant seems to be that most companies operate on a contingency agreement model (i.e pay only if review gets removed).

**Analyzing reviews**

ReviewMeta is a site that analyzes Amazon reviews and tells you how many are legitimate. The creator of the site, Tommy Noonan, mentions in an [interview with NPR](https://www.npr.org/sections/money/2018/06/27/623990036/episode-850-the-fake-review-hunter) that the main giveaway that a product is soliciting fake reviews is:

* A large, suspicious flood of positive reviews at the exact same time. For example, a 3 day stretch of time constituting 30% of total reviews.
* Phrases and words that are constantly repeated, especially in the section with no cons
* Brand monogamists (only review products from one company)

Whilst the last two bullets are hard to track, the first can be used to analyze different companies’ reviews and to check if there might be some funky business going on.

After a couple of days, I have the ability to track review volume and review ratings over time for any company that I specify:

https://preview.redd.it/ehcbw2oje20b1.png?width=1653&format=png&auto=webp&s=b448ff35eb9878fbb1686de2fa8cf031e4ed3e05

Let the games begin.

## Voluntary Response Bias

One of the biggest challenges that review platforms face is the Voluntary Response bias.

Research shows many of today’s most popular online review platforms (e.g Amazon) have a distribution of opinion that is highly polarized, with many extreme positive and/or negative reviews, and few moderate opinions.

Think about it: have you ever felt moderately satisfied at your job and thought to yourself, now would be a great time to leave a Glassdoor review? Probably not.

On the other hand, if you’ve had a terrible experience or even just had one thing really flip you off, you might be quite likely to leave an angry review.

Consider when a company goes through layoffs. You’re going to have a flood of angry reviews coming your way and are likely going to experience a “spike” in reviews.

**Note:** Just like the Wall Street Journal’s methodology described [here](https://archive.is/20201016094732/https://www.wsj.com/articles/companies-manipulate-glassdoor-by-inflating-rankings-and-pressuring-employees-11548171977#selection-3965.0-3968.0), I considered there to be a spike if the total number of reviews in the month was greater than three standard deviations above the mean of the surrounding months.

Let’s take the company below. Here’s a graph of of their review volume since Jan 2020, including when they announced one of their first round of layoffs in June 2022:

https://preview.redd.it/n6kd9ejle20b1.png?width=3216&format=png&auto=webp&s=9eea2f3836617feca37eb88b1d3f67c8fa1b6fe2

In June 2022, approximately 19% of this company's 52 reviews were 1 star reviews (compared to an overall average of around 10%). This is what we could call a statistically significant spike in reviews. It also illustrates how the employees most likely to leave reviews are the ones that obviously had a bad experience (i.e getting laid off).

Here’s another company that had a similar spike in negative reviews due to layoffs in November 2022:

https://preview.redd.it/4vcnr1ine20b1.png?width=2408&format=png&auto=webp&s=f3877fb315ccc5d9a9294306a9f86616cb0fabd2

This company had an approximate 20% 1 star review rate (compared to an overall average of 12%) in November 2022, as well as an Avg Rating of 2.96 that month (compared to an overall average rating of 3.73).Unless HR is proactive, their reviews page risks succumbing to an echochamber of negative reviews that can really tilt one way.

**Note:** Glassdoor does state (based on [this video](https://www.youtube.com/watch?v=3iy0JWOS1gs) from 2017) that about 75% of the reviews on their platform are neutral. Their “give to get policy” has helped in keeping the platform from becoming too polarized.

I can understand why HR teams, like the ones that Nader talked to me about earlier, take a proactive stance towards managing their reviews. If they don’t try to control their reputation themselves, then their reputation risks getting controlled by the employees that had the worst possible experience.

## Goodhart’s Law

Goodhart’s law states the following:

*""When a measure becomes a target, it ceases to be a good measure""*

Every October, Glassdoor publishes their Best Places To Work ranking.

In a [report](https://www.wsj.com/articles/companies-manipulate-glassdoor-by-inflating-rankings-and-pressuring-employees-11548171977) that the WSJ did a couple of years ago, they found large spikes in the number of reviews that some companies (e.g SpaceX, Bain & Co, etc) got in September. The logic here is that some companies try to artificially inflate their Glassdoor reviews right before the October deadline.

I decided to revisit some of this analysis with Glassdoor’s 2023 Best Places To Work Ranking.

One of the companies I examined is rated as one of the best places to work in 2023. Let’s refer to this company as FunPlaceToWork.

Here is how their review volume looks like for all of 2022:

https://preview.redd.it/4e656zkqe20b1.png?width=2516&format=png&auto=webp&s=07141a66c56be7a6818efb9b1a4d912ee0021c91

FunPlaceToWork got around 50 reviews in September 2022. Of those 50 reviews, 96% were 5 star reviews.

FunPlaceToWork averaged 12 reviews per month up till then in 2022. Also, in the prior six months, the average percent of 5 star reviews received every month was \~75%.

Both the spike in volume of reviews and the spike in percentage of five star reviews are statistically significant.

I find it strange that Glassdoor’s proprietary algorithm and/or Human Content Moderation team did not find a spike of this nature unusual. If we look at Glassdoor’s eligibility criteria for the award, it’s as follows:

https://preview.redd.it/hag04y7se20b1.png?width=2868&format=png&auto=webp&s=ec2b920e126a8ea42b40d35aaa55d5341e69d022

The goal, according to Glassdoor, is to collect “authentic and unbiased reviews”.

Whilst there’s nothing against the rules for asking your employees to leave you reviews, I find the statistically significant spike of reviews at odds with the goal of collecting ""unbiased and authentic"" reviews (which Glassdoor states is the purpose of the awards).

Glassdoor states that an employer is allowed to ask its employees to leave reviews, but that they are not allowed to “coerce” them. Examples of what you can’t do:

* Offer incentives like Gift Cards in exchange for positive reviews.
* Withholding their reference letter unless they leave you a positive review.
* Anything that leads you to require proof for the employee to show you that they wrote a review.

It is possible to play by the rules (i.e not break any of the above rules) and to still in my opinion not collect authentic and unbiased reviews.

They say that you shouldn’t hate the player but the game – I think **FunPlaceToWork** played by the rules, won fair and square, and that this is simply a perfect example of Goodhart’s Law.

I reached out to Glassdoor ([awards@glassdoor.com](mailto:awards@glassdoor.com)) about the above and this is the reply I got:

https://preview.redd.it/x0dqq39ue20b1.png?width=4800&format=png&auto=webp&s=c0102c963be9486370b340f2f473cbc6650fc48a

**Conclusion**

When I was 22, on an [F1 visa with 3 months to find work](https://www.careerfair.io/job-hunt-story), I didn’t give a damn about bad reviews. I needed a job and I’d sign any piece of paper you put in front of me.

Compare that to someone at the peak of their career, someone with optionality and a multitude of job offers; an “A-Player”, as the experts call it, would absolutely have the luxury of choice and discard a job offer based on bad company reviews.

For most people, the impact of online company reviews lies somewhere in the middle. In marketing, there’s a concept of a “marketing touchpoint” - an interaction with the brand over the course of the whole buying journey.

Company reviews are one of the many touchpoints a job seeker experiences over their interview process. And with the technology industry booming the past couple of years, companies couldn’t afford to slack on any touchpoints, including this one.

After all, when others start to game the system, you’re at a disadvantage if you don’t. The rewards can be quite high. Certainly higher than just trying to be as transparent as possible.

HR leaders are often more incentivized to inflate their metrics than to get honest feedback. Fake review writers have bills to pay. ORMs know that companies are desperate. And the platforms, well, aren’t always paying attention.

The result is a potluck of interests that leads to an underground economy.

One that ends up hurting the job seeker.

\*\*\*

Whew. That took a while (about 3 months in fact). Thanks for reading. For more content like this, [subscribe](https://www.careerfair.io/subscribe) to my newsletter. It's my best content delivered to your inbox once every 2 weeks."
104,11szca1,datascience,GPT-3,relevance,2023-03-16 16:45:29,"What is your digital workspace, tools, setup, etc. for ETL, research, production?",BreathAether,False,0.95,36,https://www.reddit.com/r/datascience/comments/11szca1/what_is_your_digital_workspace_tools_setup_etc/,16,1678985129.0,"I'm new to this and so I've been wanting to know what other people have been using to make their work feel as smooth as butter. Since I've been learning lots and not just the industry standard stuff, I wanted to share what little I found to be valuable which others may want to try. **The main goal of this post is to share, critique, and provide suggestions so that we can all find the setup we like most. I  am also looking for new, up and coming tech, and definitely not afraid to try new things!**

IDE: **VSCode with the Jupyter Notebook Extension**. What I like about it is that I can view data structures like series/dataframes in a table format by clicking the variable in the Jupyter: Variables pane at the bottom. I started with plain vanilla jupyter notebooks from Anaconda so this was pretty nice. I have seen demos that **Jupyter Lab** has something like this, so if anyone has used both VSCode's notebooks and used Lab, your input would be appreciated. I hear good things about **PyCharm and Spyder**. Some people also use **Google Collab, DataSpell, and DeepNote** but I don't know enough about it. I did play around with DeepNote, and it was very cool but I didn't feel compelled to switch (and you have to pay for it!). 

Tools:

* A code helper: A few months back I was googling everything and I would've listed **Stackoverflow**. I might actually use that occasionally, but these days I use **ChatGPT** and **Bing AI**. For more current info or news-based I'll use Bing AI since it uses live search results, and for information that is knowledge based I might use ChatGPT. ChatGPT saves conversations so it's great for exploring topics in depth and referencing that conversation later. For those who have used both, maybe you know what I'm talking about and can provide a better explanation as to which is better for what purpose.
* Software: **Excel** is an obvious one. For instance, if I have a huge dataset and I just want to delete out columns that I don't need with Ctrl+click to select, it's easier and quicker than copy + pasting or typing out each of the string column names I want to ""df.drop()"". Excel is great for quick and simple stuff. Some software I have been learning about are I guess what I would consider as no- or low-code data analytics platforms, such as **Alteryx, KNIME, and Orange**. These software let you practically run an entire ETL pipeline. I believe Alteryx and KNIME are the gold-standard in this category, and Orange is a ""lite"" version of the two and is available in Anaconda. I think these are pretty cool, and I personally haven't found a huge use case for them since I've been chugging away in my notebooks with Python, but I can see the value. Would love for someone to chime in on these tools and how they compare to manually doing stuff in code, especially for large datasets.
* Version Control: This is where I'm primarily lacking, but I know that **Github** is the go-to. I don't use this but I know that a ton of people do. I don't even know where to start to be honest. I usually just create a new .ipynb file for each analysis or phase of an ETL pipeline haha. I'm also not too aware of what other innovative tools for version control exist.
* Python Libraries: Besides the obvious stuff like Pandas/NumPy, MatplotLib/Seaborn, and your popular ML libraries, I've recently found out about this library called **Polars**. It's basically a Rust version of Pandas, and it's super powerful. Some operations that I've run, that would've taken hours with Pandas, took me minutes. But I've been hearing that **Pandas 2.0** which will be released some time this month, has been looking at using PyArrow dtypes (if I recall correctly) and the speed is comparable to Polars. I mean these two are FAST. Another contender is **DuckDB** but I think the new Pandas and Polars are still faster. I mostly use Pandas but if there is some heavy lifting, I'll swap the dataframe to a polars one with a quick function, run it with polars, then back to pandas.

Anyway, that's just some things I can immediately think of. Looking forward to your suggestions! Bonus points for anything new and innovative. Cheers.

https://preview.redd.it/qj2cywt1r4oa1.png?width=1920&format=png&auto=webp&s=c4e02f5fec0b3768df336c7a3f63cc382b3954a8"
