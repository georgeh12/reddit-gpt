,id,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,url,num_comments,created,body
0,za73dc,deeplearning,GPT,top,2022-12-02 01:35:02,GPT-3 Generated Rap Battle between Yann LeCun & Gary Marcus,hayAbhay,False,0.99,136,https://i.redd.it/ybfcfvez1e3a1.png,16,1669944902.0,
1,10mhyek,deeplearning,GPT,top,2023-01-27 10:45:48,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.95,115,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
2,12c43uu,deeplearning,GPT,top,2023-04-05 01:36:40,Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT,Time_Key8052,False,0.95,85,https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/,19,1680658600.0,"Vicuna : ChatGPT Alternative, Open-Source, High Quality and Low Cost 

&#x200B;

[ Relative Response Quality Assessed by GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a072b91)

Vicuna-13B has demonstrated competitive performance against other open-source models, such as Stanford Alpaca, by fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.

Evaluation using GPT-4 as a judge shows that Vicuna-13B achieves more than 90% of the quality of OpenAI ChatGPT and Google Bard AI, while outperforming other models such as Meta LLaMA (Large Language Model Meta AI) and Stanford Alpaca in more than 90% of cases.

The cost of training Vicuna-13B is approximately $300.

The training and serving code, along with an online demo, are publicly available for non-commercial use.

&#x200B;

More Information : [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT](https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)

Discord Server : [https://discord.gg/h6kCZb72G7](https://discord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysorg](https://twitter.com/lmsysorg)"
3,1325a0j,deeplearning,GPT,top,2023-04-28 18:10:08,The Little Book of Deep Learning is a 140 page (phone-formatted!) technical introduction of the necessary background for denoising diffusion and GPT models. BY-NC-SA.,FrancoisFleuret,False,0.98,81,https://fleuret.org/public/lbdl.pdf,6,1682705408.0,
4,10fw22o,deeplearning,GPT,top,2023-01-19 07:55:49,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.89,70,https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/,11,1674114949.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=webp&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
5,1096byl,deeplearning,GPT,top,2023-01-11 14:41:25,What do you all think about these “SEO is Dead” articles?,Aggressive-Twist-252,False,0.92,69,https://www.reddit.com/r/deeplearning/comments/1096byl/what_do_you_all_think_about_these_seo_is_dead/,3,1673448085.0,"I keep seeing [articles](https://jina.ai/news/seo-is-dead-long-live-llmo/) like this over the years and it made me wonder. Is SEO really dead? Or will it evolve? Back then I kept wondering if it’s true or not. Some believe SEO is dead, some don’t. But now with tools like Chat GPT and Midjourney, I think it’s time to take a look back and see how this might change SEO or if it will “kill” SEO.

I keep seeing threads and discussions seeing how people are excited and worried at the same time with how AI might be able to do a better job. But the way I see it, AI content still needs a person to tell it what to do and make the writing look nice. And also I think that the internet will have a lot of writing that was made by AI and that might change how we find things online. You might also see a ton of content being written by AI and trigger some plagiarism detectors and have a lot of websites get penalized. Hopefully the internet won’t be filled with boilerplate copy/pasted content coming from Chat GPT.

Well we have Google to filter out trash content anyway. But I know Google has some issues lately that they need to fix. One is that they also have AI that can help people find things on the internet with their search engine, and they need to make sure they are still the best in terms of search. 

The second is that Google needs to find a way to tell if something is really good or not, like how some websites that show art do. Google wants to show the best thing first, but it's hard because sometimes the thing that is the best is also something that Google's customers want people to see. It’s possible that some AI generated contentSo it's kind of tricky.

I have a feeling companies that already make SEO-writing and checking bots are gonna roll out some fresh new models soon. They're gonna be even better than before. These bots are going to write some good articles and product descriptions that are almost perfect. It almost looks like a human wrote the article or description. And all a human will do is quickly check for any false claims and write a headline that doesn't sound like a robot wrote it. 

We can only really tell 5-10 years from now. In the meantime, I’ll probably go back practicing some handyman skills and also go back teaching people how to drive and also be a service driver. These jobs I had in the past were way different from what I am earning now but if the worst comes to worst, at least I have these physical skills ready."
6,10okyg3,deeplearning,GPT,top,2023-01-29 22:39:07,[P] We built a browser extension that unlocks browser mode capabilities using ChatGPT: MULTI·ON: AI Web Co-Pilot powered by ChatGPT,DragonLord9,False,0.95,66,https://v.redd.it/2hw47h0b82fa1,8,1675031947.0,
7,10gs1ik,deeplearning,GPT,top,2023-01-20 08:53:58,Gotcha,actual_rocketman,False,0.94,64,https://i.redd.it/yyh41pnje7da1.jpg,5,1674204838.0,
8,125pbbf,deeplearning,GPT,top,2023-03-29 14:13:46,AI Startup Cerebras releases open source ChatGPT-like alternative models,Time_Key8052,False,0.96,49,https://gpt4chatgpt.tistory.com/entry/Cerebras-releases-open-source-ChatGPT-like-alternative-models,14,1680099226.0,
9,121agx4,deeplearning,GPT,top,2023-03-25 04:24:49,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.93,49,https://www.reddit.com/r/deeplearning/comments/121agx4/do_we_really_need_100b_parameters_in_a_large/,54,1679718289.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
10,112u10p,deeplearning,GPT,top,2023-02-15 09:25:30,[P] From “iron manual” to “Iron Man” — Augmenting GPT for fast editable memory to enable context aware question & answering,skeltzyboiii,False,1.0,43,https://i.redd.it/yujf2enambia1.gif,7,1676453130.0,
11,zaxg5m,deeplearning,GPT,top,2022-12-02 20:59:47,Everyone: AI will make it easy to spread misinformation; Me: Stop hitting yourself GPT3!,hayAbhay,False,0.92,40,https://i.redd.it/mrf9rz0ltj3a1.png,0,1670014787.0,
12,ylj1ux,deeplearning,GPT,top,2022-11-03 23:55:15,BlogNLP: AI Writing Tool,britdev,False,0.98,35,https://www.reddit.com/r/deeplearning/comments/ylj1ux/blognlp_ai_writing_tool/,9,1667519715.0,"Hey everyone,

I created this web app using Open AI's GPT-3 (Davinci model). The purpose here is to provide a free tool to allow people to generate blog content/outlines/headlines and help with writer's block. Will continue to improve it over time, but just a side project I figured would provide some value to you all. Hope you all enjoy and please share ❤️

[https://www.blognlp.com/](https://www.blognlp.com/)"
13,1350qtu,deeplearning,GPT,top,2023-05-01 20:45:08,What are some small LLM models or free LLM APIs for tiny fun project?,silent_lantern,False,0.97,37,https://www.reddit.com/r/deeplearning/comments/1350qtu/what_are_some_small_llm_models_or_free_llm_apis/,19,1682973908.0,"Hi, I'm looking for a free/opensource api to build a small GPT webapp for fun. I want to deploy it on something like Heroku and use Flask in the backend. 


I'm also open to uploading a small-ish llm model on Heroku and use that to answer chat like queries from users.


Do you know of any such small foss models and/or free APIs?"
14,zk5esp,deeplearning,GPT,top,2022-12-12 17:29:39,ChatGPT context length,Wild-Ad3931,False,0.97,30,https://www.reddit.com/r/deeplearning/comments/zk5esp/chatgpt_context_length/,10,1670866179.0,"How come ChatGPT can follow entire discussions whereas nowaday's LLM are limited (to the best of my knowledge) 4096 tokens ?

I asked it and it is not able to answer neither, and I found nothing on Google because no paper is published. I was also curious to understand how come Beamsearch was so fast with ChatGPT.

https://preview.redd.it/o6djsmpb5i5a1.png?width=1126&format=png&auto=webp&s=98baae5bf6fa294db408f0530214f8afa8a32a0b"
15,12wxrrd,deeplearning,GPT,top,2023-04-24 01:17:58,Can an average person learn how to build a LLM model?,sch1zoph_,False,0.71,26,https://www.reddit.com/r/deeplearning/comments/12wxrrd/can_an_average_person_learn_how_to_build_a_llm/,29,1682299078.0,"Hello everyone. I am a 30-year-old Korean male.

To be honest, I have never really studied properly in my life. It's a little embarrassing, but that's the truth.

Recently, while using ChatGPT, I had a dream for the first time. I want to create a chatbot that can provide a light comfort to people who come for advice. I would like to create an LLM model using Transformer, and use our country's beginner's counseling manual as the basis for the database.

I am aware that there are clear limits to the level of comfort that can be provided. Therefore, if the problem is too complex or serious for this chatbot to handle, I would like to recommend the nearest mental hospital or counseling center based on the user's location. And, if the user can prove that they have visited the hospital (currently considering a direction where the hospital or counseling center can provide direct certification), I would like to create a program that provides simple benefits (such as a free Starbucks coffee coupon).

I also thought about collecting a database of categories related to people's problems (excluding personal information) and selling it to counseling or psychiatric societies. I think this could be a great help to these societies.

The problem is that I have never studied ""even once,"" and I feel scared and fearful of the unfamiliar sensation. I have never considered myself a smart person.

However, I really want to make this happen! Our country is now in a state of constant conflict, and people hate and despise each other due to strong propaganda.

As a result, the birth rate has dropped to less than 1%, leading to a decline in the population. Many people hide their pain inside and have no will to solve it. They just drink with their friends to relieve their pain. This is obviously not a solution. Therefore, Korea has a really serious suicide rate.

I may not be able to solve this problem, but I want to put one small brick to build a big barrier to stop hatred. Can an ordinary person who knows nothing learn the common sense and study needed to build an LLM model? And what direction should one take to study one by one?"
16,12jb4xz,deeplearning,GPT,top,2023-04-12 05:21:13,Is OpenAI’s Study On The Labor Market Impacts Of AI Flawed?,LesleyFair,False,0.96,26,https://www.reddit.com/r/deeplearning/comments/12jb4xz/is_openais_study_on_the_labor_market_impacts_of/,1,1681276873.0,"[Example img\_name](https://preview.redd.it/f3hrmeet1eta1.png?width=1451&format=png&auto=webp&s=20e20b142a2f88c3d495177e540f34bc8ea4312b)

We all have heard an uncountable amount of predictions about how AI will ***terk err jerbs!***

However, here we have a proper study on the topic from OpenAI and the University of Pennsylvania. They investigate how Generative Pre-trained Transformers (GPTs) could automate tasks across different occupations \[1\].

Although I’m going to discuss how the study comes with a set of “imperfections”, the findings still make me really excited. The findings suggest that machine learning is going to deliver some serious productivity gains.

People in the data science world fought tooth and nail for years to squeeze some value out of incomplete data sets from scattered sources while hand-holding people on their way toward a data-driven organization. At the same time, the media was flooded with predictions of omniscient AI right around the corner.

*Let’s dive in and take an* exciting glimpse into the future of labor markets\*!\*

# What They Did

The study looks at all US occupations. It breaks them down into tasks and assesses the possible level of for each task. They use that to estimate how much automation is possible for a given occupation.

The researchers used the [O\*NET database,](https://www.onetcenter.org/database.html) which is an occupation database specifically for the U.S. market. It lists 1,016 occupations along with its standardized descriptions of tasks.

The researchers annotated each task once manually and once using GPT-4. Thereby, each task was labeled as either somewhat (<50%) or significantly (>50%) automatable through LLMs. In their judgment, they considered both the direct “exposure” of a task to GPT as well as to a secondary GPT-powered system, e.g. LLMs integrated with image generation systems.

To reiterate, a higher “exposure” means that an occupation is more likely to get automated.

Lastly, they enriched the occupation data with wages and demographic information. This was used to determine whether e. g. high or low-paying jobs are at higher risk to be automated.

So far so good. This all sounds pretty decent. Sure, there is a lot of qualitative judgment going into their data acquisition process. However, we gotta cut them some slag. These kinds of studies always struggle to get any hard data and so far they did a good job.

However, there are a few obvious things to criticize. But before we get to that let’s look at their results.

# Key Findings

The study finds that 80% of the US workforce, across all industries, could have at least some tasks affected. Even more significantly, 19% of occupations are expected to have at least half of their tasks significantly automated!

Furthermore, they find that higher levels of automation exposure are associated with:

* Programming and writing skills
* Higher wages (contrary to previous research!)
* Higher levels of education (Bachelor’s and up)

Lower levels of exposure are associated with:

* Science and critical thinking skills
* Manual work and tasks that might potentially be done using physical robots

This is somewhat unsurprising. We of course know that LLMs will likely not increase productivity in the plumbing business. However, their findings underline again how different this wave is. In the past, simple and repetitive tasks fell prey to automation.

*This time it’s the suits!*

If we took this study at face value, many of us could start thinking about life as full-time pensioners.

But not so fast! This, like all the other studies on the topic, has a number of flaws.

# Necessary Criticism

First, let’s address the elephant in the room!

OpenAI co-authored the study. They have a vested interest in the hype around AI, both for commercial and regulatory reasons. Even if the external researchers performed their work with the utmost thoroughness and integrity, which I am sure they did, the involvement of OpenAI could have introduced an unconscious bias.

*But there’s more!*

The occupation database contains over 1000 occupations broken down into tasks. Neither GPT-4 nor the human labelers can possibly have a complete understanding of all the tasks across all occupations. Hence, their judgment about how much a certain task can be automated has to be rather hand-wavy in many cases.

Flaws in the data also arise from the GPT-based labeling itself.

The internet is flooded with countless sensationalist articles about how AI will replace jobs. It is hard to gauge whether this actually causes GPT models to be more optimistic when it comes to their own impact on society. However, it is possible and should not be neglected.

The authors do also not really distinguish between labor-augmenting and labor-displacing effects and it is hard to know what “affected by” or “exposed to LLMs” actually means. Will people be replaced or will they just be able to do more?

Last but not least, lists of tasks most likely do not capture all requirements in a given occupation. For instance ""making someone feel cared for"" can be an essential part of a job but might be neglected in such a list.

# Take-Away And Implications

GPT models have the world in a frenzy - rightfully so.

Nobody knows whether 19% of knowledge work gets heavily automated or if it is only 10%.

As the dust settles, we will begin to see how the ecosystem develops and how productivity in different industries can be increased. Time will tell whether foundational LLMs, specialized smaller models, or vertical tools built on top of APIs will be having the biggest impact.

In any case, these technologies have the potential to create unimaginable value for the world. At the same time, change rarely happens without pain. I strongly believe in human ingenuity and our ability to adapt to change. All in all, the study - flaws aside - represents an honest attempt at gauging the future.

Efforts like this and their scrutiny are our best shot at navigating the future. Well, or we all get chased out of the city by pitchforks.

Jokes aside!

What an exciting time for science and humanity!

As always, I really enjoyed making this for you and I sincerely hope you found value in it!

If you are not subscribed to the newsletter yet, [click here to sign up](https://thedecoding.net/)! I send out a thoughtful 5-minute email every week to keep you in the loop about machine learning research and the data economy.

*Thank you for reading and I see you next week ⭕!*

**References:**

\[1\] [https://arxiv.org/abs/2303.10130](https://arxiv.org/abs/2303.10130)"
17,11ium8l,deeplearning,GPT,top,2023-03-05 11:10:56,LLaMA model parallelization and server configuration,ChristmasInOct,False,1.0,26,https://www.reddit.com/r/deeplearning/comments/11ium8l/llama_model_parallelization_and_server/,8,1678014656.0,"Hey everyone,

First of all, tldr at bottom, typed more than expected here.  

Please excuse the rather naive perspective I have here.  I've followed along with great interest, but this is not my industry.

Regardless, I have spent the past 3-4 days falling down a brutally obsessive rabbit hole, and I cannot seem to find this information.  I'm assuming it's just that I am missing context of course, and regardless of whether there is a clear answer, I'm trying to get a better understanding of this topic so that I could better appraise the situation myself.

Really I suppose I have two questions.  **The first** is regarding model parallelization.

I'm assuming this is not generic whatsoever.  What is the typical process engineers go about for designing such a pipeline?  Specifically in regards to these new LLaMA models, is something like ALPA relevant?  Deepspeed?

More importantly, what information should I be seeking to determine this myself?

This roughly segues to my **second inquiry**.

The reason I'm curious about splitting the model pipeline etc., is that I am potentially in interested in standing a server up for this.  Although I don't have much of a budget for this build (\~$30-40K is the rough top-end, but I'd be a lot happier around $20-25K), the money is there if I can genuinely satisfy my use-case.

I work at a small, but borderline manic startup working on enterprise software; 90% of the work we're doing based in the react/node ecosystem, some low-level work for backend services, and some very interesting database work that I have very little to do with.  I am a fullstack engineer that grew up playing with C++ => C#, and somehow ended up spending all of my time r/w'ing javascript.  Lol.  Anyways.

Part of our roadmap since GPT-3 and the playground were made publicly accessible, involves usage of these transformer models, and their ability to interpret natural language inputs, whether from user inputs, or scraped input values generated somewhere in a chain of requests / operations.

Seeing GPT-3 in action made me specifically realize that my estimations on this technology had been wildly off.  Seeing ChatGPT in action and uptick, the API's becoming available, has me further panicked.

Running our inference through their API has never really been an option for us.  I haven't even really looked that far into it, but bottom line the data running through our platform is all back-office, highly sensitive business information, and many have agreements explicitly restricting the movement of data to or from any cloud services, with Microsoft, Amazon, and Google all specifically mentioned.

Regardless of the reasoning for these contracts, the LLaMA release has had me obsessed over this topic in more detail than before, and whether or not I would be able to get this setup privately, for our use-case.

**To get to the actual second inquiry**:

Say I want to throw a budget rig together for this in a server cabinet.  Am I able to effectively parallelize the LLaMA model, well enough to justify going with 24GB VRAM 4090's in the rig?  Say I do so with DeepSpeed, or some of the standard model parallelization libraries.

Is the performance cost low enough to justify taking the extra compute here over 1/3 - 1/2 as many RTX6000 ADA's?

Or should I be grabbing the 48GB ADA's?

Like I said, I apologize for the naivety, I'm really looking for more information so that I can start to put this picture together better on my own.  It really isn't the easiest topic to research with how quickly things seem to move, and the giant gap between conversation depths (gamer || phd in a lot of the most interesting or niche discussions, little between).

Thank you very much for your time.

TL;DR - Any information on LLaMA model parallelization at the moment?  Will it be compatible with things like zero or alpa?  How about for throwing a rig together right now for fine-tuning and then running inference on the LLaMA models?  48GB 6000 ADA's, or 24GB 4090's?

Planning on putting it in a mostly empty 42U cabinet that also houses our primary web server and networking hardware, so if there is a sales pitch for 4090's across multiple nodes here, I do have a massive bias as the kind of nerd that finds that kind of hardware borderline erotic.

Hydro and cooling are not an issue, just usage of the budget and understanding the requirements / approach given memory limitations, and how to avoid communication bottlenecks or even balance them against raw compute.

Thanks again everyone!"
18,zen8l4,deeplearning,GPT,top,2022-12-07 00:33:41,Are currently state of art model for logical/common-sense reasoning all based on NLP(LLM)?,Accomplished-Bill-45,False,0.94,21,https://www.reddit.com/r/deeplearning/comments/zen8l4/are_currently_state_of_art_model_for/,6,1670373221.0,"Not very familiar with NLP, but I'm playing around with OpenAI's ChatGPT; particularly impressed by its reasoning, and its thought-process.

Are all good reasoning models derived from NLP (LLM) models with RL training method at the moment?

What are some papers/research team to read/follow to understand this area better and stay on updated?

&#x200B;

&#x200B;

for ChatGPT. I've tested it with following cases

Social reasoning ( which does a good job; such as: if I'm going to attend meeting tonight. I have a suit, but its dirty and size doesn't fit. another option is just wear underwear, the underwear is clean and fit in size. Which one should I wear to attend the meeting. )

Psychological reasoning ( it did a bad job.I asked it to infer someone's intention given his behaviours, expression, talks etc.)

Solving math question ( it’s ok, better then Minerva)

Asking LSAT logic game questions ( it gives its thought process, but failed to give correct answers)

I also wrote up a short mystery novel, ( like 200 words, with context) ask if it can tell is the victim is murdered or committed suicide; if its murdered, does victim knows the killer etc. It actually did ok job on this one if the context is clearly given that everyone can deduce some conclusion using common sense."
19,12yqpnp,deeplearning,GPT,top,2023-04-25 17:53:47,"Microsoft releases SynapseMl v0.11 with support for ChatGPT, GPT-4, causal learning, and more",mhamilton723,False,0.87,22,https://www.reddit.com/r/deeplearning/comments/12yqpnp/microsoft_releases_synapseml_v011_with_support/,0,1682445227.0,"Today Microsoft launched SynapseML v0.11 with support for ChatGPT, GPT-4, distributed training of huggingface and torchvision models, an ONNX Model hub integration, Causal Learning with EconML, 10x memory reductions for LightGBM, and a newly refactored integration with Vowpal Wabbit. To learn more check out our release notes and please feel give us a star if you enjoy the project!

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.11.0](https://github.com/microsoft/SynapseML/releases/tag/v0.11.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919)

Thank you to all the contributors in the community who made the release possible!

&#x200B;

[What's new in SynapseML v0.11](https://preview.redd.it/9pqj1mowj2wa1.png?width=4125&format=png&auto=webp&s=a358e73760c847a09cc76f2ed17dc58e15aed5ed)"
20,zb2kkc,deeplearning,GPT,top,2022-12-03 00:17:31,A GPT-3 based Chrome Extension that debugs your code!,VideoTo,False,0.97,21,https://www.reddit.com/r/deeplearning/comments/zb2kkc/a_gpt3_based_chrome_extension_that_debugs_your/,0,1670026651.0,"Link - [https://chrome.google.com/webstore/detail/clerkie-ai/oenpmifpfnikheaolfpabffojfjakfnn](https://chrome.google.com/webstore/detail/clerkie-ai/oenpmifpfnikheaolfpabffojfjakfnn)

Built  a quick tool I thought would be interesting - it’s a chrome extension  that uses GPT-3 under the hood to help debug your programming errors  when you paste them into Google (“eg. TypeError:…”).

This is definitely early days, so **if   this is something you would find valuable and wouldn't mind testing a   couple iterations of, please feel free to join the discord** \-> [https://discord.gg/KvG3azf39U](https://discord.gg/KvG3azf39U)

&#x200B;

https://i.redd.it/tt6hcqn2tk3a1.gif"
21,10irh5u,deeplearning,GPT,top,2023-01-22 19:11:36,Apple M2 Max 96 GB unified memory for larger models vs multiple 24GB GPUs or 40GB A100s?,lol-its-funny,False,1.0,23,https://www.reddit.com/r/deeplearning/comments/10irh5u/apple_m2_max_96_gb_unified_memory_for_larger/,14,1674414696.0,"How feasible is it to use an Apple Silicon M2 Max, which has about [96 GB unified memory](https://www.apple.com/shop/buy-mac/macbook-pro/16-inch-space-gray-apple-m2-max-with-12-core-cpu-and-38-core-gpu-1tb) for ""large model"" deep learning? I'm inspired by the the [Chinchilla](https://arxiv.org/abs/2203.15556) paper that shows a lot of promise at 70B parameters. Outperforming ultra large models like Gopher (280B) or GPT-3 (175B) there is hope for working with < 70B parameters without needing a super computer. At least for fine tuning. I've been working with GPT-J but want to scale/tinker with larger open-sourced models.

However, I don't know how clearly the CompSci theory (M2 Max's 38-core GPU, 16 core Neural Engine accessing 96 GB unified memory) maps out to the IT reality (toolkits and libraries on macOS actually using it). My exposure is mostly around Jupyter books on Colab Pro+ (A100s) and nvidia 3080 GPUs (locally). 

I appreciate your guidance."
22,12qq3mz,deeplearning,GPT,top,2023-04-18 15:00:24,Uni project: a FOSS LLM comparison tool - would you find this useful?,copywriterpirate,False,1.0,20,https://www.reddit.com/gallery/12qq3mz,3,1681830024.0,
23,11yy5es,deeplearning,GPT,top,2023-03-22 21:55:31,ChatLLaMA – A ChatGPT style chatbot for Facebook's LLaMA,imgonnarelph,False,0.93,18,https://chatllama.baseten.co/,2,1679522131.0,
24,12nvtm3,deeplearning,GPT,top,2023-04-16 04:52:51,"BERT Explorer - Analyzing the ""T"" of GPT",msahmad,False,0.88,17,https://www.reddit.com/r/deeplearning/comments/12nvtm3/bert_explorer_analyzing_the_t_of_gpt/,0,1681620771.0,"If you want to dig deeper into NLP, LLM, Generative AI, you might consider starting with a model like BERT. This tool helps in exploring the inner working of Transformer-based model like BERT. It helped me understands some key concepts like word embedding, self-attention, multi-head attention, encoder, masked-language model, etc. Give it a try and explore BERT in a different way.

BERT == Bidirectional Encoder Representations from TransformersGPT == Generative Pre-trained Transformer

They both use the Transformer model, but BERT is relatively simpler because it only uses the encoder part of the Transformer.

BERT Explorer[https://www.101ai.net/text/bert](https://www.101ai.net/text/bert)

https://i.redd.it/bxxboyyuhaua1.gif"
25,11mdvb9,deeplearning,GPT,top,2023-03-09 00:50:22,"AI generated video chapter titles (YouTube, Vimeo, etc)",happybirthday290,False,0.88,18,https://i.redd.it/h6utxsxg2mma1.png,1,1678323022.0,
26,10n8c80,deeplearning,GPT,top,2023-01-28 06:34:28,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,1.0,17,https://www.reddit.com/r/deeplearning/comments/10n8c80/a_python_module_to_generate_optimized_prompts/,0,1674887668.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features 🚀

* 🧙‍♀️ NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* 🔨 Easily add one-shot, two-shot, or few-shot examples to the prompt
* ✌ Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* 💥 Custom examples and samples can be easily added to the prompt
* 💰 Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Example](https://preview.redd.it/sjvhtd8b3nea1.png?width=1236&format=png&auto=webp&s=e9a3a28f41f59cd25fe8e95bd1fca56b15f27a6e)

&#x200B;

https://preview.redd.it/fnb05bys3nea1.png?width=1398&format=png&auto=webp&s=096f4e2cbd0a71e795f30cc5e3720316b5e5caf6"
27,11nfrhw,deeplearning,GPT,top,2023-03-10 05:22:26,[POC] ChatGPT Audio Bot (like Google Assistant and Alexa) - Opensource,JC1DA,False,0.84,17,https://www.reddit.com/r/deeplearning/comments/11nfrhw/poc_chatgpt_audio_bot_like_google_assistant_and/,2,1678425746.0,"Hey guys, just wanna share this bot that I quickly built using ChatGPT API.

GitHub page: [https://github.com/LanyTek/ChatGPT\_Audio\_Bot](https://github.com/LanyTek/ChatGPT_Audio_Bot)

This service allows you to keep talking to the bot using your voice like you often do with Google Assistant or Alexa. It can be used in a lot of scenarios like teaching, gossiping, or quickly retrieving information without the need of typing. 

I have a quick video demo here if you wanna check [https://www.youtube.com/watch?v=e9n0BJfMyKw](https://www.youtube.com/watch?v=e9n0BJfMyKw)

It's open source so feel free to use it for anything you like :)"
28,12ehc2m,deeplearning,GPT,top,2023-04-07 10:58:54,Text-to-image Diffusion Models in Generative AI: A Survey,Learningforeverrrrr,False,0.83,13,https://www.reddit.com/r/deeplearning/comments/12ehc2m/texttoimage_diffusion_models_in_generative_ai_a/,0,1680865134.0,"Diffusion models have become a SOTA generative modeling method for numerous content types, such as images, audio, graph, etc. As the number of articles on diffusion models has grown exponentially over the past few years, there is an increasing need for survey works to summarize them. Recognizing the existence of such works, our team has completed multiple field-specific surveys on diffusion models. We promote our works here and hope they can be helpful to researchers in relative fields: text-to-image diffusion models [\[a survey\]](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey), audio diffusion models [\[a survey\]](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI), and graph diffusion models [\[a survey\]](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material) .

In the following, we briefly summarize our survey on text-to-image diffusion models.

[Text-to-image Diffusion Models in Generative AI: A Survey](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)

As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis, i.e., text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.

Moreover, we have also completed two survey works on generative AI (AIGC) [\[a survey\]](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need) and ChatGPT [\[a survey\]](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era), respectively. Interested readers may give it a look."
29,11q8tir,deeplearning,GPT,top,2023-03-13 12:50:10,Learning logical relationships with neural networks with differential ILP,Neurosymbolic,False,1.0,14,https://www.reddit.com/r/deeplearning/comments/11q8tir/learning_logical_relationships_with_neural/,3,1678711810.0,"Since last week’s post on my lab’s software package [PyReason](https://neurosymoblic.asu.edu/pyreason/), I got a lot of questions on if it would be possible for a neural network to learn logical relationships from data. After all, ChatGPT seems to be able to generate Python code, and Meta released a NeurIPS paper to show you can learn math equations from data using a transformer-based model, so why not logic? In this article, we will one line of research in this area – differential ILP.

The research in this area really kicked off with a 2018 [paper from DeepMind](https://www.reddit.com/r/deeplearning/jair.org/index.php/jair/article/view/11172) where Richard Evans and Edward Grefenstette showed that you could adapt techniques from “inductive logic programming” to use gradient descent, and learn logical rules from data. Previous (non-neural) work on inductive logic programming was generally not designed to work with noisy data and instead fit the historical examples in a precise manner. Evans and Grefenstette utilized a neural architecture and a loss function – and they showed they could handle noisy data and even do some level of integration with CNN’s. Their neural architecture mimicked a set of candidate logical rules – and the rules assigned higher weights by gradient descent would be thought to best fit the data. However, a downside to this approach is that the neural network was [quintic in the size of the input](https://www.youtube.com/watch?v=SOnAE0EyX8c&list=PLpqh-PUKX-i7URwnkTqpAkSchJHvbxZHB&index=5). This is why they only applied their approach on very small problems – it did not see very wide adoption.

That said, in the last two years, there have been some notable follow-ons to this work. Researchers out of Kyoto University and NTT introduced a manner to learn rules that are more expressive in a different manner by allowing function symbols in the logical language ([Shindo et al., AAAI 2021](https://ojs.aaai.org/index.php/AAAI/article/view/16637/16444)). They leverage a clause search and refinement process to limit the number of candidate rules – hence limiting the size of the neural network. A student team from ASU created a presentation on their work for our recent seminar course on neuro symbolic AI. We released a three part video series from their talk:

[Part 1: Review of differentiable inductive logic programming](https://www.youtube.com/watch?v=JIS78a40q8U&t=270s)

[Part 2: Clause search and refinement In our recent video series](https://www.youtube.com/watch?v=nzfbxlHUwuE&t=345s)

[Part 3: Experiments](https://www.youtube.com/watch?v=-fKWNtHUIN0&t=27s)

[Slides](https://labs.engineering.asu.edu/labv2/wp-content/uploads/sites/82/2022/10/Shindo_dILP.pdf)

Some think that the ability to learn such relationships will represent a significant advancement in ML, specifically addressing shortcomings in areas such as knowledge graph completion and reasoning about scene graphs. However, the gap still remains wide, and ILP techniques, including differentiable ILP still have a ways to go. Really interested in what your thoughts are, feel free to comment below."
30,12xzadf,deeplearning,GPT,top,2023-04-24 22:41:22,AbridgIt - a browser extension that uses GPT to summarize any article you find on the web with a single click,nick313,False,0.71,11,https://www.reddit.com/r/deeplearning/comments/12xzadf/abridgit_a_browser_extension_that_uses_gpt_to/,4,1682376082.0,"Hi everyone,

I’d love your feedback on a new project I’m working on called [AbridgIt](http://www.abridgit.com/). When playing with GPT, one of my favorite things to ask it is to summarize long text. So, I built a simple Chrome browser extension that will automatically summarize any article you find on the web with a single click. This is version 1 so it’s pretty simple, but I would love to get some people to try it (it’s free) and give some feedback.

Example of how it works:

&#x200B;

https://preview.redd.it/m1ryu2u9uwva1.png?width=640&format=png&auto=webp&s=4626472cfaed0b1cedbb3492f1a1209491a8a265

 Check it out and let me know what you think."
31,zth8rl,deeplearning,GPT,top,2022-12-23 14:35:17,How to change career trajectory to NLP engineer,Creative-Milk-8266,False,0.84,12,https://www.reddit.com/r/deeplearning/comments/zth8rl/how_to_change_career_trajectory_to_nlp_engineer/,3,1671806117.0," A little of my background - 5 years experience in data science. Mostly related to prototyping statistical models and optimization problems, bringing them into production. Some experience in building pipeline and orchestration flow with AWS services.

I have basic understanding on Transformers, BERT, GPT. Did my first NLP Kaggle competition the first time recently.

I'd like my next job to be a NLP engineer. How should I prepare myself for it?

Here's some of the items I'm thinking

&#x200B;

1. More hands on projects I can put on resume, including integration with cloud services. Any recommendations on what kinds of projects I should pick?  
 
2. Tryout techniques of speeding up models like distilled model, dynamic shape, quantization. Anything else that would be helpful?  
 
3. Understand lower level of GPU programming knowledges. Not sure if this is helpful for me finding a NLP job. If so, what kind of things I can do to go deeper on this subject. I'm currently taking [Intro to Parallel Programming](https://classroom.udacity.com/courses/cs344) CS344 course on Udemy (highly recommend btw).  
 
4. Grind leetcode :/  
 

Please point out other important directions I missed."
32,129t3tl,deeplearning,GPT,top,2023-04-02 18:10:37,Should we draw inspiration from Deep learning/Computer vision world for fine-tuning LLMs?,Vegetable-Skill-9700,False,0.81,13,https://www.reddit.com/r/deeplearning/comments/129t3tl/should_we_draw_inspiration_from_deep/,8,1680459037.0,"With HuggingGPT, BloombergGPT, and OpenAI's chatGPT store, it looks like the world is moving towards specialized GPTs for specialized tasks. What do you think are the best tips & tricks when it comes to fine-tuning and refining these task-specific GPTs?

Over the last decade, I have built many computer vision models (for human pose estimation, action classification, etc.), and our general approach was always based on Transfer learning. Take a state-of-the-art public model and fine-tune it by collecting data for the given use case.

Do you think that paradigm still holds true for LLMs?

Based on my experience, I believe observing the model's performance as it interacts with real-world data, identifying failure cases (where the model's outputs are wrong), and using them to create a high-quality retraining dataset will be the key.

&#x200B;

P.S. I am building an open-source project UpTrain ([https://github.com/uptrain-ai/uptrain](https://github.com/uptrain-ai/uptrain)), which helps data scientists to do so. We just wrote a blog on how this principle can be applied to fine-tune an LLM for a conversation summarization task. Check it out here: [https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation\_summarization](https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation_summarization)"
33,zboc8w,deeplearning,GPT,top,2022-12-03 19:29:01,BlogNLP: AI Blog Writing Tool,britdev,False,0.88,12,https://www.reddit.com/r/deeplearning/comments/zboc8w/blognlp_ai_blog_writing_tool/,7,1670095741.0,"Hey everyone,

I developed this web app with Open AI's GPT-3 to provide a free, helpful resource for generating blog content, outlines, and more - so you can beat writer's block! I'm sure you'll find it useful and I'd really appreciate it if you shared it with others ❤️.

[https://www.blognlp.com/](https://www.blognlp.com/)"
34,1148t20,deeplearning,GPT,top,2023-02-17 02:54:52,How likely is ChatGPT to be weaponized as an information pollution tool? What are the possible implementation paths? How to prevent possible attacks?,zcwang0702,False,0.7,9,https://www.reddit.com/r/deeplearning/comments/1148t20/how_likely_is_chatgpt_to_be_weaponized_as_an/,14,1676602492.0,
35,13ib22w,deeplearning,GPT,top,2023-05-15 15:10:52,[P] ts-tok: Time-Series Forecasting with Classification,arpytanshu,False,0.93,12,https://www.reddit.com/r/deeplearning/comments/13ib22w/p_tstok_timeseries_forecasting_with_classification/,3,1684163452.0,"Hey everyone!  
I wanted to share with you a weekend project I've been working on called **ts-tok**. It's an experimental approach to time-series forecasting that uses classification instead of regression.  
Essentially, we take a range of time-series values and transform them into a fixed vocabulary of tokens. This allows for a seamless training of GPT like models without changing the architecture or loss function.  
There are some subtleties required for data preparation for training, and I've outlined these in the README, so feel free to check it out!  
While this approach 'may' not have practical applications in the real world, it's been a fun experiment to explore.  
I've included some forecasting results in the output/ folder, so feel free to check those out! Open to feedback from the community about potential use cases and limitations of this approach.  
Thanks for taking the time to read about this project!  
[https://github.com/arpytanshu1/ts-tok](https://github.com/arpytanshu1/ts-tok)"
36,11rfgbs,deeplearning,GPT,top,2023-03-15 00:07:51,GPTMinusOne - AI that hides the use of ChatGPT and GPT4,tomd_96,False,0.87,11,https://github.com/tom-doerr/gpt_minus_one,3,1678838871.0,
37,129k24i,deeplearning,GPT,top,2023-04-02 12:37:38,[N] Software 3.0 Blog Post Release 🔥,DragonLord9,False,0.73,10,https://www.reddit.com/r/deeplearning/comments/129k24i/n_software_30_blog_post_release/,3,1680439058.0,"Hi all, excited to share my blog post on [**Software 3.0**](https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm_campaign=post&utm_medium=web)

https://preview.redd.it/9b4hjkkhugra1.png?width=1500&format=png&auto=webp&s=e341f3ab4c3c8abb206df8daa17428a297ff61e2

The blog post offers an insightful read on the new GPT-powered programming paradigm where the new programming language is simply ""*English*"", as well as recent developments in AI.

The post was originally written before GPT-4 release, and the predictions seem to have held surprisingly well. Knowledge cutoff date 28 Feb 2023.

Please read and share!! Happy to answer any follow-ups here or on DM 😊

Tweet: [https://twitter.com/DivGarg9/status/1642229948185280521?s=20](https://twitter.com/DivGarg9/status/1642229948185280521?s=20)

Blog: [https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm\_campaign=post&utm\_medium=web](https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm_campaign=post&utm_medium=web)"
38,12egmab,deeplearning,GPT,top,2023-04-07 10:28:52,"Series of Surveys on ChatGPT, Generative AI (AIGC), and Diffusion Models",Learningforeverrrrr,False,0.85,8,https://www.reddit.com/r/deeplearning/comments/12egmab/series_of_surveys_on_chatgpt_generative_ai_aigc/,0,1680863332.0,"* **A survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)
* **A survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)
* **A survey on Text-to-image diffusion models:** [**Text-to-image Diffusion Models in Generative AI: A Survey**](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)
* **A survey on Audio diffusion models:** [**A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI**](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI)
* **A survey on Graph diffusion models:** [**A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material**](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material)

**ChatGPT goes viral.** Launched by OpenAI on November 30, 2022, ChatGPT has attracted unprecedented attention due to its powerful abilities all over the world.  It took only 5 days \[1\] and 2 months \[2\] for ChatGPT to have 1 million users and 100 million monthly users after launch, making it the fastest-growing consumer application in history. ChatGPT can be seen as the milestone for the GPT family to go viral. In academia, ChatGPT has also inspired a large number of works discussing its applications in multiple fields, with **more than 500 papers within four months** after release and **the number is still increasing rapidly.**  This brings a huge challenge for a researcher who hopes to have an overview of ChatGPT applications or hopes to start his or her journey with ChatGPT in their own field.  **To help more people keep up with the latest progress of the GPT family,** we’re glad to share a self-contained survey that not only summarizes **the recent applications** of ChatGPT and other GPT variants like GPT-4, but also introduces the **underlying techniques** and **challenges.** Please refer to the following link for the paper: [One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era).

&#x200B;

**From ChatGPT to Generative AI.**  One highlighting ability of the GPT family is that it can generate natural languages, which falls into the area of Generative AI. Apart from text, Generative AI can also generate content in other modalities, such as image, audio, and graph. More excitingly, Generative AI is able to convert data from one modality to another one, such as the text-to-image task (generating images from text). **To help readers have a better overview of Generative AI,** we provide a complete survey on underlying **techniques,** summary and development of **typical tasks in academia**, and also **industrial applications.** Please refer to the following link for the paper.  [A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

&#x200B;

**From Generative AI to Diffusion Models.** The prosperity of a field is always driven by the development of technology, and so is Generative AI.  Different from ChatGPT which generates text based on the transformer, **diffuson models** have greatly accelerated the development of other fields in Generative AI, such as image synthesis.  Although we provide a summary of diffusion models and typical tasks in the Generative AI survey, we cannot include detailed discussions due to paper length limitations. **For those who are interested in the technical details of diffusion models and the recent progress of their applications in Generative AI,** we provide three self-contained surveys on **how diffusion models are applied in three typical areas: Text-to-image diffusion models** (also includes related tasks such as image editing)**, Audio diffusion models** (including text to speech synthesis and enhancement), and **Graph diffusion models** (including molecule, protein and material areas). Please refer to the following links for the paper.

* [Text-to-image Diffusion Models in Generative AI: A Survey](https://www.researchgate.net/publication/369662720_Text-to-image_Diffusion_Models_in_Generative_AI_A_Survey)
* [A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI](https://www.researchgate.net/publication/369477230_A_Survey_on_Audio_Diffusion_Models_Text_To_Speech_Synthesis_and_Enhancement_in_Generative_AI)
* [A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material](https://www.researchgate.net/publication/369716257_A_Survey_on_Graph_Diffusion_Models_Generative_AI_in_Science_for_Molecule_Protein_and_Material)

We hope our survey series will help people for a better understanding of ChatGPT and Generative AI, and we will update the survey regularly to include the latest progress. Please refer to the personal pages of the authors for the latest updates on surveys. If you have any suggestions or problems, please feel free to contact us.

\[1\] Greg Brockman, co-founder of OpenAI, [https://twitter.com/gdb/status/1599683104142430208?lang=en](https://twitter.com/gdb/status/1599683104142430208?lang=en)

\[2\] Reuters, [https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/)"
39,117l2vf,deeplearning,GPT,top,2023-02-20 21:27:58,Fine tuning a GPT for text generation,nashcaps2724,False,0.82,7,https://www.reddit.com/r/deeplearning/comments/117l2vf/fine_tuning_a_gpt_for_text_generation/,6,1676928478.0,"Hi all, let me lay out my problem…

Imagine there are two corpora, Corpus A (100,000~) and Corpus B (20,000,000~). 

Individuals create reports for corpus A based on the information in corpus B. 

My idea was to pretrain a GPT on corpus A, and fine tune it to take documents from corpus B as an input, and output text in the style of corpus A (essentially a mix of text generation and summarization). 

Is this something folks think is even feasible? Should I be pretaining the GPT on both corpora or just corpus A? I thought of both fine tuning an OpenAI GPT and training from scratch. 

Any advice would be welcome!"
40,121agx4,deeplearning,GPT,comments,2023-03-25 04:24:49,Do we really need 100B+ parameters in a large language model?,Vegetable-Skill-9700,False,0.93,47,https://www.reddit.com/r/deeplearning/comments/121agx4/do_we_really_need_100b_parameters_in_a_large/,54,1679718289.0,"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \~25x smaller than GPT-3, challenging the notion that is big always better?

From my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?

Would love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?

P.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset"
41,12cvkvu,deeplearning,GPT,comments,2023-04-05 19:44:10,AI vs Humans: Can You Tell the Difference?,YoutubeStruggle,False,0.67,4,https://www.reddit.com/r/deeplearning/comments/12cvkvu/ai_vs_humans_can_you_tell_the_difference/,29,1680723850.0,"We would greatly appreciate your feedback on our[AI Content Detector](https://ai-content-detector.online/) that detects text generated by ChatGPT, a large language model trained by OpenAI. Our aim is to provide a reliable tool for distinguishing between human-written text and machine-generated text, and we would love to hear your thoughts on how effective the tool is in achieving this goal. Specifically, we would like to know if you found the site easy to navigate if the results provided were accurate, and if there are any additional features you would like to see implemented. Your feedback will help us to continue improving the site and provide the best possible experience for our users. Thank you in advance for your valuable input!"
42,12wxrrd,deeplearning,GPT,comments,2023-04-24 01:17:58,Can an average person learn how to build a LLM model?,sch1zoph_,False,0.72,27,https://www.reddit.com/r/deeplearning/comments/12wxrrd/can_an_average_person_learn_how_to_build_a_llm/,29,1682299078.0,"Hello everyone. I am a 30-year-old Korean male.

To be honest, I have never really studied properly in my life. It's a little embarrassing, but that's the truth.

Recently, while using ChatGPT, I had a dream for the first time. I want to create a chatbot that can provide a light comfort to people who come for advice. I would like to create an LLM model using Transformer, and use our country's beginner's counseling manual as the basis for the database.

I am aware that there are clear limits to the level of comfort that can be provided. Therefore, if the problem is too complex or serious for this chatbot to handle, I would like to recommend the nearest mental hospital or counseling center based on the user's location. And, if the user can prove that they have visited the hospital (currently considering a direction where the hospital or counseling center can provide direct certification), I would like to create a program that provides simple benefits (such as a free Starbucks coffee coupon).

I also thought about collecting a database of categories related to people's problems (excluding personal information) and selling it to counseling or psychiatric societies. I think this could be a great help to these societies.

The problem is that I have never studied ""even once,"" and I feel scared and fearful of the unfamiliar sensation. I have never considered myself a smart person.

However, I really want to make this happen! Our country is now in a state of constant conflict, and people hate and despise each other due to strong propaganda.

As a result, the birth rate has dropped to less than 1%, leading to a decline in the population. Many people hide their pain inside and have no will to solve it. They just drink with their friends to relieve their pain. This is obviously not a solution. Therefore, Korea has a really serious suicide rate.

I may not be able to solve this problem, but I want to put one small brick to build a big barrier to stop hatred. Can an ordinary person who knows nothing learn the common sense and study needed to build an LLM model? And what direction should one take to study one by one?"
43,1321qjc,deeplearning,GPT,comments,2023-04-28 16:55:02,Tokenization of numerical series,Turbulent-Bet-6326,False,0.72,3,https://www.reddit.com/r/deeplearning/comments/1321qjc/tokenization_of_numerical_series/,24,1682700902.0,"Hello,

im trying to use GPT architecture on numerical data and i need to tokenize the input sequence of floats and then process it using GPT model. Any ideas how i could do that ? I tried to search the internet for it but with no luck.

&#x200B;

Much thanks"
44,1350qtu,deeplearning,GPT,comments,2023-05-01 20:45:08,What are some small LLM models or free LLM APIs for tiny fun project?,silent_lantern,False,0.96,34,https://www.reddit.com/r/deeplearning/comments/1350qtu/what_are_some_small_llm_models_or_free_llm_apis/,19,1682973908.0,"Hi, I'm looking for a free/opensource api to build a small GPT webapp for fun. I want to deploy it on something like Heroku and use Flask in the backend. 


I'm also open to uploading a small-ish llm model on Heroku and use that to answer chat like queries from users.


Do you know of any such small foss models and/or free APIs?"
45,12c43uu,deeplearning,GPT,comments,2023-04-05 01:36:40,Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT,Time_Key8052,False,0.94,85,https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/,19,1680658600.0,"Vicuna : ChatGPT Alternative, Open-Source, High Quality and Low Cost 

&#x200B;

[ Relative Response Quality Assessed by GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a072b91)

Vicuna-13B has demonstrated competitive performance against other open-source models, such as Stanford Alpaca, by fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.

Evaluation using GPT-4 as a judge shows that Vicuna-13B achieves more than 90% of the quality of OpenAI ChatGPT and Google Bard AI, while outperforming other models such as Meta LLaMA (Large Language Model Meta AI) and Stanford Alpaca in more than 90% of cases.

The cost of training Vicuna-13B is approximately $300.

The training and serving code, along with an online demo, are publicly available for non-commercial use.

&#x200B;

More Information : [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT](https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)

Discord Server : [https://discord.gg/h6kCZb72G7](https://discord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysorg](https://twitter.com/lmsysorg)"
46,10mhyek,deeplearning,GPT,comments,2023-01-27 10:45:48,⭕ What People Are Missing About Microsoft’s $10B Investment In OpenAI,LesleyFair,False,0.95,119,https://www.reddit.com/r/deeplearning/comments/10mhyek/what_people_are_missing_about_microsofts_10b/,16,1674816348.0,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/sg24cw3zekea1.png?width=720&format=png&auto=webp&s=9eeae99b5e025a74a6cbe3aac7a842d2fff989a1)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

# A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

# OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

# The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models requires deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

# Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

# The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

# What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (aubsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)"
47,za73dc,deeplearning,GPT,comments,2022-12-02 01:35:02,GPT-3 Generated Rap Battle between Yann LeCun & Gary Marcus,hayAbhay,False,0.99,137,https://i.redd.it/ybfcfvez1e3a1.png,16,1669944902.0,
48,125pbbf,deeplearning,GPT,comments,2023-03-29 14:13:46,AI Startup Cerebras releases open source ChatGPT-like alternative models,Time_Key8052,False,0.94,46,https://gpt4chatgpt.tistory.com/entry/Cerebras-releases-open-source-ChatGPT-like-alternative-models,14,1680099226.0,
49,10irh5u,deeplearning,GPT,comments,2023-01-22 19:11:36,Apple M2 Max 96 GB unified memory for larger models vs multiple 24GB GPUs or 40GB A100s?,lol-its-funny,False,1.0,22,https://www.reddit.com/r/deeplearning/comments/10irh5u/apple_m2_max_96_gb_unified_memory_for_larger/,14,1674414696.0,"How feasible is it to use an Apple Silicon M2 Max, which has about [96 GB unified memory](https://www.apple.com/shop/buy-mac/macbook-pro/16-inch-space-gray-apple-m2-max-with-12-core-cpu-and-38-core-gpu-1tb) for ""large model"" deep learning? I'm inspired by the the [Chinchilla](https://arxiv.org/abs/2203.15556) paper that shows a lot of promise at 70B parameters. Outperforming ultra large models like Gopher (280B) or GPT-3 (175B) there is hope for working with < 70B parameters without needing a super computer. At least for fine tuning. I've been working with GPT-J but want to scale/tinker with larger open-sourced models.

However, I don't know how clearly the CompSci theory (M2 Max's 38-core GPU, 16 core Neural Engine accessing 96 GB unified memory) maps out to the IT reality (toolkits and libraries on macOS actually using it). My exposure is mostly around Jupyter books on Colab Pro+ (A100s) and nvidia 3080 GPUs (locally). 

I appreciate your guidance."
50,1148t20,deeplearning,GPT,comments,2023-02-17 02:54:52,How likely is ChatGPT to be weaponized as an information pollution tool? What are the possible implementation paths? How to prevent possible attacks?,zcwang0702,False,0.75,12,https://www.reddit.com/r/deeplearning/comments/1148t20/how_likely_is_chatgpt_to_be_weaponized_as_an/,14,1676602492.0,
51,133f4m4,deeplearning,GPT,comments,2023-04-30 03:53:26,Why do neural networks like ChatGPT use memory and processing power while at rest?,Will_Tomos_Edwards,False,0.38,0,https://www.reddit.com/r/deeplearning/comments/133f4m4/why_do_neural_networks_like_chatgpt_use_memory/,13,1682826806.0,"It is surprising to me that the default behavior of many neural networks is to run processes and use resources (RAM, CPU or their equivalent) . Why are they not just stored in memory by default? Obviously they must take up a lot of memory, but I don't understand why the default behaviour is to be active."
52,12sccd7,deeplearning,GPT,comments,2023-04-19 22:19:12,Would it be possible to help transformer models avoid lying by having the RLHF stage include 'invalid' statements?,brainhack3r,False,0.77,9,https://www.reddit.com/r/deeplearning/comments/12sccd7/would_it_be_possible_to_help_transformer_models/,13,1681942752.0,"I'm trying to understand the transformer/GPT models and one of the things I've been curious about is the tendency for LLMs to lie.

My background is search + big data and I'm pivoting into AI so still trying to understand a lot of this stuff.

My understanding is that GPT4 was trained with a base model, then it was aligned via RLHF,.

My thinking is that you could train GPT4 to not lie by generating a number of 'invalid' statements.

Such as:

Mickey Mouse was elected President of the United States in [invalid] 

The idea here would be to have predictions for things that are generally not true so that the model can realize when it's 'lying'"
53,10fw22o,deeplearning,GPT,comments,2023-01-19 07:55:49,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.9,70,https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/,11,1674114949.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=webp&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
54,zk5esp,deeplearning,GPT,comments,2022-12-12 17:29:39,ChatGPT context length,Wild-Ad3931,False,0.97,30,https://www.reddit.com/r/deeplearning/comments/zk5esp/chatgpt_context_length/,10,1670866179.0,"How come ChatGPT can follow entire discussions whereas nowaday's LLM are limited (to the best of my knowledge) 4096 tokens ?

I asked it and it is not able to answer neither, and I found nothing on Google because no paper is published. I was also curious to understand how come Beamsearch was so fast with ChatGPT.

https://preview.redd.it/o6djsmpb5i5a1.png?width=1126&format=png&auto=webp&s=98baae5bf6fa294db408f0530214f8afa8a32a0b"
55,1180x0e,deeplearning,GPT,comments,2023-02-21 11:06:33,I created a Search Engine For Books using GPT-3 🔎📘. Here's how you can create it too:,Pritish-Mishra,False,0.69,5,https://youtu.be/SXFP4nHAWN8,8,1676977593.0,
56,ylj1ux,deeplearning,GPT,comments,2022-11-03 23:55:15,BlogNLP: AI Writing Tool,britdev,False,1.0,36,https://www.reddit.com/r/deeplearning/comments/ylj1ux/blognlp_ai_writing_tool/,9,1667519715.0,"Hey everyone,

I created this web app using Open AI's GPT-3 (Davinci model). The purpose here is to provide a free tool to allow people to generate blog content/outlines/headlines and help with writer's block. Will continue to improve it over time, but just a side project I figured would provide some value to you all. Hope you all enjoy and please share ❤️

[https://www.blognlp.com/](https://www.blognlp.com/)"
57,10okyg3,deeplearning,GPT,comments,2023-01-29 22:39:07,[P] We built a browser extension that unlocks browser mode capabilities using ChatGPT: MULTI·ON: AI Web Co-Pilot powered by ChatGPT,DragonLord9,False,0.95,64,https://v.redd.it/2hw47h0b82fa1,8,1675031947.0,
58,12ff87f,deeplearning,GPT,comments,2023-04-08 07:55:07,need help. GPT-3.5 can't solve it.,ryanultralifeio,False,0.29,0,https://www.reddit.com/r/deeplearning/comments/12ff87f/need_help_gpt35_cant_solve_it/,8,1680940507.0,"Trying to make a schedule for the league, here are the constraints.  I think it should be tailormade for AI.


Schedule May 2023 Games.

￼￼

I need you to schedule games between 7 teams, on 4 fields, beginning on Monday May 1st for the whole month of May 2023. 

The 4 fields are; Quincy, Portola, Chester and Loyalton. Fields in Quincy, Portola and Loyalton are available beginning May 1st. The field in Chester is available beginning May 8th.

 Saturdays can have 3 games per day at either 10am, 1pm, or 4 pm. 

No games on Sunday. 

Monday, Tuesday, Wednesday, Thursday, and Friday games are at 5:00. 

Mondays, Tuesdays, Wednesdays, Thursdays, and Fridays can have games played on 3 different fields at the same time. 

There are 7 teams. Quincy Red, Quincy Blue, Quincy Grey, Portola Padres, Portola Dodgers, Chester Giants and Loyalton. 

All teams can only play each other 2 times in May with the exceptions of Quincy Grey and Quincy Red, Quincy Grey and Quincy Blue, and Quincy Grey and Chester Giants, who can only play each other 1 time in May. 

Only Loyalton cannot play on May 3,4, or 5 for Sierra Nevada Journeys. 

All teams are unavailable to play May 26,27,29 for Memorial Day Weekend. 

All teams are unavailable to play May 17,18,19 for 6th grade field trip. 

Each team will play one home game against each other, except for the teams only playing one game. 

Quincy Blue only plays home games on Quincy field on Mondays, and Thursdays. 

Quincy Grey only plays home games on Quincy field on Wednesdays, and Fridays. 

Quincy Red only plays home games on Quincy Field on Tuesdays, Thursdays, and Fridays. 

Loyalton only plays home games on Loyalton field. 

Chester Giants only play Home Games on Chester field. 

Portola Padres only play home games on Portola field. 

Portola Dodgers only play home games on Portola field. 

Each team can play a maximum of two games per week. 

A team cannot play without two calenders days between games. 

A team cannot play two games on consecutive days.

A team cannot play two games on the same day. 

Teams must have at least 9 games.

Put the total number of games played per team at the bottom of the whole months schedule.

2+ hours a no good results........."
59,13gv1zj,deeplearning,GPT,comments,2023-05-13 22:42:26,Domain specific chatbot. Semantic search isn't enough.,mldlbr,False,0.84,8,https://www.reddit.com/r/deeplearning/comments/13gv1zj/domain_specific_chatbot_semantic_search_isnt/,8,1684017746.0,"Hi guys, I'm struggling to find a reliable solution to this specific problem.

I  have a huge dataset with chat conversations, about several topics. I  want to ask questions and retrieve information about these conversations in a chatbot way.

I have tried  semantic search with chatGPT to answer questions about these  conversations. The problem is that semantic search only returns top  similar sentences, and doesn't ‘read’ all conversations, that’s not  enough to answer generic questions, just very specific ones. For  example, if I ask “What are these people talking about person X?” it  will return only the top sentences (through semantic similarity) and  that will not tell the whole story. The LLM’s models have a limit of  tokens, so I can’t send the whole dataset as context.

Is there any approach to giving a reliable answer based on reading all the messages?

Any ideas on how to approach this problem?"
60,129t3tl,deeplearning,GPT,comments,2023-04-02 18:10:37,Should we draw inspiration from Deep learning/Computer vision world for fine-tuning LLMs?,Vegetable-Skill-9700,False,0.76,12,https://www.reddit.com/r/deeplearning/comments/129t3tl/should_we_draw_inspiration_from_deep/,8,1680459037.0,"With HuggingGPT, BloombergGPT, and OpenAI's chatGPT store, it looks like the world is moving towards specialized GPTs for specialized tasks. What do you think are the best tips & tricks when it comes to fine-tuning and refining these task-specific GPTs?

Over the last decade, I have built many computer vision models (for human pose estimation, action classification, etc.), and our general approach was always based on Transfer learning. Take a state-of-the-art public model and fine-tune it by collecting data for the given use case.

Do you think that paradigm still holds true for LLMs?

Based on my experience, I believe observing the model's performance as it interacts with real-world data, identifying failure cases (where the model's outputs are wrong), and using them to create a high-quality retraining dataset will be the key.

&#x200B;

P.S. I am building an open-source project UpTrain ([https://github.com/uptrain-ai/uptrain](https://github.com/uptrain-ai/uptrain)), which helps data scientists to do so. We just wrote a blog on how this principle can be applied to fine-tune an LLM for a conversation summarization task. Check it out here: [https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation\_summarization](https://github.com/uptrain-ai/uptrain/tree/main/examples/coversation_summarization)"
61,zen8l4,deeplearning,GPT,comments,2022-12-07 00:33:41,Are currently state of art model for logical/common-sense reasoning all based on NLP(LLM)?,Accomplished-Bill-45,False,0.97,22,https://www.reddit.com/r/deeplearning/comments/zen8l4/are_currently_state_of_art_model_for/,6,1670373221.0,"Not very familiar with NLP, but I'm playing around with OpenAI's ChatGPT; particularly impressed by its reasoning, and its thought-process.

Are all good reasoning models derived from NLP (LLM) models with RL training method at the moment?

What are some papers/research team to read/follow to understand this area better and stay on updated?

&#x200B;

&#x200B;

for ChatGPT. I've tested it with following cases

Social reasoning ( which does a good job; such as: if I'm going to attend meeting tonight. I have a suit, but its dirty and size doesn't fit. another option is just wear underwear, the underwear is clean and fit in size. Which one should I wear to attend the meeting. )

Psychological reasoning ( it did a bad job.I asked it to infer someone's intention given his behaviours, expression, talks etc.)

Solving math question ( it’s ok, better then Minerva)

Asking LSAT logic game questions ( it gives its thought process, but failed to give correct answers)

I also wrote up a short mystery novel, ( like 200 words, with context) ask if it can tell is the victim is murdered or committed suicide; if its murdered, does victim knows the killer etc. It actually did ok job on this one if the context is clearly given that everyone can deduce some conclusion using common sense."
62,11ium8l,deeplearning,GPT,comments,2023-03-05 11:10:56,LLaMA model parallelization and server configuration,ChristmasInOct,False,0.97,24,https://www.reddit.com/r/deeplearning/comments/11ium8l/llama_model_parallelization_and_server/,8,1678014656.0,"Hey everyone,

First of all, tldr at bottom, typed more than expected here.  

Please excuse the rather naive perspective I have here.  I've followed along with great interest, but this is not my industry.

Regardless, I have spent the past 3-4 days falling down a brutally obsessive rabbit hole, and I cannot seem to find this information.  I'm assuming it's just that I am missing context of course, and regardless of whether there is a clear answer, I'm trying to get a better understanding of this topic so that I could better appraise the situation myself.

Really I suppose I have two questions.  **The first** is regarding model parallelization.

I'm assuming this is not generic whatsoever.  What is the typical process engineers go about for designing such a pipeline?  Specifically in regards to these new LLaMA models, is something like ALPA relevant?  Deepspeed?

More importantly, what information should I be seeking to determine this myself?

This roughly segues to my **second inquiry**.

The reason I'm curious about splitting the model pipeline etc., is that I am potentially in interested in standing a server up for this.  Although I don't have much of a budget for this build (\~$30-40K is the rough top-end, but I'd be a lot happier around $20-25K), the money is there if I can genuinely satisfy my use-case.

I work at a small, but borderline manic startup working on enterprise software; 90% of the work we're doing based in the react/node ecosystem, some low-level work for backend services, and some very interesting database work that I have very little to do with.  I am a fullstack engineer that grew up playing with C++ => C#, and somehow ended up spending all of my time r/w'ing javascript.  Lol.  Anyways.

Part of our roadmap since GPT-3 and the playground were made publicly accessible, involves usage of these transformer models, and their ability to interpret natural language inputs, whether from user inputs, or scraped input values generated somewhere in a chain of requests / operations.

Seeing GPT-3 in action made me specifically realize that my estimations on this technology had been wildly off.  Seeing ChatGPT in action and uptick, the API's becoming available, has me further panicked.

Running our inference through their API has never really been an option for us.  I haven't even really looked that far into it, but bottom line the data running through our platform is all back-office, highly sensitive business information, and many have agreements explicitly restricting the movement of data to or from any cloud services, with Microsoft, Amazon, and Google all specifically mentioned.

Regardless of the reasoning for these contracts, the LLaMA release has had me obsessed over this topic in more detail than before, and whether or not I would be able to get this setup privately, for our use-case.

**To get to the actual second inquiry**:

Say I want to throw a budget rig together for this in a server cabinet.  Am I able to effectively parallelize the LLaMA model, well enough to justify going with 24GB VRAM 4090's in the rig?  Say I do so with DeepSpeed, or some of the standard model parallelization libraries.

Is the performance cost low enough to justify taking the extra compute here over 1/3 - 1/2 as many RTX6000 ADA's?

Or should I be grabbing the 48GB ADA's?

Like I said, I apologize for the naivety, I'm really looking for more information so that I can start to put this picture together better on my own.  It really isn't the easiest topic to research with how quickly things seem to move, and the giant gap between conversation depths (gamer || phd in a lot of the most interesting or niche discussions, little between).

Thank you very much for your time.

TL;DR - Any information on LLaMA model parallelization at the moment?  Will it be compatible with things like zero or alpa?  How about for throwing a rig together right now for fine-tuning and then running inference on the LLaMA models?  48GB 6000 ADA's, or 24GB 4090's?

Planning on putting it in a mostly empty 42U cabinet that also houses our primary web server and networking hardware, so if there is a sales pitch for 4090's across multiple nodes here, I do have a massive bias as the kind of nerd that finds that kind of hardware borderline erotic.

Hydro and cooling are not an issue, just usage of the budget and understanding the requirements / approach given memory limitations, and how to avoid communication bottlenecks or even balance them against raw compute.

Thanks again everyone!"
63,zboc8w,deeplearning,GPT,comments,2022-12-03 19:29:01,BlogNLP: AI Blog Writing Tool,britdev,False,0.88,12,https://www.reddit.com/r/deeplearning/comments/zboc8w/blognlp_ai_blog_writing_tool/,7,1670095741.0,"Hey everyone,

I developed this web app with Open AI's GPT-3 to provide a free, helpful resource for generating blog content, outlines, and more - so you can beat writer's block! I'm sure you'll find it useful and I'd really appreciate it if you shared it with others ❤️.

[https://www.blognlp.com/](https://www.blognlp.com/)"
64,112u10p,deeplearning,GPT,comments,2023-02-15 09:25:30,[P] From “iron manual” to “Iron Man” — Augmenting GPT for fast editable memory to enable context aware question & answering,skeltzyboiii,False,0.97,41,https://i.redd.it/yujf2enambia1.gif,7,1676453130.0,
65,1325a0j,deeplearning,GPT,comments,2023-04-28 18:10:08,The Little Book of Deep Learning is a 140 page (phone-formatted!) technical introduction of the necessary background for denoising diffusion and GPT models. BY-NC-SA.,FrancoisFleuret,False,0.97,81,https://fleuret.org/public/lbdl.pdf,6,1682705408.0,
66,12z08ni,deeplearning,GPT,comments,2023-04-25 23:50:32,Research on the political biases of ChatGPT,Mysterious_Potato132,False,0.86,5,/r/ChatGPT/comments/12yz49i/research_on_the_political_biases_of_chatgpt/,6,1682466632.0,
67,117l2vf,deeplearning,GPT,comments,2023-02-20 21:27:58,Fine tuning a GPT for text generation,nashcaps2724,False,0.82,7,https://www.reddit.com/r/deeplearning/comments/117l2vf/fine_tuning_a_gpt_for_text_generation/,6,1676928478.0,"Hi all, let me lay out my problem…

Imagine there are two corpora, Corpus A (100,000~) and Corpus B (20,000,000~). 

Individuals create reports for corpus A based on the information in corpus B. 

My idea was to pretrain a GPT on corpus A, and fine tune it to take documents from corpus B as an input, and output text in the style of corpus A (essentially a mix of text generation and summarization). 

Is this something folks think is even feasible? Should I be pretaining the GPT on both corpora or just corpus A? I thought of both fine tuning an OpenAI GPT and training from scratch. 

Any advice would be welcome!"
68,10gs1ik,deeplearning,GPT,comments,2023-01-20 08:53:58,Gotcha,actual_rocketman,False,0.94,64,https://i.redd.it/yyh41pnje7da1.jpg,5,1674204838.0,
69,11evrik,deeplearning,GPT,comments,2023-03-01 05:52:05,Career pivot for a SWE considering chatGPT disruption?,Which-Distance1384,False,0.6,1,https://www.reddit.com/r/deeplearning/comments/11evrik/career_pivot_for_a_swe_considering_chatgpt/,5,1677649925.0,"tl;dr : what to do to board GPT ship?

&#x200B;

I really dont think ChatGPT will replace jobs very soon. Not until many software and apps are developed to replace workers, e.g., something that can do HR job, read internal docs and summarize, code from a design, etc etc. And all that needs experts. Given the field is pretty new and there are not many experts in this field, at least for the scale needed, I think the process of job loss takes some time. For near future probably we all SWEs can enjoy the more efficient coding, documentation readintg, etc. But for sure in the future there will be changes.

&#x200B;

I have been bored with my job as a SWE for a while. I have worked mostly in Cloud all my career (AWS and GCP) and always wanted to try something new.

&#x200B;

I find ChatGPT disruptive and interesting and want to be a part of it. I wonder what is the best way for me to join Large Language Model efforts? Which ways have best ROI?

\- Going to a PhD program and becoming a researcher?

\- Getting an MSC and become some sort of NLP AI engineer?

\- Finding internal teams that are willing to give me a try on their research/product?

&#x200B;

&#x200B;"
70,11x3p2u,deeplearning,GPT,comments,2023-03-21 02:06:28,CoDev- A GPT 4.0 Virtual Developer To Generate Apps,aisaint,False,0.75,8,https://www.reddit.com/r/deeplearning/comments/11x3p2u/codev_a_gpt_40_virtual_developer_to_generate_apps/,5,1679364388.0,"&#x200B;

&#x200B;

CoDev is a GPT 4.0 virtual developer prompt to help you create and refine boilerplates/apps. You can get the prompt from my GitHub link below, paste it in a new Chat session, and issue the commands (see How To Use CoDev). In this article, we will use CoDev to create a React/Typescript/MUI dashboard boiler plate

[https://medium.com/@etherlegend/codev-a-gpt-4-0-virtual-developer-to-build-app-boilerplates-34a431e779c7](https://medium.com/@etherlegend/codev-a-gpt-4-0-virtual-developer-to-build-app-boilerplates-34a431e779c7)"
71,12ljbt8,deeplearning,GPT,comments,2023-04-14 04:09:58,AgentGPT and AutoGPT with Self-planning Capabilities,deeplearningperson,False,0.6,2,https://youtu.be/1ohmpaA_IWo,5,1681445398.0,
72,12xfegq,deeplearning,GPT,comments,2023-04-24 13:14:39,Applications of GPT,AcornWizard,False,0.3,0,https://www.reddit.com/r/deeplearning/comments/12xfegq/applications_of_gpt/,5,1682342079.0,"Hello. Is ChatGPT currently the only implemented application that uses GPT? Looking on the internet I see a lot of flashy but often vague talk about potential applications, yet I have not found more implemented uses."
73,12xzadf,deeplearning,GPT,comments,2023-04-24 22:41:22,AbridgIt - a browser extension that uses GPT to summarize any article you find on the web with a single click,nick313,False,0.74,13,https://www.reddit.com/r/deeplearning/comments/12xzadf/abridgit_a_browser_extension_that_uses_gpt_to/,4,1682376082.0,"Hi everyone,

I’d love your feedback on a new project I’m working on called [AbridgIt](http://www.abridgit.com/). When playing with GPT, one of my favorite things to ask it is to summarize long text. So, I built a simple Chrome browser extension that will automatically summarize any article you find on the web with a single click. This is version 1 so it’s pretty simple, but I would love to get some people to try it (it’s free) and give some feedback.

Example of how it works:

&#x200B;

https://preview.redd.it/m1ryu2u9uwva1.png?width=640&format=png&auto=webp&s=4626472cfaed0b1cedbb3492f1a1209491a8a265

 Check it out and let me know what you think."
74,1096byl,deeplearning,GPT,comments,2023-01-11 14:41:25,What do you all think about these “SEO is Dead” articles?,Aggressive-Twist-252,False,0.91,63,https://www.reddit.com/r/deeplearning/comments/1096byl/what_do_you_all_think_about_these_seo_is_dead/,3,1673448085.0,"I keep seeing [articles](https://jina.ai/news/seo-is-dead-long-live-llmo/) like this over the years and it made me wonder. Is SEO really dead? Or will it evolve? Back then I kept wondering if it’s true or not. Some believe SEO is dead, some don’t. But now with tools like Chat GPT and Midjourney, I think it’s time to take a look back and see how this might change SEO or if it will “kill” SEO.

I keep seeing threads and discussions seeing how people are excited and worried at the same time with how AI might be able to do a better job. But the way I see it, AI content still needs a person to tell it what to do and make the writing look nice. And also I think that the internet will have a lot of writing that was made by AI and that might change how we find things online. You might also see a ton of content being written by AI and trigger some plagiarism detectors and have a lot of websites get penalized. Hopefully the internet won’t be filled with boilerplate copy/pasted content coming from Chat GPT.

Well we have Google to filter out trash content anyway. But I know Google has some issues lately that they need to fix. One is that they also have AI that can help people find things on the internet with their search engine, and they need to make sure they are still the best in terms of search. 

The second is that Google needs to find a way to tell if something is really good or not, like how some websites that show art do. Google wants to show the best thing first, but it's hard because sometimes the thing that is the best is also something that Google's customers want people to see. It’s possible that some AI generated contentSo it's kind of tricky.

I have a feeling companies that already make SEO-writing and checking bots are gonna roll out some fresh new models soon. They're gonna be even better than before. These bots are going to write some good articles and product descriptions that are almost perfect. It almost looks like a human wrote the article or description. And all a human will do is quickly check for any false claims and write a headline that doesn't sound like a robot wrote it. 

We can only really tell 5-10 years from now. In the meantime, I’ll probably go back practicing some handyman skills and also go back teaching people how to drive and also be a service driver. These jobs I had in the past were way different from what I am earning now but if the worst comes to worst, at least I have these physical skills ready."
75,12qq3mz,deeplearning,GPT,comments,2023-04-18 15:00:24,Uni project: a FOSS LLM comparison tool - would you find this useful?,copywriterpirate,False,0.93,18,https://www.reddit.com/gallery/12qq3mz,3,1681830024.0,
76,129k24i,deeplearning,GPT,comments,2023-04-02 12:37:38,[N] Software 3.0 Blog Post Release 🔥,DragonLord9,False,0.69,8,https://www.reddit.com/r/deeplearning/comments/129k24i/n_software_30_blog_post_release/,3,1680439058.0,"Hi all, excited to share my blog post on [**Software 3.0**](https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm_campaign=post&utm_medium=web)

https://preview.redd.it/9b4hjkkhugra1.png?width=1500&format=png&auto=webp&s=e341f3ab4c3c8abb206df8daa17428a297ff61e2

The blog post offers an insightful read on the new GPT-powered programming paradigm where the new programming language is simply ""*English*"", as well as recent developments in AI.

The post was originally written before GPT-4 release, and the predictions seem to have held surprisingly well. Knowledge cutoff date 28 Feb 2023.

Please read and share!! Happy to answer any follow-ups here or on DM 😊

Tweet: [https://twitter.com/DivGarg9/status/1642229948185280521?s=20](https://twitter.com/DivGarg9/status/1642229948185280521?s=20)

Blog: [https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm\_campaign=post&utm\_medium=web](https://open.substack.com/pub/divgarg/p/software-3?r=ccbhq&utm_campaign=post&utm_medium=web)"
77,10efwno,deeplearning,GPT,comments,2023-01-17 16:06:37,What nobody tells you about chatGPT and GPT-4,thomas999999,False,0.25,0,https://www.reddit.com/r/deeplearning/comments/10efwno/what_nobody_tells_you_about_chatgpt_and_gpt4/,3,1673971597.0,i wanted to share a nice writeup my friend made about chatGPT [https://medium.com/@christian.bernhard97/what-nobody-tells-you-about-chatgpt-and-gpt-4-c8d97ae9f92d](https://medium.com/@christian.bernhard97/what-nobody-tells-you-about-chatgpt-and-gpt-4-c8d97ae9f92d)
78,11rfgbs,deeplearning,GPT,comments,2023-03-15 00:07:51,GPTMinusOne - AI that hides the use of ChatGPT and GPT4,tomd_96,False,0.86,10,https://github.com/tom-doerr/gpt_minus_one,3,1678838871.0,
79,12b50ng,deeplearning,GPT,comments,2023-04-04 01:28:22,Should generative models be explainable?,sanjeethboddi,False,0.57,1,https://www.reddit.com/r/deeplearning/comments/12b50ng/should_generative_models_be_explainable/,3,1680571702.0,"In case of discriminator models, we wan't to understand what factors/features responsible for the model's decision.   


I'm not sure if generative models should be explainable too. Can anyone explain why/why not generative models should be explainable? Why a DALL-E or GPT response need to be explainable? Aren't we happy with the response it generated?"
80,zth8rl,deeplearning,GPT,comments,2022-12-23 14:35:17,How to change career trajectory to NLP engineer,Creative-Milk-8266,False,0.84,12,https://www.reddit.com/r/deeplearning/comments/zth8rl/how_to_change_career_trajectory_to_nlp_engineer/,3,1671806117.0," A little of my background - 5 years experience in data science. Mostly related to prototyping statistical models and optimization problems, bringing them into production. Some experience in building pipeline and orchestration flow with AWS services.

I have basic understanding on Transformers, BERT, GPT. Did my first NLP Kaggle competition the first time recently.

I'd like my next job to be a NLP engineer. How should I prepare myself for it?

Here's some of the items I'm thinking

&#x200B;

1. More hands on projects I can put on resume, including integration with cloud services. Any recommendations on what kinds of projects I should pick?  
 
2. Tryout techniques of speeding up models like distilled model, dynamic shape, quantization. Anything else that would be helpful?  
 
3. Understand lower level of GPU programming knowledges. Not sure if this is helpful for me finding a NLP job. If so, what kind of things I can do to go deeper on this subject. I'm currently taking [Intro to Parallel Programming](https://classroom.udacity.com/courses/cs344) CS344 course on Udemy (highly recommend btw).  
 
4. Grind leetcode :/  
 

Please point out other important directions I missed."
81,13ib22w,deeplearning,GPT,comments,2023-05-15 15:10:52,[P] ts-tok: Time-Series Forecasting with Classification,arpytanshu,False,0.92,11,https://www.reddit.com/r/deeplearning/comments/13ib22w/p_tstok_timeseries_forecasting_with_classification/,3,1684163452.0,"Hey everyone!  
I wanted to share with you a weekend project I've been working on called **ts-tok**. It's an experimental approach to time-series forecasting that uses classification instead of regression.  
Essentially, we take a range of time-series values and transform them into a fixed vocabulary of tokens. This allows for a seamless training of GPT like models without changing the architecture or loss function.  
There are some subtleties required for data preparation for training, and I've outlined these in the README, so feel free to check it out!  
While this approach 'may' not have practical applications in the real world, it's been a fun experiment to explore.  
I've included some forecasting results in the output/ folder, so feel free to check those out! Open to feedback from the community about potential use cases and limitations of this approach.  
Thanks for taking the time to read about this project!  
[https://github.com/arpytanshu1/ts-tok](https://github.com/arpytanshu1/ts-tok)"
82,12c43uu,deeplearning,GPT,relevance,2023-04-05 01:36:40,Vicuna : an open source chatbot impresses GPT-4 with 90% of the quality of ChatGPT,Time_Key8052,False,0.95,88,https://www.reddit.com/r/deeplearning/comments/12c43uu/vicuna_an_open_source_chatbot_impresses_gpt4_with/,19,1680658600.0,"Vicuna : ChatGPT Alternative, Open-Source, High Quality and Low Cost 

&#x200B;

[ Relative Response Quality Assessed by GPT-4 ](https://preview.redd.it/oaj1s995zyra1.png?width=599&format=png&auto=webp&s=1fb01b017b3b8b4f9149d4b80f40c48d3a072b91)

Vicuna-13B has demonstrated competitive performance against other open-source models, such as Stanford Alpaca, by fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.

Evaluation using GPT-4 as a judge shows that Vicuna-13B achieves more than 90% of the quality of OpenAI ChatGPT and Google Bard AI, while outperforming other models such as Meta LLaMA (Large Language Model Meta AI) and Stanford Alpaca in more than 90% of cases.

The cost of training Vicuna-13B is approximately $300.

The training and serving code, along with an online demo, are publicly available for non-commercial use.

&#x200B;

More Information : [https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT](https://gpt4chatgpt.tistory.com/entry/Vicuna-an-open-source-chatbot-impresses-GPT-4-with-90-of-the-quality-of-ChatGPT)

Discord Server : [https://discord.gg/h6kCZb72G7](https://discord.gg/h6kCZb72G7)

Twitter : [https://twitter.com/lmsysorg](https://twitter.com/lmsysorg)"
83,12ljbt8,deeplearning,GPT,relevance,2023-04-14 04:09:58,AgentGPT and AutoGPT with Self-planning Capabilities,deeplearningperson,False,0.6,2,https://youtu.be/1ohmpaA_IWo,5,1681445398.0,
84,12dcnrm,deeplearning,GPT,relevance,2023-04-06 07:43:06,A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?,Learningforeverrrrr,False,0.5,0,https://www.reddit.com/r/deeplearning/comments/12dcnrm/a_complete_survey_on_generative_ai_aigc_is/,0,1680766986.0,"We recently completed two surveys: one on generative AI and the other on ChatGPT. Generative AI and ChatGPT are two fast-evolving research fields, and we will update the content soon, for which your feedback is appreciated (you can reach out to us through emails on the paper).

The title of this post refers to the first one, however, we put both links below.

**Link to a survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

**Link to a survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)

The following is the **abstract** of the **survey on generative AI** with a summary **figure**.

As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible to miss the opportunity to glimpse AIGC from a certain angle.  In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? To answer this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its **techniques** to **applications**. Modern generative AI relies on various technical foundations, ranging from **model architecture** and **self-supervised pretraining** to **generative modeling** methods (like GAN and diffusion models). After introducing the fundamental techniques, this work focuses on the technological development of various AIGC tasks based on their output type, including **text**, **images**, **videos**, **3D content**, **etc**., which depicts the full potential of ChatGPT's future. Moreover, we summarize their significant applications in some mainstream **industries**, such as **education** and **creativity** content. Finally, we discuss the **challenges** currently faced and present an **outlook** on how generative AI might evolve in the near future.

&#x200B;

https://preview.redd.it/scbpeabnx7sa1.png?width=1356&format=png&auto=webp&s=445da6a707ceb6af75e5305137ad30dcd06c32fe

**Link to a survey on Generative AI (AIGC):** [**A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?**](https://www.researchgate.net/publication/369385153_A_Complete_Survey_on_Generative_AI_AIGC_Is_ChatGPT_from_GPT-4_to_GPT-5_All_You_Need)

**Link to a survey on ChatGPT:** [**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**](https://www.researchgate.net/publication/369618942_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era)"
85,11rihli,deeplearning,GPT,relevance,2023-03-15 01:53:32,How good is GPT-4 compared to ChatGPT?,OnlyProggingForFun,False,0.2,0,https://youtu.be/GroMQETFXLc,1,1678845212.0,
86,10efwno,deeplearning,GPT,relevance,2023-01-17 16:06:37,What nobody tells you about chatGPT and GPT-4,thomas999999,False,0.27,0,https://www.reddit.com/r/deeplearning/comments/10efwno/what_nobody_tells_you_about_chatgpt_and_gpt4/,3,1673971597.0,i wanted to share a nice writeup my friend made about chatGPT [https://medium.com/@christian.bernhard97/what-nobody-tells-you-about-chatgpt-and-gpt-4-c8d97ae9f92d](https://medium.com/@christian.bernhard97/what-nobody-tells-you-about-chatgpt-and-gpt-4-c8d97ae9f92d)
87,12u3j5x,deeplearning,GPT,relevance,2023-04-21 14:29:14,Is there any nano-gpt/pico-gpt like implementation available for stable-diffusion models?,Blue_Dude3,False,0.67,1,https://www.reddit.com/r/deeplearning/comments/12u3j5x/is_there_any_nanogptpicogpt_like_implementation/,1,1682087354.0,"The original paper skips some implementation details like -

* how exactly does the attention mechanism work? What are the query, key, value pairs?
* The loss function of the auto encoder is not clear at all.

and many other small details where the authors have just referenced some other papers.

The implementations available on Github (mainly from stability AI and CompVis) is too complicated to understand since it is written for different architectures, tasks. And the code base does not have comments which is also not helpful.

I would like to have a simple implementation of stable-diffusion model for any one particular task like (text to image or image to image). Understand the purpose of each module / block with reference to the paper.

Can anyone suggest such implementation of stable-diffusion that achieves some reasonable results (like nano-gpt)?"
88,12xfegq,deeplearning,GPT,relevance,2023-04-24 13:14:39,Applications of GPT,AcornWizard,False,0.27,0,https://www.reddit.com/r/deeplearning/comments/12xfegq/applications_of_gpt/,5,1682342079.0,"Hello. Is ChatGPT currently the only implemented application that uses GPT? Looking on the internet I see a lot of flashy but often vague talk about potential applications, yet I have not found more implemented uses."
89,12yqpnp,deeplearning,GPT,relevance,2023-04-25 17:53:47,"Microsoft releases SynapseMl v0.11 with support for ChatGPT, GPT-4, causal learning, and more",mhamilton723,False,0.87,22,https://www.reddit.com/r/deeplearning/comments/12yqpnp/microsoft_releases_synapseml_v011_with_support/,0,1682445227.0,"Today Microsoft launched SynapseML v0.11 with support for ChatGPT, GPT-4, distributed training of huggingface and torchvision models, an ONNX Model hub integration, Causal Learning with EconML, 10x memory reductions for LightGBM, and a newly refactored integration with Vowpal Wabbit. To learn more check out our release notes and please feel give us a star if you enjoy the project!

Release Notes: [https://github.com/microsoft/SynapseML/releases/tag/v0.11.0](https://github.com/microsoft/SynapseML/releases/tag/v0.11.0)

Blog: [https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919](https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/what-s-new-in-synapseml-v0-11/ba-p/3804919)

Thank you to all the contributors in the community who made the release possible!

&#x200B;

[What's new in SynapseML v0.11](https://preview.redd.it/9pqj1mowj2wa1.png?width=4125&format=png&auto=webp&s=a358e73760c847a09cc76f2ed17dc58e15aed5ed)"
90,zk5esp,deeplearning,GPT,relevance,2022-12-12 17:29:39,ChatGPT context length,Wild-Ad3931,False,0.97,30,https://www.reddit.com/r/deeplearning/comments/zk5esp/chatgpt_context_length/,10,1670866179.0,"How come ChatGPT can follow entire discussions whereas nowaday's LLM are limited (to the best of my knowledge) 4096 tokens ?

I asked it and it is not able to answer neither, and I found nothing on Google because no paper is published. I was also curious to understand how come Beamsearch was so fast with ChatGPT.

https://preview.redd.it/o6djsmpb5i5a1.png?width=1126&format=png&auto=webp&s=98baae5bf6fa294db408f0530214f8afa8a32a0b"
91,11wat6c,deeplearning,GPT,relevance,2023-03-20 06:27:48,GPT-4,Genius_feed,False,0.38,0,https://i.redd.it/h1ov2l5p8uoa1.jpg,0,1679293668.0,
92,10okyg3,deeplearning,GPT,relevance,2023-01-29 22:39:07,[P] We built a browser extension that unlocks browser mode capabilities using ChatGPT: MULTI·ON: AI Web Co-Pilot powered by ChatGPT,DragonLord9,False,0.96,68,https://v.redd.it/2hw47h0b82fa1,8,1675031947.0,
93,10n8c80,deeplearning,GPT,relevance,2023-01-28 06:34:28,"A python module to generate optimized prompts, Prompt-engineering & solve different NLP problems using GPT-n (GPT-3, ChatGPT) based models and return structured python object for easy parsing",StoicBatman,False,0.96,17,https://www.reddit.com/r/deeplearning/comments/10n8c80/a_python_module_to_generate_optimized_prompts/,0,1674887668.0,"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis. A common challenge when utilizing LLMs such as #GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.

To address this, we developed **Promptify**, a library that allows for the use of LLMs to solve NLP problems, including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

Features 🚀

* 🧙‍♀️ NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc.) in 2 lines of code with no training data required
* 🔨 Easily add one-shot, two-shot, or few-shot examples to the prompt
* ✌ Output is always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* 💥 Custom examples and samples can be easily added to the prompt
* 💰 Optimized prompts to reduce OpenAI token costs

&#x200B;

* GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)
* Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)
* For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

Try out and share your feedback. Thanks :)

Join our discord for Prompt-Engineering, LLMs and other latest research discussions  
[discord.gg/m88xfYMbK6](https://discord.gg/m88xfYMbK6)

[NER Example](https://preview.redd.it/sjvhtd8b3nea1.png?width=1236&format=png&auto=webp&s=e9a3a28f41f59cd25fe8e95bd1fca56b15f27a6e)

&#x200B;

https://preview.redd.it/fnb05bys3nea1.png?width=1398&format=png&auto=webp&s=096f4e2cbd0a71e795f30cc5e3720316b5e5caf6"
94,zi62fr,deeplearning,GPT,relevance,2022-12-10 22:44:46,InstructGPT,MRMohebian,False,0.75,6,https://www.reddit.com/r/deeplearning/comments/zi62fr/instructgpt/,1,1670712286.0,"Recently, ChatGPT became trendy. It was adopting an algorithm by the name of InstructGPT. 
We go through the paper ""Training Language Models to Follow Instructions with Human Feedback"" in this video and go into extensive detail about how InstructGPT works. 

Please subscribe, leave a comment and share with your friends.

[R]
https://youtu.be/lYRWzCPGM2Q"
95,12rtzak,deeplearning,GPT,relevance,2023-04-19 13:51:18,Alpaca Electron: ChatGPT Locally!,oridnary_artist,False,0.63,4,https://youtu.be/0oz3RaLlTlM,0,1681912278.0,
96,125pbbf,deeplearning,GPT,relevance,2023-03-29 14:13:46,AI Startup Cerebras releases open source ChatGPT-like alternative models,Time_Key8052,False,0.96,48,https://gpt4chatgpt.tistory.com/entry/Cerebras-releases-open-source-ChatGPT-like-alternative-models,14,1680099226.0,
97,11mnvcr,deeplearning,GPT,relevance,2023-03-09 09:27:13,ChatGPT vs Other Chatbots!!,Genius_feed,False,0.31,0,https://i.redd.it/7eqx02homoma1.png,2,1678354033.0,
98,za73dc,deeplearning,GPT,relevance,2022-12-02 01:35:02,GPT-3 Generated Rap Battle between Yann LeCun & Gary Marcus,hayAbhay,False,0.99,139,https://i.redd.it/ybfcfvez1e3a1.png,16,1669944902.0,
99,12z08ni,deeplearning,GPT,relevance,2023-04-25 23:50:32,Research on the political biases of ChatGPT,Mysterious_Potato132,False,0.86,5,/r/ChatGPT/comments/12yz49i/research_on_the_political_biases_of_chatgpt/,6,1682466632.0,
100,12ff87f,deeplearning,GPT,relevance,2023-04-08 07:55:07,need help. GPT-3.5 can't solve it.,ryanultralifeio,False,0.25,0,https://www.reddit.com/r/deeplearning/comments/12ff87f/need_help_gpt35_cant_solve_it/,8,1680940507.0,"Trying to make a schedule for the league, here are the constraints.  I think it should be tailormade for AI.


Schedule May 2023 Games.

￼￼

I need you to schedule games between 7 teams, on 4 fields, beginning on Monday May 1st for the whole month of May 2023. 

The 4 fields are; Quincy, Portola, Chester and Loyalton. Fields in Quincy, Portola and Loyalton are available beginning May 1st. The field in Chester is available beginning May 8th.

 Saturdays can have 3 games per day at either 10am, 1pm, or 4 pm. 

No games on Sunday. 

Monday, Tuesday, Wednesday, Thursday, and Friday games are at 5:00. 

Mondays, Tuesdays, Wednesdays, Thursdays, and Fridays can have games played on 3 different fields at the same time. 

There are 7 teams. Quincy Red, Quincy Blue, Quincy Grey, Portola Padres, Portola Dodgers, Chester Giants and Loyalton. 

All teams can only play each other 2 times in May with the exceptions of Quincy Grey and Quincy Red, Quincy Grey and Quincy Blue, and Quincy Grey and Chester Giants, who can only play each other 1 time in May. 

Only Loyalton cannot play on May 3,4, or 5 for Sierra Nevada Journeys. 

All teams are unavailable to play May 26,27,29 for Memorial Day Weekend. 

All teams are unavailable to play May 17,18,19 for 6th grade field trip. 

Each team will play one home game against each other, except for the teams only playing one game. 

Quincy Blue only plays home games on Quincy field on Mondays, and Thursdays. 

Quincy Grey only plays home games on Quincy field on Wednesdays, and Fridays. 

Quincy Red only plays home games on Quincy Field on Tuesdays, Thursdays, and Fridays. 

Loyalton only plays home games on Loyalton field. 

Chester Giants only play Home Games on Chester field. 

Portola Padres only play home games on Portola field. 

Portola Dodgers only play home games on Portola field. 

Each team can play a maximum of two games per week. 

A team cannot play without two calenders days between games. 

A team cannot play two games on consecutive days.

A team cannot play two games on the same day. 

Teams must have at least 9 games.

Put the total number of games played per team at the bottom of the whole months schedule.

2+ hours a no good results........."
101,11o5zyl,deeplearning,GPT,relevance,2023-03-11 00:38:10,Generate READMEs Using ChatGPT,tomd_96,False,0.92,10,https://www.reddit.com/r/deeplearning/comments/11o5zyl/generate_readmes_using_chatgpt/,0,1678495090.0,"&#x200B;

https://i.redd.it/k375our2a0na1.gif

&#x200B;

You can use this program I wrote to generate readmes: [https://github.com/tom-doerr/codex-readme](https://github.com/tom-doerr/codex-readme)

&#x200B;

It's far from perfect, but I now added ChatGPT and it is surprisingly good at inferring what the project is about. It often generates interesting usage examples and explains the available command line options.

&#x200B;

You probably won't yet use this for larger projects, but I think this can make sense for small projects or single scripts. Many small scripts are very useful but might never be published because of the work that is required to document and explain it. Using this AI might assist you with that.

&#x200B;

Reportedly GPT-4 is coming out next week, which probably would make it even better.

&#x200B;

What do you think?"
102,117l2vf,deeplearning,GPT,relevance,2023-02-20 21:27:58,Fine tuning a GPT for text generation,nashcaps2724,False,0.9,8,https://www.reddit.com/r/deeplearning/comments/117l2vf/fine_tuning_a_gpt_for_text_generation/,6,1676928478.0,"Hi all, let me lay out my problem…

Imagine there are two corpora, Corpus A (100,000~) and Corpus B (20,000,000~). 

Individuals create reports for corpus A based on the information in corpus B. 

My idea was to pretrain a GPT on corpus A, and fine tune it to take documents from corpus B as an input, and output text in the style of corpus A (essentially a mix of text generation and summarization). 

Is this something folks think is even feasible? Should I be pretaining the GPT on both corpora or just corpus A? I thought of both fine tuning an OpenAI GPT and training from scratch. 

Any advice would be welcome!"
103,12nvtm3,deeplearning,GPT,relevance,2023-04-16 04:52:51,"BERT Explorer - Analyzing the ""T"" of GPT",msahmad,False,0.95,18,https://www.reddit.com/r/deeplearning/comments/12nvtm3/bert_explorer_analyzing_the_t_of_gpt/,0,1681620771.0,"If you want to dig deeper into NLP, LLM, Generative AI, you might consider starting with a model like BERT. This tool helps in exploring the inner working of Transformer-based model like BERT. It helped me understands some key concepts like word embedding, self-attention, multi-head attention, encoder, masked-language model, etc. Give it a try and explore BERT in a different way.

BERT == Bidirectional Encoder Representations from TransformersGPT == Generative Pre-trained Transformer

They both use the Transformer model, but BERT is relatively simpler because it only uses the encoder part of the Transformer.

BERT Explorer[https://www.101ai.net/text/bert](https://www.101ai.net/text/bert)

https://i.redd.it/bxxboyyuhaua1.gif"
104,12hbq89,deeplearning,GPT,relevance,2023-04-10 08:13:15,Summarize documents with ChatGPT via Python scripts,rottoneuro,False,0.5,0,https://levelup.gitconnected.com/summarize-documents-with-chatgpt-a43456841cc4,1,1681114395.0,
105,12oj6bi,deeplearning,GPT,relevance,2023-04-16 18:03:39,ChatGPT Math Problem Challenge! (AAAI-MAKE 2023),Neurosymbolic,False,0.33,0,https://youtube.com/watch?v=iRhbOE9U_Tk&feature=share,0,1681668219.0,
106,11evrik,deeplearning,GPT,relevance,2023-03-01 05:52:05,Career pivot for a SWE considering chatGPT disruption?,Which-Distance1384,False,0.6,1,https://www.reddit.com/r/deeplearning/comments/11evrik/career_pivot_for_a_swe_considering_chatgpt/,5,1677649925.0,"tl;dr : what to do to board GPT ship?

&#x200B;

I really dont think ChatGPT will replace jobs very soon. Not until many software and apps are developed to replace workers, e.g., something that can do HR job, read internal docs and summarize, code from a design, etc etc. And all that needs experts. Given the field is pretty new and there are not many experts in this field, at least for the scale needed, I think the process of job loss takes some time. For near future probably we all SWEs can enjoy the more efficient coding, documentation readintg, etc. But for sure in the future there will be changes.

&#x200B;

I have been bored with my job as a SWE for a while. I have worked mostly in Cloud all my career (AWS and GCP) and always wanted to try something new.

&#x200B;

I find ChatGPT disruptive and interesting and want to be a part of it. I wonder what is the best way for me to join Large Language Model efforts? Which ways have best ROI?

\- Going to a PhD program and becoming a researcher?

\- Getting an MSC and become some sort of NLP AI engineer?

\- Finding internal teams that are willing to give me a try on their research/product?

&#x200B;

&#x200B;"
107,128tfvc,deeplearning,GPT,relevance,2023-04-01 17:50:06,Fine-tune GPT on sketch data (stroke-3),mellamo_maria,False,1.0,1,https://www.reddit.com/r/deeplearning/comments/128tfvc/finetune_gpt_on_sketch_data_stroke3/,0,1680371406.0," These past days I have started a personal project where I would like to build a model that, given an uncompleted sketch, it can finish it. I was planning on using some pretrained models that are available in HuggingFace and fine-tune them with my sketch data for my task. The sketch data I have is in stoke-3 format, like the following example:  
\[  
\[10, 20, 1\],  
\[20, 30, 1\],  
\[30, 40, 1\],  
\[40, 50, 0\],  
\[50, 60, 1\],  
\[60, 70, 0\]  
\]  
The first value of each triple is the X-coordinate, the second value the Y-coordinate and the last value is a binary value indicating whether the pen is down (1) or up (0). I was wondering if you guys could give me some instruction/tips about how should I approach this problem? How should I prepare/preprocess the data so I can fit it into the pre-trained models like BERT, GPT, etc. Since it's stroke-3 data and not text or a sequence of numbers, I don't really know how should I treat/process the data.

Thanks a lot! :)"
108,10fw22o,deeplearning,GPT,relevance,2023-01-19 07:55:49,GPT-4 Will Be 500x Smaller Than People Think - Here Is Why,LesleyFair,False,0.9,70,https://www.reddit.com/r/deeplearning/comments/10fw22o/gpt4_will_be_500x_smaller_than_people_think_here/,11,1674114949.0,"&#x200B;

[Number Of Parameters GPT-3 vs. GPT-4](https://preview.redd.it/xvpw1erngyca1.png?width=575&format=png&auto=webp&s=d7bea7c6132081f2df7c950a0989f398599d6cae)

The rumor mill is buzzing around the release of GPT-4.

People are predicting the model will have 100 trillion parameters. That’s a *trillion* with a “t”.

The often-used graphic above makes GPT-3 look like a cute little breadcrumb that is about to have a live-ending encounter with a bowling ball.

Sure, OpenAI’s new brainchild will certainly be mind-bending and language models have been getting bigger — fast!

But this time might be different and it makes for a good opportunity to look at the research on scaling large language models (LLMs).

*Let’s go!*

Training 100 Trillion Parameters

The creation of GPT-3 was a marvelous feat of engineering. The training was done on 1024 GPUs, took 34 days, and cost $4.6M in compute alone \[1\].

Training a 100T parameter model on the same data, using 10000 GPUs, would take 53 Years. To avoid overfitting such a huge model the dataset would also need to be much(!) larger.

So, where is this rumor coming from?

The Source Of The Rumor:

It turns out OpenAI itself might be the source of it.

In August 2021 the CEO of Cerebras told [wired](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/): “From talking to OpenAI, GPT-4 will be about 100 trillion parameters”.

A the time, that was most likely what they believed, but that was in 2021. So, basically forever ago when machine learning research is concerned.

Things have changed a lot since then!

To understand what happened we first need to look at how people decide the number of parameters in a model.

Deciding The Number Of Parameters:

The enormous hunger for resources typically makes it feasible to train an LLM only once.

In practice, the available compute budget (how much money will be spent, available GPUs, etc.) is known in advance. Before the training is started, researchers need to accurately predict which hyperparameters will result in the best model.

*But there’s a catch!*

Most research on neural networks is empirical. People typically run hundreds or even thousands of training experiments until they find a good model with the right hyperparameters.

With LLMs we cannot do that. Training 200 GPT-3 models would set you back roughly a billion dollars. Not even the deep-pocketed tech giants can spend this sort of money.

Therefore, researchers need to work with what they have. Either they investigate the few big models that have been trained or they train smaller models in the hope of learning something about how to scale the big ones.

This process can very noisy and the community’s understanding has evolved a lot over the last few years.

What People Used To Think About Scaling LLMs

In 2020, a team of researchers from OpenAI released a [paper](https://arxiv.org/pdf/2001.08361.pdf) called: “Scaling Laws For Neural Language Models”.

They observed a predictable decrease in training loss when increasing the model size over multiple orders of magnitude.

So far so good. But they made two other observations, which resulted in the model size ballooning rapidly.

1. To scale models optimally the parameters should scale quicker than the dataset size. To be exact, their analysis showed when increasing the model size 8x the dataset only needs to be increased 5x.
2. Full model convergence is not compute-efficient. Given a fixed compute budget it is better to train large models shorter than to use a smaller model and train it longer.

Hence, it seemed as if the way to improve performance was to scale models faster than the dataset size \[2\].

And that is what people did. The models got larger and larger with GPT-3 (175B), [Gopher](https://arxiv.org/pdf/2112.11446.pdf) (280B), [Megatron-Turing NLG](https://arxiv.org/pdf/2201.11990) (530B) just to name a few.

But the bigger models failed to deliver on the promise.

*Read on to learn why!*

What We know About Scaling Models Today

It turns out you need to scale training sets and models in equal proportions. So, every time the model size doubles, the number of training tokens should double as well.

This was published in DeepMind’s 2022 [paper](https://arxiv.org/pdf/2203.15556.pdf): “Training Compute-Optimal Large Language Models”

The researchers fitted over 400 language models ranging from 70M to over 16B parameters. To assess the impact of dataset size they also varied the number of training tokens from 5B-500B tokens.

The findings allowed them to estimate that a compute-optimal version of GPT-3 (175B) should be trained on roughly 3.7T tokens. That is more than 10x the data that the original model was trained on.

To verify their results they trained a fairly small model on vastly more data. Their model, called Chinchilla, has 70B parameters and is trained on 1.4T tokens. Hence it is 2.5x smaller than GPT-3 but trained on almost 5x the data.

Chinchilla outperforms GPT-3 and other much larger models by a fair margin \[3\].

This was a great breakthrough!The model is not just better, but its smaller size makes inference cheaper and finetuning easier.

*So What Will Happen?*

What GPT-4 Might Look Like:

To properly fit a model with 100T parameters, open OpenAI needs a dataset of roughly 700T tokens. Given 1M GPUs and using the calculus from above, it would still take roughly 2650 years to train the model \[1\].

So, here is what GPT-4 could look like:

* Similar size to GPT-3, but trained optimally on 10x more data
* ​[Multi-modal](https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley) outputting text, images, and sound
* Output conditioned on document chunks from a memory bank that the model has access to during prediction \[4\]
* Doubled context size allows longer predictions before the model starts going off the rails​

Regardless of the exact design, it will be a solid step forward. However, it will not be the 100T token human-brain-like AGI that people make it out to be.

Whatever it will look like, I am sure it will be amazing and we can all be excited about the release.

Such exciting times to be alive!

If you got down here, thank you! It was a privilege to make this for you. At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] D. Narayanan, M. Shoeybi, J. Casper , P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee , M. Zaharia, [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021), SC21

\[2\] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,… & D. Amodei, [Scaling laws for neural language model](https://arxiv.org/abs/2001.08361)s (2020), arxiv preprint

\[3\] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. Casas, L. Hendricks, J. Welbl, A. Clark, T. Hennigan, [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022). *arXiv preprint arXiv:2203.15556*.

\[4\] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. Driessche, J. Lespiau, B. Damoc, A. Clark, D. Casas, [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) (2021). *arXiv preprint arXiv:2112.04426*.Vancouver"
109,zdr6l7,deeplearning,GPT,relevance,2022-12-06 01:38:42,ChatGPT explained in 5 minutes,OnlyProggingForFun,False,0.5,0,https://youtu.be/AsFgn8vU-tQ,0,1670290722.0,
110,11x3p2u,deeplearning,GPT,relevance,2023-03-21 02:06:28,CoDev- A GPT 4.0 Virtual Developer To Generate Apps,aisaint,False,0.75,8,https://www.reddit.com/r/deeplearning/comments/11x3p2u/codev_a_gpt_40_virtual_developer_to_generate_apps/,5,1679364388.0,"&#x200B;

&#x200B;

CoDev is a GPT 4.0 virtual developer prompt to help you create and refine boilerplates/apps. You can get the prompt from my GitHub link below, paste it in a new Chat session, and issue the commands (see How To Use CoDev). In this article, we will use CoDev to create a React/Typescript/MUI dashboard boiler plate

[https://medium.com/@etherlegend/codev-a-gpt-4-0-virtual-developer-to-build-app-boilerplates-34a431e779c7](https://medium.com/@etherlegend/codev-a-gpt-4-0-virtual-developer-to-build-app-boilerplates-34a431e779c7)"
111,133f4m4,deeplearning,GPT,relevance,2023-04-30 03:53:26,Why do neural networks like ChatGPT use memory and processing power while at rest?,Will_Tomos_Edwards,False,0.4,0,https://www.reddit.com/r/deeplearning/comments/133f4m4/why_do_neural_networks_like_chatgpt_use_memory/,13,1682826806.0,"It is surprising to me that the default behavior of many neural networks is to run processes and use resources (RAM, CPU or their equivalent) . Why are they not just stored in memory by default? Obviously they must take up a lot of memory, but I don't understand why the default behaviour is to be active."
112,114j09j,deeplearning,GPT,relevance,2023-02-17 12:18:21,"ChatGPT - model, alignment and training explained",Combination-Fun,False,0.72,3,/r/ChatGPT/comments/114izlj/chatgpt_model_alignment_and_training_explained/,0,1676636301.0,
113,136v5n3,deeplearning,GPT,relevance,2023-05-03 18:21:05,Has any one used sentence embeddings of chat gpt? [D],hippier579,False,0.67,2,https://www.reddit.com/r/deeplearning/comments/136v5n3/has_any_one_used_sentence_embeddings_of_chat_gpt_d/,0,1683138065.0,
114,12ivvad,deeplearning,GPT,relevance,2023-04-11 20:07:47,What’s the difference between AutoGPT and BabyAGI?,naed900,False,0.25,0,https://www.reddit.com/r/deeplearning/comments/12ivvad/whats_the_difference_between_autogpt_and_babyagi/,2,1681243667.0,"Read tons of stuff about this, but still can’t see the differences. Help :)?"
115,12cnu4c,deeplearning,GPT,relevance,2023-04-05 15:23:45,Lifeline - Arxiv Conversational Search Assistant Demo (using ChatGPT),CommercialLynx7233,False,0.71,3,https://www.reddit.com/r/deeplearning/comments/12cnu4c/lifeline_arxiv_conversational_search_assistant/,1,1680708225.0,"Hey guys,

I wanted to share a quick side project I built called [Lifeline](https://www.lifeline.dev/). [Lifeline](https://www.lifeline.dev/) is a search assistant on Arxiv Computer Science papers, leveraging ChatGPT. You can use it to find papers on specific topics, get summaries, ask questions about particular CS topics, find datasets or get similar papers. **Essentially, think of it as a conversational assistant that has knowledge about every CS paper published on Arxiv on or after 2022.**

Here are some sample questions: (Here's a [video](https://www.youtube.com/watch?v=VpFRkbKprLE) where I go through some examples)

* Are there any papers examining consciousness in recent AI systems, specifically large language models?
* What is the difference between chain of thought and augmenting language models with API calls?
* Summarize the new GPT-4 model
* Is GPT-4 better than lawyers on the bar exam? (lol...)
* What are some recent approaches for 3D object construction, from natural language?

If you want to contribute or have any questions, email me at: [rahul@lifeline.dev](mailto:rahul@lifeline.dev) .

Thank you!"
116,12gt77m,deeplearning,GPT,relevance,2023-04-09 19:34:12,"ChatGPT for free now , GPT4ALL is now here",oridnary_artist,False,0.79,8,https://www.youtube.com/watch?v=WiCYfi3SUTE&t=1s,0,1681068852.0,
117,11st80q,deeplearning,GPT,relevance,2023-03-16 12:48:36,Alpaca - Train Your GPT-4 for Less Than $100,deeplearningperson,False,0.46,0,https://youtu.be/6qdzsDSduww,2,1678970916.0,
118,132ogn4,deeplearning,GPT,relevance,2023-04-29 09:34:51,Connecting assistants to ChatGPT is nuts! JARVIS is ever closer!,Lewenhart87,False,0.33,0,https://v.redd.it/4257us79jswa1,1,1682760891.0,
119,zctzmf,deeplearning,GPT,relevance,2022-12-05 02:22:37,Building A Virtual Machine Inside ChatGPT,x_abyss,False,0.81,3,https://www.engraved.blog/building-a-virtual-machine-inside/,0,1670206957.0,
120,zck620,deeplearning,GPT,relevance,2022-12-04 20:11:36,5 ChatGPT Tutorial for Total Beginners,dulldata,False,0.4,0,https://www.youtube.com/watch?v=gMb4iYHaONQ,0,1670184696.0,
121,12u89zf,deeplearning,GPT,relevance,2023-04-21 15:38:04,StableLM: The New Best Open Source Base Models For GPT Apps!,l33thaxman,False,0.83,4,https://www.reddit.com/r/deeplearning/comments/12u89zf/stablelm_the_new_best_open_source_base_models_for/,2,1682091484.0,"Stability AI recently release 3B and 7B of what they are calling StableLM.  If the early metrics are anything to go by these models will be the best models to build from for your generative AI applications. StableLM trains on more data like the LLama models, has the largest open source context window of 4096, and is under a permission license! 

[https://youtu.be/z1sFnzgKw\_Q](https://youtu.be/z1sFnzgKw_Q)"
122,10g9ntd,deeplearning,GPT,relevance,2023-01-19 18:46:25,Fine-tuning GPT Models With Docker and WandB,l33thaxman,False,1.0,1,https://www.reddit.com/r/deeplearning/comments/10g9ntd/finetuning_gpt_models_with_docker_and_wandb/,0,1674153985.0,"GPT models are very powerful.  What makes them even more powerful is fine-tuning the models on your own data.  However, installing all the needed packages can be a large headache if you want to fine-tune the larger variants.

This video goes over a repo that allows one to use a docker image and wandb to easily fine-tune models without headaches.

[https://youtu.be/usz8JOxgQFs](https://youtu.be/usz8JOxgQFs)"
123,11nfrhw,deeplearning,GPT,relevance,2023-03-10 05:22:26,[POC] ChatGPT Audio Bot (like Google Assistant and Alexa) - Opensource,JC1DA,False,0.84,17,https://www.reddit.com/r/deeplearning/comments/11nfrhw/poc_chatgpt_audio_bot_like_google_assistant_and/,2,1678425746.0,"Hey guys, just wanna share this bot that I quickly built using ChatGPT API.

GitHub page: [https://github.com/LanyTek/ChatGPT\_Audio\_Bot](https://github.com/LanyTek/ChatGPT_Audio_Bot)

This service allows you to keep talking to the bot using your voice like you often do with Google Assistant or Alexa. It can be used in a lot of scenarios like teaching, gossiping, or quickly retrieving information without the need of typing. 

I have a quick video demo here if you wanna check [https://www.youtube.com/watch?v=e9n0BJfMyKw](https://www.youtube.com/watch?v=e9n0BJfMyKw)

It's open source so feel free to use it for anything you like :)"
124,124uq0t,deeplearning,GPT,relevance,2023-03-28 16:41:49,Cerebras Open Sources Seven GPT models and Introduces New Scaling Law,CS-fan-101,False,0.83,4,/r/mlscaling/comments/124t0hz/cerebras_open_sources_seven_gpt_models_and/,0,1680021709.0,
