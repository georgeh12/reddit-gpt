,id,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,url,num_comments,created,body
0,11vdhrb,datasets,GPT,top,2023-03-19 06:25:24,[Synthetic] datasetGPT - A command-line tool to generate datasets by inferencing LLMs at scale. It can even make two ChatGPT agents talk with one another.,radi-cho,False,0.97,62,https://www.reddit.com/r/datasets/comments/11vdhrb/synthetic_datasetgpt_a_commandline_tool_to/,0,1679207124.0,"GitHub: [https://github.com/radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT)

It can generate texts by varying input parameters and using multiple backends. But, personally, the conversations dataset generation is my favorite: It can produce dialogues between two ChatGPT agents.

Possible use cases may include:

* Constructing textual corpora to train/fine-tune detectors for content written by AI.
* Collecting datasets of LLM-produced conversations for research purposes, analysis of AI performance/impact/ethics, etc.
* Automating a task that a LLM can handle over big amounts of input texts. For example, using GPT-3 to summarize 1000 paragraphs with a single CLI command.
* Leveraging APIs of especially big LLMs to produce diverse texts for a specific task and then fine-tune a smaller model with them.

What would you use it for?"
1,121y4s5,datasets,GPT,top,2023-03-25 20:30:04,scrapeghost. Web scrape using gpt-4 (experimental),cavedave,False,0.96,33,https://jamesturk.github.io/scrapeghost/,9,1679776204.0,I've nothing to do with this. I just thought it looked cool
2,zrr2yr,datasets,GPT,top,2022-12-21 15:38:39,Sample Peyote: generate multi-table synthetic data on any topic using GPT-3,abegong,False,0.91,16,https://www.reddit.com/r/datasets/comments/zrr2yr/sample_peyote_generate_multitable_synthetic_data/,7,1671637119.0,"Last weekend, I created a tool that uses GPT-3 to create synthetic datasets. I call it Sample Peyote, because it hallucinates sample data sets.

Here's a [Star Wars dataset](https://htmlpreview.github.io/?https://github.com/abegong/sample_peyote/blob/main/data/221215-051820-star-wars-character-data/summary-star-wars-character-data.html) that it generated. There are several more examples linked from the [README on github](https://github.com/abegong/sample_peyote). Source code is there, too.

&#x200B;

This was mostly a kick-the-tires project to understand what GPT is capable of, but I wanted it to be based in a real workflow with nontrivial requirements:

* **Start from scratch**: Most synthetic data generators work by taking a sample of real data, and generating a fake dataset that has similar properties. I want to generate (aka ""hallucinate"") data starting from just an idea.
* **Cover any topic**: I want to be able to generate data related to many different topics.
* **Generate a database, not just a table**: I don't just want to generate a table. I want to generate a realistic-feeling database, with multiple tables and realistic use of things like foreign keys, ENUMs, and timestamps.
* **Pass the** [**Enhance That! test**](https://github.com/abegong/sample_peyote/blob/main/README.md#whats-the-enhance-that-test): Generate data that ""feels authentic.""

&#x200B;

I'd love feedback, and ideas for use cases."
3,12jqweq,datasets,GPT,top,2023-04-12 16:07:30,Unlimited data for creating dataset for Intent Recognition and other NLU models,KMiNT21,False,0.67,1,https://www.reddit.com/r/datasets/comments/12jqweq/unlimited_data_for_creating_dataset_for_intent/,0,1681315650.0,"Nice idea to use chatGPT. It would be great if someone took on the task of creating an open datasets, so that resources wouldn't be wasted on work that has  already been done.

[Breaking Through the Limits: How Unlimited Data Collection and Generation Can Overcome Traditional Barriers in Intent Recognition](https://icexp.com/diy/breaking-through-the-limits-how-unlimited-data-collection-and-generation-can-overcome-traditional-barriers-in-intent-recognition-04-12.html)"
4,121y4s5,datasets,GPT,comments,2023-03-25 20:30:04,scrapeghost. Web scrape using gpt-4 (experimental),cavedave,False,0.96,33,https://jamesturk.github.io/scrapeghost/,9,1679776204.0,I've nothing to do with this. I just thought it looked cool
5,zrr2yr,datasets,GPT,comments,2022-12-21 15:38:39,Sample Peyote: generate multi-table synthetic data on any topic using GPT-3,abegong,False,0.84,14,https://www.reddit.com/r/datasets/comments/zrr2yr/sample_peyote_generate_multitable_synthetic_data/,7,1671637119.0,"Last weekend, I created a tool that uses GPT-3 to create synthetic datasets. I call it Sample Peyote, because it hallucinates sample data sets.

Here's a [Star Wars dataset](https://htmlpreview.github.io/?https://github.com/abegong/sample_peyote/blob/main/data/221215-051820-star-wars-character-data/summary-star-wars-character-data.html) that it generated. There are several more examples linked from the [README on github](https://github.com/abegong/sample_peyote). Source code is there, too.

&#x200B;

This was mostly a kick-the-tires project to understand what GPT is capable of, but I wanted it to be based in a real workflow with nontrivial requirements:

* **Start from scratch**: Most synthetic data generators work by taking a sample of real data, and generating a fake dataset that has similar properties. I want to generate (aka ""hallucinate"") data starting from just an idea.
* **Cover any topic**: I want to be able to generate data related to many different topics.
* **Generate a database, not just a table**: I don't just want to generate a table. I want to generate a realistic-feeling database, with multiple tables and realistic use of things like foreign keys, ENUMs, and timestamps.
* **Pass the** [**Enhance That! test**](https://github.com/abegong/sample_peyote/blob/main/README.md#whats-the-enhance-that-test): Generate data that ""feels authentic.""

&#x200B;

I'd love feedback, and ideas for use cases."
6,11vdhrb,datasets,GPT,comments,2023-03-19 06:25:24,[Synthetic] datasetGPT - A command-line tool to generate datasets by inferencing LLMs at scale. It can even make two ChatGPT agents talk with one another.,radi-cho,False,0.96,60,https://www.reddit.com/r/datasets/comments/11vdhrb/synthetic_datasetgpt_a_commandline_tool_to/,0,1679207124.0,"GitHub: [https://github.com/radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT)

It can generate texts by varying input parameters and using multiple backends. But, personally, the conversations dataset generation is my favorite: It can produce dialogues between two ChatGPT agents.

Possible use cases may include:

* Constructing textual corpora to train/fine-tune detectors for content written by AI.
* Collecting datasets of LLM-produced conversations for research purposes, analysis of AI performance/impact/ethics, etc.
* Automating a task that a LLM can handle over big amounts of input texts. For example, using GPT-3 to summarize 1000 paragraphs with a single CLI command.
* Leveraging APIs of especially big LLMs to produce diverse texts for a specific task and then fine-tune a smaller model with them.

What would you use it for?"
7,12jqweq,datasets,GPT,comments,2023-04-12 16:07:30,Unlimited data for creating dataset for Intent Recognition and other NLU models,KMiNT21,False,0.67,1,https://www.reddit.com/r/datasets/comments/12jqweq/unlimited_data_for_creating_dataset_for_intent/,0,1681315650.0,"Nice idea to use chatGPT. It would be great if someone took on the task of creating an open datasets, so that resources wouldn't be wasted on work that has  already been done.

[Breaking Through the Limits: How Unlimited Data Collection and Generation Can Overcome Traditional Barriers in Intent Recognition](https://icexp.com/diy/breaking-through-the-limits-how-unlimited-data-collection-and-generation-can-overcome-traditional-barriers-in-intent-recognition-04-12.html)"
8,121y4s5,datasets,GPT,relevance,2023-03-25 20:30:04,scrapeghost. Web scrape using gpt-4 (experimental),cavedave,False,0.91,30,https://jamesturk.github.io/scrapeghost/,9,1679776204.0,I've nothing to do with this. I just thought it looked cool
9,11vdhrb,datasets,GPT,relevance,2023-03-19 06:25:24,[Synthetic] datasetGPT - A command-line tool to generate datasets by inferencing LLMs at scale. It can even make two ChatGPT agents talk with one another.,radi-cho,False,0.97,62,https://www.reddit.com/r/datasets/comments/11vdhrb/synthetic_datasetgpt_a_commandline_tool_to/,0,1679207124.0,"GitHub: [https://github.com/radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT)

It can generate texts by varying input parameters and using multiple backends. But, personally, the conversations dataset generation is my favorite: It can produce dialogues between two ChatGPT agents.

Possible use cases may include:

* Constructing textual corpora to train/fine-tune detectors for content written by AI.
* Collecting datasets of LLM-produced conversations for research purposes, analysis of AI performance/impact/ethics, etc.
* Automating a task that a LLM can handle over big amounts of input texts. For example, using GPT-3 to summarize 1000 paragraphs with a single CLI command.
* Leveraging APIs of especially big LLMs to produce diverse texts for a specific task and then fine-tune a smaller model with them.

What would you use it for?"
10,zrr2yr,datasets,GPT,relevance,2022-12-21 15:38:39,Sample Peyote: generate multi-table synthetic data on any topic using GPT-3,abegong,False,0.91,16,https://www.reddit.com/r/datasets/comments/zrr2yr/sample_peyote_generate_multitable_synthetic_data/,7,1671637119.0,"Last weekend, I created a tool that uses GPT-3 to create synthetic datasets. I call it Sample Peyote, because it hallucinates sample data sets.

Here's a [Star Wars dataset](https://htmlpreview.github.io/?https://github.com/abegong/sample_peyote/blob/main/data/221215-051820-star-wars-character-data/summary-star-wars-character-data.html) that it generated. There are several more examples linked from the [README on github](https://github.com/abegong/sample_peyote). Source code is there, too.

&#x200B;

This was mostly a kick-the-tires project to understand what GPT is capable of, but I wanted it to be based in a real workflow with nontrivial requirements:

* **Start from scratch**: Most synthetic data generators work by taking a sample of real data, and generating a fake dataset that has similar properties. I want to generate (aka ""hallucinate"") data starting from just an idea.
* **Cover any topic**: I want to be able to generate data related to many different topics.
* **Generate a database, not just a table**: I don't just want to generate a table. I want to generate a realistic-feeling database, with multiple tables and realistic use of things like foreign keys, ENUMs, and timestamps.
* **Pass the** [**Enhance That! test**](https://github.com/abegong/sample_peyote/blob/main/README.md#whats-the-enhance-that-test): Generate data that ""feels authentic.""

&#x200B;

I'd love feedback, and ideas for use cases."
11,12jqweq,datasets,GPT,relevance,2023-04-12 16:07:30,Unlimited data for creating dataset for Intent Recognition and other NLU models,KMiNT21,False,0.67,1,https://www.reddit.com/r/datasets/comments/12jqweq/unlimited_data_for_creating_dataset_for_intent/,0,1681315650.0,"Nice idea to use chatGPT. It would be great if someone took on the task of creating an open datasets, so that resources wouldn't be wasted on work that has  already been done.

[Breaking Through the Limits: How Unlimited Data Collection and Generation Can Overcome Traditional Barriers in Intent Recognition](https://icexp.com/diy/breaking-through-the-limits-how-unlimited-data-collection-and-generation-can-overcome-traditional-barriers-in-intent-recognition-04-12.html)"
12,120lpox,datasets,GPT,relevance,2023-03-24 14:17:51,Similarity semantic search sentences or paragraphs,MultiTiger,False,0.86,5,https://www.reddit.com/r/datasets/comments/120lpox/similarity_semantic_search_sentences_or_paragraphs/,5,1679667471.0,"Hi! I am doing experiments in semantic similarity search. Given a sentence, I need to find the most similar sentence to the given sentence in a data set that consists of sentences or paragraphs, using semantic search. Which means I need to have sentences, that I know are similar. How would I go about finding similar sentences and comprising the data set?"
