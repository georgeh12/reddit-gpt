,id,subreddit,query,sort,date,title,author,stickied,upvote_ratio,score,url,num_comments,created,body
0,11vdhrb,datasets,GPT-3,top,2023-03-19 06:25:24,[Synthetic] datasetGPT - A command-line tool to generate datasets by inferencing LLMs at scale. It can even make two ChatGPT agents talk with one another.,radi-cho,False,0.97,63,https://www.reddit.com/r/datasets/comments/11vdhrb/synthetic_datasetgpt_a_commandline_tool_to/,0,1679207124.0,"GitHub: [https://github.com/radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT)

It can generate texts by varying input parameters and using multiple backends. But, personally, the conversations dataset generation is my favorite: It can produce dialogues between two ChatGPT agents.

Possible use cases may include:

* Constructing textual corpora to train/fine-tune detectors for content written by AI.
* Collecting datasets of LLM-produced conversations for research purposes, analysis of AI performance/impact/ethics, etc.
* Automating a task that a LLM can handle over big amounts of input texts. For example, using GPT-3 to summarize 1000 paragraphs with a single CLI command.
* Leveraging APIs of especially big LLMs to produce diverse texts for a specific task and then fine-tune a smaller model with them.

What would you use it for?"
1,zrr2yr,datasets,GPT-3,top,2022-12-21 15:38:39,Sample Peyote: generate multi-table synthetic data on any topic using GPT-3,abegong,False,0.85,16,https://www.reddit.com/r/datasets/comments/zrr2yr/sample_peyote_generate_multitable_synthetic_data/,7,1671637119.0,"Last weekend, I created a tool that uses GPT-3 to create synthetic datasets. I call it Sample Peyote, because it hallucinates sample data sets.

Here's a [Star Wars dataset](https://htmlpreview.github.io/?https://github.com/abegong/sample_peyote/blob/main/data/221215-051820-star-wars-character-data/summary-star-wars-character-data.html) that it generated. There are several more examples linked from the [README on github](https://github.com/abegong/sample_peyote). Source code is there, too.

&#x200B;

This was mostly a kick-the-tires project to understand what GPT is capable of, but I wanted it to be based in a real workflow with nontrivial requirements:

* **Start from scratch**: Most synthetic data generators work by taking a sample of real data, and generating a fake dataset that has similar properties. I want to generate (aka ""hallucinate"") data starting from just an idea.
* **Cover any topic**: I want to be able to generate data related to many different topics.
* **Generate a database, not just a table**: I don't just want to generate a table. I want to generate a realistic-feeling database, with multiple tables and realistic use of things like foreign keys, ENUMs, and timestamps.
* **Pass the** [**Enhance That! test**](https://github.com/abegong/sample_peyote/blob/main/README.md#whats-the-enhance-that-test): Generate data that ""feels authentic.""

&#x200B;

I'd love feedback, and ideas for use cases."
2,zrr2yr,datasets,GPT-3,comments,2022-12-21 15:38:39,Sample Peyote: generate multi-table synthetic data on any topic using GPT-3,abegong,False,0.92,18,https://www.reddit.com/r/datasets/comments/zrr2yr/sample_peyote_generate_multitable_synthetic_data/,7,1671637119.0,"Last weekend, I created a tool that uses GPT-3 to create synthetic datasets. I call it Sample Peyote, because it hallucinates sample data sets.

Here's a [Star Wars dataset](https://htmlpreview.github.io/?https://github.com/abegong/sample_peyote/blob/main/data/221215-051820-star-wars-character-data/summary-star-wars-character-data.html) that it generated. There are several more examples linked from the [README on github](https://github.com/abegong/sample_peyote). Source code is there, too.

&#x200B;

This was mostly a kick-the-tires project to understand what GPT is capable of, but I wanted it to be based in a real workflow with nontrivial requirements:

* **Start from scratch**: Most synthetic data generators work by taking a sample of real data, and generating a fake dataset that has similar properties. I want to generate (aka ""hallucinate"") data starting from just an idea.
* **Cover any topic**: I want to be able to generate data related to many different topics.
* **Generate a database, not just a table**: I don't just want to generate a table. I want to generate a realistic-feeling database, with multiple tables and realistic use of things like foreign keys, ENUMs, and timestamps.
* **Pass the** [**Enhance That! test**](https://github.com/abegong/sample_peyote/blob/main/README.md#whats-the-enhance-that-test): Generate data that ""feels authentic.""

&#x200B;

I'd love feedback, and ideas for use cases."
3,11vdhrb,datasets,GPT-3,comments,2023-03-19 06:25:24,[Synthetic] datasetGPT - A command-line tool to generate datasets by inferencing LLMs at scale. It can even make two ChatGPT agents talk with one another.,radi-cho,False,0.96,61,https://www.reddit.com/r/datasets/comments/11vdhrb/synthetic_datasetgpt_a_commandline_tool_to/,0,1679207124.0,"GitHub: [https://github.com/radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT)

It can generate texts by varying input parameters and using multiple backends. But, personally, the conversations dataset generation is my favorite: It can produce dialogues between two ChatGPT agents.

Possible use cases may include:

* Constructing textual corpora to train/fine-tune detectors for content written by AI.
* Collecting datasets of LLM-produced conversations for research purposes, analysis of AI performance/impact/ethics, etc.
* Automating a task that a LLM can handle over big amounts of input texts. For example, using GPT-3 to summarize 1000 paragraphs with a single CLI command.
* Leveraging APIs of especially big LLMs to produce diverse texts for a specific task and then fine-tune a smaller model with them.

What would you use it for?"
4,zrr2yr,datasets,GPT-3,relevance,2022-12-21 15:38:39,Sample Peyote: generate multi-table synthetic data on any topic using GPT-3,abegong,False,0.92,18,https://www.reddit.com/r/datasets/comments/zrr2yr/sample_peyote_generate_multitable_synthetic_data/,7,1671637119.0,"Last weekend, I created a tool that uses GPT-3 to create synthetic datasets. I call it Sample Peyote, because it hallucinates sample data sets.

Here's a [Star Wars dataset](https://htmlpreview.github.io/?https://github.com/abegong/sample_peyote/blob/main/data/221215-051820-star-wars-character-data/summary-star-wars-character-data.html) that it generated. There are several more examples linked from the [README on github](https://github.com/abegong/sample_peyote). Source code is there, too.

&#x200B;

This was mostly a kick-the-tires project to understand what GPT is capable of, but I wanted it to be based in a real workflow with nontrivial requirements:

* **Start from scratch**: Most synthetic data generators work by taking a sample of real data, and generating a fake dataset that has similar properties. I want to generate (aka ""hallucinate"") data starting from just an idea.
* **Cover any topic**: I want to be able to generate data related to many different topics.
* **Generate a database, not just a table**: I don't just want to generate a table. I want to generate a realistic-feeling database, with multiple tables and realistic use of things like foreign keys, ENUMs, and timestamps.
* **Pass the** [**Enhance That! test**](https://github.com/abegong/sample_peyote/blob/main/README.md#whats-the-enhance-that-test): Generate data that ""feels authentic.""

&#x200B;

I'd love feedback, and ideas for use cases."
5,11vdhrb,datasets,GPT-3,relevance,2023-03-19 06:25:24,[Synthetic] datasetGPT - A command-line tool to generate datasets by inferencing LLMs at scale. It can even make two ChatGPT agents talk with one another.,radi-cho,False,0.97,63,https://www.reddit.com/r/datasets/comments/11vdhrb/synthetic_datasetgpt_a_commandline_tool_to/,0,1679207124.0,"GitHub: [https://github.com/radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT)

It can generate texts by varying input parameters and using multiple backends. But, personally, the conversations dataset generation is my favorite: It can produce dialogues between two ChatGPT agents.

Possible use cases may include:

* Constructing textual corpora to train/fine-tune detectors for content written by AI.
* Collecting datasets of LLM-produced conversations for research purposes, analysis of AI performance/impact/ethics, etc.
* Automating a task that a LLM can handle over big amounts of input texts. For example, using GPT-3 to summarize 1000 paragraphs with a single CLI command.
* Leveraging APIs of especially big LLMs to produce diverse texts for a specific task and then fine-tune a smaller model with them.

What would you use it for?"
